<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GRAPH MACHINE LEARNING BASED DOUBLY ROBUST ESTIMATOR FOR NETWORK CAUSAL EFFECTS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-05-31">31 May 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">S</forename><forename type="middle">Baharan</forename><surname>Khatami</surname></persName>
							<email>skhatami@ucsd.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">UC San Diego</orgName>
								<orgName type="institution" key="instit2">Johns Hopkins University</orgName>
								<orgName type="institution" key="instit3">UC San Diego</orgName>
								<orgName type="institution" key="instit4">Duke University</orgName>
								<orgName type="institution" key="instit5">UC San Diego</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Harsh</forename><surname>Parikh</surname></persName>
							<email>hparikh4@jh.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">UC San Diego</orgName>
								<orgName type="institution" key="instit2">Johns Hopkins University</orgName>
								<orgName type="institution" key="instit3">UC San Diego</orgName>
								<orgName type="institution" key="instit4">Duke University</orgName>
								<orgName type="institution" key="instit5">UC San Diego</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Haowei</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">UC San Diego</orgName>
								<orgName type="institution" key="instit2">Johns Hopkins University</orgName>
								<orgName type="institution" key="instit3">UC San Diego</orgName>
								<orgName type="institution" key="instit4">Duke University</orgName>
								<orgName type="institution" key="instit5">UC San Diego</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sudeepa</forename><surname>Roy</surname></persName>
							<email>sudeepa@cs.duke.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">UC San Diego</orgName>
								<orgName type="institution" key="instit2">Johns Hopkins University</orgName>
								<orgName type="institution" key="instit3">UC San Diego</orgName>
								<orgName type="institution" key="instit4">Duke University</orgName>
								<orgName type="institution" key="instit5">UC San Diego</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Babak</forename><surname>Salimi</surname></persName>
							<email>bsalimi@ucsd.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">UC San Diego</orgName>
								<orgName type="institution" key="instit2">Johns Hopkins University</orgName>
								<orgName type="institution" key="instit3">UC San Diego</orgName>
								<orgName type="institution" key="instit4">Duke University</orgName>
								<orgName type="institution" key="instit5">UC San Diego</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">GRAPH MACHINE LEARNING BASED DOUBLY ROBUST ESTIMATOR FOR NETWORK CAUSAL EFFECTS</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-05-31">31 May 2024</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2403.11332v2[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Causal Inference</term>
					<term>Semi-Parametric Inference</term>
					<term>Double Machine Learning</term>
					<term>Graph Neural Networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We address the challenge of inferring causal effects in social network data. This results in challenges due to interference -where a unit's outcome is affected by neighbors' treatments -and networkinduced confounding factors. While there is extensive literature focusing on estimating causal effects in social network setups, a majority of them make prior assumptions about the form of networkinduced confounding mechanisms. Such strong assumptions are rarely likely to hold especially in high-dimensional networks. We propose a novel methodology that combines graph machine learning approaches with the double machine learning framework to enable accurate and efficient estimation of direct and peer effects using a single observational social network. We demonstrate the semiparametric efficiency of our proposed estimator under mild regularity conditions, allowing for consistent uncertainty quantification. We demonstrate that our method is accurate, robust, and scalable via extensive simulation study. We use our method to investigate the impact of Self-Help Group participation on financial risk tolerance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Our paper addresses the challenge of causal inference from social networks, a problem crucial for decision-making across various vital domains such as social media, healthcare, and economic networks <ref type="bibr" target="#b31">(Jackson et al., 2008;</ref><ref type="bibr" target="#b48">Ogburn et al., 2022;</ref><ref type="bibr" target="#b2">Atanasov and Black, 2016;</ref><ref type="bibr" target="#b18">Gassen, 2014)</ref>. For instance, causal inference helps understand the impact of recommendation algorithms on user engagement and preferences in social media, evaluate the effect of public health interventions like self-quarantine or school closures on the spread of infectious diseases within specific communities, and assess the influence of participation in self-help groups (SHG) on individuals' financial behaviors. Our paper uses the latter as a case study to investigate whether participation in SHG affects financial risk-taking behavior, observable through outstanding loans as a proxy measure.</p><p>Estimating causal effects from observational network data is challenging due to several factors: First, dependencies among individuals deviate from the traditional i.i.d. assumption, causing standard methods to fail. Second, network dependencies introduce interference between units, where their neighbors' treatments influence an individual's outcome, further complicating causal analysis <ref type="bibr" target="#b29">(Hudgens and Halloran, 2008b;</ref><ref type="bibr" target="#b49">Ogburn and VanderWeele, 2014;</ref><ref type="bibr" target="#b1">Aronow and Samii, 2017;</ref><ref type="bibr" target="#b23">Halloran and Hudgens, 2012;</ref><ref type="bibr" target="#b63">VanderWeele and Tchetgen Tchetgen, 2011)</ref>. Third, a unit's neighbors' covariates act as confounders that require adjusting for a complex set of potentially high-dimensional covariates with variable sizes for each unit, influenced by network structure and topology <ref type="bibr" target="#b64">(VanderWeele and An, 2013;</ref><ref type="bibr" target="#b49">Ogburn and VanderWeele, 2014)</ref>.</p><p>Existing techniques often use simple predefined aggregates to summarize network information and develop estimators with theoretical guarantees, relying on the assumption of sufficiency of these aggregates <ref type="bibr" target="#b15">(Forastiere et al., 2021</ref>; Salimi the IPW estimators to more complex scenarios of interference within networks. Contrasting these developments, our research introduces a doubly robust estimator. This estimator stands out for its enhanced efficiency compared to IPW estimators, even when only the treatment nuisance model is correctly specified. <ref type="bibr" target="#b15">(Forastiere et al., 2021)</ref> defined new causal estimands for treatment and interference in networks and proposed the individual propensity score and neighborhood propensity score by extending the definition of propensity score under neighborhood interference. The main challenge of estimating causal effects in the network is that the potential outcomes of units in the network depend not only on the treatment assignment but also on the network structure. Units in the network receive interference according to the structures of their treated neighborhoods. It is straightforward to assume units with similar treated neighborhoods will receive similar interference. <ref type="bibr" target="#b4">(Auerbach and Tabord-Meehan, 2021)</ref> proposes a nonparametric modeling framework for causal inference under interference in a sparse network using the configuration of other agents and connections nearby as measured by path distance. A local configuration refers to the features of the network (the agents, their characteristics, treatment statuses, and how they are connected) nearby a focal agent as measured by path distance. This framework assigns distances to subgraphs based on treatment assignments and structural isomorphism. The impact of a policy or treatment assignment is then learned by pooling outcome data across similarly configured agents. Similarly, numerous methods have been developed to incorporate neighborhood information into estimation procedures, utilizing techniques like graph embedding Several papers looked into the problem of causal inference under interference in the presence of unobserved confounding and utilized network as a proxy to recover these latent confounders and subsequently adjust for them. <ref type="bibr" target="#b65">(Veitch et al., 2019)</ref> assumes that each person's treatment and outcome are independent of the network once we know that person's latent attributes. It only recovers part of the unobserved confounding relevant for the prediction of the propensity score or conditional expected outcome. Then, it plug-in the estimated values of the nuisance parameters to a standard estimator such as A-IPTW estimator to estimate the causal effect. <ref type="bibr" target="#b20">(Guo, 2019)</ref> extends this by learning representations of hidden confounders through mapping both network structure and features into a shared space, then inferring potential outcomes based on these representations. <ref type="bibr" target="#b11">(Chu et al., 2021)</ref> discusses that as network information is incorporated into the model, we face a new imbalance issue,i.e., imbalance of network structure in addition to the imbalance of observed covariate distributions. It is essential to design a new method that can capture the representation of hidden confounders implied from the imbalanced network structure and observed confounders that exist in the covariates simultaneously. To address this issue, the Graph Infomax Adversarial Learning (GIAL) method was introduced. This approach employs Graph Neural Networks combined with structure mutual information to accurately represent both hidden and observed confounders. Following this, a potential outcome generator predicts the potential outcomes for units in both treatment and control groups, using the learned representations and treatment assignments. Concurrently, a counterfactual outcome discriminator is integrated to correct any imbalances between the treatment and control group representations in this learned space. <ref type="bibr" target="#b12">(Cristali and Veitch, 2022)</ref> proposes a method for causal estimation of contagion effects by adjusting for network-inferred attributes without relying on detailed parametric assumptions. They formalize the target causal effect non-parametrically. The main challenge is that the estimand must depend on the network (because contagion requires knowing who is friends with whom) and the network must itself be modeled as a random variable which is a function of the unobserved confounders (to accommodate homophily). Then, they derive sufficient conditions for the estimated attributes to yield causal identification and give a concrete method for contagion estimation using node embedding techniques to extract the information from the network that is relevant for predicting peer influence. This research approach contrasts with our work in this paper, where we operate under the assumption of unconfoundedness. We utilize network information to account for the dependence between units, enabling us to address and adjust for the potentially complex and high-dimensional confounding present within network structures. <ref type="bibr" target="#b69">(Zhang, 2023)</ref> proposes a non-parametric framework for estimating causal effects under network interference that employs the network embeddings along with matching <ref type="bibr" target="#b53">(ROSENBAUM and RUBIN, 1983)</ref> to estimate the causal effect. A distinct group of studies focuses on representation balancing to address the issue of differing covariate space distributions among treated and control groups. To avoid biased inference, ( <ref type="bibr" target="#b34">(Johansson et al., 2016;</ref><ref type="bibr" target="#b56">Shalit et al., 2017;</ref><ref type="bibr" target="#b68">Yao et al., 2018)</ref>) propose a balancing counterfactual inference using domain-adapted representation learning. <ref type="bibr" target="#b41">(Ma and Tresp, 2020)</ref> extends this by mapping covariate vectors to a feature space, where treated and control groups are balanced through penalizing the distribution discrepancy term (HSIC) between them. This approach is equivalent to finding a feature space such that the treatment assignment and mapped representation become approximately disentangled. Additionally, Graph Neural Networks are employed, followed by an outcome prediction network tailored to the treatment assignment, with a loss function that combines outcome prediction error and distribution discrepancy in the feature space. In the same category, Similarly, <ref type="bibr" target="#b33">(Jiang and Sun, 2022)</ref> introduces NetEst, which uses GNNs to learn representations of a unit's confounders and those of its neighbors. These representations are then utilized in an adversarial learning process to effectively narrow the distribution gaps between standard graph machine learning and networked causal inference objective function by forcing the mismatched distributions to follow uniform distributions and as a result, accurately estimate the observed outcomes. <ref type="bibr" target="#b21">(Guo et al., 2020a)</ref> combines the ideas of using network as a proxy to learn hidden confounders and balancing the covariate representations across treatment and control group using a minimax game optimization problem. First, a GNN is used to map the features and the adjacency matrix of the network structure into latent space to approximate the confounders, aiming to balance confounder representations across treatment groups to fool the critic. The critic component maps the confounders' representation of an instance to a real value, with higher values suggesting a higher likelihood of receiving treatment.The objective is to maximize the distinction between treated and controlled instances. Lastly, an outcome inference function tries to infer outcomes of an instance based on its confounders' representation. The final category in the field of causal effect estimation under interference, and most closely aligned with our research, involves the development of doubly robust estimators as proposed in several studies. <ref type="bibr" target="#b44">(McNealis et al., 2023)</ref> introduces two novel estimators where the interference set is defined as the set of first-order neighbors assuming that the network is a union of disjoint components. The first estimator, a regression estimator with residual bias correction, is endowed with the double robustness property whether or not the outcome has a multilevel structure. The second estimator, a regression estimator with inverse-propensity weighted coefficients, can be shown to be doubly robust if the outcome does not follow a hierarchical model. This work applies M-estimation theory to propose appropriate asymptotic variance estimators followed by empirical proof of the double robustness and efficiency superiority of these estimators over IPW estimators, even with an accurately specified treatment model. Additionally, the research highlights the risk of latent treatment homophily in identifying causal effects and demonstrates how doubly robust estimation can effectively counter this issue. <ref type="bibr" target="#b38">(Laan, 2014)</ref> proposes TMLE, an estimator for treatment and spillover effects and prove asymptotic results under IID assumptions. Finally <ref type="bibr" target="#b48">(Ogburn et al., 2022)</ref> extends this estimator to allow for dependence due to both contagion and homophily and derive asymptotic results that allow the number of ties per node to increase as the network grows. Their approach utilizes predefined aggregates, an efficient influence function, as introduced by <ref type="bibr" target="#b38">(Laan, 2014)</ref>, combined with a moment condition to create a doubly robust estimator. The algorithm employed for estimation resolves the efficient influence function estimating equation through an iterative process. <ref type="bibr" target="#b39">(Leung and Loupos, 2022)</ref> proposes a framework for nonparametric estimation of treatment and spillover effects using observational data from a single large network where interference decays with network distance. They use graph neural networks to estimate the high-dimensional nuisance functions of a doubly robust estimator. They also establish a network analog of approximate sparsity to justify the use of shallow architectures. <ref type="bibr" target="#b39">(Leung and Loupos, 2022)</ref> requires the treatment as well as the neighborhood exposures to be discrete. This can be highly unrealistic in network scenarios. For instance, when studying the effectiveness of vaccination, the protection due to social neighbors' vaccination is often not binary but depends on the proportion of neigbhors vaccinated. Similar is true for social media settings where one is interested in the effectiveness of an ad campaign. Our work allows for the exposures and treatments to be continuous. This is of a prime interest to us especially for our applied example dealing with self-help groups, microinsurance and risk appetite. Additionally in our research, we calculate two causal estimands: the average direct effect and the average peer effect. This distinction is vital across various real-life scenarios where it's essential to differentiate between the impact of a unit's treatment and that of its neighbors. For example, in the case study highlighted in our paper, we examine the effects of Self-Help Group participation on the financial risk tolerance of both units and their neighboring entities. This analytical separation enhances policy-making insights. In contrast, <ref type="bibr" target="#b39">(Leung and Loupos, 2022)</ref> concentrates solely on the total effect. Furthermore, our theoretical contributions establish guarantees for a semi-parametric estimator using different proof techniques than <ref type="bibr" target="#b39">(Leung and Loupos, 2022)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Causal Inference and Networks</head><p>In this section, we introduce the required notations and the setup of our problem, including causal estimands and necessary assumptions for identification. As a convention in our paper, we represent random variables with capital letters (e.g., A ), scalars with lowercase letters (e.g., a ), matrices with script letters (e.g., A ), vectors with boldface symbols (e.g., A ), and sets with blackboard bold symbols (e.g., A ). Further, we also denote the shape of the matrix or vector as a subscript when and where necessary e.g. A m×p and A m×1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Formal Setup and Assumptions</head><p>Consider a social network G = (V n , A n , Z n ), where V n = {1 . . . n} denotes the set of n units, A n ∈ {0, 1} n×n is the adjacency matrix representing the connectivity structure of the network across the n units. If A n i,j = 1 for i, j ∈ V n , then units i and j are connected. The feature matrix Z n can be decomposed as Z n = (X n , T n , Y n ) , where X n ∈ R n×d is the matrix of pretreatment covariates, T n = {T 1 , T 2 , . . . , T n } is the vector of treatments for all units in V n , and Y n = {Y 1 , Y 2 , . . . , Y n } is the vector of outcomes for all units in the network. The potential outcome of unit i under treatment vector t n is denoted by Y i (t n ). We drop the superscript n indicating the sample size for parsimony in the rest of the paper.</p><p>Let N i = {j : A i,j = 1} be the set of nodes sharing ties with node i (i.e., the neighborhood of node i ). Having '-i' in the subscript denotes everything but i, hence N -i = V \ (N i ∪ {i}) is the set of non-neighbors of unit i. The vectors of treatments and outcomes for all nodes except node i are denoted as T -i and Y -i respectively, and the matrix of covariates for all nodes except for node i as X -i . Similarly, for the neighbors of i , we denote the vectors of their treatments and outcomes as T N i and Y N i , respectively, and the matrix of their covariates as X Ni . We assume that our network data is generated via the mechanism defined by the following structural equations:</p><formula xml:id="formula_0">T i = ϕ T (X i , X Ni ) + ϵ T i E[ϵ T | X ] = 0 Y i = θ 0 (X i )T i + α 0 (X i )ϕ Y T (T N i ) + ϕ Y X (X i , X Ni ) + ϵ Y i E[ϵ Y | X , T] = 0 where {ϵ} = {ϵ T i } i ∪ {ϵ Y i }</formula><p>i is a set of unobserved exogenous variables affecting random variables X i , T i and Y i , and ϕ's are a set of functional mappings that describe the causal dependence of the observed variables. ϕ Y X summarizes the covariates of the unit and its peers, i.e. W i = ϕ Y X (X i , X Ni ). Akin to the effective treatment function in <ref type="bibr" target="#b42">Manski (2013)</ref>, ϕ Y T is an exposure map that, for any unit i, summarizes network peers' treatments T Ni to an effective treatment exposure <ref type="bibr" target="#b0">Aronow and Samii (2013)</ref>, i.e., Z i = ϕ Y T (T N i ). In other words, an exposure map is supposed to capture the full nature of interference of a unit from all other units. Given Z i , the outcome Y i can be determined, rendering it independent of the treatments of the remaining network: Y i (T) = Y i (T i , Z i ). We operate under the assumption that the exposure mapping ϕ Y T is well-defined and known. This assumption is common across the literature (see e.g, <ref type="bibr" target="#b49">Ogburn and VanderWeele (2014)</ref>; <ref type="bibr" target="#b33">Jiang and Sun (2022)</ref>; <ref type="bibr" target="#b60">Toulis and Kao (2013)</ref>; <ref type="bibr" target="#b70">Zigler and Papadogeorgou (2021)</ref>; <ref type="bibr" target="#b50">Papadogeorgou and Samanta (2023)</ref>; <ref type="bibr" target="#b15">Forastiere et al. (2021)</ref>).</p><p>Estimand: Our objective is to estimate two key causal estimands: the average direct effect (ADE), denoted τ ADE , and the average peer effect (APE), denoted τ APE . ADE aims to capture the direct impact of treatment on the outcomes within individual units, whereas APE assesses the effect of treatments on a unit through its connections within a network. To illustrate the practical implications of these concepts, consider a friendship network where the treatment is the recommendation of a product in an advertisement to users, and the outcome is the purchasing of the product. This scenario prompts two pertinent questions: How does showing an advertisement to a user influence their likelihood of purchase? And, how does showing an advertisement to a user affect their friends' likelihood of purchase, considering potential discussions about the product? These questions correspond to the ADE and APE, respectively, which are well-established causal estimands in the literature <ref type="bibr" target="#b27">Hu et al. (2022)</ref>; <ref type="bibr" target="#b33">Jiang and Sun (2022)</ref>; <ref type="bibr" target="#b24">Halloran and Struchiner (1995)</ref>; <ref type="bibr" target="#b9">Blattman et al. (2021)</ref>; <ref type="bibr">Hudgens and Halloran (2008a)</ref>; <ref type="bibr" target="#b57">Sobel (2006)</ref>. The estimands are formally defined as follows:</p><formula xml:id="formula_1">τADE = E (X ,T,Y)|G 1 n i∈V τi,DE , where if t ∈ R : τi,DE = ∂Yi(t, T-i) ∂t , if t ∈ {0, 1} : τi,DE = Yi(1, T-i) -Yi(0, T-i).</formula><p>(1)</p><formula xml:id="formula_2">τAPE = E (X ,T,Y)|G 1 n i∈V τi,PE , where if z ∈ R : τi,PE = ∂Yi(Ti, z) ∂z , if z ∈ {0, 1} : τi,PE = Yi(Ti, 1-i) -Yi(Ti, 0-i).</formula><p>(2)</p><p>Here, τ i,DE and τ i,PE respectively denote the direct and peer effects on individual unit i. In the context of the structural equations presented earlier, these correspond to the parameters θ 0 (X i ) and α 0 (X i ). These effects are functions of the pre-treatment variables X i . Figure <ref type="figure" target="#fig_0">1</ref> illustrates a three-node causal graph, demonstrating network dynamics, causal interactions, and the alignment of τ i,DE and τ i,PE within the network structure.</p><p>Assumptions: We introduce the assumptions required for the identification of ADE and APE, which are standard in the causal inference literature from social networks <ref type="bibr" target="#b7">(Bhattacharya et al., 2019;</ref><ref type="bibr">Guo et al., 2020b;</ref><ref type="bibr" target="#b33">Jiang and Sun, 2022;</ref><ref type="bibr" target="#b48">Ogburn et al., 2022)</ref>. The network structure, defined by the adjacency matrix A, is considered fixed and not treated as a random variable. It serves as an information pathway, where connected units can influence each other's treatments and outcomes.</p><p>A.1 Exogeneity: Unobserved exogenous variables are assumed to be independent. Formally, for any i, j ∈ V, we assume:</p><formula xml:id="formula_3">ϵ X i ⊥ ϵ X j , ϵ T i ⊥ ϵ T j | X i , X j , ϵ Y i ⊥ ϵ Y j | X i , X j , T i , T j</formula><p>(3) A.2 Partial Interference: Each unit's potential outcome is influenced only by its own and its k-hop away neighbors' treatments. In this paper, we consider k = 1:</p><formula xml:id="formula_4">Y i (T i = t, T N i , T N -i ) = Y i (T i = t, T N i , T ′ N -i ) (4) A.</formula><p>3 Known Exposure Map: The exposure map ϕ Y T is well-defined and known a priori such that Z i = ϕ Y T (T N i ) A.4 Positivity: For all values of W i present in the population of interest, i.e. f (W i ) &gt; 0, all possible values of treatments and exposures have non-zero probabilities: </p><formula xml:id="formula_5">v1 v2 v3 X1 T1 Y1 X2 T2 Y2 ϵ T 2 ϵ Y 2 ϵ X 2 X3 T3 Y3</formula><formula xml:id="formula_6">∀(i, t, z), 0 &lt; f (T i = t, Z i = z | W i ) (5)</formula><p>where f is the probability density function. A.5 Consistency: The observed outcomes match potential outcomes under the observed treatment assignments:</p><formula xml:id="formula_7">Y i (T i = t, Z i = z) = Y i if T i = t, Z i = z.</formula><p>(6) A.6 Strong Ignorability: Conditional on the features X i and X Ni , the potential outcome is independent of treatment and peer exposure:</p><formula xml:id="formula_8">Y i (T i = t i , Z i = z) ⊥ T i , Z i | X i , X Ni<label>(7)</label></formula><p>Proposition 3.1. Under the assumptions of A.1-6, the average direct effect (ADE) and the average peer effect (APE) are identifiable.</p><p>The proof can be found in the appendix 8.</p><p>If the causal effects are constant across units i.e. θ 0 (X i ) = θ 0 and α 0 (X i ) = α 0 for all i ∈ V, then the average direct effect (ADE) is implied by θ 0 , i.e., τ ADE = θ 0 , and the average peer effect (APE) is implied by α 0 , i.e., τ APE = α 0 .</p><p>In this paper, we do not assume heterogeneity in the causal effect; hence, our focus is on estimating θ 0 and α 0 from network data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Method</head><p>In this section, we discuss our estimation strategy for ADE and APE. We operationalize double machine learning machinery with Graph Neural Networks (GNNs) to efficiently estimate ADE and APE by adjusting for complex network confounders. In this section, we illustrate our approach for ϕ Y T (T</p><formula xml:id="formula_9">N i ) = j∈N i T j . Recall that, Y = θ 0 T + α 0 AT + ϕ Y X (X i , X Ni ) + ϵ Y .</formula><p>(8) Taking expectations with respect to X on both sides, noting that A is constant, and subtracting it from Equation 8 yields:</p><formula xml:id="formula_10">Y -ℓ 0 (X , A) = (θ 0 + α 0 A) • (T -m 0 (X , A)) + ϵ Y (9) where m 0 (X , A) := E [T | X , A] and ℓ 0 (X , A) := E [Y | X , A].</formula><p>Let ζ = (θ, α) and η = (m, ℓ) to be the unknown target and nuisance parameters with ζ 0 := (θ 0 , α 0 ) and η 0 := (m 0 , ℓ 0 ) as the true values of these parameters of our interest that satisfies equation 9. Now, let W := (X , T, Y) be a random element taking values in a measurable space (W, A W ) with law determined by a probability measure P ∈ P N with (W i ) n i=1 random samples available for estimation and inference. Then, consider a squared loss derived L (W, A; ζ, η) :=</p><formula xml:id="formula_11">B ⊺ 1×n Bn×1 2 where B n×1 := [Y -ℓ (X , A) -(θ + αA) (T -m (X , A))</formula><p>] such that the partial derivatives of the loss function with respect to target parameters and nuisance parameters, evaluated at ζ 0 and η 0 yields zero:</p><formula xml:id="formula_12">E P ∂ ζ L (W, A; ζ, η)| ζ0,η0 = 0, E P ∂ η L (W, A; ζ, η)| ζ0,η0 = 0</formula><p>Thus, the target parameters can be identified by minimizing the following squared loss:</p><formula xml:id="formula_13">ζ 0 , η 0 ∈ arg min ζ,η E P [L (W, A; ζ, η)] ,</formula><p>Now, we construct an efficient score function, ψ that enables doubly robust estimation of target parameters, similar to <ref type="bibr" target="#b10">Chernozhukov et al. (2018)</ref> and <ref type="bibr" target="#b46">Morucci et al. (2023)</ref>:</p><formula xml:id="formula_14">ψ (W, A; ζ, η) = ∂ ζ L (W, A; ζ, η) -µ∂ η L (W, A; ζ, η) ,</formula><p>where µ is an orthogonalization parameter matrix such that its optimal value solves the equation: J ζη -µJ ηη = 0 where,</p><formula xml:id="formula_15">J ζζ J ζη J ηζ J ηη = ∂ (ζ ′ ,η ′ ) E P ∂ (ζ ′ ,η ′ ) ′ L (W ; ζ, η) ζ0;η0 .</formula><p>The detailed derivation of the score function is provided in the appendix 9 for further reference. The score function is identified as follows:</p><formula xml:id="formula_16">ψ (W, A; ζ, η) = (Y -ℓ (X , A) -(θ + αA) (T -m (X , A))) ⊺ (T -m (X , A)) (Y -ℓ (X , A) -(θ + αA) (T -m (X , A))) ⊺ A (T -m (X , A)) . (<label>10</label></formula><formula xml:id="formula_17">)</formula><p>We can now use the score function to construct an estimator for ζ such that ψ W, A; ζ, η = 0 where η = ℓ, m are the estimates of nuisance parameters. Thus,</p><formula xml:id="formula_18">Y -ℓ (X , A) ⊺ (T -m (X , A)) = θ [(T -m (X , A)) ⊺ (T -m (X , A))] + α [(T -m (X , A)) ⊺ A ⊺ (T -m (X , A))] and Y -ℓ (X , A) ⊺ A (T -m (X , A)) = θ [(T -m (X , A)) ⊺ A (T -m (X , A))] + α [(T -m (X , A)) ⊺ A ⊺ A (T -m (X , A))] .</formula><p>For accurate and consistent estimation of nuisance parameters η 0 , we leverage graph machine learning approaches, specifically using GNNs. Since nuisance parameters m and ℓ depend on both individual unit covariates and their social neighbors' covariates, GNNs are essential for aggregating neighborhood information. GNNs efficiently handle the complex dependencies in network data, as shown in <ref type="bibr" target="#b67">Xu et al. (2018)</ref>; <ref type="bibr" target="#b36">Kipf and Welling (2016)</ref>; <ref type="bibr" target="#b66">Veličković et al. (2017)</ref>; <ref type="bibr" target="#b25">Hamilton et al. (2017)</ref>. In our experiments, we use the Graph Isomorphism Network (GIN) <ref type="bibr" target="#b67">Xu et al. (2018)</ref> due to its superior performance over other GNN architectures like <ref type="bibr">GCN Kipf and Welling (2016)</ref>, <ref type="bibr">GAT Veličković et al. (2017), and</ref><ref type="bibr">GraphSAGE Hamilton et al. (2017)</ref>. For consistent estimation of nuisance parameters and to address non-i.i.d. data, we use a focal set approach similar to <ref type="bibr" target="#b3">Athey et al. (2015)</ref>. Our algorithm first constructs a set of units that are independent of each other, referred to as the focal set, defined formally below, and then performs cross-fitting to train the graph machine learning model for modeling the nuisance parameters on the focal set. This independence across units aids in consistent estimation of uncertainty around the estimated target parameters by avoiding bias due to dependence between the units.</p><p>Below, we formally define the focal set:</p><formula xml:id="formula_19">Definition 4.1. Focal set S * ⊆ V is a maximal set of nodes, in which ∀u, v ∈ S * , N u ∩ N v = ∅.</formula><p>We denote the size of the focal set S * as n f , i.e.</p><formula xml:id="formula_20">|S * | = n f</formula><p>According to the partial interference assumption, since neighborhoods of nodes in the focal set do not overlap, (X , T, Y) of these nodes will not be correlated, ensuring that the samples are independent of each other.</p><p>Further, as discussed by <ref type="bibr" target="#b10">(Chernozhukov et al., 2018;</ref><ref type="bibr" target="#b71">Zivich and Breskin, 2021;</ref><ref type="bibr" target="#b51">Parikh et al., 2022)</ref>, for the error term of the estimator to vanish, to overcome overfitting, and to gain full efficiency, we cross-fit our estimator. Consider a K-fold random partition (I k ) K k=1 of our data {1, ..., n f }, such that each fold I k will be of size</p><formula xml:id="formula_21">n f K . Let I -k = {1, ..., n f }\I k .</formula><p>For each k, let I -k be the train split and I k be the estimation split. We construct a ML estimator ηk of the nuisance function η 0 using the train split:</p><formula xml:id="formula_22">ηk = η (W i ) i∈I -k .<label>(11)</label></formula><p>Then, for each k ∈ {1 . . . K}, we plugin the estimated nuisance parameters ηk to estimate ζk as the solution to</p><formula xml:id="formula_23">K n f i∈I k ψ (W i , A; ζ, ηk ) = 0</formula><p>Our final estimation would be an aggregation of the estimators:</p><formula xml:id="formula_24">ζ = 1 K K k=1 ζk .</formula><p>Note that the choice of K may have a significant impact in small sample sizes. Intuitively, selecting larger values of K yields more observations in I -k , which can be advantageous for estimating high-dimensional nuisance functions, which seems to be the more difficult part of the problem. Empirical evidence and simulations indicate that moderate values of K, such as 4 or 5, yield more reliable estimations than using K = 2. This underscores the importance of carefully selecting K based on the sample size and the complexity of the functions being estimated.</p><p>Putting Everything Together. To estimate ADE and APE, our method begins by constructing a 'focal set' of units using a greedy approach to create an independent set of nodes. This focal set is the core of our analysis. Using graph machine learning, we train models on this focal set to accurately model the nuisance functions. To enhance accuracy and robustness, we employ cross-fitting by partitioning the data into multiple folds. In each fold, we perform linear regression to estimate the parameters θ 0 and α 0 , representing the direct and peer effects, respectively. These estimations are then aggregated across all folds to construct our comprehensive model of network dynamics. This integrative approach, combining the precision of graph machine leaning with the robustness of cross-fitting and the targeted analysis of the focal set, enables a nuanced and precise understanding of causal relationships in social networks. Figure <ref type="figure" target="#fig_1">2</ref> illustrates the proposed framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Theory</head><p>Now, we establish the theoretical results on the consistency and asymptotic normality of the proposed estimator. Detailed proofs are provided in Appendix 10. We consider a nested sequence of networks with an increasing number of units, {V n , A n , Z n } ∞ n=1 , such that key features of the network topology, e.g. degree distribution and clustering, are preserved. We assume that the maximum degree of the units in A n is d n ≤ √ n -1. This growth rate of the maximum degree of the network is a common trait in many real-world social networks where most units possess a low degree, and a smaller proportion of units have a high degree, with the maximum degree dependent on the size of the network <ref type="bibr" target="#b47">(Newman and Park, 2003)</ref>. This characteristic ensures broad applicability of our model to real-world social networks.</p><p>To prove the theoretical results, we need some regularity conditions adopted from the DML framework <ref type="bibr" target="#b10">(Chernozhukov et al., 2018)</ref> and adapted to social networks. These conditions capture sufficient assumptions about the behavior and properties of the data and models to ensure stable and valid inference. Intuitively, these conditions ensure enough variability in the treatment and outcome models, prevent the alignment of error terms that could distort causal effect estimation, and ensure accurate and reliable estimators for nuisance parameters. Additionally, they guarantee that the nuisance parameter estimators converge to their true values as the sample size increases, which is crucial for the consistency and asymptotic normality of the causal estimators, allowing for valid statistical inference. Now, we discuss the theoretical results supporting the consistency and asymptotic normality of the proposed estimator. To maintain a clear and focused narrative in the main text, we have relegated all the detailed proofs to the appendix 10. We consider a nested sequence of networks with an increasing number of units, {V n , A n , Z n } ∞ n=1 , such that key features of the network topology, e.g. degree distribution and clustering, are preserved. We assume that the maximum degree of the units in</p><formula xml:id="formula_25">A n is d n ≤ √ n -1.</formula><p>This growth rate of the maximum degree of the network is a common trait in many real-world social networks where most units possess a low degree, and a smaller proportion of units have a high degree, with the maximum degree dependent on the size of the network <ref type="bibr" target="#b47">(Newman and Park, 2003)</ref>. This characteristic ensures that our model remains applicable to a wide range of real-world social networks. Now, we assume the following regularity conditions: Assumption 5.1. (Regularity Conditions) Let c &gt; 0, C &gt; 0, c 1 ⩾ c 0 &gt; 0, q &gt; 4 and K ⩾ 2 be some finite constants, and let {δ n } ∞ n=1 and {∆ n } ∞ n=1 be some sequences of positive constants converging to zero such that</p><formula xml:id="formula_26">δ n ⩾ n -1/2 f</formula><p>. For all probability laws P ∈ P for the triple W = (T, Y, X ) the following conditions hold:</p><formula xml:id="formula_27">1. c ⩽ ∥ϵ T ∥ P,2 ; ∥ϵ T ∥ P,q ⩽ C; ⩽ ∥ϵ Y ∥ P,q ⩽ C 2. c ⩽ ∥ϵ Y ⊺ ϵ T ∥ P,2 ; c ⩽ E P ϵ T ⊺ ϵ T ; c ⩽ E P ϵ T ⊺ A ⊺ ϵ T 3. ∥Y∥ P,q ⩽ C</formula><p>4. ϵ T and ϵ Y are not eigen vectors of A.</p><p>5. Given a random subset I of {1 . . . , n f } of size n ′ = n f /K, the nuisance parameter estimator η = η (W i ) i∈I -k belongs to the realization set Γ n with probability at least 1 -∆ n , where η 0 ∈ Γ n .</p><p>6. With P -probability no less than 1 -∆ N , ∥ η -η 0 ∥ P,q ⩽ C, ∥ η -η 0 ∥ P,2 ⩽ δ n , and for the score ψ, where η 0 = m 0 , ℓ 0 ,</p><formula xml:id="formula_28">∥ m -m0∥ P,2 ∥ m -m0∥ P,2 + ℓ -ℓ0 P,2 ⩽ δn n 1/2 f .</formula><p>Theorem 5.2. Under regularity conditions 5.1<ref type="foot" target="#foot_0">foot_0</ref> , the estimator ζ0 concentrates in a σ/ √ n f -neighborhood of ζ 0 and the sampling error</p><formula xml:id="formula_29">√ n f ζ0 -ζ 0 is asymptotically normal √ n f ζ0 -ζ 0 ⇝ N 0 2×1 , σ 2 2×2</formula><p>with mean zero and variance given by</p><formula xml:id="formula_30">σ 2 := (J 0 ) -1 E [ψ (W ; ζ 0 , η 0 ) ψ (W ; ζ 0 , η 0 ) ⊺ ] ((J 0 ) -1 ) ⊺ where J 0 = E (ψ a (W ; η 0 )), if</formula><p>the score function is linear in the parameters ζ. For these score functions, estimates of the variance, σ2 , are obtained by</p><formula xml:id="formula_31">( Ĵ0) -1 1 n f K k=1 i∈I k ψ Wi; ζ, ηk ψ Wi; ζ, ηk ⊺ (( Ĵ0) -1 ) ⊺ , where Ĵ0 = 1 n f K k=1 i∈I k ψa (Wi; ηk ) ψa = -(T -m(X , A)) ⊺ (T -m(X , A)) -(T -m(X , A)) ⊺ A ⊺ (T -m(X , A)) -(T -m(X , A)) ⊺ A(T -m(X , A)) -(T -m(X , A)) ⊺ A ⊺ A(T -m(X , A))</formula><p>The confidence interval is given by ζ0</p><formula xml:id="formula_32">± σ/ √ n f Z -1 (1 -α/2)</formula><p>The result of Theorem 5.2 guarantees that our estimator is consistent, asymptotically normal, and statistically efficient in the size of the focal set such that the standard deviation shrinks at the rate of √ n f . The proof of the theorem is in Appendix 10</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Empirical Analysis and Results</head><p>This section details the empirical evaluation of our framework via semi-synthetic and real-data case studies. These experiments aim to examine our framework's effectiveness and compare its performance with state-of-the-art baseline methods.</p><p>In Appendix 12, we present additional empirical experiments to investigate the performance of our approach under varying levels of graph density and corresponding effective sample sizes. We evaluate the coverage probability of estimated 95% confidence intervals and explore the performance of an alternative graph aggregation tool combined with our framework, demonstrating the framework's generality beyond GNN models.</p><p>Setup: We use real-world networks from the Cora <ref type="bibr" target="#b43">(McCallum et al., 2000)</ref>, Pubmed <ref type="bibr" target="#b55">(Sen et al., 2008)</ref>, and Flickr <ref type="bibr">(Guo et al., 2020b</ref>) datasets. To access potential outcomes, we use the networks from these datasets and generate semi-synthetic data with synthetic covariates, treatments and outcomes, ensuring ground truth availability. Details of these datasets and the data generation processes are provided in Appendix 11.1 and 11.2. While our method can be used with any graph learning algorithm, we employ the GIN <ref type="bibr" target="#b13">(Douglas, 2011)</ref> for learning the propensity score and outcome models, which outperforms other methods we tested. We use a single layer of GINConv, as per the partial interference assumption, followed by two fully connected layers and an additional softmax layer for estimating the propensity score. Nonlinearity is introduced using ReLU, and dropout with a probability of p = 0.5 is used for regularization. The GIN models are trained over 300 epochs with a batch size of 16, using the Adam optimizer with a learning rate of 0.01. We consider K = 3 folds for cross-fitting. The codebase will be publicly available for further exploration and reference.</p><p>Experiments on Cora and Pubmed are executed on MacBook Pro 18 with Apple M1 Pro chip, 10 CPUs (8 performance and 2 efficiency) and 16 GB RAM. The experiments on Flickr are executed on the server with GPU model NVIDIA RTX-3090.</p><p>Baselines: We compare our method against six primary baselines. NetEst <ref type="bibr" target="#b33">(Jiang and Sun, 2022)</ref> uses GNNs for learning confounder representations with an adversarial learning process. Net-TMLE <ref type="bibr" target="#b48">(Ogburn et al., 2022)</ref> derives a doubly robust estimator using an efficient influence function and moment condition. The T-Learner <ref type="bibr" target="#b37">(Künzel et al., 2019)</ref> creates two estimators for each treatment arm using GNNs for modeling. DML with predefined aggregates applies DML in the i.i.d. setting using aggregates like max, min, and mean for neighbor information. <ref type="bibr">Ma &amp; Tresp method (Ma and Tresp, 2020)</ref> maps covariate representations to a new space using HSIC as a regularization term, with GNNs aggregating neighbor covariate information. Lastly, the L&amp;L method (Leung and Loupos, 2022) utilizes a doubly robust estimator combined with GNNs, requiring binary conversion of exposure data (this is a working paper with no public available code, hence we implemented it ourselves; details are in the Appendix 11.3). Similarly, <ref type="bibr">(Guo et al., 2020b)</ref> captures hidden confounder influence but reduces to the T-learner under our assumptions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results:</head><p>We compare two versions of our method: GDML w/o FS, which does not use a focal set and encompasses the entire dataset, and GDML, which operationalize our method using focal set to evaluate the effect of this strategy on the quality of the results. Table <ref type="table" target="#tab_0">1</ref> demonstrates the results, and Table <ref type="table" target="#tab_1">2</ref> reports the running time. We report the mean squared error (MSE) calculated over 100 simulations for each evaluated method (relative errors are reported in Figure <ref type="figure">3</ref> in the appendix).</p><p>Across all three semi-synthetic datasets, our GDML approach performs on par with or better than the state-of-the-art baseline methods (see Table <ref type="table" target="#tab_0">1</ref>) and scales significantly better (see Table <ref type="table" target="#tab_1">2</ref>). This performance enhancement can be attributed to the use of graph ML method (such as GIN) to adjust for network confounders. Further, the DML framework guarantees consistency, and efficiency while using complex ML methods with regularization. While Tresp &amp; Ma and L&amp;L occasionally have MSE marginally smaller, our approach consistently delivers robust results across various scenarios while being scalable and running in a reasonable time. Comparing GDML with the GIN-based T-learner shows that the GIN-based T-learner has higher MSE due to regularization-induced bias. Methods that employ predefined aggregation functions, such as Net-TMLE and DML methods using predefined aggregates (min, max, sum, and average), fall short as these aggregates do not capture complex network functions as effectively as GNNs. Tresp &amp; Ma, while slightly better in certain cases, fails to scale well, taking more than two and a half hours on PubMed and failing to terminate on Flickr within a 12-hour time limit. Additionally, Tresp &amp; Ma does not provide confidence intervals, which prevents us from performing uncertainty quantification-a key aspect when applying real-world datasets without access to the true data generative process. Among all these eight methods, only our GDML approach, Net-TMLE, and L&amp;L also yield consistent confidence intervals necessary for statistical inference.</p><p>A pivotal aspect of our methodology is the emphasis on focal sets analysis, exploiting the independence between units to enhance performance. While restricting to local sets reduces the size of training data for nuisance parameter estimation, this focus has demonstrably outperformed variants of our method that omit focal sets, highlighting the strategic value of considering focal sets in the analysis of networked data. This ensures robust and accurate estimations, validating our approach in dealing with networked data.</p><p>Analysis of Real Data For a case study, we used the Indian Village dataset from Karnataka, India, encompassing 16,995 individuals across 77 villages, with 15 features and 12 social networks <ref type="bibr" target="#b6">(Banerjee et al., 2014;</ref><ref type="bibr" target="#b32">Jackson et al., 2012)</ref>. This dataset's rich social structure provides insights into economic and social behaviors, such as borrowing, lending, and advice networks, making it invaluable for understanding the impacts of social networks on individual and collective outcomes. We operationalize our approach to estimating the causal effect of participation in self-help groups (SHGs) on financial risk tolerance, measured by the existence of an outstanding credit/loan. Specifically, we are interested in estimating the direct and peer effects of SHG participation. We construct the focal set that consists of 1766 individuals. Our analysis indicates that the point estimate for the average direct effect (ADE) is 0.315 with a 95% confidence interval ranging from -1.570 to 2.200. The positive point estimate suggests a potential positive effect of SHG participation. However, given the limited effective sample size in the social network setting (which is equal to the size of the focal set in our case), these estimates are not statistically significant, which is expected as the potential effect size in such social interventions is typically small. Additionally, our results show that the APE is approximately zero (0.050), indicating minimal to no benefit from peers' participation in SHGs. We provide point estimates from the baselines in Appendix12.2 -our estimates agree with these point estimates as well as with the literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions and Broader Impacts</head><p>Our work proposes a double-machine learning framework integrated with graph representation learning techniques to adjust for complex network confounders and efficiently estimate treatment effects. Evaluations through thorough simulation studies and real data case studies highlight its effectiveness. However, our framework has limitations, such as reliance on assumptions about GNN convergence rates, which are still an emerging area of research. Additionally, it requires the observation of all confounders, albeit accommodating high-dimensional covariates with complex mappings for propensity scores and outcome models, and necessitates domain-specific knowledge about exposure mappings. The efficacy of our framework diminishes with densely connected graphs due to the reduction in effective sample size. Future directions include adapting the framework for relational data scenarios with heterogeneous graphs, exploring higher-order graphs to enhance expressiveness beyond the limitations of message-passing GNNs, and investigating the impact of missing network ties on estimation in partially observable graphs, aiming to broaden the scope and applicability of our framework.</p><p>Our research improves causal inference in social networks, aiding policy-making with more accurate evaluations of intervention effectiveness. This enhances public health, education, and economic policies. It also supports economic development by identifying effective social programs like self-help groups, which promote financial inclusion and risk management in underserved communities. Nonetheless, ethical, transparent, and correct application of these tools is crucial to avoid privacy issues and potential biases.</p><formula xml:id="formula_33">τ ADE = E 1 n i∈V τ i,DE (12) = E 1 n i∈V Y i (1, T -i ) -Y i (0, T -i ) = (13) = E 1 n i∈V Y i (1, T N i ) -Y i (0, T N i ) = (14) = E 1 n i∈V Y i (1, z i ) -Y i (0, z i ) = (15) = E X E 1 n i∈V Y i (1, z i ) -Y i (0, z i ) | X = (16) = E X E 1 n i∈V Y i (1, z i ) -Y i (0, z i ) | X i , X Ni = (17) = E X E 1 n i∈V Y i (1, z i ) | X i , X Ni , t i , z i - E 1 n i∈V Y i (0, z i ) | X i , X Ni , t i , z i (18) = E X E 1 n i∈V Y i | X i , X Ni , t i = 1, z i - E 1 n i∈V Y i | X i , X Ni , t i = 0, z i<label>(19)</label></formula><p>Equation 14 uses partial interference assumption, 15 uses the assumption that the exposure map is well-defined and known, 16 uses law of total expectation, 17 uses partial interference assumption, 18 uses strong ignorability and 19 uses consistency assumption.</p><formula xml:id="formula_34">τ AP E = E 1 n i∈V τ i,P E (20) = E 1 n i∈V Y i (T i , 1 -i ) -Y i (T i , 0 -i ) = (21) = E 1 n i∈V Y i (T i , T N i = 1) -Y i (T i , T N i = 0) = (22) = E 1 n i∈V Y i (T i , z ′ i ) -Y i (T i , z ′′ i ) = (23) = E X E 1 n i∈V Y i (T i , z ′ i ) -Y i (T i , z ′′ i ) | X = (24) = E X E 1 n i∈V Y i (T i , z ′ i ) -Y i (T i , z ′′ i ) | X i , X Ni = (25) = E X E 1 n i∈V Y i (T i , z ′ i ) | X i , X Ni , t i , z i - E 1 n i∈V Y i (T i , z ′′ i ) | X i , X Ni , t i , z i (26) = E X E 1 n i∈V Y i | X i , X Ni , t i , z i = z ′ i - E 1 n i∈V Y i | X i , X Ni , t i , z i = z ′′ i<label>(27)</label></formula><p>Equation 22 uses partial interference assumption, 23 uses the assumption that the exposure map is well-defined and known, 24 uses law of total expectation, 25 uses partial interference assumption, 26 uses strong ignorability and 27 uses consistency assumption.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Derivation of score function</head><p>In this section, we introduce the concept of the neyman orthogonal score function and proceed to derive the corresponding score function pertinent to our study. This derivation is structured around our specific set of structural equations and is guided by the methodology outlined in <ref type="bibr" target="#b10">(Chernozhukov et al., 2018)</ref>. Let ζ ∈ Z ⊂ R d ζ and β ∈ B ⊂ R d β be the target and nuisance parameters respectively. Suppose the true parameter values ζ 0 and β 0 that solves the following optimization problem max ζ∈Z,β∈B</p><formula xml:id="formula_35">E P [L(W ; ζ, β)]</formula><p>where W is a random element taking values in a measurable space (W, A W ) with law determined by a probability measure P ∈ P n and L(W ; ζ, β) is a known criterion function. ζ 0 and β 0 satisfy</p><formula xml:id="formula_36">E P [∂ ζ L (W ; ζ 0 , β 0 )] = 0, E P [∂ β L (W ; ζ 0 , β 0 )] = 0</formula><p>Definition. (neyman orthogonality) The score ψ = (ψ 1 , . . . , ψ d θ ) ′ obeys the orthogonality condition at (θ 0 , η 0 ) with respect to the nuisance realization set Γ n ⊂ T if E P [ψ (W ; θ 0 , η 0 )] = 0 holds and the pathwise derivative map D r [η -η 0 ] exists for all r ∈ [0, 1) and η ∈ Γ n and vanishes at r = 0; namely, ∂ η E P ψ (W ; θ 0 , η 0 ) [η -η 0 ] = 0, for all η ∈ Γ n . We remark here that the condition holds with Γ n = T when η is a finite-dimensional vector as long as ∂ η E P [ψ j (W ; θ 0 , η 0 )] = 0 for all j = 1, . . . , d θ , where ∂ η E P [ψ j (W ; θ 0 , η 0 )] denotes the vector of partial derivatives of the function η → E P [ψ j (W ; θ 0 , η)] for η = η 0 .</p><p>The neyman orthogonal score function is</p><formula xml:id="formula_37">ψ(W, A; ζ, η) = ∂ ζ L(W ; ζ, β) -µ∂ β L(W ; ζ, β)</formula><p>where ψ = (ψ 1 , ..., ψ d ζ ) ′ is a vector of known score functions,the nuisance parameter is </p><formula xml:id="formula_38">η = (β ′ , vec(µ) ′ ) ′ ∈ T = B × R d ζ d β ⊂ R p , p = d β + d ζ d β ,</formula><formula xml:id="formula_39">J = J ζζ J ζβ J βζ J ββ = ∂ (ζ ′ ,β ′ ) E P ∂ (ζ ′ ,β ′ ) ′ L(W ; ζ, β) ζ=ζ0;β=β0</formula><p>The true value of the nuisance parameter η is</p><formula xml:id="formula_40">η 0 = β ′ 0 , vec (µ 0 ) ′ ′</formula><p>and when J ββ is invertible, it has the unique solution,</p><formula xml:id="formula_41">µ 0 = J ζβ J -1 ββ</formula><p>If J ββ is not invertible, the equation typically has multiple solutions. In this case, it is convenient to focus on a minimal norm solution, µ 0 = arg min ∥µ∥ such that ∥J ζβ -µJ ββ ∥ q = 0 for a suitably chosen norm ∥ • ∥ q on the space of d ζ × d β matrices.</p><p>In our case, we consider the following criterion function, which is the negative of standard squared loss:</p><formula xml:id="formula_42">L(W; ζ, β) 1×1 = - B ⊺ 1×n B n×1 2 ; B n×1 = [Y -ℓ(X , A) -θ(T -m(X , A)) -α(A(T -m(X , A)))]</formula><p>where ζ = (θ, α) are the target parameters and β = (m, ℓ) are nuisance parameters. m and ℓ are estimates of m 0 (X , A) and ℓ 0 (X , A) where m 0 (X , A)</p><formula xml:id="formula_43">= E P [T |X , A] and ℓ 0 (X , A) = E P [Y |X , A]</formula><p>. Thus, we want to solve the following maximization problem and find θ 0 and α 0 such that θ 0 , α 0 = arg max θ∈Θ,α∈△</p><formula xml:id="formula_44">E P [L(W; ζ, β) 1×1 ]</formula><p>We take the derivatives to build the score function</p><formula xml:id="formula_45">∂ θ L(W; ζ, β) 1×1 = B ⊺ 1×n (T -m(X , A)) n×1 ∂ α L(W; ζ, β) 1×1 = B ⊺ 1×n A(T -m(X , A)) n×1 ∂ m L(W; ζ, β) 1×n = -B ⊺ 1×n (θI n + αA) n×n ∂ ℓ L(W; ζ, β) 1×n = B ⊺ 1×n I n = B ⊺ 1×n</formula><p>I n is identity matrix with dimension n × n.</p><formula xml:id="formula_46">Let B 0n×1 = Y -ℓ 0 (X , A) -θ 0 (T -m 0 (X , A)) -α 0 (A(T -m 0 (X , A))) J ββ = ∂ β ′ E P [∂ β L(W; ζ, β)]| ζ=ζ0;β=β0 = -[(θ 0 I n + α 0 A) ⊺ (θ 0 I n + α 0 A)] n×n [(θ 0 I n + α 0 A) ⊺ ] n×n [(θ 0 I n + α 0 A)] n×n -[I n ] n×n 2n×2n ⇒ not invertible</formula><p>Since J ββ is not invertible, we need to find the minimal norm solution µ 0 = arg min ∥µ∥ such that ∥J ζβ -µJ ββ ∥ q = 0 Here µ 0 and J ζβ are 2 × 2n matrices and J ββ is a 2n × 2n matrix.</p><formula xml:id="formula_47">J ζβ = ∂ ζ ′ EP [∂ β L(W; ζ, β)]| ζ=ζ 0 ;β=β 0 = -EP [B0 ⊺ + (m0(X , A) -T) ⊺ (θ0In + α0A)]1×n EP [(m0(X , A) -T) ⊺ ]1×n -EP [B ⊺ 0 A + (m0(X , A) -T) ⊺ A ⊺ (θ0In + α0A)]1×n EP [(m0(X , A) -T) ⊺ A ⊺ ]1×n 2×2n Since m 0 (X , A) = E P [T |X , A] and ℓ 0 (X , A) = E P [Y |X , A], E P [m 0 (X , A) -T ] = 0 and E P [ℓ 0 (X , A) -Y ] = 0.</formula><p>The expectation of multiplication of a fixed matrix in each of these vectors would also be zero because it would be a linear combination of elements with zero expectation. Thus, J ζβ = 0 and by inspection, due to the fact that ∥.∥ ≥ 0, µ = 0 would make this norm minimum.</p><p>Hence, the score function would be:</p><formula xml:id="formula_48">ψ(W, A; ζ, η) = (Y -ℓ(X , A) -θ(T -m(X , A)) -α(A(T -m(X , A))) ⊺ (T -m(X , A)) (Y -ℓ(X , A) -θ(T -m(X , A)) -α(A(T -m(X , A))) ⊺ A(T -m(X , A))</formula><p>10 Proof of Theorem 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">and Corresponding Conditions to Verify</head><p>In this section, we describe the essential regularity conditions and provide their respective proofs. These conditions form the foundational basis for proving Theorem 5.2. By demonstrating that the score function fulfills specific assumptions, we can effectively invoke Theorems 3.1 and 3.2, along with Corollary 3.1 from <ref type="bibr" target="#b10">Chernozhukov et al. (2018)</ref>. This application is crucial for establishing two key properties of our estimators: consistency and asymptotic normality. These 2 are the asymptotic properties of an estimator. Asymptotic refers to a mathematical property of a sequence of random variables or a statistical estimator as the sample size approaches infinity. More specifically, it refers to the behavior of the estimator as the sample size becomes larger and larger. An asymptotic result holds in the limit as the sample size grows infinitely large. We say that an estimate θ is consistent if θ → θ 0 in probability as n → ∞, where θ 0 is the 'true' unknown parameter of the distribution of the sample.</p><p>We say that θ is asymptotically normal if</p><formula xml:id="formula_49">√ n θ -θ 0 d - → N 0, σ 2 θ0</formula><p>where σ 2 θ0 is called the asymptotic variance of the estimate θ. Asymptotic normality says that the estimator not only converges to the unknown parameter, but it converges fast enough, at a rate 1/ √ n, where n is the sample size.</p><p>These properties are fundamental in reinforcing the statistical robustness and reliability of our estimators in both finite sample and asymptotic contexts. Besides, they allow us to perform uncertainty quantification and build confidence intervals.</p><p>The invocation of Theorems 3.1 and 3.2 along with Corollary 3.1 from <ref type="bibr" target="#b10">Chernozhukov et al. (2018)</ref> are sufficient for proving our Theorem 5.2.</p><p>In the following discussion, we delve into two distinct sets of conditions as outlined in <ref type="bibr" target="#b10">Chernozhukov et al. (2018)</ref> that are necessary to invoke these theorems. We use ∥.∥ P,q to denote the L q (P ) norm, i.e. ∥f ∥ P,q := ∥f (W )∥ P,q := ( | f (w) | q dP (w))</p><p>1 q . Assumption 10.1. (Regularity Conditions) Let c &gt; 0, C &gt; 0, c 1 ⩾ c 0 &gt; 0, q &gt; 4 and K ⩾ 2 be some finite constants; and let {δ n } ∞ n=1 and {∆ n } ∞ n=1 be some sequences of positive constants converging to zero such that</p><formula xml:id="formula_50">δ n ⩾ n -1/2 f</formula><p>. For all probability laws P ∈ P for the triple W = (T, Y, X ) the following conditions hold:</p><formula xml:id="formula_51">1. Equation set ?? holds 2. c ⩽ ∥ϵ T ∥ P,2 , ∥ϵ T ∥ P,q ⩽ C, ⩽ ∥ϵ Y ∥ P,q ⩽ C 3. c ⩽ ∥ϵ Y ⊺ ϵ T ∥ P,2 , c ⩽ E P ϵ T ⊺ ϵ T , c ⩽ E P ϵ T ⊺ A ⊺ ϵ T 4. ∥Y∥ P,q ⩽ C 5. ϵ T and ϵ Y are not eigen vectors of A. 6. Given a random subset I of [n f ] of size n ′ = n f /K, the nuisance parameter estimator η 0 = η 0 (W i ) i∈I c</formula><p>belongs to the realization set T n with probability at least 1 -∆ n , where η 0 ∈ T n .</p><p>7. Given a random subset I of [n f ] of size n ′ = n f /K, the nuisance parameter estimator η 0 = η 0 (W i ) i∈I c obeys the following conditions: With P -probability no less than 1 -∆ n ,</p><formula xml:id="formula_52">∥ η 0 -η 0 ∥ P,q ⩽ C, ∥ η 0 -η 0 ∥ P,2 ⩽ δ n , and</formula><p>for the score ψ, where η 0 = m 0 , ℓ 0 ,</p><formula xml:id="formula_53">∥ m 0 -m 0 ∥ P,2 × ∥ m 0 -m 0 ∥ P,2 + ℓ 0 -ℓ 0 P,2 ⩽ δ n n -1/2 f .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.1">Condition Set 1: Linear Scores with Approximate Neyman Orthogonality</head><p>For all n f ⩾ 3 and probability measures P ∈ P n that determines the underlying law of W :</p><p>1. Moment condition vanishes at the true parameter</p><formula xml:id="formula_54">ζ 0 : E P [ψ (W ; ζ 0 , η 0 )] = 0</formula><p>2. The score function is linear in the sense that:</p><formula xml:id="formula_55">ψ(W, A; ζ, η) = ψ a (W, A; ζ, η)θ + ψ b (W ; ζ, η)α + ψ c (W ; ζ, η) 3. The map η → E P [ψ (W ; ζ 0 , η 0 )] is twice continuously Gateaux-differentiable.</formula><p>4. The score ψ is Neyman orthogonal or, more generally, it is Neyman λ n near-orthogonal at (ζ 0 , η 0 ) with respect to the nuisance realization set Γ n ⊂ T for</p><formula xml:id="formula_56">λ n := sup η∈Γn ∥∂ η E P ψ (W ; ζ 0 , η 0 ) [η -η 0 ]∥ ⩽ δ n n -1/2 f</formula><p>5. The identification condition holds; namely, the singular values of the matrix</p><formula xml:id="formula_57">J 0,a := E P [ψ a (W ; η 0 )]</formula><p>are between c 0 and c 1 .</p><p>10.2 Condition Set 2: Score Regularity and Quality of nuisance Parameter Estimators For all n f ⩾ 3 and P ∈ P n , the following conditions hold:</p><p>1. Given a random subset I of [n f ] of size n ′ = n f /K, the nuisance parameter estimator η 0 = η 0 (W i ) i∈I c belongs to the realization set Γ n with probability at least 1 -∆ n , where Γ n contains η 0 and is constrained by the next conditions. 2. The moment conditions hold:</p><formula xml:id="formula_58">m n := sup η∈Γn (E P [∥ψ (W ; ζ 0 , η)∥ q ]) 1/q ⩽ c 1 , m ′ n := sup η∈Γn (E P [∥ψ a (W ; η)∥ q ]) 1/q ⩽ c 1 .</formula><p>3. The following conditions on the statistical rates r n , r ′ n , and λ ′ n hold:</p><formula xml:id="formula_59">r n := sup η∈Γn ∥E P [ψ a (W ; η)] -E P [ψ a (W ; η 0 )]∥ ⩽ δ n , r ′ n := sup η∈Γn E P ∥ψ (W ; ζ 0 , η) -ψ (W ; ζ 0 , η 0 )∥ 2 1/2 ⩽ δ n , λ ′ n := sup r∈(0,1),η∈Γn ∂ 2 r E P [ψ (W ; ζ 0 , η 0 + r (η -η 0 ))] ⩽ δ n / √ n f .</formula><p>4. The variance of the score ψ is non-degenerate: All eigenvalues of the matrix</p><formula xml:id="formula_60">E P ψ (W ; ζ 0 , η 0 ) ψ (W ; ζ 0 , η 0 ) ′ are bounded from below by c 0 .</formula><p>In the rest of this section, we attempt to prove the condition sets 10.1 and 10.2 under regularity assumptions 10.1. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.3">Proof of</head><formula xml:id="formula_61">E P [∂ ζ L (W ; ζ 0 , β 0 )] = 0, E P [∂ β L (W ; ζ 0 , β 0 )] = 0 The neyman orthogonal score function is ψ(W, A; ζ, η) = ∂ ζ L(W ; ζ, β) -µ∂ β L(W ; ζ, β)</formula><p>Thus, by definition of ζ 0 and η 0 , we have:</p><formula xml:id="formula_62">E P [ψ (W ; ζ 0 , η 0 )] = 0 C.1.2</formula><p>The score function is linear in the sense that:</p><formula xml:id="formula_63">ψ(W, A; ζ, η) = (Y -ℓ(X , A) -θ(T -m(X , A)) -α(A(T -m(X , A))) ⊺ (T -m(X , A)) (Y -ℓ(X , A) -θ(T -m(X , A)) -α(A(T -m(X , A))) ⊺ A(T -m(X , A)) = -(T -m(X , A)) ⊺ (T -m(X , A)) -(T -m(X , A)) ⊺ A ⊺ (T -m(X , A)) -(T -m(X , A)) ⊺ A(T -m(X , A)) -(T -m(X , A)) ⊺ A ⊺ A(T -m(X , A)) ψ a θ α + (Y -ℓ(X , A)) ⊺ (T -m(X , A)) (Y -ℓ(X , A)) ⊺ A(T -m(X , A)) ψ b C.1.3</formula><p>The score function can trivially be shown to be twice Gateaux differentiable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1.4</head><p>To show neyman orthogonality, we need to show that Gateaux derivative vanishes in addition to the moment condition. The Gateaux derivative in the direction η -η 0 = (m -m 0 , ℓ -ℓ 0 ) is:</p><formula xml:id="formula_64">∂ η E P ψ (W ; ζ 0 , η 0 ) [η -η 0 ] = lim r→0 E P (Y -(ℓ0 + r(ℓ -ℓ0)) -θ0(T -(m0 + r(m -m0))) -α0(A(T -(m0 + r(m -m0))))) ⊺ (T -(m0 + r(m -m0))) (Y -(ℓ0 + r(ℓ -ℓ0)) -θ0(T -(m0 + r(m -m0))) -α0(A(T -(m0 + r(m -m0))))) ⊺ A(T -(m0 + r(m -m0))) -E P (Y -ℓ0 -θ0(T -m0) -α0(A(T -m0)) ⊺ (T -m0) (Y -ℓ0 -θ0(T -m0) -α0(A(T -m0)) ⊺ A(T -m0) r = lim r→0 E P ( ϵ Y Y -ℓ0 -θ0(T -m0) -α0(A(T -m0 -r(m -m0))) - G r(ℓ -ℓ0) + θ 0 D θ0r(m -m0) + α 0 AD α0Ar(m -m0)) ⊺ ( ϵ T T -m0 - D r(m -m0)) (Y -ℓ0 -θ0(T -m0) -α0(A(T -m0 -r(m -m0))) -r(ℓ -ℓ0) + θ0r(m -m0) + α0Ar(m -m0)) ⊺ A(T -m0 -r(m -m0)) -E P ( ϵ Y ⊺ Y -ℓ0 -θ0(T -m0) -α0(A(T -m0)) ⊺ ϵ T (T -m0) (Y -ℓ0 -θ0(T -m0) -α0(A(T -m0)) ⊺ A(T -m0) r = lim r→0 E P    $ $ $ ϵ Y ⊺ ϵ T -ϵ Y ⊺ D -G ⊺ ϵ T + θ0D ⊺ ϵ T goes to 0 includes r 2 +GD -θ0D ⊺ D -α0D ⊺ A ⊺ D +α0D ⊺ A ⊺ ϵ T -$ $ $ ϵ Y ⊺ ϵ T $ $ $ $ ϵ Y ⊺ Aϵ T -ϵ Y ⊺ AD -G ⊺ Aϵ T + θ0D ⊺ Aϵ T goes to 0 includes r 2 +GAD -θ0D ⊺ AD -α0D ⊺ A ⊺ AD +α0D ⊺ A ⊺ Aϵ T -$ $ $ $ ϵ Y ⊺ Aϵ T    r = lim r→0 £ rE P -ϵ Y ⊺ (m -m0) -(ℓ -ℓ0) ⊺ ϵ T + θ0(m -m0) ⊺ ϵ T + α0(m -m0) ⊺ A ⊺ ϵ T -ϵ Y ⊺ A(m -m0) -(ℓ -ℓ0) ⊺ Aϵ T + θ0(m -m0) ⊺ Aϵ T + α0(m -m0) ⊺ A ⊺ Aϵ T £ r = E P -ϵ Y ⊺ (m -m0) -(ℓ -ℓ0) ⊺ ϵ T + θ0(m -m0) ⊺ ϵ T + α0(m -m0) ⊺ A ⊺ ϵ T -ϵ Y ⊺ A(m -m0) -(ℓ -ℓ0) ⊺ Aϵ T + θ0(m -m0) ⊺ Aϵ T + α0(m -m0) ⊺ A ⊺ Aϵ T</formula><p>Consider the first term in the above expectation. We use Law of Iterated Expectations:</p><formula xml:id="formula_65">EP [ϵ Y ⊺ (m -m0)] = EX,D,Y [ϵ Y ⊺ (m -m0)] = EX [E Y T |X [ϵ Y ⊺ constant given X (m -m0) | X]] = EX [(m -m0)E Y T |X [ 0 (ϵ Y ⊺ ) | X]] = 0</formula><p>A similar argument can be used to show that other expectation terms are 0.</p><formula xml:id="formula_66">C.1.5 J 0,a := E P [ψ a (W ; η 0 )] = E P -(T -m 0 (X , A)) ⊺ (T -m 0 (X , A)) -(T -m 0 (X , A)) ⊺ A ⊺ (T -m 0 (X , A)) -(T -m 0 (X , A)) ⊺ A(T -m 0 (X , A)) -(T -m 0 (X , A)) ⊺ A ⊺ A(T -m 0 (X , A)) = A 1 A 2 A 3 A 4 J ⊺ 0,a J 0,a = A 1 A 3 A 2 A 4 A 1 A 2 A 3 A 4 = A 1 2 + A 3 2 A 1 A 2 + A 3 A 4 A 1 A 2 + A 3 A 4 A 2 2 + A 4</formula><p>The eigen values of this matrix are the roots of the following quadratic equation:</p><formula xml:id="formula_67">λ 2 -λ(A 1 2 + A 2 2 + A 3 2 + A 4 2 ) + (A 1 2 + A 3 2 )(A 2 2 + A 4 2 ) -(A 1 A 2 + A 3 A 4 ) 2 = 0</formula><p>We know that in a quadratic equation of form a 2 x 2 + a 1 x + a 0 = 0, the sum of the roots are -a1 a2 and the product of the roots are a0 a2 . To ensure that all the eigen values are positive, we need to make sure both -a1 a2 and a0 a2 are positive:</p><formula xml:id="formula_68">-a 1 a 2 = A 1 2 + A 2 2 + A 3 2 + A 4 2 ⩾ c &gt; 0 (28) a 0 a 2 = (A 1 2 + A 3 2 )(A 2 2 + A 4 2 ) -(A 1 A 2 + A 3 A 4 ) 2 = (A 1 A 4 -A 2 A 3 ) 2 (29)</formula><p>28 holds since the summation of squared elements are non-negative and</p><formula xml:id="formula_69">A 2 1 = E P [(T -m 0 (X , A)) ⊺ (T - m 0 (X , A))] 2 = ∥ϵ T ∥ ⩾ c</formula><p>by assumption 2. For 29 to hold, since the squared value is non-negative, we need to show it is not zero, i.e. A 1 A 4 ̸ = A 2 A 3 . By Cauchy-Schwarz inequality, we know that:</p><formula xml:id="formula_70">E P ϵ T ⊺ ϵ T E P ϵ T ⊺ A ⊺ Aϵ T ⩾ E P ϵ T ⊺ Aϵ T 2</formula><p>where the equality holds if ∥ϵ T ∥ = 0 or ∥Aϵ T ∥ = 0, which does not hold by assumption 2 and the fact that A is a non-zero matrix. Also the equality can happen if ∃r : Aϵ T = rϵ T , which does not hold by assumption 5. Thus, summation and product of the eigen values are positive, leading to positivity of the singular values of J 0,a .</p><p>Following proposition is derived from <ref type="bibr" target="#b17">(Gallier and Quaintance, 2023</ref>): Proposition 10.2. For every norm ∥∥ on C n ( or R n ), for every matrix</p><formula xml:id="formula_71">A ∈ M n (C) (or A ∈ M n (R) ), there is a real constant C A ⩾ 0, such that ∥Au∥ ≤ C A ∥u∥, for every vector u ∈ C n (or u ∈ R n if A is real).</formula><p>10.2 states that every linear map on a finite-dimensional space is bounded. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2.2</head><p>We prove the boundedness of norms of these matrices by showing the bound on the norm of the elements considering the fact that if norm of each element is bounded, then the norm of the matrix is bounded. We first show the bound for the first elements of ψ a and ψ.:</p><formula xml:id="formula_72">ψ a (W ; η) = -(T -m(X , A)) ⊺ (T -m(X , A)) -(T -m(X , A)) ⊺ A ⊺ (T -m(X , A)) -(T -m(X , A)) ⊺ A(T -m(X , A)) -(T -m(X , A)) ⊺ A ⊺ A(T -m(X , A)) ψ(W ; ζ 0 , η) = (Y -ℓ(X , A) -θ 0 (T -m(X , A)) -α 0 (A(T -m(X , A))) ⊺ (T -m(X , A)) (Y -ℓ(X , A) -θ 0 (T -m(X , A)) -α 0 (A(T -m(X , A))) ⊺ A(T -m(X , A)) E P ∥ψ a 11 (W ; η)∥ q/2 2/q = ∥ψ a 11 (W ; η)∥ P,q/2 = ∥-(T -m(X , A)) ⊺ (T -m(X , A)∥ P,q/2 = ∥-((T -m 0 (X , A)) -(m(X , A) -m 0 (X , A))) ⊺ ((T -m 0 (X , A)) -(m(X , A) -m 0 (X , A)))∥ P,q/2 = ((m(X , A) -m 0 (X , A)) -ϵ T ) ⊺ (ϵ T -(m(X , A) -m 0 (X , A))) P,q/2 = (m(X , A) -m 0 (X , A)) ⊺ ϵ T -(m(X , A) -m 0 (X , A)) ⊺ (m(X , A) -m 0 (X , A)) -ϵ T ⊺ ϵ T + ϵ T ⊺ (m(X , A) -m 0 (X , A)) P,q/2</formula><p>⩽ ∥m(X , A) -m 0 (X , A)∥ P,q ϵ T P,q + ∥m(X , A) -m 0 (X , A)∥ P,q ∥m(X , A) -m 0 (X , A)∥ P,q + ϵ T P,q ϵ T P,q + ϵ T P,q ∥m(X , A) -m 0 (X , A)∥ P,q ⩽ 4C 2 by assumptions 2 and 7. Following the exact same approach along with proposition 10.2, we can derive an upperbound for other elements of ∥ψ a (W ; η)∥ P,q/2 , which gives the bound on m ′ n in condition 2.</p><p>Next, we establish an upper-bound for the first element of E P ∥ψ(W ; ζ 0 , η)∥ q/2 2/q</p><p>. First, we need an upper-bound on θ 0 and α 0 , which will be used later.</p><formula xml:id="formula_73">E P [ψ(W ; ζ 0 , η 0 )] = E P [ψ a (W ; η 0 )] θ α + E P ψ b (W ; η 0 ) = 0 θ 0 = E P [(Y -ℓ 0 (X , A)) ⊺ (m 0 (X , A) -T)] E P [(T -m 0 (X , A)) ⊺ (m 0 (X , A) -T)] = E P (Y -ℓ 0 (X , A)) ⊺ ϵ T E P ϵ T ⊺ ϵ T α 0 = E P [(Y -ℓ 0 (X , A)) ⊺ (m 0 (X , A) -T)] E P [(T -m 0 (X , A)) ⊺ A ⊺ (m 0 (X , A) -T)] = E P (Y -ℓ 0 (X , A)) ⊺ ϵ T E P ϵ T ⊺ A ⊺ ϵ T | θ 0 |= | E P [(Y -ℓ 0 (X , A)) ⊺ ϵ T ] | | E P ϵ T ⊺ ϵ T | ⩽ c -1 C(∥Y ∥ P,q + ∥ℓ 0 (X , A)∥ P,q ) ⩽ 2c -1 C(∥Y ∥ P,q ) ⩽ 2C 2 /c | α 0 |= | E P [(Y -ℓ 0 (X , A)) ⊺ ϵ T ] | | E P ϵ T ⊺ A ⊺ ϵ T | ⩽ c -1 C(∥Y ∥ P,q + ∥ℓ 0 (X , A)∥ P,q ) ⩽ 2c -1 C(∥Y ∥ P,q ) ⩽ 2C 2 /c E P ∥ψ 11 (W ; ζ 0 , η)∥ q/2 2/q = ∥ψ 11 (W ; ζ 0 , η)∥ P,q/2 = ∥(Y -ℓ(X , A) -θ 0 (T -m(X , A)) -α 0 (A(T -m(X , A))) ⊺ (T -m(X , A))∥ P,q/2 = (Y -ℓ 0 (X , A) -(ℓ(X , A) -ℓ 0 (X , A)) -θ 0 ((T -m 0 (X , A)) -(m(X , A) -m 0 (X , A)))- α 0 (A((T -m 0 (X , A)) -(m(X , A) -m 0 (X , A))))) ⊺ ((T -m 0 (X , A)) -(m(X , A) -m 0 (X , A))) P,q/2 = (ϵ Y -(ℓ(X , A) -ℓ 0 (X , A)) + θ 0 (m(X , A) -m 0 (X , A))+ α 0 A(m(X , A) -m 0 (X , A))) ⊺ (ϵ T -(m(X , A) -m 0 (X , A))) ⩽ (2C + 2C 3 /c + 2C A C 3 /c)2C = 4C 2 + 4C 4 /c + 4C A C 4 /c</formula><p>where C A is the constant term introduced in proposition 10.2, which gives the bound on m n in condition 2. Following the exact same approach along with proposition 10.2, we can derive an upperbound for other elements of ∥ψ(W ; ζ 0 , η)∥ P,q/2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2.3</head><p>Following the same argument in the previous section, we prove the boundedness of elements of these matrices:</p><formula xml:id="formula_74">∥E P [ψ a 11 (W ; η)] -E P [ψ a 11 (W ; η 0 )]∥ = |E P [ψ a 11 (W ; η) -ψ a 11 (W ; η 0 )]| = |E P [-(T -m(X , A)) ⊺ (T -m(X , A)) + (T -m 0 (X , A)) ⊺ (T -m 0 (X , A))]| = E P -((T -m 0 (X , A)) -(m(X , A) -m 0 (X , A))) ⊺ ((T -m 0 (X , A)) -(m(X , A) -m 0 (X , A)))+ (T -m 0 (X , A)) ⊺ (T -m 0 (X , A)) = E P ((m(X , A) -m 0 (X , A)) -ϵ T ) ⊺ (ϵ T -(m(X , A) -m 0 (X , A)))+ ϵ T ⊺ ϵ T = E P (m(X , A) -m 0 (X , A)) ⊺ ϵ T + ϵ T ⊺ (m(X , A) -m 0 (X , A))- (m(X , A) -m 0 (X , A)) ⊺ (m(X , A) -m 0 (X , A)) = 2 ϵ T P,2 ∥m(X , A) -m 0 (X , A)∥ P,2 + ∥m(X , A) -m 0 (X , A)∥ 2 P,2 ⩽ 2Cδ n + δ 2 n ⩽ δ ′ n</formula><p>by assumption 7, which gives the bound on r n in condition 3. Further,</p><formula xml:id="formula_75">E P ∥ψ 11 (W ; ζ 0 , η) -ψ 11 (W ; ζ 0 , η 0 )∥ 2 1/2 = ∥ψ 11 (W ; θ 0 , η) -ψ 11 (W ; θ 0 , η 0 )∥ P,2 = (Y -ℓ(X , A) -θ 0 (T -m(X , A)) -α 0 (A(T -m(X , A))) ⊺ (T -m(X , A))) -((Y -ℓ 0 (X , A)- θ 0 (T -m 0 (X , A)) -α 0 (A(T -m 0 (X , A))) ⊺ (T -m 0 (X , A)))) P,2 = -ϵ Y ⊺ (m(X , A) -m 0 (X , A)) + (ℓ 0 (X , A) -ℓ(X , A) + θ 0 (m(X , A) -m 0 (X , A))+ α 0 A(m(X , A) -m 0 (X , A))) ⊺ ϵ T -(ℓ 0 (X , A) -ℓ(X , A) + θ 0 (m(X , A) -m 0 (X , A))+ α 0 A(m(X , A) -m 0 (X , A))) ⊺ (m(X , A) -m 0 (X , A)) P,2 ⩽ (C + 2C 3 /c + 2C A C 3 /c) m(X , A) -m 0 (X , A) + C ℓ(X , A) -ℓ 0 (X , A) + ( ℓ(X , A) -ℓ 0 (X , A) + (2C 2 /c + 2C A C 2 /c) m(X , A) -m 0 (X , A) ) m(X , A) -m 0 (X , A) ⩽ (1 + 2C 2 /c + 2C A C 2 /c)δ n n -1/2 f ⩽ (1 + 2C 2 /c + 2C A C 2 /c)δ n ⩽ δ ′ n</formula><p>by assumption 7. Following the same approach along with proposition 10.2, we can derive an upper bound for the other dimensions of ψ and ψ a . This upper bound provides the bound on r n in condition 3. Lastly, let f (r) := E P [ψ (W ; θ 0 , η 0 + r (η -η 0 )] , r ∈ (0, 1).</p><p>Then for any r ∈ (0, 1), for the first dimension of the score function:</p><formula xml:id="formula_76">f (r) = E P (Y -(ℓ + r(ℓ -ℓ 0 )) -θ 0 (T -(m + r(m -m 0 ))) -α 0 A(T -(m + r(m -m 0 )))) ⊺ (T -(m + r(m -m 0 ))) ∂f (r) = E P (ℓ 0 -ℓ + θ 0 (m -m 0 ) + α 0 A(m -m 0 )) ⊺ (T -m -r(m -m 0 ))+ (Y -ℓ -r(ℓ -ℓ 0 ) -θ 0 (T -m -r(m -m 0 )) -α 0 A(T -m -r(m -m 0 ))) ⊺ (m 0 -m) ∂ 2 f (r) = E P (ℓ 0 -ℓ + θ 0 (m -m 0 ) + α 0 A(m -m 0 )) ⊺ (m 0 -m) +(ℓ 0 -ℓ + θ 0 (m -m 0 ) + α 0 A(m -m 0 )) ⊺ (m 0 -m) = 2E P (ℓ 0 -ℓ + θ 0 (m -m 0 ) + α 0 A(m -m 0 )) ⊺ (m 0 -m) ⩽ 2(∥ℓ -ℓ 0 ∥ + 2C 2 /c∥m -m 0 ∥ + 2C A C 2 /c∥m -m 0 ∥)∥m -m 0 ∥ ⩽ 2(1 + 2C 2 /c + 2C A C 2 /c)δ n n -1/2 f ⩽ δ ′ n n -1/2 f which gives the bound on λ ′ n in condition 3. C.2.4 ψ(W ; ζ 0 , η 0 ) = (Y -ℓ 0 (X , A) -θ 0 (T -m 0 (X , A)) -α 0 (A(T -m 0 (X , A))) ⊺ (T -m 0 (X , A)) (Y -ℓ 0 (X , A) -θ 0 (T -m 0 (X , A)) -α 0 (A(T -m 0 (X , A))) ⊺ A(T -m 0 (X , A)) = ϵ Y ⊺ ϵ T ϵ Y ⊺ Aϵ T E P ψ (W ; ζ 0 , η 0 ) ψ (W ; θ 0 , η 0 ) ′ = E P ϵ Y ⊺ ϵ T ϵ Y ⊺ Aϵ T ϵ Y ⊺ ϵ T ϵ Y ⊺ Aϵ T = E P (ϵ Y ⊺ ϵ T ) 2 (ϵ Y ⊺ ϵ T )(ϵ Y ⊺ Aϵ T ) (ϵ Y ⊺ Aϵ T )(ϵ Y ⊺ ϵ T ) (ϵ Y ⊺ Aϵ T ) 2<label>(30)</label></formula><p>The eigen values of this matrix are the roots of the following quadratic equation:</p><formula xml:id="formula_77">λ 2 -λ(E P [(ϵ Y ⊺ ϵ T ) 2 + (ϵ Y ⊺ Aϵ T ) 2 ]) + E P [(ϵ Y ⊺ ϵ T ) 2 ] + E P [(ϵ Y ⊺ Aϵ T ) 2 ] -E P [(ϵ Y ⊺ ϵ T )(ϵ Y ⊺ Aϵ T )] 2 = 0 (31)</formula><p>We know that in a quadratic equation of form a 2 x 2 + a 1 x + a 0 = 0, the sum of the roots are -a1 a2 and the product of the roots are a0 a2 . To ensure that all the eigen values are positive, we need to make sure both -a1 a2 and a0 a2 are positive:</p><formula xml:id="formula_78">-a 1 a 2 = ∥ϵ Y ⊺ ϵ T ∥ P,2 + ∥ϵ Y ⊺ Aϵ T ∥ P,2 &gt; 0 (32) a 0 a 2 = E P [(ϵ Y ⊺ ϵ T ) 2 ] + E P [(ϵ Y ⊺ Aϵ T ) 2 ] -E P [(ϵ Y ⊺ ϵ T )(ϵ Y ⊺ Aϵ T )] 2 &gt; 0 (33)</formula><p>32 holds according to assumption 3. Equation 33 also holds according to Cauchy-Schwarz inequality. The equality in Cauchy-Schwarz inequality for two random variables X and Y happens when ∥X∥ = 0 or ∥Y ∥ = 0 or Y = rX for some r ̸ = 0. neither of these cases hold: ∥ϵ Y ⊺ ϵ T ∥ P,2 &gt; 0 based on 3. ∥ϵ Y ⊺ Aϵ T ∥ P,2 &gt; 0 based on 3 and the fact that A is the adjacency matrix with non-negative elements and A ̸ = 0. Also, ∄r ̸ = 0 : ϵ Y ⊺ Aϵ T = rϵ Y ⊺ ϵ T according to 5. Thus, the roots of equation 31, which are the eigen values of matrix 30 are bounded from below by some positive c 0 . Thus, all conditions 10.1 and 10.2 are verified. This completes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11">Complementary Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.1">Datasets Details</head><p>We use the following network datasets for our evaluations:</p><p>• Real World Data -IndianVillage <ref type="bibr" target="#b6">(Banerjee et al., 2014;</ref><ref type="bibr" target="#b32">Jackson et al., 2012)</ref> We treated all these connections uniformly, ensuring a consistent network where all edges carried the same meaning.</p><p>• Semi-Synthetic Data (X , T, Y) are generated based on data generative process ?? and the network comes from real-world network dataset below:</p><p>- • Synthetic Data (X , T, Y) are generated based on data generative process ?? and the network comes from the synthetic network generative process below:</p><p>-Stochastic Block Model (SBM) <ref type="bibr" target="#b26">(Holland et al., 1983)</ref>: It is a generative model for networks. We also tried our method on a synthetic network produced by SBM, to have more control over the network parameters. In SBM, nodes are partitioned into multiple blocks or communities, and the probability of an edge existing between two nodes depends on their respective block assignments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.2">Data Generative Process</head><p>The covariates (X ), treatment assignments (T), and outcomes (Y) are synthetically generated following a specific data generative process outlined in Section 3.1. This section details one such data-generative process used in our experiments:</p><p>X ∼ N (0, 1)</p><formula xml:id="formula_79">π = 1 + exp( X + γAX ) -10 -1 T ∼ Bin(π) Y = X + AX + T × θ 0 + αAT (<label>34</label></formula><formula xml:id="formula_80">)</formula><p>Where AT is the exposure map and would be the sum of treated neighbors for each node. In our setup, we assumed that this exposure map is known. In the experiments in which we compare our method against baselines, the target parameters are θ 0 = 10 and α 0 = 5. For the Pubmed and Flickr datasets, we adopt this data generative process to generate (X , T, Y).</p><p>We also simulated another data generative process, which is more complex and involves non-linearity:</p><formula xml:id="formula_81">X i ∼ N (0, 1) π = 1 + exp( X + γAX ) -10 -1 T i ∼ Bin(π i ) Y i = σ( j X ij + M AX j ((AX ) ij )) + T i × θ 0 + αA i T<label>(35)</label></formula><p>where σ denotes the sigmoid function. The target parameters are θ 0 = 20 and α 0 = 5. For the Cora dataset, we adopt this data generative process to generate (X , T, Y).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.3">Extended Baselines</head><p>This section provides a more thorough description of the baselines used.</p><p>1. NetEst (Jiang and Sun, 2022): Utilizes GNNs for learning representations of confounders for individual units and their neighbors, coupled with an adversarial learning process to align distributions for networked causal inference.</p><p>2. Net-TMLE <ref type="bibr" target="#b48">(Ogburn et al., 2022)</ref>: Employs an efficient influence function and moment condition to derive a doubly robust estimator. This approach leverages the efficiency of targeted maximum likelihood estimation (TMLE) to improve the robustness and accuracy of causal effect estimates in the presence of network interference. <ref type="bibr" target="#b37">(Künzel et al., 2019)</ref>: Creates two separate models to predict outcomes for each treatment arm based on unit and neighbor covariates, with estimations modeled using GNNs. This method provides a straightforward way to estimate treatment effects by splitting the problem into two learning tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">T-Learner</head><p>4. DML with predefined aggregates: Applies Double Machine Learning (DML) in the i.i.d. setting but uses predefined aggregates like max, min, and mean for neighbor information aggregation. This approach simplifies the network structure into summary statistics, facilitating the application of traditional DML techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.</head><p>Tresp &amp; Ma <ref type="bibr" target="#b41">(Ma and Tresp, 2020)</ref>: Maps the representation of covariates to a new space where treatment and covariates are disentangled, incorporating the Hilbert-Schmidt Independence Criterion (HSIC) as a regularization term. Subsequently, GNNs are employed to aggregate covariate information from neighboring nodes. Two separate models are then trained to estimate the outcome based on the output of the GNNs and a predefined exposure map for the treatment and control groups.</p><p>6. L&amp;L method <ref type="bibr" target="#b39">(Leung and Loupos, 2022)</ref>.In our paper, we designate this approach as the "L&amp;L" method in the experiment section. L&amp;L is a working paper and the code is not publicly available and according to the authors, will become available after the work is published. We implemented a version of their method to the best of our understanding. Unlike methods that utilize continuous exposure measures, the L&amp;L method requires the conversion of exposure data into a binary format for application to our dataset. Specifically, for each node, if more than half of its neighbors, including the node itself, are subject to treatment, we assign an exposure value of 1; if not, the exposure value is set to 0. Utilizes a standard doubly robust estimator combined with GNNs to estimate the total effect.</p><p>12 Complementary Experimental Results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="12.1">Coverage Study</head><p>In our investigation, we performed a comprehensive coverage analysis leveraging the closed-form formula for calculating variance and confidence intervals as detailed in Section 5.2. This analysis involved applying our proposed methodology across multiple executions-specifically, 100 iterations-on each dataset under consideration. For each iteration, we computed confidence intervals and assessed the frequency at which the true value of the target parameter fell within these intervals. This measure of frequency serves as a critical indicator of the reliability and precision of our methodology in capturing the parameter of interest across varied datasets. The results are presented in table 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Dataset θ α</head><p>Cora 100% 100% Pubmed 100% 100% Flickr 92% 52%  <ref type="bibr">-1.570, 2.200</ref>] and <ref type="bibr">[-1.017, 1.116</ref>] respectively with a 95% confidence. In the literature, <ref type="bibr" target="#b19">(Gilad et al., 2021)</ref> also quantified the direct effect of SHG membership on the probability of possessing an outstanding loan as 0.30.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="12.3">Graph density</head><p>To assess the impact of network data sparsity on estimation performance, we utilize Stochastic Block Model (SBM) synthetic graphs, providing greater control over graph generation.</p><p>We fix the number of components as 200, the number of nodes as 3000, and the probability of existence of an edge between components as 0.0001. Subsequently, we vary the probability of edge existence within the component, denoted as P intra , to modulate the sparsity of the graph. For each P intra , we generate a single graph and for each graph, we generate 100 different datasets X , T and Y and report the average of estimated direct effect. Table <ref type="table" target="#tab_5">5</ref> presents the results. notably, as P intra increases, the number of edges rises. Given the fixed number of nodes, this causes a reduction in the size of the focal set (sample size), resulting in an increased bias in the estimation process. This result showcases that our methodology exhibits enhanced performance in sparser networks. As the number of edges increases within a network with a fixed number of nodes, we observe a corresponding rise in both the relative error and the variance of our estimations. This trend suggests a direct relationship between network density and the performance of our method. Relative Error (%) 12.4 Generality of GDML Framework: Choice of Nuisance Function Approximator</p><p>For accurate and consistent estimation of nuisance parameters, we leverage the flexible machine learning approach using GNNs. However, our framework can integrate with any graph aggregation tool to estimate propensity scores and outcome models. As the nuisance parameters are functions of both the covariates of an individual unit and those of their social neighbors, their estimation requires aggregating information across the neighborhood. In an effort to demonstrate the generality of our framework, we adopted Network Random Forests (NeRF+) <ref type="bibr" target="#b58">(Tang et al., 2024)</ref>, which is a family of network-assisted prediction models built upon a generalization of random forests. These models may lack the representational power of GNNs; however, they are interpretable and can be an ideal choice for certain applications.</p><p>In our work, we employ the Graph Isomorphism Network (GIN) <ref type="bibr" target="#b67">Xu et al. (2018)</ref> due to its superior performance over other GNN architectures like <ref type="bibr">GCN Kipf and Welling (2016)</ref>, <ref type="bibr">GAT Veličković et al. (2017), and</ref><ref type="bibr">GraphSAGE Hamilton et al. (2017)</ref>. GIN's alignment with the representational capabilities of the Weisfeiler-Lehman test <ref type="bibr" target="#b45">Morris et al. (2019)</ref> makes it an ideal choice for effectively capturing the intricate dynamics inherent in social network structures. Table6 presents the result of two variations of our framework combined with GIN and NeRF+ on the Cora dataset with data generative process 34. The performance of GIN in this case is superior; however, the performance of NeRF+ is also close to the ground truth. As mentioned earlier, NeRF+ is interpretable, which may be necessary for some applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Partial causal graph of a network with three nodes. The left side shows the network topology, and the right side depicts the causal graph for each node with X , T, and Y as confounder, treatment, and outcome, respectively. Solid circles represent endogenous variables; dotted circles, exogenous. Blue edges indicate within-unit confounding, green edges show neighbor confounding, red edges represent direct effects, and yellow edges denote treatment interference.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Framework schema. The focal set is partitioned into train and estimation folds I -k and I k for cross-fitting. Propensity score and outcome models are learned over I -k using graph machine learning. Estimations of E[T | X, A] and E[Y | X, A] for I k are computed to derive residuals resT , resP E, and resY . Finally, resY is regressed on resT and resP E to obtain θk and αk for ADE and APE. This process is repeated across folds, and results are aggregated for final estimations of θ and α.</figDesc><graphic coords="8,72.00,72.00,467.96,130.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>and µ is the d ζ × d β orthogonalization parameter matrix whose true value µ 0 solves the equation J ζβ -µJ ββ = 0 for</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>The true parameter values ζ 0 and β 0 solve the following optimization problem max ζ∈Z,β∈B E P [L(W ; ζ, β)] where L(W ; ζ, β) is a known criterion function. ζ 0 and β 0 satisfy</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>10. 4</head><label>4</label><figDesc>Proof of Condition Set 2 C.2.1 Condition C.2.1 holds by the construction of the set Γ n and Assumption 6.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Cora (McCallum et al., 2000): It comprises academic research papers and their citation links, forming a graph structure. It consists of 2708 scientific publications classified into one of seven classes. The citation network consists of 5429 links.-Pubmed: Similar to Cora, it is a citation network, consists of 19717 scientific publications from PubMed database classified into one of three classes. The citation network consists of 44338 links. -Flickr: It is a network derived from Flickr, one of the largest platform for sharing photos. Each node in the graph represents an image, and if two images have shared characteristics like geographic location, gallery, or comments by the same user, there will be an edge connecting their respective nodes. It consists of 105938 nodes and 2316948 edges.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison of mean squared error of our GDML approach with other baselines. For T-Learner and Net TMLE methods, peer effect estimation is not applicable. L&amp;L's framework concentrates on total effect and does not calculate ADE and APE separately. *: results for Net TMLE and MaTresp on Flickr are not reported because it ran out of system memory.<ref type="bibr" target="#b37">Künzel et al., 2019)</ref> NetEst<ref type="bibr" target="#b33">(Jiang and Sun, 2022)</ref> Net TMLE<ref type="bibr" target="#b48">(Ogburn et al., 2022)</ref> L&amp;L<ref type="bibr" target="#b39">(Leung and Loupos, 2022)</ref> <ref type="bibr" target="#b41">Ma &amp; Tresp(Ma and Tresp, 2020</ref>) GDML w/o FS GDML</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Cora</cell><cell></cell><cell></cell><cell>Pubmed</cell><cell></cell><cell></cell><cell>Flickr</cell></row><row><cell></cell><cell></cell><cell></cell><cell>ADE</cell><cell>APE</cell><cell>ATE</cell><cell>ADE</cell><cell>APE</cell><cell>ATE</cell><cell>ADE</cell><cell>APE</cell><cell>ATE</cell></row><row><cell>PA</cell><cell></cell><cell></cell><cell>0.31 ±0.83</cell><cell>1.02 ±2.90</cell><cell>1.41 ±4.05</cell><cell>0.35 ±0.69</cell><cell>10.69 ±6.52</cell><cell>14.30 ±9.66</cell><cell cols="2">1133 ±4700 37719 ±143300 51790 ±199836</cell></row><row><cell cols="3">T-learner(Künzel et al., 2019)</cell><cell>9.84 ±51.32</cell><cell>N/A</cell><cell>N/A</cell><cell>1.67 ±4.58</cell><cell>N/A</cell><cell>N/A</cell><cell>2380 ±5495</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell cols="3">NetEst(Jiang and Sun, 2022)</cell><cell cols="2">174.66 ±1.07 9.48 ±1.75</cell><cell>71.96 ±4.35</cell><cell cols="4">1655.8 ±30.94 0.44 ±0.39 1603.53 ±45.65 53827 ±921</cell><cell>103 ±105</cell><cell>58503 ±3444</cell></row><row><cell cols="3">Net TMLE(Ogburn et al., 2022)</cell><cell>13.67 ±6.47</cell><cell>N/A</cell><cell>N/A</cell><cell>1.24 ±1.36</cell><cell>N/A</cell><cell>N/A</cell><cell>N/A*</cell><cell>N/A</cell><cell>N/A</cell></row><row><cell cols="3">L&amp;L(Leung and Loupos, 2022)</cell><cell>N/A</cell><cell>N/A</cell><cell>120.20 ±34.31</cell><cell>N/A</cell><cell>N/A</cell><cell>42.76 ±2.92</cell><cell>N/A</cell><cell>N/A</cell><cell>25.32 ±6.58</cell></row><row><cell cols="3">Ma &amp; Tresp(Ma and Tresp, 2020)</cell><cell>3.87 ±42.98</cell><cell>0.02 ±0.14</cell><cell>4.26 ±47.37</cell><cell>0.02 ±0.03</cell><cell>0.01 ±0.01</cell><cell>0.04 ±0.06</cell><cell>N/A*</cell><cell>N/A*</cell><cell>N/A*</cell></row><row><cell cols="2">GDML w/o FS</cell><cell></cell><cell>0.26 ±0.83</cell><cell>0.99 ±2.79</cell><cell>1.37 ±3.72</cell><cell>0.04 ±0.13</cell><cell>0.30 ±0.73</cell><cell>0.45 ±1.21</cell><cell>4.92 ±10.91</cell><cell>121 ±308</cell><cell>95 ±276</cell></row><row><cell>GDML</cell><cell></cell><cell></cell><cell>0.33 ±0.79</cell><cell>0.29 ±0.80</cell><cell>0.88 ±2.21</cell><cell>0.03±0.11</cell><cell>0.28 ±0.84</cell><cell>0.30 ±0.87</cell><cell>76 ±211</cell><cell>26.01 ±26.07</cell><cell>84 ±272</cell></row><row><cell>Dataset</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Method</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">PA T-learner(Cora 2</cell><cell>3.32</cell><cell></cell><cell>19020</cell><cell>4</cell><cell></cell><cell>5</cell><cell></cell><cell>26</cell><cell>9</cell><cell>5</cell></row><row><cell>Pubmed</cell><cell>55</cell><cell>48</cell><cell></cell><cell>22560</cell><cell cols="2">2464</cell><cell>133</cell><cell></cell><cell>8604</cell><cell>104</cell><cell>64</cell></row><row><cell>Flickr</cell><cell>1666</cell><cell>1712</cell><cell></cell><cell>31118</cell><cell cols="2">N/A</cell><cell>2909</cell><cell></cell><cell>N/A</cell><cell>2296</cell><cell>1832</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Comparison of runtime in seconds for different methods on each dataset. N/A indicates that the method did not return any results within a 12-hour runtime limit.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>: It is a 2010 survey data from villages in Karnataka, India. The survey gathered information from 16,995 individuals residing in 77 villages. It includes 15 features like age, occupation, gender, and more. Additionally, the dataset incorporated 12 distinct social networks involving 69,000 individuals, which included both the surveyed group of 16,995 individuals and others. These networks represented relationships like friendships, relatives, social visits, and financial exchanges.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>The results of coverage study our approach across datasets over 100 trials with the 95% confidence interval12.2 Case Study: SHG ParticipationBelow is the table presenting the results of six baseline methods and two versions of our framework on the Indian Village dataset:</figDesc><table><row><cell></cell><cell cols="2">GDML w/o Focal Set</cell><cell>PA</cell><cell cols="5">T-learner NetEst Net TMLE L&amp;L Ma &amp; Tresp</cell></row><row><cell>ADE</cell><cell>0.315</cell><cell>0.390</cell><cell>0.209</cell><cell>0.469</cell><cell>N/A*</cell><cell>0.291</cell><cell>N/A</cell><cell>0.295</cell></row><row><cell>APE</cell><cell>0.050</cell><cell>-0.002</cell><cell>-0.004</cell><cell>N/A</cell><cell>N/A*</cell><cell>N/A</cell><cell>N/A</cell><cell>0.016</cell></row><row><cell>ATE</cell><cell>0.365</cell><cell>0.388</cell><cell>0.205</cell><cell>N/A</cell><cell>N/A*</cell><cell>N/A</cell><cell>0.113</cell><cell>0.311</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>The results of six baseline methods and two versions of our framework on the Indian Village dataset NetEst was not stable during training on this data and resulted in NaN values. Note that the confidence intervals for ADE and APE generated by our framework are [</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Figure4: Relative error of SBM for different P intra with 3000 nodes, 200 components and P inter = 0.0001 P intra focal set size # edges MSE Mean squared error (MSE) for our Gnn-DML approach on graph generated using a stochastic block model with 3000 units and 200 blocks for different values of intra-block tie probabilities, represented as P intra .</figDesc><table><row><cell>0.01</cell><cell>2382</cell><cell>655</cell><cell>0.052</cell></row><row><cell>0.05</cell><cell>1788</cell><cell>1536</cell><cell>0.093</cell></row><row><cell>0.1</cell><cell>1349</cell><cell>2588</cell><cell>0.124</cell></row><row><cell>0.25</cell><cell>636</cell><cell>5673</cell><cell>0.351</cell></row><row><cell>0.5</cell><cell>271</cell><cell cols="2">10872 1.193</cell></row><row><cell>0.75</cell><cell>200</cell><cell cols="2">16157 2.120</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>These conditions are discussed in more depth in Appendix 10</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ADE APE ATE</head><p>GDML + NeRF+ 1.61    </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Aronow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Samii</surname></persName>
		</author>
		<title level="m">Estimating average causal effects under interference between units. arXiv: Statistics Theory</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Estimating average causal effects under general interference</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Aronow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Samii</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>with application to a social network experiment</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Shock-based causal inference in corporate finance and accounting research</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">A</forename><surname>Atanasov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Critical Finance Review</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="207" to="304" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Exact P-values for Network Interference</title>
		<author>
			<persName><forename type="first">S</forename><surname>Athey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Eckles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Imbens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NBER Working Papers 21313</title>
		<imprint>
			<publisher>National Bureau of Economic Research, Inc</publisher>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">The local approach to causal inference under network interference</title>
		<author>
			<persName><forename type="first">E</forename><surname>Auerbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tabord-Meehan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.03810</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">S</forename><surname>Baird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Bohren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mcintosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ozler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note>Designing experiments to measure spillover effects</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Gossip: Identifying central individuals in a social network</title>
		<author>
			<persName><forename type="first">A</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Chandrasekhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Duflo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">O</forename><surname>Jackson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>National Bureau of Economic Research</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Causal inference under interference and network uncertainty</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Malinsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Shpitser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Uncertain Artificial Intelligence</title>
		<imprint>
			<biblScope unit="page">372</biblScope>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Cluster randomised trials in the medical literature: two bibliometric surveys</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Bland</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004-08-13">2004-08-13</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">21</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Place-Based Interventions at Scale: The Direct and Spillover Effects of Policing and City Services on Crime [Clustering as a Design Problem]</title>
		<author>
			<persName><forename type="first">C</forename><surname>Blattman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tobón</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the European Economic Association</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2022" to="2051" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Double/debiased machine learning for treatment and structural parameters</title>
		<author>
			<persName><forename type="first">V</forename><surname>Chernozhukov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chetverikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Demirer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Duflo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Newey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Robins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Graph infomax adversarial learning for treatment effect estimation with networked observational data</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Rathbun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining, KDD &apos;21</title>
		<meeting>the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining, KDD &apos;21<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="176" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Using embeddings for causal estimation of peer influence in social networks</title>
		<author>
			<persName><forename type="first">I</forename><surname>Cristali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Veitch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Oh</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Belgrave</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">L</forename><surname>Douglas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1101.5211</idno>
		<title level="m">The weisfeiler-lehman method and graph isomorphism testing</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Design and analysis of experiments in networks: Reducing bias from interference</title>
		<author>
			<persName><forename type="first">D</forename><surname>Eckles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Karrer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ugander</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Causal Inference</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">20150021</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Identification and estimation of treatment and interference effects in observational studies on networks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Forastiere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Airoldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Mealli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="issue">534</biblScope>
			<biblScope unit="page" from="901" to="918" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Estimating causal effects on social networks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Forastiere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Mealli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Airoldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE 5th International Conference on Data Science and Advanced Analytics (DSAA)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="60" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Algebra</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gallier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Quaintance</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Topology, Differential Calculus, and Optimization Theory For Computer Science and Machine Learning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">336</biblScope>
		</imprint>
		<respStmt>
			<orgName>University of Pennsylvania</orgName>
		</respStmt>
	</monogr>
	<note>Proposition 9</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Causal inference in empirical archival financial accounting research</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gassen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Accounting, Organizations and Society</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="535" to="544" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Gilad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Salimi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.10591</idno>
		<title level="m">Heterogeneous treatment effects in social networks</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning individual causal effects from networked observational data</title>
		<author>
			<persName><forename type="first">R</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th International Conference on Web Search and Data Mining</title>
		<meeting>the 13th International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Ignite: A minimax game toward learning individual treatment effects from networked observational data</title>
		<author>
			<persName><forename type="first">R</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2020">2020a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning individual causal effects from networked observational data</title>
		<author>
			<persName><forename type="first">R</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th international conference on web search and data mining</title>
		<meeting>the 13th international conference on web search and data mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="232" to="240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Causal inference for vaccine effects on infectiousness</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Halloran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Hudgens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Biostatistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="40" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Causal inference in infectious diseases</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Halloran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Struchiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Epidemiology</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="142" to="151" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Stochastic blockmodels: First steps</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Holland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">B</forename><surname>Laskey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Leinhardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Social Networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="109" to="137" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Average direct and indirect causal effects under interference</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wager</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1165" to="1172" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Toward causal inference with interference</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hudgens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Halloran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="page" from="832" to="842" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Toward causal inference with interference</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Hudgens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Halloran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="page" from="832" to="842" />
			<date type="published" when="2008">2008b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Imai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Malani</surname></persName>
		</author>
		<title level="m">Causal inference with interference and noncompliance in two-stage randomized experiments</title>
		<imprint>
			<publisher>Publisher: Taylor &amp; Francis</publisher>
			<date type="published" when="2021-04-03">2021-04-03</date>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="page" from="632" to="644" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Social and economic networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">O</forename><surname>Jackson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Princeton university press Princeton</publisher>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Social capital and social quilts: Network patterns of favor exchange</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">O</forename><surname>Jackson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rodriguez-Barraquer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Economic Review</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1857" to="1897" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Estimating causal effects on networked observational data via representation learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st ACM International Conference on Information &amp; Knowledge Management</title>
		<meeting>the 31st ACM International Conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="852" to="861" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning representations for counterfactual inference</title>
		<author>
			<persName><forename type="first">F</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Shalit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sontag</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 33rd International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</editor>
		<meeting>The 33rd International Conference on Machine Learning<address><addrLine>New York, New York, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="3020" to="3029" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Network experimentation at scale</title>
		<author>
			<persName><forename type="first">B</forename><surname>Karrer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bhole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Konutgan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th acm sigkdd conference on knowledge discovery &amp; data mining</title>
		<meeting>the 27th acm sigkdd conference on knowledge discovery &amp; data mining</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3106" to="3116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Metalearners for estimating heterogeneous treatment effects using machine learning</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Künzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Sekhon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Bickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="4156" to="4165" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Causal inference for a population of causally connected units</title>
		<author>
			<persName><forename type="first">M</forename><surname>Laan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Causal Inference</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Loupos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2211.07823</idno>
		<title level="m">Unconfoundedness with network interference</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">On inverse probability-weighted estimators in the presence of interference</title>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Hudgens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Becker-Dreps</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="829" to="842" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Causal inference under networked interference and intervention policy enhancement</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Tresp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Identification of treatment response with social interactions</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Manski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Econometrics Journal</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="S23" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Automating the construction of internet portals with machine learning</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nigam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rennie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Seymore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Retrieval</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="127" to="163" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Doubly robust estimation of causal effects in network-based observational studies</title>
		<author>
			<persName><forename type="first">V</forename><surname>Mcnealis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">E</forename><surname>Moodie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.00230</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Weisfeiler and leman go neural: Higher-order graph neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ritzert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Lenssen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rattan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Grohe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019</title>
		<meeting>the Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019<address><addrLine>Honolulu, Hawaii, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2019-01-27">2019. January 27 -February 1, 2019</date>
			<biblScope unit="page" from="4602" to="4609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">A double machine learning approach to combining experimental and observational data</title>
		<author>
			<persName><forename type="first">M</forename><surname>Morucci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Orlandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rudin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Volfovsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.01449</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Why social networks are different from other types of networks</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Newman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical review E</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">36122</biblScope>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Causal inference for social network data</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Ogburn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Sofrygin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Diaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Van Der Laan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Ogburn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Vanderweele</surname></persName>
		</author>
		<title level="m">Causal diagrams for interference</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Spatial causal inference in the presence of unmeasured confounding and interference</title>
		<author>
			<persName><forename type="first">G</forename><surname>Papadogeorgou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Samanta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.08218</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Malts: Matching after learning to stretch</title>
		<author>
			<persName><forename type="first">H</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rudin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Volfovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="10952" to="10993" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Testing for arbitrary interference on experimentation platforms</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Saint-Jacques</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Saveski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Airoldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="929" to="940" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">The central role of the propensity score in observational studies for causal effects</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">R</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="55" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Causal relational learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Salimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kayali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Suciu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 ACM SIGMOD international conference on management of data</title>
		<meeting>the 2020 ACM SIGMOD international conference on management of data</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="241" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName><forename type="first">P</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">93</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Estimating individual treatment effect: generalization bounds and algorithms</title>
		<author>
			<persName><forename type="first">U</forename><surname>Shalit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">D</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sontag</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Precup</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</editor>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="3076" to="3085" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">What do randomized studies of housing mobility demonstrate</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Sobel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">476</biblScope>
			<biblScope unit="page" from="1398" to="1407" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Interpretable network-assisted prediction via random forests</title>
		<author>
			<persName><forename type="first">T</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Levina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">Working Paper</note>
	<note>Unpublished</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">On causal inference in the presence of interference</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J T</forename><surname>Tchetgen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Vanderweele</surname></persName>
		</author>
		<idno type="PMID">21068053</idno>
	</analytic>
	<monogr>
		<title level="j">Statistical Methods in Medical Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="55" to="75" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Estimation of causal peer influence effects</title>
		<author>
			<persName><forename type="first">P</forename><surname>Toulis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kao</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Dasgupta</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</editor>
		<meeting>the 30th International Conference on Machine Learning<address><addrLine>Atlanta, Georgia, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1489" to="1497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Graph cluster randomization: network exposure to multiple universes</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ugander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Karrer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Backstrom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kleinberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;13</title>
		<meeting>the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;13<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="329" to="337" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Randomized graph cluster randomization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ugander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Causal Inference</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">20220014</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Bounding the infectiousness effect in vaccine trials</title>
		<author>
			<persName><forename type="first">T</forename><surname>Vanderweele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Tchetgen Tchetgen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Epidemiology</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="686" to="693" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Social networks and causal inference. Handbook of causal analysis for social research</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Vanderweele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>An</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="353" to="374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Using embeddings to correct for unobserved confounding in networks</title>
		<author>
			<persName><forename type="first">V</forename><surname>Veitch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><forename type="middle">D</forename><surname>Alché-Buc</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.00826</idno>
		<title level="m">How powerful are graph neural networks? arXiv preprint</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Representation learning for treatment effect estimation from observational data</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Huai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Cesa-Bianchi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Causal inference under network interference: Network embedding matching</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Bipartite causal inference with interference</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Zigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Papadogeorgou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistical science: a review journal of the Institute of Mathematical Statistics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">109</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Machine learning for causal inference: on the use of cross-fit estimators</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Zivich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Breskin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Epidemiology</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">393</biblScope>
			<date type="published" when="2021">2021</date>
			<pubPlace>Cambridge, Mass</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">8 Proof of Identifiability In this section, we present a detailed, step-by-step proof of the identifiability of the Average Direct Effect (ADE) and the Average Partial Effect (APE)</title>
		<imprint/>
	</monogr>
	<note>based on the assumptions outlined in Section 3.1</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
