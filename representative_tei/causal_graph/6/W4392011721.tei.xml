<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Graph Out-of-Distribution Generalization via Causal Intervention</title>
				<funder ref="#_SaMbCXT #_UMGa5Z4">
					<orgName type="full">NSFC</orgName>
				</funder>
				<funder ref="#_zT7w2vQ">
					<orgName type="full">SJTU Trans-med Awards Research</orgName>
					<orgName type="abbreviated">STAR</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-08-16">16 Aug 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Qitian</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Fan</forename><surname>Nie</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Chenxiao</forename><surname>Yang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Tianyi</forename><surname>Bao</surname></persName>
						</author>
						<author role="corresp">
							<persName><forename type="first">Junchi</forename><surname>Yan</surname></persName>
							<email>yanjunchi@sjtu.edu.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University Shanghai</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Shanghai Jiao Tong University Shanghai</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Shanghai Jiao Tong University Shanghai</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Shanghai Jiao Tong University Shanghai</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Shanghai Jiao Tong University Shanghai</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Graph Out-of-Distribution Generalization via Causal Intervention</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-08-16">16 Aug 2024</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3589334.3645604</idno>
					<idno type="arXiv">arXiv:2402.11494v2[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Graph Representation Learning</term>
					<term>Graph Neural Networks</term>
					<term>Distribution Shifts</term>
					<term>Out-of-Distribution Generalization</term>
					<term>Causal Inference</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Out-of-distribution (OOD) generalization has gained increasing attentions for learning on graphs, as graph neural networks (GNNs) often exhibit performance degradation with distribution shifts. The challenge is that distribution shifts on graphs involve intricate interconnections between nodes, and the environment labels are often absent in data. In this paper, we adopt a bottom-up datagenerative perspective and reveal a key observation through causal analysis: the crux of GNNs' failure in OOD generalization lies in the latent confounding bias from the environment. The latter misguides the model to leverage environment-sensitive correlations between ego-graph features and target nodes' labels, resulting in undesirable generalization on new unseen nodes. Built upon this analysis, we introduce a conceptually simple yet principled approach for training robust GNNs under node-level distribution shifts, without prior knowledge of environment labels. Our method resorts to a new learning objective derived from causal inference that coordinates an environment estimator and a mixture-of-expert GNN predictor. The new approach can counteract the confounding bias in training data and facilitate learning generalizable predictive relations. Extensive experiment demonstrates that our model can effectively enhance generalization with various types of distribution shifts and yield up to 27.4% accuracy improvement over state-of-the-arts on graph OOD generalization benchmarks. Source codes are available at <ref type="url" target="https://github.com/fannie1208/CaNet">https://github.com/fannie1208/CaNet</ref>.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Graph neural networks (GNNs) <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b34">35]</ref> have emerged as a de facto class of encoder backbones for modeling interdependent data and efficiently computing node representations that can be readily adapted to diverse graph-based applications, including social network analysis <ref type="bibr" target="#b32">[33]</ref>, recommender systems <ref type="bibr" target="#b37">[38]</ref>, clinical healthcare <ref type="bibr" target="#b43">[44]</ref>, traffic control <ref type="bibr" target="#b10">[11]</ref>, anomaly detection <ref type="bibr" target="#b45">[46]</ref>, etc.</p><p>Despite solid advances in the expressivity and representational power of GNNs, most of existing models focus on improving the accuracy on in-distribution data, i.e., the testing nodes generated from an identical distribution as the training ones. However, recent evidence <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b46">47]</ref> suggests that GNNs can perform unsatisfactorily on out-of-distribution (OOD) data where the data-generating distributions exhibit differences from training observations. We illustrate such an issue through a typical example for node property prediction with GNNs, as shown in Fig. <ref type="figure" target="#fig_0">1</ref>. Let us consider a social network where the nodes correspond to users and the goal is to predict whether a user likes playing basketball. In general, if a user's friends love sports, then the conditional probability for the user liking basketball would be high, which can be treated as a stable (or interchangeably, environment-insensitive) relation from the ego-graph feature (the GNN model actually processes as input of each node) to the label of the target node. Yet, there also exists positive correlation between "a user's friends are young" and "the user likes basketball" on condition that the social network is formed in a university where the marginal probability for "a user's friends are young" and "a user likes playing basketball" are both high. The relation from such an ego-graph feature to the label is unstable (or interchangeably, environment-sensitive), since this correlation does not hold elsewhere like LinkedIn where the marginal distributions for user's ages and hobbies have considerable diversity. The prediction relying on the latter unstable relation would fail once the environment changes from universities to LinkedIn, which causes Nevertheless, the challenge is that distribution shifts on graphs are associated with the inter-connecting nature of data generation, which requires the model to accommodate the structural features among neighbored nodes for OOD generalization <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b49">50]</ref>. Second, unlike image data <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref> where the dataset often contains the context for each image instance that serves as environment labels indicating the source distribution of each instance, in the graph learning problem, the environment labels for nodes are often unavailable <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b42">43]</ref>. This poses an obstacle for inferring useful environment information from observed data which can properly guide the model to learn generalizable patterns for prediction.</p><p>In this paper, we adopt causal analysis from a bottom-up datagenerative perspective to investigate the learning behaviors of GNNs under distribution shifts. We reveal that the crux of GNNs' deficiency for OOD generalization lies in the latent environment confounder that leads to the confounding bias and over-fitting on the environment-sensitive relations. On top of this, we propose a provably effective approach, dubbed as Causal Intervention for Network Data (CaNet), for guiding GNNs to learn stable predictive relations from training data, without prior knowledge of environment labels. We introduce a new learning objective (derived from backdoor adjustment and variational inference) that collaboratively trains an environment estimator and a mixture-of-expert GNN predictor. The former aims to infer pseudo environment labels based on input ego-graphs to partition nodes in the graph into clusters from disparate distributions. The GNN predictor resorts to mixtureof-expert propagation networks dynamically selected by the pseudo environments. The new objective can alleviate the confounding bias in training data and helps to capture the environment-insensitive predictive relations that are generalizable across environments.</p><p>To evaluate the approach, we conduct extensive experiments on six graph datasets with various types of distribution shifts. The results manifest that the proposed approach can significantly improve the generalization performance of different GNN models when distribution shifts occur and yield up to 27.4% performance improvements over the state-of-the-arts for graph out-of-distribution generalization. We summarize the contributions below.</p><p>â€¢ We analyze the generalization ability of GNNs under distribution shifts from a causal data-generative perspective, and identify that GNNs trained with maximum likelihood estimation would capture the unstable relations from ego-graph features to labels due to the confounding bias of unobserved environments.</p><p>â€¢ We propose a simple yet principled approach for training GNNs under distribution shifts. The model resorts to a novel learning objective that facilitates GNNs to capture environment-insensitive predictive patterns, by means of an environment estimator that infers pseudo environments to eliminate the confounding bias.</p><p>â€¢ We apply our model to various datasets with different types of distribution shifts from training to testing nodes. The results consistently demonstrate the superiority of our model over other graph OOD generalization methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORKS</head><p>Graph Neural Networks. GNNs come into the spotlight due to their effectiveness for learning high-quality representations from graph data <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b48">49]</ref>. While GNNs' expressiveness and representational power have been extensively studied, their generalization capability has remained largely an open question, particularly generalization to testing data generated from different distributions than training data <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b15">16]</ref>. The analysis in this paper reveals that the crux of out-of-distribution generalization on graph data lies in the unobserved environments as a latent confounder, built upon which we propose a provably effective model for addressing this challenge.</p><p>Out-of-Distribution Learning on Graphs. Learning with distribution shifts on graphs has aroused increasing interest in the graph learning community <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b42">43]</ref>. One line of research focuses on endowing the models with capability for outof-distribution detection <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b36">37]</ref>. For out-of-distribution generalization which is the focus of this work, some recent works explore size generalization of GNNs under specific data-generative assumptions <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b44">45]</ref>. However, their discussions focus on graph classification, which is different from node property prediction where node instances are inter-dependent <ref type="bibr" target="#b12">[13]</ref>. For OOD generalization in node-level prediction, recent works propose to harness invariant learning <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b46">47]</ref>, the connection between GNNs and MLPs <ref type="bibr" target="#b41">[42]</ref> and multi-view consistency <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b49">50]</ref> as effective means for improving the generalization capability of graph neural networks. Different from these works, we explore a new approach by means of causal inference for generalization with graph data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROBLEM FORMULATION</head><p>In this section, we introduce notations and the problem setup. All the vectors are column vectors by default and denoted by bold lowercase letters. We adopt bold capital letters to denote matrices and small capital letters to denote random variables. We use ğ‘ to represent the data distribution (ğ‘ ğ‘¡ğ‘Ÿ /ğ‘ ğ‘¡ğ‘’ is used for specifying training/testing data) while ğ‘ ğœƒ denotes the predictive distribution induced by the model with parameterization ğœƒ . Besides, ğ‘ and ğ‘ ğœ™ denote other distributions, typically the variational distributions.</p><p>Predictive Tasks on Graphs. Assume a graph G = (V, E) with ğ‘ nodes, where V and E denote the node set and edge set, respectively. Besides, X = [x ğ‘£ ] ğ‘£ âˆˆ V âˆˆ R ğ‘ Ã—ğ· denotes the node feature matrix, where ğ· is the input feature dimension, and A = [ğ‘ ğ‘£ğ‘¢ ] ğ‘£,ğ‘¢ âˆˆ V âˆˆ {0, 1} ğ‘ Ã—ğ‘ denotes the adjacency matrix. If there exists an edge between node ğ‘¢ and ğ‘£, then ğ‘ ğ‘¢ğ‘£ = 1, and otherwise 0. Each node corresponds to a label, denoted by a one-hot vector y ğ‘£ âˆˆ {0, 1} ğ¶ where ğ¶ is the number of classes. The predictive tasks on graphs can be defined as: given labels {y ğ‘£ } ğ‘£ âˆˆ V ğ‘¡ğ‘Ÿ for training nodes V ğ‘¡ğ‘Ÿ , one aims to predict labels {y ğ‘£ } ğ‘£ âˆˆ V ğ‘¡ğ‘’ for testing nodes V ğ‘¡ğ‘’ = V \ V ğ‘¡ğ‘Ÿ with node features X and graph adjacency A.</p><p>From a data-generating perspective, the input graph G can be seen as a collection of (overlapping) pieces of ego-networks <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b38">39]</ref>. For node ğ‘£, its ğ¿-hop ego-graph is denoted as</p><formula xml:id="formula_0">G (ğ¿) ğ‘£ = (X (ğ¿) ğ‘£ , A (ğ¿) ğ‘£ ) where X (ğ¿) ğ‘£ and A (ğ¿)</formula><p>ğ‘£ are the node feature matrix and the adjacency matrix induced by nodes in ğ‘£'s ğ¿-hop neighborhood. To keep notations clean, we omit the superscript and use G ğ‘£ to represent the ego-graph of ğ‘£ unless otherwise specified for emphasizing the order. Furthermore, we define ğº as a random variable of ego-graphs G ğ‘£ 's and ğ‘Œ as a random variable for node labels y ğ‘£ 's.</p><p>Distribution Shifts on Graphs. The distribution shifts induce that ğ‘ ğ‘¡ğ‘Ÿ (ğº, ğ‘Œ ) â‰  ğ‘ ğ‘¡ğ‘’ (ğº, ğ‘Œ ), i.e. the data distributions that generate the ego-graphs and labels of training and testing nodes are different. A crucial concept in OOD generalization is the environment<ref type="foot" target="#foot_0">foot_0</ref> that serves as the direct cause for the data-generating distribution <ref type="bibr" target="#b38">[39]</ref>. In node property prediction, the environment can be a general reflection for where or when the nodes in a graph are generated. For example, as shown by the social network example in Section 1, the environment is where the graph is collected ("university" or "LinkedIn"). In protein networks <ref type="bibr" target="#b50">[51]</ref>, the environment can be the species that the protein belongs to. In citation networks <ref type="bibr" target="#b12">[13]</ref>, the environment can be when the paper is published (e.g., "before 2010" or "from 2010 to 2015"). The specific physical meanings for environments depend on particular datasets. Without loss of generality, define ğ¸ as the random variable of environments and ğ‘’ as its realization, and the data-generating distribution can be characterized by ğ‘ƒ (ğº, ğ‘Œ |ğ¸) = ğ‘ƒ (ğº |ğ¸)ğ‘ƒ (ğ‘Œ |ğº, ğ¸), i.e., ğ¸ impacts the generation process of ğº and ğ‘Œ . Fig. <ref type="figure" target="#fig_0">1(c</ref>) illustrates the dependence of three random variables through a causal diagram which highlights that the environment ğ¸ is the common cause for ğº and ğ‘Œ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">PROPOSED MODEL</head><p>We next present our analysis and proposed method. In Section 4.1, we first analyze the generalization behaviors of common GNNs in node property prediction under distribution shifts and reveal what causes the deficiency of GNNs w.r.t. out-of-distribution data. Based on the analysis, in Section 4.2 and 4.3, we introduce the formulation and instantiations for our proposed model, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Causal Analysis for Graph Neural Networks</head><p>To understand the generalization behaviors of GNNs, we present a proof-of-concept causal analysis on the dependence among variables of interest regarding node property prediction. The results attribute the failure of GNN models for out-of-distribution (OOD) generalization to the confounding bias of unobserved environments.</p><p>The Confounding Bias of Latent Environments. Common GNN models take a graph G as input, iteratively update node representations through aggregating neighbored nodes' features and output the estimated label for each node. Specifically, assume z (ğ‘™ ) ğ‘£ as the representation of node ğ‘£ at the ğ‘™-th layer and the updating rule of common GNNs can be written as</p><formula xml:id="formula_1">z (ğ‘™+1) ğ‘£ = ğœ Conv (ğ‘™ ) {z (ğ‘™ ) ğ‘¢ |ğ‘¢ âˆˆ N ğ‘£ âˆª {ğ‘£ }} ,<label>(1)</label></formula><p>where N ğ‘£ is the set of nodes connected with ğ‘£ in the graph and Conv (ğ‘™ ) is a graph convolution operator over node representations. For example, in vanilla GCN <ref type="bibr" target="#b13">[14]</ref>, Conv (ğ‘™ ) is instantiated as a parameterized linear transformation of node representations and a normalized aggregation. Also, the initial node embeddings are often computed by the node features z</p><p>ğ‘£ = ğœ™ ğ‘–ğ‘› (x ğ‘£ ) and the predicted labels are given by the last-layer embeddings Å·ğ‘£ = ğœ™ ğ‘œğ‘¢ğ‘¡ (z (ğ¿+1) ğ‘£</p><p>) if using ğ¿ layers of propagation. Here ğœ™ ğ‘–ğ‘› and ğœ™ ğ‘œğ‘¢ğ‘¡ can be shallow neural networks. Notice that, for each node ğ‘£, what the GNN model actually processes as input for prediction is its ego-graph G ğ‘£ (in particular, for an ğ¿-layer GNN, G ğ‘£ consists of all the ğ¿-hop neighbored nodes centered at ğ‘£). Therefore, the prediction for node ğ‘£ can be denoted by Å·ğ‘£ = ğ‘“ ğœƒ (G ğ‘£ ) where ğ‘“ ğœƒ denotes the GNN model with trainable parameter set ğœƒ . We define Å¶ as a random variable for predicted node labels Å·ğ‘£ 's and ğ‘ ğœƒ ( Å¶ |ğº) denotes the predictive distribution induced by the model ğ‘“ ğœƒ .</p><p>The common practice is to adopt maximum likelihood estimation (MLE) as the training objective which maximizes the likelihood ğ‘ ğœƒ ( Å¶ |ğº). For node property prediction, the negative log-likelihood that is minimized as training objective is the cross-entropy loss:</p><formula xml:id="formula_3">ğœƒ * = arg min ğœƒ - 1 |V ğ‘¡ğ‘Ÿ | âˆ‘ï¸ ğ‘£ âˆˆ V ğ‘¡ğ‘Ÿ y âŠ¤ ğ‘£ log ğ‘“ ğœƒ (G ğ‘£ ).<label>(2)</label></formula><p>Based on the above illustration of the GNN's modeling and learning on graphs, the dependence among the (input) ego-graph ğº, the predicted label Å¶ and the latent environment ğ¸ can be characterized by another causal diagram as shown in Fig. <ref type="figure">2(a)</ref>. We next illustrate the rationales behind each dependence edge shown in Fig. <ref type="figure">2(a)</ref>.</p><p>â€¢ ğ‘® â†’ Å¶ . The dependence is given by the feed-forward computation of GNN model Å·ğ‘£ = ğ‘“ ğœƒ (G ğ‘£ ), i.e., the model predictive distribution ğ‘ ğœƒ ( Å¶ |ğº). The relation between ğº and ğ‘Œ becomes deterministic if given fixed model parameter ğœƒ .</p><p>â€¢ ğ‘¬ â†’ ğ‘®. This dependence is given by ğ‘ (ğº |ğ¸) in data generation.</p><p>â€¢ ğ‘¬ â†’ Å¶ . This relation is embodied through the learning process. Since ğ¸ affects the distribution for observed data via ğ‘ (ğº, ğ‘Œ |ğ¸) = ğ‘ (ğº |ğ¸)ğ‘ (ğ‘Œ |ğº, ğ¸), if we denote by ğ‘ ğ‘¡ğ‘Ÿ (ğ¸) the distribution for (unobserved) training environments, the learning algorithm yields</p><formula xml:id="formula_4">ğœƒ * = arg min ğœƒ E ğ‘’âˆ¼ğ‘ ğ‘¡ğ‘Ÿ (ğ¸ ),( G ğ‘£ ,y ğ‘£ )âˆ¼ğ‘ (ğº,ğ‘Œ |ğ¸=ğ‘’ ) [-y âŠ¤ ğ‘£ log ğ‘“ ğœƒ (G ğ‘£ )].<label>(3)</label></formula><p>This suggests that the well-trained model parameter ğœƒ * is dependent on the distribution of ğ¸, leading to the dependence of Å¶ on ğ¸. Such a causal relation can also be interpreted intuitively with two facts:</p><p>1) ğ¸ affects the generation of data used for training the GNN model, and 2) Å¶ is the output of the trained model.</p><p>Interpretations for Harmful Effects. From Fig. <ref type="figure">2</ref>(a) and the above illumination, we can see that if we optimize the likelihood ğ‘ ğœƒ ( Å¶ |ğº), the confounding effect of ğ¸ on ğº and Å¶ will mislead the GNN model to capture the shortcut predictive relation between the ego-graph G ğ‘£ and the label ğ‘¦ ğ‘£ , i.e., the existing correlation that is induced by certain ğ‘’'s in training data (e.g., "G ğ‘£ : a user's friends are young" and "ğ‘¦ ğ‘£ : the user likes playing basketball" are both with high probability due to the unobserved environment ğ‘’ "university" in social networks). Therefore, the training process would incline to purely increase the training accuracy by exploiting such easyto-capture yet unreliable correlation (e.g., the predictive relation from "G ğ‘£ : a user's friends are young" to "ğ‘¦ ğ‘£ : the user likes playing basketball") in observational data. The issue, however, is that this kind of correlation is non-stable and sensitive to distribution shifts: for testing data that has distinct environment context, i.e., ğ‘ƒ ğ‘¡ğ‘’ (ğ¸) â‰  ğ‘ƒ ğ‘¡ğ‘Ÿ (ğ¸) (e.g., testing users are from another environment "LinkedIn"), the above-mentioned correlation does not necessarily hold. The model that mistakenly over-fits the environment-sensitive relations in training data would suffer from failures or undesired prediction on out-of-distribution data in testing stage.</p><p>Implications for Potential Solutions. The analysis enlightens one potential solution for improving the OOD generalization ability of GNNs in node property prediction: one can guide the model to uncover the stable predictive relations behind data, particularly the ones insensitive to environment variation (e.g., the relation from "G ğ‘£ : a user's friends love sports" to "ğ‘¦ ğ‘£ : the user likes playing basketball"). Formally speaking, we can train the model by optimizing ğ‘ ğœƒ ( Å¶ |ğ‘‘ğ‘œ (ğº)), where in causal literature the ğ‘‘ğ‘œ-operation means removing the dependence from other variables on the target, to cancel out the effect of ğ¸ on ğº such that the unstable correlation between G ğ‘£ and ğ‘¦ ğ‘£ will no longer be captured by the model. Compared with ğ‘ ğœƒ ( Å¶ |ğº) where the condition is on a given observation G ğ‘£ of ğº, the ğ‘‘ğ‘œ-operation in ğ‘ ğœƒ ( Å¶ |ğ‘‘ğ‘œ (ğº)) enforces the condition that intervenes the value of ğº as G ğ‘£ and removes the effects from other variables on ğº (i.e., ğ¸ in our case), as conceptually shown by Fig. <ref type="figure">2(b</ref>). We next discuss how to put this general idea into practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Model Formulation: A Causal Treatment</head><p>An ideal way for exactly computing ğ‘ ğœƒ ( Å¶ |ğ‘‘ğ‘œ (ğº)) is to physically intervene ğº, e.g., by randomized controlled trial (RCT) <ref type="bibr" target="#b24">[25]</ref> where data is recollected from a prohibitively large quantity of random samples. The randomized experiments gather new data that removes the bias from the environment by enumerating any possible environment context in a physical scene, based on which the model can learn stable relations from ğº to ğ‘Œ and generalize well to new distributions. Nevertheless, this can be intractable due to limited resources in practice. We thereby resort to a learning approach based on observational data (i.e., G ğ‘£ 's and ğ‘¦ ğ‘£ 's).</p><p>First, we harness the backdoor adjustment <ref type="bibr" target="#b24">[25]</ref> that gives rise to (see derivation in Appendix A.1)</p><formula xml:id="formula_5">ğ‘ ğœƒ ( Å¶ |ğ‘‘ğ‘œ (ğº)) = E ğ‘ 0 (ğ¸ ) [ğ‘ ğœƒ ( Å¶ |ğº, ğ¸)],<label>(4)</label></formula><p>where ğ‘ 0 is the prior distribution of environments that reflects how plausible the environment context could happen without any information of observed data. Though we have known that the latent environment plays an important role in data generation and impact the generalizability of GNNs, the actual meaning or form of environments is often unknown. This leads to the difficulty of instantiating ğ‘ ğœƒ ( Å¶ |ğº, ğ¸) to particular forms involving the effect of the environments on the prediction. Even for cases where the environment information could be partially reflected by certain node features (e.g., publication years of papers in citation networks or species groups of proteins in PPI networks), we empirically found that the environment labels may not be informative enough for guiding GNNs to learn stable relations with distribution shifts. To effectively handle the confounding effects of unobserved environments, we next introduce an approximation method by inferring the latent environments in a data-driven manner. Approximated Intervention with Environment Inference. Our basic idea is to generate pseudo environment labels as latent variables (agnostic of specific actual environments) that are regularized to be independent of the ego-graph features and guide the model to capture stable relations between ğº and ğ‘Œ . To implement this idea, we consider collaborative learning of two models: i) an environment estimator ğ‘ ğœ™ (ğ¸|ğº) with parameterization ğœ™ that takes the ego-graph features G ğ‘£ as input to infer the pseudo environment ğ‘’ ğ‘£ for node ğ‘£; ii) a GNN predictor ğ‘ ğœƒ ( Å¶ |ğº, ğ¸) whose prediction is based on input ego-graph G ğ‘£ and the inferred pseudo environment ğ‘’ ğ‘£ . The learning objective can be derived from a variational lower bound of Eqn. 4 (see derivation in Appendix A.2):</p><formula xml:id="formula_6">log ğ‘ ğœƒ ( Å¶ |ğ‘‘ğ‘œ (ğº)) â‰¥ E ğ‘ ğœ™ (ğ¸ |ğº ) [log ğ‘ ğœƒ ( Å¶ |ğº, ğ¸)] -L ğ‘ ğ‘¢ğ‘ -ğ¾ğ¿(ğ‘ ğœ™ (ğ¸|ğº)âˆ¥ğ‘ 0 (ğ¸)) -L ğ‘Ÿğ‘’ğ‘” . (<label>5</label></formula><formula xml:id="formula_7">)</formula><p>The first term can optimize the predictive power of the GNN and the second term regularizes that the pseudo environments should be independent of ego-graphs. The maximization of Eqn. 5 contributes to lifting the variational lower bound of ğ‘ ğœƒ ( Å¶ |ğ‘‘ğ‘œ (ğº)) that facilitates the GNN model ğ‘ ğœƒ ( Å¶ |ğº, ğ¸) trained with inferred pseudo environments to capture the stable correlation between ego-graphs and labels. The learning objective of Eqn. 5 approximately achieves the ideal goal (of causal intervention) such that the model's prediction would be guided to be primarily based on the environmentinsensitive patterns in ğº and robust to distribution shifts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Model Instantiations</head><p>Notice again that we do not require environment labels in data or any prior knowledge of the physical meaning of unobserved environments, and neither require that the pseudo environments should reflect the actual contextual information. Therefore, we assume the pseudo environments as latent variables represented by numerical vectors for each node ğ‘£. Nevertheless, we expect the representation of the pseudo environments to be informative enough, on top of which the model can learn useful patterns from observed data to benefit learning stable relations for better generalization. As mentioned in Section 1, one observation is that the distribution shifts on graphs often involve inter-connection of nodes, i.e., the structural features of ego-graphs can be informative and contain the desired stable patterns. Therefore, for better capacity, we generalize the notion of pseudo environments to a series of vector representations pertaining to each layer of the GNN model, as illustrated in Fig. <ref type="figure" target="#fig_4">3</ref> with details described below.</p><p>Pseudo Environment Estimator ğ‘ ğœ™ (ğ¸|ğº). We assume e </p><formula xml:id="formula_8">ğ… (ğ‘™ ) ğ‘£ = Softmax(W (ğ‘™ ) ğ‘† z (ğ‘™ ) ğ‘£ ),<label>(6)</label></formula><p>where</p><formula xml:id="formula_9">W (ğ‘™ )</formula><p>ğ‘† âˆˆ R ğ» Ã—ğ¾ is a trainable weight matrix and ğ» is the hidden dimension of z ğ‘£ ) would result in non-differentiability, to enable back-propagation for training, we adopt the Gumbel-Softmax trick <ref type="bibr" target="#b22">[23]</ref> which specifically gives (for</p><formula xml:id="formula_10">ğ‘˜ = 1, â€¢ â€¢ â€¢ , ğ¾) ğ‘’ (ğ‘™ ) ğ‘£ğ‘˜ = exp ğœ‹ (ğ‘™ ) ğ‘£ğ‘˜ + ğ‘” ğ‘˜ /ğœ ğ‘˜ exp((ğœ‹ (ğ‘™ ) ğ‘£ğ‘˜ + ğ‘” ğ‘˜ )/ğœ) , ğ‘” ğ‘˜ âˆ¼ Gumbel(0, 1),<label>(7)</label></formula><p>where ğ‘” ğ‘˜ is a noise sampled from Gumbel distribution and ğœ controls the closeness of the result to discrete samples. Since in our case we do not require that the pseudo environment should be a categorical variable, we can still use moderate value for ğœ (e.g., ğœ = 1 which we found works smoothly in practice).</p><p>Mixture-of-Expert GNN Predictor ğ‘ ğœƒ ( Å¶ |ğº, ğ¸). The GNN predictor aims to encode input ego-graph G ğ‘£ conditioned on the inferred pseudo environment ğ‘’ ğ‘£ given by ğ‘ ğœ™ (ğ¸|ğº). To accommodate the layer-specific environment inference, we consider layer-wise updating controlled by ğ¾ mixture-of-expert (MoE) propagation units, instantiated by two models. The first model implements a  GCN-like MoE architecture with layer-wise updating rule:</p><formula xml:id="formula_11">z (ğ‘™+1) ğ‘¢ = ğœ ğ¾ âˆ‘ï¸ ğ‘˜=1 ğ‘’ (ğ‘™ ) ğ‘¢,ğ‘˜ âˆ‘ï¸ ğ‘£,ğ‘ ğ‘¢ğ‘£ =1 1 âˆš ğ‘‘ ğ‘¢ ğ‘‘ ğ‘£ W (ğ‘™,ğ‘˜ ) ğ· z (ğ‘™ ) ğ‘£ + W (ğ‘™,ğ‘˜ ) ğ‘† z (ğ‘™ ) ğ‘¢ ,<label>(8)</label></formula><p>where ğ‘‘ ğ‘¢ denotes the degree of node ğ‘¢, W (ğ‘™,ğ‘˜ ) ğ· âˆˆ R ğ» Ã—ğ» and W (ğ‘™,ğ‘˜ ) ğ‘† are trainable weight matrices for the ğ‘˜-th branch at the ğ‘™-th layer, and ğœ denotes activation function (e.g., ReLU). We call this model implementation CaNet-GCN that can be seen as a generalized implementation of Graph Convolution Networks <ref type="bibr" target="#b13">[14]</ref>, where e </p><p>(ğ‘™,ğ‘˜ ) ğ´ âˆˆ R ğ» Ã—ğ» and b (ğ‘™,ğ‘˜ ) âˆˆ R 2ğ» are trainable parameters. The model which we call CaNet-GAT can be seen as a generalized version of Graph Attention Networks <ref type="bibr" target="#b34">[35]</ref> with ğ¾ attention networks in each layer selected by e (ğ‘™ ) ğ‘¢ for attentive propagation. With ğ¿-layer propagation, the model (with instantiation (8) or ( <ref type="formula">9</ref>)) outputs z (ğ¿+1) ğ‘¢ that is further transformed by a fully-connected layer into node-wise prediction Å·ğ‘£ . The above models extend the notion of environments for each node ğ‘£ to a series of layer-specific vectors {e (ğ‘™ ) ğ‘£ } ğ¿ ğ‘™=1 that control the propagation network in each GNN layer. Such a design allows sufficient interactions between two modules: 1) the GNN's message passing helps to combine neighbored information in G ğ‘£ conditioned on layer-specific environment inference (as given by ( <ref type="formula" target="#formula_8">6</ref>)); 2) the inferred pseudo environments endow the GNN predictor with adaptive feature propagation w.r.t. different contexts (as defined by ( <ref type="formula" target="#formula_11">8</ref>) and ( <ref type="formula">9</ref>)). This guides each layer of the GNN predictor to extract stable relations from ego-graph features, particularly the complex structural patterns that are informative for prediction and insensitive to distribution shifts.</p><p>Optimization and Algorithm. For model training, we adopt gradient-based optimization for ğ‘ ğœ™ and ğ‘ ğœƒ with the objective <ref type="bibr" target="#b4">(5)</ref>. We assume ğ‘ 0 (ğ¸) for pseudo environments as a trivial uniform distribution (with equal probabilities for ğ¾ possible choices) and the loss function induced by ( <ref type="formula" target="#formula_6">5</ref>) can be written as </p><formula xml:id="formula_13">1 |V ğ‘¡ğ‘Ÿ | âˆ‘ï¸ ğ‘£ âˆˆ V ğ‘¡ğ‘Ÿ -y âŠ¤ ğ‘£ log Å·ğ‘£ + ğœ† ğ¿ ğ¿ âˆ‘ï¸ ğ‘™=1 ğ¾ âˆ‘ï¸ ğ‘˜=1 ğ‘’ (ğ‘™ ) ğ‘£ğ‘˜ log ğœ‹ (ğ‘™ ) ğ‘£ğ‘˜ + ğ‘’ (ğ‘™ ) ğ‘£ğ‘˜ log ğ¾ ,<label>(11)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>We apply our model to various datasets to evaluate its generalization capability. Overall, we aim to answer the questions: â€¢ (R1) How does CaNet perform compared to state-of-the-art models for handling distribution shifts on graphs? â€¢ (R2) Are the proposed components of CaNet effective for OOD generalization? â€¢ (R3) What is the sensitivity of CaNet w.r.t. the number of MoE branches (ğ¾) and the Gumbel-Softmax temperature (ğœ)? â€¢ (R4) Do different propagation branches learn different patterns?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experiment Setup</head><p>Datasets. We adopt six node property prediction datasets of different sizes and properties, including Cora, Citeseer, Pubmed, Twitch, Arxiv and Elliptic. Following <ref type="bibr" target="#b38">[39]</ref>, we consider different ways to construct an in-distribution (ID) portion and an out-of-distribution (OOD) portion for each dataset. For Cora, Citeseer and Pubmed <ref type="bibr" target="#b29">[30]</ref>, we keep the original node labels and synthetically create spurious node features to introduce distribution shifts between ID and OOD data. For Arxiv <ref type="bibr" target="#b12">[13]</ref>, we use publication years for data splits: papers published within 2005-2014 as ID data and after 2014 as OOD data.</p><p>For Twitch <ref type="bibr" target="#b26">[27]</ref>, we consider subgraph-level data splits: nodes in the subgraph DE, PT and as ID data, and nodes in ES, FR and EN as OOD data. For Elliptic <ref type="bibr" target="#b23">[24]</ref>, we use the first five graph snapshots as ID data and the remaining as OOD data. We summarize the dataset information in Table <ref type="table" target="#tab_2">2</ref>, with detailed descriptions and preprocessing presented in Appendix B.</p><p>Evaluation Protocol. For each dataset, the nodes of ID data are further randomly split into training/validation/testing with the ratio 50%/25%/25%. We use the training data for model training and the performance on validation data for model selection and early stopping. We test the model with the performance on both the testing data within the ID portion and the OOD data, respectively, where the latter quantifying the OOD generalization capabilities is our major focus. We follow the common practice, and use Accuracy as the metric for Cora, Citeseer, Pubmed and Arxiv, ROC-AUC for Twitch, and macro F1 score for Elliptic. We run the experiment for each case with five trails using different initializations and report the means and standard deviations for the metric.</p><p>Competitors. We basically compare with empirical risk minimization (ERM) that trains the model with standard supervised loss. We adopt GCN and GAT as the backbone to compare with CaNet-GCN and CaNet-GAT, respectively. Besides, we consider two sets of competitors that are agnostic to encoder backbones. The first line of models are designed for OOD generalization in general settings (where the instances, e.g., images, are assumed to be independent), including IRM <ref type="bibr" target="#b0">[1]</ref>, DeepCoral <ref type="bibr" target="#b31">[32]</ref>, DANN <ref type="bibr" target="#b7">[8]</ref>, GroupDRO <ref type="bibr" target="#b27">[28]</ref> and Mixup <ref type="bibr" target="#b47">[48]</ref>. Another line of works focus on learning with distribution shifts and out-of-distribution generalization on graphs, including the state-of-the-art models SR-GNN <ref type="bibr" target="#b49">[50]</ref> and EERM <ref type="bibr" target="#b38">[39]</ref>. For all the competitors, we use GCN and GAT as their encoder backbones, respectively. Details for implementation and competitors are deferred to Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Comparative Results (R1)</head><p>Distribution Shifts on Synthetic Data. We report the testing accuracy on Cora, Citeseer and Pubmed in Table <ref type="table" target="#tab_1">1</ref>. We found that using either GCN or GAT as the backbone, CaNet consistently outperforms the corresponding competitors by a significant margin on the OOD data across two types of distribution shifts and three datasets, and yield highly competitive results on the ID data. This demonstrates the effectiveness of our proposed model for OOD generalization with a guarantee of decent performance on the ID data. Apart from the relative improvement over the competitors, we observed that on Cora and Citeseer, the absolute performance of CaNet on the OOD data is very close to that on ID data. These results show that our model can effectively handle distribution shifts w.r.t. node features and graph structures.</p><p>Distribution Shifts on Temporal Graphs. In Table <ref type="table" target="#tab_3">3</ref> we report the testing accuracy on Arxiv where we further divide the out-of-distribution data into three-fold according to the publication years of papers: we use papers published within 2014-2016 as OOD 1, 2016-2018 as OOD 2, and 2018-2020 as OOD 3. As the time gap between training and testing data goes large, the distribution shift becomes more significant as observed by <ref type="bibr" target="#b38">[39]</ref>, and we found that the performance of all the models exhibits a more or less degradation. In contrast with other models, however, the performance drop of CaNet is much less severe, and our two model versions outperform the corresponding competitors by a large margin on the most difficult 2018-2020 testing set, with 14.1% and 27.4% improvements over the runner-up, respectively.</p><p>Distribution Shifts across Subgraphs. Table <ref type="table" target="#tab_3">3</ref> also presents the testing ROC-AUC on Twitch where we compare the performance on three OOD subgraphs separately (here OOD 1/2/3 refers to the subgraph ES/FR/EN). This dataset is challenging for generalization, since the nodes in different subgraphs are disconnected and the model needs to generalize to nodes in new unseen graphs collected with different context (i.e., regions). We found CaNet achieves overall superior performance over the competitors. This demonstrates the efficacy of our model for tackling OOD generalization across graphs in inductive learning.</p><p>Distribution Shifts across Dynamic Graph Snapshots. We report the macro F1 score of testing data on Elliptic in Fig. <ref type="figure" target="#fig_7">4</ref>. Since the out-of-distribution data contains snapshots of a long time span, we chronologically split these testing snapshots into eight subsets with an equal size. Overall, we found that CaNet can yield consistently better performance than other competitors, with average 12.16% improvement over the runner-ups. Notably, the performance gap between CaNet and the runner-ups that differ in each subset is significantly larger than the margin among other competitors. These results can be strong evidence that verifies the  superiority of our model for generalizing to previously unseen graph snapshots in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Ablation Studies (R2)</head><p>Ablation Study on Regularization Loss. We remove the regularization loss term in Eqn. 5 and only use the supervised loss for training. We compare the learning curves (training accuracy and testing accuracy on OOD-Struct) of our model and its simplified variant on Cora in Fig. <ref type="figure" target="#fig_8">5</ref>(a). We found that the regularization loss can indeed help to improve generalization to OOD testing data. In Fig. <ref type="figure" target="#fig_8">5</ref>(b), we further report the OOD testing accuracy on Arxiv after removing the regularization loss (w/o Reg Loss) and replacing the trivial prior distribution ğ‘ 0 (ğ¸) with a complex one (w/ VPrior Reg), i.e., using the generated results from random inputs to estimate the probability as is done by <ref type="bibr" target="#b33">[34]</ref>. The results again verify the effectiveness of the regularization loss for generalization, and further show that using trivial uniform distribution for ğ‘ 0 (ğ¸) works better than the complex one since it can push the model to equally attend on each pseudo environment candidate as an effective regularization for facilitating generalization.</p><p>Ablation Study on Environment Inference. We further replace the pseudo environment representation e (ğ‘™ ) ğ‘¢ for each layer by a single one e ğ‘¢ that is shared across all layers. In such a case, the model degrades to a simplified variant (called w/o Layer Env where the global pseudo environment estimation controls the propagation in each layer. Moreover, we further replace the trainable environment estimator with a non-parametric mean pooling over ğ¾ propagation branches at each layer (we call this variant w/o Para Env). Fig. <ref type="figure" target="#fig_8">5</ref>(b) presents the results of these two simplified variants on Arxiv where we can see clear performance drop in both cases, which validates the effectiveness of the layer-dependent environment inference that can provide better capacity to capture complex structural patterns useful for generalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Hyper-parameter Analysis (R3)</head><p>Impact of ğ¾. We study the impact of the number of pseudo environments ğ¾ and present the results in Fig. <ref type="figure" target="#fig_10">6</ref>(a) and 6(b) where we increase ğ¾ from 2 to 7 on Arxiv and Twitch, respectively. We found that the model performance on OOD data is overall not sensitive to the value of ğ¾ on Twitch. On Arxiv, different ğ¾'s have negligible impact on the performance on the testing set OOD 1 and affect the performance on the other two testing sets to a certain degree. The possible reason is that the distribution shifts of the latter are more significant than the former and the generalization would be more challenging. In such cases, smaller ğ¾ may not be expressive for learning informative pseudo environments and larger ğ¾ may lead to potential redundancy and over-fitting.</p><p>Impact of ğœ. We next investigate into the impact of the temperature coefficient ğœ in the Gumbel-Softmax. In Fig. <ref type="figure" target="#fig_10">6(c</ref>) and 6(d) we present the performance on different OOD sets of Arxiv and Twitch, respectively, w.r.t. the variation of ğœ. We found that a moderate value of ğœ (e.g., ğœ = 1) contributes to the best performance. Overall, smaller ğœ can yield stably good performance, while larger ğœ would cause performance drop. The reason could be that ğœ controls the sharpness of the sampled results, and excessively large ğœ tends to over-smooth the output, thereby causing samples to converge towards an uninformative uniform distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Visualization (R4)</head><p>We visualize the weights W (ğ‘™,ğ‘˜ ) ğ· of different branches (ğ¾ = 3) at the first and the last layers on Arxiv and Twitch in Fig. <ref type="figure">7</ref>, 8, 9 and   10 (located in the appendix), respectively. We found the weights of different branches exhibit clear differences, which suggests that the ğ¾ branches in the MoE architecture transform node embeddings in different manners and indeed learn distinct patterns from observed data. In fact, each branch corresponds to one pseudo environment, and this gives rise to an expressive model that helps to exploit predictive relations useful for generalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper, we focus on the generalization of graph neural networks w.r.t. node-level distribution shifts which require the model  to deal with out-of-distribution nodes from testing set. Our methodology is built on causal analysis for the learning behaviors of GNNs trained with MLE loss on observed data, on top of which we propose a new learning objective that is provably effective for capturing environment-insensitive predictive relations between ego-graph features and node labels. Extensive empirical results verify the effectiveness of the proposed model for handling various distribution shifts in graph-based node property prediction. Future Works. There exist some interesting aspects that can be explored in future works. One promising direction is to extend the model architecture from standard graph neural networks (that focus on local message passing) to graph Transformers (that consider global attention) <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b40">41]</ref>. Another potential direction is to apply the methodology to handle out-of-distribution generalization in domain-specific applications, like molecular discovery <ref type="bibr" target="#b42">[43]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A PROOFS AND DERIVATIONS</head><p>A.1 Derivation for Equation <ref type="formula" target="#formula_5">4</ref>We are to derive a tractable objective form for log ğ‘ ğœƒ ( Å¶ |ğ‘‘ğ‘œ (ğº)). Before the proof, we first introduce two fundamental rules of ğ‘‘ğ‘œcalculus <ref type="bibr" target="#b24">[25]</ref> which will be used as the building blocks later. Consider a causal directed acyclic graph A with three nodes: ğµ, ğ· and ğ¸. We denote A ğµ as the intervened causal graph by cutting off all arrows coming into ğµ, and A ğµ as the graph by cutting off all arrows going out from ğµ. For any interventional distribution compatible with A, the ğ‘‘ğ‘œ-calculus induces the following two fundamental rules.</p><p>i) Action/observation exchange:</p><formula xml:id="formula_14">ğ‘ƒ (ğ‘‘ |ğ‘‘ğ‘œ (ğ‘), ğ‘‘ğ‘œ (ğ‘’)) = ğ‘ƒ (ğ‘‘ |ğ‘‘ğ‘œ (ğ‘), ğ‘’), if (ğ· âŠ¥ âŠ¥ ğ¸|ğµ) A ğµğ¸ . ii) Insertion/deletion of actions: ğ‘ƒ (ğ‘‘ |ğ‘‘ğ‘œ (ğ‘), ğ‘‘ğ‘œ (ğ‘’)) = ğ‘ƒ (ğ‘‘ |ğ‘‘ğ‘œ (ğ‘)), if (ğ· âŠ¥ âŠ¥ ğ¸|ğµ) A</formula><p>ğµğ¸ . Back to our case where we have a causal graph with three variables ğ¸, ğº, Å¶ whose dependence relationships are shown in Fig. <ref type="figure">2(b)</ref>. We have</p><formula xml:id="formula_15">ğ‘ƒ ( Å¶ |ğ‘‘ğ‘œ (ğº)) = âˆ‘ï¸ ğ‘’ ğ‘ƒ ( Å¶ |ğ‘‘ğ‘œ (ğº), ğ¸ = ğ‘’)ğ‘ƒ (ğ¸ = ğ‘’ |ğ‘‘ğ‘œ (ğº)) = âˆ‘ï¸ ğ‘’ ğ‘ƒ ( Å¶ |ğº, ğ¸ = ğ‘’)ğ‘ƒ (ğ¸ = ğ‘’ |ğ‘‘ğ‘œ (ğº)) = âˆ‘ï¸ ğ‘’ ğ‘ƒ ( Å¶ |ğº, ğ¸ = ğ‘’)ğ‘ƒ (ğ¸ = ğ‘’),<label>(12)</label></formula><p>where the first step is given by the law of total probability, the second step is according to the first rule (since Å¶ âŠ¥ âŠ¥ ğº |ğ¸ in A ğº ), and the third step is due to the second rule (since we have ğ¸ âŠ¥ âŠ¥ ğº in A ğº ). The above derivation shows that ğ‘ ğœƒ ( Å¶ |ğ‘‘ğ‘œ (ğº)) = E ğ‘ 0 (ğ¸ ) [ğ‘ ğœƒ ( Å¶ |ğº, ğ¸)],</p><p>where ğ‘ 0 is the prior distribution of environments.</p><p>A.2 Derivation for Equation <ref type="formula" target="#formula_6">5</ref>We derive a variational lower bound for log ğ‘ ğœƒ ( Å¶ |ğ‘‘ğ‘œ (ğº)):  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B DATASET INFORMATION</head><p>â€¢ Cora, Citeseer and Pubmed are three commonly used citation networks <ref type="bibr" target="#b29">[30]</ref> for node property prediction. Since there is no explicit information that can be used to partition the nodes from distinct distributions, we consider a synthetic way that creates spurious node features for introducing distribution shifts. Specifically, for each dataset, we keep the original node labels in the dataset and synthetically create node features to generate graphs from multiple domains (with id ğ‘– = 1, 2, 3, 4, 5, 6) that involve distribution shifts. For creating domain-specific node features, we consider a randomly initialized GCN network: it takes the node label y ğ‘£ and domain id ğ‘– to generate spurious node features x(ğ‘–) ğ‘£ for the ğ‘–-th domain. Then we concatenate the generated features with the original one x ğ‘£ ] ğ‘£ âˆˆ V of the ğ‘–-th domain. Then we use the graphs with node features X (1) , X (2) and X (3) as ID data. The OOD data consists of three graphs with node features X (4) , X (5) and X (6) , respectively. These synthetic datasets can be used for testing the model when generalizing to OOD data with distribution shifts of node features.</p><p>â€¢ Twitch is a multi-graph dataset <ref type="bibr" target="#b26">[27]</ref> where each subgraph is a social network from a particular region. We use the nodes in different subgraphs for data splitting, since these subgraphs have different sizes, densities and node degrees <ref type="bibr" target="#b38">[39]</ref>. In specific, we use the nodes from subgraphs DE, PT, RU as in-distribution data and the nodes from subgraphs ES, FR, EN as out-of-distribution data.</p><p>â€¢ Arxiv is a temporal citation network <ref type="bibr" target="#b12">[13]</ref> where each node, a paper, has a time label indicating the publication year. The papers published in different years can be seen as samples from different distributions, and the distribution shift becomes more significant when the time gap between training and testing is enlarged. We use the papers published between 2005 and 2014 as in-distribution data, and the papers published after 2014 as out-of-distribution data.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An illustration based on a social network example for solving node property prediction by GNNs. (a) The task aims at predicting the (target) node's label ğ‘¦ ğ‘£ based on its ego-graph features G ğ‘£ , i.e., what the GNN model processes as input. (b) Two relations existing in social networks trigger different generalization effects for GNNs trained with observed data. (c) A causal graph describing the dependence among ego-graph features ğº, node label ğ‘Œ and the unobserved environment ğ¸. The latter is a latent confounder, the common cause for ğº and ğ‘Œ in the data generation. distribution shifts. The deficiency of GNNs for OOD generalization urges us to build a shift-robust model for graph representations.Nevertheless, the challenge is that distribution shifts on graphs are associated with the inter-connecting nature of data generation, which requires the model to accommodate the structural features among neighbored nodes for OOD generalization<ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b49">50]</ref>. Second, unlike image data<ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref> where the dataset often contains the context for each image instance that serves as environment labels indicating the source distribution of each instance, in the graph learning problem, the environment labels for nodes are often unavailable<ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b42">43]</ref>. This poses an obstacle for inferring useful environment information from observed data which can properly guide the model to learn generalizable patterns for prediction.In this paper, we adopt causal analysis from a bottom-up datagenerative perspective to investigate the learning behaviors of GNNs under distribution shifts. We reveal that the crux of GNNs' deficiency for OOD generalization lies in the latent environment confounder that leads to the confounding bias and over-fitting on the environment-sensitive relations. On top of this, we propose a provably effective approach, dubbed as Causal Intervention for Network Data (CaNet), for guiding GNNs to learn stable predictive relations from training data, without prior knowledge of environment labels. We introduce a new learning objective (derived from backdoor adjustment and variational inference) that collaboratively trains an environment estimator and a mixture-of-expert GNN predictor. The former aims to infer pseudo environment labels based on input ego-graphs to partition nodes in the graph into clusters from disparate distributions. The GNN predictor resorts to mixtureof-expert propagation networks dynamically selected by the pseudo environments. The new objective can alleviate the confounding bias in training data and helps to capture the environment-insensitive predictive relations that are generalizable across environments.To evaluate the approach, we conduct extensive experiments on six graph datasets with various types of distribution shifts. The results manifest that the proposed approach can significantly improve the generalization performance of different GNN models when distribution shifts occur and yield up to 27.4% performance improvements over the state-of-the-arts for graph out-of-distribution generalization. We summarize the contributions below.â€¢ We analyze the generalization ability of GNNs under distribution shifts from a causal data-generative perspective, and identify that GNNs trained with maximum likelihood estimation would capture the unstable relations from ego-graph features to labels due to the confounding bias of unobserved environments.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>ğ¾ as the inferred pseudo environment for node ğ‘£ at the ğ‘™-th layer of feature aggregation. Here e(ğ‘™ )ğ‘£ is a ğ¾-dimensional numerical vector and can be modeled by a categorical distribution M (ğ… (ğ‘™ ) ğ‘£ ) where e (ğ‘™ ) ğ‘£ is sampled. We model the probabilities ğ… (ğ‘™ ) ğ‘£ conditioned on the node representations {z (ğ‘™ ) ğ‘£ } at the current layer:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Illustration for the proposed model CaNet whose layer-wise computation entails a layer-specific environment estimator and a special feature propagation layer conditioned on the inferred pseudo environment.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>ğ‘¢dynamically selects convolution filters among ğ¾ candidates in each layer for propagation. In the second model, we further harness an attention network for each branch to model the adaptive pairwise influence between connected nodes:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>where ğœ† is a hyper-parameter weight. Alg. 1 in the appendix presents the model's feed-forward and training. The complexity of our model is O (ğ¿ğ¾ |E |), where |E | denotes the number of edges in the graph.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Macro F1 score on eight testing sets (by chronologically grouping the testing snapshots) of Elliptic.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Ablation studies. (a) Learning curves on Cora w/ and w/o regularization loss. (b) Ablation results on Arxiv.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Model performance with different ğ¾'s and ğœ's.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>1 ğ‘ 15 ) 1</head><label>1151</label><figDesc>log âˆ‘ï¸ ğ‘’ ğ‘ ğœƒ ( Å¶ |ğº, ğ¸ = ğ‘’)ğ‘ƒ (ğ¸ = ğ‘’) = log âˆ‘ï¸ ğ‘’ ğ‘ ğœƒ ( Å¶ |ğº, ğ¸ = ğ‘’)ğ‘ 0 (ğ¸ = ğ‘’) ğ‘ ğœ™ (ğ¸ = ğ‘’ |ğº) ğ‘ ğœ™ (ğ¸ = ğ‘’ |ğº) â‰¥ âˆ‘ï¸ ğ‘’ ğ‘ ğœ™ (ğ¸ = ğ‘’ |ğº) log ğ‘ ğœƒ ( Å¶ |ğº, ğ¸ = ğ‘’)ğ‘ 0 (ğ¸ = ğ‘’) ğœ™ (ğ¸ = ğ‘’ |ğº) = âˆ‘ï¸ ğ‘’ ğ‘ ğœ™ (ğ¸ = ğ‘’ |ğº) log ğ‘ ğœƒ ( Å¶ |ğº, ğ¸ = ğ‘’) -âˆ‘ï¸ ğ‘’ ğ‘ ğœ™ (ğ¸ = ğ‘’ |ğº) log ğ‘ ğœ™ (ğ¸ = ğ‘’ |ğº) ğ‘ 0 (ğ¸ = ğ‘’) , (14)where the penultimate step uses the Jensen's Inequality. The above derivation gives rise to log ğ‘ ğœƒ ( Å¶ |ğ‘‘ğ‘œ (ğº)) â‰¥ E ğ‘ ğœ™ (ğ¸ |ğº ) [log ğ‘ ğœƒ ( Å¶ |ğº, ğ¸)] -ğ¾ğ¿(ğ‘ ğœ™ (ğ¸|ğº)âˆ¥ğ‘ 0 (ğ¸)). (Algorithm Feed-forward and Training for CaNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>ğ‘£ = [x ğ‘£ âˆ¥ x(ğ‘–) ğ‘£ ] as the node features X (ğ‘– ) = [x (ğ‘– )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>The training of common GNNs is affected by the latent confounder ğ¸ that misguides the model to rely on environment-insensitive correlation between ğº and ğ‘Œ and leads to unsatisfactory OOD generalization. In contrast, our approach resorts to a new learning objective that essentially cuts off the dependence between ğ¸ and ğº.</figDesc><table><row><cell></cell><cell cols="2">latent training environment</cell><cell></cell></row><row><cell>ego-graph features</cell><cell>GNN encoder</cell><cell>node-level prediction</cell><cell>labels</cell></row><row><cell cols="4">(a) Causal graph (left) and data pipeline (right) for prior art</cell></row><row><cell cols="2">environment</cell><cell></cell><cell></cell></row><row><cell>ego-graph features</cell><cell>inference GNN</cell><cell>node-level prediction</cell><cell>labels</cell></row><row><cell></cell><cell>encoder</cell><cell></cell><cell></cell></row><row><cell cols="4">(b) Causal graph (left) and data pipeline (right) for CaNet</cell></row><row><cell cols="4">Figure 2: Structural Causal Models and data pipelines for</cell></row><row><cell cols="4">(a) standard GNNs' learning process and (b) our proposed</cell></row><row><cell>approach CaNet's.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Test (meanÂ±standard deviation) Accuracy (%) for citation networks on out-of-distribution (OOD) and in-distribution (ID) data. OOM indicates out-of-memory error on a GPU with 16GB memory.</figDesc><table><row><cell>Backbone</cell><cell>Method</cell><cell cols="2">Cora</cell><cell cols="2">Citeseer</cell><cell cols="2">Pubmed</cell></row><row><cell></cell><cell></cell><cell>OOD</cell><cell>ID</cell><cell>OOD</cell><cell>ID</cell><cell>OOD</cell><cell>ID</cell></row><row><cell></cell><cell>ERM</cell><cell>74.30 Â± 2.66</cell><cell>94.83 Â± 0.25</cell><cell>74.93 Â± 2.39</cell><cell>85.76 Â± 0.26</cell><cell>81.36 Â± 1.78</cell><cell>92.76 Â± 0.10</cell></row><row><cell></cell><cell>IRM</cell><cell>74.19 Â± 2.60</cell><cell>94.88 Â± 0.18</cell><cell>75.34 Â± 1.61</cell><cell>85.34 Â± 0.46</cell><cell>81.14 Â± 1.72</cell><cell>92.80 Â± 0.12</cell></row><row><cell></cell><cell>Coral</cell><cell>74.26 Â± 2.28</cell><cell>94.89 Â± 0.18</cell><cell>74.97 Â± 2.53</cell><cell>85.64 Â± 0.28</cell><cell>81.56 Â± 2.35</cell><cell>92.78 Â± 0.11</cell></row><row><cell></cell><cell>DANN</cell><cell>73.09 Â± 3.24</cell><cell>95.03 Â± 0.16</cell><cell>74.74 Â± 2.78</cell><cell>85.75 Â± 0.49</cell><cell>80.77 Â± 1.43</cell><cell>93.20 Â± 0.42</cell></row><row><cell>GCN</cell><cell>GroupDRO</cell><cell>74.25 Â± 2.61</cell><cell>94.87 Â± 0.25</cell><cell>75.02 Â± 2.05</cell><cell>85.33 Â± 0.36</cell><cell>81.07 Â± 1.89</cell><cell>92.76 Â± 0.08</cell></row><row><cell></cell><cell>Mixup</cell><cell>92.77 Â± 1.27</cell><cell>94.84 Â± 0.30</cell><cell>77.28 Â± 5.28</cell><cell>85.00 Â± 0.50</cell><cell>79.76 Â± 4.44</cell><cell>92.68 Â± 0.13</cell></row><row><cell></cell><cell>SRGNN</cell><cell>81.91 Â± 2.64</cell><cell>95.09 Â± 0.32</cell><cell>76.10 Â± 4.04</cell><cell>85.84 Â± 0.37</cell><cell>84.75 Â± 2.38</cell><cell>93.52 Â± 0.31</cell></row><row><cell></cell><cell>EERM</cell><cell>83.00 Â± 0.77</cell><cell>89.17 Â± 0.23</cell><cell>74.76 Â± 1.15</cell><cell>83.81 Â± 0.17</cell><cell>OOM</cell><cell>OOM</cell></row><row><cell></cell><cell>CaNet</cell><cell>96.12 Â± 1.04</cell><cell>97.87 Â± 0.23</cell><cell>94.57 Â± 1.92</cell><cell>95.18 Â± 0.17</cell><cell>88.82 Â± 2.30</cell><cell>97.37 Â± 0.11</cell></row><row><cell></cell><cell>ERM</cell><cell>91.10 Â± 2.26</cell><cell>95.57 Â± 0.40</cell><cell>82.60 Â± 0.51</cell><cell>89.02 Â± 0.32</cell><cell>84.80 Â± 1.47</cell><cell>93.98 Â± 0.24</cell></row><row><cell></cell><cell>IRM</cell><cell>91.63 Â± 1.27</cell><cell>95.72 Â± 0.31</cell><cell>82.73 Â± 0.37</cell><cell>89.11 Â± 0.36</cell><cell>84.95 Â± 1.06</cell><cell>93.89 Â± 0.26</cell></row><row><cell></cell><cell>Coral</cell><cell>91.82 Â± 1.30</cell><cell>95.74 Â± 0.39</cell><cell>82.44 Â± 0.58</cell><cell>89.05 Â± 0.37</cell><cell>85.07 Â± 0.95</cell><cell>94.05 Â± 0.23</cell></row><row><cell></cell><cell>DANN</cell><cell>92.40 Â± 2.05</cell><cell>95.66 Â± 0.28</cell><cell>82.49 Â± 0.67</cell><cell>89.02 Â± 0.31</cell><cell>83.94 Â± 0.84</cell><cell>93.46 Â± 0.31</cell></row><row><cell>GAT</cell><cell>GroupDRO</cell><cell>90.54 Â± 0.94</cell><cell>95.38 Â± 0.23</cell><cell>82.64 Â± 0.61</cell><cell>89.13 Â± 0.27</cell><cell>85.17 Â± 0.86</cell><cell>94.00 Â± 0.18</cell></row><row><cell></cell><cell>Mixup</cell><cell>92.94 Â± 1.21</cell><cell>94.66 Â± 0.10</cell><cell>82.77 Â± 0.30</cell><cell>89.05 Â± 0.05</cell><cell>81.58 Â± 0.65</cell><cell>92.79 Â± 0.18</cell></row><row><cell></cell><cell>SRGNN</cell><cell>91.77 Â± 2.43</cell><cell>95.36 Â± 0.24</cell><cell>82.72 Â± 0.35</cell><cell>89.10 Â± 0.15</cell><cell>83.40 Â± 0.67</cell><cell>93.21 Â± 0.29</cell></row><row><cell></cell><cell>EERM</cell><cell>91.80 Â± 0.73</cell><cell>91.37 Â± 0.30</cell><cell>74.07 Â± 0.75</cell><cell>83.53 Â± 0.56</cell><cell>OOM</cell><cell>OOM</cell></row><row><cell></cell><cell>CaNet</cell><cell>97.30 Â± 0.25</cell><cell>95.94 Â± 0.29</cell><cell>95.33 Â± 0.33</cell><cell>89.57 Â± 0.65</cell><cell>89.89 Â± 1.92</cell><cell>95.04 Â± 0.16</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Statistics for experimental datasets.</figDesc><table><row><cell>Datasets</cell><cell cols="4">#Nodes #Edges #Classes #Features</cell><cell>Shift Types</cell></row><row><cell>Cora</cell><cell>2708</cell><cell>5429</cell><cell>7</cell><cell>1433</cell><cell>spurious features</cell></row><row><cell>Citeseer</cell><cell>3327</cell><cell>4732</cell><cell>6</cell><cell>3703</cell><cell>spurious features</cell></row><row><cell>Pubmed</cell><cell>19717</cell><cell>44338</cell><cell>3</cell><cell>500</cell><cell>spurious features</cell></row><row><cell>Twitch</cell><cell>34120</cell><cell>892346</cell><cell>2</cell><cell>2545</cell><cell>disconnected subgraphs</cell></row><row><cell>Arxiv</cell><cell cols="2">169343 1166243</cell><cell>40</cell><cell>128</cell><cell>time attributes</cell></row><row><cell cols="2">Elliptic 203769</cell><cell>234355</cell><cell>2</cell><cell>165</cell><cell>dynamic snapshots</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Test (meanÂ±standard deviation) Accuracy (%) for Arxiv and ROC-AUC (%) for Twitch on different subsets of out-ofdistribution data. We use publication years and subgraphs for data splits on Arxiv and Twitch, respectively.</figDesc><table><row><cell></cell><cell></cell><cell>Backbone</cell><cell></cell><cell>Method</cell><cell></cell><cell>Arxiv</cell><cell></cell><cell></cell><cell></cell><cell>Twitch</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>OOD 1</cell><cell>OOD 2</cell><cell>OOD 3</cell><cell>ID</cell><cell>OOD 1</cell><cell>OOD 2</cell><cell>OOD 3</cell><cell>ID</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>ERM</cell><cell>56.33 Â± 0.17</cell><cell>53.53 Â± 0.44</cell><cell>45.83 Â± 0.47</cell><cell>59.94 Â± 0.45</cell><cell>66.07 Â± 0.14</cell><cell>52.62 Â± 0.01</cell><cell>63.15 Â± 0.08</cell><cell>75.40 Â± 0.01</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>IRM</cell><cell>55.92 Â± 0.24</cell><cell>53.25 Â± 0.49</cell><cell>45.66 Â± 0.83</cell><cell>60.28 Â± 0.23</cell><cell>66.95 Â± 0.27</cell><cell>52.53 Â± 0.02</cell><cell>62.91 Â± 0.08</cell><cell>74.88 Â± 0.02</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Coral</cell><cell>56.42 Â± 0.26</cell><cell>53.53 Â± 0.54</cell><cell>45.92 Â± 0.52</cell><cell>60.16 Â± 0.12</cell><cell>66.15 Â± 0.14</cell><cell>52.67 Â± 0.02</cell><cell>63.18 Â± 0.03</cell><cell>75.40 Â± 0.01</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>DANN</cell><cell>56.35 Â± 0.11</cell><cell>53.81 Â± 0.33</cell><cell>45.89 Â± 0.37</cell><cell>60.22 Â± 0.29</cell><cell>66.15 Â± 0.13</cell><cell>52.66 Â± 0.02</cell><cell>63.20 Â± 0.06</cell><cell>75.40 Â± 0.02</cell></row><row><cell></cell><cell></cell><cell>GCN</cell><cell cols="2">GroupDRO Mixup</cell><cell>56.52 Â± 0.27 56.67 Â± 0.46</cell><cell>53.40 Â± 0.29 54.02 Â± 0.51</cell><cell>45.76 Â± 0.59 46.09 Â± 0.58</cell><cell>60.35 Â± 0.27 60.09 Â± 0.15</cell><cell>66.82 Â± 0.26 65.76 Â± 0.30</cell><cell>52.69 Â± 0.02 52.78 Â± 0.04</cell><cell>62.95 Â± 0.11 63.15 Â± 0.08</cell><cell>75.03 Â± 0.01 75.47 Â± 0.06</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>SRGNN</cell><cell>56.79 Â± 1.35</cell><cell>54.33 Â± 1.78</cell><cell>46.24 Â± 1.90</cell><cell>60.02 Â± 0.52</cell><cell>65.83 Â± 0.45</cell><cell>52.47 Â± 0.06</cell><cell>62.74 Â± 0.23</cell><cell>75.75 Â± 0.09</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>EERM</cell><cell>OOM</cell><cell>OOM</cell><cell>OOM</cell><cell>OOM</cell><cell>67.50 Â± 0.74</cell><cell>51.88 Â± 0.07</cell><cell>62.56 Â± 0.02</cell><cell>74.85 Â± 0.05</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>CaNet</cell><cell>59.01 Â± 0.30</cell><cell>56.88 Â± 0.70</cell><cell>56.27 Â± 1.21</cell><cell>61.42 Â± 0.10</cell><cell>67.47 Â± 0.32</cell><cell>53.59 Â± 0.19</cell><cell>64.24 Â± 0.18</cell><cell>75.10 Â± 0.08</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>ERM</cell><cell>57.15 Â± 0.25</cell><cell>55.07 Â± 0.58</cell><cell>46.22 Â± 0.82</cell><cell>59.72 Â± 0.35</cell><cell>65.67 Â± 0.02</cell><cell>52.00 Â± 0.10</cell><cell>61.85 Â± 0.05</cell><cell>75.75 Â± 0.15</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>IRM</cell><cell>56.55 Â± 0.18</cell><cell>54.53 Â± 0.32</cell><cell>46.01 Â± 0.33</cell><cell>59.94 Â± 0.18</cell><cell>67.27 Â± 0.19</cell><cell>52.85 Â± 0.15</cell><cell>62.40 Â± 0.24</cell><cell>75.30 Â± 0.09</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Coral</cell><cell>57.40 Â± 0.51</cell><cell>55.14 Â± 0.71</cell><cell>46.71 Â± 0.61</cell><cell>60.59 Â± 0.30</cell><cell>67.12 Â± 0.03</cell><cell>52.61 Â± 0.01</cell><cell>63.41 Â± 0.01</cell><cell>75.20 Â± 0.01</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>DANN</cell><cell>57.23 Â± 0.18</cell><cell>55.13 Â± 0.46</cell><cell>46.61 Â± 0.57</cell><cell>59.72 Â± 0.14</cell><cell>66.59 Â± 0.38</cell><cell>52.88 Â± 0.12</cell><cell>62.47 Â± 0.32</cell><cell>75.82 Â± 0.27</cell></row><row><cell></cell><cell></cell><cell>GAT</cell><cell cols="2">GroupDRO Mixup</cell><cell>56.69 Â± 0.27 57.17 Â± 0.33</cell><cell>54.51 Â± 0.49 55.33 Â± 0.37</cell><cell>46.00 Â± 0.59 47.17 Â± 0.84</cell><cell>60.03 Â± 0.32 59.84 Â± 0.50</cell><cell>67.41 Â± 0.04 65.58 Â± 0.13</cell><cell>52.99 Â± 0.08 52.04 Â± 0.04</cell><cell>62.29 Â± 0.03 61.75 Â± 0.13</cell><cell>75.74 Â± 0.02 75.72 Â± 0.07</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>SRGNN</cell><cell>56.69 Â± 0.38</cell><cell>55.01 Â± 0.55</cell><cell>46.88 Â± 0.58</cell><cell>59.39 Â± 0.17</cell><cell>66.17 Â± 0.03</cell><cell>52.84 Â± 0.04</cell><cell>62.07 Â± 0.04</cell><cell>75.45 Â± 0.03</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>EERM</cell><cell>OOM</cell><cell>OOM</cell><cell>OOM</cell><cell>OOM</cell><cell>66.80 Â± 0.46</cell><cell>52.39 Â± 0.20</cell><cell>62.07 Â± 0.68</cell><cell>75.19 Â± 0.50</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>CaNet</cell><cell>60.44 Â± 0.27</cell><cell>58.54 Â± 0.72</cell><cell>59.61 Â± 0.28</cell><cell>62.91 Â± 0.35</cell><cell>68.08 Â± 0.19</cell><cell>53.49 Â± 0.14</cell><cell>63.76 Â± 0.17</cell><cell>76.14 Â± 0.07</cell></row><row><cell></cell><cell></cell><cell>CaNet</cell><cell>ERM</cell><cell>EERM</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>GroupDRO</cell><cell>IRM</cell><cell>DANN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>F1 Score</cell><cell>60 80</cell><cell>Mixup</cell><cell>Coral</cell><cell>SRGNN</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>40</cell><cell cols="3">OOD Test Subsets 1 2 3 4 5 6 7 8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>1 :</head><label>1</label><figDesc>Input: Input node features X = [x ğ‘£ ] ğ‘£ âˆˆV , adjacency matrix A. Initialized GNN predictor parameter ğœƒ , initialized environment estimator parameter ğœ™. ğ›¼ 1 , learning rate for ğœ™. ğ›¼ 2 , learning rate for ğœƒ . ğ›½ 1 = 0.9, ğ›½ 2 = 0.999, Adam parameters. 2: while not converged do ğ‘£ = ğœ™ ğ‘–ğ‘› (x ğ‘£ ); Update the environment estimator ğœ™ â† Adam( L, ğœ™, ğ›¼ 1 , ğ›½ 1 , ğ›½ 2 ) Update the GNN predictor ğœƒ â† Adam( L, ğœƒ, ğ›¼ 2 , ğ›½ 1 , ğ›½ 2 ) 18: end while 19: Output: Trained model parameters ğœƒ * , ğœ™ * .</figDesc><table><row><cell>3:</cell><cell cols="2">Compute initial node embeddings z</cell><cell cols="2">(1)</cell></row><row><cell>4: 5:</cell><cell cols="4">for ğ‘™ = 1 to ğ¿ do Estimate pseudo environment distribution ğ…</cell><cell>(ğ‘™ ) ğ‘£ via (6) for ğ‘£ âˆˆ V;</cell></row><row><cell>6:</cell><cell cols="4">Obtain inferred pseudo environment e</cell><cell>(ğ‘™ ) ğ‘£ through (7) for ğ‘£ âˆˆ V;</cell></row><row><cell>7:</cell><cell cols="4">if use the propagation of CaNet-GCN then</cell></row><row><cell>8:</cell><cell>Update node embeddings z</cell><cell cols="2">(ğ‘™ +1) ğ‘£</cell><cell>with (8);</cell></row><row><cell>9:</cell><cell>end if</cell><cell></cell><cell></cell></row><row><cell>10:</cell><cell cols="4">if use the propagation of CaNet-GAT then</cell></row><row><cell>11:</cell><cell>Update node embeddings z</cell><cell cols="2">(ğ‘™ +1) ğ‘£</cell><cell>with (9);</cell></row><row><cell>12:</cell><cell>end if</cell><cell></cell><cell></cell></row><row><cell>13: 14:</cell><cell cols="4">end for Compute predicted labels Å·ğ‘£ = ğœ™ ğ‘œğ‘¢ğ‘¡ (z</cell><cell>(ğ¿+1) ğ‘£</cell><cell>);</cell></row><row><cell>15:</cell><cell>Compute loss L based on (11);</cell><cell></cell><cell></cell></row><row><cell>16:</cell><cell></cell><cell></cell><cell></cell></row><row><cell>17:</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>In OOD generalization literature[5, 9,  </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p><ref type="bibr" target="#b25">26]</ref>, environment and domain are interchangeably used and refer to an indicator of which distribution a sample is generated from.</p></note>
		</body>
		<back>

			<div type="funding">
<div><p>The work was partly supported by <rs type="funder">NSFC</rs> (<rs type="grantNumber">92370201</rs>, <rs type="grantNumber">62222607</rs>) and <rs type="funder">SJTU Trans-med Awards Research (STAR)</rs> (<rs type="grantNumber">20210106</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_SaMbCXT">
					<idno type="grant-number">92370201</idno>
				</org>
				<org type="funding" xml:id="_UMGa5Z4">
					<idno type="grant-number">62222607</idno>
				</org>
				<org type="funding" xml:id="_zT7w2vQ">
					<idno type="grant-number">20210106</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>â€¢ Elliptic is a dynamic graph for bitcoin transaction records <ref type="bibr" target="#b23">[24]</ref> that comprise a sequence of graph snapshots where each snapshot is generated at one time. We can naturally treat nodes in different snapshots as samples from different distributions since the underlying mechanism behind transactions is heavily dependent on the time and market. We use the first five graph snapshots as indistribution data and the remaining snapshots as out-of-distribution data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C IMPLEMENTATION DETAILS</head><p>Our implementation is based on PyTorch 1.13.0 and PyTorch Geometric 2.1.0. All of our experiments are run on a Tesla V100 with 16 GB memory. We adopt Adam with weight decay for training and set a fixed training budget with 500 epochs. The testing performance achieved by the epoch where the model gives the best performance on validation data is reported.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Hyper-parameter Settings</head><p>We instantiate ğœ™ ğ‘–ğ‘› and ğœ™ ğ‘œğ‘¢ğ‘¡ as a fully-connected layer. The detailed architecture of CaNet is decribed as follows. The model architecture consists of the following modules in sequential order: â€¢ A fully-connected layer with hidden size ğ· Ã— ğ» (transforming ğ·-dim input raw features into ğ» -dim embeddings).</p><p>â€¢ ğ¿-layer GNN network with hidden size ğ» Ã—ğ» (each layer contains ğ¾ branches that have independent parameterization), based on the two instantiations in Sec. 4.3.</p><p>â€¢ A fully-connected layer with hidden size ğ» Ã— ğ¶ (mapping ğ» -dim embeddings to ğ¶ classes).</p><p>In each layer, we use ReLU activation, dropout and residual link. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Competitors</head><p>For competitors, we use their public implementation. We also use the validation set to tune the hyper-parameters (GNN layers, hidden dimension, dropout ratio and learning rate) using the same searching space as ours. For other hyper-parameters that differ in each model, we refer to their default settings reported by the original paper. We present more information for these competitors below.</p><p>The first line of competitors is designed for handling out-ofdistribution generalization in the general setting, e.g., image data, where the samples are assumed to be independent. The competitors include IRM <ref type="bibr" target="#b0">[1]</ref>, DeepCoral <ref type="bibr" target="#b31">[32]</ref>, DANN <ref type="bibr" target="#b7">[8]</ref>, GroupDRO <ref type="bibr" target="#b27">[28]</ref> and Mixup <ref type="bibr" target="#b47">[48]</ref>. These approaches resort to different strategies to improve the generalization of the model. Mixup aims to augment the training data by interpolation of the observed samples, while other four methods propose robust learning algorithms that can guide the model to learn stable predictive relations against distribution shifts. For accommodating the structural information and data interdependence, We use GCN and GAT as the encoder backbone for computing node representation and predicting node labels.</p><p>Another line of works concentrates on out-of-distribution generalization with graph data, where the observed samples (i.e., nodes) are inter-connected, including two recently proposed models SR-GNN <ref type="bibr" target="#b49">[50]</ref> and EERM <ref type="bibr" target="#b38">[39]</ref>. SR-GNN proposes a regularization loss for enhancing the generalization of the model to new data. EERM leverages the invariance principle to develop an adversarial training approach for environment exploration. These models are agnostic to encoder backbones. For fair comparison, we use GCN and GAT as their encoder backbones.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">LÃ©on</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.02893</idno>
		<title level="m">Invariant risk minimization</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Artem Babenko, and Liudmila Prokhorenkova</title>
		<author>
			<persName><forename type="first">Gleb</forename><surname>Bazhenov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denis</forename><surname>Kuznedelev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrey</forename><surname>Malinin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.13875</idno>
	</analytic>
	<monogr>
		<title level="m">Evaluating Robustness and Uncertainty of Graph Models Under Structural Distributional Shifts</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Size-Invariant Graph Representations for Graph Classification Extrapolations</title>
		<author>
			<persName><forename type="first">Beatrice</forename><surname>Bevilacqua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangze</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruno</forename><surname>Ribeiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="837" to="851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">SizeShiftReg: a Regularization Method for Improving Size-Generalization in Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Davide</forename><surname>Buffelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>LiÃ³</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Vandin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>In NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Invariance, causality and robustness</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>BÃ¼hlmann</surname></persName>
		</author>
		<idno>CoRR abs/1812.08233</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Simple and Deep Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Ming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhewei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zengfeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bolin</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaliang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1725" to="1735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Shaohua Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bai</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.10657</idno>
		<title level="m">Generalizing Graph Neural Networks on Out-Of-Distribution Graphs</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Domain-Adversarial Training of Neural Networks</title>
		<author>
			<persName><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evgeniya</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hana</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">FranÃ§ois</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><forename type="middle">S</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">35</biblScope>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Domain Adaptation with Conditional Transferable Components</title>
		<author>
			<persName><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clark</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>SchÃ¶lkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2839" to="2848" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A new model for learning in graph domains</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Monfardini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="729" to="734" />
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Attention Based Spatial-Temporal Graph Convolutional Networks for Traffic Flow Forecasting</title>
		<author>
			<persName><forename type="first">Shengnan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youfang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huaiyu</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="922" to="929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Inductive Representation Learning on Large Graphs</title>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1024" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Open Graph Benchmark: Datasets for Machine Learning on Graphs</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS 2020</title>
		<editor>
			<persName><forename type="first">Marc</forename><forename type="middle">'</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Aurelio</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Maria-Florina</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Hsuan-Tien</forename><surname>Lin</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Predict then Propagate: Graph Neural Networks meet Personalized PageRank</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Klicpera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>GÃ¼nnemann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">WILDS: A Benchmark of inthe-Wild Distribution Shifts</title>
		<author>
			<persName><forename type="first">Pang</forename><surname>Wei Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiori</forename><surname>Sagawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henrik</forename><surname>Marklund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sang</forename><surname>Michael Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marvin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akshay</forename><surname>Balsubramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michihiro</forename><surname>Yasunaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">Lanas</forename><surname>Phillips</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irena</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tony</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Etienne</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Stavness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Berton</forename><surname>Earnshaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Imran</forename><surname>Haque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sara</forename><forename type="middle">M</forename><surname>Beery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anshul</forename><surname>Kundaje</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emma</forename><surname>Pierson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5637" to="5664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Out-ofdistribution generalization via risk extrapolation (rex)</title>
		<author>
			<persName><forename type="first">David</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joern-Henrik</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amy</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Binas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinghuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Remi</forename><surname>Le Priol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Ood-gnn: Out-ofdistribution generalized graph neural network</title>
		<author>
			<persName><forename type="first">Haoyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TKDE</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">GraphDE: A Generative Framework for Debiased Learning and Out-of-Distribution Detection on Graphs</title>
		<author>
			<persName><forename type="first">Zenan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qitian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Structural Re-weighting Improves Graph Domain Adaptation</title>
		<author>
			<persName><forename type="first">Shikun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianchun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongbin</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nhan</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pan</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">FLOOD: A Flexible Invariant Learning Framework for Outof-Distribution Generalization on Graphs</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunshan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuan</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="1548" to="1558" />
		</imprint>
	</monogr>
	<note>Tat-Seng Chua, and Qing He</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Subgroup generalization and fairness of graph neural networks</title>
		<author>
			<persName><forename type="first">Jiaqi</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junwei</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1048" to="1061" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables</title>
		<author>
			<persName><forename type="first">Chris</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andriy</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">EvolveGCN: Evolving Graph Convolutional Networks for Dynamic Graphs</title>
		<author>
			<persName><forename type="first">Aldo</forename><surname>Pareja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giacomo</forename><surname>Domeniconi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengfei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toyotaro</forename><surname>Suzumura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hiroki</forename><surname>Kanezashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Kaler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><forename type="middle">E</forename><surname>Schardl</surname></persName>
		</author>
		<author>
			<persName><surname>Leiserson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5363" to="5370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Causal inference in statistics: A primer</title>
		<author>
			<persName><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madelyn</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><forename type="middle">P</forename><surname>Jewell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Invariant Models for Causal Transfer Learning</title>
		<author>
			<persName><forename type="first">Mateo</forename><surname>Rojas-Carulla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>SchÃ¶lkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Peters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">34</biblScope>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Benedek</forename><surname>Rozemberczki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rik</forename><surname>Sarkar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.03091[cs.SI]</idno>
		<title level="m">Twitch Gamers: a Dataset for Evaluating Proximity Preserving and Structural Role-based Node Embeddings</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Distributionally robust neural networks for group shifts: On the importance of regularization for worst-case generalization</title>
		<author>
			<persName><forename type="first">Shiori</forename><surname>Sagawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pang</forename><surname>Wei Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatsunori</forename><forename type="middle">B</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.08731</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The Graph Neural Network Model</title>
		<author>
			<persName><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ah</forename><surname>Chung Tsoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName><forename type="first">Prithviraj</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Galileo</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mustafa</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lise</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tina</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="93" to="93" />
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Unleashing the power of graph data augmentation on covariate distribution shift</title>
		<author>
			<persName><forename type="first">Yongduo</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qitian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiancan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Longfei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep coral: Correlation alignment for deep domain adaptation</title>
		<author>
			<persName><forename type="first">Baochen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2016 Workshops</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="443" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Social influence analysis in large-scale networks</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimeng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zi</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="807" to="816" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">VAE with a VampPrior</title>
		<author>
			<persName><forename type="first">Jakub</forename><surname>Tomczak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>In AISTATS</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Graph Attention Networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>LiÃ²</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>In ICLR</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Simplifying Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Felix</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amauri</forename><forename type="middle">H</forename><surname>Souza</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Fifty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kilian</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<editor>ICML, Kamalika Chaudhuri and Ruslan Salakhutdinov</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6861" to="6871" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Energy-based Out-of-Distribution Detection for Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Qitian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenxiao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Dual Graph Attention Networks for Deep Latent Representation of Multifaceted Social Effects in Recommender Systems</title>
		<author>
			<persName><forename type="first">Qitian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hengrui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaofeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guihai</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The World Wide Web Conference</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2091" to="2102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Handling Distribution Shifts on Graphs: An Invariance Perspective</title>
		<author>
			<persName><forename type="first">Qitian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hengrui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wipf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Node-Former: A Scalable Graph Structure Learning Transformer for Node Classification</title>
		<author>
			<persName><forename type="first">Qitian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wentao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zenan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">P</forename><surname>Wipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">SGFormer: Simplifying and Empowering Transformers for Large-Graph Representations</title>
		<author>
			<persName><forename type="first">Qitian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wentao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenxiao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hengrui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haitian</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yatao</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Graph Neural Networks are Inherently Good Generalizers: Insights by Bridging GNNs and MLPs</title>
		<author>
			<persName><forename type="first">Chenxiao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qitian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiahua</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning Substructure Invariance for Out-of-Distribution Molecular Representations</title>
		<author>
			<persName><forename type="first">Nianzu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaipeng</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qitian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaosong</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">MoleRec: Combinatorial Drug Recommendation with Substructure-Aware Molecular Representation Learning</title>
		<author>
			<persName><forename type="first">Nianzu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaipeng</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qitian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Web Conference</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="4075" to="4085" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">From Local Structures to Size Generalization in Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Gilad</forename><surname>Yehudai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Fetaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eli</forename><forename type="middle">A</forename><surname>Meirom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gal</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haggai</forename><surname>Maron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="11975" to="11986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Fast and Accurate Anomaly Detection in Dynamic Graphs with a Two-Pronged Approach</title>
		<author>
			<persName><forename type="first">Minji</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Hooi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kijung</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Faloutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGKDD International Conference on Knowledge Discovery</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="647" to="657" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Mind the Label Shift of Augmentationbased Graph OOD Generalization</title>
		<author>
			<persName><forename type="first">Junchi</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ran</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">Hongyi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moustapha</forename><surname>Cisse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.09412</idno>
		<title level="m">mixup: Beyond empirical risk minimization</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning High-Order Graph Convolutional Networks via Adaptive Layerwise Aggregation Combination</title>
		<author>
			<persName><forename type="first">Tianqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qitian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="5144" to="5155" />
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Shift-robust gnns: Overcoming the limitations of localized graph training data</title>
		<author>
			<persName><forename type="first">Qi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Ponomareva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="27965" to="27977" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Predicting multicellular function through multi-layer tissue networks</title>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="DOI">10.1093/bioinformatics/btx252</idno>
		<ptr target="https://doi.org/10.1093/bioinformatics/btx252" />
	</analytic>
	<monogr>
		<title level="j">Bioinform</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="190" to="198" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
