<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Catch Causal Signals from Edges for Label Imbalance in Graph Classification</title>
				<funder>
					<orgName type="full">SF Tech</orgName>
				</funder>
				<funder>
					<orgName type="full">Applied Basic Research Foundation</orgName>
				</funder>
				<funder ref="#_7YVuE4m">
					<orgName type="full">Research Grants Council</orgName>
					<orgName type="abbreviated">RGC</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2025-01-07">7 Jan 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Fengrui</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Hong Kong Baptist University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yujia</forename><surname>Yin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Hong Kong Baptist University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hongzong</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">BayVax Biotech Limited</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Yifan</forename><surname>Chen</surname></persName>
							<email>〈yifanc@hkbu.edu.hk〉</email>
							<affiliation key="aff0">
								<orgName type="institution">Hong Kong Baptist University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">†</forename><surname>Tianyi Qu</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">SF Tech</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Catch Causal Signals from Edges for Label Imbalance in Graph Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-01-07">7 Jan 2025</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2501.01707v2[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Causal discovery</term>
					<term>causal intervention</term>
					<term>graph label imbalance</term>
					<term>graph neural networks</term>
					<term>Node Attention Edge Attention Graph Splitting Trival Attended graph Causal Attended graph Edge-featured Graph Encoder(EGAT) Edge-featured Graph Encoder(EGAT) Random addition Readout Readout Readout Cross Entopy Loss Cross Entropy Loss KL divergence with uniform distribution Trival graph representation Causal graph representation</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Despite significant advancements in causal research on graphs and its application to cracking label imbalance, the role of edge features in detecting the causal effects within graphs has been largely overlooked, leaving existing methods with untapped potential for further performance gains. In this paper, we enhance the causal attention mechanism through effectively leveraging edge information to disentangle the causal subgraph from the original graph, as well as further utilizing edge features to reshape graph representations. Capturing more comprehensive causal signals, our design leads to improved performance on graph classification tasks with label imbalance issues. We evaluate our approach on real-word datasets PTC, Tox21, and ogbg-molhiv, observing improvements over baselines. Overall, we highlight the importance of edge features in graph causal detection and provide a promising direction for addressing label imbalance challenges in graph-level tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Graph Neural Networks (GNNs) have emerged as a powerful framework for learning from graph-structured data, deeply grounded in spectral graph theory and neural networks <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>. They have been widely applied to graph classification and regression tasks, such as molecular property prediction <ref type="bibr" target="#b2">[3]</ref>- <ref type="bibr" target="#b4">[5]</ref> and protein structure analysis <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>. Their ability to capture intricate interactions between entities has proven highly effective. However, GNNs face considerable challenges in realworld scenarios involving distribution shifts, such as label imbalance <ref type="bibr" target="#b7">[8]</ref>- <ref type="bibr" target="#b10">[11]</ref>.</p><p>To address these challenges, researchers have extended GNNs to out-of-distribution (OOD) settings <ref type="bibr" target="#b11">[12]</ref>- <ref type="bibr" target="#b14">[15]</ref>. One common approach involves environment-based methods <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, which depend on prior knowledge of environments or require environment detection algorithms. Structured causal models offer an interpretable and flexible alternative for handling OOD data. For instance, researchers have combined environment adjustment with causal detection <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, while others focus solely on causal modeling, introducing frameworks like causal inference <ref type="bibr" target="#b19">[20]</ref>, mutual information for identifying causal components <ref type="bibr" target="#b20">[21]</ref>, and re-weighting techniques to mitigate confounding effects <ref type="bibr" target="#b21">[22]</ref>. Recent advances, such as causal attention mechanisms and structured causal models, * Equal contribution.</p><p>have further improved the understanding of causal relationships in graph-structured data. These mechanisms identify causal patterns by separating true causal features from shortcut features <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>, enabling better generalization in OOD scenarios.</p><p>Despite these advancements, many causal models overlook edge features, which are essential for capturing relational structures in graphs. Studies have demonstrated the potential of edge features in enhancing GNNs, including integrating edge information into GCNs <ref type="bibr" target="#b24">[25]</ref>, extending GATs to handle edge-featured graphs <ref type="bibr" target="#b25">[26]</ref>- <ref type="bibr" target="#b27">[28]</ref>, and incorporating edge encoding <ref type="bibr" target="#b28">[29]</ref>.</p><p>In this paper, we enhance causal attention mechanisms by incorporating edge features. Specifically, we first reproduce the graph attention module with the capability to take edge features, and then further strengthen the design through learning from edge feature evolution (dubbed as EGATv1, EGATv2, respectively; c.f. Section III-A). These modules are integrated into the causal learning mechanism to detect causal subgraphs more effectively (corresponding to the "attention estimation" step, c.f. Section II) and accordingly strengthen the output graph representation. In summary,</p><p>• We recognize that causal models on graphs usually overlook the causal signals from edges. • We introduce two edge-enhanced modules to tweak the causal attention mechanism, which leads to a better detection of causal structures in graphs .</p><p>• Extensive experiments are performed on label-imbalance graph classification tasks, which verifies the efficacy of incorporating edge features into causal models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. PRELIMINARIES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Task formulation</head><p>For a general graph G = (V, E) composed of the node set V and the edge collection E, we represent it as a triplet of matrices (A, X, E), where:</p><p>• A ∈ R |V|×|V| is the adjacency matrix indicating the connectivity between the nodes. • X ∈ R |V|×dv is the node feature matrix, whose rows X i 's correspond to d v -dimensional feature vectors for each node. • E ∈ R |E|×de is the edge feature matrix, where for (i, j) ∈ E, e ij ∈ R de denotes the d e -dimensional feature vector for the edge connecting nodes i and j in the graph. The task is to learn a function f : G → y, where y ∈ Y is one of the possible graph labels. Due to distributional shifts, the distribution of a graph G in the testing set usually differs from the one in training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Causal discovery and causal attention mechanisms</head><p>Causal discovery aims to identify the causal components in complex data, such as graphs, by distinguishing causality from spurious correlations. Attention mechanisms have proven effective in this domain by selectively focusing on relevant nodes and edges, enabling more accurate inference.</p><p>A typical framework for causal attention mechanism <ref type="bibr" target="#b23">[24]</ref> is composed of two steps. ❶ Attention estimation: the framework will first dynamically adjust the attention score to locate the most relevant nodes/edges and thus differentiate between causal and spurious correlations; as an illustration, this step mainly corresponds to the top yellow "EGAT" box in Figure <ref type="figure" target="#fig_0">1</ref>. ❷ Graph representation: attaining causal and non-causal subgraphs, the framework will then further apply attention modules to extract features; this step is indicated by the bottom two yellow "EGAT" box in Figure <ref type="figure" target="#fig_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. METHODS</head><p>In this work, we aim to address the challenge of labelimbalance graph classification by incorporating edge features into a causal attention mechanism. Our proposed framework is illustrated in Figure <ref type="figure" target="#fig_0">1</ref>, highlighting the process from edgefeatured graph encoding/splitting to the training loss.</p><p>The process starts by passing the input graph through an edge-featured graph encoder to extract node and edge causality. Edge and node attention scores are estimated (step ❶, c.f. Section III-A) to identify significant components. As described in Section III-B, these scores split the graph into a causal subgraph with key elements and a trivial subgraph with less relevant components. Both subgraphs are processed by separate encoders (step ❷). The training loss includes three parts: cross-entropy loss for the causal subgraph, KL divergence for uniformity of the trivial subgraph, and cross-entropy loss after randomly pairing causal and trivial subgraphs within the same batch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Incorporating edge features into attention</head><p>Regarding the integration of edge features into the attention mechanism, it is intuitive to use the Edge-Featured Graph Attention Network (EGAT) <ref type="bibr" target="#b26">[27]</ref>. In this module, the causality attention score α i for each node i and the causality attention score α ij for each edge (i, j) ∈ E (i.e., node j must be within N (i), the neighborhood of node i) are calculated considering the information from the node and the edge. (We note this module, referred to as EGATv1, will shortly be enhanced and ultimately not employed in step ❶; however, this module will be evaluated in step ❷ graph representation.)</p><p>The node features X i , X j and the edge feature e ij are first transformed through linear projections; the concatenation of the transformed features can be expressed as:</p><formula xml:id="formula_0">X ′ i ∥ X ′ j ∥ e ′ ij ,</formula><p>where</p><formula xml:id="formula_1">X ′ i = W (i) x X i , X ′ j = W (j)</formula><p>x X j , and e ′ ij = W e e ij . The node embedding from the EGAT is computed as</p><formula xml:id="formula_2">X out i = MLP   j∈N (i) a ij X ′ j  <label>(1)</label></formula><p>where the attention score a ij involves the comprehensive information from</p><formula xml:id="formula_3">X ′ i ∥ X ′ j ∥ e ′ ij , computed as a ij = exp LeakyReLU a T X ′ i ∥ X ′ j ∥ e ′ ij k∈N (i) exp (LeakyReLU (a T [X ′ i ∥ X ′ k ∥ e ′ ik ]))</formula><p>, where a indicates learnable parameters. Then we adopt two MLPs to estimate the node causality attention scores α i and the edge causality attention scores α ij respectively based on</p><formula xml:id="formula_4">X out i and e ′ ij : α i , 1 -α i = σ(MLP node (X out i )) (2) α ij , 1 -α ij = σ(MLP edge (e ′ ij ))<label>(3)</label></formula><p>where σ(•) is the softmax function.</p><p>An enhanced EGAT alternative: we note the edge causality attention scores in Equation (3) are computed using edge information e ′ ij , which is isolated from the neighborhood information. To better leverage edge features, we use another EGAT design <ref type="bibr" target="#b29">[30]</ref>, and we call it EGATv2 to distinguish it from the design above. This structure updates both edge and node features, allowing for a more effective capture of the dynamic relationships between nodes; in EGATv2, given the feature</p><formula xml:id="formula_5">X ′ i ∥ X ′ j ∥ e ′ ij</formula><p>, the edge embedding is first calculated as (W out e indicates learnable parameters):</p><formula xml:id="formula_6">e out ij = LeakyReLU W out e X ′ i ∥ e ′ ij ∥ X ′ j .</formula><p>The node embedding is next generated based on e out ij :</p><formula xml:id="formula_7">X out i = MLP   j∈N (i) a ij e out ij X ′ j   ,<label>(4)</label></formula><p>where a ij is now a function of e out ij and reads</p><formula xml:id="formula_8">a ij = exp a T e out ij k∈N (i) exp (a T e out ik )</formula><p>.</p><p>We then follow a process akin to EGATv1, utilizing two MLPs to estimate the node causality attention scores and edge causality attention scores, with X out i , e out ij as inputs this time:</p><formula xml:id="formula_9">α i , 1 -α i = σ(MLP node (X out i )) α ij , 1 -α ij = σ(MLP edge (e out ij )</formula><p>) where σ(•) is the softmax function. Compared to EGATv1, the edge causality attention scores are now estimated by the more comprehensive edge embedding e out ij . We term this design (with EGATv2) for step ❶ attention estimation as edgeenhanced causal attention learning (ECAL) along this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Graph causal representation with edge information</head><p>The core idea within our approach is to enhance causal discovery by incorporating edge features to improve OOD generalization in graph-level classification. In the following, we propose the complete design to create better representations of causal structures by leveraging both node and edge features.</p><p>1) Causal subgraph splitting: With the causality attention scores estimated in Section III-A, we are now able to generate two subgraphs, the causal subgraph G c and the trivial subgraph G t , from the original graph G:</p><formula xml:id="formula_10">G c = (A, M x • X, M e • E),<label>(5)</label></formula><formula xml:id="formula_11">G t = (A, Mx • X, Me • E),<label>(6)</label></formula><p>where M x = diag(α i ), M e = diag(α ij ) will filter out the non-causal nodes and edges; accordingly, their "complement" Mx = I x -M x , Me = I e -M e , where I x and I e are the identity matrix. As a result, nodes and edges with higher causality attention scores receive larger weights in G c , emphasizing their relevance to the task. Conversely, those with lower attention values are weighted less in G t , reflecting their diminished importance for the causal task.</p><p>2) Edge-featured causal and trivial representations: For the causal subgraph G c and the trivial subgraph G t , we compute their representations by incorporating both node and edge features, allowing stronger ability to capture significant causal relationships while distinguishing them from trivial ones.</p><p>To simplify the notations, in this part the causal and the trivial subgraphs are referred to as:</p><formula xml:id="formula_12">G c = {A, X (c) , E (c) }, G t = {A, X (t) , E (t) }. (<label>7</label></formula><formula xml:id="formula_13">)</formula><p>We reuse the EGAT modules (both v1 and v2 are evaluated in the experiments, c.f. the experiments in Table <ref type="table" target="#tab_0">I</ref>) to obtain the representations of the attended graphs. The final label prediction ŷ(x) is obtained via a readout function f</p><formula xml:id="formula_14">(x)</formula><p>readout (•) and a classifier Φ x (the script x can be c or t, corresponding to "causal" or "trivial"), which can be formulated as :</p><formula xml:id="formula_15">ŷ(c) = Φ c (h (c) ), with h (c) = f (c) readout (EGAT c (X (c) , E (c) )), ŷ(t) = Φ t (h (t) ), with h (t) = f (t) readout (EGAT t (X (t) , E (t) )).</formula><p>This approach ensures that the model focuses on the most relevant parts of the graph while down-weighting less informative features, ultimately leading to better generalization in out-of-distribution (OOD) graph classification tasks.</p><p>3) Loss computation: We train the model using both the causal subgraph G c and the trivial subgraph G t . The supervised label corresponding to the causal subgraph should be the ground-truth one, and we apply the cross-entropy function to obtain the classification loss:</p><formula xml:id="formula_16">L CE = - k y T k log ŷ(c) k<label>(8)</label></formula><p>where y k is the one-hot label vector for the k th sample.</p><p>In addition, to make causality detection feasible, two more loss terms are involved. For the trivial subgraph, we apply a KL divergence loss to push these predictions ŷ(t) towards a uniform distribution U, ensuring that less important elements contribute minimally:</p><formula xml:id="formula_17">L KL = k KL ŷ(t) k ∥ U<label>(9)</label></formula><p>Furthermore, guided by the spirit of intervention in backdoor adjustment <ref type="bibr" target="#b30">[31]</ref>, the random addition technique is applied to isolate the effectiveness of the causal subgraph from the trivial subgraph on the representation level <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b32">[33]</ref>. In more details, in this step we randomly pair the causal graph with a mismatched trivial graph from the same training batch, and compute a separate cross-entropy loss using the predictions from the randomized representation ŷ(k,k ′ ) = Φ(h</p><formula xml:id="formula_18">(c) k + h (t) k ′ ) (Φ(•)</formula><p>is a third classifier) and the target output:</p><formula xml:id="formula_19">L BA = - k k ′ y T k log ŷ(k,k ′ )<label>(10)</label></formula><p>The total training loss is a weighted combination of the three aforementioned loss terms:</p><formula xml:id="formula_20">L = L CE + L CD (λ 1 , λ 2 ),<label>(11)</label></formula><p>where L CD (λ 1 , λ 2 ) = λ 1 L KL + λ 2 L BA represents the loss for causality detection. This strategy ensures that the model focuses on learning from the causal subgraph while minimizing the influence of trivial features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENT RESULTS</head><p>We conducted experiments on three edge-featured graph classification datasets, including Tox21 <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>, ogbgmolhiv <ref type="bibr" target="#b35">[36]</ref>, and PTC-MR <ref type="bibr" target="#b36">[37]</ref>. We manipulate the dataset to introduce label imbalance so that the distribution of training datasets and that of testing datasets are different.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Baselines and main comparison</head><p>To illustrate the superiority of the proposed edge-enhanced framework for the causal attention mechanism, we employ three relevant frameworks for graph classification as baselines:</p><p>• General models: GCN <ref type="bibr" target="#b37">[38]</ref>, GAT <ref type="bibr" target="#b38">[39]</ref> • General models + edge info: EGATv1 <ref type="bibr" target="#b26">[27]</ref>, EGATv2 <ref type="bibr" target="#b29">[30]</ref> • OOD models: CAL+GCN <ref type="bibr" target="#b23">[24]</ref>, CAL+GAT <ref type="bibr" target="#b23">[24]</ref> The evaluation metric is the classification accuracy, and the results are summarized in Table <ref type="table" target="#tab_0">I</ref>. To assess statistical significance, p-values calculated from 10 independent trials are shown in Table <ref type="table" target="#tab_1">II</ref>, confirming the consistent improvements achieved by ECAL over CAL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Ablation studies 1) Ablating causality detection:</head><p>To evaluate the impact of causal intervention techniques and highlight the importance of incorporating edge features in causal learning, we remove the causality detection loss terms in ECAL and CAL. Specifically, we compare removing the KL divergence term (9) (KL), the backdoor adjustment term (10) (BA), or both (CD). Results are shown in Figure <ref type="figure" target="#fig_1">2</ref>. For CAL, which does not use edge features, removing these terms has little effect on accuracy under label imbalance. In contrast, ECAL with edge features improves test accuracy in such settings by leveraging causality detection loss terms.</p><p>2) Adding noise to ablate edge features: In causal learning, features are considered non-causal if adding noise does not reduce OOD accuracy. To emphasize the causality of edge features, we randomly permute a proportion of edge feature vectors. As shown in Figure <ref type="figure" target="#fig_2">3</ref>, adding noise consistently reduces OOD accuracy on ogbg-molhiv, confirming the causal significance of edge features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSIONS</head><p>In this paper, we proposed an edge-enhanced framework for causal attention mechanism, which applies edge-featured graph attention networks to better separate causal and noncausal subgraphs and extract the graph representation. This new design extends the capabilities of causal attention mechanisms, offering a promising direction for tackling diverse  graph-level tasks in real-world applications. We validate the effectiveness of our proposed models on benchmark datasets (label imbalanced), including PTC, Tox21, and ogbg-molhiv; our proposed methods demonstrate superiority in label imbalance scenarios compared to existing methods and showcase the necessity of incorporating edge features.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Framework overview. The proposed model integrates causal attention mechanisms with edge feature incorporation, enhancing the generalization capabilities of GNNs in OOD scenarios. Best viewed zoom in.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The effect on OOD accuracy of removing causality detection loss terms, for both CAL-based and ECAL-based models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig.3. OOD accuracy under different noise levels (proportion of permutation) and label imbalance levels on ogbg-molhiv.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I TEST</head><label>I</label><figDesc>ACCURACY IN GRAPH CLASSIFICATION TASKS. THE BEST ACCURACY RESULTS ARE BOLDFACED.</figDesc><table><row><cell>Dataset</cell><cell cols="7">PTC-FM PTC-MM Tox21-AR Tox21-ATAD5 Tox21-PPAR-gamma Tox21-aromatase ogbg-molhiv</cell></row><row><cell>GCN</cell><cell>0.457</cell><cell>0.559</cell><cell>0.981</cell><cell>0.992</cell><cell>0.984</cell><cell>0.932</cell><cell>0.801</cell></row><row><cell>GAT</cell><cell>0.400</cell><cell>0.485</cell><cell>0.989</cell><cell>0.990</cell><cell>0.993</cell><cell>0.935</cell><cell>0.796</cell></row><row><cell>EGATv1</cell><cell>0.386</cell><cell>0.485</cell><cell>0.994</cell><cell>0.988</cell><cell>0.987</cell><cell>0.937</cell><cell>0.806</cell></row><row><cell>EGATv2</cell><cell>0.443</cell><cell>0.515</cell><cell>0.99</cell><cell>0.988</cell><cell>0.984</cell><cell>0.932</cell><cell>0.793</cell></row><row><cell>CAL+GCN</cell><cell>0.429</cell><cell>0.574</cell><cell>0.985</cell><cell>0.989</cell><cell>0.983</cell><cell>0.943</cell><cell>0.801</cell></row><row><cell>CAL+GAT</cell><cell>0.429</cell><cell>0.456</cell><cell>0.99</cell><cell>0.988</cell><cell>0.98</cell><cell>0.931</cell><cell>0.806</cell></row><row><cell>ECAL+EGATv1</cell><cell>0.386</cell><cell>0.691</cell><cell>0.994</cell><cell>0.982</cell><cell>0.989</cell><cell>0.921</cell><cell>0.85</cell></row><row><cell>ECAL+EGATv2</cell><cell>0.671</cell><cell>0.691</cell><cell>0.996</cell><cell>0.995</cell><cell>0.997</cell><cell>0.947</cell><cell>0.81</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II p</head><label>II</label><figDesc>-VALUES IN TWO-SAMPLE t-TESTS FOR COMPARING ECAL WITH CAL.</figDesc><table><row><cell>Dataset</cell><cell>PTC-FM</cell><cell>PTC-MM</cell><cell>Tox21-AR</cell><cell>Tox21-ATAD5</cell><cell>Tox21-PPAR-gamma</cell><cell>Tox21-aromatase</cell><cell>ogbg-molhiv</cell></row><row><cell>p-value</cell><cell>3.9e-06</cell><cell>2.9e-08</cell><cell>7.4e-12</cell><cell>3.7e-4</cell><cell>3.3e-05</cell><cell>0.018</cell><cell>7.3e-14</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>This work is supported by <rs type="funder">SF Tech</rs>. <rs type="person">Yifan Chen</rs> is supported by the <rs type="funder">Research Grants Council (RGC)</rs> under grant <rs type="grantNumber">ECS-22303424</rs>, and <rs type="person">GuangDong Basic</rs> and <rs type="funder">Applied Basic Research Foundation</rs> through the general project grant.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_7YVuE4m">
					<idno type="grant-number">ECS-22303424</idno>
				</org>
			</listOrg>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The preprint version is available at <ref type="url" target="https://arxiv.org/abs/2501.01707">https://arxiv.org/abs/2501.01707</ref>. The model implementation details and the codes are available on <ref type="url" target="https://github.com/fengrui-z/ECAL">https://github.com/fengrui-z/ECAL</ref>.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on neural networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A comprehensive survey on graph neural networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Philip</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="4" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Chemistry-intuitive explanation of graph neural networks for molecular property prediction with substructure masking</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dejun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-S</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Communications</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="5" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Analyzing learned molecular representations for property prediction</title>
		<author>
			<persName><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Swanson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Coley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Eiden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Guzman-Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hopper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kelley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mathea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of chemical information and modeling</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3370" to="3388" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Iparraguirre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bombarell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deeprank-gnn: a graph neural network framework to learn patterns in protein-protein interfaces</title>
		<author>
			<persName><forename type="first">M</forename><surname>Réau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Renaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Bonvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">759</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Prediction of protein-protein interaction using graph neural networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific Reports</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">8360</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imbalanced graph classification via graph-of-graph neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Derr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st ACM International Conference on Information &amp; Knowledge Management</title>
		<meeting>the 31st ACM International Conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2067" to="2076" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Graphsmote: Imbalanced node classification on graphs with graph neural networks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM international conference on web search and data mining</title>
		<meeting>the 14th ACM international conference on web search and data mining</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="833" to="841" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multi-class imbalanced graph convolutional network learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Ninth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">20</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Ins-gnn: Improving graph imbalance learning with self-supervision</title>
		<author>
			<persName><forename type="first">X</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">637</biblScope>
			<biblScope unit="page">118935</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Good: A graph out-of-distribution benchmark</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="2059" to="2073" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.04468</idno>
		<title level="m">A survey of graph neural networks in real world: Imbalance, noise, privacy and ood challenges</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Recent advances in reliable deep graph learning: Inherent noise, distribution shift, and adversarial attack</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.07114</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Trustworthy graph neural networks: aspects, methods, and trends</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE</title>
		<meeting>the IEEE</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning substructure invariance for out-of-distribution molecular representations</title>
		<author>
			<persName><forename type="first">N</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page">978</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Environment-aware dynamic graph learning for out-of-distribution generalization</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Joint learning of label and environment causal independence for graph out-of-distribution generalization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Out-of-distribution generalized dynamic graph neural network with disentangled intervention and invariance promotion</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.14255</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Towards human-like perception: Learning structural causal model in heterogeneous graph</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">103600</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning causally invariant representations for out-ofdistribution generalization on graphs</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kaili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="22" to="131" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Generalizing graph neural networks on out-of-distribution graphs</title>
		<author>
			<persName><forename type="first">S</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Causal discovery with attentionbased convolutional neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Nauta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bucur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Seifert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning and Knowledge Extraction</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">19</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Causal attention for interpretable and generalizable graph classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1696" to="1705" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Exploiting edge features for graph neural networks</title>
		<author>
			<persName><forename type="first">L</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="9211" to="9219" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Strategies for pre-training graph neural networks</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.12265</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Egat: Edge-featured graph attention network</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Neural Networks and Machine Learning-ICANN 2021: 30th International Conference on Artificial Neural Networks</title>
		<meeting><address><addrLine>Bratislava, Slovakia</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">September 14-17, 2021. 2021</date>
			<biblScope unit="page" from="253" to="264" />
		</imprint>
	</monogr>
	<note>Proceedings, Part I 30</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Multi-agent trajectory prediction with heterogeneous edge-enhanced graph attention network</title>
		<author>
			<persName><forename type="first">X</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="9554" to="9567" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Do transformers really perform badly for graph representation?</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="28" to="877" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Rossmann-toolbox: a deep learning-based protocol for the prediction and design of cofactor specificity in rossmann fold proteins</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kamiński</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ludwiczak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jasiński</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bukala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Madaj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Szczepaniak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dunin-Horkawicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Briefings in bioinformatics</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">371</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Causality</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Causal effect identification by adjustment under confounding and selection biases</title>
		<author>
			<persName><forename type="first">J</forename><surname>Correa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bareinboim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Generalized adjustment under confounding and selection biases</title>
		<author>
			<persName><forename type="first">J</forename><surname>Correa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bareinboim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deeptox: toxicity prediction using deep learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mayr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Klambauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Environmental Science</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">80</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Tox21challenge to build predictive models of nuclear receptor and stress response pathways as mediated by exposure to environmental chemicals and drugs</title>
		<author>
			<persName><forename type="first">R</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D.-T</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sakamuru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Shahane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rossoshek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Simeonov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Environmental Science</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">85</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="22" to="118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Unsupervised inductive graph-level representation learning via graph-graph proximity</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Marinovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.01098</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
