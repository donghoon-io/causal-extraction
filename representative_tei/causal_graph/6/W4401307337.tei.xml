<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Graph Representation Learning via Causal Diffusion for Out-of-Distribution Recommendation</title>
				<funder ref="#_5Y5ErNY #_VnweVHB">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_QqfVRJW">
					<orgName type="full">Science and Technology projects in Liaoning Province</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2025-04-02">2 Apr 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Chu</forename><surname>Zhao</surname></persName>
							<email>chuzhao@stumail.neu.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Enneng</forename><surname>Yang</surname></persName>
							<email>ennengyang@stumail.neu.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Yuliang</forename><surname>Liang</surname></persName>
							<email>liangyuliang@stumail.neu.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Pengxiang</forename><surname>Lan</surname></persName>
							<email>pengxianglan@stumail.neu.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Yuting</forename><surname>Liu</surname></persName>
							<email>yutingliu@stumail.neu.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Jianzhe</forename><surname>Zhao</surname></persName>
							<email>zhaojz@swc.neu.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Guibing</forename><surname>Guo</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Xingwei</forename><surname>Wang</surname></persName>
							<email>wangxw@mail.neu.edu.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Northeastern University Shenyang</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Northeastern University Shenyang</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Northeastern University Shenyang</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Northeastern University Shenyang</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="institution">Northeastern University Shenyang</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="institution">Northeastern University Shenyang</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="institution">Northeastern University Shenyang</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff7">
								<orgName type="institution">Northeastern University Shenyang</orgName>
								<address>
									<addrLine>13 pages. https: //doi.org/10</addrLine>
									<postCode>2025 1145/3696410 3714849</postCode>
									<settlement>Sydney New York</settlement>
									<region>NSW Australia. ACM NY</region>
									<country>China USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Graph Representation Learning via Causal Diffusion for Out-of-Distribution Recommendation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-04-02">2 Apr 2025</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3696410.3714849</idno>
					<idno type="arXiv">arXiv:2408.00490v4[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS</term>
					<term>Information systems â†’ Recommender systems Graph Recommendation</term>
					<term>Distributionally Robust Optimization</term>
					<term>Out-of-Distribution</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph Neural Networks (GNNs)-based recommendation algorithms typically assume that training and testing data are drawn from independent and identically distributed (IID) spaces. However, this assumption often fails in the presence of out-of-distribution (OOD) data, resulting in significant performance degradation. In this study, we construct a Structural Causal Model (SCM) to analyze interaction data, revealing that environmental confounders (e.g., the COVID-19 pandemic) lead to unstable correlations in GNN-based models, thus impairing their generalization to OOD data. To address this issue, we propose a novel approach, graph representation learning via causal diffusion (CausalDiffRec) for OOD recommendation. This method enhances the model's generalization on OOD data by eliminating environmental confounding factors and learning invariant graph representations. Specifically, we use backdoor adjustment and variational inference to infer the real environmental distribution, thereby eliminating the impact of environmental confounders. This inferred distribution is then used as prior knowledge to guide the representation learning in the reverse phase of the diffusion process to learn the invariant representation. In addition, we provide a theoretical derivation that proves optimizing the objective function of CausalDiffRec can encourage the model to learn environment-invariant graph representations, thereby achieving excellent generalization performance in recommendations under distribution shifts. Our extensive experiments validate the effectiveness of CausalDiffRec in improving the generalization of OOD</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Graph Neural Networks (GNN) <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b58">59]</ref>, due to their exceptional ability to learn high-order features, have been widely applied in recommendation systems. GNN-based recommendation algorithms <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b42">43]</ref> learn user and item representations by aggregating information from neighboring nodes in the user-item interaction graph and then computing their similarity to predict user preferences. In addition, researchers have introduced various other techniques to continuously improve GNN-based recommendation algorithms. For example, integrating attention mechanisms <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b24">25]</ref> with knowledge graphs <ref type="bibr" target="#b13">[14]</ref> led to improving recommendation accuracy. Furthermore, the introduction of contrastive learning aims to improve the robustness of recommendation algorithms <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b50">51]</ref>.</p><p>Despite significant progress in enhancing recommendation accuracy, most existing methods assume that test and training datasets follow an independently and identically distributed (IID) pattern, focusing on performance improvements under this assumption. Such methods struggle to generalize effectively to out-of-distribution (OOD) data, where test data distributions markedly differ from training data <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b47">48]</ref>. For example, as shown in Figure <ref type="figure" target="#fig_0">1</ref>, the popularity of medical supplies surged during the COVID-19 pandemic, alongside increased demand for fitness equipment and electronics due to government-imposed homestays. The recommender systems might infer that purchasing masks correlates with buying these additional items, driven by the common 'pandemic' factor rather than direct causation. Once the pandemic subsides, demand shifts, reducing the popularity of masks and related items. Consequently, the system may inaccurately recommend fitness equipment and electronics to mask buyers under the new distribution, leading to poor performance. Additionally, tests on the Yelp2018 dataset, comparing IID and OOD sets, demonstrate a significant performance drop. LightGCN <ref type="bibr" target="#b8">[9]</ref> experiences an average 29.03% decline across three metrics on OOD data compared to IID settings, highlighting the robustness issues of GNN-based models in OOD scenarios. This challenge motivates the development of a recommendation framework with strong generalization capabilities for handling distribution shifts.</p><p>Several studies have aimed to enhance the generalization of recommender systems on OOD datasets by using causal inference to address data distribution shifts. For instance, CausPref <ref type="bibr" target="#b10">[11]</ref> builds on NeuMF <ref type="bibr" target="#b9">[10]</ref> by implementing invariant user preference causal learning and anti-preference negative sampling to boost model generalization. COR <ref type="bibr" target="#b30">[31]</ref> utilizes a Variational Auto-Encoder for causal modeling by inferring unobserved user features from historical interactions. However, these methods are not tailored for GNNs, complicating their adaptation to GNN-based approaches.</p><p>Other researchers have adopted techniques like graph contrastive learning and graph data augmentation to improve the robustness of GNN-based recommendation algorithms, such as SGL <ref type="bibr" target="#b39">[40]</ref>, SimGCL <ref type="bibr" target="#b49">[50]</ref>, and LightGCL <ref type="bibr" target="#b0">[1]</ref>. These approaches mainly address noise or popularity bias but underperform when test data distributions are unknown or varied, as evidenced by experimental results. Recently, a few GNN-based methods <ref type="bibr" target="#b52">[53]</ref> have been proposed to enhance generalization across multiple distributions, but they lack strong theoretical backing.</p><p>Given these limitations, there is an urgent need to design theoretically grounded GNN-based methods to address distribution shifts. In this paper, we use invariant learning to improve the generalization of the OOD dataset. Utilizing insights from the prior knowledge of environment distribution and invariant learning <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b56">57]</ref> enhances model stability across varied environments. This is achieved by acquiring invariant representations, boosting the model's generalization capabilities and overall robustness. Following the research approach of prior work <ref type="bibr" target="#b51">[52]</ref>, we further analyze and argue that designing recommendation models based on invariant learning theory still faces the following two challenges:</p><p>â€¢ (1) How to infer the distribution of underlying environments from observed user-item interaction data? â€¢ (2) How to recognize environment-invariant patterns amid changing user behaviors and preferences?</p><p>In this paper, we first develop a Structural Causal Model (SCM) to analyze data generation processes in recommender systems, specifically addressing the impact of data distribution shifts on GNN-based recommendation algorithms. We find that latent environmental variables can lead these models to capture unstable We found a significant average performance drop (i.e., 29.03%) in OOD data across three metrics. correlations, hindering their generalization to OOD data. To tackle this, we introduce CausalDiffRec, a novel method using causal inference to remove these unstable correlations by learning invariant representations across different environments. CausalDiffRec comprises an environment generator to create diverse data distributions, an environment inference module to identify and utilize environmental components, and a diffusion module guiding invariant representation learning. Theoretically, we prove that CausalDiffRec can achieve better OOD generalization by identifying invariant representations across varying environments. The contributions of this paper are concluded as follows:</p><p>â€¢ Causal Analysis. We construct the SCM and analyze the generalization ability of GNN-based recommendation models on OOD data from the perspective of data generation. Based on our analysis and experimental results, we conclude that environmental confounders lead the model to capture unstable correlations, which is the key reason for its failure to generalize under distribution shifts. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRELIMINARY 2.1 GNN-based Recommendation</head><p>Given the observed implicit interaction matrix R âˆˆ {0, 1} ğ‘šÃ—ğ‘› , in which U = {ğ‘¢ 1 , ğ‘¢ 2 , . . . , ğ‘¢ ğ‘š } represents the set of users, I = {ğ‘– 1 , ğ‘– 2 , . . . , ğ‘– ğ‘› } represents the set of items, ğ‘š and ğ‘› denote the number of users and items, respectively. For the elements in the interaction matrix, ğ‘Ÿ ğ‘¢ğ‘– = 1 indicates an interaction between user ğ‘¢ and item ğ‘–, otherwise 0. In GNN-based recommendation algorithms, the user-item interaction matrix R is first transformed into a bipartite graph ğº = {V, E}. We employ V to represent the node set and E = {(ğ‘¢, ğ‘–)|ğ‘¢ âˆˆ U, ğ‘– âˆˆ I, ğ‘Ÿ ğ‘¢ğ‘– = 1} denotes the edge set. Given a user-item interaction graph G ğ‘¢ and the true user interactions ğ‘¦ ğ‘¢ with respect to (w.r.t) user ğ‘¢, the optimization objective of GNN-based methods can be expressed as:</p><formula xml:id="formula_0">arg min ğœƒ E ( G ğ‘¢ ,ğ‘¦ ğ‘¢ )âˆ¼ğ‘ƒ (ğº,ğ‘Œ ) [ğ‘™ (ğ‘“ ğœƒ (G ğ‘¢ ; ğœƒ ), ğ‘¦ ğ‘¢ )],<label>(1)</label></formula><p>where ğ‘“ ğœƒ (â€¢) is a learner that learns representations by aggregating high-order neighbor information from the user-item interaction graph. ğ‘™ denotes the loss function and ğ‘ƒ (ğº, ğ‘Œ ) represents the joint distribution of the interaction graph ğº and true label ğ‘Œ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Denoising Diffusion Probabilistic Models</head><p>Denoising Diffusion Probabilistic Models (DDPM) <ref type="bibr" target="#b11">[12]</ref> have been widely used in the field of image and video generation. The key idea of DDPM is to achieve the generation and reconstruction of the input data distribution through a process of gradually adding and removing noise. It leverages neural networks to learn the reverse denoising process from noise to real data distribution. The diffusion process in recommender system models the evolution of user preferences and item information through noise addition and iterative recovery. Initially, data x 0 sampled from ğ‘(x) undergo a forward diffusion to generate noisy samples x 1 , . . . , x ğ‘‡ over ğ‘‡ steps. Each step adds Gaussian noise, transforming the data distribution incrementally <ref type="bibr" target="#b11">[12]</ref>:</p><formula xml:id="formula_1">ğ‘(x ğ‘¡ |x ğ‘¡ -1 ) = N x ğ‘¡ ; âˆšï¸ 1 -ğ›½ ğ‘¡ x ğ‘¡ -1 , ğ›½ ğ‘¡ I ,<label>(2)</label></formula><p>where ğ›½ ğ‘¡ âˆˆ (0, 1) controls the level of the added noise at step ğ‘¡. In the reverse phase, the aim is to restore the original data by learning a model ğ‘ ğœƒ to approximate the reverse diffusion from x ğ‘‡ to x 0 . The process, governed by ğ‘ ğœƒ (x ğ‘¡ -1 |x ğ‘¡ ), uses the mean ğœ‡ ğœƒ and covariance Î£ ğœƒ learned via neural networks:</p><formula xml:id="formula_2">ğ‘ ğœƒ (x ğ‘¡ -1 |x ğ‘¡ ) = N (x ğ‘¡ -1 ; ğœ‡ ğœƒ (x ğ‘¡ , ğ‘¡), Î£ ğœƒ (x ğ‘¡ , ğ‘¡)) .<label>(3)</label></formula><p>The reverse process is optimized to minimize the variational lower bound (VLB), balancing the fidelity of reconstruction and the simplicity of the model <ref type="bibr" target="#b37">[38]</ref>:</p><formula xml:id="formula_3">L ğ‘‰ ğ¿ğµ = E ğ‘ (x 1:ğ‘‡ |x 0 ) ğ‘‡ âˆ‘ï¸ ğ‘¡ =1 ğ· ğ¾ğ¿ (ğ‘(x ğ‘¡ -1 |x ğ‘¡ , x 0 )||ğ‘ ğœƒ (x ğ‘¡ -1 |x ğ‘¡ )) -log ğ‘ ğœƒ (x 0 |x 1 ),<label>(4)</label></formula><p>where ğ· ğ¾ğ¿ denotes the Kullback-Leibler (KL) divergence. Following <ref type="bibr" target="#b11">[12]</ref>, we mitigate the training instability issue in the model by expanding and reweighting each KL divergence term in the VLB with specific parameterization. Therefore, we have the following mean squared error loss:</p><formula xml:id="formula_4">L simple = E ğ‘¡,x 0 ,ğ ğ‘¡ ğ ğ‘¡ -ğ ğœƒ âˆš ğ›¼ ğ‘¡ x 0 + âˆš 1 -ğ›¼ ğ‘¡ ğ, ğ‘¡ 2 ,<label>(5)</label></formula><p>where ğœ– ğ‘¡ âˆ¼ N (0, I) is the noise for injection in forward process,  to effectively learn noise-free representations and improve recommendation accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Invariant Pattern Recognition Mechanism</head><p>Invariant learning <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b51">52]</ref> is often used to develop predictive models that can generalize to out-of-distribution (OOD) data, which originates from different environments. Existing invariant learning methods typically rely on the following assumptions: Assumption: For a given user-item interaction graph (i.e., data distribution ğ·), these interaction data are collected from ğ¾ different environments ğ¸. User behavior patterns exist independently of the environment and can be used to generalize out-of-distribution user preference prediction. There exists an optimal invariant graph representation learning ğ¹ * (â€¢) satisfying: â€¢ Invariance Property. âˆ€ğ‘’ âˆˆ ğ· (ğ¸) , ğ‘ƒ ğœƒ (ğ‘Œ |ğ¹ * (ğº), ğ¸ = ğ‘’, ğ¼ ) = ğ‘ƒ (ğ‘Œ |ğ¹ * (ğº), ğ¼ ). â€¢ Sufficiency Condition. ğ‘Œ = ğ¹ * (ğº) + ğœ–, ğœ–âŠ¥ğ¸, where âŠ¥ indicates statistical independence and ğœ– is random noise.</p><p>The invariance property assumption indicates that a graph representation learning model exists capable of learning invariant user-item representations across different data distribution environments. The sufficiency condition assumption means that the learned invariant representations enable the model to make accurate predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHODOLOGY</head><p>In this section, we first construct SCM and identify environmental confounders as the key reason for the failure of GNN-based models to generalize on OOD (out-of-distribution) data. Subsequently, we introduce the variational inference to infer the true distribution of the environment. We use the diffusion model to learn the representation based on invariant learning. Finally, we provide rigorous theoretical proof of CausalDiffRec that can achieve great generalization. The model framework is illustrated in Figure <ref type="figure" target="#fig_3">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">SCM of GNN-based Recommendation</head><p>To explore the reasons behind the failure of GNN-based models to generalize on OOD data, we follow previous works <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b46">47]</ref> and first construct the SCM for data generation and data modeling in recommendation systems, as shown in Figure <ref type="figure" target="#fig_1">2</ref> (a) and Figure <ref type="figure" target="#fig_2">2 (b)</ref>. We find that environmental confounding factors are the key reason for the generalization failure of GNN-based methods. Finally, we design an intervention model in Figure <ref type="figure" target="#fig_1">2</ref> (c) to eliminate the impact of environmental confounding factors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3.1.1</head><p>Causal View in GNN-based Recommendation. In Figure <ref type="figure" target="#fig_1">2</ref>, the three causal relations are derived from the definitions of data generation. The detailed causal analysis behind them is presented as follows:</p><p>â€¢ Environmental Factors (ğ¸): These represent unseen factors such as sudden events or policies that affect user-item interaction graphs (ğº) and true labels (ğ‘Œ ). Invariant attributes (ğ¼ ), like user gender and item categories, remain unaffected by these factors.</p><p>Prior research <ref type="bibr" target="#b10">[11]</ref> suggests leveraging these invariant features can enhance model generalization in OOD environments. â€¢ ğ¸ â†’ ğ‘Œ : The environment directly impacts the user behavior label ğ‘Œ , independent of user-item interactions. For example, during holidays, users might be more inclined to buy holiday-related items regardless of past interactions.</p><p>In real-world scenarios, training data is collected from heterogeneous environments. Therefore, the environment directly influences the distribution of the data and the prediction result, which can be explicitly represented as ğ‘ƒ (ğ‘Œ, ğº |ğ¸) = ğ‘ƒ (ğº |ğ¸)ğ‘ƒ (ğ‘Œ |ğº, ğ¸). If we employ ğ· tr (ğ¸) to represent the training data distribution for unobserved environments, the GNN-based model, when faced with OOD data, can rewrite Eq. ( <ref type="formula" target="#formula_0">1</ref>) as:</p><formula xml:id="formula_5">arg min ğœƒ E ğ‘’âˆ¼ğ· ğ‘¡ğ‘Ÿ (ğ¸ ),( G ğ‘¢ ,ğ‘¦ ğ‘¢ )âˆ¼ğ‘ƒ (ğº,ğ‘Œ |ğ¸=ğ‘’ ) [ğ‘™ (ğ‘“ ğœƒ (G ğ‘¢ ; ğœƒ ), ğ‘¦ ğ‘¢ )|ğ‘’],<label>(6)</label></formula><p>Eq. <ref type="bibr" target="#b5">(6)</ref> shows that environment ğ¸ affects the data generation used for training the GNN-based recommendation model. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Confounding Effect of E.</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Intervention.</head><p>Through the above analysis, we can improve the generalization ability of GNN-based recommendation models by guiding the model to uncover stable predictive relationships behind the training data, specifically those that are less sensitive to environmental changes. Thus, we can eliminate the influence of environmental confounders on model predictions. Specifically, we learn stable correlations between user item interaction G ğ‘¢ and ground truth ğ‘¦ ğ‘¢ by optimizing ğ‘ƒ ğœƒ (ğ‘Œ |ğ‘‘ğ‘œ (ğº)) instead of ğ‘ƒ ğœƒ (ğ‘Œ |ğº). In causal theory, the ğ‘‘ğ‘œ-operation signifies removing the dependencies between the target variable and other variables. As shown in Figure <ref type="figure" target="#fig_1">2</ref> (c), by cutting off the causal relationship between the environment variables and the user interaction graph, the model no longer learns the unstable correlations between G ğ‘¢ and ğ‘¦ ğ‘¢ . The ğ‘‘ğ‘œ-operation simulates the generation process of the interaction graph ğº, where environmental factors do not influence the useritem interactions. This operation blocks the unstable backdoor path ğº â† ğ¸ â†’ ğ‘Œ , enabling the GNN-based recommendation model to capture the desired causal relationship that remains invariant under environmental changes.</p><p>Theoretically, ğ‘ƒ ğœƒ (ğ‘Œ |ğ‘‘ğ‘œ (ğº)) can be computed through randomized controlled trials, which involve randomly collecting new data from any possible environment to eliminate environmental bias. However, such physical interventions are challenging. For instance, in a short video recommendation setting, it is impossible to expose all short videos to a single user, and it is also impractical to control the environment of data interactions. In this paper, we achieve a statistical estimation of ğ‘ƒ ğœƒ (ğ‘Œ |ğ‘‘ğ‘œ (ğº)) by leveraging backdoor adjustment. We have:</p><formula xml:id="formula_6">ğ‘ƒ ğœƒ (ğ‘Œ |ğ‘‘ğ‘œ (ğº)) = E ğ‘’âˆ¼ğ· ğ‘¡ğ‘Ÿ (ğ¸ ) [ğ‘ƒ ğœƒ (ğ‘Œ |ğº, ğ¸, ğ¼ )].<label>(7)</label></formula><p>The derivation process is shown in Appendix B.1. Through the aforementioned backdoor adjustment, the influence of the environment ğ¸ on the generation of ğº can be eliminated, enabling the model to learn correlations independent of the environment. However, in recommendation scenarios, environmental variables are typically unobservable or undefined, and their prior distribution ğ‘ƒ (ğ¸ = ğ‘’) cannot be computed. Therefore, directly optimizing the Eq. ( <ref type="formula" target="#formula_6">7</ref>) is challenging.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model Instantiations</head><p>3.2.1 Environment Inference. This work introduces a variational inference method and proposes a variational inference-based environment instantiation mechanism. The core idea is to use variational inference to approximate the true distribution of environments and generate environment pseudo-labels as latent variables.</p><p>The following tractable evidence lower bound (ELBO) can be obtained as the learning objective:</p><formula xml:id="formula_7">logğ‘ƒ ğœƒ (ğ‘Œ |ğ‘‘ğ‘œ (ğº)) â‰¥ L ğ‘’ğ‘›ğ‘£ğ¼ğ‘›ğ‘“ = E ğ‘„ ğœ™ (ğ¸ |ğº,ğ¼ ) [logğ‘ƒ ğœƒ (ğ‘Œ |ğº, ğ¸, ğ¼ )] -ğ· ğ¾ğ¿ (ğ‘„ ğœ™ (ğ¸|ğº, ğ¼ ) âˆ¥ ğ‘ƒ ğœƒ (ğ¸)),<label>(8)</label></formula><p>where ğ‘„ ğœ™ (ğ¸|ğº, ğ¼ ) denotes environment estimation, which draws samples from the true distribution of the environment ğ¸. ğ· ğ¾ğ¿ represents the Kullback-Leibler (KL) divergence of the volitional distribution ğ‘„ ğœ™ (ğ¸|ğº, ğ¼ ) and the prior distribution ğ‘ƒ ğœƒ (E). ğ‘ƒ ğœƒ (ğ‘Œ |ğº, ğ¸, ğ¼ ) is the graph representation learning module that employs the useritem interaction graph and the node attributes of users and items as input to learn invariant representations. Section 3.2.2 will provide a detailed introduction to the graph representation learning module. The derivation process of Eq. ( <ref type="formula" target="#formula_7">8</ref>) is displayed in Appendix B.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Invariant Representation</head><p>Learning. This section mainly consists of an environment generator, a diffusion-based graph representation learning module, and a recommendation module. Next, we will detail how they collaborate to enhance the generalizability of GNN-based models on OOD data and improve recommendation accuracy.</p><p>Environment Generator. In real-world recommendation scenarios, training datasets are collected in various environments. However, for a single user-centric interaction graph, the training dataset comes from a single environment. We need to learn environment-invariant correlations from training data originating from different environments to achieve the generalization capability of GNN-based recommendation models under distribution shifts. To circumvent this dilemma, this paper designs an environment generator ğ‘” ğœ” ğ‘˜ (â€¢) (1 â‰¤ ğ‘˜ â‰¤ ğ¾), which takes the user's original interaction graph ğº as input and generates a set of ğ¾ interaction graphs {ğº ğ‘– } ğ¾ ğ‘–=1 to simulate training data from different environments. The optimization objective is expressed as follows:</p><formula xml:id="formula_8">L ğ‘”ğ‘’ğ‘›ğ‘’ğ‘Ÿğ‘ğ‘¡ğ‘œğ‘Ÿ = Var(L (ğ‘” ğœ” ğ‘˜ (ğº)) : 1 â‰¤ ğ‘˜ â‰¤ ğ¾) ,<label>(9)</label></formula><p>where ğ‘‰ ğ‘ğ‘Ÿ (â€¢) denotes the variance and L (â€¢) is the loss function. Following existing work <ref type="bibr" target="#b41">[42]</ref>, we modify the graph structure by adding and removing edges. Given a Boolean matrix ğµ ğ‘˜ , the adjacency matrix ğ´ of the graph, and its complement ğ´ â€² , the ğ‘˜-th generated view for the original view is</p><formula xml:id="formula_9">ğ´ ğ‘˜ = ğ´ + ğµ ğ‘˜ âŠ™ (ğ´ -ğ´ â€² ).</formula><p>Since ğµ ğ‘˜ is a discrete matrix and not differentiable, it cannot be optimized directly. To address this issue, we borrow the idea from <ref type="bibr" target="#b41">[42]</ref> and use reinforcement learning to treat graph generation as a decision process and edge editing as actions. Specifically, for view ğ‘˜, we consider a parameter matrix ğœƒ ğ‘˜ = {ğœƒ ğ‘˜ ğ‘›ğ‘š }. For the ğ‘›-th node, the probability of exiting the edge between it and the ğ‘š-th node is given by:</p><formula xml:id="formula_10">â„(ğ‘ ğ‘˜ ğ‘›ğ‘š ) = exp(ğœƒ ğ‘˜ ğ‘›ğ‘š ) ğ‘š â€² =ğ‘š ğ‘š â€² =1 exp(ğœƒ ğ‘˜ ğ‘›ğ‘š â€² ) . (<label>10</label></formula><formula xml:id="formula_11">)</formula><p>We then sample ğ‘  actions {ğ‘ ğ‘˜ ğ‘›ğ‘šğ‘¡ } ğ‘  ğ‘¡ =1 from a multinomial distribution ğ‘€ (â„(ğ›¼ ğ‘˜ ğ‘›1 ), . . . , â„(ğ›¼ ğ‘˜ ğ‘›ğ‘š )), which give the nonzero entries in the ğ‘›th row of ğµ ğ‘˜ . The reward function ğ‘…(ğº ğ‘˜ ) can be defined as the inverse loss. We can use the reinforcement algorithm to optimize the generator with the gradient:</p><formula xml:id="formula_12">âˆ‡ ğœƒ ğ‘˜ log â„ ğœƒ ğ‘˜ (ğ´ ğ‘˜ )ğ‘…(ğº ğ‘˜ ),<label>(11)</label></formula><p>where ğœƒ ğ‘˜ is the model parameters and â„ ğœƒ ğ‘˜ (ğ´ ğ‘˜ ) = ğ‘› ğ‘  ğ‘¡ =1 â„(ğ‘ ğ‘˜ ğ‘›ğ‘šğ‘¡ ). Optimizing Eq. ( <ref type="formula" target="#formula_8">9</ref>) ensures that the generated graphs have large differences.</p><p>Causal Diffusion. Given the generated interaction graph ğº ğ‘˜ = (ğ´ ğ‘˜ , ğ¼ ) where ğ´ is the adjacency matrix, and ğ¼ is the feature matrix of users or items, instead of directly using ğº ğ‘˜ as input for diffusion, we use the encoder from the Variational Graph Autoencoder (VGAE) to compress ğº ğ‘˜ to a low-dimensional vector x 0 ğ‘˜ âˆ¼ N (ğœ‡ ğ‘˜ , ğœ ğ‘˜ ) for subsequent environment inference and graph invariant representation learning. The encoding process is as follows:</p><formula xml:id="formula_13">ğ‘ ğœ“ (x 0 ğ‘˜ |ğ´ ğ‘˜ , ğ¼ ) = ğ‘ (x 0 ğ‘˜ |ğœ‡ ğ‘˜ , ğœ ğ‘˜ ),<label>(12)</label></formula><p>where ğœ‡ ğ‘˜ = ğºğ¶ğ‘ ğœ‡ (ğ´ ğ‘˜ , ğ¼ ) is matrix of mean vectors and ğœ ğ‘˜ = ğºğ¶ğ‘ ğœ (ğ´ ğ‘˜ , ğ¼ ) denotes standard deviation. The ğºğ¶ğ‘ (â€¢) is the graph convolution network in the graph variational autoencoder. According to the reparameterization trick, x 0 ğ‘˜ can be calculated as follows:</p><formula xml:id="formula_14">x 0 ğ‘˜ = ğœ‡ ğ‘˜ + ğœ ğ‘˜ âŠ™ ğœ–,<label>(13)</label></formula><p>where ğœ– âˆ¼ ğ‘ (0, ğ¼ ) and âŠ™ is the element product. Latent embedding x 0 ğ‘˜ will be used as the input for the environment inference module to generate environment pseudo-labels. Meanwhile, x 0 ğ‘˜ will do the forward and reverse processes in the latent space to learn the user/item embeddings in DDPM. The forward process can be calculated as:</p><formula xml:id="formula_15">ğ‘(x 1:ğ‘‡ ğ‘˜ | x 0 ğ‘˜ ) = ğ‘‡ ğ‘¡ =1 ğ‘(x ğ‘¡ ğ‘˜ | x ğ‘¡ -1 ğ‘˜ ).<label>(14)</label></formula><p>After obtaining the environment approximation variable ğ‘§ ğ‘ğ‘ğ‘¢ğ‘ ğ‘ğ‘™ = ğ‘„ (ğ¸|ğº ğ‘˜ , ğ¼ ) according to Eq. ( <ref type="formula" target="#formula_7">8</ref>), the pair of latent variables (ğ‘§ ğ‘ğ‘ğ‘¢ğ‘ ğ‘ğ‘™ , x ğ‘‡ ğ‘˜ ) to learn the invariant graph representation. We approximate the inference distribution by parameterizing the probabilistic decoder through a conditional DDPM ğ‘ ğœƒ (x ğ‘¡ -1 ğ‘˜ |x ğ‘‡ ğ‘˜ , ğ‘§ ğ‘ğ‘ğ‘¢ğ‘ ğ‘ğ‘™ ). Using DDPM, the forward process is entirely deterministic except for ğ‘¡ = 1. We define the joint distribution of the reverse generative process as follows:</p><formula xml:id="formula_16">ğ‘ ğœƒ (x 0:ğ‘‡ ğ‘˜ | ğ‘§ ğ‘ğ‘ğ‘¢ğ‘ ğ‘ğ‘™ ) = ğ‘ (x ğ‘‡ ğ‘˜ ) ğ‘‡ ğ‘¡ =1 ğ‘ ğœƒ (x ğ‘¡ -1 ğ‘˜ | x ğ‘¡ ğ‘˜ , ğ‘§ ğ‘ğ‘ğ‘¢ğ‘ ğ‘ğ‘™ ).<label>(15)</label></formula><formula xml:id="formula_17">ğ‘ ğœƒ (x ğ‘¡ -1 ğ‘˜ |x) = N x ğ‘¡ -1 ğ‘˜ ; ğœ‡ ğœƒ (x ğ‘¡ ğ‘˜ , ğ‘§ ğ‘ğ‘ğ‘¢ğ‘ ğ‘ğ‘™ , ğ‘¡), Î£ ğœƒ (x ğ‘¡ ğ‘˜ , ğ‘§ ğ‘ğ‘ğ‘¢ğ‘ ğ‘ğ‘™ , ğ‘¡) . (<label>16</label></formula><formula xml:id="formula_18">)</formula><p>The loss function in Eq. ( <ref type="formula" target="#formula_4">5</ref>) can be rewritten as:</p><formula xml:id="formula_19">L Invsample = E ğ‘¡,x 0 ,ğ ğ‘¡ ğ ğ‘¡ -ğ ğœƒ x ğ‘¡ ğ‘˜ , ğ‘§ ğ‘ğ‘ğ‘¢ğ‘ ğ‘ğ‘™ , ğ‘¡ 2 ,<label>(17)</label></formula><p>where</p><formula xml:id="formula_20">x ğ‘¡ ğ‘˜ = âˆš ğ›¼ ğ‘¡ x 0 ğ‘˜ + âˆš 1 -ğ›¼ ğ‘¡ ğ.</formula><p>After obtaining the reconstructed output vector R = x 0 â€² ğ‘˜ from DDPM, it will be used as the input for the decoder of the variational graph decoder, which then reconstructs the input graph. The entire process is illustrated as follows:</p><formula xml:id="formula_21">Ã‚ğ‘˜ = ğœ™ (RR âŠ¤ ),<label>(18)</label></formula><p>where ğœ™ (â€¢) is the activation function (the sigmoid function is used in this paper). The VGAE is optimized by the variational lower bound:</p><formula xml:id="formula_22">L ğ‘‰ ğºğ´ğ¸ = E ğ‘ ğœ“ (x 0 ğ‘˜ |ğ´ ğ‘˜ ,ğ¼ ) log ğ‘ ğœƒ ( Ã‚ğ‘˜ |x 0 ğ‘˜ ) -ğ· ğ¾ğ¿ ğ‘ ğœ“ (x 0 ğ‘˜ |ğ´ ğ‘˜ , ğ¼ ) âˆ¥ ğ‘ (x 0 ğ‘˜ ) . (<label>19</label></formula><formula xml:id="formula_23">)</formula><p>Prediction and Joint Optimization. Using the well-trained diffusion model to sample the final embeddings for user preference modeling:</p><p>rğ‘¢,ğ‘– = ğ‘’ âŠ¤ ğ‘¢ ğ‘’ ğ‘– , (20) where ğ‘’ ğ‘¢ and ğ‘’ ğ‘– denote the final user embedding w.r.t ğ‘¢-th user and item embedding w.r.t ğ‘–-th item, respectively. Without loss of generality, LightGCN is used as the recommendation backbone, and Bayesian Personalized Ranking (BRP) loss is employed to optimize the model parameters:</p><formula xml:id="formula_24">L ğ‘Ÿğ‘’ğ‘ = âˆ‘ï¸ ğ‘¢,ğ‘£ + ,ğ‘£ - -log ğœ ( rğ‘¢,ğ‘£ + -rğ‘¢,ğ‘£ -),<label>(21)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>â€¦</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Original Graph Graph Encoder</head><p>Environment Generator</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>â€¦ â€¦ â€¦</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Causal Diffusion</head><p>Graph Decoder</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>â€¦</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Causal Diffusion</head><p>Training Process where (ğ‘¢, ğ‘£ + , ğ‘£ -) is a triplet sample for pairwise recommendation training. ğ‘£ + represents positive samples from which the user has interacted, and ğ‘£ -are the negative samples that are randomly drawn from the set of items with which the user has not interacted, respectively. We use a joint learning strategy to optimize CausalDiffRec:</p><formula xml:id="formula_25">1 ) (x | x t t k k q 1 G 2 G k G k G 2 G 1 G 1 1 ( , ) N ï­ ï³ ( , ) k k N ï­ ï³ t steps ( , ) k k N ï­ ï³ 1 1 ( , ) N ï­ ï³ ( | , , ) P Y G E I ï± ( | , ) Q E e G I ï¦ 1 (x | x , z ) t t k k causal p ï± causal z 1 ) (x | x</formula><formula xml:id="formula_26">L =L ğ‘Ÿğ‘’ğ‘ + ğœ† 1 â€¢ L generator + ğœ† 2 â€¢ (L VGAE + L Invsample ) + ğœ† 3 â€¢ L envInf ,<label>(22)</label></formula><p>where ğœ† 1 , ğœ† 2 , and ğœ† 3 are hyper-parameters. We provide rigorous theoretical proof in appendix A that optimizing the loss function in Eq. ( <ref type="formula" target="#formula_26">22</ref>) can encourage the model to learn environment-invariant graph representations, thereby achieving generalization in out-ofdistribution data recommendations. Model complexity analysis is provided in Appendix F.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In this section, we conducted extensive experiments to validate the performance of CausalDiffRec and address the following key research questions:</p><p>â€¢ RQ1: How does CausalDiffRec compare to the state-of-the-art strategies in both OOD and IID test evaluations? â€¢ RQ2: Are the proposed components of CausalDiffRec effective for OOD generalization? â€¢ RQ3: How do hyperparameter settings affect the performance of CausalDiffRec?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Settings</head><p>Datasets.We evaluate the performance of our proposed CausalD-iffRec method under three common data distribution shifts across four real-world datasets: Food 1 , KuaiRec 2 Yelp2018 3 , and Douban 4 comprise raw data from the Douban system. Detailed statistics of the datasets are presented in Table <ref type="table" target="#tab_4">1</ref>. The detailed information and processing specifics of the dataset can be found in Appendix C.</p><p>Baselines. We compare the CusalDiffRec with the state-of-theart models: LightGCN <ref type="bibr" target="#b39">[40]</ref>, SGL <ref type="bibr" target="#b39">[40]</ref>, SimGCL <ref type="bibr" target="#b49">[50]</ref>, LightGCL <ref type="bibr" target="#b0">[1]</ref>, InvPref <ref type="bibr" target="#b38">[39]</ref>, InvCF <ref type="bibr" target="#b54">[55]</ref>, AdvDrop <ref type="bibr" target="#b52">[53]</ref>, AdvInfo <ref type="bibr" target="#b53">[54]</ref>, and DR-GNN <ref type="bibr" target="#b27">[28]</ref>. Appendix E presents the detailed information of the baselines. This section compares CausalDiffRec's performance and baselines under various data shifts and conducts a performance analysis.</p><p>Evaluation on temporal shift: Table <ref type="table" target="#tab_4">1</ref> shows that CausalD-iffRec significantly outperforms SOTA models on the Food dataset, with improvements of 1.99%, 24.89%, 6.03%, and 9.68% in Recall and NDCG. This indicates CausalDiffRec's effectiveness in handling temporal shift. DRO also excels in this area, with a 15% improvement over LightGCN in NDCG@20, due to its robust optimization across various data distributions. CDR surpasses GNN-based models thanks to its temporal VAE-based architecture, capturing preference shifts from temporal changes.</p><p>Evaluation on exposure shift: In real-world scenarios, only a small subset of items is exposed to users, leading to non-random missing interaction records. Using the fully exposed KuaiRec dataset, CausalDiffRec consistently outperforms baselines, with improvements ranging from 6.90% to 28.83%, indicating its capability to handle exposure bias. DRO and AdvInfoNce also show superior performance in NDCG and Recall metrics, enhancing the generalization of GNN-based models and demonstrating robustness compared to LightGCN.</p><p>Evaluation on popularity shift. We compare model performance on the Yelp2018 and Douban datasets, showing that our model significantly outperforms the baselines. On Douban, CausalD-iffRec achieves 8.22% to 17.96% improvement, and on Yelp2018, the improvements range from 11.24% to 36.73%. Methods using contrastive learning (e.g., SimGCL, LightGCL, AdvInfoNce) outperform other baselines in handling popularity shifts. This is because the InfoNCE loss helps the model learn a more uniform representation distribution, reducing bias towards popular items. InvPref performs best among the baselines on Yelp2018, using clustering for contextual labels, unlike our variational inference approach. Our method, tailored for graph data, aggregates neighbor information for better recommendation performance than matrix factorization-based methods.</p><p>Additionally, in Table <ref type="table" target="#tab_11">4</ref>, we report the performance of CausalD-iffRec compared to several baseline models that use LightGCN as the backbone. From the table, we observe the following: 1) These baseline models outperform LightGCN on IID datasets; 2) CausalDiffRec outperforms all baseline models across all metrics. This indicates that CausalDiffRec also performs well on IID datasets. We attribute 0.0044 0.0070 0.0023 0.0031 CausalDiffRec 0.0094 0.0197 0.0050 0.0079 the performance improvement to our use of data augmentation and the incorporation of auxiliary information in modeling user preferences.</p><p>In summary, the analysis of experimental results demonstrates that our proposed CausalDiffRec can handle different types of distribution shifts and achieve good generalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">In-depth Analysis (RQ2)</head><p>In this section, we conduct ablation experiments to study the impact of each component of CausalDiffRec on recommendation performance. The main components include the environment generator module and the environment inference module. Additionally, we use t-SNE to visualize the item representations captured by the baseline model and CausalDiffRec, to compare the models' generalization capabilities on OOD data.</p><p>Ablation studies. Table <ref type="table" target="#tab_5">2</ref> presents the results of the ablation study that compares LightGCN, CausalDiffRec, and its two variants: 'w/o Gen. ' (without the environment generator) and 'w/o Env. ' (without the environment inference). The results show that removing these modules causes a significant drop in all metrics across four datasets. For example, on Yelp2018, Recall, and NDCG decreased by 148.15% and 126.83%, respectively, demonstrating the effectiveness of CausalDiffRec based on invariant learning theory for enhancing recommendation performance on OOD datasets. Additionally, even without the modules, CausalDiffRec still outperforms LightGCN on popularity shift datasets (Yelp2018 and Douban) due to the effectiveness of data augmentation and environment inference. However, on the Food and KuaiRec datasets, removing either module results in worse performance than LightGCN, likely due to multiple biases in these datasets. Without one module, the model struggles to handle multiple data distributions, leading to a performance drop. Overall, the ablation experiments highlight the importance of all modules in CausalDiffRec for improving recommendation performance and generalizing on OOD data.</p><p>Visualization analysis. In Figure <ref type="figure" target="#fig_14">6</ref> and Figure <ref type="figure" target="#fig_15">7</ref>, we used t-SNE to visualize the item representations learned by LightGCN, SimGCL, and CausalDiffRec on Douban and Yelp2018 datasets to better observe our model's ability to handle distribution shifts. Following previous work <ref type="bibr" target="#b27">[28]</ref>, we recorded the popularity of each item in the training set and designated the top 10% most popular items as 'popular items' and the bottom 10% as 'unpopular items'. It is obvious that the embeddings of popular and unpopular items learned by LightGCN still exhibit a gap in the representation space. In contrast, the embeddings learned by CausalDiffRec are more evenly distributed within the same space. This indicates that CausalDiffRec Recall@20 NDCG@20</p><p>Recall@20 NDCG@20 (a) Food Recall@20 NDCG@20</p><p>Recall@20 NDCG@20 (b) KuaiRec Recall@20 NDCG@20</p><p>Recall@20 NDCG@20 (c) Yelp2018 (c) Yelp2018 (f) Yelp2018 can mitigate the popularity shift caused by popular items. Additionally, we found that the embeddings of popular and unpopular items learned by SimGCL are more evenly distributed compared to LightGCN. This is because contrastive learning can learn a uniform representation distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Hyperparameter Investigation (RQ3)</head><p>Effect of Diffusion Step T. We conducted experiments to investigate the impact of the number of diffusion steps on performance; in CausalDiffRec, we used the same number of steps in the forward and reverse phases. we compare the performance with ğ‘‡ changing from 10 to 500. We present the results in Figure <ref type="figure" target="#fig_7">4</ref>, and we have the following findings:</p><p>â€¢ When the number of steps is chosen within the range {10, 50, 100}, CausalDiffRec achieves the best performance across all datasets. We find that appropriately increasing the number of steps significantly improves Recall@20 and NDCG@20. These performance enhancements are mainly attributed to the diffusion enriching the representation capabilities of users and items. â€¢ Nevertheless, as we continue to increase the number of steps, the model will face overfitting issues. For example, on the food and yelp2018 datasets, Recall@20 and NDCG@20 consistently decrease. Although there is an upward trend on KuaiRec, the optimal solution is not achieved. Additionally, it is evident that more steps also lead to longer training times. We should carefully adjust the number of steps to find the optimal balance between enhancing representation ability and avoiding overfitting.</p><p>Effect of the number of Environments. Figure <ref type="figure" target="#fig_8">5</ref> shows the impact of the number of environments on the model's performance. We can see that as the number of environments increases, the performance of CausalDiffRec improves across the three datasets. This indicates that more environments help enhance the model's generalization on OOD (Out-of-Distribution) data. However, as the number of environments further increases, performance declines, which we believe is due to the model overfitting to too many environments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RELATED WORK 5.1 GNN-based Recommendation</head><p>Recent developments in graph-based recommender systems have leveraged graph neural networks to model user-item interactions as a bipartite graph, enhancing recommendation accuracy through complex interaction capture <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b42">43]</ref>. Notably, LightGCN focuses on neighborhood aggregation without additional transformations, while other approaches employ attention mechanisms to prioritize influential interactions <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37]</ref>. Further research explores non-Euclidean spaces like hyperbolic space to better represent user-item relationships <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b55">56]</ref>. Knowledge graphs also enhance these systems by integrating rich semantic and relational data directly into the recommendation process <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b33">34]</ref>. Despite these advancements, graph-based systems often struggle with outof-distribution data due to the IID assumption and are challenged by multiple distribution shifts <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b49">50]</ref>. Additionally, contrastive learning methods in these systems rely on a fixed paradigm that lacks robust theoretical support, limiting adaptability to varied data shifts. However, the aforementioned models are trained on datasets where the training and test data distributions are drawn from the same distribution, leading to generalization failure when facing OOD data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Diffusion based Recoomendation</head><p>The integration of diffusion processes into recommender systems leverages diffusion mechanisms to model dynamic propagation of user preferences and item information through interaction networks, enhancing recommendation accuracy and timeliness <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b44">45]</ref>. These models capture evolving user behaviors and have shown potential in various recommendation contexts, from sequential recommendations to location-based services. For instance, DiffRec <ref type="bibr" target="#b32">[33]</ref> applies diffusion directly for recommendations, while Diff-POI <ref type="bibr" target="#b25">[26]</ref> models location preferences. Furthermore, approaches like DiffKG <ref type="bibr" target="#b15">[16]</ref> and RecDiff <ref type="bibr" target="#b21">[22]</ref> utilize diffusion for denoising entity representations in knowledge graphs and user data in social recommendations, respectively, enhancing the robustness and reliability of the systems. These studies underscore diffusion's suitability for advanced representation learning in recommender systems. However, these methods cannot solve the OOD problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Out-of-Distribution Recommendation</head><p>Researchers have focused on recommendation algorithms for outof-distribution (OOD) data. COR <ref type="bibr" target="#b30">[31]</ref> infers latent environmental factors in OOD data. CausPref <ref type="bibr" target="#b10">[11]</ref> learns invariant user preferences and causal structures using anti-preference negative sampling. CaseQ <ref type="bibr" target="#b46">[47]</ref> employs backdoor adjustment and variational inference for sequential recommendations. InvPref <ref type="bibr" target="#b38">[39]</ref> separates invariant and variant preferences by identifying heterogeneous environments. However, these methods don't directly apply to graph-based recommendation models and fail to address OOD in graph structures. AdaDrop <ref type="bibr" target="#b52">[53]</ref> uses adversarial learning and graph neural networks to enhance performance by decoupling user preferences. DRO <ref type="bibr" target="#b27">[28]</ref> integrates Distributionally Robust Optimization into Graph Neural Networks to handle distribution shifts in graph-based recommender systems. Distinct from these GNN-based methods, this paper explores how to use the theory of invariant learning to design GNNbased methods with good generalization capabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>This paper introduces CausalDiffRec, an innovative GNN-based model designed for OOD recommendation. CausalDiffRec aims to learn environment-invariant graph representations to improve model generalization on OOD data. It utilizes the backdoor criterion from causal inference and variational inference to mitigate environmental confounders, alongside a diffusion-based sampling strategy. Rooted in invariant learning theory, we theoretically demonstrate that optimizing CausalDiffRec's objective function enhances its ability to identify invariant graph representations, boosting generalization on OOD data. Experiments on four real-world datasets show CausalDiffRec surpasses baseline models, with ablation studies confirming its effectiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A THEORETICAL PROOF</head><p>CausalDiffRec aims to learn the optimal generator ğ¹ * (â€¢) as stated in the assumption in section 2.3, thereby obtaining invariant graph representations to achieve OOD generalization in recommendation performance under data distribution shifts. Before starting the theoretical derivation, let's do some preliminary work. For the convenience of theoretical proof, we rewrite Eq. ( <ref type="formula" target="#formula_26">22</ref>) as:</p><formula xml:id="formula_27">argmin ğœƒ (L task + L infer ),<label>(23)</label></formula><p>where L ğ‘¡ğ‘ğ‘ ğ‘˜ = L ğ‘Ÿğ‘’ğ‘ + L generator + L VGAE + L Invsample and L infer = L envInf , and for the derivation convenience, we temporarily ignore the penalty coefficient. The L task and L infer can be further abstracted as:</p><formula xml:id="formula_28">L task = arg min ğœƒ E ğ‘’âˆ¼ğ· ğ‘¡ğ‘Ÿ (ğ¸ ),( G ğ‘¢ ,ğ‘¦ ğ‘¢ )âˆ¼ğ‘ƒ (ğ‘Œ ,ğº |ğ¸=ğ‘’ ) [ğ‘™ (ğ‘“ ğœƒ (G ğ‘¢ ; ğœƒ ), ğ‘¦ ğ‘¢ )] L infer = min ğ‘ (ğ‘Œ |ğ‘§ ğ‘ğ‘ğ‘¢ğ‘ ğ‘ğ‘™ ) Var{E ğ‘’âˆ¼ğ· ğ‘¡ğ‘Ÿ (ğ¸ ),( G ğ‘¢ ,ğ‘¦ ğ‘¢ )âˆ¼ğ‘ƒ (ğ‘Œ ,ğº |ğ¸=ğ‘’ ) [ğ‘™ (ğ‘“ ğœƒ (G ğ‘¢ ; ğœƒ ), ğ‘¦ ğ‘¢ )|ğ‘‘ğ‘œ (G ğ‘¢ )]}.<label>(24)</label></formula><p>We follow the proof technique from <ref type="bibr" target="#b51">[52]</ref> and show the optimality of the Eq. ( <ref type="formula" target="#formula_27">23</ref>) with the following two propositions that can achieve OOD recommendation.</p><p>Proposition A.1. Minimizing Eq. ( <ref type="formula" target="#formula_27">23</ref>) promotes the model's adherence to the Invariance Property and the Sufficient Condition outlined in Assumption (in Sec. 2.3).</p><p>Proposition A.2. Optimizing Eq. ( <ref type="formula" target="#formula_27">23</ref>) corresponds to minimizing the upper bound of the OOD generalization error described in Eq. ( <ref type="formula" target="#formula_5">6</ref>).</p><p>Proposition A.1 and Proposition A.2, respectively, avoid strong hypotheses and ensure that the OOD generalization error bound of the learned model is within the expected range. In fact, this can also be explained from the perspective of the SCM model in Figure <ref type="figure" target="#fig_1">2</ref>. Optimizing Eq. ( <ref type="formula" target="#formula_5">6</ref>) eliminates the negative impact of unstable correlations learned by the model, which are caused by latent environments, on modeling user preferences. At the same time, it enhances the model's ability to learn invariant causal features across different latent environments. Proofs for Proposition A.1 and Proposition A.2 are shown as follows:</p><p>Before starting the proof, we directly follow previous work <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b51">52]</ref> to propose the following lemma, using information theory to interpret the invariance property and sufficient condition in Assumption and to assist in the proof of Proposition A.1. Using the Mutual Information I(; ), the invariance property and sufficient condition in Assumption can be equivalently expressed as follow lemma:</p><p>Lemma A. </p><p>For the sufficient condition, we employ the method of contradiction and prove it through the following two steps: First, we prove that for ğ‘Œ , P * ğ¼ğ‘›ğ‘£ , and ğ¼ satisfying P * ğ¼ğ‘›ğ‘£ = arg max P ğ¼ğ‘›ğ‘£ I(ğ‘Œ ; P ğ¼ğ‘›ğ‘£ , ğ¼ ), they also satisfy that I(ğ‘Œ ; P * ğ¼ğ‘›ğ‘£ , ğ¼ ) is maximized. We leverage the method of contradiction to prove this. Assume P * ğ¼ğ‘›ğ‘£ â‰  arg max P ğ¼ğ‘›ğ‘£ I(ğ‘Œ ; P ğ¼ğ‘›ğ‘£ , ğ¼ ), and there exists P â€² ğ¼ğ‘›ğ‘£ = arg max P ğ¼ğ‘›ğ‘£ I(ğ‘Œ ; P ğ¼ğ‘›ğ‘£ , ğ¼ ), where P â€² ğ¼ğ‘›ğ‘£ â‰  P * ğ¼ğ‘›ğ‘£ . We can always find a mapping function ğ‘€ such that P â€² ğ¼ğ‘›ğ‘£ = ğ‘€ (P * ğ¼ğ‘›ğ‘£ , ğ‘…), where ğ‘… is a random variable. Then we have:</p><formula xml:id="formula_30">I(ğ‘Œ ; P â€² ğ¼ğ‘›ğ‘£ , ğ¼ ) = I(ğ‘Œ ; P * ğ¼ğ‘›ğ‘£ , ğ‘…, ğ¼ ) = I(ğ‘Œ ; P * ğ¼ğ‘›ğ‘£ , ğ¼ ) + I(ğ‘Œ ; ğ‘…|P * ğ¼ğ‘›ğ‘£ , ğ¼ ). (<label>26</label></formula><formula xml:id="formula_31">)</formula><p>Since ğ‘… is a random variable and does not contain any information about ğ‘Œ , we have I(ğ‘Œ ; ğ‘…|P * ğ¼ğ‘›ğ‘£ , ğ¼ ) = 0. Therefore: </p><formula xml:id="formula_32">I</formula><p>From this, we can deduce that:</p><formula xml:id="formula_34">P â€² ğ¼ğ‘›ğ‘£ = arg max P ğ¼ğ‘›ğ‘£ I(ğ‘Œ ; P ğ¼ğ‘›ğ‘£ , ğ¼ ),<label>(29)</label></formula><p>where contradicts P * ğ¼ğ‘›ğ‘£ = arg max P ğ¼ğ‘›ğ‘£ I(ğ‘Œ ; P ğ¼ğ‘›ğ‘£ , ğ¼ ). Since the assumption leads to a contradiction, and the assumption does not hold. Therefore, P * ğ¼ğ‘›ğ‘£ = arg max P ğ¼ğ‘›ğ‘£ I(ğ‘Œ ; P ğ¼ğ‘›ğ‘£ , ğ¼ ) holds. This proves that I(ğ‘Œ ; P * ğ¼ğ‘›ğ‘£ , ğ¼ ) is maximized. The lemma A.3 is complicated proven. Proof of Proposition A.1. First, optimizing the first term L task in Eq. ( <ref type="formula" target="#formula_27">23</ref>) enables the model to satisfy the sufficient condition. Analyzing the SCM in Figure <ref type="figure" target="#fig_1">2</ref> </p><p>Based on the above derivation, we have:</p><formula xml:id="formula_36">I(ğ‘Œ , ğº |ğ‘§ ğ‘ğ‘ğ‘¢ğ‘ ğ‘ğ‘™ ) â‰¤ min ğ‘ (ğ‘Œ |ğ‘§ ğ‘ğ‘ğ‘¢ğ‘ ğ‘ğ‘™ ) ğ· ğ¾ğ¿ (ğ‘ (ğ‘Œ |ğº, ğ¸)âˆ¥ğ‘ (ğ‘Œ |ğ‘§ ğ‘ğ‘ğ‘¢ğ‘ ğ‘ğ‘™ )).<label>(31)</label></formula><p>Besides, we have: (33) Thus, we have demonstrated that minimizing the expectation term (L ğ‘¡ğ‘ğ‘ ğ‘˜ ) in Eq. ( <ref type="formula" target="#formula_27">23</ref>) is equivalent to minimizing the upper bound of I(ğ‘Œ ; ğº | ğ‘§ ğ‘ğ‘ğ‘¢ğ‘ ğ‘ğ‘™ ). This results in maximizing I(ğ‘Œ ; P * ğ¼ğ‘›ğ‘£ , ğ¼ ), thereby helping to ensure that the model satisfies the Sufficient Condition.</p><formula xml:id="formula_37">ğ· ğ¾ğ¿ (ğ‘ (ğ‘Œ |ğº, ğ¸) âˆ¥ğ‘ (ğ‘Œ |ğ‘§ ğ‘ğ‘ğ‘¢ğ‘ ğ‘ğ‘™ )) = E ğ‘’ âˆˆğ· ğ‘¡ğ‘Ÿ (ğ¸ ) E (ğº,</formula><p>Next, we prove that optimizing the first term L ğ‘¡ğ‘ğ‘ ğ‘˜ in Eq. ( <ref type="formula" target="#formula_27">23</ref>) enables the model to satisfy the Invariance Property. Similar to Eq. ( <ref type="formula" target="#formula_35">30</ref>), we have:</p><formula xml:id="formula_38">I(ğ‘Œ ; ğ¸ = ğ‘’ | ğ‘§ ğ‘ğ‘ğ‘¢ğ‘ ğ‘ğ‘™ ) = ğ· ğ¾ğ¿ (ğ‘ (ğ‘Œ | ğ‘§ ğ‘ğ‘ğ‘¢ğ‘ ğ‘ğ‘™ , ğ‘’) âˆ¥ ğ‘ (ğ‘Œ | ğ‘§ ğ‘ğ‘ğ‘¢ğ‘ ğ‘ğ‘™ )) = ğ· ğ¾ğ¿ (ğ‘ (ğ‘Œ | ğ‘§ ğ‘ğ‘ğ‘¢ğ‘ ğ‘ğ‘™ , ğ¸) âˆ¥ E ğ‘’ âˆˆğ· (ğ¸ ) [ğ‘ (ğ‘Œ | ğ‘§ ğ‘ğ‘ğ‘¢ğ‘ ğ‘ğ‘™ , ğ‘’)]) = ğ· ğ¾ğ¿ (ğ‘(ğ‘Œ | ğ‘§ ğ‘ğ‘ğ‘¢ğ‘ ğ‘ğ‘™ ) âˆ¥ E ğ‘’ âˆˆğ· (ğ¸ ) [ğ‘(ğ‘Œ | ğ‘§ ğ‘ğ‘ğ‘¢ğ‘ ğ‘ğ‘™ )]) -ğ· ğ¾ğ¿ (ğ‘(ğ‘Œ | ğ‘§ ğ‘ğ‘ğ‘¢ğ‘ ğ‘ğ‘™ ) âˆ¥ ğ‘ (ğ‘Œ | ğ‘§ ğ‘ğ‘ğ‘¢ğ‘ ğ‘ğ‘™ , ğ‘’)) -ğ· ğ¾ğ¿ (E ğ‘’ âˆˆğ· (ğ¸ ) [ğ‘ (ğ‘Œ | ğ‘§ ğ‘ğ‘ğ‘¢ğ‘ ğ‘ğ‘™ , ğ‘’)] âˆ¥ E ğ‘’ âˆˆğ· (ğ¸ ) [ğ‘(ğ‘Œ | ğ‘§ ğ‘ğ‘ğ‘¢ğ‘ ğ‘ğ‘™ )]) â‰¤ ğ· ğ¾ğ¿ (ğ‘(ğ‘Œ | ğ‘§ ğ‘ğ‘ğ‘¢ğ‘ ğ‘ğ‘™ ) âˆ¥ E ğ‘’ âˆˆğ· (ğ¸ ) [ğ‘(ğ‘Œ | ğ‘§ ğ‘ğ‘ğ‘¢ğ‘ ğ‘ğ‘™ )]).<label>(34)</label></formula><p>Besides, the last term in Eq. ( <ref type="formula" target="#formula_38">34</ref>) can be further expressed as:</p><formula xml:id="formula_39">ğ· ğ¾ğ¿ (ğ‘(ğ‘Œ | ğ‘§ ğ‘ğ‘ğ‘¢ğ‘ ğ‘ğ‘™ ) âˆ¥ E ğ‘’ âˆˆğ· (ğ¸ ) [ğ‘(ğ‘Œ | ğ‘§ ğ‘ğ‘ğ‘¢ğ‘ ğ‘ğ‘™ )]) = E ğ‘’ âˆˆğ· ğ‘¡ğ‘Ÿ (ğ¸ ) E (ğº,ğ‘Œ )âˆ¼ğ‘ (ğº,ğ‘Œ |ğ‘’ ) E ğ‘§ ğ‘ğ‘ğ‘¢ğ‘ ğ‘ğ‘™ âˆ¼ğ‘ (ğ‘§ ğ‘ğ‘ğ‘¢ğ‘ ğ‘ğ‘™ |ğº,ğ¼ ) log ğ‘ (ğ‘Œ |ğº, ğ‘’) E ğ‘’ âˆˆğ· (ğ¸ ) ğ‘(ğ‘Œ |ğ‘§ ğ‘ğ‘ğ‘¢ğ‘ ğ‘ğ‘™ ) (Jensen Inequality) â‰¤ E ğ‘’ âˆˆğ· (ğ¸ ) [|ğ‘™ (ğ‘“ ğœƒ (G ğ‘¢ ; ğœƒ ), ğ‘¦ ğ‘¢ ) -E ğ‘’ âˆˆğ· (ğ¸ ) [ğ‘™ (ğ‘“ ğœƒ (G ğ‘¢ ; ğœƒ ), ğ‘¦ ğ‘¢ )] |]<label>(35)</label></formula><p>where the last term in Eq. ( <ref type="formula" target="#formula_39">35</ref>) is the upper bound for the ğ· ğ¾ğ¿ (ğ‘(ğ‘Œ | ğ‘§ ğ‘ğ‘ğ‘¢ğ‘ ğ‘ğ‘™ ) âˆ¥ E ğ‘’ âˆˆğ· (ğ¸ ) [ğ‘(ğ‘Œ | ğ‘§ ğ‘ğ‘ğ‘¢ğ‘ ğ‘ğ‘™ )]). Finally, we have:</p><formula xml:id="formula_40">min ğ‘ (ğ‘Œ |ğ‘§ ğ‘ğ‘ğ‘¢ğ‘ ğ‘ğ‘™ ) ğ· ğ¾ğ¿ (ğ‘(ğ‘Œ | ğ‘§ ğ‘ğ‘ğ‘¢ğ‘ ğ‘ğ‘™ ) âˆ¥ E ğ‘’ âˆˆğ· (ğ¸ ) [ğ‘(ğ‘Œ | ğ‘§ ğ‘ğ‘ğ‘¢ğ‘ ğ‘ğ‘™ )]) â‡” min ğ‘ (ğ‘Œ |ğ‘§ ğ‘ğ‘ğ‘¢ğ‘ ğ‘ğ‘™ ) Var{E ğ‘’âˆ¼ğ· ğ‘¡ğ‘Ÿ (ğ¸ ),( G ğ‘¢ ,ğ‘¦ ğ‘¢ )âˆ¼ğ‘ƒ (ğ‘Œ ,ğº |ğ¸=ğ‘’ ) [ğ‘™ (ğ‘“ ğœƒ (G ğ‘¢ ; ğœƒ ), ğ‘¦ ğ‘¢ )|ğ‘‘ğ‘œ (G ğ‘¢ )]}.</formula><p>(36) Hence, minimizing the variance term (L ğ‘Ÿğ‘–ğ‘ ğ‘˜ ) in Eq. ( <ref type="formula" target="#formula_27">23</ref>) effectively reduces the upper bound of I(ğ‘Œ ; ğ¸ = ğ‘’ | ğ‘§ ğ‘ğ‘ğ‘¢ğ‘ ğ‘ğ‘™ ). Thereby ensuring the model adheres to the Invariance Property.</p><p>Proof of Proposition A.2. Optimizing Eq. ( <ref type="formula" target="#formula_27">23</ref>) is tantamount to reducing the upper bound of the OOD generalization error in Eq. ( <ref type="formula" target="#formula_5">6</ref>). Let ğ‘(ğ‘Œ | ğº) represent the inferred variational distribution of the true distribution ğ‘ (ğ‘Œ | ğº, ğ¸). The OOD generalization error can be quantified by the KL divergence between these two distributions:</p><formula xml:id="formula_41">ğ· ğ¾ğ¿ (ğ‘ (ğ‘Œ |ğº, ğ¸)âˆ¥ğ‘(ğ‘Œ |ğº)) = E ğ‘’ âˆˆğ· ğ‘¡ğ‘Ÿ (ğ¸ ) E (ğ‘Œ ,ğº )âˆ¼ğ‘ (ğ‘Œ ,ğº |ğ¸ ) log ğ‘ (ğ‘Œ |ğº, ğ¸ = ğ‘’) ğ‘(ğ‘Œ |ğº) .<label>(37)</label></formula><p>Following previous work, we use information theory to assist in the proof of Proposition A.2. We propose the lemma A.4 to rewrite the OOD generalization, which is shown as follows:</p><p>Lemma A.4. The out-of-distribution generalization error is limited by: <ref type="bibr" target="#b37">(38)</ref> where ğ‘(ğ‘Œ |ğ‘§ ğ‘ğ‘ğ‘¢ğ‘ ğ‘ğ‘™ ) is the inferred variational environment distribution. The proof of Lemma A.4 is shown as:</p><formula xml:id="formula_42">ğ· ğ¾ğ¿ (ğ‘ (ğ‘Œ |ğº, ğ¸)âˆ¥ğ‘(ğ‘Œ |ğº)) â‰¤ ğ· ğ¾ğ¿ [ğ‘ (ğ‘Œ |ğº, ğ¸)âˆ¥ğ‘(ğ‘Œ |ğ‘§ ğ‘ğ‘ğ‘¢ğ‘ ğ‘ğ‘™ )],</formula><formula xml:id="formula_43">ğ· ğ¾ğ¿ (ğ‘ (ğ‘Œ |ğº, ğ¸ = ğ‘’)âˆ¥ğ‘(ğ‘Œ |ğ‘§ ğ‘ğ‘ğ‘¢ğ‘ ğ‘ğ‘™ )) = E ğ‘’ âˆˆğ· (ğ¸ ) E (ğ‘Œ ,ğº )âˆ¼ğ‘ (ğº,ğ‘Œ |ğ¸=ğ‘’ ) log ğ‘ (ğ‘Œ |ğº, ğ¸ = ğ‘’) ğ‘(ğ‘Œ |ğº) = E ğ‘’ âˆˆğ· (ğ¸ ) E (ğ‘Œ ,ğº )âˆ¼ğ‘ (ğº,ğ‘Œ |ğ¸=ğ‘’ ) log ğ‘ (ğ‘Œ |ğº, ğ¸ = ğ‘’) E ğ‘§ ğ‘ğ‘ğ‘¢ğ‘ ğ‘ğ‘™ âˆ¼ğ‘ (ğ‘§ ğ‘ğ‘ğ‘¢ğ‘ ğ‘ğ‘™ |ğº,ğ¼ ) ğ‘(ğ‘Œ |ğ‘§ ğ‘ğ‘ğ‘¢ğ‘ ğ‘ğ‘™ ) â‰¤ E ğ‘’ âˆˆğ· (ğ¸ ) E (ğ‘Œ ,ğº )âˆ¼ğ‘ (ğº,ğ‘Œ |ğ¸=ğ‘’ ) E ğ‘§ ğ‘ğ‘ğ‘¢ğ‘ ğ‘ğ‘™ âˆ¼ğ‘ (ğ‘§ ğ‘ğ‘ğ‘¢ğ‘ ğ‘ğ‘™ |ğº,ğ¼ ) log ğ‘ (ğ‘Œ |ğº, ğ‘’) ğ‘(ğ‘Œ |ğ‘§ ğ‘ğ‘ğ‘¢ğ‘ ğ‘ğ‘™ ) = ğ· ğ¾ğ¿ [ğ‘ (ğ‘Œ |ğº, ğ¸)âˆ¥ğ‘(ğ‘Œ |ğ‘§ ğ‘ğ‘ğ‘¢ğ‘ ğ‘ğ‘™ )],<label>(39)</label></formula><p>The Lemma A.4 has been fully proven. Based on Lemma A.3 and Proposition A.1, the Eq. ( <ref type="formula" target="#formula_27">23</ref>) can be adapted as:  Hence, according to Lemma A.4, we confirm that minimizing Eq. ( <ref type="formula" target="#formula_27">23</ref>) is equivalent to minimizing the upper bound of the OOD generalization error in Eq. ( <ref type="formula" target="#formula_5">6</ref>), meaning that: </p><formula xml:id="formula_44">argmin ğœƒ (L ğ‘¡ğ‘ğ‘ ğ‘˜ + L ğ‘–ğ‘›ğ‘“ ğ‘’ğ‘Ÿ ) â‡” min ğ‘ (ğ‘§ ğ‘ğ‘ğ‘¢ğ‘ ğ‘ğ‘™ |ğº,ğ¼ ),ğ‘ (ğ‘Œ ,ğ‘§ ğ‘ğ‘ğ‘¢ğ‘ ğ‘ğ‘™ ) ğ· ğ¾ğ¿ (ğ‘ (ğ‘Œ |ğº, ğ¸ = ğ‘’)âˆ¥ğ‘(ğ‘Œ |ğ‘§ ğ‘ğ‘ğ‘¢ğ‘ ğ‘ğ‘™ )) + I(ğ‘Œ , ğ¸ = ğ‘’ |ğ‘§ ğ‘ğ‘ğ‘¢ğ‘ ğ‘ğ‘™ ) (I(ğ‘Œ, ğ¸ = ğ‘’ |ğ‘§ ğ‘ğ‘ğ‘¢ğ‘ ğ‘ğ‘™ ) ğ‘–ğ‘  ğ‘›ğ‘œğ‘› -ğ‘›ğ‘’ğ‘”ğ‘ğ‘¡ğ‘–ğ‘£ğ‘’) â‰¥ min ğ‘ (ğ‘§ ğ‘ğ‘ğ‘¢ğ‘ ğ‘ğ‘™ |ğº,ğ¼ ),ğ‘ (ğ‘Œ ,ğ‘§ ğ‘ğ‘ğ‘¢ğ‘ ğ‘ğ‘™ ) ğ· ğ¾ğ¿ (ğ‘ (ğ‘Œ |ğº, ğ¸ = ğ‘’)âˆ¥ğ‘(ğ‘Œ |ğ‘§ ğ‘ğ‘ğ‘¢ğ‘ ğ‘ğ‘™ )) â‰¥ ğ· ğ¾ğ¿ (ğ‘ (ğ‘Œ |ğº, ğ¸) âˆ¥ğ‘(ğ‘Œ |ğº)).<label>(41</label></formula><p>B.2 Derivation for Equation <ref type="formula" target="#formula_7">8</ref>Taking the logarithm on both sides of Eq. ( <ref type="formula" target="#formula_7">8</ref>) and according to Jensen's Inequality, we have:   C DATASET DETAIL Processing Details. We retain only those users with at least 15 interactions on the Food dataset, at least 25 interactions on the Yelp2018 and Douban datasets, and items with at least 50 interactions on these datasets. For all three datasets, only interactions with ratings of 4 or higher are considered positive samples. For the KuaiRec dataset, interactions with a watch ratio of 2 or higher are considered positive samples.</p><p>We directly follow <ref type="bibr" target="#b27">[28]</ref> to process the dataset above to construct three common types of out-of-distribution data: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D HYPERPARAMETER SETTINGS</head><p>We implement our CausalDiffRec in Pytorch. All experiments are conducted on a single RTX-4090 with 24G memory. Following the default hyperparameter search settings of the baselines, we expand their hyperparameter search space and tune the hyperparameters.</p><p>For our CausalDiffRec, we tune the learning rates in {1ğ‘’ -3, 1ğ‘’ - for all ğ‘˜ âˆˆ {1, 2, . . . , ğ¾ } do 5:</p><p>Get the modified modified graphs ğº ğ‘˜ by Eq. ( <ref type="formula" target="#formula_8">9</ref>); 6:</p><p>Infer the causal environment label ğ‘§ ğ‘ğ‘ğ‘¢ğ‘ ğ‘ğ‘™ from Eq. (43); 7:</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Left and Middle: An example illustrates the popularity distribution shift, i.e., how the popularity of masks, disinfectants, exercise equipment, and electronic products changes with the COVID-19 pandemic. Right: We constructed both IID and OOD sets on the Yelp2018 dataset and compared the performance of the LightGCN model [9] on these datasets.We found a significant average performance drop (i.e., 29.03%) in OOD data across three metrics. correlations, hindering their generalization to OOD data. To tackle this, we introduce CausalDiffRec, a novel method using causal inference to remove these unstable correlations by learning invariant representations across different environments. CausalDiffRec comprises an environment generator to create diverse data distributions, an environment inference module to identify and utilize environmental components, and a diffusion module guiding invariant representation learning. Theoretically, we prove that CausalDiffRec can achieve better OOD generalization by identifying invariant representations across varying environments.The contributions of this paper are concluded as follows:</figDesc><graphic coords="2,487.97,90.97,70.08,60.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The structure causal model for GNN-based recommendation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 (</head><label>2</label><figDesc>a) and Figure 2 (b) illustrate the causal relationships in data generation and model training for graph-based recommendation algorithms. ğ¸ acts as the confounder and directly optimizing ğ‘ƒ (ğ‘Œ |ğº) leads the GNN-based recommendation model to learn the shortcut predictive relationship between G ğ‘¢ and ğ‘¦ ğ‘¢ , which is highly correlated with the environment ğ¸. During the model training process, there is a tendency to use this easily captured shortcut relationship to model user preferences. However, this shortcut relationship is highly sensitive to the environment ğ¸. When the environment of the test set is different from that of the training set (i.e., ğ· ğ‘¡ğ‘Ÿ (ğ¸) â‰  ğ· ğ‘¡ğ‘  (ğ¸)), this relationship becomes unstable and invalid. The recommendation model that excessively learns environment-sensitive relationships in the training data will struggle to accurately model user preferences when faced with OOD data during the testing phase, resulting in decreased recommendation accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Overall framework illustration of the proposed CausalDiffRec model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Effects of the number of diffusion steps T.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Effects of the number of environments K.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>3 .</head><label>3</label><figDesc>(1) Invariance: âˆ€ğ‘’ âˆˆ ğ· (ğ¸), ğ‘ƒ ğœƒ (ğ‘Œ |P * ğ¼ğ‘›ğ‘£ , ğ¸ = ğ‘’, ğ¼ ) = ğ‘ƒ (ğ‘Œ |P * ğ¼ğ‘›ğ‘£ , ğ¼ ) â‡” I(ğ‘Œ ; ğ¸|P * ğ¼ğ‘›ğ‘£ , ğ¼ ) = 0 where P * ğ¼ğ‘›ğ‘£ = ğ¹ * (ğº). (2) Sufficiency: I(ğ‘Œ ; P * ğ¼ğ‘›ğ‘£ , ğ¼ ) is maxmized. For the invariance property, it is easy to get the following equation: I(ğ‘Œ ; ğ¸|P * Inv , ğ¼ ) = E P * Inv ,ğ¼ D KL ğ‘ƒ (ğ‘Œ, ğ¸|P * Inv , ğ¼ )âˆ¥ğ‘ƒ (ğ‘Œ |P * Inv , ğ¼ )ğ‘ƒ (ğ¸|P * Inv , ğ¼ )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>(c), we have the fact that max ğ‘ (ğ‘§ |ğº,ğ¼ ) I(ğ‘Œ , ğ‘§ ğ‘ğ‘ğ‘¢ğ‘ ğ‘ğ‘™ ) is equivalent to min ğ‘ (ğ‘§ |ğº,ğ¼ ) I(ğ‘Œ, ğº |ğ‘ ğ‘ğ‘ğ‘¢ğ‘ ğ‘ğ‘™ ), as we use ğ‘‘ğ‘œ (ğº) to eliminate the unstable correlations between Y and G caused by the latent environment. We have: I(ğ‘Œ , ğº |ğ‘§ ğ‘ğ‘ğ‘¢ğ‘ ğ‘ğ‘™ ) = ğ· ğ¾ğ¿ (ğ‘ (ğ‘Œ |ğº, ğ¸)âˆ¥ğ‘ (ğ‘Œ |ğ‘§ ğ‘ğ‘ğ‘¢ğ‘ ğ‘ğ‘™ , ğ¸)) = ğ· ğ¾ğ¿ (ğ‘ (ğ‘Œ |ğº, ğ¸)âˆ¥ğ‘ (ğ‘Œ |ğ‘§ ğ‘ğ‘ğ‘¢ğ‘ ğ‘ğ‘™ )) -ğ· ğ¾ğ¿ (ğ‘ (ğ‘Œ |ğ‘§ ğ‘ğ‘ğ‘¢ğ‘ ğ‘ğ‘™ , ğ¸)âˆ¥ğ‘(ğ‘Œ |ğ‘§ ğ‘ğ‘ğ‘¢ğ‘ ğ‘ğ‘™ )) â‰¤ ğ· ğ¾ğ¿ (ğ‘ (ğ‘Œ |ğº, ğ¸)âˆ¥ğ‘ (ğ‘Œ |ğ‘§ ğ‘ğ‘ğ‘¢ğ‘ ğ‘ğ‘™ )).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>min ğ‘ (ğ‘§ ğ‘ğ‘ğ‘¢ğ‘ ğ‘ğ‘™ |ğº,ğ¼ ),ğ‘ (ğ‘Œ ,ğ‘§ ğ‘ğ‘ğ‘¢ğ‘ ğ‘ğ‘™ ) ğ· ğ¾ğ¿ (ğ‘ (ğ‘Œ |ğº, ğ¸ = ğ‘’)âˆ¥ğ‘(ğ‘Œ |ğ‘§ ğ‘ğ‘ğ‘¢ğ‘ ğ‘ğ‘™ )) + I(ğ‘Œ, ğ¸ = ğ‘’ |ğ‘§ ğ‘ğ‘ğ‘¢ğ‘ ğ‘ğ‘™ )<ref type="bibr" target="#b39">(40)</ref> </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>) The Proposition A.2 is completely proven. B DERIVATION B.1 Derivation for Equation 7 ğ‘ƒ ğœƒ (ğ‘Œ |ğ‘‘ğ‘œ (ğº)) = âˆ‘ï¸ ğ‘’ ğ‘ƒ ğœƒ (ğ‘Œ |ğ‘‘ğ‘œ (ğº), ğ¸ = ğ‘’, ğ¼ )ğ‘ƒ ğœƒ (ğ¸ = ğ‘’ |ğ‘‘ğ‘œ (ğº))ğ‘ƒ ğœƒ (ğ¼ ) = âˆ‘ï¸ ğ‘’ ğ‘ƒ ğœƒ (ğ‘Œ |ğº, ğ¸ = ğ‘’, ğ¼ )ğ‘ƒ ğœƒ (ğ¸ = ğ‘’ |ğ‘‘ğ‘œ (ğº))ğ‘ƒ ğœƒ (ğ¼ ) = âˆ‘ï¸ ğ‘’ ğ‘ƒ ğœƒ (ğ‘Œ |ğº, ğ¸ = ğ‘’, ğ¼ )ğ‘ƒ ğœƒ (ğ¸ = ğ‘’)ğ‘ƒ ğœƒ (ğ¼ ) = âˆ‘ï¸ ğ‘’ ğ‘ƒ ğœƒ (ğ‘Œ |ğº, ğ¸ = ğ‘’, ğ¼ )ğ‘ƒ ğœƒ (ğ¸ = ğ‘’, ğ¼ ) = E ğ‘’âˆ¼ğ· ğ‘¡ğ‘Ÿ (ğ¸ ) [ğ‘ƒ ğœƒ (ğ‘Œ |ğº, ğ¸, ğ¼ )],</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>logğ‘ƒ ğœƒ (ğ‘Œ |ğ‘‘ğ‘œ (ğº)) = log E ğ‘’âˆ¼ğ· ğ‘¡ğ‘Ÿ (ğ¸ ) [ğ‘ƒ ğœƒ (ğ‘Œ |ğº, ğ¸, ğ¼ )] = log âˆ‘ï¸ ğ‘’ ğ‘ƒ ğœƒ (ğ‘Œ |ğº, ğ¸ = ğ‘’, ğ¼ )ğ‘ƒ ğœƒ (ğ¸ = ğ‘’, ğ¼ ) ğ‘„ ğœ™ (ğ¸ = ğ‘’ |ğº, ğ¼ ) ğ‘„ ğœ™ (ğ¸ = ğ‘’ |ğº, ğ¼ ) â‰¥ âˆ‘ï¸ ğ‘’ ğ‘„ ğœ™ (ğ¸ = ğ‘’ |ğº, ğ¼ ) log ğ‘ƒ ğœƒ (ğ‘Œ |ğº, ğ¸ = ğ‘’, ğ¼ )ğ‘ƒ ğœƒ (ğ¸ = ğ‘’, ğ¼ ) 1 ğ‘„ ğœ™ (ğ¸ = ğ‘’ |ğº, ğ¼ ) = âˆ‘ï¸ ğ‘’ [ğ‘„ ğœ™ (ğ¸ = ğ‘’ |ğº, ğ¼ ) log ğ‘ƒ ğœƒ (ğ‘Œ |ğº, ğ¸ = ğ‘’, ğ¼ )log ğ‘„ ğœ™ (ğ¸ = ğ‘’ |ğº, ğ¼ )ğ‘ƒ ğœƒ (ğ¸ = ğ‘’, ğ¼ ) ğ‘„ ğœ™ (ğ¸ = ğ‘’ |ğº, ğ¼ ) ] = E ğ‘„ ğœ™ (ğ¸=ğ‘’ |ğº,ğ¼ ) [logğ‘ƒ ğœƒ (ğ‘Œ |ğº, ğ¸ = ğ‘’, ğ¼ )] -ğ· ğ¾ğ¿ (ğ‘„ ğœ™ (ğ¸ = ğ‘’ |ğº, ğ¼ ) âˆ¥ ğ‘ƒ ğœƒ (ğ¸ = ğ‘’)).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Visualization of user embedding distributions using various methods on the Douban dataset. CausalDiffRec ensures that hot items and cold items have representations that are nearly co-located within the same space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Visualization of user embedding distributions using various methods on the Yelp2018 dataset. CausalDiffRec ensures that hot items and cold items have representations that are nearly co-located within the same space.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>ğœ– ğœƒ (â€¢) denotes a function approximator that neural networks can replace, and ğ›¼ ğ‘¡ = ğ‘‡ ğ‘¡ =1 1 -ğ›½ ğ‘¡ . This framework allows the model</figDesc><table><row><cell></cell><cell>ğºğº</cell><cell></cell><cell>ğºğº</cell><cell></cell><cell>ğºğº</cell><cell>ğ¼ğ¼</cell></row><row><cell>P(ğºğº|ğ¸ğ¸)</cell><cell cols="2">P(ğ‘Œğ‘Œ|ğºğº, ğ¸ğ¸)</cell><cell>P(ğºğº|ğ¸ğ¸)</cell><cell>ğ‘“ğ‘“ ğœƒğœƒ (â‹…)</cell><cell>Deconfound</cell><cell></cell></row><row><cell>ğ¸ğ¸</cell><cell>P(ğ‘Œğ‘Œ|ğ¸ğ¸)</cell><cell>ğ‘Œğ‘Œ</cell><cell>ğ¸ğ¸</cell><cell>ğ‘Œğ‘Œ</cell><cell>ğ¸ğ¸</cell><cell>ğ‘Œğ‘Œ</cell></row><row><cell cols="2">(a) Data generation</cell><cell></cell><cell cols="2">(b) Model training</cell><cell cols="2">(c) Interventional model</cell></row><row><cell></cell><cell cols="2">Observed variables</cell><cell cols="2">Unobserved variables</cell><cell cols="2">User/item invariant features</cell></row><row><cell cols="7">E: Environmental factors that cannot be directly observed, e.g., policies and economic conditions.</cell></row><row><cell cols="4">G: Bipartite graph of user-item interactions.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Y: The true label.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">I: Invariant attributes of users or items.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>â€¢ ğ¸ â†’ ğº: This describes the direct effect of the environment on user-item interactions, defined by the probability ğ‘ƒ (ğº |ğ¸). For instance, cold weather might increase user interactions with warm clothing.â€¢ ğº â†’ ğ‘Œ : This reflects how the interaction graph G influences the user behavior label ğ‘Œ , characterized by the GNN model ğ‘Œ = ğ‘“ ğœƒ (ğº).With fixed model parameters ğœƒ , the relationship between ğº and ğ‘Œ is deterministic. â€¢ ğ¼ â†’ ğ‘Œ : Invariant attributes directly affect the user behavior label ğ‘Œ . For instance, a user might consistently prefer a specific restaurant, where attributes like the location remain constant.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 :</head><label>1</label><figDesc>The performance comparison between the baselines and CausalDiffRec on the four datasets with three data distribution shifts. The best results are highlighted in bold, and the second-best results are underlined. 'Impro.' denotes the relative improvements of CausalDiffRec over the second-best results.</figDesc><table><row><cell cols="7">Dataset Metric LightGCN SGL SimGCL LightGCL InvPref InvCF CDR AdvDrop AdvInfo DR-GNN Ours Impro.</cell></row><row><cell></cell><cell>R@10</cell><cell>0.0234 0.0198 0.0233</cell><cell>0.0108</cell><cell>0.0029 0.0382 0.0260 0.0240</cell><cell>0.0227</cell><cell>0.0266 0.0281 5.63%</cell></row><row><cell>Food</cell><cell>N@10 R@20</cell><cell>0.0182 0.0159 0.0186 0.0404 0.0324 0.0414</cell><cell>0.0101 0.0181</cell><cell>0.0014 0.0237 0.0195 0.0251 0.0294 0.0392 0.0412 0.0371</cell><cell>0.0135 0.0268</cell><cell>0.0205 0.0296 17.93% 0.0436 0.0464 6.42%</cell></row><row><cell></cell><cell>N@20</cell><cell>0.0242 0.0201 0.0249</cell><cell>0.0121</cell><cell>0.0115 0.0240 0.0254 0.0237</cell><cell>0.0159</cell><cell>0.0279 0.0306 9.68%</cell></row><row><cell></cell><cell>R@10</cell><cell>0.0742 0.0700 0.0763</cell><cell>0.0630</cell><cell>0.0231 0.1023 0.0570 0.1014</cell><cell>0.1044</cell><cell>0.0808 0.1116 6.90%</cell></row><row><cell>KuaiRec</cell><cell>N@10 R@20</cell><cell>0.5096 0.4923 0.5180 0.1120 0.1100 0.1196</cell><cell>0.4334 0.1134</cell><cell>0.2151 0.2242 0.2630 0.3290 0.0478 0.1034 0.0860 0.1214</cell><cell>0.4302 0.1254</cell><cell>0.5326 0.6474 21.55% 0.1266 0.1631 28.83%</cell></row><row><cell></cell><cell>N@20</cell><cell>0.4268 0.4181 0.4446</cell><cell>0.4090</cell><cell>0.2056 0.2193 0.2240 0.3289</cell><cell>0.4305</cell><cell>0.4556 0.5392 18.35%</cell></row><row><cell></cell><cell>R@10</cell><cell>0.0014 0.0027 0.0049</cell><cell>0.0022</cell><cell>0.0049 0.0004 0.0011 0.0027</cell><cell>0.0047</cell><cell>0.0044 0.0067 36.73%</cell></row><row><cell>Yelp2018</cell><cell>N@10 R@20</cell><cell>0.0008 0.0017 0.0028 0.0035 0.0051 0.0106</cell><cell>0.0015 0.0054</cell><cell>0.0030 0.0026 0.0006 0.0017 0.0108 0.0013 0.0016 0.0049</cell><cell>0.0024 0.0083</cell><cell>0.0029 0.0039 30.00% 0.0076 0.0120 11.65%</cell></row><row><cell></cell><cell>N@20</cell><cell>0.0016 0.0026 0.0047</cell><cell>0.0026</cell><cell>0.0049 0.0008 0.0008 0.0024</cell><cell>0.0038</cell><cell>0.0041 0.0055 11.11%</cell></row><row><cell></cell><cell>R@10</cell><cell>0.0028 0.0022 0.0086</cell><cell>0.0070</cell><cell>0.0052 0.0030 0.0014 0.0051</cell><cell>0.0076</cell><cell>0.0028 0.0094 9.30%</cell></row><row><cell>Douban</cell><cell>N@10 R@20</cell><cell>0.0015 0.0013 0.0045 0.0049 0.0047 0.0167</cell><cell>0.0038 0.0113</cell><cell>0.0026 0.0012 0.0007 0.0021 0.0093 0.0033 0.0200 0.0046</cell><cell>0.0042 0.0103</cell><cell>0.0011 0.0050 11.11% 0.0038 0.0197 17.96%</cell></row><row><cell></cell><cell>N@20</cell><cell>0.0019 0.0020 0.0073</cell><cell>0.0050</cell><cell>0.0038 0.0013 0.0019 0.0021</cell><cell>0.0053</cell><cell>0.0015 0.0079 8.22%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 2 :</head><label>2</label><figDesc>Outcomes from ablation studies on four datasets. The top-performing results are highlighted in bold, while those that are second-best are underlined.</figDesc><table><row><cell>Dataset</cell><cell>Ablation</cell><cell>R@10</cell><cell>R@20 N@10 N@20</cell></row><row><cell></cell><cell>LightGCN</cell><cell cols="2">0.0234 0.0404 0.0182 0.0242</cell></row><row><cell></cell><cell>w/o Gen.</cell><cell cols="2">0.0165 0.0259 0.0114 0.0148</cell></row><row><cell>Food</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>w/o Env.</cell><cell cols="2">0.0084 0.0144 0.0077 0.0098</cell></row><row><cell></cell><cell cols="3">CausalDiffRec 0.0251 0.0409 0.0296 0.0306</cell></row><row><cell></cell><cell>LightGCN</cell><cell cols="2">0.0808 0.1266 0.5326 0.4556</cell></row><row><cell></cell><cell>w/o Gen.</cell><cell cols="2">0.0966 0.1571 0.0445 0.3078</cell></row><row><cell>KuaiRec</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>w/o Env.</cell><cell cols="2">0.0047 0.1740 0.0697 0.0784</cell></row><row><cell></cell><cell cols="3">CausalDiffRec 0.1116 0.1631 0.0674 0.5392</cell></row><row><cell></cell><cell>LightGCN</cell><cell cols="2">0.0014 0.0035 0.0008 0.0016</cell></row><row><cell></cell><cell>w/o Gen.</cell><cell cols="2">0.0041 0.0054 0.0037 0.0042</cell></row><row><cell>Yelp2018</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>w/o Env.</cell><cell cols="2">0.0027 0.0058 0.0042 0.0043</cell></row><row><cell></cell><cell cols="3">CausalDiffRec 0.0067 0.0120 0.0039 0.0055</cell></row><row><cell></cell><cell>LightGCN</cell><cell cols="2">0.0028 0.0049 0.0015 0.0019</cell></row><row><cell></cell><cell>w/o Gen.</cell><cell cols="2">0.0044 0.0079 0.0030 0.0045</cell></row><row><cell>Douban</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>w/o Env.</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>Next, we prove that for ğ‘Œ , P * ğ¼ğ‘›ğ‘£ , and ğ¼ satisfying P * ğ¼ğ‘›ğ‘£ = arg max P ğ¼ğ‘›ğ‘£ I(ğ‘Œ ; P ğ¼ğ‘›ğ‘£ , ğ¼ ), they also satisfy that I(ğ‘Œ ; P * ğ¼ğ‘›ğ‘£ , ğ¼ ) is maximized. Assume P * ğ¼ğ‘›ğ‘£ â‰  arg max P ğ¼ğ‘›ğ‘£ I(ğ‘Œ ; P ğ¼ğ‘›ğ‘£ , ğ¼ ), and there exists P â€² ğ¼ğ‘›ğ‘£ = arg max P ğ¼ğ‘›ğ‘£ I(ğ‘Œ ; P ğ¼ğ‘›ğ‘£ , ğ¼ ), where P â€²</figDesc><table><row><cell>(ğ‘Œ ; P â€² ğ¼ğ‘›ğ‘£ , ğ¼ ) = I(ğ‘Œ ; P  *  ğ¼ğ‘›ğ‘£ , ğ¼ ).</cell><cell>(27)</cell></row><row><cell>This leads to a contradiction.</cell><cell></cell></row><row><cell cols="2">ğ¼ğ‘›ğ‘£ â‰  P  *  ğ¼ğ‘›ğ‘£ . We have the follow-</cell></row><row><cell>ing inequality:</cell><cell></cell></row><row><cell>I(ğ‘Œ ; P</cell><cell></cell></row></table><note><p>* ğ¼ğ‘›ğ‘£ , ğ¼ ) â‰¤ I(ğ‘Œ ; P â€² ğ¼ğ‘›ğ‘£ , ğ¼ ).</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>ğ‘Œ )âˆ¼ğ‘ (ğº,ğ‘Œ |ğ‘’ ) E ğ‘§ ğ‘ğ‘ğ‘¢ğ‘ ğ‘ğ‘™ âˆ¼ğ‘ (ğ‘§ ğ‘ğ‘ğ‘¢ğ‘ ğ‘ğ‘™ |ğº,ğ¼ ) log ğ‘(ğ‘Œ |ğº, ğ‘’) ğ‘ (ğ‘Œ |ğ‘§ ğ‘ğ‘ğ‘¢ğ‘ ğ‘ğ‘™ ) â‰¤ E ğ‘’ âˆˆğ· ğ‘¡ğ‘Ÿ (ğ¸ ) E (ğº,ğ‘Œ )âˆ¼ğ‘ (ğº,ğ‘Œ |ğ‘’ ) log ğ‘ (ğ‘Œ |ğº, ğ‘’) E ğ‘§ ğ‘ğ‘ğ‘¢ğ‘ ğ‘ğ‘™ âˆ¼ğ‘ (ğ‘§ ğ‘ğ‘ğ‘¢ğ‘ ğ‘ğ‘™ |ğº,ğ¼ ) ğ‘(ğ‘Œ |ğ‘§ ğ‘ğ‘ğ‘¢ğ‘ ğ‘ğ‘™ ) ğ‘ (ğ‘Œ |ğ‘§ ğ‘ğ‘ğ‘¢ğ‘ ğ‘ğ‘™ ) ğ· ğ¾ğ¿ (ğ‘ (ğ‘Œ |ğº, ğ¸)âˆ¥ğ‘ (ğ‘Œ |ğ‘§ ğ‘ğ‘ğ‘¢ğ‘ ğ‘ğ‘™ )) â‡” arg min ğœƒ E ğ‘’âˆ¼ğ· ğ‘¡ğ‘Ÿ (ğ¸ ),( G ğ‘¢ ,ğ‘¦ ğ‘¢ )âˆ¼ğ‘ƒ (ğ‘Œ ,ğº |ğ¸=ğ‘’ ) [ğ‘™ (ğ‘“ ğœƒ (G ğ‘¢ ; ğœƒ ), ğ‘¦ ğ‘¢ )].</figDesc><table><row><cell>(Jensen Inequality).</cell></row><row><cell>(32)</cell></row><row><cell>Finally, we reach:</cell></row><row><cell>min</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 3 :</head><label>3</label><figDesc>Detailed statistics for each dataset.</figDesc><table><row><cell cols="4">Dataset #Users #Items #Interactions</cell><cell>Density</cell></row><row><cell>Food</cell><cell>7,809</cell><cell>6,309</cell><cell>216,407</cell><cell>4.4 Ã— 10 -3</cell></row><row><cell>KuaiRec</cell><cell>7,175</cell><cell>10,611</cell><cell>1,153,797</cell><cell>1.5 Ã— 10 -3</cell></row><row><cell cols="2">Yelp2018 8,090</cell><cell>13,878</cell><cell>398,216</cell><cell>3.5 Ã— 10 -3</cell></row><row><cell>Douban</cell><cell>8,735</cell><cell>13,143</cell><cell>354,933</cell><cell>3.1 Ã— 10 -3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>â€¢</head><label></label><figDesc>Popularity shift: We randomly select 20% of interactions to form the OOD test set, ensuring a uniform distribution of item popularity. The remaining data is split into training, validation, and IID test sets in a ratio of 7:1:2, respectively. This type of distribution shift is applied to the Yelp2018 and Douban datasets. â€¢ Temporal shift: We sort the dataset by timestamp in descending order and designate the most recent 20% of each user's interactions as the OOD test set. The remaining data is split into training, validation, and IID test sets in a ratio of 7:1:2, respectively. The food dataset is used for this type of distribution shift. â€¢ Exposure shift: In KuaiRec, the smaller matrix, which is fully exposed, serves as the OOD test set. The larger matrix collected from the online platform is split into training, validation, and IID test sets in a ratio of 7:1:2, respectively, creating a distribution shift.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 4 :</head><label>4</label><figDesc>Model performance comparison on IID datasets. Training of CausalDiffRec under Multiple Environments 1: Input: The user-item interaction graph ğº (V, E) and node feature matrix X; Using the ğœ”, ğœƒ 1 , and ğœƒ 2 to initial environment generator ğ‘” ğœ” (â€¢), environment ğ‘ƒ ğœƒ 1 (â€¢), and graph representation learner (ie., sampling approximator) ğ‘“ ğœƒ 2 (â€¢), respectively. 2: while not converged do</figDesc><table><row><cell>Dataset</cell><cell>Ablation</cell><cell>R@10</cell><cell>R@20 N@10 N@20</cell></row><row><cell></cell><cell>LightGCN</cell><cell cols="2">0.0154 0.0174 0.0272 0.0210</cell></row><row><cell></cell><cell>AdvDrop</cell><cell cols="2">0.0431 0.0258 0.0469 0.0276</cell></row><row><cell>KuaiRec</cell><cell>AdvInfo</cell><cell cols="2">0.0514 0.0298 0.0518 0.0300</cell></row><row><cell></cell><cell>DRO</cell><cell cols="2">0.0307 0.0226 0.0505 0.0291</cell></row><row><cell cols="4">CausalDiffRec 0.0567 0.0472 0.0634 0.0707</cell></row><row><cell></cell><cell>LightGCN</cell><cell cols="2">0.0023 0.0022 0.0039 0.0029</cell></row><row><cell></cell><cell>AdvDrop</cell><cell cols="2">0.0061 0.0051 0.0063 0.0050</cell></row><row><cell>Yelp2018</cell><cell>AdvInfo</cell><cell cols="2">0.0052 0.0040 0.0062 0.0049</cell></row><row><cell></cell><cell>DRO</cell><cell cols="2">0.0069 0.0550 0.0086 0.0065</cell></row><row><cell cols="4">CausalDiffRec 0.0102 0.0120 0.0182 0.0134</cell></row><row><cell>Algorithm 1</cell><cell></cell><cell></cell><cell></cell></row></table><note><p><p>3:</p>for all ğ‘¢ âˆˆ ğ‘ˆ do 4:</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>This work is partially supported by the <rs type="funder">National Natural Science Foundation of China</rs> under Grant (No. <rs type="grantNumber">62032013</rs>, <rs type="grantNumber">62102074</rs>), the <rs type="funder">Science and Technology projects in Liaoning Province</rs> (No. <rs type="grantNumber">2023JH3 / 10200005</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_5Y5ErNY">
					<idno type="grant-number">62032013</idno>
				</org>
				<org type="funding" xml:id="_VnweVHB">
					<idno type="grant-number">62102074</idno>
				</org>
				<org type="funding" xml:id="_QqfVRJW">
					<idno type="grant-number">2023JH3 / 10200005</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Obtain the damage representation x ğ‘¡ ğ¾ by Eq. (</p><p>// Forward process  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E BASELINES</head><p>We compare the CusalDiffRec with the following state-of-the-art models:</p><p>â€¢ LightGCN <ref type="bibr" target="#b8">[9]</ref>. It is an effective collaborative filtering method based on graph convolutional networks (GCNs) that streamline NGCF's message propagation scheme by eliminating non-linear projection and activation. â€¢ SGL <ref type="bibr" target="#b39">[40]</ref>. It uses LightGCN as the backbone and incorporates a series of structural augmentations to enhance representation learning.</p><p>â€¢ SimGCL <ref type="bibr" target="#b49">[50]</ref>. This model employs a simple contrastive learning (CL) approach that avoids graph augmentations and introduces uniform noise into the embedding space to generate contrastive views.</p><p>â€¢ LigthGCL <ref type="bibr" target="#b0">[1]</ref>. This uses LightGCN as its backbone and introduces uniform noise into the embedding space for contrastive learning without relying on graph augmentations â€¢ CDR <ref type="bibr" target="#b31">[32]</ref>. It captures preference shifts using a temporal variational autoencoder and learns the sparse influence from multiple environments.</p><p>â€¢ InvPref <ref type="bibr" target="#b38">[39]</ref>. It is a general debiasing model that iteratively decomposes invariant and variant preferences from biased observational user behaviors by estimating heterogeneous environments corresponding to different types of latent bias. â€¢ InvCF <ref type="bibr" target="#b54">[55]</ref>. This model aims to mitigate popularity shift to discover disentangled representations that faithfully reveal the latent preference and popularity semantics without making any assumption about the popularity distribution. â€¢ AdvInfoNCE <ref type="bibr" target="#b53">[54]</ref>. It is an InfoNCE variant that leverages a detailed hardness-aware ranking criterion to improve the recommender's ability to generalize â€¢ AdvDrop <ref type="bibr" target="#b52">[53]</ref>. It is designed to alleviate general biases and inherent bias amplification in graph-based collaborative filtering by enforcing embedding-level invariance from learned bias-related views. â€¢ DR-GNN <ref type="bibr" target="#b27">[28]</ref>. A GNN-based OOD recommendation algorithm solves the data distribution shift via the Distributionally Robust Optimization theory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F MODEL COMPLEXITY ANALYSIS</head><p>We analyze the time complexity of our CausalDiffRec across its different components: i) The environment inference component has a time complexity of ğ‘‚ (1). In summary, CausalDiffRec can achieve overall complexity comparable to state-of-the-art diffusion-based recommendation models such as DiffRec <ref type="bibr" target="#b32">[33]</ref> and RecDiff <ref type="bibr" target="#b21">[22]</ref>.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Xuheng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianghao</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xubin</forename><surname>Ren</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.08191</idno>
		<title level="m">LightGCL: Simple yet effective graph contrastive learning for recommendation</title>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Unifying knowledge graph learning and recommendation: Towards a better understanding of user preferences</title>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zikun</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>WWW</publisher>
			<biblScope unit="page" from="151" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Sequential recommendation with graph neural networks</title>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiqun</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanan</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Depeng</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="378" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning causally invariant representations for out-of-distribution generalization on graphs</title>
		<author>
			<persName><forename type="first">Yongqiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonggang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yatao</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Kaili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Binghui</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="22131" to="22148" />
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Environment inference for invariant learning</title>
		<author>
			<persName><forename type="first">Elliot</forename><surname>Creager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">JÃ¶rn-Henrik</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<editor>ML. PMLR</editor>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2189" to="2200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Attention branch network: Learning of attention mechanism for visual explanation</title>
		<author>
			<persName><forename type="first">Hiroshi</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsubasa</forename><surname>Hirakawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takayoshi</forename><surname>Yamashita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hironobu</forename><surname>Fujiyoshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="10705" to="10714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Attention mechanisms in computer vision: A survey</title>
		<author>
			<persName><forename type="first">Meng-Hao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tian-Xing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiang-Jiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng-Ning</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng-Tao</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tai-Jiang</forename><surname>Mu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song-Hai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Ming</forename><surname>Ralph R Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shi-Min</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational visual media</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="331" to="368" />
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A survey on knowledge graph-based recommender systems</title>
		<author>
			<persName><forename type="first">Qingyu</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuzhen</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hengshu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TKDE</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="3549" to="3568" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Lightgcn: Simplifying and powering graph convolution network for recommendation</title>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuan</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval</title>
		<meeting>the 43rd International ACM SIGIR conference on research and development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="639" to="648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Neural collaborative filtering</title>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizi</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th international conference on world wide web</title>
		<meeting>the 26th international conference on world wide web</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Causpref: Causal preference learning for out-of-distribution recommendation</title>
		<author>
			<persName><forename type="first">Yue</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zimu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yafeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Jiang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="410" to="421" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajay</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="6840" to="6851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Generalized odin: Detecting out-of-distribution image without learning from out-of-distribution data</title>
		<author>
			<persName><forename type="first">Yen-Chang</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yilin</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxia</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zsolt</forename><surname>Kira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="10951" to="10960" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A survey on knowledge graphs: Representation, acquisition, and applications</title>
		<author>
			<persName><forename type="first">Shaoxiong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pekka</forename><surname>Marttinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TNNLS</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="494" to="514" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Adaptive graph contrastive learning for recommendation</title>
		<author>
			<persName><forename type="first">Yangqin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianghao</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="4252" to="4261" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Diffkg: Knowledge graph diffusion model for recommendation</title>
		<author>
			<persName><forename type="first">Yangqin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianghao</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="313" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Contrastive self-supervised learning in recommender systems: A survey</title>
		<author>
			<persName><forename type="first">Mengyuan</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanmin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianzi</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOIS</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="1" to="39" />
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Exploring chemical space with score-based out-of-distribution generation</title>
		<author>
			<persName><forename type="first">Seul</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaehyeong</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sung</forename><forename type="middle">Ju</forename><surname>Hwang</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="18872" to="18892" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Graph transformer for recommendation</title>
		<author>
			<persName><forename type="first">Chaoliu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianghao</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xubin</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaowen</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="1680" to="1689" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning invariant graph representations for out-of-distribution generalization</title>
		<author>
			<persName><forename type="first">Haoyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="11828" to="11841" />
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Diffurec: A diffusion model for sequential recommendation</title>
		<author>
			<persName><forename type="first">Zihao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aixin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenliang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOIS</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="1" to="28" />
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Zongwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianghao</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2406.01629</idno>
		<title level="m">Diffusion Model for Social Recommendation</title>
		<imprint>
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">Jiashuo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheyan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingxuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renzhe</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.13624</idno>
		<title level="m">Towards out-of-distribution generalization: A survey</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Plug-in diffusion model for sequential recommendation</title>
		<author>
			<persName><forename type="first">Haokai</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruobing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leyu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhanhui</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In AAAI</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="8886" to="8894" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A review on the attention mechanism of deep learning</title>
		<author>
			<persName><forename type="first">Zhaoyang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoqiang</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">452</biblScope>
			<biblScope unit="page" from="48" to="62" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A diffusion model for poi recommendation</title>
		<author>
			<persName><forename type="first">Yifang</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongjun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOIS</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="1" to="27" />
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Hgcf: Hyperbolic graph convolution networks for collaborative filtering</title>
		<author>
			<persName><forename type="first">Jianing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaoyue</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saba</forename><surname>Zuberi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felipe</forename><surname>PÃ©rez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maksims</forename><surname>Volkovs</surname></persName>
		</author>
		<idno>WWW. 593-601</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Distributionally Robust Graph-based Recommendation System</title>
		<author>
			<persName><forename type="first">Bohao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changdong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qihao</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Can</forename><surname>Wang</surname></persName>
		</author>
		<idno>WWW. 3777-3788</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Session-based recommendation with hypergraph attention networks</title>
		<author>
			<persName><forename type="first">Jianling</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaize</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Caverlee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SDM</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="82" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Shoujin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Longbing</forename><surname>Mehmet A Orgun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.06339</idno>
		<title level="m">Graph learning based recommender systems: A review</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Causal representation learning for out-of-distribution recommendation</title>
		<author>
			<persName><forename type="first">Wenjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
		<idno>WWW. 3562-3571</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Causal disentangled recommendation against user preference shifts</title>
		<author>
			<persName><forename type="first">Wenjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liuhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunshan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOIS</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="1" to="27" />
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Xiangnan He, and Tat-Seng Chua. 2023. Diffusion recommender model</title>
		<author>
			<persName><forename type="first">Wenjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiyan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<biblScope unit="page" from="832" to="841" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Kgat: Knowledge graph attention network for recommendation</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="950" to="958" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Neural graph collaborative filtering</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="165" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Disentangled graph collaborative filtering</title>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongye</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">An</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1001" to="1010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Disenhan: Disentangled heterogeneous graph attention network for recommendation</title>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suyao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuntong</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiping</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1605" to="1614" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning to schedule in diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">Yunke</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anh-Dung</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="2478" to="2488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Invariant preference learning for general debiasing in recommendation</title>
		<author>
			<persName><forename type="first">Zimu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiashuo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenchao</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="1969">2022. 1969-1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Self-supervised graph learning for recommendation</title>
		<author>
			<persName><forename type="first">Jiancan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxun</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="726" to="735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Graph out-of-distribution generalization via causal intervention</title>
		<author>
			<persName><forename type="first">Qitian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenxiao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
		<idno>WWW. 850-860</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Handling distribution shifts on graphs: An invariance perspective</title>
		<author>
			<persName><forename type="first">Qitian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hengrui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wipf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.02466</idno>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Graph neural networks in recommender systems: a survey</title>
		<author>
			<persName><forename type="first">Shiwen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wentao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Surveys</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="1" to="37" />
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A comprehensive survey on graph neural networks</title>
		<author>
			<persName><forename type="first">Zonghan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fengwen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TNNLS</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="4" to="24" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Diff4rec: Sequential recommendation with curriculum-scheduled diffusion augmentation</title>
		<author>
			<persName><forename type="first">Zihao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaidong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lifeng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="9329" to="9335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Automated self-supervised learning for recommendation</title>
		<author>
			<persName><forename type="first">Lianghao</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunzhen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kangyi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Kao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WWW</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="992" to="1002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Towards out-of-distribution sequential event prediction: A causal treatment</title>
		<author>
			<persName><forename type="first">Chenxiao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qitian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingsong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiqiang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="22656" to="22670" />
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Generalized out-of-distribution detection: A survey</title>
		<author>
			<persName><forename type="first">Jingkang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiyang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="page" from="1" to="28" />
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning substructure invariance for out-of-distribution molecular representations</title>
		<author>
			<persName><forename type="first">Nianzu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaipeng</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qitian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaosong</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="12964" to="12978" />
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Are graph augmentations necessary? simple graph contrastive learning for recommendation</title>
		<author>
			<persName><forename type="first">Junliang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzhi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizhen</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Viet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hung</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1294" to="1303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Self-supervised learning for recommender systems: A survey</title>
		<author>
			<persName><forename type="first">Junliang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongzhi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jundong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zi</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TKDE</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="335" to="355" />
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Environment-Aware Dynamic Graph Learning for Out-of-Distribution Generalization</title>
		<author>
			<persName><forename type="first">Haonan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingyun</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingcheng</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">General Debiasing for Graph-based Collaborative Filtering via Adversarial Graph Dropout</title>
		<author>
			<persName><forename type="first">An</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenchang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengbo</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leheng</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<idno>WWW. 3864-3875</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Empowering Collaborative Filtering with Principled Adversarial Contrastive Loss</title>
		<author>
			<persName><forename type="first">An</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leheng</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhibo</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Invariant collaborative filtering to popularity distribution shift</title>
		<author>
			<persName><forename type="first">An</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingnan</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yancheng</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
		<idno>WWW. 1240-1251</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Geometric disentangled collaborative filtering</title>
		<author>
			<persName><forename type="first">Yiding</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaozhuo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangjie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiwei</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="80" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Spectral invariant learning for dynamic graphs under distribution shifts</title>
		<author>
			<persName><forename type="first">Zeyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weigao</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<author>
			<persName><forename type="first">Xin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Di</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.07082</idno>
		<title level="m">Graph neural networks for graphs with heterophily: A survey</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Graph neural networks: A review of methods and applications</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ganqu</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengding</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changcheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI open</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="57" to="81" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
