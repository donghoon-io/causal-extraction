<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Causality Extraction Model Based on Two-stage GCN</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2022-04-25">April 25th, 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Guangli</forename><surname>Zhu</surname></persName>
							<email>glzhu@aust.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Anhui University of Science and Technology</orgName>
								<address>
									<postCode>232001</postCode>
									<settlement>Huainan</settlement>
									<country>People&apos;s Republic of China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhengyan</forename><surname>Sun</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Anhui University of Science and Technology</orgName>
								<address>
									<postCode>232001</postCode>
									<settlement>Huainan</settlement>
									<country>People&apos;s Republic of China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shunxiang</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Anhui University of Science and Technology</orgName>
								<address>
									<postCode>232001</postCode>
									<settlement>Huainan</settlement>
									<country>People&apos;s Republic of China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Subo</forename><surname>Wei</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Engineering</orgName>
								<orgName type="institution">Anhui University of Science and Technology</orgName>
								<address>
									<postCode>232001</postCode>
									<settlement>Huainan</settlement>
									<country>People&apos;s Republic of China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ching</forename><surname>Kuan</surname></persName>
						</author>
						<author>
							<persName><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Kuanching</forename><surname>Li</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science and Information Engineering (CSIE)</orgName>
								<orgName type="institution">Providence University</orgName>
								<address>
									<postCode>43301</postCode>
									<settlement>Taizhong</settlement>
									<country key="TW">Taiwan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Anhui University of Science and Technology</orgName>
								<orgName type="institution" key="instit2">Anhui University of Science and Technology</orgName>
								<orgName type="institution" key="instit3">Anhui University of Science and Technology</orgName>
								<orgName type="institution" key="instit4">Anhui University of Science and Technology</orgName>
								<orgName type="institution" key="instit5">Providence University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Causality Extraction Model Based on Two-stage GCN</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-04-25">April 25th, 2022</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.21203/rs.3.rs-1449992/v1</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-21T20:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>two-stage GCN</term>
					<term>BERT</term>
					<term>GCN</term>
					<term>causality extraction two-stage GCN</term>
					<term>BERT</term>
					<term>GCN</term>
					<term>causality extraction</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In the traditional methods, the low identification accuracy of cascade implicit causalities is caused by the lack of causal inference. To solve this problem, we propose a causality extraction model based on GCN to infer the causality of the text. It can analyze the cause-effect existing in the text and realize the deep extraction under semantic enhancement. First, the data are preprocessed, and BERT is used for pre-training. In the pre-training, the candidate entities are selected by entity links. The text encoding is used context semantics and embedding location coding. Then, the semantic dependency graph is used to obtain the relationship between entities. In addition, the nodes and edges obtained by the previous step are input into the first-stage GCN to extract the whole causality. Finally, the entity relation graph obtained by the first-stage GCN is introduced into the second-stage GCN for cascade inference. The causality of the cascade is inferred and extracted by a two-stage GCN. Experiments show that the model can find implicit causality more accurately generate new implicit causal entities on the original causality.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Relation extraction gives a given text, extracts entities/objects, and infer their relations in the text, forming a triple ( S, R, O ). S represents the subject entity, R represents the existing relationship, and O represents the object entity. At present, entity relation extraction technology impacts all aspects of natural language processing tasks and has been widely used. Causality is one of the most famous relationships.</p><p>Traditional methods can quickly identify explicit causal entities, but the identification accuracy is not high because of the need for inference for implicit causal entities. At the same time, because of the uniqueness and diversity of text, the complexity of semantic structure <ref type="bibr" target="#b34">[35]</ref>, the variety of expression and other factors, it is inevitable to increase the difficulty of inference, which leads to many potential implicit causal entities are not identified.</p><p>Given the above problems in the research on causality extraction, the following points are mainly considered. <ref type="bibr" target="#b0">(1)</ref> Entity extraction. Previous work has proposed a key sentences extraction algorithm for Chinese microblog comments <ref type="bibr" target="#b0">[1]</ref>, which considers multiple factors attributes to identify the key sentences of comments. So this algorithm can provide technical assistance for entity extraction. <ref type="bibr" target="#b1">(2)</ref> Feature extraction and relational inference. In the microblog hot topic word extraction model <ref type="bibr" target="#b1">[2]</ref>, a feature cooccurrence method is proposed, providing a reference for causal relationship feature extraction and relational inference. (3) Causality identification, especially can effectively identify the implicit causality. Previous work has used ALN (Association link Network) <ref type="bibr" target="#b2">[3]</ref> to achieve the hierarchical division of associated semantics. Learning the semantic information of the text provides technical support for causality identification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Based on the above considerations, we combine BERT technology and GCN graph neural network to</head><p>propose a new model called the causality extraction model based on a two-stage GCN. Different from the traditional method, the proposed method identifies a potential causality between cascading causal entities by two-stage inference of GCN and extracts it. The motivation is that the network can learn more complex entity structures, capture more abundant local and non-local features and realize multi-hop inference of entities. The model framework is shown in Figure <ref type="figure">1</ref>, and the main contributions of this paper can be summarized as the following two points.</p><p>‚Ä¢ Construction of causal entity candidate library. We use a combination of causal prior knowledge and semantic data to extract causal entities from the corpus. Cluster analysis of the extracted entities, and the same cluster of entities randomly selected as adjacent nodes into GCN. Firstly, the review text is preprocessed, and the sentences without causality or the sentences with default components are deleted. Then, the "NLTK" word segmentation is used to segment the words in the text. Secondly, using emotional intensity to establish causality seed lexicon. Finally, the K-means clustering algorithm is used for clustering calculation to select the final causal entity library. </p><formula xml:id="formula_0">X 1 X 2 X 3 X 4 X 1 X 2 X 3 X 4 Input hidden layer classifier Output X 1 X 2 X 3 X 4 X 1 X 2 X 3 X 4 hidden layer X 1 X 2 X 3 X 4 Output X 1 X 2 X 3 X 4 classifier (S,R,O)</formula><p>The advantage is that the two-stage GCN method can achieve better multi-hop relational inference and identify more implicit cascade causality. The main contributions of this paper can be summarized as the following two points :</p><p>‚Ä¢ Effective construction of causality candidate entity library. we take into account that causal entities often have phrases with emotional polarities. Therefore, the candidate seed bank of causal entity is constructed by emotional intensity, which is convenient for constructing entity knowledge base through entity learning.</p><p>‚Ä¢ 2 Related works</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Research on causality</head><p>More and more scholars have paid attention to causality extraction in recent years. Dasgupta et al. <ref type="bibr" target="#b3">[4]</ref> uses complex formulas to represent causality annotates the cause, result, and causal connectives in sentences through bidirectional long-term and short-term memory networks. It converts the whole sentence into a words vector sequence and adds a language layer on Bi-LSTM to achieve good results.</p><p>Zhang Shu et al. <ref type="bibr" target="#b4">[5]</ref> consider the sequence information of long sentences because of the location uncertainty of causal information. Therefore, word embedding <ref type="bibr" target="#b28">[29]</ref> is used as the input feature to extract causal events <ref type="bibr" target="#b33">[34]</ref> based on the Bi-LSTM method. Silva Tharini et al. <ref type="bibr" target="#b5">[6]</ref> compared the two methods.</p><p>Based on the characteristics of knowledge and deep learning, the first group of experiments trained the SVM-based causal relationship classification model on the features of semantic knowledge and selected several different CNNs for experiments.</p><p>Li et al. <ref type="bibr" target="#b6">[7]</ref> introduced other vital features of causality to improve the performance of CNN by reducing the dimension, considering that the construction of features requires a lot of engineering. An et al. <ref type="bibr" target="#b7">[8]</ref> extracted causality from the literature from the perspective of rules improved the syntactic pattern matching method to simplify sentences and established a verb seed set to learn the characteristics of verbs, and finally achieved good results. Chaveevan et al. <ref type="bibr" target="#b8">[9]</ref>extracted causal relationships from web documents, mainly extracting the causal path, using the causal path can explain or express some concepts, using supervised learning method to improve the accuracy. Abbas et al. <ref type="bibr" target="#b9">[10]</ref>aimed to extract causality from biomedical literature and implemented and evaluated several commonly used models to reduce class imbalances and improve the performance of models by random oversampling techniques. Duc et al. <ref type="bibr" target="#b10">[11]</ref> focused on the causality extraction of document-level texts and constructed a network. Fu et al. <ref type="bibr" target="#b21">[22]</ref> proposed an event network. Shao et al. <ref type="bibr" target="#b11">[12]</ref> proposed a BEL method to simplify sentences and improve the accuracy of causality extraction by BERT.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Research on Graph Neural Network</head><p>Xu et al. <ref type="bibr" target="#b12">[13]</ref> introduced the commonly used graph convolution neural network in recent years, introduced two typical graph convolution neural network methods, the general method and the spatial method, and described these two methods in detail. They also introduced various applications of graph convolution neural networks. Bosselut et al. <ref type="bibr" target="#b13">[14]</ref> transformed implicit knowledge into explicit knowledge through the COMET model, established a commonsense knowledge base, and produced more highquality new knowledge. Experiments show that new knowledge can be inferred through knowledge graphs. Zhu et al. <ref type="bibr" target="#b14">[15]</ref> proposed a GP-GCNs to generate parameters for relation extraction and described the embedding module, propagation module, and classification module in detail. GCN adjusted the hyper-parameters in the process of propagation. Finally, qualitative analysis and quantitative analysis were carried out. The model can be used for inferring relations through the multi-hop mechanism.</p><p>Fu et al. <ref type="bibr" target="#b15">[16]</ref> and others use relational weighted GCN to extract the relationship. This paper mainly considers the text's sequential features and local features and uses a dependency structure for GCN to learn the implicit features between all words in the text. The GCN model improves the prediction results</p><p>with high accuracy on public data sets. Ali et al. <ref type="bibr" target="#b16">[17]</ref> extracted event information by dependency graph and proposed an attention-based GCN to capture potential relationships between events. MHGCN presented by Gao et al. <ref type="bibr" target="#b17">[18]</ref> embeds entities in each view of GCN to realize cross-language entity pair and supplement knowledge map. In addition to paying attention to the characteristics of entities themselves, Gao et al. also considered the characteristics of relational semantics and entity attributes to learn the structural features of entities better. Zhou et al. <ref type="bibr" target="#b18">[19]</ref> used grammar dependence and GCN to establish a commonsense knowledge map, and GCN could flexibly combine syntactic information into emotion analysis tasks. Zhang et al. <ref type="bibr" target="#b19">[20]</ref> used GCN learning to extract more accurate features. <ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref> and <ref type="bibr" target="#b31">[32]</ref> The effectiveness of convolutional neural networks is proved.</p><p>Causality extraction <ref type="bibr" target="#b20">[21]</ref> can quickly extract causal entities for explicit causality, but it often requires inferring to extract causality for an implicit causality. The traditional GCN can learn the features of adjacent nodes through the hidden layer to infer, and further inference is needed for the implicit causality of a cascade. Therefore, this paper proposes a two-stage GCN to implement the cascade implicit causality inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Construction of causality candidate entity library</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Data preprocessing</head><p>The main tasks of data preprocessing include two aspects. One is to preliminarily screen the content of the text, delete the valueless information or unify the sentence format. The second is to mark the selected sentences. For example, causality pairs use cause phrases to represent a cause, and result phrases mean an effect. Since this article involves sequence annotations, punctuation is also annotated as words  Sentence:</p><formula xml:id="formula_1">C C C O E E E O O O</formula><p>This paper aims to extract all the causality entity pairs, as shown in Figure <ref type="figure" target="#fig_3">3</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Extraction of causality candidate entities</head><p>Because causality is usually a phrase with more significant emotional tendencies, this paper will prioritize the emotional intensity of the words. The existing sentiment lexicon, such as the sentiment lexicon of the Taiwan University of China and the HowNet sentiment lexicon of CNKI, are all familiar.</p><p>In the field of corpus sentiment analysis, it is impossible to judge the emotional intensity of some specific words accurately, so we need to build a corpus-oriented sentiment lexicon, as follows.</p><p>(1) Data preprocessing, cleaning, and removing incomplete and repetitive data in the corpus. The motivation is to ensure the corpus belongs to the same field. Stop words and special symbols are processed by word segmentation.</p><p>(2) The construction of the word vector model. We use the Word2Vec model in deep learning to transform words into word vectors, which build the foundation for constructing the subsequent neural network.</p><p>( This paper uses the neural network to construct a binary classifier of word emotion. It is known that the training corpus is emotional words, and the corresponding label is the polarity of emotional words.</p><p>Each emotional word is converted into a 100-dimensional word vector through Word2Vec, and the judgment of the sentiment polarity of words belongs to the classification problem. Therefore, we use the fully connected neural network to construct the classifier.</p><p>Step1: Determine the seed word set. According to the characteristics of the fields, the corresponding selection criteria are formulated, and the words in the corpus are extracted as seed words and added into the seed word set.</p><p>Step2: Determine the set of candidate emotion words. The seed words are converted into the word vector and calculated the vector of similarity formula (cosine similarity). N-words that are most similar to each seed word are obtained as the candidate emotion words set.</p><p>Step3: Use the trained classifier to judge the sentiment polarity of each candidate word. the candidate words with sentiment polarity are integrated and added to the sentiment lexicon for specific fields.</p><p>As for the causality in the corpus field, it may be positive or negative emotions, so we consider more about emotional intensity. Firstly, we calculate the emotional intensity of the causality, combined with the emotional intensity of manual annotation in the comment text. Whether a word frequently appears in the corpus and has strong sentiment polarity. When the conditions are met, it can be considered as a candidate seed word. The emotional intensity is divided into four levels: level-0, level-1, level-2, and level-3. The basis for the division mainly considers the following aspects: the completeness of components, the weighted average of the emotional intensity of each word, and the frequency of words that appears in the whole annotation corpus. Therefore, the following definition is given.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 1: Emotional Intensity of Causality Entity (EICE)</head><p>Emotional Intensity of Causality Entity is used to measure the emotional intensity of entity i in the annotated corpus. The sentiment polarity intensity of the causality entity is calculated from the causal word. With the help of the emotional intensity characteristics, the causality entity can be extracted more accurately, as shown in Formula (1).</p><formula xml:id="formula_3">i i * * | * | = i i i EIC F I E W (1)</formula><p>where Fi represents the frequency of causal word i in the whole annotated corpus, Ii represents the emotional intensity of causal word in the sentiment lexicon, and finally takes the absolute value. Ei represents the composition of the causality entity in the corpus (subject, predicate, object, etc.). And Wi represents the initial weight of the causality entity.</p><p>The emotional intensity values calculated by Formula (1) can be used to establish a causality seed lexicon, and appropriate weights can be given to causality entities with different emotional intensity levels. The emotional intensity level is mapped to the range of 1-4, and the integer is selected simultaneously. Therefore, the weights are 0.5, 1, 1.5, and 2, increasing by 1 for each ascending emotional intensity level. On this basis, the K-means clustering algorithm is used to cluster the candidate seed lexicon, and the candidate causality entity set is selected. The specific algorithm process is shown as follows.</p><p>Algorithm The specific examples of the causality candidate entity library are shown in Table <ref type="table" target="#tab_2">1</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Construction of causality extraction model 4.1 Screening of nodes and edges</head><p>The representation of the reference node, candidate entity node, and meta-dependent path (MDP) node is constructed. MDP represents a set of shortest independent paths full reference nodes in a sentence. The representation of the candidate entity node is calculated as the average of the reference node and MDP node. At the same time, the dependency edges constructed by the syntactic dependency graph and semantic dependency graph are regarded as an adjacency matrix. The dependency syntactic graph created is centered on the core verbs in the graph. Therefore, in the input text, each sequence component is a node of the graph, and the dependency between words is the edge of the graph. In this paper, the HanLP tool is used to realize the syntactic analysis and dependency syntax analysis, and it is obtained as shown in Figure <ref type="figure" target="#fig_4">4</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Pre-training</head><p>Context word representation is usually trained on unstructured and unmarked texts that do not contain clear semantics with real-world entities. So it usually cannot remember entities other than these entities.</p><p>For each sentence, use the integrated entity linker to retrieve the relevant entity embedding and then update the context word representation in the form of entity attention. The key idea is to model entities explicitly and use an entity linker to retrieve relevant entity embeddings from the constructed entity library. The motivation is to form knowledge-enhanced entity representation. The model is shown in Figure <ref type="figure" target="#fig_6">5</ref>. 10% of the words are replaced by an arbitrary word, and 10% of the words remain unchanged to predict a word. The model does not know whether the words that embed the corresponding position are correct.</p><p>So it can force the model to rely more on context information to predict words and give the model a specific ability to correct errors.</p><p>Multiple self-attention consists of three parts: query, key, and value, allowing each vector to focus on other vectors. We train BERT to minimize the objective function that combines the next sentence prediction ( NSP ) with masking LM logarithmic likelihood ( MLM ).</p><formula xml:id="formula_4">1 2 1 2 ( , , ) ( , ) ( , ) ÔÅ± ÔÅ± ÔÅ± ÔÅ± ÔÅ± ÔÅ± ÔÅ± =+ BERT NSP MLM L L L (2)</formula><p>where Œò is the encoder parameter in BERT. Œò2 is the parameter in the output layer connected to the encoder in the Mask-LM task, and Œò1 is the classifier parameter associated with the encoder in the prediction task. Therefore, in the first part of the loss function, if the masked word set is M and its size is |V|, so it is also a multi-classification problem, then precisely loss function is formula (3).</p><p>ÔÅõ ÔÅù</p><formula xml:id="formula_5">1 1 i1 ( , )=-log ( | , ), 1, 2,...,| | ÔÅ± ÔÅ± ÔÅ± ÔÅ± = = ÔÉé ÔÉ• M NSP i i L p m m m V (3)</formula><p>It is also a loss function of a classification problem in the sentence prediction task.</p><formula xml:id="formula_6">21 j1 ( , )=-log ( | , ), [ , ] ÔÅ± ÔÅ± ÔÅ± ÔÅ± = =ÔÉé ÔÉ• N MLM i i L p n n n IsNext NotNext (4)</formula><p>Therefore, the joint learning loss function of the two tasks is :</p><formula xml:id="formula_7">1 2 1 1 i 1 j 1 ( , , ) -log ( | , )-log ( | , ) ÔÅ± ÔÅ± ÔÅ± ÔÅ± ÔÅ± ÔÅ± ÔÅ± = = = = = ÔÉ•ÔÉ• M N BERT i i L p m m p n n (5)</formula><p>In the pre-training process, the entity linker is used to link the causal entity library introduced in the previous section, which is mainly divided into the following three sub-modules.</p><p>(1) The candidate entity generation module. It is responsible for detecting the entity mention set M (including all the entities mentioned in the input text) and finding the related entity set Em corresponding to each entity mention m ‚àà M from the given causality entity library.</p><p>(2) Related entity ranking module. It is responsible for scoring and ranking multiple related entities in the related entity set Em (each entity mentions m) and outputs the related entity with the highest score as the entity link result of m.</p><p>(3) Unlinkable mention prediction. It is responsible for predicting which entities mentioned in the input text cannot be linked to the causality entity library.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Two-stage GCN</head><p>In this section, we use two-stage training GCN. GCN mainly learns entity features and corresponding relationship features in the first stage. GCN mainly considers implicit features between all entities and further infers causality in the second stage.</p><p>The first stage is GCN, and the original input is a sentence sequence. After BERT pre-training, the entity node and edge are obtained. The first stage uses GCN to extract regional dependency features. The principle of GCN is shown in Figure <ref type="figure" target="#fig_7">6</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Intput</head><p>In each layer, ReLU represents the feature activation function. In this paper, three hidden layers are selected. In general, GCN accepts all the vertex feature messages transmitted from the former layer, makes corresponding transformations, and adds them together. Finally, an activation function is used as the output of this layer. For each hidden layer, there is formula <ref type="bibr" target="#b5">(6)</ref>.</p><p>( )</p><formula xml:id="formula_8">( ) 1 Re + ÔÉé ÔÉ¶ÔÉ∂ =+ ÔÉßÔÉ∑ ÔÉßÔÉ∑ ÔÉ®ÔÉ∏ ÔÉ• l l u u u N u LU Wh b h (6)</formula><p>Here, ‚Ñé ùë¢ ùëô Shows the features of word u in hidden layer l, including all the words transmitted from word u and all the words introduced, including word u itself. W represents the weight. We connect the output and input word features as the last word features. Firstly, each node sends its feature information to neighbor nodes after transformation and extracts the feature information of the node. This step is to integrate the local structural information of nodes, namely the sum operation in the above formula <ref type="bibr" target="#b5">(6)</ref> (for all neighbor nodes). And then do nonlinear transformation after gathering the previous information to increase the expression ability of the model.</p><p>The entity is predicted using the word features extracted from GCN, and the causality between entities is extracted. The dependency edges are removed and all entities are predicted. Based on the results of three hidden layers, entity causality is obtained.</p><p>( ) ( )</p><formula xml:id="formula_9">1 12 3 1 2 2 w , , Re =ÔÉÖ r r w r w ce w S W LU W h W h<label>(7)</label></formula><p>(w1,r,w2)denotes the scores of entity pairs (w1,w2) obtained under causality. When extracting causal triples, the relationship between each word pair is judged and identified as causality as possible.</p><p>In the second stage GCN, the entities and relationships extracted in the first stage are not very good for long-distance. Therefore, the second stage GCN is proposed to extract the implicit cascade causalities for long-distance multi-hop inference. The second stage considers the implicit features between all causal entities in the text so that the accuracy of the extraction is higher. In the first stage, a complete correlation weighted graph is established for each pair of causality. Where (w1, w2)is the weight of the edge to represent the probability that w1 and w2 entities are causalities. To extract the causality between each entity pair more accurately, the GCN in the second stage should carry out weighted propagation to achieve a more robust relationship prediction. The formula is shown as <ref type="bibr" target="#b7">(8)</ref> to propagate between hidden layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>( ) (</head><p>)</p><formula xml:id="formula_10">r h Re P , +b ÔÉé ÔÉ¶ ÔÉ∂ = ÔÇ¥ + ÔÉß ÔÉ∑ ÔÉ® ÔÉ∏ ÔÉ• l+1 l l l l u r v r u vV LU u v W h h (8)</formula><p>where Pr(u,v) denotes the weight with edge weight as edge, which indicates the probability that the two entities w1 and w2 are causal, and Wr and br are the weight text of layer l of GCN hidden layer, which excludes all words and all relationships.</p><p>Finally, a threshold is set to extract causality entity pairs. If Pr(u,v)&gt; 0.5, the entity pair is considered to have a causality, and vice versa. The classification module takes the target entity pair (w1, w2) as input, and we stack the embedding of (w1, w2) together to infer the underlying relationship between each pair of entities. So we can obtain the causality of each pair of entities, as shown in formula 9.</p><p>( )</p><formula xml:id="formula_11">( ) ( ) 12 r 1 2 w , , P w , , softmax = ce w ce w S (<label>9</label></formula><formula xml:id="formula_12">)</formula><p>Here we use cross-entropy as the final classification loss function, as shown in formula 10.</p><p>( ) ri oss log P w , ,</p><formula xml:id="formula_13">ÔÉéÔÇπ = ÔÉ•ÔÉ• j s S i j L ce w<label>(10)</label></formula><p>S is a set representing a collection of all entities, Pr(wi,wj) denotes the probability that entities wi and wj are causality. The entire two-stage GCN algorithm process is as follows.</p><p>Algorithm 2: Two-stage GCN algorithm</p><formula xml:id="formula_14">Input: G=(V,E), Entity 12 { } , n V V V V = ÔÇºÔÇºÔºå , dependency edge 12 { } ,, n E E E E = ÔÇºÔÇº Output: Causal entity triples (wi , cause-effect, wj) 1.For i in 12 { } , n V V V V = ÔÇºÔÇºÔºå : 2.</formula><p>Calculate similarity( vi, vj ),learning features; 3.</p><p>Propagation between hidden layers: ( ) ( )  </p><formula xml:id="formula_15">, = l l l f H A d AH W</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental setup</head><p>The datasets used are from open source databases SCIFI <ref type="bibr" target="#b27">[28]</ref>„ÄÅ ECauSE Corpus2.0 <ref type="bibr" target="#b28">[29]</ref>„ÄÅ CaTeRS <ref type="bibr" target="#b29">[30]</ref>„ÄÅ NYT <ref type="bibr" target="#b15">[16]</ref>„ÄÅWebNLG <ref type="bibr" target="#b15">[16]</ref>. SCIFI contains a total of 1270 valid data, 1803 sentences in the ECauSE Corpus2.0 corpus contain causality, and a third of them involve overlapping relationships. CaTeRS annotates a total of 1600 sentences in 320 five-sentence short stories extracted from the ROCStories corpus, which all contain causality. NYT and WebNLG are datasets for relation extraction, which contain overlapping entity relations. Among them, NYT contains 1230 sentences with causality, and WebNLG contains 1420 sentences. The dataset is mainly divided into a training set, test set, and verification set, which are divided according to 8: 1: 1. The construction of the dataset is shown in Table <ref type="table" target="#tab_4">2</ref>. We use SCIFI and NYT datasets to verify our method. To verify the effect of this model on causality extraction, the specific experimental operation is as follows.</p><p>Step 1: The causality dataset is obtained. The open-source databases SCIFI, ECauSE Corpus2.0, CaTeRS, NYT, and WebNLG are used for causality extraction. After denoising, 7323 sentences that are conducive to causal analysis are selected.</p><p>Step 2: Causal entity labeling. For the 7323 text sentences obtained, the causal entity labeling is unified. For the entities appearing in the sentences, the special symbol # is used to mark them, such as #entity#, and 7323 sentences with causal entity labeling are obtained.</p><p>Step 3: Candidate causality entity. Extract causality entity from a corpus and put it into entity library.</p><p>Step4: Extract the causal entity triples from the obtained review text. The BERT pre-training is used to convert the text with the semantics of each word into a word vector. At the same time, the entity linker is added to the pre-training, which can better learn the entity word vector. Then the two-stage GCN is used for feature learning. The first stage is to learn the local feature of the entity node, and the second stage is to learn the global feature. The probability of each pair of causality entities satisfies the condition for the threshold value.</p><p>To make the method more persuasive, we compare the models in many aspects. Firstly, the method is tested on multiple datasets to illustrate the effectiveness of the causality extraction model. At the same time, this paper also conducts comparative experiments on other baseline models. Including Bi-LSTM+GCN, Bi-LSTM+CRF, 3-layers CNN, GP-GCNs, CNN+RNN, CNN+BiGUR+CRF <ref type="bibr" target="#b22">[23]</ref>, Bi-LSTM+Attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experimental analysis</head><p>According to the above analysis, this paper did the following experiments.</p><p>The GCN model directly extracts the causal triples. The Bi-LSTM context <ref type="bibr" target="#b23">[24]</ref> coding is mainly to  <ref type="table" target="#tab_5">3</ref>. It can be seen from Table <ref type="table" target="#tab_6">4</ref> that each overlapping entity needs to be inferred, and GCN can achieve this inference. But the effect of using GCN only once is less than that of two-stage GCN, as shown in Figure <ref type="figure" target="#fig_10">7</ref>. The number of layers of GCN is also an essential factor. To prove the influence of the number of layers, we also compare the models of different layers. It can be seen from Figure <ref type="figure" target="#fig_11">8</ref>. In the two datasets, the second layer has the best effect. Although the impact of three layers is also good, the time of three layers is significantly more than two layers. It shows that more layers are considered in the inference process will lead to better performance, especially when there are more entities. The above experiments show that the cascade causality entity extraction relies not just on local features but also needs an inference of global features. To get more comprehensive causal entities, each pair of causality entity are judged. The above experiments prove the effectiveness of this method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>For how to effectively extract causality, we propose a causality extraction model based on a two-stage GCN. This model mainly uses two-stage GCN to extract causality and analyzes all causal entity triples in the text. Especially the cascade implicit causality, to realize the deep extraction under semantic enhancement. The contributions of this paper mainly include the following aspects.</p><p>(1) The two-stage GCN has been proposed for entity relationship inferring. In the first stage, the local features of adjacent entity nodes are learned, and in the second stage, the features of all nodes are learned. The inferring of long-distance learning entities was carried out. Each pair of entities made causality judgment, and more cascade causality was identified.</p><p>(2) The causal entity library has been constructed. In the pre-training, the entity linker was added to find the three closest entities from the entity library so that the pre-training entity vector can learn more features. At the same time, it can better learn the features of the entity. This paper introduced the new concept of the difference coefficient of positive and negative corpus (DC-PNC) to judge the sentiment polarity of words. To a certain extent, the combination of PMI and DC-PNC improved the screening precision of new sentiment words.</p><p>In the future, the method in this paper can be considered to be applied to all relational extraction texts.</p><p>The causality extraction model based on two-stage GCN can help the software platform or relevant departments to extract the causality effectively. It can carry out management measures or coping strategies, help make the best decisions, and build the foundation for subsequent emotional analysis.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Strong reasoning of two-stage GCN. When the proposed method is used to construct the causal relationship extraction model, to further strengthen the reasoning ability of the model, a two-stage GCN reasoning model is introduced. This paper is organized as follows. Section 2 introduces the related work of relation extraction. Section 3 gives the method of building the causality entity library, and Section 4 shows the construction method of the causality extraction model. Section 5 provides the experimental analysis of the model. Section 6 summarizes this article and future work.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>(</head><label></label><figDesc>labeled ' O ' ). The causality trigger words are not labeled. The causality extraction in this paper is not limited to the explicit causal relationship with markers. The labeling example is shown in Figure 2. C represents the cause, O represents the other, and E represents the effect.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Sample causation annotation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Examples of entity extraction</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>)( 4 )</head><label>4</label><figDesc>The construction of a neural network. Through the dataset, we construct the corpus needed for training. At the same time, the word vector model is used to convert words into vectors for neural network training, and the emotional classifier is obtained finally. Construction of domain sentiment lexicon. The sentiment lexicon is mainly composed of the obtained sentiment lexicon and the candidate emotional words in the corpus field. The neural network classifier is used to judge its sentiment polarity, and the required domain sentiment lexicon is obtained.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .[</head><label>4</label><figDesc>Figure 4. Semantic dependency graph and dependency syntax graph</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Pre-training process</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. The principle of GCN</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>14 .End Algorithms 2 ,</head><label>142</label><figDesc>Steps 1-6 describe the algorithm process of GCN in the first stage, mainly learning the local characteristics of nodes. Then construct a new graph. Steps 7-10 illustrate the process of the GCN algorithm in the second stage. Steps 11-14 completed the extraction of causality triples and improved extraction accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>improve the pre-training process. The CNN model is often used to learn local features and three layers are selected here. GCN can learn long-distance relationship inference. The causality extraction model based on two-stage GCN, Bi-LSTM+GCN, Bi-LSTM+CRF, 3-layer GNN, GP-GCNs, CNN+RNN, CNN+BiGUR+CRF are used. Bi-LSTM+Attention extracts causality from SCIFI and ECauSE Corpus2.0 datasets and calculates the recall. Finally, the experimental results are compared, and the specific experimental results are shown in Table</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Comparison learning ability of two-stage and one-stage GCN</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Comparison of Layers of GCN</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>means clustering algorithm Construction of causality candidate lexicon BERT pre-training The first stage GCN causality extraction The second stage GCN Entity nodes edge</head><label></label><figDesc></figDesc><table><row><cell>Text</cell><cell>preprocessing</cell><cell>average weighted</cell><cell>emotional intensity calculation</cell><cell>sort</cell><cell>Emotional intensity phrases</cell><cell>noun</cell><cell>candidate library</cell></row><row><cell>sentiment</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>k-</cell><cell></cell><cell></cell></row><row><cell>lexicon</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p><p>‚Ä¢ Extraction of causality tuples. Semantic encoding by BERT combined with context. The reference nodes, entity nodes and meta-dependent path (MDP) nodes are constructed. So candidate entities are obtained. Its also are represented as node inputs, and the syntactic relations obtained from the syntactic dependency graph are introduced into the two-stage GCN network as edges. Use the full connection to capture more structural information and train deeper models.</p>Figure 1. the framework of the two-stage GCN model</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>1: Construction of the causality candidate library algorithm Input:</head><label></label><figDesc>Preprocessed text S = { S1, S2,..., Sn } Output: The candidate causality entity library L = { l1, l2,......, ln }</figDesc><table><row><cell cols="3">1.for i in Si,S = { S1, S2,..., Sn }:</cell><cell>// select each sentence after preprocessing</cell></row><row><cell cols="2">2. for j in wi:</cell><cell>//wi represents each word</cell></row><row><cell>3.</cell><cell cols="2">calculated EICi ;</cell></row><row><cell>4.</cell><cell cols="2">sentence partition Set D = { D1, D2,... Dn } ;</cell></row><row><cell>5.</cell><cell cols="2">calculate the Avg (EICi) of each phrase Dt ;</cell></row><row><cell>6.</cell><cell cols="2">select 2 Max (Dt) ;</cell></row><row><cell cols="3">7.for Dt in D : / / select phrases with strong polarity ;</cell></row><row><cell cols="2">8. if is noun :</cell><cell>// judge whether this action is a noun</cell></row><row><cell>9.</cell><cell cols="2">extracts co-exist in the causal candidate library L = { l1, l2,......, ln };</cell></row><row><cell cols="2">10. end for;</cell></row><row><cell>12.end</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>examples of the causality candidate entity library</figDesc><table><row><cell>phrase</cell><cell cols="3">component Weight Emotional intensity</cell></row><row><cell>Account balance</cell><cell>subject</cell><cell>1</cell><cell>1</cell></row><row><cell cols="2">Accident year basis subject</cell><cell>2</cell><cell>3</cell></row><row><cell>Abatement of tax</cell><cell>object</cell><cell>1.5</cell><cell>2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 .</head><label>2</label><figDesc>Dataset construction</figDesc><table><row><cell>Dataset</cell><cell cols="3">Train Test Verify</cell></row><row><cell>SCIFI</cell><cell cols="2">1443 127</cell><cell>127</cell></row><row><cell cols="3">ECauSE Corpus2.0 1016 180</cell><cell>180</cell></row><row><cell>CaTeRS</cell><cell cols="2">1280 160</cell><cell>160</cell></row><row><cell>NYT</cell><cell>984</cell><cell>123</cell><cell>123</cell></row><row><cell>WebNLG</cell><cell cols="2">1136 142</cell><cell>142</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 .</head><label>3</label><figDesc>Experimental resultThe results of table3show that based on the two-stage GCN causality extraction method for SCIFI and CaTeRS datasets, it has a higher recall score. Contrast Bi-LSTM+GCN, Bi-LSTM+CRF, 3-layer GNN, GP-GCNs, CNN+RNN, CN+BiGUR+CRF, Bi-LSTM+Attention, can be found with GP-GCNs in SCIFI dataset recall is higher than other methods.For ECauSE Corpus2.0, NYT and WebNLG, due to many overlapping relationships, the overlapping relationships are mainly divided into the following types, as shown in Table4.</figDesc><table><row><cell>Corpus</cell><cell>SCIFI</cell><cell></cell><cell></cell><cell>CaTeRS</cell><cell></cell><cell></cell></row><row><cell></cell><cell>P</cell><cell>R</cell><cell>F</cell><cell>P</cell><cell>R</cell><cell>F</cell></row><row><cell>Bi-LSTM+GCN</cell><cell>0.87</cell><cell>0.82</cell><cell>0.84</cell><cell>0.84</cell><cell>0.82</cell><cell>0.82</cell></row><row><cell>Bi-LSTM+CRF</cell><cell>0.85</cell><cell>0.81</cell><cell>0.83</cell><cell>0.82</cell><cell>0.80</cell><cell>0.81</cell></row><row><cell>3 layerCNN</cell><cell>0.88</cell><cell>0.84</cell><cell>0.86</cell><cell>0.83</cell><cell>0.81</cell><cell>0.82</cell></row><row><cell>GP-GCNs</cell><cell>0.90</cell><cell>0.88</cell><cell>0.89</cell><cell>0.85</cell><cell>0.83</cell><cell>0.84</cell></row><row><cell>CNN+RNN</cell><cell>0.80</cell><cell>0.77</cell><cell>0.78</cell><cell>0.77</cell><cell>0.75</cell><cell>0.76</cell></row><row><cell>CNN+ BiGUR+CRF</cell><cell>0.82</cell><cell>0.80</cell><cell>0.81</cell><cell>0.78</cell><cell>0.77</cell><cell>0.77</cell></row><row><cell>Bi-LSTM+Attention</cell><cell>0.81</cell><cell>0.80</cell><cell>0.80</cell><cell>0.76</cell><cell>0.74</cell><cell>0.75</cell></row><row><cell>Two-stage GCN</cell><cell>0.92</cell><cell>0.89</cell><cell>0.90</cell><cell>0.88</cell><cell>0.86</cell><cell>0.87</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 .</head><label>4</label><figDesc>Relationship of overlapping entities</figDesc><table><row><cell></cell><cell>Entity pair overlap</cell><cell>Relation</cell><cell>Location</cell></row><row><cell>X</cell><cell>Y</cell><cell>intersection</cell><cell>The end of X equals the start of Y</cell></row><row><cell>X Y</cell><cell></cell><cell>overlap</cell><cell>XY starts the same</cell></row><row><cell>Y</cell><cell>X</cell><cell>overlap</cell><cell>The end of XY is the same</cell></row><row><cell>X</cell><cell>Y</cell><cell>intersection</cell><cell>The end of X equals the start of Y</cell></row><row><cell>X</cell><cell>Y</cell><cell>overlap</cell><cell>XY overlaps at a certain position</cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conflict of Interest:</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Sentiment classification model for Chinese micro-blog comments based on key sentences extraction</title>
		<author>
			<persName><forename type="first">Zhang</forename><surname>Shunxiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hu</forename><surname>Zhaoya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhu</forename><surname>Guangli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Ming</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Kuan-Ching</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Soft Computing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="463" to="476" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The method for extracting new login sentiment words from Chinese micro-blog basedf on improved mutual information</title>
		<author>
			<persName><forename type="first">Liu</forename><surname>Wenting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhu</forename><surname>Guangli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhang</forename><surname>Shunxiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Intelligent Systems and Computing</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1017</biblScope>
			<biblScope unit="page" from="1394" to="1403" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Hierarchy-cutting model based association semantic for analyzing domain topic on the web</title>
		<author>
			<persName><forename type="first">Xu</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhang</forename><surname>Shunxiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kim-Kwang</forename><surname>Choo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mei</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luo</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hu</forename><surname>Xiangfeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liu</forename><surname>Chuanping</surname></persName>
		</author>
		<author>
			<persName><surname>Yunhuai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Industrial Informatics</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1941" to="1950" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Automatic extraction of causal relations from text using linguistictally informed deep neural networs</title>
		<author>
			<persName><forename type="first">Dasgupta</forename><surname>Tirthankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saha</forename><surname>Rupsa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dey</forename><surname>Lipika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naskar</forename><surname>Abir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the SIGDIAL 2018 Conference</title>
		<meeting>the SIGDIAL 2018 Conference<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018-07">2018. July, 2018</date>
			<biblScope unit="page" from="306" to="316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Idirectional long short-term memory networks for relation classification</title>
		<author>
			<persName><forename type="first">Zhang</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Dequan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Hu Xinchen</surname></persName>
		</author>
		<author>
			<persName><surname>Ming</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">th Pacific Asia Conference on Language, Information and Computation(PACLIC)</title>
		<meeting><address><addrLine>Shanghai, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-10-30">2015. October 30-November 1, 2015</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="73" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Causal relation identification using convolutional neural networks and knowledge based features</title>
		<author>
			<persName><forename type="first">Silva</forename><surname>Tharini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhibo</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kezhi</forename><surname>Mao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">World Academy of Science</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="697" to="702" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Knowledge-oriented Convolutional Neural Network for Causal Relation Extraction from Natural Language Texts</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Pengfei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mao</forename><surname>Kezhi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Expert Systems With Applications</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="512" to="523" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Extracting causal relations from the literature with word vector mapping</title>
		<author>
			<persName><forename type="first">An</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Yongbo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Jiaoyun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gil</forename><surname>Alterovitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers in Biology and Medicine. Online</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page">103524</biblScope>
			<date type="published" when="2019-11-27">2019. 27 November, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Causal pathway extraction from web-board documents</title>
		<author>
			<persName><forename type="first">Piriyakul</forename><surname>Pechsiri Chaveevan</surname></persName>
		</author>
		<author>
			<persName><surname>Rapepun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Sciences-Basel. Online</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">21</biblScope>
			<biblScope unit="page">10342</biblScope>
			<date type="published" when="2021-11-03">2021. 3 November, 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Causal relationship extraction from biomedical text using deep neural models: A comprehensive survey</title>
		<author>
			<persName><forename type="first">Abbas</forename><surname>Akkasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mari-Francine</forename><surname>Moens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Biomedical Informatics. Online</title>
		<imprint>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page">103820</biblScope>
			<date type="published" when="2021-05-24">2021. May 24, 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Extracting emporal and causal relations based on event networks</title>
		<author>
			<persName><forename type="first">Feras</forename><surname>Duc-Thuanvo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ebrahim</forename><surname>Al-Obeidat</surname></persName>
		</author>
		<author>
			<persName><surname>Bagheri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing &amp; Management. Online</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">102319</biblScope>
			<date type="published" when="2020-06-20">2020. June 20, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Extraction of causal relations based on SBEL and BERT model</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Shao Yifan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gu</forename><surname>Haoru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Jinghang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Longhua</surname></persName>
		</author>
		<author>
			<persName><surname>Guodong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Database. Open Access</title>
		<imprint>
			<biblScope unit="volume">2021</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Overview of graph convolution neural network</title>
		<author>
			<persName><forename type="first">Xu</forename><surname>Bingbing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cen</forename><surname>Keting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huang</forename><surname>Junjie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shen</forename><surname>Huawei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Xueqi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer Science</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="755" to="780" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">COMET: commonsense transformers for automatic knowledge graph construction</title>
		<author>
			<persName><forename type="first">Bosselut</forename><surname>Antoine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rashkin</forename><surname>Hannah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sap</forename><surname>Maarten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Malaviya</forename><surname>Chaianya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Choi</forename><surname>Celikyilmaz</surname></persName>
		</author>
		<author>
			<persName><surname>Yejin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-08-02">July 28-August 2, 2019</date>
			<biblScope unit="page" from="4762" to="4779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Graph neural networks with generated parameters for relation extraction</title>
		<author>
			<persName><forename type="first">Zhu</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Yankai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fu</forename><surname>Liu Zhiyuan</surname></persName>
		</author>
		<author>
			<persName><surname>Jie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-08-02">July 28-August 2, 2019</date>
			<biblScope unit="page" from="1331" to="1339" />
		</imprint>
	</monogr>
	<note>Chua Tat-seng, Sun Maosong</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">GraphRel: Modeling text as relational graphs for joint entity and relation extraction</title>
		<author>
			<persName><forename type="first">Fu</forename><surname>Tsu-Jui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Peng-Hsuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ma</forename><surname>Wei-Yun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-08-02">July 28-August 2, 2019</date>
			<biblScope unit="page" from="1409" to="1419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Joint event extraction along shortest dependency path susing graph convolutional networks</title>
		<author>
			<persName><forename type="first">Ali</forename><surname>Balalia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masoud</forename><surname>Asadpoura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Camposb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Jatowt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge-Based Systems, Online</title>
		<imprint>
			<biblScope unit="volume">210</biblScope>
			<biblScope unit="page">106492</biblScope>
			<date type="published" when="2020-10-01">2020. October 1, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">MHGCN: Multiview highway graph convolutional network for cross-lingual entity alignment</title>
		<author>
			<persName><forename type="first">Jianliang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><surname>Xiangyue</surname></persName>
		</author>
		<author>
			<persName><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Yibo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tsinghua Science and Technology</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="719" to="728" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">SK-GCN: Modeling syntax and knowledge via graph convolutional network for aspect-level sentiment classification. Knowledge-Based Systems</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><forename type="middle">Xiangji</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinminvivian</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Online</title>
		<imprint>
			<biblScope unit="volume">205</biblScope>
			<biblScope unit="page">106292</biblScope>
			<date type="published" when="2020-07-17">2020. July 17, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A graph-voxel joint convolution neural network for ALS point cloud segmentation</title>
		<author>
			<persName><forename type="first">Zhang</forename><surname>Jinming</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dai</forename><surname>Hu Xiangyun</surname></persName>
		</author>
		<author>
			<persName><surname>Hengming</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="139781" to="139791" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">An overview of event causality extraction based on deep learning</title>
		<author>
			<persName><forename type="first">Wang</forename><surname>Zhujun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Xueqing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhu</forename><surname>Junwu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer applications</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1247" to="1255" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Event causal relationship extraction based on the stratified conditional random field</title>
		<author>
			<persName><forename type="first">Fu</forename><surname>Jianfeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liu</forename><surname>Liu Zongtian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern recognition and artificial intelligence</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="567" to="573" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Event causality extraction method based on double CNN-BiGRU-CRF model</title>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Qiaofen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wu</forename><surname>Zhendong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zou</forename><surname>Junying</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer engineering</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="58" to="64" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Knowledge enhanced contextual word representations</title>
		<author>
			<persName><forename type="first">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neumann</forename><surname>Matthew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Logan</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Schwartz</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshi</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Singh</forename><surname>Vidur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Smith</forename><forename type="middle">A</forename><surname>Sameer</surname></persName>
		</author>
		<author>
			<persName><surname>Noah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of 9th International Joint Conference on Natural Language Processing(IJCNLP)</title>
		<meeting>eeding of 9th International Joint Conference on Natural Language essing(IJCNLP)<address><addrLine>Suzhou, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-12-04">2019. December 4-7, 2019</date>
			<biblScope unit="page" from="43" to="54" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A convolutional neural network and graph convolutional network-based method for predicting the classification of anatomical therapeutic chemicals</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Zhao Haochen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename><surname>Yaohang</surname></persName>
		</author>
		<author>
			<persName><surname>Jianxin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page" from="2841" to="2847" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Graph edge convolutional neural networks for skeleton-based action recognition</title>
		<author>
			<persName><forename type="first">Xu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Dacheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhang</forename><surname>Xikun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tian</forename><surname>Xinmei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3047" to="3060" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Spherical kernel for efficient graph convolution on 3D point clouds</title>
		<author>
			<persName><forename type="first">Huan</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naveed</forename><surname>Akhtar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ajmal</forename><surname>Mian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="3664" to="3680" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Causality Extraction based on Self-Attentive BiLSTM-CRF with Transferred Embeddings</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Zhaoning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ren</forename><surname>Zou Xiaotian</surname></persName>
		</author>
		<author>
			<persName><surname>Jiangtao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing, Online</title>
		<imprint>
			<biblScope unit="volume">423</biblScope>
			<biblScope unit="page" from="207" to="219" />
			<date type="published" when="2020">2021. October 24 , 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The BECauSE Corpus 2.0: Annotating Causality and Overlapping Relations</title>
		<author>
			<persName><forename type="first">Dunietz</forename><surname>Jesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Levin</forename><forename type="middle">S</forename><surname>Lori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Carbonell</surname></persName>
		</author>
		<author>
			<persName><surname>Jaime</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LAW 2017-11th Linguistic Annotation Workshop, Proceedings of the Workshop</title>
		<meeting><address><addrLine>Valencia, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-03">2017. April 3, 2017</date>
			<biblScope unit="page" from="95" to="104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">CaTeRS: Causal and temporal relation scheme for semantic annotation of event structures</title>
		<author>
			<persName><forename type="first">Mostafazadeh</forename><surname>Nasrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grealish</forename><surname>Alyson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chambers</forename><surname>Nathanael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Allen</forename><forename type="middle">F</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vanderwende</forename><surname>Lucy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies(NAACL-HLT)</title>
		<meeting><address><addrLine>San Diego, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-17">2016. June 17, 2016</date>
			<biblScope unit="page" from="51" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Richer Event Description: Integrating event coreference with temporal, causal and bridging annotation</title>
		<author>
			<persName><forename type="first">Wright-Bettner</forename><surname>O'gorman Tim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Palmer</forename><surname>Kristin</surname></persName>
		</author>
		<author>
			<persName><surname>Martha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd Workshop on Computing News Storylines</title>
		<meeting>the 2nd Workshop on Computing News Storylines<address><addrLine>Austin, TX</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-11-05">2016. November 5, 2016</date>
			<biblScope unit="page" from="47" to="56" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Application of convolution neural network in web query session mining for personalised web search</title>
		<author>
			<persName><forename type="first">Suruchi</forename><surname>Chawla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computational Science and Engineering</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="417" to="428" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Causal event extraction using causal event element-oriented neural network</title>
		<author>
			<persName><forename type="first">Xu</forename><surname>Kai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luo</forename><surname>Xiangfeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gao</forename><surname>Jianqi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computational Science and Engineering</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="621" to="628" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Semantic segmentation of high-resolution remote sensing images using fully convolutional network with adaptive threshold</title>
		<author>
			<persName><forename type="first">Gao</forename><surname>Wu Zhihuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Yongming</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xue</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Junshi</surname></persName>
		</author>
		<author>
			<persName><surname>Yuntao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Connection Science</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="169" to="184" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A community partitioning algorithm based on network enhancement</title>
		<author>
			<persName><forename type="first">Wang</forename><surname>Hu Junjie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Zhanquan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dai</forename><surname>Jiequan</surname></persName>
		</author>
		<author>
			<persName><surname>Yonghui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Connection Science</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="42" to="61" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
