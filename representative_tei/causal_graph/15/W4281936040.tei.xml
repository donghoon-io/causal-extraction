<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CAUSALITY LEARNING WITH WASSERSTEIN GENERATIVE ADVERSARIAL NETWORKS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Hristo</forename><surname>Petkov</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer and Information Sciences</orgName>
								<orgName type="institution">University of Strathclyde</orgName>
								<address>
									<settlement>Glasgow</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Colin</forename><surname>Hanley</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Management Science</orgName>
								<orgName type="institution">University of Strathclyde</orgName>
								<address>
									<settlement>Glasgow</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Feng</forename><surname>Dong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer and Information Sciences</orgName>
								<orgName type="institution">University of Strathclyde</orgName>
								<address>
									<settlement>Glasgow</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CAUSALITY LEARNING WITH WASSERSTEIN GENERATIVE ADVERSARIAL NETWORKS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.5121/ijaia.2022.13301</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-21T20:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Generative Adversarial Networks</term>
					<term>Wasserstein Distance</term>
					<term>Bayesian Networks</term>
					<term>Causal Structure Learning</term>
					<term>Directed Acyclic Graphs</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Conventional methods for causal structure learning from data face significant challenges due to combinatorial search space. Recently, the problem has been formulated into a continuous optimization framework with an acyclicity constraint to learn Directed Acyclic Graphs (DAGs). Such a framework allows the utilization of deep generative models for causal structure learning to better capture the relations between data sample distributions and DAGs. However, so far no study has experimented with the use of Wasserstein distance in the context of causal structure learning. Our model named DAG-WGAN combines the Wasserstein-based adversarial loss with an acyclicity constraint in an auto-encoder architecture. It simultaneously learns causal structures while improving its data generation capability. We compare the performance of DAG-WGAN with other models that do not involve the Wasserstein metric in order to identify its contribution to causal structure learning. Our model performs better with high cardinality data according to our experiments.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Causal relationships constitute important scientific knowledge. Bayesian Networks (BN) are graphical models representing casual structures between variables and their conditional dependencies in the form of Directed Acyclic Graphs (DAG). They are widely used for causal inference in many application areas such as medicine, genetics and economics <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref>.</p><p>Learning causal structure from data is difficult. One of the major challenges arises from the combinatorial nature of the search space. Increasing the number of variables leads to a superexponential increase of possible DAGs. This makes the problem computationally intractable.</p><p>Over the last few years, several methods have been proposed to address the problem of discovering DAGs from data <ref type="bibr" target="#b3">[4]</ref>, including the score-based methods and constraint-based methods. Kuipers-et-al <ref type="bibr" target="#b4">[5]</ref>, Heckerman-et-al <ref type="bibr" target="#b5">[6]</ref> and Bouckaert <ref type="bibr" target="#b6">[7]</ref> have proposed score-based methods (SBMs). They formulate the causal structure learning problem into the optimization of a score function with acyclicity constraint. SBMs rely on the utilization of general optimization techniques to discover DAG structures. Additional structure assumptions and approximate searches are often needed because the complexity of the search space remains super-exponential.</p><p>Pearl <ref type="bibr" target="#b7">[8]</ref>, Spirtes-et-al <ref type="bibr" target="#b0">[1]</ref> and Zhang <ref type="bibr" target="#b8">[9]</ref> have proposed constraint-based methods (CBMs) which utilize conditional independence tests to reduce the DAG search space and provide graphs that satisfy a set of conditional independencies. However, CBMs often lead to spurious results. Also they are not very robust over sampling noise. Some works have tried to combine SBMs with CBMs. For example, in the MMHC algorithm <ref type="bibr" target="#b9">[10]</ref>, Hill Climbing is used as the score function and Min-Max Parents and Children (MMPC) is used to check for relations between variables.</p><p>Recently Zheng et al. <ref type="bibr" target="#b10">[11]</ref> made a breakthrough by transforming the causal structure learning problem from combinatorial to continuous optimization with an acyclicity constraint, which can be efficiently solved using conventional optimization methods. This technique has been extended to cover nonlinear cases. Yu et al. <ref type="bibr" target="#b11">[12]</ref> have proposed a model named DAG-GNN based on a variational auto-encoder architecture. Their method can handle linear, non-linear, continuous and categorical data. Another model named GraN-DAG <ref type="bibr" target="#b12">[13]</ref> has also been proposed to learn causality from both linear and non-linear data.</p><p>Our work is related to the use of generative adversarial networks (GANs) for causal structure learning. GANs is a powerful approach to generate synthetic data. They have also been experimented with to synthesize tabular data <ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref>. To leverage GANs for causal structure learning, a fundamental question is to identify how much the data distribution metrics contribute to causal structure learning. Recently, Gao et al. <ref type="bibr" target="#b15">[16]</ref> developed a GAN-based model (DAG-GAN) that learns causal structures from tabular data based on Maximum Mean Discrepancy (MMD) <ref type="bibr" target="#b16">[17]</ref>. In contrast, our work has investigated Wasserstein GAN (WGAN) for learning causal structures from tabular data. The Wasserstein distance metric from optimal transport distance <ref type="bibr" target="#b17">[18]</ref> is an established metric that preserves basic metric properties <ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref><ref type="bibr" target="#b21">[22]</ref> which has led to Wasserstein GAN (WGAN) to achieve significant improvement in training stability and convergence by addressing the gradient vanishing problem and partially removing mode collapse <ref type="bibr" target="#b22">[23]</ref>. However, to the best of our knowledge, so far no study has been conducted to experiment with the Wasserstein metric for causality learning.</p><p>Our proposed new DAG-WGAN model combines WGAN-GP with an auto-encoder. A critic (discriminator) is involved to measure the Wasserstein distance between the real and synthetic data. In essence, the model learns causal structure in a generative process that trains the model to realistically generate synthetic data. With the explicit modelling of learnable causal relations (i.e. DAGs), the model learns how to generate synthetic data by simultaneously optimizing the causal structure and the model parameters via end-to-end training. We compare the performance of DAG-WGAN with other models that do not involve the Wasserstein metric in order to identify the contribution from the Wasserstein metric in causal structure learning.</p><p>According to our experiments, the new DAG-WGAN model performs better than other models by a margin in tabular data with wide columns. The model works well with both continuous and discrete data while being capable of producing less noisy and more realistic data samples. It can handle multiple data types, including linear, non-linear, continuous and discrete. We conclude that the involvement of the Wasserstein metric helps causal structure learning in the generative process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>DAGs consist of a set of nodes (variables) and directed edges (connections) between the nodes to indicate direct causal relationships between the variables. When a DAG entails conditional independencies of the variables in a joint distribution, the faithfulness condition allows us to recover the DAG from the joint distribution <ref type="bibr" target="#b23">[24]</ref>. We perform DAG learning with data distributions that are exhibited in data samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Traditional DAG Learning Approaches</head><p>There are three main approaches for learning DAGs from data, including the constraint-based, score-based and hybrid approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.1.">Constraint-based Approach</head><p>Constraint-based search methods limit the graph search space by running local independence tests to reduce the combinatorial complexity of the search space <ref type="bibr" target="#b24">[25]</ref>. Examples of the constraintbased algorithms include Causal Inference (CI) <ref type="bibr" target="#b7">[8]</ref> and Fast Causal Inference (FCI) <ref type="bibr" target="#b25">[26]</ref>; <ref type="bibr" target="#b8">[9]</ref>. However, typically these methods only produce a set of candidate causal structures that all satisfy the same conditional independencies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.2.">Score-based Approach</head><p>Score-based methods associate a score to each DAG in the search space in order to measure how well each graph fits the data. Typical score functions include Bayesian Gaussian equivalent (BGe) <ref type="bibr" target="#b4">[5]</ref>, Bayesian Discrete equivalent (BDe) <ref type="bibr" target="#b5">[6]</ref>, Bayesian Information Criterion (BIC) <ref type="bibr" target="#b26">[27]</ref>, Minimum Description Length (MDL) <ref type="bibr" target="#b6">[7]</ref>. Assigning a score to each possible graph is difficult due to the combinatorial nature of the problem. As a result, additional assumptions about the DAGs have to be made -the most commonly used ones are bounded tree-width <ref type="bibr" target="#b27">[28]</ref>, treestructure <ref type="bibr" target="#b28">[29]</ref> and sampling-based structure learning <ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.3.">Hybrid Approach</head><p>Hybrid methods involve both score-based and constraint-based methods for DAG learning. A subset of candidate graphs is generated by using independence tests and they are scored by a score function. One example of the hybrid methods is named Max-Min-Hill-Climbing <ref type="bibr" target="#b9">[10]</ref>. Another example is RELAX <ref type="bibr" target="#b32">[33]</ref>, which introduces "constraint relaxation" of possibly inaccurate independence constraints of the search space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">DAG Learning with Continuous Optimization</head><p>Recently, a new approach named DAG-NOTEARS was proposed by Zheng et al. <ref type="bibr" target="#b10">[11]</ref> to transform the causal graph structure learning problem from a combinatorial problem into a continuous optimization problem. This facilitates the use of deep generative models for causal structure learning to leverage relations between data sample distributions and DAGs. However, the original DAG-NOTEARS model can only handle linear and continuous data.</p><p>Based on DAG-NOTEARS, new solutions have been developed to cover non-linearity. Yu et al. <ref type="bibr" target="#b11">[12]</ref> developed DAG-GNN, which performs causal structure learning by using a Variational Auto-Encoder architecture. GraN-DAG proposed by Lachapelle et al. <ref type="bibr" target="#b12">[13]</ref> is another extension from DAG-NOTEARS to handle non-linearity. In GraN-DAG, the calculation of the neural network weights is constrained by the acyclicity constraint between the variables. It can work with both continuous and discrete data. Meanwhile, the latest work in DAG-NoCurl from Yu et al. <ref type="bibr" target="#b33">[34]</ref> shows that it is also possible to learn DAGs without using explicit DAG constraints.</p><p>Notably, DAG-GAN proposed by Gao et al. <ref type="bibr" target="#b15">[16]</ref> is one of the latest works that uses GAN for causal structure learning. The work involves the use of Maximum Mean Discrepancy (MMD) <ref type="bibr" target="#b16">[17]</ref> in its loss function. The concept of "Information Maximization" <ref type="bibr" target="#b34">[35]</ref><ref type="bibr" target="#b35">[36]</ref><ref type="bibr" target="#b36">[37]</ref> is incorporated into the GAN framework. The resulting model handles multiple data types (continuous and discrete). However, their experiments have only covered up to 40 nodes in the graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">GENERATIVE MODEL ARCHITECTURES</head><p>Our model is based on the combination of Auto-encoder (AE) and Wasserstein Generative Adversarial Network (WGAN).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Auto-encoder architecture</head><p>The auto-encoder architecture consists of an encoder which takes input data and produces a latent variable z containing encoded features of the input. The latent variable z is then used by a decoder to reconstructs the input data. This can be represented mathematically as: <ref type="bibr" target="#b0">(1)</ref> where both the encoder Enc and the decoder Dec are functions parameterized by φ and θ respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Wasserstein Generative Adversarial Network</head><p>A standard generative adversarial network consists of a pair of generator/discriminator that compete against each other. The generator synthesizes data samples from random noise samples z. The discriminator determines whether the generated data is real or fake. WGAN is an important improvement over the original GANs. Unlike GANs (in which the discriminator only tells whether the data samples are real or fake), WGAN calculates the distance between the real and fake data samples with Wasserstein Distance. WGAN-GP (Wasserstein GAN with Gradient Penalty) is an improvement of the WGAN model by adding a gradient penalty to satisfy the Lipschitz constraint. The loss function of WGAN-GP can be represented using the following equations:</p><p>(2)</p><p>where the discriminator (critic) D needs to satisfy the 1-Lipschitz constraints, ℙr and ℙg denote the real and synthetic data distributions, and ℙX̂ is a distribution between real and generated data samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">DAG-WGAN</head><p>Our proposed DAG-WGAN model involves causal structure in the model architecture by incorporating an adjacency matrix under an acyclicity constraint -see Figure <ref type="figure" target="#fig_0">1</ref>. The model has two main components: (1) an auto-encoder which computes the latent representations of the input data; and (2) a WGAN which synthesizes the data with adversarial loss. The decoder of the autoencoder is used as the generator for the WGAN to generate synthetic data. The encoder is trained with the reconstruction loss while the decoder is trained according to both the reconstruction and adversarial loss. We cover both continuous and discrete data types. The joint WGANs and autoencoders architecture is motivated by the success of VAE (variational auto-encoder) GAN to better capture data and feature representation <ref type="bibr" target="#b37">[38]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Auto-encoder (AE) and Reconstruction Loss</head><p>The use of the auto-encoder is to make sure that the latent space contains meaningful representations, which are used as the noise input to the generator in the training for adversarial loss. The representations are regularized to prevent over-fitting. Similar to Yu et al. <ref type="bibr" target="#b11">[12]</ref>, we explicitly model the causal structure in both the encoder and decoder by using structural causal model (SCM). The encoder Enc is as follows:</p><p>(3) where f1 is a parameterized function to transform X, X ∈ ℝ m×d is a data sample from a joint distribution of m variables in d dimensions. Z ∈ ℝ m×d is the latent representation. A ∈ ℝ m×m is the weighted adjacency matrix. The corresponding decoder Dec is as follows: <ref type="bibr" target="#b3">(4)</ref> where f2 is also a parameterized function that conceptually inverses f1. The functions f1 and f2 can perform both linear and non-linear transformations on Z and X. Each variable corresponds to a node in the weighted adjacency matrix A.</p><p>The AE is trained through a reconstruction loss, which can be defined as: <ref type="bibr" target="#b4">(5)</ref> where MX is a product of the decoder.</p><p>To avoid over-fitting, a regularizer is added to the reconstruction loss. The regularizer loss term takes the following form: <ref type="bibr" target="#b5">(6)</ref> where MZ is the output of the encoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">WGAN-GP</head><p>The decoder of the auto-encoder is also used as the generator of the WGAN model. Alongside the auto-encoder, we use a critic to provide the adversarial loss with gradient penalty. All together, we utilize the critic loss LD to train the critic, the generator loss LG (Equation <ref type="formula">2</ref>) to train the generator (namely the decoder in the AE), and the reconstruction loss LR (Equation <ref type="formula">5</ref>) together with the regularizer (Equation <ref type="formula">6</ref>) to train both the encoder and decoder.</p><p>The critic is based upon the popular PacGAN <ref type="bibr" target="#b38">[39]</ref> framework with the aim of successfully handling mode collapse and is implemented as follows: <ref type="bibr" target="#b6">(7)</ref> where x ̃ is the data produced from the generator and X is the input data used for the model. Leaky-ReLU is the activation function and Dropout is used for stability and overfitting prevention. GP stands for Gradient Penalty <ref type="bibr" target="#b21">[22]</ref> and is used in the loss term of the critic. Pac is a notion coming from PacGAN <ref type="bibr" target="#b38">[39]</ref> and is used to prevent mode collapse in categorical data, which we found practically useful in terms of improving the outcomes. We use 10 data samples per 'pac' to prevent mode collapse. Note that the number of data samples per 'pac' may vary, however so far we found that 10 produces good results. Furthermore, one might argue that we can remove the 'pac' term entirely when dealing with data which is not categorical. However, our experiment results are not in favour of this idea.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Acyclicity Constraint</head><p>Neither minimizing the reconstruction loss, nor optimizing the adversarial loss ensures acyclicity. Therefore, an additional term needs to be added to the loss function of the model. Here we use the acyclicity constraint proposed by Yu et al. <ref type="bibr" target="#b11">[12]</ref>. <ref type="bibr" target="#b7">(8)</ref> where A is the weighted adjacency matrix of the causal graph, α is a positive hyper-parameter greater, m is the number of the variables, tr is a matrix trace and • is the Hadamard product <ref type="bibr" target="#b39">[40]</ref> of A. This acyclicity requirement associated with DAG structure learning reformulates the nature of the structure learning approach into a constrained continuous optimization. As such, we treat our approach as a constrained optimization problem and use the popular augmented Lagrangian method <ref type="bibr" target="#b40">[41]</ref> to solve it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Discrete Variables</head><p>In addition, our model naturally handles discrete variables by reformulating the reconstruction loss term using the Cross-Entropy Loss (CEL) as follows: <ref type="bibr" target="#b8">(9)</ref> where PX is the output of the decoder and X is the input data for the auto-encoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">EXPERIMENTS</head><p>This section provides experimental results of the model performance by comparing against other related approaches. In particular, our experiments try to identify the contribution from the Wasserstein loss to causal structure learning by making a direct comparison with DAG-GNN <ref type="bibr" target="#b11">[12]</ref> where a similar auto-encoder architecture was used without involving the Wasserstein loss. Furthermore, we have also compared the results against DAG-NOTEARS <ref type="bibr" target="#b10">[11]</ref> and DAG-NoCurl <ref type="bibr" target="#b33">[34]</ref>. All the comparisons are measured using the Structural Hamming Distance (SHD) <ref type="bibr" target="#b41">[42]</ref>. More specifically, we measure the SHD between the learned causal graph and the ground truth graph. Moreover, we also test the integrity of the generated data against CorGAN <ref type="bibr" target="#b13">[14]</ref>.</p><p>The implementation was based on PyTorch <ref type="bibr" target="#b42">[43]</ref>. In addition, we used learning rate schedulers and Adam optimizers for both discriminator and auto-encoder with a learning rate of 3-e3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Continuous data</head><p>To evaluate the model with continuous data, our experiments tried to learn causal graphs from synthetic data that were created with known causal graph structures and equations. To allow comparisons, we employed the same underlying graphs and equations like those in the related work, namely DAG-GNN <ref type="bibr" target="#b11">[12]</ref>, DAG-NoCurl <ref type="bibr" target="#b33">[34]</ref> and DAG-NOTEARS <ref type="bibr" target="#b10">[11]</ref>.</p><p>More specifically, the data synthesis was performed in two steps: 1) generating the ground truth causal graph and 2) generating samples from the graph based on the linear SEM of the ground truth graph. In Step (1), we generated an Erdos-Renyi directed acyclic graph with an expected node degree of 3. The DAG was represented in a weighted adjacency matrix A. In Step (2), a sample X was generated based on the following equations. We used the linear SEM X = A T x+z for the linear case, and two different equations, namely X = A T h(x)+z (non-linear-1) and X = 2sin(A T (x+0.5)) + A T cos(x+0,5) + z (non-linear-2) for the nonlinear case.</p><p>The experiments were conducted with 5000 samples per graph. The graph sizes used in the experiments were 10, 20, 50 and 100. We measured the SHD (averaged over five different iterations of each model) between the output of a model and the ground truth, and the outcome was compared against those from the related work models (i.e. those mentioned at the beginning of Section 4). In addition to the mean SHD, confidence intervals were also measured based on the variance in the estimated means. These provide insight into the consistency of the model. Tables <ref type="table" target="#tab_0">1</ref><ref type="table" target="#tab_1">2</ref><ref type="table" target="#tab_2">3</ref>show the results on continuous data samples: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Benchmark discrete data</head><p>To evaluate the model with discrete data, we used the benchmark datasets available at the Bayesian Network Repository <ref type="url" target="https://www.bnlearn.com/bnrepository/">https://www.bnlearn.com/bnrepository/</ref> . The repository provides a variety of datasets together with their ground truth graphs (Discrete Bayesian Networks, Gaussian Bayesian Networks and Conditional Linear Gaussian Bayesian Networks) in different sizes (Small Networks, Medium Networks, Large Networks, Very Large Networks and Massive Networks). To test the scalability of our model, we used datasets of multiple sizes. The datasets utilized in the experiment were Sachs, Alarm, Child, Hailfinder and Pathfinder. The SHD metric was used to measure the performance. Table <ref type="table" target="#tab_3">4</ref> contains the results from the experiment. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Data integrity</head><p>DAG-WGAN was also evaluated by comparing its data generation capabilities against other models. More specifically, we compare the data generation capabilities of the models on a 'dimension-wise probability' basis by measuring how well these models learn the real data distributions per dimension. We used the MIMIC-III dataset <ref type="bibr" target="#b43">[44]</ref> in the experiments as the same dataset was also used in other comparable works. The data is presented in the form of a patient record, where each record has a fixed size of 1071 entries. We use the same data and preprocessing steps, in order to ensure a fair comparison between all models.</p><p>Figure <ref type="figure" target="#fig_1">2</ref> depicts the results of the experiment. We have only compared with CorGAN <ref type="bibr" target="#b13">[14]</ref> as it out-performs the other similar models such as medGAN <ref type="bibr" target="#b44">[45]</ref> and DBM <ref type="bibr" target="#b45">[46]</ref> -see <ref type="bibr" target="#b13">[14]</ref> where results of the other models are available. We present the results in a scatter plot, where each point represents one of the 1071 entries and the x and y axes represent the success rate for real and synthetic data respectively. In addition, we use a diagonal line to mark the ideal scenario. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">DISCUSSION</head><p>We discuss the results of both continuous and discrete datasets. The limitations of the model are also addressed together with the plans for future work.</p><p>The results on the continuous datasets are competitive across all three cases (linear, non-linear-1 and non-linear-2). According to Tables 1, 2 and 3, our model dominates DAG-NOTEARS and DAG-NoCurl, producing better results throughout all experiments and outperforming DAG-GNN in most of the cases.</p><p>For small-scale datasets (e.g. d=10 and d=20), our model performs better than DAG-GNN in most cases and is superior to DAG-NOTEARS and DAG-NoCurl in all the experiments.</p><p>For large-scale datasets (e.g. d=50 and d=100), our model outperforms all the other models used in the study by a significantly large margin, which implies that the model can scale better, which is a significant advantage.</p><p>Notably, our experiments have mainly focused on the comparisons with DAG-GNN, as we aim to identify contributions from the Wasserstein loss to causal structure learning. In DAG-GNN, a very similar auto-encoder architecture was employed and DAG-WGAN has added WGAN as an additional component. Hence the comparison is meaningful in order to identify contributions from the Wasserstein metric.</p><p>For the discrete case, the results from the comparison between the two models are competitive. Out of the five experiments conducted during the study, four results were clearly in favour of our model (i.e.DAG-WGAN) and in one case DAG-GNN was slightly better. These are encouraging results compared to the state of the art model of DAG-GNN.</p><p>Also, according to the results illustrated in Figure <ref type="figure" target="#fig_1">2</ref>, dimension-wise, the data generated using DAG-WGAN is more accurate and of higher quality than the ones generated using CorGAN, medGAN or DBM.</p><p>These results show that DAG-WGAN can handle both continuous and discrete data effectively.</p><p>They have also demonstrated the quality of the generated data. As the improvement was achieved by introducing the Wasserstein loss in addition to the auto-encoder architecture, the comparisons between them show that the hypothesis on the contribution from the Wasserstein metric to causal structure learning stands.</p><p>However, the discrepancy which occurred in the synthetic continuous data results provides us with an insight into the limitations of the model. Some of our early analysis shows that further improvement can be achieved by generalizing the current auto-encoder architecture. Furthermore, as it stands currently, our model does not handle vector or mixed-typed data. These aspects will be further experimented with and reported in our future work.</p><p>On the topic of potential improvements, the capability of recovering latent representation places the generative models in a good position to address the hidden confounder challenges in causality learning -some earlier work from <ref type="bibr" target="#b38">[39]</ref><ref type="bibr" target="#b39">[40]</ref> have moved towards this direction. We will further investigate whether DAG-WGAN can contribute. Last but not least, the latest work in DAG-NoCurl <ref type="bibr" target="#b28">[29]</ref> shows that the speed performance can be improved by avoiding the DAG constraints. We will investigate how this new development can be adapted to DAG-WGAN to improve its overall performance.</p><p>The capability of generating latent representations places the generative models in a good position to address the hidden confounder challenges in causality learning -some earlier work from <ref type="bibr" target="#b46">[47]</ref><ref type="bibr" target="#b47">[48]</ref> have moved towards this direction. In the future work, we will further investigate whether DAG-WGAN can contribute to this by incorporating the "Information Maximization" concept <ref type="bibr" target="#b34">[35]</ref><ref type="bibr" target="#b35">[36]</ref><ref type="bibr" target="#b36">[37]</ref> using the Maximum Mean Discrepancy loss term (MMD) <ref type="bibr" target="#b16">[17]</ref>. The inclusion of the MMD loss term to the auto-encoder loss would allow for more meaningful disentangled latent representations, resulting in a potential increase in accuracy. Last but not least, the latest work in DAG-NoCurl <ref type="bibr" target="#b33">[34]</ref> shows that the speed performance can be improved by avoiding explicit DAG constraints. We will investigate how this new development can be adapted to DAG-WGAN to improve its overall performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">CONCLUSION</head><p>In this work, we investigate the contributions of the Wasserstein distance metric to causal structure learning from tabular data. We test the hypothesis with evidence that the Wasserstein metric can simultaneously improve structure learning while generating more realistic data samples. This leads to the development of a novel approach to DAG structure learning, which we coined DAG-WGAN. The effectiveness of our model has been demonstrated in a series of tests, which show the inclusion of the Wasserstein metric can indeed improve the outcomes of causal structure learning. According to our results, the Wasserstein metric allows our model to generate less noisy and more realistic output while being easier to train. The increase in quality of the synthesized data in turn leads to an improvement in structure learning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. DAG-WGAN Model Architecture</figDesc><graphic coords="5,120.00,90.00,355.20,130.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Data generation test results</figDesc><graphic coords="9,121.70,293.50,351.60,124.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparisons of DAG Structure Learning Outcomes between DAG-NOTEARS, DAG-NoCurl, DAG-GNN and DAG-WGAN with Linear Data Samples</figDesc><table><row><cell></cell><cell></cell><cell cols="2">SHD (5000 linear samples)</cell><cell></cell></row><row><cell>Model</cell><cell>d = 10</cell><cell>d = 20</cell><cell>d = 50</cell><cell>d = 100</cell></row><row><cell>DAG-</cell><cell>8.4 ± 7.94</cell><cell>2.6 ± 1.84</cell><cell>25.2 ± 19.82</cell><cell>106.56 ±</cell></row><row><cell>NOTEARS</cell><cell></cell><cell></cell><cell></cell><cell>56.51</cell></row><row><cell>DAG-NoCurl</cell><cell>7.9 ± 7.26</cell><cell>2.5 ± 1.93</cell><cell cols="2">24.6 ± 19.43 99.18 ± 55.27</cell></row><row><cell>DAG-GNN</cell><cell>6 ± 7.77</cell><cell>3.2 ± 1.6</cell><cell>21.4 ± 14.15</cell><cell>88.8 ± 47.63</cell></row><row><cell>DAG-WGAN</cell><cell>2.2 ± 4.4</cell><cell>2 ± 1.1</cell><cell>4.8 ± 4.26</cell><cell>28.20 ± 12.02</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Comparison of DAG Structure Learning Outcomes between DAG-NOTEARS, DAG-NoCurl, DAG-GNN and DAG-WGAN with Non-Linear Data Samples 1</figDesc><table><row><cell></cell><cell></cell><cell cols="2">SHD (5000 non-linear-1 samples)</cell><cell></cell></row><row><cell>Model</cell><cell>d = 10</cell><cell>d = 20</cell><cell>d = 50</cell><cell>d = 100</cell></row><row><cell>DAG-</cell><cell>11.2 ± 4.79</cell><cell>19.3 ± 3.14</cell><cell>53.7 ± 11.39</cell><cell>105.47 ±</cell></row><row><cell>NOTEARS</cell><cell></cell><cell></cell><cell></cell><cell>13.51</cell></row><row><cell>DAG-NoCurl</cell><cell>10.4 ± 4.42</cell><cell>17.4 ± 3.27</cell><cell cols="2">51.6 ± 11.43 105.7 ± 13.65</cell></row><row><cell>DAG-GNN</cell><cell>9.40 ± 0.8</cell><cell>15 ± 3.58</cell><cell>49.8 ± 7.03</cell><cell>104.8 ± 12.84</cell></row><row><cell>DAG-WGAN</cell><cell>9.8 ± 2.4</cell><cell>16 ± 5.4</cell><cell cols="2">40.40 ± 10.97 80.40 ± 9.09</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Comparison of DAG Structure Learning Outcomes between DAG-NOTEARS, DAG-NoCurl, DAG-GNN and DAG-WGAN with Non-Linear Data Samples 2</figDesc><table><row><cell></cell><cell></cell><cell cols="2">SHD (5000 non-linear-2 samples)</cell><cell></cell></row><row><cell>Model</cell><cell>d = 10</cell><cell>d = 20</cell><cell>d = 50</cell><cell>d = 100</cell></row><row><cell>DAG-</cell><cell>9.8 ± 2.61</cell><cell>22.9 ± 2.14</cell><cell>38.3 ± 13.19</cell><cell>125.21 ±</cell></row><row><cell>NOTEARS</cell><cell></cell><cell></cell><cell></cell><cell>61.19</cell></row><row><cell>DAG-NoCurl</cell><cell>7.4 ± 2.78</cell><cell>17.6 ± 2.25</cell><cell>33.6 ± 12.53</cell><cell>116.8 ± 62.3</cell></row><row><cell>DAG-GNN</cell><cell>2.6 ± 2.06</cell><cell>3.80 ± 1.94</cell><cell>13.8 ± 6.88</cell><cell>112.2 ± 59.05</cell></row><row><cell>DAG-WGAN</cell><cell>1 ± 1.1</cell><cell>3.4 ± 2.06</cell><cell cols="2">12.20 ± 7.81 20.20 ± 11.67</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Comparison of DAG Structure Learning Outcomes between DAG-WGAN and DAG-GNN with Discrete Data Samples</figDesc><table><row><cell></cell><cell></cell><cell>SHD</cell><cell></cell></row><row><cell>Dataset</cell><cell>Nodes</cell><cell>DAG-WGAN</cell><cell>DAG-GNN</cell></row><row><cell>Sachs</cell><cell>11</cell><cell>17</cell><cell>25</cell></row><row><cell>Child</cell><cell>20</cell><cell>20</cell><cell>30</cell></row><row><cell>Alarm</cell><cell>37</cell><cell>36</cell><cell>55</cell></row><row><cell>Hailfinder</cell><cell>56</cell><cell>73</cell><cell>71</cell></row><row><cell>Pathfinder</cell><cell>109</cell><cell>196</cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AUTHORS</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Computation, Causation, and Discovery</title>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Scheines</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1985">1985</date>
			<publisher>AAAI Press</publisher>
			<pubPlace>Cambridge, Massachusetts</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Finding optimal models for small gene networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Imoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Miyano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pacific symposium on Biocomputing</title>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Causal protein-signaling networks derived from multiparameter single-cell data</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sachs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pe Ér</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douglas</forename><forename type="middle">A</forename><surname>Lauffenburger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Garry</forename><forename type="middle">P</forename><surname>Nolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">308</biblScope>
			<biblScope unit="issue">5721</biblScope>
			<biblScope unit="page" from="523" to="529" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Large sample learning of Bayesian networks is np-hard</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Chickering</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Heckerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Meek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1287" to="1330" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Addendum on the scoring of Gaussian directed acyclic graphical models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kuipers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Moffa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Heckerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1689" to="1691" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning Bayesian networks: The combination of knowledge and statistical data</title>
		<author>
			<persName><forename type="first">D</forename><surname>Heckerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Chickering</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="197" to="243" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Probabilistic network construction using the minimum description length principle</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bouckaert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on symbolic and quantitative approaches to reasoning and uncertainty</title>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Causality: models, reasoning, and inference</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econometric Theory</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">46</biblScope>
			<biblScope unit="page" from="675" to="685" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">On the completeness of orientation rules for causal discovery in the presence of latent confounders and selection bias</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">172</biblScope>
			<biblScope unit="page" from="1873" to="1896" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The max-min hill-climbing Bayesian network structure learning algorithm</title>
		<author>
			<persName><forename type="first">I</forename><surname>Tsamardinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Aliferis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="31" to="78" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Aragam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<title level="m">DAGs with NOTEARS: Continuous Optimization for Structure Learning. Paper presented at 32nd Conference on Neural Information Processing Systems, Montr éal</title>
		<meeting><address><addrLine>Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">DAG-GNN: DAG Structure Learning with Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<idno>PMLR 97</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<meeting>the 36th International Conference on Machine Learning<address><addrLine>Long Beach, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note>Copyright 2019 by the author(s</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Gradient-Based Neural DAG Learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lachapelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Brouillard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Deleu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lacoste-Julien</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Paper presented at ICLR 2020</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">CORGAN: Correlation-Capturing Convolutional Neural Networks for Generating Synthetic Healthcare Records</title>
		<author>
			<persName><forename type="first">A</forename><surname>Torfi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Fox</surname></persName>
		</author>
		<idno>FLAIRS-33</idno>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Third International FLAIRS Conference</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Tabfairgan: Fair tabular data generation with generative adversarial networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rajabi</surname></persName>
		</author>
		<author>
			<persName><surname>Garibay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Ozlem</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2109.00666" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint at</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">DAG-GAN: Causal Structure Learning with Generative Adversarial Nets</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-T</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Minimax estimation of maximum mean discrepancy with radial kernels</title>
		<author>
			<persName><forename type="first">Ilya</forename><forename type="middle">O</forename><surname>Tolstikhin</surname></persName>
		</author>
		<author>
			<persName><surname>Sriperumbudur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bharath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Sch Ölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Paper presented at 30 th Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Mathematical methods of organizing and planning production</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V</forename><surname>Kantorovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Management Science</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="366" to="422" />
			<date type="published" when="1960">1960</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Sinkhorn Distances: Lightspeed Computation of Optimal Transport</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cuturi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">W2GAN: RECOVERING AN OPTIMAL TRANSPORT MAP WITH A GAN</title>
		<author>
			<persName><forename type="first">L</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>She</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Almahairi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rajeswar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>Paper presented at ICLR 2018</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning Generative Models with Sinkhorn Divergences</title>
		<author>
			<persName><forename type="first">A</forename><surname>Genevay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Peyr É</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cuturi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">21st International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Wasserstein GAN</title>
		<author>
			<persName><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Stabilizing Generative Adversarial Networks: A Survey</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wiatrak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">V</forename><surname>Albrecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nystrom</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1910.00927" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint at</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988">1988</date>
			<publisher>Morgan Kaufmann Publishers</publisher>
			<pubPlace>Los Angeles, California</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">An algorithm for fast recovery of sparse causal graphs</title>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Glymour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Social Science Computer Review</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="62" to="72" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Causal Inference in the Presence of Latent Variables and Selection Bias</title>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Meek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Paper presented at UAI&apos;95: Proceedings of the Eleventh conference on Uncertainty in artificial intelligence</title>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Efficient approximations for the marginal likelihood of Bayesian networks with hidden variables</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Chickering</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Heckerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="181" to="212" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Advances in Learning Bayesian Networks of Bounded Treewidth</title>
		<author>
			<persName><forename type="first">S</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Maua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>De Campos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Approximating discrete probability distributions with dependence trees</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="462" to="467" />
			<date type="published" when="1968">1968</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Structure learning in Bayesian networks of a moderate size by efficient sampling</title>
		<author>
			<persName><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3483" to="3536" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Bayesian graphical models for discrete data</title>
		<author>
			<persName><forename type="first">D</forename><surname>Madigan</surname></persName>
		</author>
		<author>
			<persName><surname>York</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Allard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Statistical Review / Revue Internationale de Statistique</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="215" to="232" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Being Bayesian about network structure. a Bayesian approach to structure discovery in Bayesian networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="95" to="125" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Constraint Relaxation for Learning the Structure of Bayesian Networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fast</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jensen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts Amherst</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">DAGs with No Curl: An Efficient DAG Structure Learning Approach</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">Y</forename><surname>Yue Yu</surname></persName>
		</author>
		<author>
			<persName><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning</title>
		<meeting>the 38th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Infovae: Information maximizing variational autoencoders</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1706.02262" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint at</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Abbeel: Infogan: Interpretable representation learning by information maximizing generative adversarial nets</title>
		<author>
			<persName><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">30 th Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Infovaegan : learning joint interpretable representations by information maximization and maximum likelihood</title>
		<author>
			<persName><forename type="first">F</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bors</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">28 th International Conference on Image Processing</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Auto-encoding beyond Pixels Using a Learned Similarity Metric</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B L</forename><surname>Larsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Winther</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on International Conference on Machine Learning</title>
		<meeting>the 33rd International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">PAC-GAN: Packet Generation of Network Traffic using Generative Adversarial Networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 10th Annual Information Technology, Electronics and Mobile Communication Conference</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Johnson</surname></persName>
		</author>
		<title level="m">Matrix Analysis</title>
		<meeting><address><addrLine>Cambridge, New York</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Nonlinear Programming</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bertsekas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Athena Scientific</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
	<note>nd edition</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">A comparison of structural distance measures for causal Bayesian network models. Recent Advances in Intelligent Information Systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>De Jongh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Druzdzel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="443" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">PyTorch: An Imperative Style, High-Performance Deep Learning Library</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">rd Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E W</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Pollard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-W</forename><forename type="middle">H</forename><surname>Lehman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Ghassemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Moody</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Szolovits</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Celi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Mark</surname></persName>
		</author>
		<idno type="DOI">10.1038/sdata.2016.35</idno>
		<ptr target="https://doi.org/10.1038/sdata.2016.35" />
		<title level="m">MIMIC-III, a freely accessible critical care database</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Generating Multi-Label Discrete Patient Records using Generative Adversarial Networks</title>
		<author>
			<persName><forename type="first">E</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Biswal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Malin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Duke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">F</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning for Healthcare</title>
		<meeting>Machine Learning for Healthcare</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<title level="m">Replicated Softmax: An Undirected Topic Model. Paper presented at Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">22</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Causal Effect Inference with Deep Latent-Variable Models</title>
		<author>
			<persName><forename type="first">C</forename><surname>Louizos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Shalit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Mooij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1705.08821" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint at</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">The blessings of multiple causes</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="issue">528</biblScope>
			<biblScope unit="page" from="1574" to="1596" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
