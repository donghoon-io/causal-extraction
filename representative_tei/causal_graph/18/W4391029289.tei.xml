<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Personalized Model Integrating Personality for Joint Dialogue Emotion Classi cation and Act Recognition</title>
				<funder ref="#_RPSZnU4">
					<orgName type="full">Natural Science Foundation of Inner Mongolia</orgName>
				</funder>
				<funder ref="#_tqDBnvK #_95N8fW4 #_XdcnGZW">
					<orgName type="full">Basic Scientific Research Fund for Universities directly under Inner Mongolia Autonomous Region</orgName>
				</funder>
				<funder ref="#_6eJ6J4J">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhiqiang</forename><surname>Ma</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">College of Data Science and Application</orgName>
								<orgName type="institution">Inner Mongolia University of Technology</orgName>
								<address>
									<postCode>010080</postCode>
									<settlement>Hohhot</settlement>
									<region>Inner Mongolia</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Data Based Software Service</orgName>
								<orgName type="institution">Inner Mongolia Autonomous Region Engineering &amp; Technology Research Centre of Big</orgName>
								<address>
									<postCode>010080</postCode>
									<settlement>Hohhot</settlement>
									<region>Inner Mongolia</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yutong</forename><surname>Zhou</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Biqi</forename><surname>Xu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Kai</forename><surname>Lv</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jia</forename><surname>Liu</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Inner Mongolia University of Technology</orgName>
								<orgName type="institution" key="instit2">Inner Mongolia University of Technology</orgName>
								<orgName type="institution" key="instit3">Inner Mongolia University of Technology</orgName>
								<orgName type="institution" key="instit4">Inner Mongolia University of Technology</orgName>
								<orgName type="institution" key="instit5">Inner Mongolia University of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Personalized Model Integrating Personality for Joint Dialogue Emotion Classi cation and Act Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.21203/rs.3.rs-3871693/v1</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-21T20:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Personality</term>
					<term>Graph attention mechanisms</term>
					<term>Dual-tasks Recognition</term>
					<term>Multi-head attention Personality</term>
					<term>Graph attention mechanisms</term>
					<term>Dual-tasks Recognition</term>
					<term>Multi-head attention</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The joint dialogue emotion classification and act recognition tasks essentially involves the simultaneous classification of emotional and act labels for each utterance in a dialogue. Existing dual-tasks models can recognize emotion and act based on utterance context but overlook the interaction between discourse under the influence of personality and the correlation between dual-tasks. Utterances within the same personality and across different personalities exhibit explicit temporal dependencies in the past and future. However, existing models fail to explicitly address the temporal relationships in the interaction between personality and dual-tasks, resulting in a reasoning gap in the temporal correlation of interactions under long-term personality. In this paper, A Personalized Model Integrating Personality for Joint Dialogue Emotion Classification and Act RecognitionPEDR-GATis proposed to solve these problems.Firstly, we utilize personality information and label distributions to provide initial estimates in semantics, strengthening the association between local semantics guided by personality and task labels. Secondly, the model employs graph attention mechanisms to extract global semantic temporal information and causal temporal relationships between the two tasks. Finally, PEDR-GAT, by acquiring initial local label information guided by personality and integrating personality-guided global semantic label information, introduces a common attention mechanism to maintain long-term dependencies and supplement short-term information.The experiments were conducted on three benchmark datasets: Mastodon social network (Mastodon), A Manually Labelled Multi-turn Dialogue Dataset (Dai-lyDialog), and A Large-Scale Chinese Personalized and Emotional Dialogue Dataset for Conversational AI (CPED). Compared to the baseline models, there were improvements in F1 scores for emotion classification and act recognition by 1.9% and 0.7% (Mastodon),3.3% and 1.3% (DailyDialog), and 5.3% and 2.3%(CPED), respectively. The results indicate that personality-guided semantic label acquisition enhances the association between semantics and labels, and the graph attention mechanism with personality fusion reasoning can stimulate the cognitive system's understanding of potential causal information in abstraction.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The joint recognition of dialogue emotion and act tasks involves the collaborative modeling of two tasks within dialogue: Dialogue Act Recognition (DAR) and Emotion Recognition in Conversation (ERC) <ref type="bibr" target="#b0">[1]</ref>  <ref type="bibr" target="#b1">[2]</ref>.These tasks share a potential emotional correlation, mutually facilitating and executing each other. The underlying emotional link between these tasks is two-fold. On one hand, DAR provides cues for emotion recognition (ERC), and reciprocally, emotion transitions can benefit the prediction of dialogue act <ref type="bibr" target="#b2">[3]</ref> [4] <ref type="bibr" target="#b4">[5]</ref>.In a conversation from the CPED <ref type="bibr" target="#b5">[6]</ref> dataset, as depicted in Table <ref type="table" target="#tab_0">1</ref>, when predicting Person B's act, the agreement act label of Person B suggests that both parties tend to share the same emotional label when expressing similar opinions.Furthermore, understanding emotional information aids in current act predictions. The joint recognition of dialogue act and emotion provides a more comprehensive grasp of the speaker's intent <ref type="bibr" target="#b6">[7]</ref>, contributing to machine recognition and understanding of the speaker's psychological state <ref type="bibr" target="#b7">[8]</ref>. The joint recognition of tasks can be broadly categorized into two types: one based on sequence methods and the other on graph neural networks . Sequencebased methods typically encounter challenges related to aggregating information over long distances. On the other hand, graph neural networks consider a conversation as a directed graph, with nodes representing information and edges constructing dependencies . This graph structure allows the simultaneous aggregation of multiple types of information. However, prior approaches using graph neural networks have often centered on separating discourse structures based on speakers, overlooking the intrinsic influence of speaker personality traits on emotional features. Additionally, existing work has not simultaneously considered personality-centered discourse temporal dependencies, intra-task propagation dependencies, semantic interactions, and inter-task dependencies in parameter sharing.</p><p>You also like collecting comics You mean this? I drew this. But the publisher needs me to add some commercialisation or they'll stop publishing.</p><p>I thought your original story was great. I liked it.</p><p>Then these are for you.  As illustrated in <ref type="bibr">Fig 3,</ref><ref type="bibr"></ref> we abstract different information concepts in the context as nodes based on social psychology. Speaker personality determines emotions and acts in specific contexts, with emotions and act labels providing useful references under the same personality. Different speakers in interactive processes exhibit explicit causal relationships in dialogue act, and there exists a latent correlation between emotions. Leveraging dual-task dependencies enables the model to understand internal emotional and act variations within speakers with the same personality, facilitating a better focus on the potential correlations embedded in discourse.</p><formula xml:id="formula_0">t A I t A S t A U 1 t A U - 1 t A E - 1 t A D - 1 t A I - 1 t A S - 1 t B D - 1 t B E - 1 t B U - 2 t B I - 1 t B I -</formula><p>The paper proposes a personalized model integrating personality for joint dialogue emotion classification and act recognition. The aim is to enhance the accuracy of emotion recognition in conversation and dialogue act recognition by concurrently considering speaker personality-aware discourse structures and contextual temporal interactions between the two tasks. This model comprises three hierarchical levels: a feature extraction initialization layer, a personality-fused interactive reasoning layer, and a dual-task classification layer. The feature extraction initialization layer focuses on leveraging personality-guided discourse for initial local feature extraction in dual tasks. The personality-fused interactive reasoning layer is designed to capture semantic temporal correlations between dialogues under different personalities and temporal associations both within and between the two tasks under different personalities. The dual-task classification layer integrates global discourse information and dual-task information under different personalities for classification.</p><p>The main contributions of this paper are as follows:</p><p>• We propose a personality-based model for joint emotion classification and act recog-nitionWe introduce the use of the Big Five personality traits for the joint tasks of emotion classification and behavior recognition for the first time. • We propose a graph-based model that integrates personality-aware utterance temporal graphs and dual-task interaction temporal graphs, enhancing temporal dependencies in speech and interaction dependencies in tasks among different speakers with the same personality. • Experimental results indicate that PEDR-GAT can better achieve emotion classification and behavior recognition in dialogues based on personality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In the initial stages of various text recognition tasks, the majority of research treated recognition as a standalone text classification task, utilizing traditional statistical classifiers for classification. With the advancement of recurrent neural networks, contemporary recognition methods primarily fall into two categories: those based on sequence labeling and those based on graph neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Based on sequence labeling</head><p>The sequence labeling method involves treating recognition as a sequence labeling task, addressing recognition challenges by training and memorizing encoded features within the sequence. Relying on sequence models enables a more effective capture of temporal dynamic features between sequences.</p><p>In the realm of sequence-labeling-based emotion recognition, Ayetiran et al <ref type="bibr" target="#b8">[9]</ref>. employed a combination of convolutional and bidirectional long short-term memory networks to jointly learn emotional data at both the document and aspect levels. Majumder et al <ref type="bibr" target="#b9">[10]</ref>. introduced the DialogueRNN emotion recognition model, treating users and dialogue systems as independent entities. They dynamically captured user emotion changes at the local level and then utilized static global modeling of the context for emotion recognition. Inspired by emotion cognition theory, Hu et al <ref type="bibr" target="#b10">[11]</ref>. proposed the DialogueCRN model, integrating LSTM and attention mechanisms to identify emotional nuances in dialogues.</p><p>In the realm of sequence-labeling-based act recognition tasks, Kumar et al <ref type="bibr" target="#b11">[12]</ref>. employ bidirectional time series and a random field to identify acts within dialogues. Raheja et al <ref type="bibr" target="#b12">[13]</ref>. utilize attention mechanisms and time seriesLSTM for act recognition. Duran et al <ref type="bibr" target="#b13">[14]</ref>.during the encoding phase, integrate act and sentence encoding using BERT, systematically comparing and validating various encoding mechanisms' performance differences in act classification. Malhotra et al <ref type="bibr" target="#b14">[15]</ref>. leverage gated recurrent units and a time sliding window to perceive local and global contextual cues within dialogues for act recognition.</p><p>In the domain of multitask sequence-labeling for emotion and act recognition, Cerisara et al <ref type="bibr" target="#b0">[1]</ref>. introduced a multitask framework based on a shared encoder, implicitly modeling the inter-task correlations. Kim et al <ref type="bibr" target="#b1">[2]</ref>. devised a convolutional neural network that integrates dialogue act, predictive factors, and emotion recognition into a unified model. Li et al <ref type="bibr">[LiModeling]</ref>. proposed a dual-channel perception dynamic convolutional network to dynamically generate kernels and capture crucial local context. Qin et al <ref type="bibr" target="#b2">[3]</ref>. presented a cooperative interaction layer to model the interplay between the two related tasks. Zheng et al <ref type="bibr" target="#b15">[16]</ref>. introduced a bidirectional multi-hop reasoning network that focuses on explicit correlations between act and emotion to capture richer act and emotional cues, enhancing effectiveness and accuracy in multitask recognition inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Based on graph neural networks</head><p>Graph neural networks have garnered increasing attention in recognition tasks, leveraging their ability to refine dependency features of speakers or historical contexts more effectively.</p><p>In the realm of emotion recognition based on graph neural networks, Ghosal et al <ref type="bibr" target="#b16">[17]</ref>. introduced the DialogueGCN model. This model addresses the challenge of differentiating emotions in context propagation when the speaker of a discourse cannot be distinguished, achieved by constructing a heterogeneous graph with speech and speakers as nodes, employing distinct connecting edges to differentiate speakers. Zhang et al <ref type="bibr" target="#b17">[18]</ref>. utilized a graph structure to represent sentences and speakers in the dialogue history as nodes, forming a dialogue graph by connecting these nodes with edges representing statements. Subsequently, they employed graph convolutional networks for information extraction in emotion recognition. Zhao et al <ref type="bibr" target="#b18">[19]</ref>. devised two distinct methods to execute context and speaker-specific modeling in separate threads. Zhang et al <ref type="bibr" target="#b19">[20]</ref>. structured the discourse and interaction between discourses into two graph structures, integrating information through a cross-attention mechanism for emotion recognition.</p><p>In the domain of joint recognition tasks involving both act and emotion using graph neural networks, Qin et al <ref type="bibr" target="#b20">[21]</ref>. applied graph attention networks to an undirected and disconnected graph composed of isolated speaker-specific fully connected subgraphs, concurrently executing both tasks. Xing et al <ref type="bibr" target="#b4">[5]</ref>. devised the DARER model, a graphbased structure that transforms contextual discourse relations and dual-task relations into a concatenated graph for learning context, speaker, and time-sensitive discourse representations. Xu et al <ref type="bibr" target="#b6">[7]</ref>. considering the confidence of embedded contextual discourse, proposed a bidirectional learning integrated model based on joint DAR and ERC. Li et al <ref type="bibr" target="#b21">[22]</ref>. introduced a multi-information-aware recurrent reasoning network and designed a speaker-time-aware heterogeneous graph for semantic-level interaction and self-interaction, facilitating the joint recognition of tasks. However, the above-mentioned methods overlook the correlation between the speaker's own personality information and the two tasks, i.e., the speaker's own personality reflects both dialogue behavior and emotion. Simultaneously, it ignores the dependencies within and between tasks and their relation to the speaker. This dependency is manifested in the continuity of the same task within the same speaker and the causal relationship between different speakers in different dialogues. There is often explicit correlation between categories in different tasks within the same speaker, and different tasks among different speakers often have potential mapping relationships.  Firstly, the initiation of local discourse features and task label feature representation is achieved through personality-guided label text acquisition. Secondly, the directed temporal association between personality information and discourse information, as well as between personality information and dual-task information, is established using graph neural networks, with a fusion mechanism in place to amalgamate the two. Lastly, the classifier produces recognition results based on the fused features.The following sections will introduce the structure of the model in turn.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Research Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Feature extraction initialization layer</head><p>This section provides a detailed exposition of the model that integrates personality information and task labels. The objective is to initialize the encoding of text sequences with labeled information, establishing a more intricate relationship between personality-guided labels and discourse. Refer to Figure <ref type="figure" target="#fig_5">3</ref> for illustration. The model primarily comprises Bert semantics and label extraction, along with multi-head attention layers.</p><p>For a given dialogue history U = {x 1 , x 2 , ..., x n }, where the i-th utterance is denoted as</p><formula xml:id="formula_1">x i = (w i,1 , w i,2 , w i,3 , • • •, w i,M</formula><p>)the speaker's Big Five personality traits information, denoted as p i is appended. This results in the formation of</p><formula xml:id="formula_2">X = ([CLS 1 ], p 1 , x 1 , [CLS 2 ], p 2 , x 2 , [CLS 3 ], p 3 , x 3 , • • •, [CLS N ], p N , x N )</formula><p>, and these elements are concatenated to serve as the input for BERT, generating a contextual representation endowed with discourse and personality features.</p><p>Extracting encoded information that integrates personality, emotions, and initial local features of act is the focus of this section. This process involves two branches: one encodes the initial local features of emotional aspects in dialogue, while the other encodes the initial local features of dialogue act.The feature vector H of spoken discourse x i undergoes a self-attention process, resulting in H x = MAtt(H, H, H) , followed by guidance using personality features to obtain H pe = MAtt(H x , H P ,H P ).Subsequently, H pe is employed to calculate the initial distribution probabilities for both emotional and act aspects from the distributions of act and emotional discourse.</p><formula xml:id="formula_3">d i = soft max(Fc(Matt(H pe , d t i ,d t i ))). (<label>1</label></formula><formula xml:id="formula_4">)</formula><formula xml:id="formula_5">e i = soft max(Fc(Matt(H pe , e t i ,e t i ))). (<label>2</label></formula><formula xml:id="formula_6">)</formula><p>where d t i and e t i represent local feature representations of act labels</p><formula xml:id="formula_7">D part = D t = {d t i } N i=1 and emotional labels E part =E t = {e t i } N i=1</formula><p>, respectively. where,</p><formula xml:id="formula_8">d t i = |C D |-1 k=0 p t-1 D,i [c d ] • v c d D .<label>(3)</label></formula><formula xml:id="formula_9">e t i = |C E |-1 k=0 p t-1 E,i [c e ] • v ce E<label>(4)</label></formula><p>Where, v c d D and v ce E represent embedded labels for act category c d and emotional category c e , while p t D,i and p t E,i denote the initial label distributions for act and emotion. The calculations are as follows. <ref type="formula">6</ref>) Where, W * and b * represent the weight matrix and bias, while C D and C E correspondingly denote the sets of act categories and emotion categories.</p><formula xml:id="formula_10">P 0 D = {P 0 D,i } N i=1 , P 0 D,i = softmax(W d H 0 pe + b d ) = [p 0 D,i [0], • • •, p 0 D,i (|C D | -1)]<label>(5)</label></formula><formula xml:id="formula_11">P 0 E = {P 0 E,i } N i=1 , P 0 E,i = softmax(W e H 0 pe + b e ) = [p 0 E,i [0], • • •, p 0 E,i (|C E | -1)] (</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Personality-fused interactive reasoning layer</head><p>Personality-fused interactive reasoning layer employs three graph attention neural networks to effectively capture the temporal structure of speech perceived by the speaker's personality, the temporal structure of dialogue acts and emotional expressions, as well as the interactive structural relationships between the two tasks.</p><p>Associate related discourse and tasks under the same personality and different personalities in the two timeline graphs. Since the relationship between speaker personality and discourse has ownership but lacks a temporal relationship, the model utilizes purple undirected edges for discourse ownership association and dual task ownership association with personalityUnder the same personality and different personalities, discourse exhibits bidirectional temporal relationships in the past and future. For relationships in the past within the same personality, orange solid lines are used, while relationships in the future temporally between discourses are represented by orange dashed linesWithin the same utterance, implicit emotions associated with conversational acts and temporal relationships of emotions are connected using blue directed edges. Since future acts are often feedback on past acts, and this feedback often implies emotions similar or opposite to past acts, this temporal feedback relationship is represented by green directed edges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Personality -Aware Utterance Temporal Graph</head><p>We initially expound upon the construction of Personality -Aware Utterance Temporal Graph(PAUTGAT),followed by providing an overview of the inferential computational processes employed by PAUTGAT in the generated graph within the model. To facilitate interactive information exchange among various components, four distinct node types and four different types of edges have been devised.</p><p>Node : We have established four types of representation nodes : Personality nodes, Utterance nodes, DAR nodes, and ERC nodes. Specifically, Personality nodes represent the speaker's Big Five personality features, Utterance nodes encapsulate discourse features, DAR nodes signify conversational act features within the dialogue, and ERC nodes denote emotional features within the discourse.</p><p>Edge : We constructed four types of edges : Personality edge, Utterance edge, ERC edge, and DAR edge. Among them, Personality edge connects nodes of discourse from the context of the same Big Five personality speaker. Utterance edge connects nodes of the context of discourse. DAR edge connects context act nodes that have mutual relationships to simulate potential emotional dependencies between conversational acts. ERC edge connects context emotion nodes and dialogue acts to simulate explicit dependencies between two tasks.</p><p>PAUTGAT is a comprehensive directed graph that associates the speaker's personality with discourse. It can simultaneously model more complete semantic dependencies within the same personality and between different personalities. Its definition is: G P aut = (S P aut , P P aut , R P aut ),Where S P aut represents the features of utterance x i and is initialized, P P aut represents the adjacency matrix for the personality dependencies between utterance nodes and personality nodes, and R P aut represents the adjacency matrix for the temporal dependencies between utterance nodes.</p><p>We defined four types of dependency relationships: same personality future, same personality past, different personality future, and different personality past. Specifically: Same personality future relationship refers to the relative position of the discourse under the same Big Five personality in the current conversation being in the future. The same applies to the same personality past relationship. Different personality future dependency relationship indicates that there is a temporal relationship of future and past positions between two discourses, but they belong to different personalities. These four relationships simulate how past discourse influences future discourse, vice versa, and also simulate the influence of discourse between different personalities.</p><p>Calculation of PAUTGAT in the model involves the following steps: For any nodes x i and x j , , place them into G P aut = (S P aut , P P aut , R P aut ) for updating. Specifically, for a given node S P aut , PAUTGAT aggregates information from its neighboring nodes as follows:</p><formula xml:id="formula_12">β [i][j] = Soft max(LeakyReLu(a T [W x i ||W x j ||P P aut i ||P P aut j ||R P aut [i][j] ])<label>(7)</label></formula><formula xml:id="formula_13">x paut i = k || k=1 σ( j∈N P aut i β k [i][j] W x j )<label>(8)</label></formula><p>Where,R P aut [i][j] = l P aut satisfies the discourse dependency relationship l P aut ∈ L P aut ,β [i][j] represents the weights on the edges from node S P aut i to its adjacent node S P aut j , W and a T represent trainable parameters, while || represents the concatenation operation. N P aut i represents the neighborhood nodes of the node x P aut i ∈ Rd h represents the nodes related to S P aut i after the PAUTGAT update, d h represents the dimension of the intermediate vector. After updating all nodes, it is represented as: X P aut = SP ET GAT (X, P P aut , R P aut ),X P aut ∈ RN×d h .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Personality Dual-task Interaction Temporal Graph</head><p>Personality Dual-task Interaction Temporal GraphPDITGATis a comprehensive directed graph associating speaker personality with dual-tasks. It can simultaneously model more complete temporal interaction dependencies within tasks associated with personality and between tasks. PDITGAT utilizes DAR nodes, ERC nodes, ERC edges, and DAR edges to simulate the interaction and propagation between tasks within and between different personalities. Similarly to PAUTGAT, we define PDIT-GAT as : G P dit = { (D P dit , P P dit,D , R P dit,D ) (E P dit , P P dit,E , R P dit,E ) },Where D P dit represents the features of act node d t i , P P dit,D represents the adjacency matrix for the personality dependencies between act nodes and personality nodes, and R P dit,D represents the adjacency matrix for the temporal dependencies between act nodes, E P dit , P P dit,E , R P dit,E similarly.</p><p>Similar to the work in PAUTGAT, within each recognition task in PDITGAT, there are four types of relationships. In the act task, we consider act information for the same personality in the past and future, as well as for different personalities in the past and future. In the emotion task, we consider emotion information for the same personality in the past and future, as well as for different personalities in the past and future. Simultaneously, we construct connections across tasks in the timeline graphs to explicitly utilize interaction information. By doing so, we model these two sources of information in a unified graph interaction framework with inter-task connections.</p><p>Calculation of PDITGAT in the model involves the following steps: The initial local features obtained for D t = {d t i } N i=1 and E t = {e t i } N i=1 are placed into G P dit = { (D P dit , P P dit,D , R P dit,D ) (E P dit , P P dit,E , R P dit,E ) } , and the nodes are updated as follows:</p><formula xml:id="formula_14">α D [i][j] = Soft max(LeakyReLu(a T [W d i ||W d j ||P P dit,D i ||P P dit,D j ||R P dit,D [i][j] ]) (9) d P dit,t i = k || k=1 σ( j∈N P dit,D i α D,k [i][j] W k d t-1 j ) (10) α E [i][j] = Soft max(LeakyReLu(a T [W e i ||W e j ||P P dit,E i ||P P dit,E j ||R P dit,E [i][j] ])<label>(11)</label></formula><formula xml:id="formula_15">e P dit,t i = k || k=1 σ( j∈N P dit,E i α E,k [i][j] W k e t-1 j )<label>(12)</label></formula><p>PDITGAT interactive update is as follows:</p><formula xml:id="formula_16">d P dit,t+1 i = k || k=1 σ( j∈N P dit,D i α k [i][j] W k d t j + j∈N P dit,E i α k [i][j] W k e t j )<label>(13)</label></formula><formula xml:id="formula_17">e P dit,t+1 i = k || k=1 σ( j∈ Ñ P dit,D i α k [i][j] W k e t j + j∈ Ñ P dit,D i α k [i][j] W k d t j )<label>(14)</label></formula><p>After updating all nodes, PDITGAT is represented as: D P dit E P dit = P DIT GAT { (D, P P dit,D , R P dit,D ) (E, P P dit,E , R P dit,E ) }</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Personality-Aware Utterance AND Dual-task Integrated Temporal Graph</head><p>To further integrate information from graphs PAUTGAT and PDITGAT, we expand both graphs by utilizing directed edges in the Personality-Aware Utterance AND Dual-task Integrated Temporal GraphPUDTGATand applying specific rules to establish associations.This allows us to focus on the interaction correlations between Utterance and dual-tasks under different personalities.</p><p>To further integrate information from the PAUTGAT and PDITGAT, the model employs mutual cross-attention as a bridge. The construction process of the PUDA-GAT graph is similar to the PDITGAT graph construction method. After updating the nodes in PUDAGAT, it is represented as: D P uda = sof t max(X P aut W 1 (D P dit ) T ), E P uda = sof t max(X P aut W 2 (E P dit ) T )</p><p>To enhance the learning of information from various graphs, multiple-head attention mechanisms are applied to the graphs processed by GAT for further feature extraction. Additionally, to integrate the entire Utterance and dual-tasks associations over time, the features after GAT and the features after the multi-head attention mechanism are globally fused using GRU, represented as follows:</p><formula xml:id="formula_18">A i = Atttention(QW Q i , KW K i , VW V i )<label>(15)</label></formula><formula xml:id="formula_19">H muliti = MAtt(Q, K, V) = [A 1 , • • •, A n ]W O (16) Dover = GRU (D P uda ⊕ X P aut ⊕ D P dit ) (17) Ẽover = GRU (E P uda ⊕ X P aut ⊕ E P dit ) (18) D overall = H multi + Dover ,E overall = H multi + Ẽover (<label>19</label></formula><formula xml:id="formula_20">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Dual-task classification layer</head><p>After multiple interactions with the Feature Extraction Initialization layer and the Personality-fused Interactive Reasoning layer distributed and stacked with interaction layers, the model utilizes co-attention to capture important relevant information. This process results in the output of the final interaction layer. Subsequently, separate decoders are employed for predicting conversational act and emotion, represented as follows:</p><formula xml:id="formula_21">P (y E |E l part , E l overall ) = N t=1 p(y E t |y E 1 , y E 2 , • • •, y E t-1 ; E l part , E l overall )<label>(20)</label></formula><formula xml:id="formula_22">P (y D |D l part , D l overall ) = N t=1 p(y D t |y D 1 , y D 2 , • • •, y D t-1 ; D l part , D l overall )<label>(21)</label></formula><p>Where The decoder consists of l identical blocks, where each block includes a multi-head self-attention layer, a co-attention layer, and a feedforward layer.In the emotion decoding process, the outputs of the first sub-layer M l E,part and M l E,overall , the second sub-layer C l E , and the third sub-layer E l of the l-th decoding block are calculated as follows:</p><p>M</p><formula xml:id="formula_23">l E,part = LN (SAT T (E l-1 part ) + E l-1 part ) (<label>22</label></formula><formula xml:id="formula_24">)</formula><formula xml:id="formula_25">M l E,overall = LN (SAT T (E l-1 overall ) + E l-1 overall ) (<label>23</label></formula><formula xml:id="formula_26">)</formula><formula xml:id="formula_27">C l E = LN (Co-Att(M l-1 E,part , M l-1 E,overall ) + M l-1 E,part + M l-1 E,overall )) (<label>24</label></formula><formula xml:id="formula_28">)</formula><formula xml:id="formula_29">Co-Att() = M l-1 E,overall + Sof t max(M l-1 E,overall ((M l-1 E,part ) N )M l-1 E,part<label>(25)</label></formula><formula xml:id="formula_30">E l = LN (F F N (C l E ) + C l E ) (<label>26</label></formula><formula xml:id="formula_31">)</formula><p>Where LN represents the normalization layer; SAT T represents multi-head selfattention mechanism, Co-Att represents co-attention layer, F F N represents feedforward neural network. The decoding for conversational act prediction follows the same process.</p><p>Finally, utilizing the output</p><formula xml:id="formula_32">D l = (d l 1 , d l 2 , • • •, d l N</formula><p>) and E l = (e l 1 , e l 2 , • • •, e l N ) from the last layer of the decoder, the generation of features at time t can be formalized as:</p><formula xml:id="formula_33">y E t = sof t max(W E e l t + I E t ) (<label>27</label></formula><formula xml:id="formula_34">)</formula><formula xml:id="formula_35">y D t = sof t max(W D d l t + I D t )<label>(28)</label></formula><p>Where I E t and I D t are mask vectors at decoding step t used to prevent the decoder from attending to repeated features, W E and W D are transformation matrices for conversation emotion and act, and l is the number of stacked relation layers in our framework</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Loss training</head><p>Adopting a multi-task learning framework, jointly minimize emotion loss and act loss for joint training:</p><formula xml:id="formula_36">L 1 = - N i=1 N E j=1 (ỹ (j,E) i log(y (j,E) i ) + (1 - ỹ(j,E) i ) log(1 -y (j,E) i</formula><p>)) ( <ref type="formula">29</ref>)</p><formula xml:id="formula_37">L 2 = - N i=1 N D j=1 (ỹ (j,D) i log(y (j,D) i ) + (1 - ỹ(j,D) i ) log(1 -y (j,D) i ))<label>(30)</label></formula><formula xml:id="formula_38">L = 1 2σ 2 E L E + 1 2σ 2 D L D + log σ E σ D<label>(31)</label></formula><p>Where ỹE i and ỹD i are the true emotion labels and true act labels, respectively; N E is the number of emotion labels, N D is the number of act labels.σ E and σ D are the variances of emotion loss and act loss on the training instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and Analysis 4.1 Datasets</head><p>To comprehensively compare the performance of PEDR-GAT with existing classification models, we utilized three benchmark datasets to cover varying text lengths and multiple classification tasks.</p><p>MastodonIt consists of 240 sets of training dialogues, 266 sets of testing dialogues, and 25 sets of validation dialogues. The dataset includes three emotion categories and fifteen dialogue act categories. <ref type="bibr" target="#b0">[1]</ref> DailyDialog: It includes 11,118 dialogues for training, 1,000 dialogues for validation, and 1,000 dialogues for testing. The dataset includes seven emotion categories and four dialogue act categories. <ref type="bibr" target="#b22">[23]</ref> CPED: It contains 8,085 sets of training dialogues, 933 sets of validation dialogues, and 2,814 sets of testing dialogues. Each utterance has been annotated with Big Five personality traits. The dataset includes 13 emotion categories and 19 dialogue act categories. <ref type="bibr" target="#b5">[6]</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experiment Settings</head><p>We implemented all models using PyTorch <ref type="bibr" target="#b23">[24]</ref>and trained them on an NVIDIA RTX 3060 Super GPU. The hyperparameters for PEDR-GAT are set as follows: embedding dim = 64, hidden dim = 256, dropout = 0.3, number of layers = 1, batch size = 128, and maximum sequence length = 64. We chose AdamW as the optimizer with a learning rate of 0.0001. Each model was trained for 50 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Competitor Methods</head><p>In dialogue emotion recognition, a sequence-based comparative model is Dia-logueRNN, which is a dialogue emotion recognition model based on the RNN structure. In dialogue emotion recognition, a graph neural network-based comparative model is DialogueGCN, which is a speaker-aware dialogue emotion recognition model based on graph neural networks. In dialogue act recognition, two sequence-based comparative models are:</p><p>• HEC <ref type="bibr" target="#b11">[12]</ref>: A dialogue act recognition model based on a bidirectional time-series structure.</p><p>• CASA <ref type="bibr" target="#b12">[13]</ref>: A dialogue act recognition model based on sequence fusion with selfattention mechanism.</p><p>In joint dialogue emotion classification and act recognition, four sequence-based comparative models are:</p><p>• JointDAS <ref type="bibr" target="#b0">[1]</ref>: A joint task recognition model based on the RNN structure for joint dialogue emotion classification and act recognition. • IIIM <ref type="bibr" target="#b1">[2]</ref>: A model based on a CNN convolutional network that integrates act, emotion, and predictor for joint dialogue emotion classification and act recognition tasks. • DCR-Net <ref type="bibr" target="#b2">[3]</ref>: A model that utilizes a deep collaborative interaction relationship network to explicitly model the influence between act and emotion classification tasks for joint recognition. • BCDCN <ref type="bibr" target="#b24">[25]</ref>: A context-aware dynamic convolutional network extended to a dualchannel version for joint dialogue emotion classification and act recognition tasks.</p><p>In joint dialogue emotion classification and act recognition, three graph neural network-based comparative models are:</p><p>• Co-GAT <ref type="bibr" target="#b20">[21]</ref>: A joint interactional graph attention network model with crossdialogue connections for both dialogue emotion classification and act recognition tasks.</p><p>• DARER <ref type="bibr" target="#b4">[5]</ref>: A model based on LSTM and utilizing graph structure for temporal representation in the discourse task, predicting hierarchical relationships with task temporal information for joint dialogue emotion classification and act recognition tasks.</p><p>• TSCL <ref type="bibr" target="#b6">[7]</ref>: A joint dialogue emotion classification and act recognition model based on bidirectional LSTM and incorporating speaker graph structure to transform embedded discourse into confidence scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Experiment I: Comparative experiments on unlabeled personality datasets</head><p>Experiment I compared the accuracy of PEDR-GAT and baseline models on the Mastodon dataset and DailyDialog dataset. We utilized precision, recall, and F1 as metrics for classification. Since there are no annotated Big Five personality traits in these datasets,We utilized the well-trained SVM model based on BERT for predicting the Big Five personality traits to perform speaker personality recognition <ref type="bibr" target="#b25">[26]</ref>. We conducted comparisons between PEDR-GAT and emotion recognition models, act recognition models, and models with joint learning on both the Mastodon and DailyDialog datasets. From the experimental results in Tables <ref type="table" target="#tab_1">2</ref> and<ref type="table">3</ref>, we can draw the following conclusions. <ref type="bibr" target="#b0">(1)</ref>The joint model performs better overall than individual recognition models on both datasets, demonstrating the potential connection and mutual promotion between these two tasks. (2) Our model achieves comparable or better results on most evaluation metrics for both datasets. Specifically, on the Mastodon dataset, the F1 metric for the ERC task is 1.9% higher than the latest joint model DARER, while the F1 metric for the DAR task slightly decreases by 0.7%. On the DailyDialog dataset, the F1 metric for the ERC task increases by 3.3%, and the F1  From the results of the ablation experiments, we can draw the following conclusions:</p><p>(1) Removing the speaker's personality features leads to a weakened correlation between the speaker and the discourse structure and emotional transitions. The results indicate that capturing the utterance features and dual-task dependency features of speakers with the same Big Five personality traits enables the model to understand the emotional inertia and emotional transitions of different speakers with the same personality in different contexts. Removing the initial label distribution matrices for emotion and act leads to a decrease in model performance. This is because these matrices provide useful label information for dual-task reasoning, and the initial estimates of these two tasks under personality guidance can learn valuable intrinsic information about the speaker.</p><p>(2) Removing the Personality-Aware Utterance Temporal Graph prevents the model from capturing complementary information about the speaker's own future utterance features. It also hinders the model's ability to understand the interactive influence of utterances from different personalities, leading to a disconnect between the target statement and the speaker's personality features. This graph serves as the semantic understanding core anchored to the speaker's personality, capturing semantic transitions under different personalities and providing essential context knowledge for the two recognition tasks. Without it, basic context is lost. The significant decrease in the experimental results when removing the Personality Dual-task Interaction Temporal Graph indicates that it plays a crucial role in prompting mutual inference between the transformation of different speaker personalities within the two tasks and external interactions. Without PDITGAT, the relationship between the two tasks would be severely weakened. Removing any module in the Personality-fused interactive reasoning layer leads to a decrease in the model's reasoning performance, demonstrating the necessity and effectiveness of introducing personality, comprehensive temporal semantic relationships, and dual-task temporal interaction relationships into dual-task reasoning.</p><p>(3) In the decoding stage, Co-attention integrates the initial dual-task recognition features fused with the big five personality information and the globally integrated dual-task recognition features fused with semantics through dual-task interaction. The model better considers dual-task-related information in local and global contexts under the involvement of personality through a shared attention mechanism.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Experiment IV: Case Study</head><p>For a more intuitive demonstration of the PEDR-GAT effectiveness, two dialogue scenarios were sampled for testing in different personality settings under the CPED dataset, and two sets of dialogue content were recorded for case analysis. Speaker A and Speaker C, with the same personality label, were selected for different speakers. The model learned the emotional changes and the interaction between emotion and act under the big five personality of Speaker A using the dialogue corpus and emotion-act sequences of Speaker A and Speaker B. The model was then tested and analyzed on the dialogue of Speaker C, who has the same personality as Speaker A. The speaker's personality table is shown in Table <ref type="table">3</ref>, and the training and testing cases are shown in Tables <ref type="table" target="#tab_4">6</ref> and<ref type="table" target="#tab_5">7</ref>. Observing the examples above, when Speaker A and Speaker C have the same bigfive personality annotations, they show a high consistency in matching act and emotion when facing different personalities. After the model learns the emotion probability classification, emotion duration, act probability classification, and the probability of matching act and emotion for Speaker A, it can associate contextual information in training with the emotion and act classification for Speaker C, who has the same bigfive personality. From the examples, we can see that emotions are continuous over time in different dialogue contexts, and acts are causally related over time, which is consistent with the temporal reasoning logic in our model, including utterance temporal reasoning and dual-task temporal reasoning. However, there may be cases where the recognized emotion is inconsistent with the emotion in the statement, possibly due to limited big-five personality annotations in the training data and data imbalance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This paper primarily addresses the problem of joint recognition of dialogue emotion and act under the influence of long-term personality. We have developed an integrated recognition model that combines personality factors, integrating label information based on utterance temporal relationships and dual-task causal reasoning relationships. First, we design a Feature Extraction Initialization Layer to obtain semantically relevant dual-task label information guided by personality, enhancing semantic and task representations. Secondly, we introduce the time relationships of personalityintegrated utterance and dual-task interaction into the reasoning model, allowing speaker personality information, temporal information, label information, and semantics to work together. Finally, shallow semantic information channels and co-attention mechanisms are utilized to acquire both local and global joint information. In experiments, we extensively compared the model and conducted ablation studies for each component. The results demonstrate that with the introduction of personality-related semantic temporal information and dual-tasks reasoning information, the model maintains good performance. The model addresses the limitations of personalized dual-task recognition.</p><p>Author Contributions. Zhiqiang Ma: methodology, supervision, funding acquisition, writing-review and editing. Yutong Zhou: methodology, conceptualization, software, investigation, writing-original draft. Biqi Xu: data curation, formal analysis, software. Jia Liu: data curation, formal analysis, software. Kai Lv: data curation, formal analysis, software.</p><p>Data Availability. The processed data and code relate to ongoing research and therefore cannot be shared at this time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Declarations</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>1 u 3 u 5 u</head><label>135</label><figDesc>EmotionIt's love triple-legged cat, I love it. 4 u I've only read three books, all online. It's my favourite too.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 1</head><label>1</label><figDesc>Fig. 1 Emotion classification and act recognition are performed under the interaction of dialogue history and different variable factors, where P represents speaker's Big Five personality traits, U represents utterance, D represents dialogue act, E represents dialogue emotion, I represents speaker's intent, and S represents speaker's state. This provides a more precise understanding of the relationships between different tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>This section describes the classification model in detail. The frame of the model is shown in Fig 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 2</head><label>2</label><figDesc>Fig. 2 This is PEDR-GAT model structure3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 3</head><label>3</label><figDesc>Fig.3Personality-fused interactive reasoning layer structure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>E l part = (e l part1 , e l part2 , • • •, e l partN ) and D l part = (d l part1 , d l part2 , • • •, d l partN ) are sequences generated by the personality-guided Feature Extraction Initialization layer. E l overall = (e l overall1 , e l overall2 , • • •, e l overallN ) and D l overall = (d l overall1 , d l overall2 , • • •, d l overallN ) are sequences generated by the Personality-fused Interactive Reasoning layer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Emotion distribution in CPED</figDesc><table><row><cell>Speaker</cell><cell>Utterances</cell><cell>DAR Label</cell><cell>ERC Label</cell></row><row><cell>Person A</cell><cell>U1: I've only read three of them all online.</cell><cell>statement</cell><cell>happy</cell></row><row><cell>Person A</cell><cell>U2: This reading is also a super favourite of mine.</cell><cell>agreement</cell><cell>happy</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>Comparative experiments on the Mastodon dataset</figDesc><table><row><cell>Model</cell><cell></cell><cell>ERC</cell><cell></cell><cell></cell><cell>DAR</cell><cell></cell></row><row><cell></cell><cell>Precision</cell><cell>Recall</cell><cell>F1</cell><cell>Precision</cell><cell>Recall</cell><cell>F1</cell></row><row><cell>DialogueRNN</cell><cell>0.405</cell><cell>0.428</cell><cell>0.415</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>DialogueGCN</cell><cell>0.414</cell><cell>0.434</cell><cell>0.424</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>HEC</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.565</cell><cell>0.557</cell><cell>0.561</cell></row><row><cell>CASA</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>0.557</cell><cell>0.571</cell><cell>0.564</cell></row><row><cell>JointDAS</cell><cell>0.361</cell><cell>0.416</cell><cell>0.376</cell><cell>0.556</cell><cell>0.516</cell><cell>0.532</cell></row><row><cell>IIIM</cell><cell>0.387</cell><cell>0.401</cell><cell>0.394</cell><cell>0.563</cell><cell>0.522</cell><cell>0.543</cell></row><row><cell>DCR-Net</cell><cell>0.432</cell><cell>0.473</cell><cell>0.451</cell><cell>0.603</cell><cell>0.569</cell><cell>0.586</cell></row><row><cell>BCDCN</cell><cell>0.382</cell><cell>0.620</cell><cell>0.459</cell><cell>0.573</cell><cell>0.617</cell><cell>0.594</cell></row><row><cell>Co-GAT</cell><cell>0.440</cell><cell>0.532</cell><cell>0.481</cell><cell>0.604</cell><cell>0.606</cell><cell>0.605</cell></row><row><cell>TSCL</cell><cell>0.461</cell><cell>0.587</cell><cell>0.516</cell><cell>0.612</cell><cell>0.616</cell><cell>0.608</cell></row><row><cell>DARER</cell><cell>0.560</cell><cell>0.633</cell><cell>0.595</cell><cell>0.650</cell><cell>0.618</cell><cell>0.634</cell></row><row><cell>PEDR-GAT</cell><cell>0.583</cell><cell>0.649</cell><cell>0.614</cell><cell>0.643</cell><cell>0.612</cell><cell>0.627</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4</head><label>4</label><figDesc>Comparative experiments on the CPED datasetTo further validate the effectiveness of each module in PEDR-GAT, Experiment III conducted ablation studies on CPED and DailyDialog. Ablation studies typically involve removing certain features of a model or algorithm and observing how it affects the model's performance. The experiments mainly involve removing the model features individually, including removing the personality label information and initial estimates in the Feature Extraction Initialization Layer, removing the models corresponding to Personality-Aware Utterance Temporal Graph, Personality Dual-task Interaction Temporal Graph, and Personality-Aware Utterance AND Dual-task Integrated Temporal Graph in the Personality-fused Interactive Reasoning Layer, and removing the model with co-attention in the Dual-task Classification Layer. The F1 scores on the datasets are then calculated, as shown in Table5.</figDesc><table><row><cell>Model</cell><cell></cell><cell>ERC</cell><cell></cell><cell></cell><cell>DAR</cell><cell></cell></row><row><cell></cell><cell>Precision</cell><cell>Recall</cell><cell>F1</cell><cell>Precision</cell><cell>Recall</cell><cell>F1</cell></row><row><cell>JointDAS</cell><cell>0.323</cell><cell>0.263</cell><cell>0.289</cell><cell>0.536</cell><cell>0.476</cell><cell>0.514</cell></row><row><cell>IIIM</cell><cell>0.344</cell><cell>0.259</cell><cell>0.295</cell><cell>0.542</cell><cell>0.483</cell><cell>0.510</cell></row><row><cell>DCR-Net</cell><cell>0.511</cell><cell>0.372</cell><cell>0.430</cell><cell>0.58</cell><cell>0.529</cell><cell>0.554</cell></row><row><cell>BCDCN</cell><cell>0.504</cell><cell>0.398</cell><cell>0.444</cell><cell>0.555</cell><cell>0.582</cell><cell>0.568</cell></row><row><cell>Co-GAT</cell><cell>0.595</cell><cell>0.386</cell><cell>0.468</cell><cell>0.575</cell><cell>0.577</cell><cell>0.575</cell></row><row><cell>TSCL</cell><cell>0.513</cell><cell>0.422</cell><cell>0.463</cell><cell>0.584</cell><cell>0.585</cell><cell>0.584</cell></row><row><cell>DARER</cell><cell>0.556</cell><cell>0.447</cell><cell>0.495</cell><cell>0.604</cell><cell>0.587</cell><cell>0.595</cell></row><row><cell>PEDR-GAT</cell><cell>0.647</cell><cell>0.476</cell><cell>0.548</cell><cell>0.637</cell><cell>0.601</cell><cell>0.618</cell></row><row><cell cols="7">different personalities. (3) In the Feature Extraction Initialization Layer, not only</cell></row><row><cell cols="7">does it effectively utilize label information, but it also forms emotion and act features</cell></row><row><cell cols="7">guided by personality, enhancing the correlation between personality and these two</cell></row><row><cell cols="5">tasks, and enriching contextual representation information.</cell><cell></cell><cell></cell></row><row><cell cols="4">4.6 Experiment III: Ablation Study</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5</head><label>5</label><figDesc></figDesc><table><row><cell cols="5">F1 scores of PEDR-GAT on CPED and</cell></row><row><cell>DailyDialog datasets</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Model</cell><cell cols="2">CPED</cell><cell cols="2">DailyDialog</cell></row><row><cell></cell><cell>ERC</cell><cell>DAR</cell><cell>ERC</cell><cell>DAR</cell></row><row><cell>PEDR-GAT</cell><cell>0.548</cell><cell>0.618</cell><cell>0.565</cell><cell>829</cell></row><row><cell>w/o Personatily</cell><cell>0.503</cell><cell>0.568</cell><cell>0.541</cell><cell>0.811</cell></row><row><cell>w/o E t D t</cell><cell>0.511</cell><cell>0.573</cell><cell>0.505</cell><cell>0.795</cell></row><row><cell>w/o PUATGAT</cell><cell>0.482</cell><cell>0.553</cell><cell>0.493</cell><cell>0.767</cell></row><row><cell>w/o PDITGAT</cell><cell>0.468</cell><cell>0.539</cell><cell>0.488</cell><cell>0.752</cell></row><row><cell>w/o PUDAGAT</cell><cell>0.529</cell><cell>0.582</cell><cell>0.505</cell><cell>0.772</cell></row><row><cell>w/o Co-attention</cell><cell>0.531</cell><cell>0.584</cell><cell>0.521</cell><cell>0.794</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 6</head><label>6</label><figDesc>Speaker's Big Five Personality</figDesc><table><row><cell>Big Five personality</cell><cell>Speaker A</cell><cell>Speaker B</cell><cell>Speaker C</cell><cell>Speaker D</cell></row><row><cell>Neuroticism</cell><cell>high</cell><cell>low</cell><cell>high</cell><cell>high</cell></row><row><cell>Extraversion</cell><cell>high</cell><cell>high</cell><cell>high</cell><cell>low</cell></row><row><cell>Openness</cell><cell>low</cell><cell>low</cell><cell>low</cell><cell>high</cell></row><row><cell>Agreeableness</cell><cell>high</cell><cell>low</cell><cell>high</cell><cell>high</cell></row><row><cell>Conscientiousness</cell><cell>low</cell><cell>high</cell><cell>low</cell><cell>high</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 7</head><label>7</label><figDesc>Training Examples for Joint Task Recognition in Dialogue</figDesc><table><row><cell>Index</cell><cell>Speaker</cell><cell>Utterance</cell><cell></cell><cell>Dialogue Act</cell><cell>Dialogue Emotion</cell></row><row><cell># 1</cell><cell>A</cell><cell>I think he'd be more nervous about getting her a shrink.</cell><cell></cell><cell>disagreement</cell><cell>worried</cell></row><row><cell># 2</cell><cell>B</cell><cell>We need to find a professional doctor.</cell><cell></cell><cell>statement-opinion</cell><cell>neutral</cell></row><row><cell># 3</cell><cell>A</cell><cell cols="2">I'm just suggesting that we plan to do it, not rush it.</cell><cell>statement-opinion</cell><cell>anger</cell></row><row><cell></cell><cell></cell><cell>I think psychiatrists nowadays</cell><cell></cell><cell></cell><cell></cell></row><row><cell># 4</cell><cell>A</cell><cell>are as unprofessional as those</cell><cell>.</cell><cell>statement-opinion</cell><cell>disgust</cell></row><row><cell></cell><cell></cell><cell>so-called counselling companies</cell><cell></cell><cell></cell><cell></cell></row><row><cell># 5</cell><cell>B</cell><cell cols="2">You haven't seen a psychiatrist. How do you know?</cell><cell>question</cell><cell>neutral</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 8</head><label>8</label><figDesc>Testing Examples for Joint Task Recognition in Dialogue</figDesc><table><row><cell>Index</cell><cell>Speaker</cell><cell>Utterance</cell><cell>Dialogue Act</cell><cell>Act Precision</cell><cell>Dialogue Emotion</cell><cell>Emotion Precision</cell></row><row><cell># 1</cell><cell>C</cell><cell>You drew a cat's head in six days.</cell><cell>question</cell><cell>✓</cell><cell>astonished</cell><cell>✓</cell></row><row><cell># 2</cell><cell>D</cell><cell>I can't find it in me to paint.</cell><cell>answer</cell><cell>✓</cell><cell>depress</cell><cell>✓</cell></row><row><cell></cell><cell></cell><cell>If you're in a bottleneck,</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell># 3</cell><cell>C</cell><cell>you need to relax,</cell><cell>question</cell><cell>✓</cell><cell>neutral</cell><cell>✓</cell></row><row><cell></cell><cell></cell><cell>or I'll go for a walk with you.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell># 4</cell><cell>D</cell><cell>I just need to go to the supermarket.</cell><cell>statement-opinion</cell><cell>✓</cell><cell>neutral</cell><cell>✗</cell></row><row><cell># 5</cell><cell>C</cell><cell>I'll get you anything you want.</cell><cell>question</cell><cell>✗</cell><cell>neutral</cell><cell>✓</cell></row></table></figure>
		</body>
		<back>

			<div type="funding">
<div><p>Funding Source Declaration. This work was supported by the <rs type="funder">National Natural Science Foundation of China</rs> (<rs type="grantNumber">62166029</rs>), the <rs type="funder">Natural Science Foundation of Inner Mongolia</rs> (<rs type="grantNumber">2019MS06004</rs>), and the <rs type="funder">Basic Scientific Research Fund for Universities directly under Inner Mongolia Autonomous Region</rs>(<rs type="grantNumber">JY20220074</rs>,<rs type="grantNumber">JY20220420</rs>,<rs type="grantNumber">JY20220408</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_6eJ6J4J">
					<idno type="grant-number">62166029</idno>
				</org>
				<org type="funding" xml:id="_RPSZnU4">
					<idno type="grant-number">2019MS06004</idno>
				</org>
				<org type="funding" xml:id="_tqDBnvK">
					<idno type="grant-number">JY20220074</idno>
				</org>
				<org type="funding" xml:id="_95N8fW4">
					<idno type="grant-number">JY20220420</idno>
				</org>
				<org type="funding" xml:id="_XdcnGZW">
					<idno type="grant-number">JY20220408</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The model's ability to achieve better results stems from (1) the model not only considers the discourse structure related to different speaker personalities but also takes into account the interactive influences within and between tasks under personality fusion. (2) Through local utterance learning and dual-task interactive learning with utterance context, the model can comprehensively capture the feature information of both tasks in the dialogue. (3) Edge categories in the graph, along with the relative position vectors of different nodes, are utilized to update edge weights, thereby considering the speaker's emotional dependency, act dependency, and temporal features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Experiment II: Comparative experiments on datasets annotated with Big Five personality traits</head><p>Experiment II compared the accuracy of PEDR-GAT with baseline models on the CPED dataset mentioned above. Compared to the two datasets in Experiment I, the CPED dataset has already been annotated with Big Five personality traits for dialogue sentences. The model can more accurately learn the accompanying personality features of the sentences, resulting in superior recognition results, as shown in Table <ref type="table">4</ref> .</p><p>The experimental results indicate that for joint tasks, PEDR-GAT outperforms the best baseline (DARER), with an 5.3% increase in F1 for the ERC task and a 2.3% increase in F1 for the DAR task.</p><p>We attribute the performance improvement to three aspects: (1) PEDR-GAT implements bidirectional temporal modeling of the speaker's Big Five personality traits in the dialogue context, closely associating personality features with linguistic structure features, enabling the model to learn contextual representations of different personalities. (2) The personality-associated dual-task interaction graph we designed can explicitly capture intra-task dependencies and inter-task correlations within the same personality, as well as the interaction effects between emotions and acts across Ethics Approval and Consent to Participate. This paper is committed to the ethical requirements of the journal.</p><p>Competing Interests. The authors declare no competing interests.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Multi-task dialog act and sentiment recognition on Mastodon</title>
		<author>
			<persName><forename type="first">Christophe</forename><surname>Cerisara</surname></persName>
		</author>
		<idno>ArXiv abs/1807.05013</idno>
		<ptr target="https://api.semanticscholar.org/CorpusID" />
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">49742988</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Integrated neural network model for identifying speech acts, predicators, and sentiments of dialogue utterances</title>
		<author>
			<persName><forename type="first">Minkyoung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<ptr target="https://api.semanticscholar.org/CorpusID" />
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. Lett</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page">31321412</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">DCR-Net: A Deep Co-Interactive Relation Network for Joint Dialog Act Recognition and Sentiment Classification</title>
		<author>
			<persName><forename type="first">Libo</forename><surname>Qin</surname></persName>
		</author>
		<idno>ArXiv abs/2008.06914</idno>
		<ptr target="https://api.semanticscholar.org/CorpusID" />
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">214208483</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Emotion Recognition in Conversation: Research Challenges, Datasets, and Recent Advances</title>
		<author>
			<persName><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<ptr target="https://api.semanticscholar.org/CorpusID:147703962" />
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="100943" to="100953" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">DARER: Dual-task Temporal Relational Recurrent Reasoning Network for Joint Dialog Sentiment Classification and Act Recognition</title>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivor</forename><surname>Wai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-Hung</forename><surname>Tsang</surname></persName>
		</author>
		<idno>ArXiv abs/2203.03856</idno>
		<ptr target="https://api.semanticscholar.org/CorpusID" />
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page">247315257</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">CPED: A Chinese Positive Emotion Database for Emotion Elicitation and Analysis</title>
		<author>
			<persName><forename type="first">Yulin</forename><surname>Zhang</surname></persName>
		</author>
		<ptr target="https://api.semanticscholar.org/CorpusID" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">236681893</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A novel ensemble model with two-stage learning for joint dialog act recognition and sentiment classification</title>
		<author>
			<persName><forename type="first">Yujun</forename><surname>Xu</surname></persName>
		</author>
		<ptr target="https://api.semanticscholar.org/CorpusID:254206126" />
	</analytic>
	<monogr>
		<title level="j">Pattern Recognit. Lett</title>
		<imprint>
			<biblScope unit="volume">165</biblScope>
			<biblScope unit="page" from="77" to="83" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Contrast and Generation Make BART a Good Dialogue Emotion Recognizer</title>
		<author>
			<persName><forename type="first">Shimin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xipeng</forename><surname>Qiu</surname></persName>
		</author>
		<idno>ArXiv abs/2112.11202</idno>
		<ptr target="https://api.semanticscholar.org/CorpusID:245353550" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Attention-based aspect sentiment classification using enhanced learning through cnn-Bilstm networks</title>
		<author>
			<persName><forename type="first">Eniafe</forename><surname>Festus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ayetiran</forename></persName>
		</author>
		<ptr target="https://api.semanticscholar.org/CorpusID:250969738" />
	</analytic>
	<monogr>
		<title level="j">Knowl. Based Syst</title>
		<imprint>
			<biblScope unit="volume">252</biblScope>
			<biblScope unit="page">109409</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">DialogueRNN: An Attentive RNN for Emotion Detection in Conversations</title>
		<author>
			<persName><forename type="first">Navonil</forename><surname>Majumder</surname></persName>
		</author>
		<ptr target="https://api.semanticscholar.org/CorpusID" />
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">53172956</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">DialogueCRN: Contextual Reasoning Networks for Emotion Recognition in Conversations</title>
		<author>
			<persName><forename type="first">Dou</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingwei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyong</forename><surname>Huai</surname></persName>
		</author>
		<idno>ArXiv abs/2106.01978</idno>
		<ptr target="https://api.semanticscholar.org/CorpusID" />
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page">235313788</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Dialogue Act Sequence Labeling using Hierarchical encoder with CRF</title>
		<author>
			<persName><forename type="first">Harshit</forename><surname>Kumar</surname></persName>
		</author>
		<idno>ArXiv abs/1709.04250</idno>
		<ptr target="https://api.semanticscholar.org/CorpusID" />
		<imprint>
			<date type="published" when="2017">2017. 19202736</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Dialogue Act Classification with Context-Aware Self-Attention</title>
		<author>
			<persName><forename type="first">Vipul</forename><surname>Raheja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><forename type="middle">R</forename><surname>Tetreault</surname></persName>
		</author>
		<idno>ArXiv abs/1904.02594</idno>
		<ptr target="https://api.semanticscholar.org/CorpusID" />
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">102354395</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Sentence encoding for Dialogue Act classification</title>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Duran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steve</forename><surname>Battle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jim</forename><forename type="middle">E</forename><surname>Smith</surname></persName>
		</author>
		<ptr target="https://api.semanticscholar.org/CorpusID:242094848" />
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="794" to="823" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Speaker and Time-aware Joint Contextual Learning for Dialogue-act Classification in Counselling Conversations</title>
		<author>
			<persName><forename type="first">Ganeshan</forename><surname>Malhotra</surname></persName>
		</author>
		<ptr target="https://api.semanticscholar.org/CorpusID" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining</title>
		<meeting>the Fifteenth ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page">244103083</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">A Bi-directional Multi-hop Inference Model for Joint Dialog Sentiment Classification and Act Recognition</title>
		<author>
			<persName><forename type="first">Limin</forename><surname>Zheng</surname></persName>
		</author>
		<idno>ArXiv abs/2308.04424</idno>
		<ptr target="https://api.semanticscholar.org/CorpusID:260704432" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">DialogueGCN: A Graph Convolutional Neural Network for Emotion Recognition in Conversation</title>
		<author>
			<persName><forename type="first">Deepanway</forename><surname>Ghosal</surname></persName>
		</author>
		<ptr target="https://api.semanticscholar.org/CorpusID" />
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<date type="published" when="2019">2019. 201698197</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Modeling both Context-and Speaker-Sensitive Dependence for Emotion Detection in Multi-speaker Conversations</title>
		<author>
			<persName><forename type="first">Dong</forename><surname>Zhang</surname></persName>
		</author>
		<ptr target="https://api.semanticscholar.org/CorpusID" />
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2019">2019. 199466225</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">MuCDN: Mutual Conversational Detachment Network for Emotion Recognition in Multi-Party Conversations</title>
		<author>
			<persName><forename type="first">Weixiang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanyan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<ptr target="https://api.semanticscholar.org/CorpusID:252819095" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Computational Linguistics</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">DualGATs: Dual Graph Attention Networks for Emotion Recognition in Conversations</title>
		<author>
			<persName><forename type="first">Duzhen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feilong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiuyi</forename><surname>Chen</surname></persName>
		</author>
		<ptr target="https://api.semanticscholar.org/CorpusID" />
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page">259370771</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Co-GAT: A Co-Interactive Graph Attention Network for Joint Dialog Act Recognition and Sentiment Classification</title>
		<author>
			<persName><forename type="first">Libo</forename><surname>Qin</surname></persName>
		</author>
		<ptr target="https://api.semanticscholar.org/CorpusID" />
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page">229371557</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multiple Information-Aware Recurrent Reasoning Network for Joint Dialogue Act Recognition and Sentiment Classification</title>
		<author>
			<persName><forename type="first">Shi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoting</forename><surname>Chen</surname></persName>
		</author>
		<ptr target="https://api.semanticscholar.org/CorpusID" />
	</analytic>
	<monogr>
		<title level="j">Inf</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">264940937</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">DailyDialog: A Manually Labelled Multi-turn Dialogue Dataset</title>
		<author>
			<persName><forename type="first">Yanran</forename><surname>Li</surname></persName>
		</author>
		<idno>ArXiv abs/1710.03957</idno>
		<ptr target="https://api.semanticscholar.org/CorpusID" />
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">11267601</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Automatic differentiation in PyTorch</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<ptr target="https://api.semanticscholar.org/CorpusID:40027675" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Modeling Local Contexts for Joint Dialogue Act Recognition and Sentiment Classification with Bi-channel Dynamic Convolutions</title>
		<author>
			<persName><forename type="first">Jingye</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donghong</forename><surname>Ji</surname></persName>
		</author>
		<ptr target="https://api.semanticscholar.org/CorpusID:227230352" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Computational Linguistics</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Personality Trait Detection Using Bagged SVM over BERT Word Embedding Ensembles</title>
		<author>
			<persName><forename type="first">Amirmohammad</forename><surname>Kazameini</surname></persName>
		</author>
		<idno>ArXiv abs/2010.01309</idno>
		<ptr target="https://api.semanticscholar.org/CorpusID:219572556" />
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
