<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="fr">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Université de Rennes</orgName>
								<address>
									<addrLine>1 -France</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">INSERM -France</orgName>
								<orgName type="institution" key="instit2">SouthEast University</orgName>
								<address>
									<country>-China</country>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note type="submission">Additionally, I would like to thank Professor Olivier MICHEL and Professor Didier WOLF who accepted to review my thesis, as well as Professor Dominique PASTOR and Associate Professor Sofiane BOUDAOUD who accepted to preside at</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-21T20:34+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Partial Mutual Information Power Spectrum Density Partial Transfer Entropy</term>
					<term>RKHS: Reproducing Kernel Hilbert Spaces</term>
					<term>SDE: Stochastic Differential Equation</term>
					<term>SPECT: Single-Photon Emission Computed Tomography</term>
					<term>STE: Symbolic Transfer Entropy</term>
					<term>TDMI: Time-Delayed Mutual Information</term>
					<term>TE: Transfer Entropy</term>
					<term>TLMI: Time-Lagged Mutual Information</term>
					<term>VAR: Vectorial AutoRegressive</term>
					<term>VEPs: Visual Evoked Potentials</term>
					<term>VNS: Vagus Nerve Stimulation</term>
					<term>WGC: Wiener-Granger Causality VII</term>
				</keywords>
			</textClass>
			<abstract xml:lang="en">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This thesis was performed in the frame of CRIBs (Centre de Recherche en Information Biomédicale sino-français), which is an international associate French-Chinese laboratory</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="fr">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Résumé</head><p>Les travaux présentés dans cette thèse s'inscrivent dans la problématique de la connectivité cérébrale, connectivité tripartite puisqu'elle sous-tend les notions de connectivité structurelle, fonctionnelle et effective. Ces trois types de connectivité que l'on peut considérer à différentes échelles d'espace et de temps sont bien évidemment liés et leur analyse conjointe permet de mieux comprendre comment structures et fonctions cérébrales se contraignent mutuellement. Notre recherche relève plus particulièrement de la connectivité effective qui permet de définir des graphes de connectivité qui renseignent sur les liens causaux, directs ou indirects, unilatéraux ou bilatéraux via des chemins de propagation, représentés par des arcs, entre les noeuds, ces derniers correspondant aux régions cérébrales à l'échelle macroscopique. Identifier les interactions entre les aires cérébrales impliquées dans la génération et la propagation des crises épileptiques à partir d'enregistrements intracérébraux est un enjeu majeur dans la phase pré-chirurgicale et l'objectif principal de notre travail. L'exploration de la connectivité effective suit généralement deux approches, soit une approche basée sur les modèles, soit une approche conduite par les données comme nous l'envisageons dans le cadre de cette thèse où les outils développés relèvent de la théorie de l'information et plus spécifiquement de l'entropie de transfert, la question phare que nous adressons étant celle de la précision des estimateurs de cette grandeur dans le cas des méthodes développées basées sur les plus proches voisins. Les approches que nous proposons qui réduisent le biais au regard d'estimateurs issus de la littérature sont évaluées et comparées sur des signaux simulés de type bruits blancs, processus vectoriels autorégressifs linéaires et non linéaires, ainsi que sur des modèles physiologiques réalistes avant d'être appliquées sur des signaux électroencéphalographiques de profondeur enregistrés sur un patient épileptique et comparées à une approche assez classique basée sur la fonction de transfert dirigée. En simulation, dans les situations présentant des non-linéarités, les résultats obtenus permettent d'apprécier la réduction du biais d'estimation pour des variances comparables vis-à-vis des techniques connues. Si les informations recueillies sur les données réelles sont plus difficiles à analyser, elles montrent certaines cohérences entre les méthodes même si les résultats préliminaires obtenus s'avèrent davantage en accord avec les conclusions des experts cliniciens en appliquant la fonction de transfert dirigée.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mesures Entropiques de</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Connectivité avec Application à l'Épilepsie</head><p>Quelle est la structure du cerveau ? Quel en est le fonctionnement physiologique ? Quels sont les dysfonctionnements qui peuvent le dénaturer et le faire passer d'un état sain à un état pathologique ? C'est au travers de la connectivité effective que nous allons chercher des éléments de réponse à ces questions et essayer de comprendre ce que soustend ce concept que nous aborderons sous l'angle de la théorie de l'information et plus exactement sous celui de l'entropie de transfert et de son estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Chapitre 1. Contexte Clinique</head><p>Notre recherche s'inscrit dans le contexte large d'une compréhension, générique et patient par patient, de certains types d'épilepsie dont les manifestations peuvent être notablement réduites, voire disparaître, à la suite d'un acte chirurgical consistant en une résection d'une région cérébrale qui doit, pour cela, être extrêmement bien délimitée par des examens pré-chirurgicaux incluant l'analyse de signaux électroencéphalographiques, enregistrés en dehors et pendant les périodes critiques. Rappelons que l'épilepsie est une maladie neurologique relativement répandue qui concerne environ 40 millions de personnes dans le monde, dont, en France, 7 individus sur 1000. C'est une affection chronique qui s'étale dans le temps et se caractérise par la survenue de convulsions (ou crises convulsives) qui sont le résultat de décharges électriques paroxystiques. On distingue deux types d'épilepsie, l'épilepsie généralisée, lorsque les décharges ont lieu dans l'ensemble du cortex cérébral et l'épilepsie partielle, lorsque les décharges se produisent dans une partie bien délimitée du cortex cérébral, et qui sont celles concernées ici. L'épilepsie est le résultat d'une activation survenant subitement, de manière simultanée et anor-1</p><p>Thèse en français malement soutenue, d'un nombre très important de groupes neuronaux, dans certaines régions du cerveau, dites épileptogènes. Elle peut se traduire par des manifestations neurologiques telles que des troubles visuels, olfactifs, auditifs, gustatifs, des pertes de conscience, des convulsions...Si la cause exacte de cette maladie est parfois inconnue, elle peut être par ailleurs la conséquence d'une tumeur au cerveau, d'un accident cérébral vasculaire, d'une intoxication, d'une malformation des vaisseaux cérébraux, de séquelles d'un traumatisme. Différents traitements de la maladie sont envisageables suivant la nature du syndrome, son origine et son intensité : prise de médicaments, stimulation électrique, traitement chirurgical. C'est dans ce dernier cas que se positionne ce travail, lorsque l'épilepsie est pharmaco-résistante (i.e. rebelle à tout traitement médicamenteux) et qu'il faut s'en remettre à une chirurgie curative et pratiquer une exérèse d'une partie du cortex cérébral pour supprimer la cause de l'épilepsie. Les patients souffrant d'épilepsies pharmaco-résistantes bénéficient d'une évaluation pré-chirurgicale visant à délimiter la Zone Epileptogène (ZE), responsable de la genèse et de la propagation des crises épileptiques. Une question clé dans la prise en charge de ces patients est donc l'identification de cette ZE. Le service de Neurologie du CHU de Rennes, partenaire du laboratoire, fait partie des rares unités cliniques pratiquant l'enregistrement de signaux intracérébraux (recueillis sur des électrodes de profondeur). Bien qu'invasive, cette procédure permet une exploration plus fine du cerveau, directement au contact des régions épileptiques, en vue de définir au mieux quelle(s) région(s) exciser. Cette définition implique d'identifier des interactions pathologiques entre les aires cérébrales impliquées dans la génération et la propagation des crises. Atteindre ce but à partir d'enregistrements intracérébraux est un enjeu clinique majeur dans la phase pré-chirurgicale et correspond à la motivation principale de notre travail. Etablir ainsi un graphe de connectivité cérébrale à partir de signaux enregistrés en profondeur ou en surface requiert l'application de techniques avancées de traitement du signal. Cette connectivité se décline en trois types. La connectivité anatomique (ou structurelle) réfère à l'ensemble des connexions physiques (i.e. axonales) liant des groupes neuronaux plus ou moins grands (de la connexion synaptique entre neurones individuels à l'analyse de paquets de connexions ou d'ensembles synaptiques liant des populations neuronales, voire à grande échelle, à l'analyse des connexions vues comme des chemins liant les grandes régions du cerveau). La connectivité fonctionnelle repose, quant à elle, sur l'étude des liens statistiques entre des signaux reflétant des activités cérébrales dans des régions distinctes, sans ambitionner de mettre en évidence des influences causales. Elle renvoie ainsi au concept de corrélation spatio-temporelle entre activités résultant d'interactions neuronales dynamiques permises par les connexions anatomiques permanentes. Bien évidemment, cette connectivité fonctionnelle entre deux aires cérébrales n'implique pas nécessairement l'existence d'une connexion anatomique directe entre elles, une corrélation mesurée pouvant être le résultat d'une médiation par une structure tierce. Plus avant, la connectivité effective est une notion plus forte en ce sens qu'elle s'intéresse à l'organisation des flux d'information entre régions cérébrales et est définie comme l'influence directe ou indirecte exercée par un système neuronal sur un second système.</p><p>Ces trois types de connectivité que l'on peut considérer à différentes échelles d'espace et de temps sont bien évidemment liés et leur analyse conjointe permet de mieux comprendre comment structures et fonctions cérébrales se contraignent mutuellement. Notre problématique relève plus particulièrement de la connectivité effective qui permet entre autres de définir des graphes de connectivité renseignant sur les liens causaux, directs ou indirects, unilatéraux ou bilatéraux via des chemins de propagation (représentés par des arcs) entre les noeuds, ces derniers correspondant aux régions cérébrales à l'échelle macroscopique. Au-delà de la connectivité fonctionnelle qui renvoie à la notion de couplage statistique entre signaux, il s'agit donc d'établir des graphes orientés, dans lesquels les éléments constitutifs sont pondérés par des coefficients traduisant la densité ou l'efficacité des connexions, ces graphes décrivant des flux d'informations entre populations impliquées. L'enjeu est donc de comprendre les fonctions du cerveau non seulement en identifiant correctement les régions activées mais aussi en décelant les interactions fonctionnelles au cours du temps parmi les ensembles neuronaux stimulés éventuellement éloignés. Notre questionnement concerne donc le sens de circulation de l'information entre différents sites, sens pouvant évoluer durant le décours temporel d'une crise. D'un point de vue applicatif (investigation clinique) ce à quoi nous voulons répondre à long terme se résume à : quelles sont les structures impliquées dans les activités neuronales au cours des crises ? Est-il possible de détecter des structures dominantes dans les réseaux épileptogènes ? Une réponse à ces questions doit dégager la route d'une compréhension des mécanismes mis en jeu au cours d'activités épileptiques afin d'éradiquer la survenue des crises de manière adaptée, patient par patient.</p><p>En amont de ces visées cliniques, la détection de causalités significatives à partir de signaux disponibles oblige à recourir à des techniques spécifiques de traitement du signal qui correspondent, après définition de certains indices théoriques candidats à exprimer cette causalité, à la production d'algorithmes d'estimation de ces indices, associés à une mesure de confiance statistique, généralement en termes de biais et de variance.</p><p>Différents indices et algorithmes sont disponibles dans la littérature, concernant aussi bien l'analyse de mécanismes cérébraux, que celle d'autres phénomènes physiologiques, sans compter avec de nombreuses autres applications, comme par exemple l'analyse de systèmes dynamiques non linéaires couplés en physique. Une grande partie des résultats publiés dans ce champ sont validés au moyen de simulations de systèmes dynamiques aléatoires, linéaires ou non. Deux questionnements nous ont semblé a priori légitimes. Le premier concerne l'amélioration des estimateurs d'un type particulier d'indice, l'entropie de transfert, qui tout à la fois présente des vertus de grande généralité mais également l'inconvénient de souffrir de biais d'estimation notable. Le deuxième concerne l'apport concret de certaines améliorations d'estimateurs d'entropie de transfert, parfois obtenues au prix d'une complexité calculatoire accrue, quand on les applique aux signaux réels rencontrés dans le domaine de l'épilepsie, bien connus pour leur nature complexe.</p><p>Chapitre 2. Etat de l'Art L'exploration de la connectivité effective suit généralement deux approches, soit une approche conduite par les données (incluant les méthodes de type "causalité <ref type="bibr">de Granger"</ref> ou basées sur la théorie de l'information) soit une approche basée sur les modèles comme il en va pour la modélisation causale dynamique (Dynamic Causal Modeling (DCM)).</p><p>L'idée de base pour la première approche revient à Wiener, bien que connue sous le nom de causalité de Granger (il est de fait plus correct de parler de causalité de Wiener-Granger puisqu'elle fut d'abord introduite par Wiener). Celle-ci consiste à considérer qu'un signal Y cause un signal X si Y contient des informations permettant d'affiner la prédiction de X (où la prédiction est à structure imposée, linéaire, et optimale au sens d'une variance d'erreur de prédiction minimale) relativement à une prédiction basée uniquement sur le passé de X et éventuellement sur d'autres variables contextuelles.</p><p>Autrement dit, selon Granger, X est causé par Y si la prise en compte supplémentaire de valeurs passées de Y permet de mieux prédire les valeurs de X qu'en se basant uniquement sur les valeurs passées de X .</p><p>Depuis son introduction en économétrie par Granger en 1969, cette notion de causalité linéaire en moyenne quadratique a bien évidemment fait l'objet d'un certain nombre de débats, entre autres sur son insuffisance à capter les liens de causalité indirects pouvant exister entre deux variables, dès lors qu'il existe au moins une troisième série dans le système. S'en sont ensuivies les questions de mesures de causalité conditionnelle dans le domaine temporel, étendues par la suite au domaine fréquentiel. Si la causalité de Granger est devenue une mesure très largement utilisée, elle n'en reste pas moins perfectible dans des situations où les signaux traités s'avèrent complexes, et des extensions de cette mesure dans des situations présentant des non-linéarités ont déjà été envisagées.</p><p>Plus récemment mais souvent dans la même lignée, de nouvelles approches fréquentielles ont été proposées et largement utilisées.</p><p>Parallèlement à ces méthodes conduites par les observations, se sont développées des méthodes basées sur des modèles, qui tiennent compte de certaines hypothèses a priori portant sur les mécanismes physiologiques sous-jacents, la plus connue étant sans doute l'approche de modèle causal dynamique ainsi nommée et popularisée dans les années 2000 par Friston, initialement pour l'analyse de réseaux cognitifs. Pour cela les popula-tions neuronales a priori concernées dans le réseau étudié sont modélisées par des systèmes d'équations différentielles stochastiques non linéaires incluant des paramètres physiologiquement significatifs et dont les interactions sont représentées par des paramètres dits de connectivité. Les signaux d'observation, chacun attaché à l'une des populations, sont également modélisés, ce qui constitue la sortie du modèle à confronter aux observations. Pour décider de la topologie d'un graphe de connectivité effective entre les populations du réseau, on introduit autant de modèles du réseau que de graphes orientés candidats. Une procédure (coûteuse en temps de calcul) d'estimation paramétrique est ensuite appliquée à chacun des modèles envisagés, procédure qui délivre des valeurs de paramètres de populations et de connectivité. Finalement une procédure statistique de sélection de modèle élit le modèle le plus vraisemblable et donc le graphe de connectivité correspondant. Notons cependant que la connaissance d'un graphe statique (orienté) de connectivité est par essence qualitative et ne quantifie pas en elle-même les transferts dynamiques d'information entre populations. Une évaluation de ces derniers est théoriquement possible, mais nécessiterait une reconstruction des trajectoires d'état pour chaque population, ce qui finit par constituer une méthodologie lourde, d'où l'intérêt a priori des méthodes dirigées par les données qui sont les seules considérées dans cette thèse, particulièrement celles qui ont été proposées pour permettre l'analyse de dynamiques non linéaires. Ces dernières comprennent les méthodes s'appuyant sur des régressions non linéaires paramétriques ou pseudo non paramétriques (méthodes à noyau) et les méthodes non paramétriques dites entropiques popularisées depuis l'introduction par Schreiber de l'entropie de transfert en 2002, pour laquelle on peut établir une relation avec l'indice de causalité de Granger quand les observations sont assimilées à des processus aléatoires conjointement gaussiens. L'entropie de transfert est définie initialement seulement pour une paire (ordonnée) de signaux. Il s'avère cependant que cette méthode peut s'étendre sans difficulté, du moins du point de vue de la définition d'un indice théorique, à une forme conditionnelle si tant est que l'on cherche à exclure (ou plus justement à tenir compte de) l'influence d'un signal (ou d'un groupe de signaux) tiers. L'entropie de transfert sera très largement développée dans le chapitre suivant puisqu'elle est au coeur de ce travail de recherche.</p><p>Chapitre 3. Méthodes et Matériels d'achoppement dans cette estimation réside dans le biais qui lui est attaché. Nos travaux s'inscrivent donc dans cette problématique, ce chapitre étant consacré à (i) définir les quantités entropiques concernées d'un point de vue formel, (ii) présenter des estimateurs proposés dans la littérature pour ces quantités, et enfin (iii) proposer de nouveaux estimateurs conçus pour abaisser le biais, la stabilité de la variance n'étant contrôlée qu'expérimentalement.</p><p>Dans ce chapitre, nous donnons la définition des différentes quantités entropiques (des fonctionnelles à valeurs réelles admettant en argument une ou plusieurs distributions de probabilité) impliquées dans notre travail, nommément l'entropie, les entropies conjointe et conditionnelle, l'information mutuelle, les entropies de transfert standard et conditionnelle. Des points communs entre le calcul de l'information mutuelle et celui de l'entropie de transfert sont soulignés.</p><p>Puis, avant de nous focaliser sur l'entropie de transfert elle-même, nous commençons par rappeler les techniques les plus utilisées dans l'estimation de l'entropie, à savoir les approches à noyaux et celles basées sur les plus proches voisins. Ces dernières font l'objet de nos développements ultérieurs. Elles impliquent l'utilisation d'une distance dans l'espace des observations déduite d'une norme qui doit être spécifiée, en général la norme du maximum ou la norme euclidienne. Ainsi, les deux estimateurs d'entropie qui sont considérés par la suite peuvent être implémentés tant pour la norme euclidienne que la norme du maximum. Toutefois, par la suite, sera essentiellement considéré le cas de cette dernière norme, pour laquelle une boule formelle correspond concrètement à un cube. Ces deux estimateurs issus de la littérature sont celui de Kozachenko-Leonenko et celui de Singh, menant à des estimations très proches pour de longues séquences d'observations. Ce chapitre se poursuit alors par l'estimation de l'information mutuelle qui peut s'écrire comme une somme algébrique d'entropies conjointe et marginales. Cette écriture peut faire espérer une compensation partielle des biais à condition qu'ils soient du même ordre de grandeur. Pour que cette dernière condition soit remplie, la première stratégie adoptée par Kraskov consiste à fixer le nombre k de voisins dans l'espace joint (comme d'ordinaire) et d'exporter dans les espaces marginaux la distance entre le point courant et son k ème plus proche voisin, i.e. le rayon du voisinage-boule dont le volume intervient dans l'estimation, pour y construire des boules de même rayon que dans l'espace d'origine.</p><p>Dans une deuxième étape, Kraskov suggère de reconsidérer le voisinage dans l'espace joint en remplaçant l'hyper-cube par un hyper-rectangle et en exportant cette fois deux valeurs de distance distinctes dans les deux espaces marginaux, respectivement, conduisant à l'écriture d'un second estimateur de l'information mutuelle.</p><p>La suite de ce chapitre pose logiquement la question de l'estimation de l'entropie de transfert en présentant tout d'abord deux estimateurs issus de la littérature basés sur les travaux précités de Kraskov et en ouvrant sur une discussion qui pose les bases des améliorations qui seront proposées pour les estimateurs déjà existants, soit plus précisément :</p><p>-(i) considérant l'estimation de l'information mutuelle sous la forme d'une somme algébrique d'entropies, il est possible d'exprimer le biais résultant comme une même combinaison des biais attachés à leurs estimations respectives, et un choix judicieux des rayons des voisinages dans les espaces marginaux peut permettre de le réduire (c'est la démarche suivie empiriquement par Kraskov qui l'a appliquée au cas de la norme du maximum). Toutefois, cette procédure requiert de disposer d'une expression théorique du biais (pour une norme quelconque) et de pouvoir en déduire les rayons des boules dans les espaces marginaux pour l'annuler ou du moins l'atténuer. Pour cela, les valeurs optimales des rayons marginaux doivent pouvoir s'exprimer en fonction des seules informations disponibles, ce qui exclut de les faire dépendre des fonctions de densité de probabilité et de leurs dérivées ; -(ii) une seconde idée pour réduire individuellement chaque biais est de considérer comme voisinages les hyper-rectangles de volume minimal incluant les voisins sélectionnés, aussi bien dans l'espace joint que dans les espaces marginaux.</p><p>Ces éléments de réflexion sont à l'origine des améliorations proposées dans la suite.</p><p>Ainsi, dans un premier temps, nous introduisons une forme analytique du biais pour l'estimation de l'entropie individuelle basée sur un développement de Taylor à l'ordre deux, utilisable aussi bien pour l'approche d'estimation de densité de probabilité par noyau que pour l'approche des k plus proches voisins, à la seule condition que le rayon de la boule-voisinage ne soit pas trop grand. Cette forme analytique du biais est donnée à la fois dans le cas de la norme euclidienne et de la norme du maximum. Dans le cas de signaux indépendants, une relation est alors établie entre les rayons des voisinages dans les espaces marginaux et celui déterminé par l'emplacement du k ème plus proche voisin dans l'espace joint, pour chacune de ces deux normes, afin d'annuler le biais à la fois dans l'estimation de l'information mutuelle et de l'entropie de transfert. Cependant, la condition d'indépendance s'avère, dans le cas de l'entropie de transfert, peu réaliste, car elle implique pratiquement que le signal subissant potentiellement une influence causale soit une suite de variables indépendantes et identiquement distribuées (un bruit blanc).</p><p>Même si ce ne sera pas le cas dans ce travail, la stratégie proposée pourrait être généralisée en vue d'être appliquée à l'estimation de l'entropie de transfert conditionnelle. Sous l'hypothèse de non indépendance, il n'est plus possible d'annuler le biais avec la même stratégie et notre choix s'est porté vers des combinaisons linéaires pondérées d'estimateurs que ce soit pour le calcul de l'entropie, de l'information mutuelle ou de l'entropie de Thèse en français transfert. Il s'agit en fait de faire intervenir dans les différentes grandeurs des nombres de voisins variables différemment pondérés et de trouver le meilleur compromis. Cette procédure conduit aux estimateurs que nous dénommons "estimateurs composés".</p><p>Dans une dernière partie, nous repartons des estimateurs d'entropie proposés d'une part par <ref type="bibr">Kozachenko-Leonenko, et</ref>  En conclusion de ce chapitre, pour l'information mutuelle, les résultats sur signaux simulés prouvent l'efficacité de la stratégie proposée dans le chapitre précédent. Quant à l'entropie de transfert, si l'un des estimateurs se trouve en difficulté lorsque l'on considère un nombre de voisins faible, les deux estimateurs développés apparaissent appropriés dans une vision de réduction du biais d'estimation, d'autant plus en présence de non-linéarités, et ce pour des temps de calculs comparables à ceux demandés par les estimateurs déjà existants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Chapitre 5. Analyse de Signaux Réels</head><p>Les estimateurs proposés dans le chapitre 3 s'étant avérés pertinents quand on les a évalués sur des simulations (chapitre 4), l'objectif du présent chapitre est de les confronter aux signaux réels enregistrés sur un patient épileptique, afin de vérifier si certaines hiérarchies de performances se maintiennent. Ces signaux réels proviennent d'un su- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>The detection of causal relations plays a fundamental role in many domains, where it is important to identify causal relations between a set of subsystems.</p><p>This work takes place in the context of human epileptic seizures. Epilepsy is a neurological disease, which is the fourth most common neurological disorder and affects people of all ages. This disease is characterized by the repetition of seizures, called ictal periods, whose frequency and duration are variable. Many epileptic patients may have other symptoms of neurological problems as well. In about 30% of cases, the patients do not successfully respond to anti-seizure drug therapy, or in other words, the epilepsies remain drug-resistant. In this case, surgery is an alternative for those patients, whose seizures cannot be controlled by medications.</p><p>The use of epilepsy surgery increased in the 1980s and 90s, which reflects its effectiveness as an alternative to seizure medicines. However, for this kind of treatment, there is no guarantee of success in controlling seizures, and the benefits of surgery should be weighed carefully against its risks. For a given patient, the foremost difficulty is to determine the epileptogenic zone which is responsible for the seizure, and the surgery should be carried out under the constraint that post-surgical deficits (sensitive, driving or cognitive) are limited. The initiation and the propagation of epileptic activities often take place in a network of neuronal ensembles which are distributed in distant structures. Now, the localization and the characterization of these distributed epileptogenic networks are known as difficult tasks but they are essential to expect eradicating seizures.</p><p>Technically, these tasks imply to adopt some quantitative description of causality and to develop algorithms aimed at estimating the strength of influence between different subsystems. In this work, we mainly focus on information-theoretic measures, such as mutual information and transfer entropy, which consider no assumption on the underlying model. These two quantities are generally estimated using a sum of individual entropies.</p><p>A drawback of these measures is that it is difficult to obtain an accurate estimation with a limited number of samples, which is common in real applications. Moreover, difficulty arises when those information-theoretic quantities are calculated in high-dimensional spaces, that is commonly called the "curse of dimensionality". So, it is meaningful to make effort to improve the estimation of these quantities. To this end, we consider two different ways in this work, (i) quantify the estimation bias of each individual entropy, and try to cancel out their combination in the summation, (ii) reduce the bias of each individual entropy estimation so as to decrease the total bias in the summation.</p><p>In this thesis, we first give some technical discussions on the estimation of informationtheoretic quantities, and then apply different causality detection measures to real epileptic signals.</p><p>The remainder of this thesis is organized as follows.</p><p>Chapter 1 is a general introduction to the research background. In this chapter, firstly, we make a short summary of epilepsy and its treatment, including both medication and surgery. The structure of the human brain is recalled and different types of brain connectivity are presented.</p><p>In this work, we are interested in effective connectivity, which plays an important role in the treatment of epilepsy. Chapter 2 presents a detailed study of different effective connectivity estimation methods. We classify the measurements of effective connectivity into two classes, (i) data-driven methods, including Granger causality related and information-theoretic approaches which are the only ones to be addressed in this thesis, and (ii) model-based methods such as dynamic causal modeling (DCM).</p><p>Chapter 3 concerns the estimation of information-theoretic quantities. After a detailed summary of previous works, two novel improvements are deeply investigated. First of all, a novel analytical form of the bias for the estimation of entropy is presented. Then, we apply it into the estimation of mutual information and transfer entropy. Secondly, we discuss the estimation of information-theoretic quantities using the maximum norm.</p><p>For the standard maximum norm, the determined region around the center point is a (hyper-)cube, where the sizes in all directions are identical. In this chapter, we propose to release this restriction, and use a (hyper-)rectangle instead of a (hyper-)cube, so that the sizes in the different directions can be of different lengths. In this chapter, several new mutual information and transfer entropy estimators are proposed.</p><p>In chapter 4, the performance of the different algorithms we propose is evaluated through numerical simulations under different situations, including independent and dependent signals, linear and nonlinear relations. Different types of models are tested, including white processes, linear vectorial autoregressive models, nonlinear models as well as a physiology-based model.</p><p>In chapter 5, we analyze intracranial (or intracerebral) electroencephalographic (iEEG) signals recorded in the cerebral cortex of an epileptic patient. Both Granger causality and different transfer entropy estimators are tested on these epileptic signals.</p><p>Since the directed transfer function is a measure widely used in recent literature in the analysis of electroencephalographic (EEG) datasets, we decide to compare a local connectivity index based on our two-channel algorithms.</p><p>To conclude this thesis, we summarize our contributions to the field of the effective connectivity analysis and possible directions for further work are also addressed for essential improvement in clinical context.</p><p>Chapter 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Research Background</head><p>Our research is performed in the context of epilepsy surgery and, in this chapter, we give a brief introduction to this field. In section 1.1, we present epilepsy and briefly describe the human brain in section 1.2 (including both external and internal morphologies), and, finally, in section 1.3, launch a short discussion on human brain connectivities.</p><p>1.1. Epilepsy  According to this definition, there is a persistent intrinsic epileptogenic abnormality existing in the brain of epileptic patient, even outside the duration of seizures. So, as illustrated in Fig. <ref type="figure" target="#fig_12">1</ref>.1, for the affected patient, epilepsy is not only limited to seizures, but also includes psychological and social consequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.2.">Epilepsy Treatment</head><p>Once a firm diagnosis of epilepsy has been made, the treatment of this disease must be considered, for which the desired goal is "no seizures, no side effects ". </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Epilepsy</head><p>Chapter 1</p><p>This kind of surgery is risky since it can worsen existing problems or create new ones in the brain functions, for instance loss of functions such as vision, speech, memory or movement. Therefore, epilepsy surgery is considered only within certain situations:</p><p>(a) seizures are uncontrollable with medications, (b) the seizure focus can be clearly identified and is not responsible for any critical functions, such as language, sensation and movement, and (c) the life quality of the patient is significantly affected by the seizures. Currently, iEEG is the only technique that provides direct access to electrophysiological recordings in the seizure onset zone, and it allows the determination of the depth of epileptogenic areas. It should be mentioned here that, currently, this procedure is risky, and it could lead to brain hemorrhage and infection in rare cases (less than 1%) <ref type="bibr" target="#b49">[Cossu 2005</ref>].</p><p>Chapter 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Human Brain</head><p>During the mapping, the patient has to be awakened to identify functional areas such as language, sensation or vision <ref type="bibr" target="#b74">[Ellen 2015</ref>]. These data collected with small electric probes are examined by the experts on a comprehensive epilepsy board, and surgery is processed based on their conclusions. In some rare cases, further surgery is not recommended if a single seizure focus cannot be revealed during the intracranial monitoring.</p><p>In some cases, seizures can be completely controlled after surgery, while, for others, the frequency of seizures is significantly reduced. After surgery, most patients need to continue taking anti-seizure medication for at least one year. In 2011, a study, which was performed among 615 adults who underwent epilepsy surgeries, identified long-term outcomes of epilepsy surgery in adults <ref type="bibr" target="#b58">[de Tisi 2011]</ref>. According to this study, about 52% of patients remained seizure-free five years after the surgery.</p><p>The problem is then to exploit collected iEEG signals to discover strategic informations about the spatio-temporal epileptic propagation mechanisms between involved structures in the brain. This leads to an approach which has its roots in the connectivity paradigm introduced in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Human Brain</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.1.">Introduction</head><p>The brain acts as the center of human nervous system, and a good understanding on the internal organization of the brain, which is always the central issue of modern neurobiology, would help in the treatment of brain diseases, especially epilepsy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.1.1.">Morphology of the Brain</head><p>The brain weighs about 1300 1400 grams (about 2% of total body weight) <ref type="bibr" target="#b77">[Eric 2015]</ref> and is protected by the skull. Compared with the brain of other mammals, the human brain has a larger relative size. It is composed of two hemispheres (left and right), which are nearly symmetrical. These two hemispheres are separated by a deep median furrow (longitudinal fissure of the brain, or inter-hemispheric fissure) and interconnected by a very large nerve bundle (the corpus callosum, which crosses the midline above the level of the thalamus) and two other smaller connections, the anterior commissure and hippocampal commissure <ref type="bibr" target="#b175">[Mooshagian 2008</ref>]. On the surface of the hemispheres, there is a pallium of very pleated gray matter. This superficial gray matter is the cerebral cortex, which contains many folds. On the cerebral cortex, the deepest folds are called furrows (or fissures). Each hemisphere can be conventionally divided into four lobes: the frontal lobe, parietal lobe, occipital lobe, and temporal lobe. The temporal lobe is on the side of the brain, the parietal lobe is positioned above the occipital lobe and behind the frontal lobe. Fig. <ref type="figure" target="#fig_12">1</ref>.3 and 1.4 display the lateral and medial surfaces of the cerebral hemisphere respectively. which is delimited by the cingulate sulcus (calloso-marginal fissure). This convolution is wrapped around the deep part of the hemisphere. Above the limbic convolution, one can distinguish the frontal lobe. The medial surface of the occipital lobe is the cuneus, delimited by the groove parietaloccipital sulcus (internal perpendicular fissure) and the calcarine sulcus (fissure calcarine), which is the cortical projection of vision.</p><p>Different regions on the lower face of the hemispheres are shown in Fig. <ref type="figure" target="#fig_12">1</ref>.5. In the center of the underside of the brain between two hemispheres, there is the isthmus of the brain, which corresponds to the junction of the brain stem and the brain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.1.2.">Brain Gray and White Matter</head><p>The central nervous system contains two different major components: the grey matter and the white matter (as displayed in    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.2.">Brain Connectivity</head><p>As mentioned previously, the neurons in the brain act as information processing units and assembled circuits that perform brain functions, or in other words, they form a distributed network. Therefore, the study of the connectivity between different brain components is always a central issue in neuroscience, which aims to find links between Mathematically, a network is considered as a graph structure which represents some type of links/interactions between units. In the brain network, the word "unit" may correspond to an individual neuron, a neuronal population, or an anatomically segregated brain region <ref type="bibr">[Sporns 2007</ref>]. This graph includes a list of devices (or nodes) and a set of links, grouped in a connectivity matrix. The links can be oriented or not. If they are binary values, we get a pure structural graph. Now, the graph can be valued: each unit and each link may have a value (i.e. corresponding respectively to a state of activation and neuronal synaptic weight).</p><p>There are three types of brain connectivity <ref type="bibr" target="#b86">[Friston 1994</ref>]: structural connectivity (anatomical), functional connectivity (measuring statistical dependence between neuronal activations) and effective connectivity (measuring the causal interactions or information flows on a network). These three types of connectivity are obviously linked, and their joint analysis allows a better understanding of brain structures and functions [Alexandre 2013].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Human Brain</head><p>Chapter 1</p><p>In Fig. <ref type="figure" target="#fig_12">1</ref>.8, these three types of brain connectivity <ref type="bibr">[Sporns 2007</ref>] are displayed in different forms. In the lower half, there are the connectivity matrices, where (a) structural connectivity forms a sparse and directed matrix, (b) functional connectivity leads to a full symmetric matrix and (c) effective connectivity yields a full non-symmetric matrix.</p><p>All these matrices can be weighted, with weights representing connection densities or efficacies for structural connectivity, strength of statistical dependence or proximity between two elements (neurons, recording sites, voxels) of the system for functional connectivity, and quantity of information flow for effective connectivity. Applying a threshold to such matrices will yield binary directed graphs with the setting of the threshold controlling the degree of sparsity, where the binary elements indicating the presence or absence of a connection. In this thesis, we essentially focus on functional and effective connectivities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.2.1.">Structural Connectivity</head><p>Several studies <ref type="bibr" target="#b23">[Bassett 2006</ref><ref type="bibr">, He 2007</ref><ref type="bibr" target="#b4">, Alexandre 2013]</ref> on the structural network have already shown the existence of a small-world network, at several levels, with the presence of clustered areas and long connections that permit rapid communication between these areas. This type of organization allows a compromise between structural cost and </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3.">Problem Statement</head><p>As mentioned above, epilepsy is a neurological disease that affects approximately 1% of the population and is characterized by the repetition of seizures (called ictal periods)</p><p>whose frequency and duration are variable. Epilepsies remain drug-resistant in about 30% cases, which can require a surgical operation.</p><p>For a given patient, the key-point is to determine or confirm the accurate boundaries</p><formula xml:id="formula_0">1.3. Problem Statement</formula><p>Chapter 1 of the "epileptogenic zone" (EZ), which define the brain areas to be eventually surgically resected to achieve freedom from epileptic seizures, and make sure that, if these areas are partially or completely removed after surgery, the post-surgical deficits (such as vision or language) remain limited. However, such a surgery is challenging, because, in most cases, the epileptogenic zone, which is responsible for the initiation and the propagation of seizure activities, corresponds to a network of neuronal ensembles distributed in distant structures. Due to this distributed characteristic, the localization and characterization of the epileptogenic networks are difficult tasks but they are crucial to help in delimiting the cerebral volume to be resected to avoid epileptic activity.</p><p>The long-term objective is to help to understand the mechanisms of the seizure in order to cancel it or at least to stop it. This implies to address the following issues:</p><p>What are the structures that are involved in such activities? What is the effective connectivity between these structures? Is it possible to detect dominant structures in this epileptogenic network?</p><p>Compared to functional connectivity, which underlies the notion of coupling between different structures, the issue of effective connectivity involves the information of directionality and is more challenging. The simplest investigation is the one which detects and quantifies effective connectivity marginally for all oriented pairs of iEEG signals. A more accurate investigation is the one which tries to characterize the directed connectivity from one channel towards another one conditionally to the contextual information provided by the other iEEG channels.</p><p>This second approach can be implemented through multichannel causal modeling, for instance VAR modeling, if we limit the causality characterization to causal linear effects. Now, if it is necessary to take nonlinear effects into account, the conditional statistical causal analysis may have to face identifiability condition and computational load.</p><p>Chapter 2</p><p>State of the Art  <ref type="bibr">1991</ref><ref type="bibr" target="#b165">,McIntosh 1992</ref><ref type="bibr" target="#b99">,Grafton 1994</ref><ref type="bibr" target="#b166">,McIntosh 1994</ref><ref type="bibr" target="#b86">,Friston 1994</ref>]. So far, the term "effective connectivity" has been defined by various authors <ref type="bibr" target="#b114">[Horwitz 2003]</ref>, and usually, it is understood as a measure of the impact of one neural system on another, directly or indirectly <ref type="bibr" target="#b86">[Friston 1994</ref>]. Therefore, one important feature of effective connectivity is that it contains the information on the directionality of causal influence. More specifically, if an observation of the neuronal activity in one brain region allows us to get a better prediction on the neuronal activity in another brain region, then it is said that the former region has an influence on the latter <ref type="bibr" target="#b137">[Lang 2012</ref>]. Fig. <ref type="figure" target="#fig_13">2</ref>.1 illustrated the different possibilities of the connectivities among three neural populations <ref type="bibr" target="#b255">[Wendling 2000</ref>].</p><p>The new causality indexes proposed in this thesis correspond to (semi)nonparametric entropic methods. To position this work among other possible approaches, we give in section 2.2 a list of methods developed in more or less recent literature to address this topic.</p><p>temporal gyrus) structures. The position of each electrode is strategically de®ned from the analysis of surface EEG signals, clinical data and imaging in order to record structures that play a potential role during the seizures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">In¯uence of the excitation/inhibition ratio</head><p>A Gaussian white noise was used as the model input, p n (t). The mean and variance (corresponding to a rate of 30±150 pulses/s) were adjusted so that the model produced a signal similar to the spontaneous EEG recorded from neocortical structure electrodes during interictal periods when other parameters of the model are set to standard values (Table <ref type="table">1</ref>). This signal (Fig. <ref type="figure" target="#fig_24">3a</ref>) re¯ects a normal activity which resembles that re¯ected by real SEEG signals (Fig. <ref type="figure" target="#fig_24">3f</ref>). Starting from this point, all parameters were kept st constant in the excitatory loo increased. This increase (Fi that appear sporadically (from (A = 3.6) and ®nally rhy average frequency of this rh increases as A is increased (f Visual inspection of real si spikes (usually appearing bef rhythmic discharges of spik seizures start) can be encou (Fig. <ref type="figure" target="#fig_24">3g±i</ref>). For this type o that real waves are quite ac model. From A = 8.5, the m activity to sinusoidal activit lope also depends on the val activity may be re¯ected seizures (see the ®rst 5 s of described below). The excit also be adjusted to position between two types of activity transitions occur (Fig. <ref type="figure" target="#fig_52">4</ref>). presents a real SEEG signal r the beginning of a temporal SEEG signal shows high am faster activity (quasi-sinusoid slows down to resume spik average frequency. Figure <ref type="figure" target="#fig_61">4b</ref> this period of time. A spon quasi-sinusoidal activity and One notices that the simula tively as the real one; the s amplitude and higher frequ frequency, higher-amplitude</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">In¯uence of the coupling</head><p>A key advantage of the multi ability to produce signals r between underlying populati easily controlled via the str the direction of couplings (uni Figure <ref type="figure" target="#fig_64">5</ref> illustrates the ee simple example in which thr ered. The ®rst one was made ing its excitation/inhibition r other populations remained couplings between these thre signals correspond to norma sporadic spikes appearing population and only norma the two others. When param senting the unidirectional cou population 2 and the uni population 2 to popul (K 12 = 200, K 13 = 200), th from population 1 to popula simulated (Fig. <ref type="figure" target="#fig_66">5a</ref>). As expe delays, spikes in populations population 1. Now, introdu </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">State of the Art</head><p>The measurements of effective connectivity can be categorized into two classes: (i) datadriven methods, including The notion of "causality" has been discussed by many authors <ref type="bibr" target="#b86">[Friston 1994</ref><ref type="bibr" target="#b191">, Pearl 2009</ref><ref type="bibr" target="#b244">, Valdes-Sosa 2011</ref><ref type="bibr" target="#b204">,Roebroeck 2011a</ref><ref type="bibr" target="#b205">,Roebroeck 2011b</ref>], the most conventional meaning of the word "causal" could be "a cause occurs prior to its effect" [Amblard 2012].</p><p>The basic idea of Granger causality (GC) can be traced back to the work of Wiener <ref type="bibr" target="#b263">[Wiener 1956</ref>] and Granger <ref type="bibr" target="#b100">[Granger 1969</ref>]. It measures the statistical dependence between the past of a process and the present of another one. Precisely, given two time series X and Y , including the lagged value of X, if the lagged value of Y provides statistically significant information about the future value of X (through a series of t-test and F-test [Greene 2003]), we can say that Y Granger-causes X . This concept was later generalized by Geweke <ref type="bibr">[Geweke 1982</ref><ref type="bibr" target="#b95">, Geweke 1984]</ref>, and the index of causality from Y to X was defined as the logarithm of the ratio of the asymptotic mean square error when predicting the future value of X from its own past, to the asymptotic mean square error when making the same prediction from the lagged value of both X and Y .</p><p>Granger causality was developed originally in econometrics, and nowadays, it is one of the most popular measures to infer causal interactions between time series. Due to its simplicity and effectiveness, it has been widely used in neuroscience, including both fMRI <ref type="bibr" target="#b203">[Roebroeck 2005</ref><ref type="bibr" target="#b213">, Sato 2006</ref><ref type="bibr" target="#b59">, Deshpande 2009</ref><ref type="bibr" target="#b61">, Deshpande 2010b</ref><ref type="bibr" target="#b221">, Seth 2013]</ref> and EEG/MEG <ref type="bibr" target="#b109">[Hesse 2003</ref><ref type="bibr" target="#b97">, Gow 2008</ref><ref type="bibr" target="#b199">, Ploner 2009</ref><ref type="bibr" target="#b98">, Gow 2009</ref><ref type="bibr" target="#b2">, Adhikari 2013]</ref>. Besides neuroscience, it has also numerous applications in a variety of research areas including climate <ref type="bibr" target="#b124">[Kaufmann 1997</ref><ref type="bibr" target="#b239">, Triacca 2001</ref><ref type="bibr" target="#b177">, Mosedale 2006</ref><ref type="bibr" target="#b225">, Smirnov 2009]</ref>, engineering <ref type="bibr" target="#b129">[Kim 2012</ref><ref type="bibr" target="#b273">, Yuan 2014]</ref>, energy <ref type="bibr" target="#b188">[Pao 2011</ref><ref type="bibr" target="#b33">, Bozoklu 2013</ref><ref type="bibr" target="#b238">, Tiwari 2014]</ref>, economics <ref type="bibr" target="#b48">[Comincioli 1996</ref><ref type="bibr" target="#b101">, Granger 2000</ref><ref type="bibr">, McCracken 2007</ref><ref type="bibr" target="#b47">, Chiou 2008]</ref>, and others <ref type="bibr">[Seth 2007</ref>].</p><p>Since its introduction in 1969 <ref type="bibr" target="#b100">[Granger 1969</ref>], discussions on this causal measure never  <ref type="bibr" target="#b154">Lütkepohl 2005</ref>], thus it reflects only the linear features of the signals. Additionally, it also assumes that the analyzed signals are covariance stationary <ref type="bibr">[Seth 2007</ref>]. During the past several decades, Granger causality has been greatly developed and extended in numerous studies <ref type="bibr" target="#b194">[Pereda 2005</ref>], some of its extensions being recalled in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1.2.">Nonlinear Extensions of Granger Causality</head><p>In the standard application of WGC, only the linear features are captured whereas many Another important nonlinear extension of Granger's idea was proposed by Chen et al. <ref type="bibr">[Chen 2004</ref>] in 2004, and named extended Granger causality index (EGCI). The basic idea of EGCI is that, even in nonlinear systems, one can locally approximate the dynamics linearly, and so apply linear GC to each local neighborhood and get average statistical quantities that properly reflect the nonlinear dynamics. In the experiments presented in <ref type="bibr">[Chen 2004</ref>], the effectiveness of EGCI has been proved on artificial data.</p><p>The same year, Ancona et al. <ref type="bibr" target="#b7">[Ancona 2004</ref>] proposed a nonlinear extension of Granger causality for bivariate time series, further called nonlinear Granger causality (NLGC)</p><p>in <ref type="bibr" target="#b236">[Sun 2008</ref>] and <ref type="bibr" target="#b118">[Ishiguro 2008</ref>]. In NLGC, a nonlinear kernel autoregression scheme is employed, instead of a linear autoregression one. Specifically, Ancona et al. <ref type="bibr" target="#b7">[Ancona 2004</ref>] argued that not all nonlinear prediction schemes are suitable for the extension of linear GC. These schemes should follow the following property: given two time series</p><p>X and Y , if Y is statistically independent of X , then the variance of prediction error obtained when predict the future of X with the past information of both X and Y , should be equal to the one predicted with only the past information of X itself, and analogously for the opposite direction. It should be mentioned that, according to this property, the EGCI method proposed in [Chen 2004] satisfied the above property only if the number of points in the local neighborhood, where linear regression is performed, is sufficiently high to obtain reliable statistics, but this assertion is in contradiction with the basic idea of local linearization <ref type="bibr" target="#b7">[Ancona 2004</ref>].</p><p>In We conclude this subsection with a brief presentation of the h 2 index introduced in neuroscience by <ref type="bibr" target="#b198">[Pijn 1990</ref>] in the scope of Wiener-Granger causality. This index can be considered as a nonlinear correlation index. It extends the linear r 2 index defined as the square of the Pearson correlation coefficient between X.t/ and Y.t /, maximized with respect to . This index is also equal to one minus the ratio of the estimated linear mean squared prediction error (when predicting X.t/ from Y.t / where is the optimal lag) to the estimated variance of X.t/. If is constrained to be strictly positive, log 1 1=r 2 can be considered as a degenerate Granger causality index from Y to X where the past of X is reduced to an empty set and the past of Y to Y.t /, i.e. to only one past scalar value. Now, this scalar value is optimally selected among the values Y.t /, 0 &lt; Ä max , and so r 2 implicitly depends from all past values in an authorized set specified by max . Clearly, if is constrained to be strictly negative, then log 1 1=r 2 quantifies the reverse causality, from X to Y . To obtain the h 2 index, the mean square linear prediction error in the expression of r 2 is replaced by the mean square nonlinear regression error when estimating X.t/ from Y.t /, maximized by tuning . The regression function is estimated from the observed values of X and Y , and is constrained to be a piecewise affine function on a given partition (union of intervals) of the real line. Finally, when is selected among the authorized strictly positive values, h 2 can be interpreted as a degenerate nonlinear Wiener-Granger causality index. Compared to the Wiener-Granger approach, even if the past values of X are not considered in the h 2 index, the low dimensionality of the corresponding statistical inference algorithm can lead to a not too spread estimation of the regression error variances, in comparison to more complex nonlinear regression algorithms. This can partly explain its practical efficiency, for instance when it is used to analyze real epileptic signals as in <ref type="bibr" target="#b40">[Caparos 2006</ref><ref type="bibr">, Dorr 2007]</ref>.</p><p>This subsection was devoted to give an overview of nonlinear non-entropic methods that are not considered in the rest of the document.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2.">Spectral Methods</head><p>In the past few years, several frequency-domain causality detection methods have been introduced to analyze the directional connectivity in neural systems <ref type="bibr" target="#b46">[Chicharro 2011</ref><ref type="bibr" target="#b116">, Hu 2012]</ref>. With these spectral measures, it becomes possible to detect the frequency band(s)</p><p>where causality occurs <ref type="bibr" target="#b36">[Brovelli 2004</ref><ref type="bibr">, Bressler 2008</ref><ref type="bibr" target="#b136">, Ladroue 2009]</ref>, which is a priori interesting for iEEG signals analysis.</p><p>Most spectral methods of causality are related to the spectral form of Granger causality based on transfer functions derived from multivariate autoregressive modeling. As a matter of fact, a great advantage of Granger causality is that it could be decomposed in the frequency domain <ref type="bibr">[Geweke 1982</ref><ref type="bibr" target="#b95">, Geweke 1984]</ref>. Compared to its spectral form, this quantity in the time domain may be considered as an average over all frequencies (up to the Nyquist frequency). In other words, given two time series X and Y , the spectral Granger causality from Y to X at frequency ! quantifies the directed contribution of Y to the power of X at frequency ! [Chen 2006, <ref type="bibr" target="#b22">Barrett 2014</ref>].</p><p>To our best knowledge, the first spectral decomposition of linear GC was proposed by</p><p>Geweke <ref type="bibr">[Geweke 1982</ref><ref type="bibr" target="#b95">, Geweke 1984]</ref>, and then easily extended to other different forms:</p><p>(i) conditional spectral Granger causality, which calculates the causality contribution of Y to the power of X, in addition to the causality contribution of a third group of variables to the power of X [Chen 2006], (ii) multivariate form, which computes the causality contribution from one group of variables to another group <ref type="bibr" target="#b20">[Barrett 2010</ref>]. Since its conception, this spectral measure has generated interest among scientific researchers.</p><p>For example, Bernasconi and König have used this method to find neuronal interactions in the visual cortical areas of the cat <ref type="bibr" target="#b25">[Bernasconi 1999</ref><ref type="bibr" target="#b26">,Bernasconi 2000</ref>]. In 2004, Brovelli In some special cases, for instance cognitive experiments, the signals length may be too short <ref type="bibr">[Ding 2000</ref>] to provide good estimates, and DTF becomes ineffective with such data. To overcome this issue, the repetition of the task can improve the final estimate <ref type="bibr">[Ding 2000</ref><ref type="bibr" target="#b197">,Philiastides 2006]</ref>, and/or the introduction of an important overlapping of the processed windows can allow a better estimation, as is the case in short time DTF <ref type="bibr">[Ding 2000</ref>]. Using this technique, data are processed by highly overlapped time windows, were orthogonalized. This new measure was supposed to be less affected by the volume conduction and amplitude scaling, and evaluated on both simulated models and newborns' EEG signals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.3.">Model-based Methods</head><p>As mentioned above, Granger causality is data-driven and can be estimated without any a priori physiological hypothesis. On the contrary, model-based causal measures involve neural population models and take physiological evidences into account.</p><p>An important model-based approach was first proposed by Friston <ref type="bibr" target="#b87">[Friston 2003</ref>], and called dynamic causal modeling (DCM). The basic idea of DCM is to consider the brain as a dynamic input-state-output system with multiple inputs and outputs, and construct an explicit forward or generative model. In some situations, different dynamic causal models for a system of interest can be constructed, standing for different competing hypotheses about the mechanisms that generate the observed data. In this case, considering both model fitting and relative complexity, Bayesian model selection can be used to identify the most optimal DCM <ref type="bibr" target="#b192">[Penny 2004</ref>].</p><p>Shortly after its introduction by <ref type="bibr" target="#b87">Friston in 2003</ref><ref type="bibr" target="#b87">[Friston 2003</ref>], this nonlinear and dynamic method has obtained numerous attentions, and several improvements and ex- DCM employs a biophysical realistic model and focuses on how the observed data are generated, while GC examines for statistical dependencies between different physiological responses <ref type="bibr" target="#b88">[Friston 2009</ref>]. Compared with GC, the important number of parameters to estimate in DCM may limit its practical use <ref type="bibr">[Seth 2010</ref>]. Now, Friston et al. <ref type="bibr" target="#b89">[Friston 2014</ref>] demonstrated that the spectral Granger causality can be unreliable on noisy data, and also showed that this problem can be finessed by deriving spectral causality measures based on the parameters estimated using dynamic causal modeling. a priori information on data modeling and may capture both linear and nonlinear relations between time series <ref type="bibr" target="#b120">[Jin 2010</ref>], as illustrated in papers such as <ref type="bibr">[Vicente 2011</ref>]: these techniques are fully "model-free".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.4.">Information Theory Measurement</head><p>One well-known information-theoretic quantity is mutual information (MI) <ref type="bibr" target="#b51">[Cover 2012</ref>], which is widely used to quantify the amount of uncertainty shared between two random variables. Now, the drawback of this kind of measure is its symmetrical nature, so that it only reflects some unidirectional dependence. Thus, to detect the directed information flow between two variables, the time-lagged form of mutual information As an information-theoretic implementation of Wiener's principle of observational causality, TE is related to some other causality measures. As known, GC emphasizes the concept of "prediction", while TE is framed in terms of "resolution of uncertainty".</p><p>So, TE can be considered as a measurement of the degree to how Y disambiguates the future of X beyond the degree to how X disambiguates its own future <ref type="bibr" target="#b186">[Paluš 2001</ref>].</p><p>The work by <ref type="bibr" target="#b16">Barnett et al. [Barnett 2009</ref>] specified the relation of these two causality measures for the first time, and bridges the information-theoretic and autoregressive methods. Under Gaussian assumptions, TE and GC are entirely equivalent, up to a factor of 2. Therefore, Granger causality can be understood as a linear approximation of transfer entropy <ref type="bibr" target="#b112">[Hlinka 2013</ref>]. The Gaussian assumption seems to be strict for many   Chapter 2 Also, for practical applications, the authors argued that a conspicuous amount of phenomenology in the brain can be explained by linear models, and introduced the assumption of Gaussianity <ref type="bibr" target="#b16">[Barnett 2009</ref>] to obtain computational convenience. The application of this proposed expansion was illustrated to both simulated data and two real EEG data sets.</p><p>As pointed out in <ref type="bibr" target="#b235">[Sun 2014</ref>], transfer entropy often results in erroneous identification of network connections, especially for time-dependent networks. To break this limit, in 2014, Sun and Bollt <ref type="bibr" target="#b235">[Sun 2014</ref>] developed a measure called causation entropy (CSE) to obtain reliable identification of true couplings. CSE could be considered as a generalization of transfer entropy. It measures the extra information flow between two processes in addition to the information already provided by a third group of processes. Through numerical simulations, the authors highlighted the superiority of CSE over transfer entropy, where CSE successfully inferred the real causal relationships while TE gave misinterpretations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The common definition of transfer entropy involves infinite vectors [Schreiber 2000],</head><p>which leads to the calculation of infinite-dimensional densities. Also, despite its numerous advantages, transfer entropy has been mostly applied in a bivariate scene, since it is difficult to obtain reliable TE estimation in high dimensions due to the "curse of dimensionality". To overcome this limitation, by embedding TE into the framework of graphical models <ref type="bibr" target="#b54">[Dahlhaus 2000</ref><ref type="bibr" target="#b73">, Eichler 2012</ref>], Runge et al. <ref type="bibr" target="#b208">[Runge 2012</ref>] presented a formula that decomposes TE into a sum of finite-dimensional contributions, called decomposed transfer entropy (DTE). Compared with TE, DTE drastically reduces the estimation dimension which leads to a more reliable estimation, and the graphical model also enables a richer picture of causal interactions. In <ref type="bibr" target="#b208">[Runge 2012</ref>], the advantages of this approach were demonstrated on observational climate data (sea level pressure). Despite all these advantages, it remains a tough task to obtain accurate estimations of the information-theoretic measures while carried out on finite sample length signals, particularly in the field of neuroscience <ref type="bibr" target="#b194">[Pereda 2005</ref><ref type="bibr" target="#b110">, Hlaváčková-Schindler 2007]</ref>, where getting large amounts of stationary data is still problematic. This issue is largely discussed in the next chapter.</p><p>Chapter 3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods and Materials</head><p>As mentioned previously, the information-theoretic approaches, especially transfer entropy, play an important role in the detection of causality. A common problem is how to obtain an accurate estimation of these information-theoretic quantities, which has been proven to be difficult. More precisely, a well-known problem arises when estimating entropy is the difficulty to lower the estimation bias. This chapter presents previous published work and some proposed improvements concerning this problem, and it is arranged in three parts.</p><p>In section 3.1, we introduce the mathematical definition of different informationtheoretic quantities, and discuss some similarities in the calculation of mutual information and transfer entropy. Some important non-parametric approaches for the estimation of these quantities are reviewed in section 3.2. With these previous works, several questions are raised, and to answer them, we propose two different improvements of the existing approaches. In section 3.3, we introduce an analytical form of bias for the estimation of individual entropy, and then apply it into the estimation of both mutual information and transfer entropy, where the bias reduction strategies of relation-specific distance are proposed. These strategies vary with different norms, and it should be mentioned that, when the strategy with maximum norm is retained, our results well explain the conclusions drawn by <ref type="bibr" target="#b131">Kraskov et al. in [Kraskov 2004</ref>], which were only derived from numerical experiments. Additionally, to further decrease the bias estimation of mutual information between two dependent variables, a weighted linear combination of distinct mutual information estimators is introduced. In section 3.4, based on the idea proposed in <ref type="bibr">[Kraskov 2004</ref>], we deeply discuss the improvement in the estimation of informationtheoretic quantities using the maximum norm, when a (hyper-)rectangle is used instead of a (hyper-)cube. Following two different methodologies <ref type="bibr" target="#b130">[Kozachenko 1987</ref><ref type="bibr" target="#b224">, Singh 2003]</ref>,</p><p>we extend the existing kNN (k-Nearest Neighbors) entropy estimators to the rectangular situation, which results in two other estimators. Applying these new entropy estimators into the calculation of transfer entropy, we present two novel TE estimators. Finally, in section 3.5, we discuss some other issues while applying transfer entropy in the analysis of neural signals, such like order selection. For ease of readability, the links between the concepts and methodologies described in this chapter are summarized and illustrated in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Statement</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.">Introduction to Information-theoretic Quantities</head><p>In this section, we give mathematical descriptions of different information-theoretic quantities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.1.">Entropy, Joint Entropy and Conditional Entropy</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Entropy</head><p>Initially defined in statistical physics as being proportional to the logarithm of the number of possible configurations of a physical system (typically a system of molecules), the entropy formalized by Shannon <ref type="bibr" target="#b222">[Shannon 1948</ref>] measures the amount of "uncertainty" on what can be observed at the output of a random information source. In other words, it quantifies the amount of information needed in order to specify completely this output. Furthermore, other theoretical measures were proposed by Shannon, as conditional entropy and mutual information, called entropic measures, which extended the practical and conceptual usefulness of source entropy. Besides their applications in theoretical and applied information systems, entropic measures have been applied in various fields, like Given a d X -dimensional random vector X (i.e. a measurable function X W ! R d X defined on some underlaying probability space . ; ; P /). To define the entropy of X, we first assume that the image set X D X . / (the set of possible realizations of X in R d X ) is finite or enumerable, i.e. X D fx i ; i 2 I g where I is countable. Then, if we consider that Chapter 3</p><formula xml:id="formula_1">3.1. Problem Statement log 1 P .X Dx i / Á</formula><p>is the decrease of uncertainty (i.e. the information gain) resulting from the observation of the value x i , the entropy H dis .X/ associated to the random vector X is defined as the expectation (computed with respect to the probability distribution of X on X) of this information gain. If we denote the function x ! P .X D x/ on X by p X .x/, this expectation can be written</p><formula xml:id="formula_2">H dis .X / D E Ä log 1 p X .X/ D E OElog p X .X/ D X i 2I p X .x i / log p X .x i /; (3.1)</formula><p>where 0 log.0/ 0. Here, the units of H dis .X/ are "nats" when the natural logarithm is used, and "bits" for base 2 logarithm. The change of variable p X .x i / D p i leads to H dis .X / D P i 2I p i log p i and this underlines that the entropy does not depend on the particular values set X D fx i ; i 2 I g. It depends only on the probability distribution fp i ; i 2 I g on this set, so that the random variables X and Y D h.X/ admit the same entropy whenever h is an injective transformation. The particular case fp</p><formula xml:id="formula_3">i ; i 2 I g D n 1 2 K ; i D 1; : : : ; 2 K o , i.e.</formula><p>an uniform distribution on a finite set including 2 K elements, leads to 8i W log</p><formula xml:id="formula_4">1 P .X Dx i / Á</formula><p>D log 2 K and to an expected information gain equal to H dis .X / D K bits. Now, let us consider a random variable X continuously distributed on R d X , with a probability density function denoted by p X .x/, x 2 R d X . If we try to introduce a definition of its entropy which originates from the one adopted for a discrete distribution, a natural way could be (i) introduce a discretely distributed random variable X " dis such that sup !2 X .!/ X " dis .!/ sup Ä " which approximate X better if we decrease ", (ii) impose a small " value to have a small maximal difference between</p><formula xml:id="formula_5">X "</formula><p>dis and X and (iii) define the entropy of X, approximately, by that of X " dis . To follow this idea, let us introduce a partition</p><formula xml:id="formula_6">fc i ; i 2 Ng of R d X (i.e. S i 2N c i D R d X , i ¤ j ) c i \c j D ∅</formula><p>) such that sup i 2N fdiameter .c i / ; i 2 Ng Ä " and choose for each i an arbitrary point x i in c i . For example we can retain for fc i ; i 2 Ng a coverage of R d X obtained from a lattice with the same period " along each Cartesian axis in R d X (i.e. such that the sets c i are disjoint (hyper-)cubes, half open along each direction with an edge length equal to "). Then X " dis can be defined univocally as a function of X by imposing X " dis .!/ D x i if and only if X .!/ 2 c i , where each x i is arbitrary chosen in c i . This leads to</p><formula xml:id="formula_7">H dis .X " dis / D X i 2N P X " dis D x i log P X " dis D x i D X i 2N P .X 2 c i / log .P .X 2 c i // : (3.2)</formula><p>The discrete probability distribution of the discrete random variable X " dis taking its values in the countable set fx i ; i 2 Ng can be approximated by</p><formula xml:id="formula_8">˚P X " dis D x i ' p X .x i /v .c i / ; i 2 N « (3.3)</formula><p>when " is small. So, it seems natural to approximate H dis .X " dis / as follows:</p><formula xml:id="formula_9">H dis .X " dis / ' X i 2N p X .x i /v .c i / log .p X .x i /v .c i // D X i 2N p X .x i / log .p X .x i // v .c i / X i 2N p X .x i / log .v .c i // v .c i / ;</formula><p>(3.4)</p><p>where v .c i / D ."/ d X is the Lebesgue measure of c i . Now, considering the two sums in the second line of Equ. (3.4), when the supremum value of the v .c i / is sufficiently small, we have: </p><formula xml:id="formula_10">X i 2N p X .x i / log .p X .x i // v .c i / ' Z x2R d X p X .x/ log .p X .x// v .dx/ (3.5) and X i2N p X .x i / log .v .c i // v .c i / ' log .v .c i // Z x2R d X p X .x/v .</formula><formula xml:id="formula_11">H cnt .X/ D E OElog .p X .X// : (3.8)</formula><p>This definition is used currently when the observed data are modelized as continuously distributed in probability, and will be generally retained in the sequel. It seems strange to neglect d X log ". But, In fact, the unbounded increase of d X log " (when " decreases) can be interpreted as the necessary increase of information to localize more precisely a point in R d X and can be considered as to be non-relevant to quantify the uncertainty inherent to the shape of the probability distribution of X on R d X . Moreover, if one wants to compare the uncertainty associated with two random variables X 1 and X 2 , each of them continuously distributed on R d X , we have no clear argument leading to choose two distinct values of ", " 1 and " 2 . So, we can consider that it is sufficient to compute the differential entropies to make this comparison. Finally, considering Equ. (3.1) and</p><p>(3.8), a same formula can be retained, H.X/ D E OElog p X .X/, where H.X/ corresponds to H dis .X/ or H cnt .X/ and p X . / corresponds respectively to a discrete distribution or to a density distribution. When the context is clear, this common symbolization will not introduce any confusion. But in the continuous distribution case, it must be clear that H.X/ may be negative contrary to the discrete case. Another important difference between the discrete and continuous cases is the influence of an invertible transformation It is interesting to introduce the following integral representation of entropy (which will be useful later)</p><formula xml:id="formula_12">h W R d X ! R d X on</formula><formula xml:id="formula_13">H.X/ D E OElog p X .X/ D Z R d X log Â dP X d r .x/ Ã dP X .x/; (3.9)</formula><p>where the sum is defined with respect to the probability measure P X (on the Borel sets of R d X ) induced by the random vector X W ! R d X , and which can be discrete or continuous. Here the derivative of the measure P X with respect to the reference measure r , dP X d r .x/, represents the discrete probability distribution or the probability density function (in the discrete case, this reference measure is the uniform counting measure supported by the countable set X . / D fx i ; i 2 I g, and in the continuous case it corresponds to the Lebesgue measure on R d X ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Joint Entropy</head><p>The entropy of a pair .  Mutual information is tightly related to the Kullback-Leibler divergence <ref type="bibr" target="#b138">[Latham 2009</ref><ref type="bibr" target="#b51">, Cover 2012]</ref>, which is a non-symmetrical measure of the dissimilarity between two distributions. Given two probability measures P and Q on the borel sets of R d , the Kullback-Leibler divergence of P with respect to Q is defined as  <ref type="bibr">(3.21)</ref>, mutual information can be written as the Kullback-Leibler divergence of the joint measure P X;Y with respect to the measure equal to the tensorial product P X ˝PY :</p><formula xml:id="formula_14">X W ! R d X ; Y W ! R d Y / of</formula><formula xml:id="formula_15">D kl .P jjQ/ D Z R d log Â dP dQ .x/ Ã dP .x/: (3.21) ( , ) X Y ( | ) X Y ( | ) Y X ( ) X ( ) Y ( | ) Y X ( , ) X Y ( | ) X Y Figure 3</formula><formula xml:id="formula_16">I.X; Y / D D kl .P X;Y jjP X ˝PY /: (3.22)</formula><p>Thus, mutual information can be considered as a measure of how close the joint distribution of .X; Y / is to the product of the marginal distributions of X and Y . Additionally, the concept of mutual information can also be extended to multivariate situation. For a set of random variables fX 1 ; X 2 ; : : : ; X M g, I.X 1 ; X 2 ; : : : ; X M / quantifies the information shared among these variables. For M &gt; 1, we have the following relation I.X 1 ; X 2 ; : : : ; X M / D I.X 1 ; X 2 ; : : : ; X M 1 / I.X 1 ; X 2 ; : : : ; X M 1 jX M /:</p><formula xml:id="formula_17">( | , ) Z X Y ( | , ) X Y Z ( | , ) Y X Z ( , | ) Y Z X ( , | ) X Z Y ( , | ) X Y Z ( , , ) X Y Z ( | , ) Z X Y ( , | ) X Z Y ( , | ) Y Z X ( , , ) X Y Z ( | , ) X Y Z ( , | ) X Y Z ( | , ) Y X Z ( ) Z ( ) Y ( ) X</formula><p>(3.24)</p><p>The relations between the different conditional information-theoretic quantities are illustrated in Fig. <ref type="figure" target="#fig_24">3</ref>.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1.3.">Transfer Entropy</head><p>Transfer Entropy (TE) is an information-theoretic statistical measurement, which aims at measuring the amount of time-directed information between two dynamical systems. As described in the previous chapter, given the past time evolution of a dynamical system A, the transfer entropy from another dynamical system B to the first system A is the amount of Shannon uncertainty reduction in the future time evolution of A when including the knowledge of the past evolution of B.</p><p>More precisely, let us suppose that we observe the time sampled output X i 2 R, i 2 Z, of some sensors connected to A. If the sequence X is supposed to be a mth order Markov process, i.e. if considering subsequences X</p><p>.k/ i D .X i kC1 ; X i kC2 ; ; X i /, k &gt; 0, the probability measure P X (defined on measurable subsets of real sequences) attached to X fulfills the mth order Markov hypothesis</p><formula xml:id="formula_18">8i W 8m 0 &gt; m W dP X iC1 jX .m/ i x i C1 jx .m/ i Á D dP X i C1 jX .m 0 / i x i C1 jx .m 0 / i Á ; x i C1 2 R; x .k/ i 2 R k ; (3.25)</formula><p>then the past information X</p><p>.m/ i</p><p>(before time instant i C 1) is sufficient for a prediction of X i Ck , k 1, and can be considered as a m-dimensional state vector at time i (note that, to know from X the hidden dynamical evolution of A, we need a one-to-one relation between X</p><p>.m/ i</p><p>and the physical state of A at time i ). For sake of clarity, we introduce the following notation: X p i ; X i ; Y i , i D 1; 2; : : : ; N , is an independent and identically distributed (IID) random sequence, each term following the same distribution as a random vector .X p ; X ; Y / 2 R 1CmCn whatever i (in X p , X , Y , the superscripts "p" and " " correspond to "predicted" and "past" respectively). This notation substitutes for Chapter 3 the notation X i C1 ; X .m/ i ; Y</p><p>.n/ i Á , i D 1; 2; : : : ; N and we denote by S X p ;X ;Y , S X p ;X , S X ;Y and S X the spaces in which .X p ; X ; Y /, .X p ; X /, .X ; Y / and X are respectively observed. Now, let us suppose that a causal influence exists from B to A and that an auxiliary random process Y i 2 R, i 2 Z, recorded from a sensor connected to B is such that, at each time i and for some n &gt; 0, Y i Y</p><p>.n/ i is an image (not necessarily one-to-one) of the physical state of B. The negation of this causal influence implies</p><formula xml:id="formula_19">8n &gt; 0 W 8i W dP X iC1 jX .m/ i x i C1 jx .m/ i Á D dP X iC1 jX .m/ i ;Y .n/ i x i C1 jx .m/ i ; y .n/ i Á : (3.26)</formula><p>If Equ. (3.26) holds, it is said that there is an absence of information transfer from B to A. Otherwise the process X can be no more considered strictly as a Markov process.</p><p>Let us suppose the joint process .X; Y / is Markovian, i.e. there exist a given pair .m 0 ; n 0 /, a transition function f , and an independent random sequence e i , i 2 Z, such that</p><formula xml:id="formula_20">OEX i C1 ; Y i C1 T D f X .m0/ i ; Y .n 0 / i ; e i C1 Á ; (3.27)</formula><p>where the random variable e i C1 is independent of the past random sequence X j ; Y j ; e j , j Ä i , whatever i . As X i D g X </p><formula xml:id="formula_21">i ; Y i TE Y !X;i D E h D kl P X p i jX i ;Y i jX i ; Y i jjP X p i jX i jX i Ái D Z S X ;Y g x i ; y i dP X i ;Y i x i ; y i ; (3.28)</formula><p>which can be finally written</p><formula xml:id="formula_22">TE Y !X;i D Z S X p ;X ;Y log dP X p i jX i ;Y i jx i ; y i dP X p i jX i jx i x p i ! dP X p i ;X i ;Y i x p i ; x i ; y i ; (3.29)</formula><p>where the ratio in Equ. (3.29) corresponds to the Radon-Nikodym derivative <ref type="bibr" target="#b206">[Roman 1974</ref>] (i.e. the density) of the conditional measure dP X p i jX i ;Y i jx i ; y i with respect to the conditional measure dP X p i jX i jx i . Now, given two observable scalar random time series X and Y with no a priori given model (as it is generally the case), if we are interested in defining some causal influence from Y to X through TE analysis, we must specify the dimensions of the past information vectors X and Y , i.e. m and n. Even if we impose them, it is not evident that all the coordinates in X .m/ i and Y</p><p>.n/ i will be useful. To deal with this issue, variable selection procedures have been proposed in the literature such as uniform and non-uniform embedding algorithms <ref type="bibr" target="#b133">[Kugiumtzis 2013</ref><ref type="bibr" target="#b174">, Montalto 2014]</ref>.</p><p>Here, we consider that the joint probability measure P X p i ;X i ;Y i is absolutely continuous (with respect to the Lebesgue measure in R mCnC1 denoted by mCnC1 ) with the corresponding probability density function</p><formula xml:id="formula_23">p X p i ;X i ;Y i x p i ; x i ; y i D dP X p i ;X i ;Y i x p i ; x i ; y i d nCmC1 x p i ; x i ; y i : (3.30)</formula><p>Then, we are sure that the following conditional densities probability functions exist:</p><formula xml:id="formula_24">8 &lt; : p X p i jX i x p i jx i D dP X p i jX i x p i jx i d 1 x p i p X p i jX i ;Y i x p i jx i ; y i D dP X p i jX i ;Y i x p i jx i ; y i d 1 x p i ; (3.31) and Equ. (3.29) yields to TE Y !X;i D Z R mCnC1 p X p ;X i ;Y i x p i ; x i ; y i log p X p jX i ;Y i x p i jx i ; y i p X p jX i x p i jx i ! dx p i dx i dy i D Z R mCnC1 p X p ;X i ;Y i x p i ; x i ; y i log p X p ;X i ;Y i x p i ; x i ; y i p X i x i p X i ;Y i x i ; y i p X p ;X i x p i ; x i ! dx p i dx i dy i : (3.32) Equ. (3.32) can be rewritten TE Y !X;i D E h log p X p i ;X i X p i ; X i Ái E h log p X i ;Y i X i ; Y i Ái C E h log p X p i ;X i ;Y i X p i ; X i ; Y i Ái C E h log p X i X i Ái ; (3.33) or TE Y !X;i D H X p i ; X i C H X i ; Y i H X p i ; X i ; Y i H X i ; (3.34)</formula><p>where H .U / denotes the Shannon differential entropy of a random vector U . Considering "log" as the natural logarithm, TE Y !X;i is measured in natural units (nats). Note that, if the processes Y and X are assumed to be jointly stationary, for any real function  </p><formula xml:id="formula_25">g W R mCnC1 ! R,</formula><formula xml:id="formula_26">GC Y !X D log 0 @ var lpe X p i jX i Á var lpe X p i jX i ;Y i Á 1 A ; (3.36)</formula><p>which is independent of i under the stationary hypothesis and where lpe X p i jU is the error when predicting linearly X p i from U . TE is framed in terms of reduction of the Shannon uncertainty (entropy) of the predictive probability distribution. When the probability distribution of X p i ; X i ; Y i is assumed to be Gaussian, TE and Granger causality are entirely equivalent, up to a factor of 2 <ref type="bibr" target="#b16">[Barnett 2009</ref>]:</p><formula xml:id="formula_27">TE Y !X D 1 2 GC Y !X : (3.37)</formula><p>Equ. (3.37) can be used as a reference for the comparison between different TE estimators. Consequently, in the Gaussian case, TE can be easily computed from a statistical second-order characterization of X p i ; X i ; Y i . This Gaussian assumption obviously holds when the processes Y and X are jointly normally distributed and, more particularly, when they correspond to a Gaussian autoregressive (AR) bivariate process.</p><p>In <ref type="bibr" target="#b16">[Barnett 2009</ref>] Barnett et al. discussed the relation between these two causality measures and this work bridged information-theoretic methods and autoregressive ones.</p><p>TE is used as a pairwise causality detection measure, however, as mentioned in chapter 2, this kind of pairwise approach is not able to distinguish the direct and indirect relations in multivariate systems. Similar as Granger causality, TE could also be ex- </p><formula xml:id="formula_28">TE Y !XjZ;i D Z R mCnCqC1 log dP X p i jX i ;Y i ;Z i x p i jx i ; y i ; z i dP X p i jX i ;Z i x p i jx i ; z i ! dP X p i ;X i ;Y i ;Z i x p i ; x i ; y i ; z i ; (3.38)</formula><p>where Z stands for the past of Z with order q.</p><p>Similarly as for transfer entropy, CTE could also be presented by means of conditional mutual information TE Y !XjZ D I X p ; Y jX ; Z ;</p><p>(3.39) and Equ. (3.39) could be rewritten in terms of joint and marginal entropies</p><formula xml:id="formula_29">TE Y !X jZ;i D H X i ; Y i ; Z i C H X p i ; X i ; Z i H X p i ; X i ; Y i ; Z i H X i ; Z i : (3.40)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">The Estimator Structures for MI and TE</head><p>Let us consider the estimation 3</p><formula xml:id="formula_30">TE Y !X of TE Y !X</formula><p>as a function defined on the set of observable occurrences .x i ; y i /, i D 1; : : : ; N , of a stationary sequence .X i ; Y i /, i D 1; : : : ; N .</p><p>From Equ. (3.33), assuming that X and Y are jointly strongly ergodic leads to</p><formula xml:id="formula_31">TE Y !X D lim N !1 1 N N X i D1 log p X i ;Y i X i ; Y i Á log p X p i ;X i X p i ; X i Á C log p X p i ;X i ;Y i X p i ; X i ; Y i Á C log p X i X i Á ! ; (3.41)</formula><p>where the convergence holds with probability one. Hence, a standard estimation 3 </p><formula xml:id="formula_32">TE Y !X of TE Y !X is given by 3 TE Y !X D 5 H .X ; Y / C 5 H X p ; X 8 H X p ; X ; Y 2 H .X / D 1 N N X nD1 7 log p U 1 .u 1n / 1 N N X nD1 7 log p U 2 .u 2n / C 1 N N X nD1 7 log p U 3 .u 3n / C 1 N N X nD1 7 log p U 4 .</formula><formula xml:id="formula_33">N independent trials is 3 I .X; Y / D 1 N N X nD1 6 log .p X .x n // 1 N N X nD1 6 log .p Y .y n // C 1 N N X nD1 b log p X;Y .</formula><p>x n ; y n / :</p><p>(3.44)</p><p>To conclude this section, we reviewed the mathematical definitions of different information-theoretic quantities, and discussed the similarities of the estimator structures of MI and TE. For practical use, it is important to find accurate estimators for these quantities. Deriving a good estimator includes two aspects: unbiasness and accuracy, which are reflected in estimation bias and variance. These issues are largely discussed in the literature. However, the estimation remains a tough task while carried out on finite sample length signals, for example in the field of neuroscience, where getting large amounts of stationary data is problematical <ref type="bibr" target="#b110">[Hlaváčková-Schindler 2007]</ref>. Moreover, this estimation suffers from the "curse of dimensionality" <ref type="bibr" target="#b249">[Verleysen 2005</ref>], especially for transfer entropy. In the following section, we focus on the estimation of these quantities,</p><p>and give an overview of some important existing approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Previous Works</head><p>For the estimation of information-theoretic quantities, there are two kinds of approaches, parametric and non-parametric <ref type="bibr" target="#b247">[Venelli 2010</ref>]. The parametric approaches make extra assumptions on the data to be processed. For example, they can be assumed to be issued from a known family of distributions, such as normal distribution, and the derived estimator is optimized based on this assumption. In contrast, for the non-parametric estimations, no a priori assumption is considered, and the estimators depend on the data themselves. In the remainder of this section, only efficient non-parametric estimation methods are described.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">Estimation of Entropy</head><p>Coming back to the definition of entropy in Equ. (3.1), given a random variable X with samples x i , i D 1; : : : ; N , the estimation of H.X/, 1 H.X/, can be calculated as</p><formula xml:id="formula_34">1 H.X/ D 1 N N X i D1 log 1 p.x i /; (3.45)</formula><p>which requires 1 p.x i / is the estimation of the probability density function p. / at data point x i .</p><p>To solve this problem, for scalar observations, the most straightforward approach is the histogram-based method. Let us consider for example the one-dimensional case.</p><p>The real axis is first partitioned into M bins corresponding to M equal length intervals a j D OEs C .j 1/ ; s C j with j D 1; : : : ; M , where s is the starting value of the first interval a 1 , and is the length of the intervals (bins). If the number of samples falling into interval a j is denoted by n j , and the two following conditions are satisfied and the probability p X .x/ is calculated as</p><formula xml:id="formula_35">1 p X .x/ D X j n j N 1 a j .x/; x 2 R; (3.48)</formula><p>where 1 E . / is the indicator function of the set E.</p><p>In this case, the summation in Equ. (3.45) should be rewritten, such as</p><formula xml:id="formula_36">1 H.X/ his D M X j D1 n j N log n j N ÁÁ ; (3.49)</formula><p>which corresponds, when N is large enough and small enough to get n j N ' p X .x/, x 2 a j , to a Riemann approximation of the integral R x2R p X .x/ log .p X .x// dx D H.X/.</p><p>For the histogram-based method, the choice of the interval length (related to the number of bins M ) is critical. Even if this method is efficient in a computational point of view, it only gives approximate estimation.</p><p>Beyond the histogram-based method, we can go further with density estimation.</p><p>Given a random variable X, drawn from the unknown density function p X . /, the probability that a new sample of X falls into a region L.x/ around point x is given by  So, the number k of points x i included in a (hyper-)cube centered on x and with edge length equal to R is given by</p><formula xml:id="formula_37">P .X 2 L.x// D Z L.x/ p X .</formula><formula xml:id="formula_38">k D N X i D1 H x x i R Á : (3.55)</formula><p>Finally, we obtain the following kernel density estimator</p><formula xml:id="formula_39">1 p X .x/ kde D 1 N N X i D1 1 V H x x i R Á ; (3.56)</formula><p>where V is the volume of the region L.x/. For this kind of density estimator, kernel functions H. / other than the one used in Equ. (3.54) can be chosen. A common choice is the Gaussian kernel function, and the corresponding density estimator is</p><formula xml:id="formula_40">1 p X .x/ kde D 1 N N X i D1 1 .2 R 2 / d 2 exp .x x i / 2 2R 2 ! : (3.57)</formula><p>So, with 1 p X .x/ obtained with Equ. (3.56), 1</p><p>H.X/ can be calculated with Equ. (3.45). However, for the KDE method, the choice of the fixed width parameter R is important.</p><p>If R is too large, the estimated density is over-smoothed, and, if it is too small, the estimation suffers from too much statistical variability. In Fig. <ref type="figure" target="#fig_24">3</ref>.4, three different values of R are given to highlight their influence on the results. Now, we consider the kNN approach. Using the kth NN to determine the region</p><formula xml:id="formula_41">L.x/, an unbiased estimator of p X .x/ is given by [Fukunaga 2013] b p.x/ knn D k 1 N V ; (3.58)</formula><p>where V is the volume of the neighborhood L.x/. Using this kNN density estimator, we write the entropy estimator as</p><formula xml:id="formula_42">1 H.X / knn D 1 N N X i D1 log k 1 N v i ; (3.59)</formula><p>where v i is the volume of the region L. estimators based on the kNN technique have been introduced, as shown below.</p><p>Note that these two estimators can be implemented whatever the norm. Now, in this section, when we refer to Fig. <ref type="figure" target="#fig_24">3</ref>.6, we only consider the maximum norm, for which the region of interest is a cube (Box 1 ○ in Fig. <ref type="figure" target="#fig_24">3</ref>.6).</p><p>The first kNN entropy estimator is the Kozachenko-Leonenko entropy estimator [Leo-</p><formula xml:id="formula_43">nenko 2008] (Box 2 ○ in Fig 3.6), 1 H.X/ kl D .N / C 1 N N X i D1 log .v i / .k/; (3.60)</formula><p>where v i is the volume of the smallest ball centered on x i which includes the k NNs of x i and .k/ D 0 .k/ .k/ denotes the digamma function. The second one has been derived by <ref type="bibr" target="#b224">Singh et al. in [Singh 2003</ref>] and is denoted by</p><formula xml:id="formula_44">1 H.X / sg hereafter (Box 3 ○ in Fig. 3.6), 1 H.X / sg D log.N / C 1 N N X i D1 log .v i / .k/; (3.61)</formula><p>where v i and .k/ share the same definition as in Equ. (3.60). Note that, using Equ.</p><p>(3.60) or (3.61), and for a given chosen norm, the only parameters to be fixed are the number of observations N and the number of neighbors, k. Then, for a given N and a given probability distribution, the choice of k determines all the estimation statistical properties, as bias and variance. Actually, Equ. (3.60) and (3.61) give quite similar results in practical use as, for large N , we have log.N / .N /. These two kNN entropy estimators (especially the one expressed using Equ. (3.60)) have become popular and have been largely adopted in the estimation of both mutual information and transfer entropy, these two quantities being deeper discussed in the rest of this chapter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">Estimation of Mutual Information</head><p>As mentioned previously, mutual information can be computed as a combination of joint and marginal entropies. Let .X; Y / be a pair of multidimensional random variables with a continuous distribution specified by a joint probability density p X;Y with marginal densities p X and p Y . Considering the entropy estimator in Equ. (3.45), the mutual information I.X; Y / can be estimated from a sequence of independent realizations .x i ; y i / of .X; Y / as</p><formula xml:id="formula_45">2 I.X; Y / D 1 H.X / C 1 H.Y / 3 H.X; Y / D 1 N N X i D1 log 2 p X .x i / 1 N N X i D1 log 2 p Y .y i / C 1 N N X i D1 log 5 p X;Y .x i ; y i / D 1 N N X i D1 log 5 p X;Y .x i ; y i / 2 p X .x i / 2 p Y .y i / : (3.62)</formula><p>In order to calculate the estimator expressed by Equ. (3.62), we can first calculate three individual densities 2 p X .x i /, 2 p Y .y i /, and 5 p X;Y .x i ; y i / separately by using the KDE approach introduced previously. However, KDE remains a hard problem (for instance, the tuning of the kernel) <ref type="bibr" target="#b238">[Suzuki 2008</ref>] and could lead to unreliable estimators of the entropic quantities. It is not considered in the scope of this work.</p><p>Another possible solution is to calculate 1 H.X/, 1 H.Y / and 3 H.X; Y / separately, using the kNN entropy estimators described in Equ. (3.60) or (3.61). In this way, it is possible to adapt the neighborhood determination strategy for each of the three estimators in order to cancel out (more or less) the bias errors in individual estimations and to avoid an adverse accumulation of errors. To this end, Kraskov et al. </p><formula xml:id="formula_46">3 I .X; Y / k1 D .k/ h .n X C 1/ C .n Y C 1/i C .N /; (3.63)</formula><p>where N is the signal length, k is the fixed number of neighbors in S X;Y , . / denotes the digamma function, the symbol h i stands for an averaging on the sample data set, n X and n Y are the numbers of points which fall into the S X and S Y balls which share a same radius equal to the radius of the S X;Y ball.</p><p>In 2 , (b) " x &gt; " y there is no point on the border of the interval c y C " x 2 ; c y C " x 2 .</p><p>In <ref type="bibr">[Kraskov 2004</ref>], the effectiveness of this strategy to reduce bias is attested through numerical experiments. This strategy has also been extended to the calculation of other In <ref type="bibr">[Kraskov 2004</ref>], the following interesting conjecture has been raised from simulation results: E h 3</p><formula xml:id="formula_47">I .X; Y / k1 i D E h 3 I .X; Y / k2 i D 0; iif I .X; Y / D 0: (3.65)</formula><p>In section 3.3, we propose to give some theoretical explanations to justify this result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3.">Estimation of Transfer Entropy</head><p>For the estimation of transfer entropy, using the same notations as in Equ. (3.32), and similar with Equ. (3.62), the estimator of transfer entropy can be written as</p><formula xml:id="formula_48">3 TE Y !X D 5 H .X ; Y / C 5 H X p ; X 8 H X p ; X ; Y 2 H .X / D 1 N N X i D1 2 log p X ;Y .x i ; y i / 1 N N X i D1 2 log p X p ;X .x p i ; x i / C 1 N N X i D1 7 log p X p ;X ;Y .x p i ; x i ; y i / C 1 N N X i D1 7 log p X .x i / D 1 N N X i D1 log 4 p X p ;X ;Y .x p i ; x i ; y i / 3 p X .x i / 8 p X ;Y .x i ; y i / 8 p X p ;X .x p i ; x i / ;</formula><p>(3.66)</p><p>where .x p i ; x i ; y i / is an observation (realization) of .X p ; X ; Y /. As previously, it is possible to calculate the marginal and joint density probabilities in Equ. (3.66) using KDE approach, and then estimate TE. This method has been adopted by some authors <ref type="bibr" target="#b209">[Sabesan 2007</ref><ref type="bibr" target="#b272">, Yang 2013]</ref>. In <ref type="bibr" target="#b280">[Zuo 2013</ref> 3</p><formula xml:id="formula_49">TE Y !X k1 D .k/ C 1 N N X i D1</formula><p>.n X ;i C 1/ .n .X ;Y /;i C 1/ .n .X p ;X /;i C 1/ ;</p><p>(3.67)</p><p>where n X ;i , n .X ;Y /;i and n .X p ;X /;i denote the number of points which fall into the distance " i from x i , x i ; y i and x p i ; x i in the lower-dimensional spaces S X , S X ;Y and S X p ;X , respectively. An implementation of this TE estimator is available in the TRENTOOL toolbox [Wollstadt 2015], version 3.0. Another kNN TE estimator is derived from Equ. (3.64) and written as (Box 9  ○ in Fig. <ref type="figure" target="#fig_24">3</ref>.6)</p><p>3 </p><formula xml:id="formula_50">TE Y !X k2 D 1 N N X i D1 Â .k/ 2 k C .n X ;i / .n .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4.">Discussion</head><p>In this section, different methods for the estimation of information-theoretic quantities have been reported. For the calculation of MI and TE, the most popular approaches are the kNN related methods, and these estimators use similar strategies to reduce bias.</p><p>Hereafter, we give a short summary on these strategies before raising some fundamental questions. As discussed in section 3.1.2, the estimators of MI and TE present comparable structures, so that we consider here only mutual information but the same reasoning applies to transfer entropy.</p><p>Given Of course, in this case, it is impossible to support completely the first idea, because, for a (hyper-)rectangle, the side lengths for different dimensions may be different.</p><formula xml:id="formula_51">2 I.X; Y / D 1 H.X/</formula><p>However, there are still several questions to be solved with these strategies.</p><p>Firstly, for the point (1) mentioned above (marked as a red arrow in Fig. <ref type="figure" target="#fig_24">3</ref>.6), there are three questions to answer. Firstly, the effectiveness of the strategy proposed in <ref type="bibr">[Kraskov 2004</ref>] is verified only with numerical experiments. There is currently a lack of theoretical explanation and a deeper analysis of the bias in the entropy estimation is required. For the moment, the most popular bias analysis approach is dedicated to the Edgeworth expansion <ref type="bibr" target="#b245">[Van Hulle 2005]</ref>, and it could also be used in the estimation of mutual information. However, this method is not suitable for the investigation mentioned here: (a) this method uses the Gaussian distribution and some additional correction terms These three questions are covered in section 3.3 (see the blue dotted box in Fig. <ref type="figure" target="#fig_24">3</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">First Improvement</head><p>This section<ref type="foot" target="#foot_1">foot_1</ref> deals with the control of estimation bias when estimating mutual information or transfer entropy from non-parametric approach. We focus on continuously distributed random data and the estimators we developed are based on non-parametric kNN approach for arbitrary metrics. Using a multidimensional Taylor series expansion, a general relationship between the estimation error bias and neighboring size for plug-in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1.">New Bias Expression for the Plug-in Entropy Estimator</head><p>In the following development, we consider a d X -dimensional random variable X whose outcomes are in R d X and with a probability distribution specified by the probability density function p X .x/. L.x/ standing for a small region around x in R d From observations X i (random variables) issued from P X , the corresponding differential entropy H.X / can be estimated as</p><formula xml:id="formula_52">1 H.X / D 1 N N X i D1 2 log p X .X i /; (3.77)</formula><p>where N is the number of data used in the averaging. Then, we approximate the probability density p X .y/ using a second-order Taylor approximation around x,</p><formula xml:id="formula_53">p X .y/ p X .x/ C Â @p X @x .x/ Ã T .y x/ C 1 2 .y x/ T Â @ 2 p X @x 2 .x/ Ã .y x/; (3.78)</formula><p>with the superscript T standing for matrix transposition, and analyze the bias of 1 </p><formula xml:id="formula_54">H.X/ with 1 H.X / D 1 N N X i D1 log c p X .X i / D 1 N N X i D1 log R L.X i / p X .</formula><formula xml:id="formula_55">R L.x/ p X .y/dy v.x/ p X .x/ C 1 2v.x/ Z L.x/ .y x/ T Â @ 2 p X @x 2 .x/ Ã .y x/dy D p X .x/ C 1 2v.x/ tr 0 B @ Z L.x/</formula><p>.y x/.y x/ T dy @ 2 p X @x 2 .x/ 1 C A :</p><p>(3.82)</p><p>Finally, the estimator 2 log p X .x/ of log p X .x/ is approximated by log</p><formula xml:id="formula_56">R L.x/ p X .y/dy v.x/ C " ! log 0 B @ p X .x/ C 1 2v.x/ tr 0 B @ 0 B @ Z L.x/ .y x/.y x/ T dy 1 C A Â @ 2 p X @x 2 .x/ Ã 1 C A C " 1 C A log p X .x/ C 1 p X .x/ 1 2v.x/ tr 0 B @ 0 B @ Z L.x/ .y x/.y x/ T dy 1 C A Â @ 2 p X @x 2 .x/ Ã 1 C A " ƒ‚ … B X C 1 p X .x/ "; (3.83)</formula><p>where the term</p><formula xml:id="formula_57">1 p X .x/ "</formula><p>Á is zero mean.</p><p>The bias B X in 1</p><p>H.X / is approximated by the second term in the right-hand side of Equ. (3.83) and used as a correcting term. To build L.x/ which admits x as a center of symmetry, we retain two norms, the Euclidean norm (k k D k k E ) and the maximum norm (k k D k k M ):</p><p>L.x/ D fy W ky xk Ä R.x/g :</p><p>(3.84)</p><p>The resulting domain corresponds respectively to a standard ball and to a d X dimensional cube (a cubic ball). Consequently, the value R.x/ fixes respectively the radius of the standard ball or the half of the edge length of the cube.</p><p>Note that R L.x/ .y x/ .y x/ T dy is a diagonal matrix, which can be expressed as T I , where I is the identity matrix and T is a scalar independent of x. Therefore, we can move T out of the tr. / function</p><formula xml:id="formula_58">B X 1 p X .x/ 1 2v.x/ tr 0 B @ 0 B @ Z L.x/ .y x/.y x/ T dy 1 C A Â @ 2 p X @x 2 .x/ Ã 1 C A D 1 p X .x/ 1 2v.x/ tr Â .T I / Â @ 2 p X @x 2 .x/ ÃÃ D Â T 2v.x/ Ã 1 p X .x/ tr Â @ 2 p X @x 2 .x/ Ã : (3.85)</formula><p>Now, we derive an expression for T 2v.x/ in Equ. (3.85). Assume a conventional quadratic distance function such as R 2 .x; y/ D .y x/ T A.y x/:</p><p>(3.86)</p><p>Using d X -dimensional spherical coordinates, we have (Equ. ( <ref type="formula">14</ref>) in [Fukunaga 1973])</p><formula xml:id="formula_59">Z L.x/ .y x/.y x/ T dy D 1 .d X C 2/ 2 d X Â d X C 2 2 Ã v 1C 2 d X .x/jAj 1 d X A 1 ; (3.87)</formula><p>where . / is the Gamma function, and </p><formula xml:id="formula_60">v.x/ D Z L.x/ dx D d X 2 R d X .x/ jAj 1 2 d X C2 2 Á : (3.</formula><formula xml:id="formula_61">B X .x/ R 2 .x/ 2.d X C 2/ 1 p X .x/ tr Â @ 2 p X @x 2 .x/ Ã : (3.92)</formula><p>Now, let us consider the maximum norm, for which the region L.x/ is a d -dimensional (hyper-)cube with side length 2R.x/. Also, due to the symmetry of the region, we have So, using the maximum norm distance, the bias B X can be approximated as (Box 10 ○ in Fig. <ref type="figure" target="#fig_24">3</ref>.6)</p><formula xml:id="formula_62">1 2v.x/ Z L.x/ .y x/.y x/ T dy D 1 2.2R.x// d X u 3 3 ˇR.x/ R.x/ ! .2R.x// d X 1 I D R.x</formula><formula xml:id="formula_63">B X .x/ R 2 .x/ 6 1 p X .x/ tr Â @ 2 p X @x 2 .x/ Ã : (3.95)</formula><p>Note that, with the second-order approximation, the bias B X increases with larger R.x/ whatever the norm.</p><p>Until now, no particular form of density estimator was specified in our bias analysis.</p><p>In <ref type="bibr">[Kraskov 2004</ref>], the Kozachenko-Leonenko estimator <ref type="bibr" target="#b130">[Kozachenko 1987</ref>] (Equ. (3.60))</p><p>does not use explicitly an estimation of the densities for each sample point. However, since .N / log.N / for large N , Equ. (3.60) has the following structure</p><formula xml:id="formula_64">1 H.U / D 1 N N X i D1 log e .k/ N v i ! ; (3.96)</formula><p>Chapter 3</p><p>where the term inside the brackets can be interpreted as a density probability estimation.</p><p>Using </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2.">Bias Reduction of MI/TE Estimators Based on the New Bias Expression</head><p>Coming back to the estimation of MI, we can try to decrease its bias by subtracting the bias term highlighted in Equ. (3.83):</p><formula xml:id="formula_65">2 I.X; Y / D 1 N N X i D1 Â log c p X .x i / C log c p Y .y i / log 1 p X;Y .x i ; y i / B X .x i / C B Y .y i / B X;Y .x i ; y i / Ã : (3.98)</formula><p>If X and Y are independent, we obtain for each i : tr @ 2 p X;Y @.x;y/ 2 .x i ; y</p><formula xml:id="formula_66">i / Á p X;Y .x i ; y i / D tr @ 2 p X @x 2 .x i / Á p X .x i / C tr @ 2 p Y @y 2 .y i / Á p Y .y i / : (3.99)</formula><p>In this case, we impose relationship-specific distances for different entropy estimations in Equ. (3.69) to cancel out the bias, i.e.</p><formula xml:id="formula_67">B X .x i / C B Y .y i / B X;Y .x i ; y i / D 0: (3.100)</formula><p>With the help of Equ. (3.99), for the Euclidean norm, the left side of Equ. (3.100) can be transformed into </p><formula xml:id="formula_68">B X .x i / C B Y .y i / B X;Y .x i ; y i / D Â R 2 .x i / 2.d X C 2/ R 2 .x i ; y i / 2.d X;Y C 2/ Ã tr @ 2 p X @x 2 .x i / Á p X .x i / C Â R 2 .y i / 2.d Y C 2/ R 2 .x i ; y i / 2.d X;Y C 2/ Ã tr @ 2 p Y @</formula><formula xml:id="formula_69">&lt; : R 2 .x i / 2.d X C 2/ D R 2 .x i ; y i / 2.d X;Y C 2/ R 2 .y i / 2.d Y C 2/ D R 2 .x i ; y i / 2.d X;Y C 2/ :</formula><p>(3.102)</p><p>Finally, Equ. (3.102) yields to (Box 12 ○ in Fig. <ref type="figure" target="#fig_24">3</ref>.6)</p><formula xml:id="formula_70">8 &lt; : R.x i / D s d X C 2 d X;Y C 2 R.x i ; y i / R.y i / D s d Y C 2 d X;Y C 2 R.</formula><p>x i ; y i /:</p><p>(3.103)</p><p>Similarly, using the maximum norm, we obtain (Box 13 ○ in Fig. <ref type="figure" target="#fig_24">3</ref> if X and Y are independent, using the maximum norm and constraining the values R.x i / and R.y i / to be equal to R.x i ; y i / allows to decrease the bias 2 I.X; Y / I.X; Y /. Equ. (3.103) extends this result when the Euclidean norm is used for the 3 individual spaces. We should mention that Equ. (3.99) no longer holds if signals X and Y are not independent. In this case, only a part of the bias can be expected to be cancelled out.</p><p>So, finally, in the case of independence between X and Y , we introduced the following MI estimator 2 </p><formula xml:id="formula_71">I.X; Y / D 1 N N X i D1 Â log c p X .x i / C log c p Y .y i /</formula><formula xml:id="formula_72">I .X; Y / k basic D 1 H.X / basic C 1 H.Y / basic 3 H.X; Y / basic D 1 N N X i D1 Â log k.x i / 1 N v.x i / C log k.y i / 1 N v.y i / log k.x i ; y i / 1 N v.x i ; y i / Ã ; (3.106)</formula><p>where k.x i ; y i / D k (Box 14 ○ in Fig. <ref type="figure" target="#fig_24">3</ref>.6). Hereafter, this estimator is written as (3.34) can be rewritten as</p><formula xml:id="formula_73">3 TE Y !X D 1 N N X i D1 log 2 p X i ;Y i .x i ; y i / C log 2 p X p i ;X i .x p i ; x i / log b p X i .x i / log 5 p X p i ;X i ;Y i .x p i ; x i ; y i / B X i ;Y i .x i ; y i / C B X p i ;X i .x p i ; x i / B X p i ;X i ;Y i .x p i ; x i ; y i / B X i .x i / Á ! :</formula><p>(3.107)</p><p>In the same way as for mutual information, we impose relationship-specific distances for different entropy estimations in Equ. (3.107) to cancel out the bias, and obtain the following relation</p><formula xml:id="formula_74">B X i ;Y i .x i ; y i / C B X p i ;X i .x p i ; x i / B X p i ;X i ;Y i .x p i ; x i ; y i / B X i .x i / D 0: (3.108)</formula><p>If the three random variables X p i , X i , Y i are mutually independent, after calculation, for the Euclidean norm, we have </p><formula xml:id="formula_75">8 &lt; : R 2 .x i ; y i / 2.d X i ;Y i C 2/ D R 2 .x p i ; x i ; y i / 2.d X p i ;X i ;Y i C 2/ R 2 .x p i ; x i / 2.d X p i ;X i C 2/ D R 2 .x p i ; x i ; y i / 2.d X p i ;X i ;Y i C 2/ R 2 .x i / 2.d X i C 2/ D R 2 .x p i ; x i ; y i / 2.d X p i ;X i ;Y i C 2/ ; (3</formula><formula xml:id="formula_76">i / D v u u t d X i ;Y i C 2 d X p i ;X i ;Y i C 2 R.x p i ; x i ; y i / R.x p i ; x i / D v u u t d X p i ;X i C 2 d X p i ;X i ;Y i C 2 R.x p i ; x i ; y i / R.x i / D v u u t d X i C 2 d X p i ;X i ;Y i C 2 R.x p i ; x i ; y i /:</formula><p>(3.110)</p><p>For the maximum norm, the relation becomes (Box 13 ○ in Fig. <ref type="figure" target="#fig_24">3</ref>.6) , where the distances are obtained in the joint space with the highest dimension, S X p ;X ;Y , and then projected into the other spaces, S X ;Y , S X p ;X and S X . Now we must highlight here that the independence condition between the variables X p i , X i , Y i , which is equivalent to the following relations set (the two first conditions in Equ. (3.112) being redundant as being implied by the third one) 8 &lt; :</p><formula xml:id="formula_77">8 &lt; : R.x i ; y i / D R.x p i ; x i ; y i / R.x p i ; x i / D R.x p i ; x i ; y i / R.x i / D R.x</formula><formula xml:id="formula_78">p X i ;Y i .x i ; y i / D p X i .x i /p Y i .y i / p X p i ;X i .x p i ; x i / D p X p i .x p i /p X i .x i / p X p i ;X i ;Y i .x p i ; x i ; y i / D p X p i .x p i /p X i .x i /p Y i .y i / (3.112)</formula><p>is difficult to justify in a context of causality analysis. Indeed this independence condition implies than the TE value is equal to zero but is not necessary to obtain this zero value. 3</p><formula xml:id="formula_79">TE Y !X k basic D 5 H.X ; Y / basic C 5 H.X p ; X / basic 8 H.X p ; X ; Y / basic 2 H.X / basic D 1 N N X i D1 log k.x i ; y i / 1 N v.x i ; y i / C log k.x p i ; x i / 1 N v.x p i ; x i / log k.x p i ; x i ; y i / 1 N v.x p i ; x i ; y i / log k.x i / 1 N v.x i / ! ; (3.113)</formula><p>where k.x Consequently, we introduce the following form of an ensemble estimator of entropy: Considering each bias term, we write</p><formula xml:id="formula_80">1 H.X / D 1 N N X i D1 .1 ˛i / log c p X .1/ .x i / Á ! C 1 N N X i D1 ˛i log c p X .2</formula><formula xml:id="formula_81">B X .x i / .1 ˛i /B k 1 .x i / C ˛i B k 2 .x i /: (3.118)</formula><p>The question arises of how to choose ˛i in Equ. (3.118) so that B X .x i / D 0. Given the Euclidean norm (Equ. (3.92)), we have</p><formula xml:id="formula_82">B X .x i / D .1 ˛i /B k 1 .x i / C ˛i B k 2 .x i / D .1 ˛i /R 2 k 1 .x i / C ˛i R 2 k 2 .x i / 2 .d X C 2/ p X .x i / tr Â @ 2 p X @x 2 .x i / Ã : (3.119)</formula><p>Now, zeroing out Equ. (3.119) for any i D 1; : : : ; N with respect to ˛i leads to  In the independence case, the basic strategy can be used. But we note that the values </p><formula xml:id="formula_83">˛i D R 2 k 1 .x i / R 2 k 1 .x i /</formula><formula xml:id="formula_84">H.U / k u 1 ;k u 2 ens D 1 N N X i D1 Â .1 ˛u i / log k u 1 N v k 1 .u i / C ˛u i log k u 2 N v k 2 .u i / Ã ;<label>(</label></formula><formula xml:id="formula_85">˛u i D R 2 k 1 .u i / R 2 k 1 .u i / R 2 k 2 .u i / (3.</formula><formula xml:id="formula_86">1 H.U / k 1 ;k 2 mixed D 1 N N X i D1 Â .1 ˛i / log k k 1 .u i / n v k 1 .u i / C ˛i log k k 2 .u i / n v k 2 .u i / Ã : (3.124)</formula><p>Then, we get a mixed mutual information estimator (Box 18 ○ in Fig. <ref type="figure" target="#fig_24">3</ref>.6)</p><formula xml:id="formula_87">3 I .X; Y / k 1 ;k 2 mixed D 1 H.X/ k 1 ;k 2 mixed C 1 H.Y / k 1 ;k 2 mixed 3 H.X; Y / k 1 ;k 2 mixed : (3.125)</formula><p>In summary, this mixed MI estimator is built following the three steps:</p><p>(i) Fix the number of NNs (k 1 and k 2 separately) in the joint space S X;Y to get the distances between the center point .x i ; y i / and the particular NNs (k 1 th NN and Similarly, if we consider the estimation of TE, we obtain the following mixed estimator 3</p><formula xml:id="formula_88">k 2 th NN), marked as R k 1 .x i ; y i / and R k 2 .x i ; y i / (ii) Use R k 1 .x i ; y i / and R k 2 .x i ; y i / to get respectively R k 1 .x i /, R k 1 .y i /, and R k 2 .x i /, R k 2 .y i /,</formula><formula xml:id="formula_89">TE Y !X k 1 ;k 2 mixed D 5 H .X ; Y / k 1 ;k 2 mixed C 5 H X p ; X k 1 ;k 2 mixed 8 H X p ; X ; Y k 1 ;k 2 mixed 2 H .X / k 1 ;k 2 mixed ;</formula><p>(3.126)</p><p>where 1</p><formula xml:id="formula_90">H.U / k 1 ;k 2 mixed is defined in Equ. (3.124).</formula><p>For the mixed MI/TE estimators, an optimal choice of the parameters k 1 and k 2 is not obvious. In practice, it is possible to tune these two parameters to improve the estimation, but the empirical choice for the parameters remains an issue which explains why we only considered this kind of ensemble estimator in the estimation of mutual information and not in the estimation of transfer entropy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Second Improvement</head><p>In this section<ref type="foot" target="#foot_3">foot_3</ref> , we first give an overview of the original kNN strategies on the estimation of entropy, including the Kozachenko-Leonenko entropy estimator <ref type="bibr" target="#b130">[Kozachenko 1987</ref>] </p><formula xml:id="formula_91">(Box 2 ○ in</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1.">Original k-Nearest Neighbors Strategies</head><p>In this section, we consider a sequence x i , i D 1; : : : ; N in R d X (in our context this sequence corresponds to an outcome of an IID sequence X 1 ; : : : ; X N such that the common probability distribution is equal to that of a given random vector X). The set of the k NNs of x i in this sequence (except for x i ) and the distance between x i and its kth NN are respectively denoted by k i and d x i ;k . We denote</p><formula xml:id="formula_92">D x i k i Á R d X a neighborhood of x i in R d X which is the image of x i ; k i Á</formula><p>by a set valued map. For a given norm k k on R d X a standard construction</p><formula xml:id="formula_93">x i ; k i Á 2 R d X Á kC1 ! D x i k i Á R d X (3.127)</formula><p>is the (hyper-)ball of radius equal to d x i ;k , i.e.</p><formula xml:id="formula_94">D x i k i Á D ˚x W kx x i k Ä d x i ;k « : (3.128)</formula><p>The (hyper-)volume (i.e. the Lebesgue measure) of</p><formula xml:id="formula_95">D x i k i Á is then v i D Z D x i . k i / dx; (3.129)</formula><p>where dx d d X .x/.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.1.1.">Kozachenko-Leonenko Entropy Estimator</head><p>Recalling the kNN entropy estimator defined in Equ. (3.60), to come up with a concise presentation of this estimator, we give hereafter a summary of the different steps to get it starting from <ref type="bibr">[Kraskov 2004</ref>]. First, let us consider the distance d x i ;k between x i and its kth NN (introduced above) as a realization of the random variable D x i ;k and let us denote by q x i ;k .x/, x 2 R, the corresponding probability density function (conditioned by X i D x i ). Secondly, let us consider the quantity</p><formula xml:id="formula_96">h x i ."/ D Z ku x i kÄ " 2 dP X .u/: (3.130)</formula><p>This is the probability mass of the (hyper-)ball with radius equal to " 2 and centered on x i . This probability mass is approximately equal to</p><formula xml:id="formula_97">h x i ."/ ' p X .x i / Z k kÄ " 2 d d . / D p X .x i / c d " d ; (3.131)</formula><p>if the density function is approximately constant on the (hyper-)ball. The variable c d is the volume of the unity radius d -dimensional (hyper-)ball in R d (c d D 1 with the maximum norm). Furthermore, it can be established (see <ref type="bibr">[Kraskov 2004</ref>] for details) that the expectation E log h X i D X i ;k , where h X i is the random variable associated to h x i , D X i ;k (which must not be confused with the notation</p><formula xml:id="formula_98">D x i k i Á introduced previously)</formula><p>denotes the random distance between the kth neighbor selected in the set of random vectors fX k ; 1 Ä k Ä N; k ¤ i g and the random point X i , is equal to .k/ .N / and does not depend on p X . /. Equating it with E log p X .X i / c d D X i ;k allows to write</p><formula xml:id="formula_99">.k/ .N / ' E OElog .p X .X i // C E h log c d D d X i ;k Ái D H.X i / C E OElog .V i / ;</formula><p>(3.132)</p><p>or, equivalently</p><formula xml:id="formula_100">H .X i / ' .N / .k/ C E h log c d D d X i ;k Ái : (3.133)</formula><p>Finally, by using the law of large numbers, when N is large we have</p><formula xml:id="formula_101">H .X i / ' .N / .k/ C 1 N N X i D1 log .v i / D 1 H .X/ kl ; (3.134)</formula><p>where v i is the realization of the random (hyper-)volume <ref type="bibr">Moreover, as observed in [Kraskov 2004]</ref>, it is possible to make the number of neighbors k depend on i by substituting the mean (3.60). This estimator was derived in <ref type="bibr" target="#b224">[Singh 2003</ref>] through the four following steps:</p><formula xml:id="formula_102">V i D c d D d x i ;k .</formula><p>(1) Introduce the classical entropy estimator structure</p><formula xml:id="formula_103">1 H.X/ 1 N N X i D1 log 2 p X .X i / D 1 N N X i D1 T i ; (3.136)</formula><p>where</p><formula xml:id="formula_104">2 p X .x i / k N v i : (3.137)</formula><p>(2) Assuming that the random variables T i , i D 1; : : : ; N are identically distributed so that E h 1 H.X / i D E OET 1 (note that E OET 1 depends on N , even if the notation does not make that explicit), compute the asymptotic value of E OET 1 (when N is large) by firstly computing its asymptotic cumulative probability distribution function and the corresponding probability density p T 1 , and finally compute the following expectation</p><formula xml:id="formula_105">E OET 1 D Z R t p T 1 .t/dt: (3.138) (3) It appears that E OET 1 D E h 1 H.X/ i D H.X/ C B; (3.139)</formula><p>where B is a constant which is identified with the bias.</p><p>(4) Subtract this bias from 1</p><formula xml:id="formula_106">H.X / to get 1 H.X/ sg D 1 H.X/ B (3.140)</formula><p>and the formula given in Equ. (3.61).</p><p>Note that the cancellation of the asymptotic bias does not imply that the bias obtained with a finite value of N is also exactly cancelled. Now, we explain the origin of the bias for the entropy estimator given in Equ. (3.136). Let us consider the equalities</p><formula xml:id="formula_107">E OET 1 D E h log 3 p X .X 1 / Ái D E Ä log Â k N V 1 Ã ; (3.141)</formula><p>where V 1 is the random volume for which v 1 is an outcome. Conditionally to</p><formula xml:id="formula_108">X 1 D x 1 , if we have k N V 1 pr ! N !1 p X .x 1 / ; (3.142)</formula><p>where pr denotes the convergence in probability, we could expect that</p><formula xml:id="formula_109">E OET 1 jX 1 D x 1 ! N !1 log .p X .x 1 // (3.143)</formula><p>and, by deconditioning, that</p><formula xml:id="formula_110">E OET 1 ! N !1 E OElog .p X .X 1 // D H.X/: (3.144) So, the convergence k N V 1 pr ! N !1 p X .x 1 / (3.145)</formula><p>could lead to an asymptotically unbiased estimation of H.X/. Now, this convergence in probability does not hold, even if we assume the following convergence of the mean</p><formula xml:id="formula_111">E Ä k N V 1 jX 1 D x 1 ! N !1 p X .x 1 / ; (3.146) because we do not have var Â k N V 1 jX 1 D x 1 Ã ! N !1 0: (3.147)</formula><p>The ratio</p><formula xml:id="formula_112">k N V 1 Á remains fluctuating when N ! 1, because the ratio p var.V 1 / EOEV 1 Á</formula><p>does not tend to zero even if V 1 tends to be smaller: when N increases, the neighborhoods become smaller and smaller but continue to "fluctuate". This explains informally (see <ref type="bibr">[Zhu 2014</ref>] for a more detailed analysis) why the naive estimator given by Equ. (3.136) is not asymptotically unbiased. It is interesting to note that the Kozachenko-Leonenko entropy estimator avoids this problem and so it does not need any bias subtraction asymptotically.</p><p>Observe also that, as for the Kozachenko-Leonenko estimator, it is possible to adapt Equ. (3.135) if we want to consider a number of neighbors k i depending on i . Equ.</p><p>(3.61) can then be replaced by</p><formula xml:id="formula_113">1 H.X/ sg D log.N / C 1 N N X i D1</formula><p>.log .v i / .k i //:</p><p>(3.148)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.2.">From Square to Rectangular Neighboring Region for Entropy Estimation</head><p>In .n Y;i C 1/ in Equ. (3.63). As a matter of fact, for one of these terms, no point x i (or y i ) falls exactly on the border of the (hyper-)cube D x i (or D y i ) obtained by the distance projection from the S X;Y space. As clearly illustrated in Fig. <ref type="figure" target="#fig_24">3</ref>.7 (rectangle A 0 B 0 C 0 D 0 in Fig. <ref type="figure" target="#fig_24">3</ref>.7(a) and 3.7(b)), the second strategy prevents from that issue since the border of the (hyper-)cube (in this case an interval of R) after projection from S X;Y space to S X space (or S Y space) contains one point. When the dimensions of S X and S Y are larger than one, this strategy leads to build a (hyper-)rectangle equal to the product of two (hyper-)cubes, one of them in S X and the other one in S Y . If the maximum distance of the kth NN in S X;Y is obtained in one of the directions in S X , this maximum distance, after multiplying by two, fixes the size of the (hyper-)cube in S X . To obtain the size of the second (hyper-)cube (in S Y ), the k neighbors in S X;Y are first projected on S Y and then the largest of the distances calculated from these projections fixes the size of this second (hyper-)cube. i , are represented. The 5th NN is symbolized by a star. The neighboring regions ABCD, obtained from the maximum norm around the center point, are squares, with equal edge lengths " x D " y . Reducing one of the edge lengths, " x or " y , until one point falls onto the border (in the present case, in the vertical direction), leads to the minimum size rectangle A 0 B 0 C 0 D 0 , where " x ¤ " 0 y . Two cases must be considered, illustrated respectively in Fig. <ref type="figure" target="#fig_24">3</ref>.7(a) and 3.7(b). For case (a) the 5th NN is not localized on an intersection of two edges, contrary to the case (b). This leads to obtain either two points (respectively the star and the triangle in Fig. <ref type="figure" target="#fig_24">3</ref>.7(a)) or only one point (the star in Fig. <ref type="figure" target="#fig_24">3</ref>.7(b)) on the border of A 0 B 0 C 0 D 0 . Clearly it is theoretically possible to have more than 2 points on the border of A 0 B 0 C 0 D 0 but the probability of such an occurrence is equal to zero when the probability distribution of the random points X j is continuous.</p><formula xml:id="formula_114">y   x  y x    A B C D B  C  A  D  (a) y   x  y x    A B C D B  C  A  D  (b)</formula><p>In the remainder of this section, for an arbitrary dimension d , we propose to apply this strategy to estimate the entropy of a single multidimensional variable X observed in R d . This leads to introduce a d -dimensional (hyper-)rectangle centered on x i having a minimal volume and including the set k i of neighbors. Hence the rectangular neighboring is built by adjusting its size separately in each direction in the space S X . Using this strategy, we are sure that, in any of the d directions, there is at least one point on one of the two borders (and only one with probability one). Therefore, in this approach the (hyper-)rectangle, denoted by D " 1 ;:::;" d x i</p><p>, where the sizes " 1 ; : : : ; " d in the respective d directions are completely specified from the neighbors set k i , is substituted for the basic (hyper-)square</p><formula xml:id="formula_115">D x i k i Á D ˚x W kx x i k Ä d x i ;k « : (3.149)</formula><p>It should be mentioned that the central symmetry of the (hyper-)rectangle around the center point allows for reducing the bias in the density estimation <ref type="bibr" target="#b92">[Fukunaga 1973</ref>] (cf.</p><p>Equ. (3.131) or (3.137)). Note that, when k &lt; d , there must exist neighbors positioned on some vertices or edges of the (hyper-)rectangle. With k &lt; d it is impossible that, for any direction, one point falls exactly inside a face (i.e. not on its border). For example with k D 1 and d &gt; 1 the first neighbor is on a vertex and the sizes of the edges of the reduced (hyper-)rectangle are equal to twice the absolute value of its coordinates, whatever the direction.</p><p>Hereafter, we propose to extend the entropy estimators by Kozachenko-Leonenko and Singh using the above strategy before deriving the corresponding TE estimators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.3.">Extension of the Kozachenko-Leonenko Method</head><p>As indicated before, in <ref type="bibr">[Kraskov 2004</ref>], Kraskov et al. extended the Kozachenko-Leonenko estimator (Equ. (3.60)) using the rectangular neighboring strategy to derive MI estimator. Now, focusing on entropy estimation, we can obtain another estimator of H.X/, denoted by 1</p><formula xml:id="formula_116">H.X/ kl2 (Box 21 ○ in Fig. 3.6), 1 H.X/ kl2 D .N / C 1 N N X i D1 log .v i / .k/ C d 1 k ; (3.150)</formula><p>where v i is the volume of the minimum volume (hyper-)rectangle around the point x i .</p><p>Hereafter we give the mathematical development to get Equ. (3.150). As illustrated in Fig. <ref type="figure" target="#fig_24">3</ref>.7, for d D 2 there are two cases to be distinguished: (i) " x and " y are determined by the same point, (ii) " x and " y are determined by distinct points.</p><p>Considering the probability density q i;k x ; y , x ; y 2 R 2 of the pair of random sizes " x ; " y (along x and y respectively), we can extend it to the case d &gt; 2. Hence let us denote by q d x i ;k ." 1 ; : : : ; " d /, ." 1 ; : : : ; " d / 2 R d the probability density (conditional to X i D x i ) of the d -dimensional random vector whose d components are respectively the d random sizes of the (hyper-)rectangle built from the random k NNs and denote by</p><formula xml:id="formula_117">h x i ." 1 ; : : : ; " d / D Z u2D " 1 ;:::;" d x i dP X .u/ (3.151)</formula><p>the probability mass (conditional to X i D x i ) of the random (hyper-)rectangle D 1 ;:::;</p><formula xml:id="formula_118">d x i .</formula><p>Chapter 3</p><p>In </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.4.">Extension of Singh's Method</head><p>In this section, we propose to extend Singh's entropy estimator by using a (hyper-)rectangular domain as we did for the Kozachenko-Leonenko estimator extension introduced in the preceding section. Considering a d -dimensional random vector X 2 R d continuously distributed according to a probability density function p X , we aim at estimating the entropy H.X / from the observation of a p X -distributed IID random sequence X i , i D 1; : : : ; N . For any specific data point x i and a fixed number k (1 Ä k Ä N ), the minimum (hyper-)rectangle (rectangle A 0 B 0 C 0 D 0 in Fig. <ref type="figure" target="#fig_24">3</ref>.7 is fixed, we denote this region by D " 1 ;:::;" d x i</p><p>, and its volume by v i . Let us denote i (1 Ä i Ä min.k; d /) the number of points on the border of the (hyper-)rectangle that we consider as a realization of a random variable " i . In the situation described in Fig. <ref type="figure" target="#fig_24">3</ref>.7(a) and 3.7(b), i D 2 and i D 1 respectively. According to [Fukunaga 2013] (chapter 6, page 269), if</p><formula xml:id="formula_119">D x i k i</formula><p>Á corresponds to a ball (for a given norm) of volume v i , an unbiased estimator of p X .x i / is given by</p><formula xml:id="formula_120">2 p X .x i / D k 1 N v i ; i D 1; 2; : : : ; N: (3.156)</formula><p>This implies that the classical estimator 2</p><formula xml:id="formula_121">p X .x i / D k N v i is biased and that presumably log k N v i Á is also a biased estimation of log .p X .x i // for N large as shown in [Fukunaga 2013]. Now, in case D x i k i Á is the minimal (i.e.</formula><p>with minimal (hyper-)volume) (hyper-)rectangle D " 1 ;:::;" d</p><p>x i including k i , more than one point can belong to the border, and a more general estimator B p X .x i / of p X .x i / can be a priori considered</p><formula xml:id="formula_122">B p X .x i / D Q k i N v i ; (3.157)</formula><p>where Q k i is some given function of k and i . The corresponding estimation of H.X/ is then 1</p><formula xml:id="formula_123">H.X / D 1 N N X i D1 E log .p X .x i // D 1 N N X i D1 t i ;</formula><p>(3.158) with </p><formula xml:id="formula_124">t i D log Â N v i Q k i Ã ; i</formula><formula xml:id="formula_125">: ; N W E h 1 H.X/ i D E OET i D E OET 1 : (3.160) Our goal is to derive E h 1 H.X / i H.X/ D E OET 1 H.X/ (3.161)</formula><p>for N large to correct the asymptotic bias of 1 H.X/, according to steps (1) to (3) explained in section 3.4.1.2. To this end, we must consider an asymptotic approximation of the conditional probability distribution P .T 1 Ä rjX 1 D x 1 ; " 1 D 1 / before computing the asymptotic difference between the expectation E OET 1 D E OEE OET 1 jX 1 D x 1 ; " 1 D 1 and the true entropy H.X/.</p><p>Let us consider the random Lebesgue measure V 1 of the random minimal (hyper-)rectangle D 1 ;:::; d x 1 (. 1 ; : : : ; d / denotes the random vector for which ." 1 ; : : : ; " d / 2 R d is a realization) and the relation</p><formula xml:id="formula_126">T 1 D log Â N V 1 e K 1 Ã : (3.162)</formula><p>Chapter 3</p><p>For any r &gt; 0, we have</p><formula xml:id="formula_127">P .T 1 &gt; rjX 1 D x 1 ; " 1 D 1 / DP Â log Â N V 1 e K 1 Ã &gt; rjX 1 D x 1 ; " 1 D 1 Ã DP .V 1 &gt; v r jX 1 D x 1 ; " 1 D 1 / ; (3.163) where v r D e r Q k 1 N since, conditionally to " 1 D 1 , we have e K 1 D Q k 1 .</formula><p>Property 1 : For N large,</p><formula xml:id="formula_128">P .T 1 &gt; rjX 1 D x 1 ; " 1 D 1 / ' k 1 X i D0 N 1 1 i ! .p X .x 1 /v r / i .1 p X .x 1 /v r / N 1 1 i :</formula><p>(3.164) (see Appendix B for proof of property 1).</p><p>The Poisson approximation (when N ! 1 and v r ! 0) of the binomial distribution summed in Equ. (3.164) leads to a parameter , such that</p><formula xml:id="formula_129">D .N 1 1/ p X .x 1 /v r :<label>(3.165)</label></formula><p>As N is large compared to 1 C 1, we obtain</p><formula xml:id="formula_130">' Q k 1 e r p X .x 1 / (3.166)</formula><p>and we get the approximation lim</p><formula xml:id="formula_131">N !1 P .T 1 &gt; rjX 1 D x 1 ; " 1 D 1 / ' k 1 X i D0 Q k 1 e r p X .x 1 / Á i iŠ e Q k 1 e r p X .x 1 / : (3.167) Since P.T 1 Ä rjX 1 D x 1 ; " 1 D 1 / D 1 P.T 1 &gt; rjX 1 D x 1 ; " 1 D 1 /; (3.168)</formula><p>we can get the density function of T 1 , noted g T 1 .r/, by deriving P .T 1 Ä rjX 1 D x 1 ; " 1 D 1 /.</p><p>After some mathematical developments, we obtain (see Appendix C for details):</p><formula xml:id="formula_132">g T 1 .r/ D P 0 .T 1 Ä rjX 1 D x 1 ; " 1 D 1 / D P 0 .T 1 &gt; rjX 1 D x 1 ; " 1 D 1 / D Q k 1 e r p X .x 1 / Á .k 1 C1/ .k 1 /Š e Q k 1 e r p X .x 1 / ; r 2 R;</formula><p>(3.169) and consequently (see Appendix D for details), lim</p><formula xml:id="formula_133">N !1 E OET 1 jX 1 D x 1 ; " 1 D 1 D 1 Z 1 r . Q k 1 p X .x 1 /e r / .k 1 C1/ .k 1 /Š e Q k 1 p X .x 1 /e r dr D .k 1 C 1/ log Q k 1 Á log p X .x 1 /: (3.170)</formula><p>With the definition of differential entropy H.X 1 / D EOE log .p X .X 1 //, we come to lim</p><formula xml:id="formula_134">N !1 E OET 1 D lim N !1 E OEE OET 1 jX 1 ; " 1 D E .k " 1 C 1/ log e K 1 C H.X 1 /: (3.171)</formula><p>Thus, the estimator expressed by Equ. (3.158) is asymptotically biased. Therefore, we consider a modified version, denoted by 1 H.X/ sg2 obtained by subtracting an estimation of the bias E .k " 1 C 1/ log e K 1 given by the empirical mean</p><formula xml:id="formula_135">1 N P N i D1 .k i C 1/ C 1 N P N i D1 log Q k i Á</formula><p>(according to the law of large numbers), and we obtain finally (Box</p><formula xml:id="formula_136">22 ○ in Fig. 3.6) 1 H.X/ sg2 D 1 N N X i D1 t i 1 N N X i D1 .k i C 1/ C 1 N N X i D1 log Q k i Á D 1 N N X i D1 log Â N v i Q k i Ã 1 N N X i D1 .k i C 1/ C 1 N N X i D1 log Q k i Á D log.N / C 1 N N X i D1 log .v i / 1 N N X i D1 .k i C 1/: (3.172)</formula><p>In comparison with the development of Equ. (3.150), we followed the same methodology except that we took into account (through a conditioning technique) the influence of the number of points on the border.</p><p>We observe that, after cancellation of the asymptotic bias, the choice of the function of k and i to define Q k i in Equ. (3.157) does not have any influence in the final result.</p><p>By this way, we obtain an expression for 1 H.X/ sg2 which simply takes into account the values i that could a priori influence the entropy estimation.</p><p>Note that, as for the original ) and Singh (Equ.</p><p>(3.61)) entropy estimators, both new estimation functions (Equ. (3.150) and (3.172))</p><p>hold for any value of k such that k N , and we do not have to choose a fixed k while estimating entropy in lower-dimensional spaces. So, under the framework proposed in <ref type="bibr">[Kraskov 2004</ref>], we built two different TE estimators using Equ. (3.150) and (3.172) respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.5.">Computation of the Border Points Number and of the (Hyper-) Rectangle Sizes</head><p>We explain more precisely hereafter how to determine the numbers of points i on the border. Let us denote N.B.: this determination of i may be incorrect when there exists a direction p such that the number of indices j for which D i .p; j / reaches the maximal value is larger than one: the value of i obtained with our procedure can then be underestimated.</p><formula xml:id="formula_137">x j i 2 R d ,</formula><p>However, we can argue that, theoretically, this case occurs with a probability equal to zero (because the observations are continuously distributed in probability) and so it can be a priori discarded. Now, in practice, the measure quantification errors and the roundoff errors are unavoidable and this probability will differ from zero (although remaining small when the aforesaid errors are small): theoretically distinct values D i .p; j / on the row p of D i may be erroneously confounded after quantification and rounding. But the max. / function then selects on the row p only one value for J i .p/ and so acts as an error correcting procedure. The fact that the maximum distance in the concerned p directions can then be allocated not to the right neighbor index has no consequence for the correct determination of i .</p><p>Given the entropy estimators derived from the Kozachenko-Leonenko estimator (Equ.</p><p>(3.60)) and from Singh's estimator (Equ. (3.61)), we can now derive new TE estimators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.6.">New Estimators of Transfer Entropy</head><p>From an observed realization x p i ; x i ; y i 2 S X p ;X ;Y , i D 1; 2; : : : ; N of the IID random sequence X p i ; X i ; Y i , i D 1; 2; : : : ; N and a number k of neighbors, the procedure could be summarized as follows (distances are from the maximum norm)</p><p>(1) similarly as MILCA [Sergey 2015] and TRENTOOL [Wollstadt 2015] toolboxes, normalize, for each i , the vectors x p i , x i and y i ;</p><p>(2) in joint space S X p ;X ;Y , for each point x p i ; x i ; y i , calculate the distance d .x p i ;x i ;y i /;k between x p i ; x i ; y i and its kth neighbor, then construct the (hyper-)rectangle with sizes " 1 ; : : : ; " d (d is the dimension of the vectors x p i ; x i ; y i ), for which the (hyper-)volume is v .X p ;X ;Y /;i D " 1 : : : " d and the border contains .X p ;X ;Y /;i points;</p><p>(3) for each point .x p i ; x i / in subspace S X p ;X , count the number k .X p ;X /;i of points falling within the distance d .x p i ;x i ;y i /;k , then find the smallest (hyper-)rectangle which contains all these points and for which v .X p ;X /;i and .X p ;X /;i are respectively the volume and the number of points on the border. Repeat the same procedure in subspaces S X ;Y and S X .</p><p>From Equ. (3.150), our first proposed TE estimator named TE p1 can be written as </p><formula xml:id="formula_138">(Box 23 ○ in Fig. 3.6) 3 TE Y !X p1 D 1 N N X i D1 log v .X p ;X /;i v .X ;Y /;i v .X p ;X ;Y /;i v X ;i C 1 N N X i D1 .k/ C .k X ;i / .k .X p ;X /;i / .k .X ;Y /;i / C d X p C d X 1 k .X p ;X /;i C d X C d Y 1 k .X ;Y /;i d X p C d X C d Y 1 k d X 1 k X ;i ! ; (3.176) where d X p D dim .S X p / ; d X D dim .S X / ; d Y D dim .S Y /</formula><formula xml:id="formula_139">3 TE Y !X p2 D 1 N N X iD1 log v .X p ;X /;i v .X ;Y /;i v .X p ;X ;Y /;i v X ;i C 1 N N X i D1 .k .X p ;X ;Y /;i C 1/ C .k X ;i X ;i C 1/ .k .X p ;X /;i .X p ;X /;i C 1/ .k .X ;Y /;i .X ;Y /;i C 1/ ! :<label>(3.177)</label></formula><p>In Equ. (3.176) and (3.177) the volumes v .X p ;X /;i , v .X ;Y /;i , v .X p ;X ;Y /;i , v X ;i are obtained by computing, for each of them, the product of the edges lengths of the (hyper-)rectangle, i.e. the product of d edges lengths, d being respectively equal to</p><formula xml:id="formula_140">d X p C d X , d X C d Y , d X p C d X C d Y and d X .</formula><p>In a given subspace and for a given direction, the edge length is equal to twice the largest distance between the corresponding coordinate of the reference point (at the center) and each of the corresponding coordinates of the k NNs. Hence a generic formula is</p><formula xml:id="formula_141">v U D Q dim.U / j D1</formula><p>" Uj where U is one of the symbols .X p ; X /, .X ; Y /, .X p ; X ; Y /, X and the " Uj are the edges lengths of the (hyper-)rectangle.</p><p>The new TE estimator 3 TE Y !X p1 can be compared with the TE estimator proposed in <ref type="bibr" target="#b261">[Wibral 2014a</ref>] (Equ. (3.68), implemented in the JIDT toolbox [Lizier 2014], version 1.2, referred as the Extended algorithm). The main difference with our 3 TE Y !X p1 estimator is that our algorithm uses a different length for each sub-dimension within a variable rather than one length for all sub-dimensions within the variable. We introduced this approach to make the tightest possible (hyper-)rectangle around the k NNs.</p><p>Equ. (3.68) differs from Equ. (3.176) in two ways. Firstly, the first summation in the right-hand side of Equ. (3.176) does not exist. Secondly, compared with Equ. (3.176), the numbers of neighbors k X ;i , k .X p ;X /;i and k .X ;Y /;i included in the rectangular boxes, are replaced respectively with n X ;i , n .X p ;X /;i and n .X ;Y /;i which are obtained differently. More precisely, the preceding step (2) in the Extended TE algorithm (Equ.</p><p>(3.68)) becomes:</p><p>(2 0 ) for each point .x p i ; x i / in subspace S X p ;X , n .X p ;X /;i is the number of points falling within a (hyper-)rectangle equal to the Cartesian product of two (hyper-)cubes, the first one in S X p and the second one in S X , whose edge lengths are equal, respectively, to</p><formula xml:id="formula_142">d max x p i D 2 max n x p k x p i W x p ; x ; y k 2 k .x p ;x ;y / i o (3.178) and d max x i D 2 max n x k x i W x p ; x ; y k 2 k .x p ;x ;y / i o ; (3.179) i.e. n .X p ;X /;i D card ( x p j ; x i Á W j 2 ff1; : : : ; N g fi gg &amp; x p j x p i Ä d max x p i &amp; x j x i Ä d max x i</formula><p>) :</p><p>(3.180)</p><p>Denote by v .X p ;X /;i the volume of this (hyper-)rectangle. Repeat the same procedure in subspaces S X ;Y and S X .</p><p>Note that the important difference between the construction of the neighborhoods used in 3 TE Y !X k2 and in 3 TE Y !X p1 is that, for the first case, the minimum neighborhood including the k neighbors is constrained to be a Cartesian product of (hyper-)cubes and, in the second case, this neighborhood is a (hyper-)rectangle whose edges lengths can be completely different.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Discussion and Conclusion</head><p>In this chapter, we deeply discussed the estimation of information-theoretic quantities, especially mutual information and transfer entropy. Beginning with the mathematical definition of different information-theoretic quantities, we showed similarities between the estimations of mutual information and transfer entropy. Based on previous work, especially [Kraskov 2004], several questions have been raised. To answer them, we developed new calculation strategies following two different guidelines.</p><p>In section 3.3, an analytical form of the bias for the estimation of individual entropy was proposed. Once the new bias expression derived, we discussed bias cancellation strategies in the estimation of both mutual information and transfer entropy, and introduced bias reduction strategies based on an optimal choice of the neighborhood radius. Dealing with the maximum norm, our strategy explained the conclusions drawn by <ref type="bibr" target="#b131">Kraskov et al. in [Kraskov 2004</ref>] and derived from numerical experiments. According to these conclusions, we proposed a "basic" estimator both for mutual information and transfer entropy, for different norms. The development of these strategies is based on the independence assumption provided that the bias could be partly cancelled when this hypothesis is not satisfied. So, to further eliminate the bias in dependence case, a weighted linear combination of distinct mutual information estimators, named "mixed" MI estimator, was introduced. In the same manner, we derived a "mixed" TE estimator.</p><p>In section 3.4, we focused on an idea already developed in existing literature <ref type="bibr">[Kraskov 2004]</ref>, where a (hyper-)rectangle was used instead of a (hyper-)cube. Following two different methodologies <ref type="bibr" target="#b130">[Kozachenko 1987</ref><ref type="bibr" target="#b224">,Singh 2003]</ref>, we extended the existing kNN entropy estimators to the rectangular situation, which resulted in two new entropy estimators we considered in the proposal of two novel TE estimators.</p><p>The different concepts and methodologies involved in this chapter were illustrated in Fig. <ref type="figure" target="#fig_24">3</ref>.6, and the next chapter is devoted to the experimental comparison of all these new estimators, including both mutual information and transfer entropy, with existing techniques.</p><p>Note that one important problem has not been addressed in this chapter. It concerns the selection of the predictors memory sizes (m and n) we have to deal with. Clearly, these sizes must be specified in order to define the transfer entropy before estimating it. This choice was beyond the scope of our investigations which, for a given pair .m; n/, focused on how to improve statistical performance (bias) of existing transfer entropy estimators.</p><p>The mean conditional Kullback distance introduced in Equ. (3.28) clearly indicates that TE calculation is equivalent to compare two predictive (conditional) probability distributions to characterize a directional causal link, and can allow to choose between two classes of such distributions, the one which only depends on m values in the past of X and the one which depends on m values in the past of X but also on n values in the past of Y . The averaged log-likelihood ratio in Equ. (3.29) could be considered as an averaged generalized log-likelihood ratio depending on the unknown parameters m and n introduced to test the hypothesis in Equ. (3.26). That would imply the tuning of these parameters to maximize the ratio. This more complicated and general problem is not really addressed in the literature. The currently retained sub-optimal method introduces two steps (i) order selection (estimation) and (ii) TE computation for the chosen orders.</p><p>Chapter 3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Discussion and Conclusion</head><p>For Granger causality which imposes linear autoregressive modelization, the first step (i) is implemented with standard AIC or BIC algorithms [Aho 2014] (see Appendix E).</p><p>When observed data are supposed to be generated by nonlinear mechanisms, the step (i) is generally implemented following the state reconstruction approach [Ponten 2007] which first subsamples the observation signals X and Y according to an "optimal" subsampling ratio, before selecting the n and m values. For some of the experimental results presented in chapter 5 on real signals, we used subsampling without any strict optimality requisite.</p><p>Chapter 4</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Results</head><p>Previously, we proposed different strategies for the estimation of mutual information and transfer entropy. In this chapter, the performances of different algorithms are evaluated through numerical simulations considering various situations, including independent and dependent signals, linear and nonlinear relations. When we compare different estimators, we consider both the mean value and the corresponding standard deviation. For the physiology-based model for which the theoretical value cannot be derived, we use statistical hypothesis testing to validate if our approach reveals the ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Database</head><p>First of all, we present the 8 simulation models which are used hereafter to evaluate the </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1.">Abstract Models</head><p>This section is divided in two subsections, the first one dealing with MI estimation (Models 1 to 3), the second one with TE estimation (Models 4 to 7). For transfer entropy, we tested both Gaussian IID and Gaussian AR models, as well as linear and nonlinear situations.</p><p>Model 4 was proposed to simulate an IID sequence X p i ; X i ; Y i i as introduced in chapter 3 except that the temporal statistical dependence inherent in the temporal correlation of the pair of observed processes .X; Y / has been deliberately destroyed in order to fulfill the IID observations hypothesis imposed in the theoretical derivation of the entropy estimators.</p><p>Model 4   To obtain the theoretical value of TE we started from the following expression</p><formula xml:id="formula_143">X t D aY t C bZ t C W t ; W t 2 R; Y 2 R d Y ; Z 2 R d Z ; Y; Z; W W mutually independent processes Y t N .0; C Y / ; Z t N .0; C Z / ; W t N 0; 2 W C Y D</formula><formula xml:id="formula_144">TE Z!X D H .X t jY t / H .X t jY t ; Z t / D H .U t / H .W t / (4.8) where U t D K q 1 rep 2 Z 2 t C W t .</formula><p>The entropy H.W t / is known as W t follows a Gaussian distribution. </p><formula xml:id="formula_145">H.U t / ' 1 N MC N MC X t D1 Q p U t Â K q 1 rep 2 z 2 t C w t Ã log Â Q p U t Â K q 1 rep 2 z 2 t C w t ÃÃ (4.9)</formula><p>which led to an accurate estimation of H.U t /.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2.">Physiology-based Model</head><p>This   For one population the complete system of equations can be written Since the pyramidal cells project their axons to other areas of the brain, the model accounts for this organization by using the average pulse rate of action potentials from the main cells of one population Pop X as an excitatory input to the main cells inputs of  </p><formula xml:id="formula_146">                         dx i = x i+5 dt i = 0, ..., 4 dx 5 = (AaS(x 1 -x 2 -x 3 ) -2ax 5 -a 2 x 0 )dt dx 6 = (Aa(m p + C 2 S(C 1 x 0 )) -2ax 6 -a 2 x 1 )dt + Aadβ dx 7 = (BbC 4 S(C 3 x 0 ) -2bx 7 -b 2 x 2 )dt dx 8 = (GgC 7 S(C 5 x 0 -C 6 x 4 ) -2gx 8 -g 2 x 3 )dt dx 9 = (BjS(C 3 x 0 ) -2jx 9 -j 2 x 4 )dt dx 10 = (G P H (x 6 -x 7 -x 8 ) -1 τ x 10 )dt<label>(</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2.3.">Model Order Selection</head><p>To determine approximately parameters m and n in TE estimators, we imposed m D n and computed AIC and BIC indexes versus order for a VAR model. As shown in Fig. <ref type="figure" target="#fig_52">4</ref>.5, for both AIC and BIC, the curves decrease rapidly from the starting point, and then remain quite constant. In this case, a very large optimal order can be selected. This situation should be avoided, because (i) a very large order will lead to large estimation error due to the "curse of dimensionality" , (ii) the computation time increases dramatically with an increasing model order. Hence, the model order was set to 6 in this experiment.</p><p>For MI estimators the size of the vectors, to be tested as to be independent or not, was fixed to the same value m used for TE estimators. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Simulation Results</head><p>In this section, we present the results of simulation of the different algorithms we tested.</p><p>Models 1 3 are used for the evaluation of different MI estimators (section 4.2.1), and different TE estimators are compared using Models 4 7 (section 4.2.2). For the physiologybased model, both MI and TE estimators are tested (section 4.2.3). Here, both the mean value and the corresponding standard deviation are plotted in the figures. Hence, for clarity, a small decay is set among all the curves in each figure.   Clearly, for all estimators, the error grows along with the dimension, the best result being systematically obtained with the mixed estimator based on the Euclidean norm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1.">Results on Mutual Information</head><p>Since the standard deviations are quite low, they are not shown in these figures. Using the basic estimators (or MILCA), the standard deviation varies from 0.03 to 0.06 which is extremely low compared to the estimated values of mutual information (approximately from 1 to 5). As for the mixed estimators, the standard deviation varies from 0.04 to 0.09. The increasing in standard deviation can be considered as negligible in comparison to the accuracy of the estimation. For all results, the statistical means and the standard deviations of the different estimators have been estimated on 100 trials. In Fig. <ref type="figure" target="#fig_52">4</ref>.12, to demonstrate the influence of the number of neighbors, the value of k is set to 3 and 4, while in all other figures, we fix k D 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.">Results on Transfer Entropy</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.1.">Model 4</head><p>Results for Model 4 are reported in Fig. <ref type="figure" target="#fig_52">4</ref>.9 where the dimensions d Y and d Z are identical.</p><p>We observe that, for a low dimension and a sufficient number of neighbors (Fig. <ref type="figure" target="#fig_52">4</ref>.9(a)), all TE estimators tend all the more to the theoretical value (around 0.26) that the length of the signals is large, the best estimation being obtained by the two new estimators.</p><p>Compared to Granger causality, these estimators display a greater bias, but a slightly lower variance. Due to the "curse of dimensionality", with an increasing dimension (see Fig. <ref type="figure" target="#fig_52">4</ref>.9(b)), it becomes much more difficult to obtain an accurate estimation of TE. For a high dimension, all estimators reveal a non-negligible bias, even if the two new estimators still behave better than the two reference ones (Standard and Extended algorithms). If we zoom in on Fig. <ref type="figure" target="#fig_52">4</ref>.9(b), we observe that standard deviations are within the same order of magnitude and even smaller.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.2.">Model 5 and Model 6</head><p>In this section, we firstly displayed the results of these two AR models with k D 8, and then, using Model 5 as an example, we discuss how the choice of k influences the performance of the proposed TE estimators.</p><p>As previously, for k D 8 (in Fig. <ref type="figure" target="#fig_52">4</ref>.10 and 4.11), we observe that all the transfer entropy estimators converge towards the theoretical value. This result is all the more true when the signal length increases. As expected in such linear models, Granger causality outperforms the TE estimators at the expense of a slightly larger variance. Contrary to Granger causality, TE estimators are clearly more impacted by the signal length even if their standard deviations remain lower. Here again, when comparing the different TE estimators, it appears that the TE p1 and TE p2 estimators achieve improved behavior compared to the Standard and Extended algorithms for k D 8.</p><p>For Model 5 (Fig. <ref type="figure" target="#fig_52">4</ref>.10), for both directions (from X to Y and from Y to X), the proposed TE estimators (TE p1 and TE p2 ) systematically outperform the other estimators.</p><p>For Model 6, for all directions, this hierarchy is globally preserved. In both Fig. <ref type="figure" target="#fig_52">4</ref>  In the scope of kNN algorithms, the choice of k must be a tradeoff between the estimation of bias and variance. Globally, when the value of k decreases, the bias decreases for the Standard and Extended algorithms and for the new estimator TE p1 . Now, for the second proposed estimator TE p2 , it is much more sensitive to the number of neighbors (as can be seen when comparing Fig. <ref type="figure" target="#fig_52">4</ref>.12(a) and 4.12(b)). As shown in Fig. <ref type="figure" target="#fig_52">4</ref>.10 and 4.11, the results obtained using TE p2 and TE p1 are quite comparable when the value of k is large (k D 8). Now, when the number of neighbors decreases, the second estimator we proposed, TE p2 , is much less reliable than all the other ones (Fig. <ref type="figure" target="#fig_52">4</ref>.12(b)). Concerning the variance, it remains relatively stable when the number of neighbors falls from 8 to 3.</p><p>More details on this phenomenon are given in Appendix I.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.3.">Model 7</head><p>For this nonlinear model, we tuned the parameters to obtain a strong coupling between X and Z. In this situation, the Granger causality index failed in detecting the information flow and remained equal to zero for the different sets of tested parameters (see Fig. <ref type="figure" target="#fig_52">4</ref>.13).</p><p>We observed the same issue as that pointed in <ref type="bibr" target="#b93">[Gao 2015</ref>], i.e. a very slow convergence of the kNN-based estimator when the number of observations increases, and noticed that all the 6 TE estimators revealed comparable performance (as displayed in Fig. <ref type="figure" target="#fig_52">4</ref>.13).</p><p>In this difficult case, the newly proposed methods do not outperform the existing ones.</p><p>For this type of strong coupling perhaps further improvement could be obtained at the expense of an increasing computational complexity as that proposed in <ref type="bibr" target="#b93">[Gao 2015</ref>].       </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3.">Results on the Physiology-based Model</head><formula xml:id="formula_147">X → Y Case 1 Y → X Case 1 X → Y Case 2 Y → X Case 2 X → Y Case 3 Y → X Case 3 (a) (b) (c)<label>(d)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.4.">Computational Costs</head><p>Computation time is also an important issue. In the computation of kNN-based estimators, the most time-consuming part is the procedure of nearest neighbor searching. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Discussion and Conclusion</head><p>In this chapter, we tested the MI and TE estimators proposed in chapter 3 with different models, including linear and nonlinear structures and simulating dependent and nonindependent observations. For mutual information, the experimental results prove the effectiveness of the strategy proposed in chapter 3, and both MILCA and basic estimators (whatever the norm, Box 14 ○ in Fig. <ref type="figure" target="#fig_24">3</ref>.6) outperform the MI estimators using the same k in case of independent signals. However, with models simulating non-independent signals, there are no significant differences among the results. Chapter 5</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Analysis of Real Signals</head><p>In the previous chapter, the algorithms we developed have been tested on simulated </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Database</head><p>First we present the database to be analyzed hereafter. This database is composed  some statistics measuring the global transfer information from the tested signal to the others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1.">Local Connectivity Index</head><p>In this work, to highlight the sensitivity of the causality statistics in the different epileptic phases and in the expert electrode labeling, we propose an analysis using a boxplot-based visualization. The target is to visualize the effect of the seizure phase factor (baseline/pre-  x value exceeds 6. Finally, taking account of orders estimated from many cases (different epochs, different seizures) the model order was uniformly set to 6. Additionally, to avoid the "curse of dimensionality", we chose to keep the temporal memory depth (six times the time sampling period corresponding to 256 Hz) but to subsample with a subsampling ratio equal to 0.5. Thus, given two signals X and Y , to predict x t , only OEx t 2 ; x t 4 ; x t 6 and OEy t 2 ; y t 4 ; y t 6 were used. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Experimental Results</head><p>This section is devoted to the collection and discussion on the results. The index defined by Equ. (5.1) is expected to be large when the sum of the significant influences of channel n onto the other channels is much greater than the sum of the significant influences from the other channels on it. Consequently, this index reveals whether the channel n drives the others in some manner. For large negative values of this index, the channel n is considered as being more influenced by the other nodes than influencing the others. Note that the magnitude of the values taken by this index is not important if we essentially want to observe its variations with respect to the seizure phase and the tested channel n in Equ. (5.1).</p><p>Accordingly, what we expect with the LCI and DTF boxplot values is that: (i) for group O, there should be significant increase during the ictal part, (ii) for group N, there should be no significant mean deviation compared with the baseline. With the definition in Equ. (5.1), for group P, it could be expected to observe significant mean globally not satisfying, as is the case for group N. For the analysis of the group P, it appears difficult to derive general conclusions, partly due to the fuzzy nature of this class of structure, which prevents us from detecting the most relevant methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Discussion and Conclusion</head><p>In this chapter, we applied both Granger causality and different transfer entropy estimators discussed previously to analyze human epileptic signals. Since DTF remains popular in the analysis of epileptic signals, it has been also tested. Based on the ground truth provided by the clinical experts, (i) among all the three tested TE estimators, the proposed algorithm outperforms the two others, (ii) DTF appears as the most reliable measure and gives interesting performance. Even if lots of time and effort have been spent on the analysis of real signals, we must acknowledge that the relevance of information-theoretic quantities is not so evident.</p><p>To detect significant causality index values, we introduced a decision threshold which was derived theoretically for the Granger causality index (see Appendix K). Now, to test transfer entropy, defining such a threshold is not so trivial. In this experiment, for a more effective comparison among different algorithms, we used an adaptive threshold to make sure that all algorithms retain the same number of links. Another critical issue is the "curse of dimensionality", since large dimensions always bring troubles to TE estimations, including both computation time and estimation accuracy. Here, we tried to avoid this problem by time subsampling the predictor vectors.</p><p>In our experiments, all indexes have been calculated on the whole frequency band.</p><p>In recent literature <ref type="bibr" target="#b201">[Ponten 2007</ref><ref type="bibr" target="#b170">, Mierlo 2011</ref><ref type="bibr" target="#b246">, Varotto 2012</ref><ref type="bibr" target="#b171">, Mierlo 2013]</ref>, the analysis of epileptic data is performed on different frequency bands, and this could be a perspective to this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>Our work concerns the detection of effective connectivity to be applied in the context of epilepsy without addressing the question of direct or indirect links.</p><p>In chapter 1, we presented a brief introduction to the research background, including the disease itself, the human brain structure and different types of brain connectivity.</p><p>As mentioned in our survey of the state of the art in chapter 2, various algorithms have been proposed to quantify the strength of influence between different subsystems. Among these methods, Granger causality and transfer entropy are two well-known approaches that are linked under Gaussian assumption. For Granger causality, a linear model is considered to fit the data and its calculation is relatively simple. However, for informationtheoretic approaches, such as transfer entropy, their computation becomes relatively difficult, especially in high-dimensional spaces. This is the main issue preventing us from applying these methods in their conditional form taking into account the environmental network.</p><p>In chapter 3, we deeply discussed the estimation of information-theoretic quantities In the case of the maximum norm, we obtained the same conclusion as in <ref type="bibr">[Kraskov 2004</ref>],</p><p>i.e. using the same distance for different spaces can be the optimal choice. We got the proof that, with this kind of strategy, the bias could vanish if and only if the independence assumption was satisfied. For mutual information, to further reduce the bias in the case of dependent signals, we proposed mixed estimators, for which the individual entropy was calculated with a linear combination of two different individual estimations.</p><p>Another important improvement is the use of a (hyper-)rectangle instead of a (hyper-)cube while calculating TE with maximum norm. This idea was firstly proposed in <ref type="bibr">[Kraskov 2004]</ref>, where the (hyper-)rectangle was used only in the joint space and not in the marginal spaces. In chapter 3, we extended this idea and proposed to Several possible directions can be considered in a future work. As a matter of fact, in our study, transfer entropy was seen as a pairwise approach. In our experiments, when we estimated the causality between two channels, the influence of the other channels has not been taken into account contrary to either DTF approach or other causality methods based on N -channel VAR models. To remedy this shortcoming, the conditional transfer entropy can be further investigated. However, this conditional measure also brings new challenges, e.g. (i) to reduce the complexity, we would have to decide which part of the environment influences causal relations between two specific channels, (ii) to escape the "curse of dimensionality" due to the involvement of more channels, we could consider causality graph following the approach proposed in <ref type="bibr" target="#b208">[Runge 2012</ref>].</p><p>Moreover, all the causal indexes (including GC, TE and DTF) were calculated on full frequency band in this work. However, in recent literature <ref type="bibr" target="#b201">[Ponten 2007</ref><ref type="bibr" target="#b170">, Mierlo 2011</ref><ref type="bibr" target="#b246">,Varotto 2012</ref><ref type="bibr" target="#b171">,Mierlo 2013]</ref>, the epileptic signals are analyzed on different frequency bands. So, in a further work, it would be interesting to discover the corresponding causal relations on different frequency bands.</p><p>To summarize, on the one hand we obtained interesting improvements of entropic bivariate measures of causality based on kNN approach that have been validated on simulated signals. On the other hand, in order to retrieve conclusions given by clinical experts, we applied these measures to human intracerebral epileptic signals by integrating them in a proposed local causality index derived from the bivariate causality algorithms, to be compared with the popular DTF causality index. Our preliminary conclusion offered a mixed picture. The strong non-stationarity of epileptic signals is a priori an additional challenge using entropic methods which require relatively long observation intervals and which have been developed in the literature more particularly for the analysis of cognitive networks. Now, the performance of the proposed entropic measures is not so poor compared to that of the DTF reference measure. Finally, the expected advantage of entropic methods being some enhancement in nonlinear connectivity, the existence of this nonlinear connectivity should be questioned. Even if the epileptic signals considered individually could be produced by strongly nonlinear mechanisms, it is not obvious that the connectivity mechanisms are strongly nonlinear and necessarily require nonlinear methods to be correctly detected. Supplementary investigation on sub-band signals should be considered to discuss this idea. ). These equivalences imply the equalities between conditional probability values With P.T 1 Ä rjX 1 D x 1 ; " 1 D 1 / D 1 P.T 1 &gt; rjX 1 D x 1 ; " 1 D 1 /, we take the derivative of P.T 1 Ä rjX 1 D x 1 ; " 1 D 1 / to get the conditional density function of T 1  </p><formula xml:id="formula_148">P .T 1 &gt; rjX 1 D x 1 ; " 1 D 1 / D P Â log Â N V 1 e K 1 Ã &gt; rjX 1 D x 1 ; " 1 D 1 Ã D P .V 1 &gt; v</formula><formula xml:id="formula_149">P 0 .T 1 Ä rjX 1 D x 1 ; " 1 D 1 / D P 0 .T 1 &gt; rjX 1 D x 1 ; " 1 D 1 / D 0 @ k 1 X i D0 . Q k 1 p X .x 1 /e r / i i Š e Q k 1 p X .x 1 /e r 1 A 0 D k 1 X i D0 0 @ . Q k 1 p X .x 1 /e r / i i Š ! 0 e Q k 1 p X .x 1 /e r C . Q k 1 p X .x 1 /e r / i iŠ e Q k 1 p X .x 1 /e r Á 0 1 A D k 1 X i D0 i. Q k 1 p X .x 1 /e r / i 1 . Q k 1 p X .x 1 /e r / i Š e Q k 1 p X .x 1 /e r C . Q k 1 p X .x 1 /e r / i iŠ e Q k 1 p X .x 1 /e r . Q k 1 p X .x 1 /e r / ! D k 1 X i D0 e Q k 1 p X .x 1 /e r . Q k 1 p X .x 1 /e r / i .i 1/Š . Q k 1 p X .</formula><formula xml:id="formula_150">log.z/ log Q k 1 Á log p X .x 1 / Á z k 1 .k 1 /Š e z dz D 1 .k 1 C 1/ Z 1 0 log.z/z k 1 e z Á dz log Q k 1 Á log p X .x 1 / D 1 .k 1 C 1/ Z 1 0 log.z/z .k 1 C1/ 1 e z Á dz log Q k 1 Á log p X .x 1 / D 0 .k 1 C 1/ .k 1 C 1/ log Q k 1 Á log p X .x 1 / D .k 1 C 1/ log Q k 1 Á log p X .</formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>d'autre part par Singh, dont nous résumons tout d'abord les développements mathématiques. Nous proposons ensuite de modifier ces deux estimateurs de la même manière, i.e. en substituant au voisinage hyper-cubique de volume minimal incluant les k plus proches voisins un voisinage hyper-rectangulaire de volume minimal incluant ces mêmes voisins. Avec ce type de voisinage la probabilité d'avoir plus d'un point sur la frontière (en l'occurrence 2 points dans le cas à deux dimensions) est strictement positive, ce qui nécessite de reprendre les développements mathématiques précédents pour ce nouveau type de voisinage. Finalement, nous obtenons deux nouveaux estimateurs d'entropie, le premier étendant celui de Kozachenko-Leonenko et le second l'estimateur d'entropie de Singh. Ce dernier nécessite de déterminer le nombre de points sur les frontières des hyper-rectangles sans que cela présente un inconvénient pratique. Ces deux estimateurs d'entropie sont ensuite utilisés pour proposer respectivement deux nouveaux estimateurs d'entropie de transfert.Chapitre 4. Résultats Expérimentaux Dans le chapitre précédent, nous avons proposé différentes stratégies pour l'estimation de l'information mutuelle et de l'entropie de transfert qu'il s'agit d'évaluer en considérant différentes situations, incluant signaux indépendants ou dépendants, relations linéaires et non linéaires. Les premières simulations portent sur des processus blancs gaussiens mais aussi des modèles vectoriels autorégressifs parfois non linéaires. Ces choix ont été conduits d'une part pour disposer autant que faire se peut de valeurs de référence et d'autre part pour une certaine représentativité de caractéristiques pouvant être celles de signaux réels. Dans un second temps, nous nous sommes intéressés à un modèle physiologique de populations neuronales potentiellement couplées. Pour ce modèle pour lequel nous ne disposons pas de valeur théorique de référence, des tests statistiques sont conduits pour valider nos approches. Concernant les performances des différents estimateurs d'information mutuelle, comme attendu, le premier modèle (modèle 1) testé sur deux signaux indépendants met en évidence la supériorité des estimateurs récemment proposés dans la litterature comparés à ceux utilisant le même nombre de voisins. Dans le cas de deux signaux dépendants, deux autres modèles sont testés, le premier (modèle 2) pour considérer l'effet de matrices de covariance des observations non diagonales, le second (modèle 3) pour mettre en exergue "le fléau de la dimension". Dans les deux cas, les nouveaux estimateurs présentent un meilleur comportement au regard des estimateurs issus de la littérature, pour les deux normes, ce résultat étant d'autant plus vrai que la corrélation entre signaux est élevée, leur longueur faible et la dimension importante. Pour ce qui est de l'entropie de transfert, le premier modèle testé (modèle 4) simule une suite d'observations (chacune rassemblant une valeur à prédire de X, un vecteur correspondant à son passé et un vecteur correspondant au passé de Y ) indépendantes, pour lesquelles les matrices de covariance ne sont pas nécessairement diagonales. Les deux estimateurs proposés sont testés et comparés à l'indice de causalité de Granger ainsi qu'aux algorithmes issus de la littérature (l'algorithme standard et l'algorithme étendu). Pour des dimensions faibles et un nombre de voisins suffisant, les algorithmes proposés surpassent les autres estimateurs d'entropie de transfert mais s'avèrent moins performants que l'indice de Granger. Les deux modèles suivants (modèles 5 et 6) décrivent des signaux autorégressifs linéaires respectivement au nombre de 2 et 3 présentant des connectivités bidirectionnelles pour chaque paire de signaux. Là encore, l'indice de Granger se révèle le plus pertinent au prix d'une variance parfois légèrement accrue. Toutefois, parmi les estimateurs d'entropie de transfert, les nouveaux estimateurs ont des comportements extrêmement corrects, l'un d'eux se montrant, de façon quasi-systématique, plus efficace que ceux issus de la littérature. L'intérêt des estimateurs d'entropie de transfert trouve son sens dans les résultats rapportés sur le modèle suivant (modèle 7) qui présente de fortes non-linéarités. Dans ce cas, l'indice de Granger est mis en échec alors que les différents estimateurs d'entropie de transfert continuent à bien se comporter et à tendre vers la valeur théorique pour des longueurs suffisantes de signaux. Pour finir, quatre estimateurs de transfert d'information (indice de Granger, algorithmes standard et étendu, et premier estimateur d'entropie de transfert proposé) sont appliqués sur le modèle physiologique évoqué plus haut après avoir vérifié la pertinence des estimateurs d'information mutuelle sur la dépendance ou non des populations neuronales générées. Il s'avère que l'indice de Granger et le nouvel estimateur sont les seuls à distinguer avec pertinence les trois situations considérées (populations indépendantes, connectivité unidirectionnelle et bidirectionnelle).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>jet souffrant d'une épilepsie temporale trouvant son origine dans l'hémisphère gauche et pour laquelle nous disposons de l'expertise de cliniciens. Les signaux à traiter s'avèrent nettement plus complexes que ceux produits par les modèles décrits dans le chapitre précédent, ne serait-ce que par leur caractère non stationnaire. Cette première analyse a conduit à des catégorisations et facilité l'analyse ultérieure de mesures de causalité.Dans ce cadre, nous avons choisi de tester différents indices, à commencer par l'indice de causalité de Granger. Comme mesures de transfert d'entropie, nous avons retenu les algorithmes issus de la littérature (algorithmes standard et étendu) ainsi que l'un des deux estimateurs proposés dans le chapitre 3. Après une revue de la littérature sur les outils les plus communément utilisés pour détecter les voies d'initiation et de propagation de l'épilepsie, nous avons également choisi de tester la fonction de transfert dirigée qui a souvent fait l'objet d'applications dans ce domaine. De cette mise en compétition, il ressort que, parmi les estimateurs de transfert d'entropie, celui proposé dans cette thèse est le plus efficient. Néanmoins, sur l'ensemble des estimateurs testés, malgré une certaine variabilité, la fonction de transfert dirigée se révèle la plus pertinente.Cette thèse se conclut par une discussion sur les différentes contributions apportées en indiquant dès à présent des améliorations possibles des estimateurs proposés et ouvre des perspectives de travail sur ce vaste domaine de la connectivité effective. 3.1 Problem Statement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.1.1 Introduction to Information-theoretic Quantities . . . . . . . . . . 3.1.2 The Estimator Structures for MI and TE . . . . . . . . . . . . . . 3.2 Previous Works . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.2.1 Estimation of Entropy . . . . . . . . . . . . . . . . . . . . . . . . . 3.2.2 Estimation of Mutual Information . . . . . . . . . . . . . . . . . . 3.2.3 Estimation of Transfer Entropy . . . . . . . . . . . . . . . . . . . . 3.2.4 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.3 First Improvement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.3.1 New Bias Expression for the Plug-in Entropy Estimator . . . . . . 3.3.2 Bias Reduction of MI/TE Estimators Based on the New Bias Expression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.3.3 Bias Reduction for Dependence Situations . . . . . . . . . . . . . . 3.4 Second Improvement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.4.1 Original k-Nearest Neighbors Strategies . . . . . . . . . . . . . . . 3.4.2 From Square to Rectangular Neighboring Region for Entropy Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.4.3 Extension of the Kozachenko-Leonenko Method . . . . . . . . . . . 3.4.4 Extension of Singh's Method . . . . . . . . . . . . . . . . . . . . . 3.4.5 Computation of the Border Points Number and of the (Hyper-) Rectangle Sizes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.4.6 New Estimators of Transfer Entropy . . . . . . . . . . . . . . . . . 3.5 Discussion and Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 Experimental Results 4.1 Database . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.1.1 Abstract Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.1.2 Physiology-based Model . . . . . . . . . . . . . . . . . . . . . . . . 4.2 Simulation Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4.2.1 Results on Mutual Information . . . . . . . . . . . . . . . . . . . . 4.2.2 Results on Transfer Entropy . . . . . . . . . . . . . . . . . . . . . . 4.2.3 Results on the Physiology-based Model . . . . . . . . . . . . . . . . II Contents 4.2.4 Computational Costs . . . . . . . . . . . . . . . . . . . . . . . . . . 4.3 Discussion and Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 Analysis of Real Signals 5.1 Database . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.2 Method . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.2.1 Local Connectivity Index . . . . . . . . . . . . . . . . . . . . . . . 5.2.2 Experimental Protocol . . . . . . . . . . . . . . . . . . . . . . . . . 5.3 Experimental Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.4 Discussion and Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . Conclusion Appendices A Derivation of Equ. (3.89) . . . . . . . . . . . . . . . . . . . . . . . . . . . B Proof of Property 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C Derivation of Equ. (3.169) . . . . . . . . . . . . . . . . . . . . . . . . . . . D Derivation of Equ. (3.170) . . . . . . . . . . . . . . . . . . . . . . . . . . . E AIC and BIC Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . F Development of the Theoretical MI Value for Model 2 . . . . . . . . . . . G Development of the Theoretical MI Value for Model 3 . . . . . . . . . . . H Power Spectral Densities of the Signals . . . . . . . . . . . . . . . . . . . . I Comparison between Entropy Estimators . . . . . . . . . . . . . . . . . . . J DTF Algorithm Used in Chapter 5 . . . . . . . . . . . . . . . . . . . . . . K Independence Test for Granger Causality . . . . . . . . . . . . . . . . . . . Extended Granger Causality Index • EPDC: Extended Partial Directed Coherence • EZ: Epileptogenic Zone V • fMRI: functional Magnetic Resonance Imaging • FDA: Food and Drug Administration • gOPDC: generalized Orthogonalized Partial Directed Coherence • GC: Granger Causality • GPDC: Generalized Partial Directed Coherence • iEEG: intracranial (or intracerebral) ElectroEncephaloGraphy • IID: Independent and Identically Distributed • ILAE: International League Against Epilepsy • IP: Inhomogeneous Polynomial • kNN: k-Nearest Neighbors • KDE: Kernel Density Estimation • KGC: Kernel Granger Causality • KSG: Kraskov-Stögbauer-Grassberger • LCI: Local Causality Index • LLNAR: Local Linear Nonlinear AutoRegressive • MEG: MagnetoEncephaloGraphy • MI: Mutual Information • MRI: Magnetic Resonance Imaging • MSI: Magnetic Source Imaging • MVAAR: MultiVariate Adaptive AutoRegressive • MVAR: MultiVariate AutoRegressive • NARX: Nonlinear AutoRegressive with eXogenous inputs • NLGC: NonLinear Granger Causality • PC: Partial Correlation • PDC: Partial Directed Coherence • PET: Positron Emission Tomography VI</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Epilepsy is usually characterized by repetitive seizures, which are the results of excessive and abnormal cortical nerve cell activity in the brain [Fisher 2005]. Practically, the basis for the diagnosis of epilepsy is two or more unprovoked seizures occurring more than 24 hours apart [Engel 2008]. In the document of the international league against epilepsy (ILAE) [Fisher 2005], an epileptic seizure is defined as "a transient occurrence of signs and/or symptoms due to abnormal excessive or synchronous neuronal activity in the brain". The duration of an epileptic seizure can vary from nearly undetectable to long periods, where the patient often becomes unconscious and loses control of his body [Browne 2008]. Recurrent and unprovoked epileptic seizures are the main characteristics of epilepsy, but epilepsy is more than seizures. A fundamental definition of epilepsy can be found in the document of ILAE [Fisher 2005]: "A chronic condition of the brain characterized by an enduring propensity to generate epileptic seizures, and by the neurobiological, cognitive, psychological, and social consequences of this condition."</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Figure 1.1: Different interacting factors that contribute to the totality of epilepsy [Engel 2008].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 1 . 2 :</head><label>12</label><figDesc>Figure 1.2: Intracerebral electrodes are surgically implanted into the brain tissue to map the seizure activities before removing brain structures [Ellen 2015].</figDesc><graphic coords="34,220.55,214.55,154.18,209.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 1 . 3 :</head><label>13</label><figDesc>Figure 1.3: Lateral surface of the cerebral hemisphere [Dewey 2007]. On each hemisphere, there are four lobes bounded by fissures: the frontal lobe, parietal lobe, occipital lobe, and temporal lobe. By cutting the interhemispheric commissures and opening the third ventricle, we have a vision on the medial aspect of the hemisphere. As shown in Fig. 1.4, on the medial side, there is a special cortical convolution, called cingulate gyrus (or limbic gyrus),</figDesc><graphic coords="36,151.80,148.22,291.68,223.84" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Fig. 1.6). The first one contains numerous cell bodies and relatively few myelinated axons (myelinization speeds up the neural electric propagation), while the second one is composed of long-range myelinated axon tracts and contains relatively very few cell bodies<ref type="bibr" target="#b172">[Miller 1980</ref>].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 1 . 4 :</head><label>14</label><figDesc>Figure 1.4: Medial surface of the cerebral hemisphere [Haines 2015]. Different lobes and their associated gyri and sulci are marked in this figure.</figDesc><graphic coords="37,89.29,90.71,416.70,256.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 1 . 5 :</head><label>15</label><figDesc>Figure 1.5: Lower face of the hemispheres [Bertrand 2015]. A: temporal lobe, B: frontal lobe, C: occipital lobe, D: olfactory bulb, E: olfactory tract, F: optic chiasm, G: cerebral peduncle, H: brainstem, I: pituitary gland, J: mammillary tubercle.</figDesc><graphic coords="37,151.80,394.93,291.66,274.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 1 . 6 :</head><label>16</label><figDesc>Figure 1.6: Gray and white matter in human brain [Giovannoni 2015].</figDesc><graphic coords="38,151.80,90.71,291.68,158.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 1 . 7 :</head><label>17</label><figDesc>Figure 1.7: Histological structure of the cerebral cortex [Michael 2012]. The neocortex is composed of nerve cells which arranged in six horizontal layers segregated principally by cells types and neuronal connections. Layer 1: cell surface association, Layer 2: cells of intra-hemispheric association, Layer 3: small pyramidal cells, Layer 4: projection of sensitive and sensory cells, Layer 5: large pyramidal cells of Betz (origin of pyramidal tract), Layer 6: inter-hemispheric cells association (callosal fibers).</figDesc><graphic coords="39,214.30,90.71,166.67,274.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 1 . 8 :</head><label>18</label><figDesc>Figure 1.8: Patterns of brain connectivity [Honey 2007, Sporns 2007]. The upper half of this figure illustrates structural connectivity (fiber pathways), functional connectivity (correlations), and effective connectivity (information flow) among four brain regions in macaque cortex. In the lower half of the figure, three different matrices show binary structural connections (left), symmetric mutual information (middle) and non-symmetric transfer entropy (right), respectively.</figDesc><graphic coords="40,99.71,282.46,395.86,239.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>2. 1 .</head><label>1</label><figDesc>Introduction to Effective ConnectivityDuring the past several decades, there has been an increasing interest in the research of human brain structure and functions. However, our knowledge on the effective connectivity, which could provide us a deep understanding of the brain, is still poor [Petkov 2015]. The concept of effective connectivity firstly appeared in the work of Aertsen and Preissl [Aertsen 1991]. Shortly later, this concept was studied by several authors on rat/human projects [McIntosh</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Building systems from de®ned underlying organizations. This example gives the possible ways to interconnect three neural populations from unidirectional couplings (single-sided arrow) and/ or bidirectional couplings (double-sided arrows) Figure 2.1: Possible connectivities among three neural populations, both unidirectional couplings (single-sided arrow) and bidirectional coupling (double-sided arrows) [Wendling 2000].</figDesc><graphic coords="44,193.30,91.34,207.97,426.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>Figure 2.2: Granger causality fails to distinguish these two situations: (a) there is direct influence from X to Y , (b) the influence from X to Y is mediated by Z, which is indirect.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head></head><label></label><figDesc>target systems are known to be nonlinear. Therefore, the exploration of nonlinear information transmission could provide additional information about the system [Marinazzo 2011]. To this end, several studies have been conducted to extend WGC to nonlinear situations. One possibility of extending linear GC to nonlinear situations is to replace linear autoregression with other nonlinear prediction schemes. The local linear nonlinear autoregressive model (LLNAR) is a generalization of linear autoregression. The nonlinearity of LLNAR is controlled by a parameter, the bandwidth, and LLNAR reduces to the ordinary autoregressive model when the bandwidth tends to infinity. The flexibility of LLNAR in describing nonlinear characteristics of EEG signals has been demonstrated by Hernández et al. [Hernández 1996]. In 1999, based on LLNAR, Freiwald et al. [Freiwald 1999] introduced a generalized definition of GC, which is valid for both linear and nonlinear systems. With this method, Freiwald et al. successfully revealed the existence of both unidirectional and bidirectional influences between neural groups in the macaque inferotemporal cortex.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>2008, based on the theory of reproducing kernel Hilbert spaces (RKHS) [Shawe-Taylor 2004], a kernel version of Granger causality (KGC) was proposed by Marinazzo<ref type="bibr" target="#b158">et al. [Marinazzo 2008b</ref>] to detect nonlinear cause-effect relationship. In this method, the linear GC was performed in the feature space of suitable kernel functions (Gaussian kernels and inhomogeneous polynomial (IP) kernels as those presented in<ref type="bibr" target="#b158">[Marinazzo 2008b]</ref>). KGC assumes arbitrary degree of nonlinearity, i.e. it is able to handle all orders of nonlinearity[Marinazzo 2011]. This measure was also generalized to the multivariate case to analyze dynamical networks<ref type="bibr" target="#b157">[Marinazzo 2008a</ref>]. As pointed out by the authors, the multivariate version of this approach is still able to reveal the real causalities while complexity of the model increases. Shortly after its introduction, KGC was adopted in the analysis of fMRI data<ref type="bibr" target="#b142">[Liao 2009</ref>]. Using both simulated and real fMRI data, this newly proposed approach was shown to capture the effective coupling which was missed by linear GC. The effectiveness of the multivariate KGC was validated by different studies. In<ref type="bibr" target="#b8">[Angelini 2009</ref>], Marinazzo's method was applied on several physical systems, and the results showed that the information flow was properly identified. Stramaglia et al. [Stramaglia 2011] tested the same approach on EEG data to reveal nonlinear interaction. In [Liao 2011], Liao et al. used multivariate KGC and graph theory on resting-state fMRI recordings to reveal the network architecture of the brain network. In [Bezruchko 2008], Bezruchko et al. proposed to use a polynomial of degree p instead of the linear autoregressive model to capture nonlinear causality. This method can be viewed as a special case of KGC with IP kernel functions. The well-known NARX (nonlinear autoregressive with exogenous inputs) model [Billings 2013] is another extension of the linear model, which could be used as the basis of the nonlinear Granger analysis. This possibility has been firstly explored by Li et al. [Li 2012]. Recently, in 2013, Zhao et al. [Zhao 2013] went further with this idea: different indexes were designed to investigate the linear and nonlinear causalities between time series separately. Moreover, it should be noted that, compared to all the nonlinear extensions of GC mentioned above, these methods based on NARX are time-variant, which is supposed to provide extra important insights into the signals. In both [Li 2012] and [Zhao 2013], the usefulness of their methods was demonstrated with real EEG signals.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head></head><label></label><figDesc>et al. used it to find the Beta oscillatory networks in monkey sensorimotor cortex [Brovelli Chapter 2 2004]. Chen et al. used its conditional form to analyze neural field potential time series [Chen 2006]. In 2013, Croux and Reusens [Croux 2013] used this method to investigate the predictive power for the future domestic economic activity that is contained in the domestic stock prices. In [Epstein 2014], to investigate the features of preictal seizure networks, this spectral method was employed for the analysis of iEEG signals recorded at high frequencies (500 or 1000 Hz). In 2014, one important generalization of spectral GC was introduced by He et al. [He 2014]. This method uses the nonlinear NARX [Billings 2013] signals modeling, and can be considered as the spectral decomposition of the nonlinear GC described in [Li 2012,Zhao 2013]. In their experiments, He et al. validated the effectiveness of the proposed method with both artificial data and a real human intracranial EEG data set. In 1975, the concept of causality was also introduced in the study of feedback relations between input and output variables by Caines and Chan [Caines 1975], and generalized later to the multivariate situation by Kamiński and Blinowska [Kamiński 1991] who proposed a new spectral causality measure, the directed transfer function (DTF). Using a multivariate autoregressive model to fit multi-channel signals, the DTF method, which is based on spectral domain functions, is designed to detect the directional influences between any given pair of signals in a multivariate data set. It has been demonstrated that the DTF function could be interpreted in terms of GC [Kamiński 2001]. After its introduction, DTF has been widely investigated, especially in the analysis of EEG signals, for the purpose of localizing epileptogenic foci in patients with partial epilepsy [Ding 2007, Dorr 2007, Wilke 2010, Lu 2012].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head></head><label></label><figDesc>and the multivariate autoregressive (MVAR) modeling coefficients are adapted to each window. The window's length must be short enough to consider local stationarity. Ding et al. applied this method to a visuomotor integration task, and revealed rapidly changing cortical dynamics. In [Liang 2000], short-time DTF was used to detect causal influence between cortical sites on a fraction-of-a-second time scale. In 2006, Philiastides and Sajda [Philiastides 2006] used this method for the analysis of multi-channel EEG data recorded during a face discrimination task. Standard DTF algorithm assumes that the signals are stationary. The short-window DTF mentioned above introduces a windowing technique so as to consider local sta-tionarity, while Wilke et al. [Wilke 2007, Wilke 2008] provided another solution, termed adaptive DTF (ADTF), where the coefficients of the multivariate adaptive autoregressive (MVAAR) model are time-dependent. Tested on both simulated and real electrocorticogram (ECoG) data, ADTF appeared more suited for the detection of time-variant connectivities than the standard DTF. For the time-varying causality detection algorithm mentioned above, the non-zero covariance of the model's residual was used to describe the causality phenomenon. However, in some situations, for example, the blurring of the neuronal activity with the sluggish latent response, non-zero covariance of model residuals can be observed. This zero-lag correlation would lead to spurious time-lagged causality detection [Deshpande 2010a]. To solve this problem, Xu et al. [Xu 2014] introduced another adaptive extension of DTF, termed time-lagged adaptive DTF. Compared with the adaptive algorithm proposed in [Wilke 2008], Xu's method employed a more general adaptive model, and took the influence of instantaneous connectivity into consideration. The effectiveness of this method has been demonstrated through the detection of dynamic spectral causality in real visual evoked potentials (VEPs) data. Another multivariate measure was proposed by Sameshima and Baccalá [Sameshima 1999, Baccalá 2001], called partial directed coherence (PDC). Contrary to DTF, PDC measures the direct causality between two channels discounting the influence of all other recorded channels, while DTF captures the influence of the whole network by taking all the channel transmission pathways into consideration [Sameshima 2014]. Both DTF and PDC assume that the signals are stationary, and for non-stationary data, it could be divided into windows to obtain approximate stationarity [Bressler 2011]. In 2007, Baccalá introduced a scale-invariant form of PDC, the generalized partial directed coherence (GPDC) [Baccald 2007]. The GPDC measure overcomes the drawbacks of PDC (as discussed in [Schelter 2009]): (i) GPDC is not affected by the relation between the given source and a third group of channels, (ii) GPDC is scale invariant, (iii) GPDC allows the analysis of the absolute strength of coupling. Since its introduction in 2001, PDC has interested a lot of researchers for multiple applications. With both simulated and real data, Astolfi et al. [Astolfi 2005, Astolfi 2006] tested PDC for the estimation of human cortical connectivity. In [Schelter 2006], Schelter et al. discussed the statistical properties on the estimation of PDC, and tested PDC on EEG and EMG data recorded from an essential tremor patient. Sato et al. [Sato 2009] applied PDC for connectivity analysis of multisubject fMRI data using multivariate bootstrap. In [Sun 2009], PDC was used to study the cortical connective network under audiovisual cognitive processes. In [Wang 2015], PDC was used for automatic epileptic seizure detection. Both standard PDC and GPDC are based on a MVAR model, which only describes the lagged effects among different signals. To overcome this limitation, Faes and Nollo [Faes 2010] proposed an extended version of PDC (EPDC) based on the utilization of an extended MVAR model including both instantaneous and lagged effects. The authors proved the effectiveness of EPDC with two different EEG data sets. Recently, Omidvarnia et al. [Omidvarnia 2012, Omidvarnia 2014] have extended the classical PDC connectivity analysis, and introduced a novel measure, called generalized orthogonalized PDC (gOPDC). In the development of gOPDC, the coefficients of MVAR</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head></head><label></label><figDesc>tensions have been brought to DCM for specific applications, such as fMRI [Kiebel 2007, Stephan 2007, Marreiros 2008, Stephan 2008] and EEG/MEG [Kiebel 2006, Chen 2008, Marreiros 2009, Moran 2009, Daunizeau 2009, Penny 2009]. Both aiming at detecting causality, DCM and GC have been compared in the literature [David 2008]. However, there is fundamental difference between these approaches:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head></head><label></label><figDesc>Largely spread since 1948 [Shannon 1948], information theory concerns quantities that measure the uncertainty in random variables based on a probabilistic concept, and it has been applied in numerous fields, including neuroscience [Dimitrov 2011]. Using information theory based approaches, we can measure the amount of uncertainties reduced in one random process after observing another one. Compared with the methods mentioned above, such as GC or DCM, information-theoretic quantities assume almost no</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>(</head><label></label><figDesc>TLMI) (also termed time-delayed mutual information (TDMI)<ref type="bibr" target="#b178">[Na 2002]</ref>) was introduced<ref type="bibr" target="#b110">[Hlaváčková-Schindler 2007]</ref>, which estimates the shared information between one process and the lagged version of another one. Several authors have applied TLMI to the analysis of EEG signals<ref type="bibr" target="#b119">[Jeong 2001</ref><ref type="bibr" target="#b178">, Na 2002</ref><ref type="bibr" target="#b173">, Min 2003</ref>]. However, as demonstrated by some authors<ref type="bibr" target="#b217">[Schreiber 2000</ref><ref type="bibr" target="#b122">,Kaiser 2002]</ref>, TLMI sometimes fails in revealing the actual information flow.In 2000, Schreiber [Schreiber 2000] introduced an information-theoretic statistic measurement, named transfer entropy (TE), to quantify the amount of time-delayed information between two dynamical systems. Given the past time evolution of a dynamical system A, transfer entropy from another dynamical system B to the first system A is the amount of Shannon uncertainty reduction in the future time evolution of A when including the knowledge of the past evolution of B. As demonstrated in<ref type="bibr" target="#b217">[Schreiber 2000</ref><ref type="bibr" target="#b122">,Kaiser 2002</ref>], compared with TLMI, TE is capable to distinguish the information exchanged from shared information due to common history and input signals. Like Granger causality, this method can be extended so as to get its conditional form<ref type="bibr" target="#b262">[Wibral 2014b</ref>] to exclude the influence of a third group of variables. Transfer entropy, as classical Granger causality, does not reveal instantaneous coupling. This coupling is taken into account in the more general notion of directed information (DI) [Amblard 2012].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head></head><label></label><figDesc>biological and physical mechanisms. Based on the results of<ref type="bibr" target="#b16">Barnett et al. [Barnett 2009</ref>],Hlavácková-Schindler investigated how the equivalence of the two causality measures can be extended under some conditions on probability density distributions of the data, and generalized this relation to other common types of distributions<ref type="bibr" target="#b111">[Hlaváčková-Schindler 2011]</ref>. Later, with a very general class of continuous or discrete Markov models, Barnett and Bossomaier<ref type="bibr" target="#b18">[Barnett 2012</ref>] extended TE to a log-likelihood ratio in a maximum likelihood framework.It has been indicated that<ref type="bibr" target="#b186">[Paluš 2001</ref><ref type="bibr" target="#b110">, Hlaváčková-Schindler 2007</ref>] TE is actually an equivalent expression for conditional mutual information with the history of the influenced variable in the condition. In<ref type="bibr" target="#b46">[Chicharro 2011</ref>], Chicharro discussed the spectral decomposition of several causality measures, however, he argued that, as most general information-theoretic measures related to GC, TE lacks a spectral representation in terms of the recorded processes. Standard transfer entropy may be sometimes defective. As a matter of fact, when estimating the causality between two systems, according to the definition of TE, a non-zero value of TE is a clue of causality relation. However, for unidirectional relation, it is rather possible to obtain non-zero TE value for both directions, which would cause spurious conclusions. In<ref type="bibr" target="#b225">[Smirnov 2013</ref>], Smirnov discussed the following three typical factors leading to such phenomenon (i) unobserved state variables of the driving system, (ii) low temporal resolution, and (iii) observation errors.Due to its information-theoretic background, TE is model-free (inherently nonlinear) and has already shown its superiority in the detection of effective connectivity for nonlinear interactions<ref type="bibr" target="#b181">[Nichols 2005</ref>, Lungarella 2007a, Vicente 2011]. Also, several open source tools are available for its practical use<ref type="bibr" target="#b259">[Wibral 2011b</ref><ref type="bibr" target="#b148">, Lizier 2014</ref><ref type="bibr" target="#b174">, Montalto 2014]</ref>.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head></head><label></label><figDesc>x.i C .t ik 1/ /g. A symbol then denotes the order indices b x i . b t i1 ; b t i2 ; : : : ; b t ik / thus mapping this sequence onto one of the possible kŠ permutations of the number 1 to k, reflecting the successive order of the amplitude values.<ref type="bibr" target="#b229">Staniek and Lehnertz [Staniek 2008]</ref> introduced this technique into the estimation of TE and proposed a new method called symbolic transfer entropy (STE). Compared with transfer entropy, STE is computationally faster and more robust to noise<ref type="bibr" target="#b229">[Staniek 2008</ref>]. In<ref type="bibr" target="#b230">[Staniek 2009</ref>], Staniek and Lehnertz gave another discussion on this measure of directed interactions, and demonstrated its performance in the analysis of human epileptic brain. Melzer and Schella [Melzer 2014] applied STE to analyze the behavior of changed-particle systems, and revealed the information transportation successfully. In 2008, Bettencourt et al. [Bettencourt 2008] demonstrated that, analogous to a Taylor series, mutual information between a stochastic variable and a set of other variables, could be expanded into a sum of information quantities, which allowed to obtain a new view of high-order correlations. Based on this work [Bettencourt 2008], Stramaglia et al. [Stramaglia 2012] proposed a formal expansion of transfer entropy, involving irreducible sets of variables which provided information for the future state of the target.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>Fig. 3</head><label>3</label><figDesc>Fig. 3.6. In this diagram, a box identified by a number n in a circle is designed by box n ○ hereafter. Note that only the bias analysis is addressed in the theoretical developments proposed in this thesis. The variances estimations are investigated only experimentally in other sections.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head></head><label></label><figDesc>independent component analysis [Pham 2004], image analysis [Chang 2006], genetic analysis [Martins Jr 2008], speech recognition [Jung 2008], manifold learning [Costa 2004], time delay estimation [Benesty 2007], among others [Brillouin 2013]. As explained hereafter, transfer entropy is a more recently introduced entropic measure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_26"><head></head><label></label><figDesc>tainty on X supplied by the observation of Y . The relation between the informationtheoretic quantities mentioned above given non-independent random variables X and Y is summarized in Fig.3.1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_27"><head>Figure 3 . 2 :</head><label>32</label><figDesc>Figure 3.2: An illustration for the relations of different conditional information-theoretic measures for three non-independent random variables X, Y , and Z [Abramson 1963]. The lower-left, lower-right and upper circles, stand for H.X/, H.Y / and H.Z/ respectively.For three random variables X, Y and Z, the conditional mutual information I.X; Y jZ/ measures the information shared between X and Y conditioned on Z. By applying Equ.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_28"><head></head><label></label><figDesc>According to the definition (in Equ. (3.28)), TE is not symmetric and it can be regarded as a conditional mutual information (CMI) [Hlaváčková-Schindler 2007, Paluš 2001] (sometimes also named partial mutual information (PMI) in the literature [Frenzel 2007]). Recalling that definition of conditional mutual information in Equ. (3.23), TE can be also written as TE Y !X D I X p ; Y jX : (3.35) TE can be considered as a measurement of the degree to how past Y of the process Y disambiguates the future X p of X beyond the degree to how its only past X disambiguates its future [Paluš 2001]. It is an information-theoretic implementation of Wiener's principle of observational causality. Hence TE reveals a natural relation to Granger causality. As it is well known, Granger causality emphasizes the concept of reduction of the mean square error of the linear prediction of X p i when adding Y i to X i by introducing the Granger causality index</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_29"><head></head><label></label><figDesc>tended to the conditional situation, termed conditional transfer entropy (CTE) [Yang 2012] (or partial transfer entropy (PTE) in some literature [Gómez-Herrero 2015]). Considering three random variables X, Y and Z, in order to quantify the information flow from Y to X conditioned on Z, similarly to Equ. (3.28), CTE at time i can be defined as</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_31"><head></head><label></label><figDesc>Figure 3.3: An illustration of two different ways to estimate density with a ball shape region L.x/ around the center point, (a) the ball radius R is imposed, (b) the radius of L.x/ is determined by the distance between kth NN and the center of the ball (in this example, k D 6). In this example the considered norm is the standard Euclidean norm.</figDesc><graphic coords="73,138.58,441.00,145.85,122.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_32"><head>Figure 3 . 4 :</head><label>34</label><figDesc>Figure 3.4: An example of using kernel method for density estimation with different widths R (R D 0:1, R D 0:8 and R D 2:0). Data are generated by the mixture of two equally weighted Gaussian distributions with mean values 1 D 0, 2 D 5, and 1 D 2 D 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_33"><head></head><label></label><figDesc>[Kraskov 2004] proposed to use a common neighboring size for both joint and marginal spaces when selecting NNs. This strategy consisted in fixing the number of neighbors in the joint space S X;Y , then projecting the resulting distance (which corresponds, in this space, to the maximum norm distance between the center of the cube and the kth NN) into the marginal spaces S X and S Y , i.e. the cubic balls in S X and S Y have the same radius as the cubic ball in S X;Y (Box 4 ○ in Fig. 3.6). Following this idea, the following MI estimator was proposed by Kraskov in 2004 [Kraskov 2004] (Box 5 ○ in Fig 3.6)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_34"><head></head><label></label><figDesc>Figure 3.5: An example of a cube in S X;Y space (with d X D d Y D 1) determined by the kth NN with k D 4. Denoting c x ; c y the center of the cube in S X;Y , with probability one, only one of the two following cases can arise: (a) " x &lt; " y there is no point on the border of the interval c x C " y 2 ; c x C " y</figDesc><graphic coords="77,140.67,466.34,145.85,144.14" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_36"><head></head><label></label><figDesc>to approximate the entropy of a distribution, and the size-related parameters (bandwidth for KDE, or number of neighbors for kNN) are not involved in this analysis, (b) according to previous studies [Suzuki 2008], the Edgeworth MI estimator is accurate when the underlying distributions are close to Gaussian distribution, and becomes unreliable when the distributions are far from Gaussian. Secondly, as pointed out in [Kraskov 2004], the two MI estimators could provide accurate results for 2 I.X; Y / only when X and Y are independent. The question is now: is it possible to improve the results when X and Y are dependent? Thirdly, the two kNN estimators (Equ. (3.63) for MI, and Equ. (3.67) for TE) are only implemented with the maximum norm (MI estimator in MILCA [Sergey 2015], and TE estimator in TRENTOOL [Wollstadt 2014]), where the distances are obtained in the joint space with the highest dimension, and these distances are then projected into the other spaces. Is it possible to apply this strategy to other norms, for instance, the Euclidean one?</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_37"><head></head><label></label><figDesc>entropy estimator is established without any assumption on the data for two different norms. The theoretical analysis based on the maximum norm developed coincides with the experimental results drawn from numerical tests made by Kraskov et al. [Kraskov 2004]. To further validate the novel relation, a weighted linear combination of distinct mutual information estimators is proposed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_38"><head></head><label></label><figDesc>for the Euclidean norm and by 2 I.X; Y / k basic;M for the maximum norm, and called "basic estimator". Now, we consider the estimation of transfer entropy. Similarly with Equ. (3.98), Equ.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_39"><head></head><label></label><figDesc>111) corresponds to the TE estimator (Equ. (3.67)) adapted in the TREN-TOOL toolbox [Wollstadt 2015]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_40"><head></head><label></label><figDesc>More precisely the second equality in Equ. (3.112) amounts to say that X is a white sequence, what is not generally the case. So actually the TE estimator in Equ. (3.67) seems to us more difficult to justify. To conclude about this question, only a part of the bias can be expected to be cancelled out if Equ. (3.112) is not satisfied. In the same manner as previously (Equ. (3.106)), we can also define the basic esti-mator 3 TE Y !X k basic for transfer entropy (Box 15 ○ in Fig. 3.6)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_41"><head>piTE</head><label></label><figDesc>; x i ; y i / D k. Hereafter, this estimator is written as 3 for the maximum norm.Note that, this development for the strategy of relation-specific distances can be generalized to the estimation of entropy combinations other than MI and TE, such asCTE [Yang 2012].3.3.3. Bias Reduction for Dependence SituationsPreviously, we discussed the bias reduction strategies of relation-specific distances for the estimation of MI and TE. However, these strategies work well in independence situations, and the bias is only partly cancelled out when the independence conditions are not satisfied. To further eliminate the bias in the general case, we still consider the estimation of individual entropies. Removing the bias B X in Equ. (3.83) is not an easy task since its mathematical expression depends on the unknown probability density. However, we can expect to cancel it out considering a weighted linear combination [Sricharan 2013].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_42"><head></head><label></label><figDesc>Fig. 3.6) and the one by Singh [Singh 2003] (Box 3 ○), then introduce the idea of rectangle (Box 20 ○). After that, we discuss the extensions of the existing kNN entropy estimators based on the idea of rectangle, which results in two novel entropy estimators (Boxes 21 ○ and 22 ○). Based on them, two new TE estimators are proposed (Boxes 23</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_43"><head></head><label></label><figDesc>[Kraskov 2004], to estimate MI, as illustrated in Fig.3.7, Kraskov et al. discussed two different techniques to build the neighboring region to compute 2 I.X; Y /: in the standard technique (square ABCD in Fig. 3.7(a) and 3.7(b)), the region determined by the first k NNs is a (hyper-)cube and leads to Equ. (3.63), and, in the second technique (rectangle A 0 B 0 C 0 D 0 in Fig. 3.7(a) and 3.7(b)), the region determined by the first k NNs is a (hyper-)rectangle. Note that the TE estimator mentioned in the previous section (Equ. (3.67)) is based on the first situation (square ABCD in Fig. 3.7(a) or 3.7(b)). The introduction of the second technique by Kraskov et al. was to circumvent the fact that Equ. (3.135) was not applied rigorously to obtain the terms .n X;i C 1/ or</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_44"><head>Figure 3 . 7 :</head><label>37</label><figDesc>Figure 3.7: In this 2-dimensional example, k D 5. The origin of the Cartesian axis corresponds to the current point x i . Only the 5 NNs of this point, i.e. the points in the set ki , are represented. The 5th NN is symbolized by a star. The neighboring regions ABCD, obtained from the maximum norm around the center point, are squares, with equal edge lengths " x D " y . Reducing one of the edge lengths, " x or " y , until one point falls onto the border (in the present case, in the vertical direction), leads to the minimum size rectangle A 0 B 0 C 0 D 0 , where " x ¤ " 0 y . Two cases must be considered, illustrated respectively in Fig.3.7(a) and 3.7(b). For case (a) the 5th NN is not localized on an intersection of two edges, contrary to the case (b). This leads to obtain either two points (respectively the star and the triangle in Fig.3.7(a)) or only one point (the star in Fig.3.7(b)) on the border of A 0 B 0 C 0 D 0 . Clearly it is theoretically possible to have more than 2 points on the border of A 0 B 0 C 0 D 0 but the probability of such an occurrence is equal to zero when the probability distribution of the random points X j is continuous.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_45"><head></head><label></label><figDesc>performance of the algorithms presented in chapter 3. The first seven models are denoted Model 1,. . . , Model 7 and are abstract models. The eighth model is named physiologybased model because it is built from electrophysiological hypothesis concerning the origin of the iEEG signals. The first three models are dedicated to test mutual information estimators in the case of IID observations. The remaining five models are dedicated to test transfer entropy estimators. Except the physiology-based model, for all other models tested, the theoretical value of the estimated measures can be derived (Models 1 to 6) or computed by a Monte Carlo numerical approach (Model 7). Only Model 7 and the physiology-based model are nonlinear models. Nevertheless, linear models are relevant to compare statistical performance (bias and variance) of different existing and proposed MI or TE estimators.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_46"><head></head><label></label><figDesc>toeplitz.1; ˛; : : : ; ˛dY 1 /; C Z D toeplitz.1; ˛; : : : ; ˛dZ 1 / (4.4) For the matrix C Y , we chose ˛D 0:5, and, for C Z , ˛D 0:2. The standard deviation W was set to 0.5. The vectors a and b were such that a D 0:1 OE1; 2; : : : ; d Y and b D 0:1 OEd Z ; d Z 1; : : : ; 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_47"><head>Figure 4 . 1 :</head><label>41</label><figDesc>Figure 4.1: Experimental marginal distribution for .X t ; Z t / with K D 1, rep D 0:1, Â D 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_48"><head></head><label></label><figDesc>Fig. 2.9 -Voir figure 2.10</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_49"><head>Figure 4 . 3 :</head><label>43</label><figDesc>Figure 4.3: Interactions between different neuronal subpopulations of the hippocampus [Wendling 2005, Frogerais 2008].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_50"><head>:</head><label></label><figDesc>dx i D x i C5 dt i D 0; : : : ; 4dx 5 D AaS .x 1 x 2 x 3 / 2ax 5 a 2 x 0 dt dx 6 D Aa m p C C 2 S .C 1 x 0 / 2ax 6 a 2 x 1 dt C Aadď x 7 D BbC 4 S .C 3 x 0 / 2bx 7 b 2 x 2 dt dx 8 D GgC 7 S .C 5 x 0 C 6 x 4 / 2gx 8 g 2 x 3 dt dx 9 D BjS .C 3 x 0 / 2j x 9 j 2 x 4 dt dx 10 D Â G PH .x 6 x 7 x 8 C i , iD1; : : : ; 7, the synaptic time constants 1=a, 1=b and 1=g, and the mean and standard deviation of the input Gaussian process were fixed as in [Wendling 2001, Wendling 2005]. Their values are reported in Tab. 4.1. Generated signals and their corresponding PSD are shown in Fig. 4.4. for two triplets fA; B; Gg to simulate either background activity or narrow band activity observed at a seizure onset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_51"><head></head><label></label><figDesc>Figure 4.4: Example of signals generated by the model. (a) simulated EEG signal (A D 3:67, B D 2 and G D 22:45) and simulated background signal (A D 2:8, B D 1 and G D 40). (b) corresponding PSD.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_52"><head>Figure 4</head><label>4</label><figDesc>Figure 4.5: AIC and BIC indexes versus order computed under VAR hypothesis on the physiology-based model outputs (case 1: unidirectional connectivity).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_53"><head>For</head><label></label><figDesc>Fig. 4.6 displays the performance of different approaches in the independence case (Model 1) with the two norms (maximum and Euclidean norms). For the two estimators using the same k, the performance drastically falls with increasing ˛(Fig. 4.6(a)) and high dimensions (greater than 4) (Fig. 4.6(b)). The estimators with a given neighborhood size (basic estimators and MILCA) clearly outperform the former significantly whatever the norm, in terms of estimation bias and standard deviation. Unsurprisingly, in such an independence situation, the mixed estimators, which are designed for the dependence situation, suffer from a large bias whatever the norm. In other words, the new justified strategy (Boxes 12○ and 13 ○ in Fig.3.6) provides reliable mutual information values for independence test, even with short signal lengths or high dimensional signals.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_54"><head>Figure 4</head><label>4</label><figDesc>Figure 4.8: (Model 3) Mutual information and mean estimation error (estimation bias) using different estimators with 100 trials. (a) Mutual information (in nats) estimated with varying ˇ, d D 3, N D 512. (b) Mutual information (in nats) estimated with different signals lengths, ˇD 0:4, d D 3. (c) Mean estimation error ÿ I.X; Y / I.X; Y / (in nats) with varying dimension, ˇD 0:5, N D 512.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_55"><head></head><label></label><figDesc>For a complete comparison, beyond the theoretical value of TE, we also computed the Granger causality index as a reference (as indicated previously, in the case of linear Gaussian signals, transfer entropy and Granger causality index are equivalent up to a factor of 2). In the following figures, GCi/2 corresponds to Granger causality index GC divided by 2, transfer entropy estimated by the free TRENTOOL toolbox (corresponding to Equ. (3.67)) is marked as Standard algorithm, that estimated by JIDT (corresponding to Equ. (3.68)) is marked as Extended algorithm, TE p1 is the transfer entropy estimator given by Equ. (3.176) and TE p2 is the transfer entropy estimator given by Equ. (3.177). Additionally, the basic TE estimators (Equ. (3.113) with both Euclidean and maximum norms) are also tested for comparison. Concerning the basic TE estimator with maximum norm and the Standard algorithm, they use the same distance for both joint space and marginal spaces. The former one estimates the probability density for each data sample, whereas the Standard algorithm calculates TE directly (based on the kNN entropy estimator, Equ. (3.60)).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_56"><head>Figure 4</head><label>4</label><figDesc>Figure 4.9: (Model 4) Information transfer (in nats) from Z to X estimated for two different dimensions with k D 8. The figure displays the mean values and the standard deviations, (a) d Y D d Z D 3, (b) d Y D d Z D 8.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_57"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4.10: (Model 5) Information transfer (in nats), mean values and standard deviations, k D 8. (a) From X to Y . (b) from Y to X .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_58"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4.12: (Model 5 with different k) Information transfer (in nats) from X to Y , mean values and standard deviations. (a) k D 4. (b) k D 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_59"><head>Figure 4 .Figure 4</head><label>44</label><figDesc>Figure 4.13: (Model 7) Information transfer from Z to X , mean values and standard deviations, k D 8, Â D 1, 100 trials. (a) K D 1, rep D 0:8. (b) K D 0:6, rep D 0:8. (c) K D 1, rep varies, signal length N D 1024.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_61"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4.15: (Physiology-based model) Boxplots of information flow estimated values, k D 30. For each case (1, 2 or 3) boxplots are displayed for original data (left-hand side) and surrogate data (i.e. under H 0 hypothesis, right-hand side). (a) Granger causality. (b) Standard algorithm, k D 30. (c) Extended algorithm, k D 30. (d) Proposed algorithm (TE p1 ), k D 30.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_62"><head></head><label></label><figDesc>signals. Now, we apply the proposed causality measures to real signals recorded in the cerebral cortex of an epileptic patient. To our knowledge entropic methods, and particularly transfer entropy, have not been applied yet to analyze effective connectivity in epileptic brain in the course of seizures (about 10 seconds before the onset of the seizure, during and after the seizure), as they have been more investigated in cognitive tasks. Beyond a comparison between the causality measures we developed in chapter 3, it seems also interesting to compare these techniques tested on real signals with some reference method. In most works in the literature dedicated to epileptic seizures analysis, the direct transfer function (DTF)<ref type="bibr" target="#b170">[Mierlo 2011</ref><ref type="bibr" target="#b121">, Jung 2011</ref><ref type="bibr" target="#b171">, Mierlo 2013</ref><ref type="bibr" target="#b274">, Zhang 2015]</ref> remains a widely used measure. Hereafter, a N -variate statistic, called local causality index (LCI), is introduced. Hence, besides the comparison of Granger causality with different TE estimators discussed in the previous chapter, we also compare them with the directed transfer function through the LCI index to discuss the information they provide with a view to a better understanding of the patient seizure organization. To this end, we use boxplot-based visualization of LCI values computed either with the causality indexes detailed in chapter 3 or with DTF. For each tested causality index, to highlight the effect of each epileptic phase, i.e. before, during or after the epileptic seizure onset, a Student's t-test is carried out to compare statistically this phase with a reference phase corresponding to background activity. Based on the ground truth provided by the clinical experts, the performances of the different algorithms are discussed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_63"><head></head><label></label><figDesc>of 72-second length iEEG signals recorded on 20 channels in the cerebral cortex of an epileptic patient. These signals were recorded in brain structures whose activities are a priori interesting to investigate according to preliminary clinical and electrophysiological examinations. A schematic diagram of the placement of 12 iEEG electrodes on a lateral view (left hemisphere) is displayed in Fig. 5.1. In this figure the symbol A' means "electrode in anatomical region of type A in the left hemisphere". Around 10 to 15 sensors were placed along each electrode, for example A'1 to A'15. Each iEEG signal is bipolar, obtained by the difference of the potentials recorded on two adjacent sensors. All channels (signals) were sampled at 256 Hz. As shown in Fig. 5.2, a seizure onset up to 32 seconds was recorded in this database. This recording can be divided into three parts: pre-ictal phase (0s 20s), ictal phase (20s 52s) and post-ictal phase (52s 72s) (some examples of PSD of these signals are displayed in Appendix H, Fig. H.3). Additionally, to get a reference in our experiments, another segment of iEEG signals has been recorded on the same 20 channels, far apart from the seizure onset. This additional recording is called hereafter "baseline". According to the clinical experts, the 20 channels can be categorized into three groups, as listed in Tab. 5.1, named respectively Onset group (group O), Propagation group (group P) and Not-involved group (group N). The signals associated with the Onset group are supposed to be linked to activities in brain regions from which the seizure starts. The Propagation group contains electrodes corresponding to brain structures acting as relays which are stimulated with some delay after the beginning of the seizure and which possibly stimulate other structures later. The Not-involved group corresponds to structures which are neutral with respect to epileptic processes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_64"><head>Figure 5</head><label>5</label><figDesc>Figure 5.1: Schematic diagram of iEEG electrodes placement (left hemisphere).</figDesc><graphic coords="140,151.80,522.68,291.68,185.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_65"><head>Figure 5</head><label>5</label><figDesc>Figure 5.3: An example of the determined significant links obtained by the different indexes for one 2-second length sliding window (2s 4s). (a) For Granger causality, using the methods introduced in Appendix K, 45 links are considered as significant, so that we kept the same number of links for the other 4 measures, (b) Standard algorithm, (c) Extended algorithm, (d) proposed algorithm, (e) DTF algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_66"><head>Figure 5</head><label>5</label><figDesc>Figure 5.4: AIC and BIC indexes values versus order (computed on channel Cp1 and on the 72-second length signal).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_67"><head>Figure 5 . 5 :</head><label>55</label><figDesc>Figure 5.5: Boxplot of different indexes, (a) Granger causality, (b) Standard algorithm, (c) Extended algorithm, (d) proposed algorithm, (e) DTF algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_68"><head></head><label></label><figDesc>based on k-Nearest Neighbors (kNN) techniques. The estimation of mutual information (MI) and transfer entropy (TE) is always an important issue, especially in neuroscience, where getting large amounts of stationary data is problematic. Both MI and TE can be calculated as a summation of different individual entropy estimations. Until now, the most widely used MI/TE estimator follows the kNN strategy proposed in[Kraskov   2004]: compute the distance in the highest-dimensional space, and use the same distance in other marginal spaces. In[Kraskov 2004], the effectiveness of this strategy was proved by numerical simulations. Using multi-dimensional Taylor expansion, we introduced a novel analytical form of the individual entropy estimation bias depending on the norm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_69"><head></head><label></label><figDesc>use (hyper-)rectangles in both joint space and marginal spaces, where the maximum distances in different directions could be different. To this end, we first investigated the estimation of Shannon entropy based on the kNN technique including a rectangular neighboring region and introduced two different kNN entropy estimators. We derived mathematically these new entropy estimators by extending the results and methodology developed in[Kraskov 2004] and<ref type="bibr" target="#b224">[Singh 2003</ref>]. Given the new entropy estimators, two novel TE estimators have been proposed, implying no extra computation cost compared to existing similar kNN algorithms.In chapter 4, the new MI/TE estimators were tested with various kinds of models. To validate the performance of the proposed estimators, we compared them with the MI/TE estimators available in existing toolboxes: (i) Kraskov-Stögbauer-Grassberger (KSG) MI estimator available in MILCA toolbox, (ii) Standard TE estimator and Extended TE estimator available in TRENTOOL and JIDT toolboxes respectively. For mutual information, as expected, in independence situations, the MI estimators following the proposed strategy performed very well (mutual information very close to zero). For time correlated observations sequences, the new mixed estimators (with both Euclidean and maximum norms) gave the best results. However, these mixed MI estimators were very sensitive to the selection of the number of neighbors, and so more difficult to use. For transfer entropy, under Gaussian assumption, experimental results proved the effectiveness of the new estimators for IID data in comparison with the standard TE estimator. This conclusion still held when comparing the new algorithms with the extended TE estimator. Globally, all TE estimators satisfactorily converged to the theoretical TE value, i.e. to half value of Granger causality, while the newly proposed TE estimators showed lower bias for a number of neighbors sufficiently large (in comparison with the reference TE estimators) with comparable variance estimation errors. We noticed that the two proposed TE algorithms produced quite comparable results when the number of neighbors was sufficiently large. However, one of the new TE (TE p2 ) estimators suffered from noticeable error when the number of neighbors was small. Finally, experimental results on simulated iEEG signals showed that (i) all tested MI algorithms gave the expected results, (ii) for the detection of information flow, only Granger causality and the first proposed TE algorithm (TE p1 ) successfully distinguished the three following situations: independency, unidirectional and bidirectional propagation flows, at 99% confident level.In chapter 5, we applied different causal approaches, including Granger causality, transfer entropy and directed transfer function, to analyze real signals, for which the ground truth was given by clinical experts. We first proposed a boxplot-based visualization to compare the different algorithms. A test derived from the Granger-Wald test was introduced for Granger causality to determine if the relation between two channels was significant or not. For TE and DTF, the adaptive threshold was used to retain the same number of links as Granger causality. According to the results, the proposed algorithm TE p1 outperformed all other tested TE estimators, Granger causality and DTF being slightly better.Concerning the computation time of the different TE algorithms, we mainly have to consider the procedure of NN searching, which is the most time-consuming part in the computation of kNN-based estimators. The newly proposed TE estimators (TE p1 and TE p2 ) involve supplementary information, (i) the maximum distance of the first k NNs in each dimension (also used in the Extended TE algorithm), and (ii) the number of points on the border. The most widely used neighbor searching algorithms, such as kd tree, provide not only information on the kth neighbor but also on the first k NNs. So, these informations required here can be retained without additional cost. Therefore, it can be considered that there is no significant increase in computation cost for the newly proposed kNN TE estimators. This work is a first step in a more general context of connectivity investigation for neurophysiological activities obtained either from nonlinear physiology-based modeling or from real human epileptic recordings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_70"><head></head><label></label><figDesc>equivalence expresses the fact that the (hyper-)volume of D 1 ;:::; d x 1 is larger than v r if and only if the normalized domain D 0 1 ;:::; 0 d x 1does not contain more than .k 1 / points x j (as 1 of them are on the border of D 1 ;:::; d x 1 which is necessarily not included in D 0 1 ;:::; 0 d x 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_71"><head></head><label></label><figDesc>r jX 1 D x 1 ; " 1 D 1 / considered because the variable X 1 and the 1 variable(s) on the border of D 1 ;:::; d x 1 must be discarded. Moreover, these events are independent. Hence the probability value in Equ. (B.2) can be developed as followsP .T 1 &gt; rjX 1 D x 1 ; " 1 D 1 / ' x 1 /v r (note that the randomness of 0 1 ; : : : ; 0 d does not influence this approximation C. Derivation of Equ. (3.169)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_72"><head>E</head><label></label><figDesc>OET 1 jX 1 D x 1 ; "</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_73"><head></head><label></label><figDesc>Figure H.1: Gaussian AR signals (left) and the corresponding PSD (right). (a) Model 5. (b) Model 6.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>cohol misuse, and in some rare cases, it is linked to genetic mutations [Malani 2012].However, currently, the exact cause of most cases of epilepsy remains unknown.</figDesc><table><row><cell>Nowadays, epilepsy is one of the most serious common neurological diseases, and</cell></row><row><cell>there is a growing recognition of its harm on modern society. Worldwide, about 50</cell></row><row><cell>million people (1% of the world's total population) are affected by epilepsy, and nearly</cell></row><row><cell>80% of cases occur in the developing countries. In 1990, about 111,000 people died from</cell></row><row><cell>epilepsy, and this number increased to 116,000 in 2013 [Naghavi 2015]. In Europe, due</cell></row><row><cell>to epilepsy, the direct economic lost is around 15.5 billion Euros in 2004 [Nunes 2012]. It</cell></row><row><cell>also impairs the quality of people's life; as a matter of fact, in many areas of the world,</cell></row><row><cell>due to the high risk of being involved in a traffic accident, people with epilepsy have</cell></row></table><note><p><p><p><p><p><p>1.1.1. Introduction</p>Epilepsy</p><ref type="bibr" target="#b155">[Magiorkinis 2010</ref></p>] is a group of neurological disorders, and it includes many different manifestations depending on various factors, like the age of the individual, the part of the brain that is affected, the underlying causes, among others</p><ref type="bibr" target="#b226">[Smithson 2012</ref></p>]. This kind of disease has a very long history, and the first known detailed record of the disorder can be traced back to more than 3,000 years ago, which is written in a Babylonian cuneiform medical text [WHO 2005]. restrictions placed on their ability to drive or are not permitted to drive [Devlin 2012]. This kind of disease can be caused by brain injury, stroke, brain tumor, and drug/al-1.1. Epilepsy Chapter 1</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>random vectors must correspond, clearly, to the entropy of the (column) vector X T ; Y T T . It is named joint entropy and is denoted by H.X; Y /. Consequently, we have: If X and Y are continuous random variables with dimensions d X and d Y respectively,</figDesc><table><row><cell>Chapter 3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">3.1. Problem Statement</cell></row><row><cell>the definition changes to</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>I cnt .X; Y / D</cell><cell>Z</cell><cell>dy</cell><cell>Z</cell><cell cols="3">p X;Y .x; y/ log</cell><cell>Â</cell><cell cols="3">p X;Y .x; y/ p X .x/p Y .y/</cell><cell>Ã</cell><cell>dx:</cell><cell>(3.16)</cell></row><row><cell></cell><cell>y2R d Y</cell><cell cols="3">x2R d X</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="11">Conditional Entropy Equ. (3.15) and (3.16) can be summarized by the following integral with respect to</cell></row><row><cell cols="11">the measure P X;Y The conditional entropy H.XjY / measures the uncertainty about X when Y is ob-served beforehand H.X jY / D E log p XjY .XjY / ; .x;y/2R d X R d Y (3.12) I.X; Y / D Z log Â dP X;Y d .P X ˝PY / .x; y/ Ã dP X;Y .x; y/; (3.17)</cell></row><row><cell cols="11">where p X jY .xjy/ D p X;Y .x;y/ p Y .y/ probability density function. Note that the expectation in Equ. (3.12) is computed with is the conditional discrete distribution or the conditional where the derivative in brackets is defined with respect to the tensor product of P X</cell></row><row><cell></cell><cell></cell><cell>y2Y</cell><cell>X x2X</cell><cell>p X;Y .x; y/ log</cell><cell>Â</cell><cell cols="3">p X;Y .x; y/ p X .x/p Y .y/</cell><cell>Ã</cell><cell>;</cell><cell>(3.15)</cell></row><row><cell></cell><cell cols="8">H.X; Y / D E log p X;Y .X; Y / :</cell><cell></cell><cell>(3.10)</cell></row></table><note><p>This definition can be easily extended to a finite set comprising more than two random Chapter 3 vectors. Equ. (3.10) implies that, if X and Y are independent, i.e. if p X;Y . ; / D p X . /p Y . /, we have the following relation H.X; Y / D H.X/ C H.Y /: (3.11) respect to the joint probability distribution of .X; Y / as indicated below H.X jY / D E log p X jY .X jY / D Z R d X log Â dP XjY . jy/ d r .x/ Ã dP X;Y .x; y/ ; (3.13) where r is defined on the Borel sets of R d X . The following basic property is easy to verify: H.XjY / D H.X; Y / H.Y /: (3.14) 3.1.1.2. Mutual Information Mutual Information (MI), besides its historical central role in telecommunication theory and engineering, is a widely used information-theoretic independence measurement which has received particular attention during the past few years [Urbanczik 2003, Stögbauer 2004, Wissman 2011, Foster 2011, Dunleavy 2012]. For two discrete random variables X and Y , with outcomes x and y from X and Y separately, the mutual information I.X; Y / is defined as I dis .X; Y / D X where p X;Y .x; y/ D P .X D x; Y D y/, p X .x/ D P .X D x/ and p Y .y/ D P .Y D y/. and P Y (by definition, P X ˝PY is equal to the joint probability measure for a pair of random vectors U and V , respectively valuated in R d X and R d Y , such that P U D P X and P V D P Y ). Theoretically, I.X; Y / is always non-negative, and I.X; Y / D 0 if and only if X and Y are independent (i.e. p X;Y . ; / D p X . /p Y . /). Mutual information is symmetric, I.X; Y / D I.Y; X/; (3.18) and it can also be expressed in terms of entropies I.X; Y / D H.X/ C H.Y / H.X; Y /; (3.19) or, equivalently I.X; Y / D H.X/ H.X jY /: (3.20) According to Equ. (3.20), I.X; Y / can also be considered as the decrease of uncer-</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>.1: The relation between different information-theoretic quantities with nonindependent random variables X and Y [Abramson 1963]. The area contained by both circles is the joint entropy H.X; Y /. The left and right circles stand for the individual entropies H.X/ and H.Y / respectively.</figDesc><table /><note><p>The Kullback-Leibler divergence is also frequently named Kullback distance, improperly, as this measure is not symmetric, i.e. D kl .P jjQ/ ¤ D kl .QjjP /. The main property is that D kl .P jjQ/ 0 with equality to zero if and only if P D Q. According to Equ.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head></head><label></label><figDesc>Áidoes not depend on i . Consequently, TE Y !X;i does not depend on i (and so can be simply denoted by TE Y !X ), nor all the quantities defined in Equ. (3.28) to (3.34). In theory, TE is never negative and is equal to zero if and only if Equ. (3.26) holds.</figDesc><table><row><cell>the expectation E</cell><cell>h g X i C1 ; X i .m/</cell><cell>.n/ i ; Y</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>, U 2 , U 3 and U 4 stand respectively for .X ; Y /, .X p ; X /, .X p ; X ; Y / and X . For each n, 6 log .p U .u n // is an estimated value of log .p U .u n // computed as a func-.U / is completely specified by the chosen estimation functions f n . Note that if, for N fixed, these functions correspond respectively to unbiased estimators of log .p .u n //, then 3 TE Y !X is also unbiased, otherwise we can only expect that 3However, the chapter 4 will present results not only on IID observations but also on non-IID stationary AR processes as our goal was to verify if some improvements can be nonetheless obtained for non-IID data such as AR data.</figDesc><table><row><cell>u 4n / ;</cell></row><row><cell>(3.42)</cell></row><row><cell>where U 1 If we come back to MI defined by Equ. (3.19) and compare it with Equ. (3.34), it is</cell></row><row><cell>obvious that estimating MI and TE shares similarities. Hence, similarly to Equ. (3.42)</cell></row><row><cell>for TE, a basic estimation 3</cell></row></table><note><p><p><p><p><p>tion f n .u 1 ; : : : ; u N / of the observed sequence u n , n D 1; : : : ; N . With the kNN approach, f n .u 1 ; : : : ; u N / explicitly depends only on u n and on its k NNs (nearest neighbors). So, the calculation of 1</p>HTE Y !X</p>is asymptotically unbiased (for N large). It is like that if the estimators of log .p U .u n // are asymptotically unbiased. Now, the theoretical derivation and analysis of the most currently used estimators 1 H.U / .u 1 ; : : : ; u N / D 1 N N X nD1 4 log .p.u n // (3.43)</p>for the estimation of H .U / generally suppose that u 1 ; : : : ; u N are N independent occurrences of the random vector U , i.e. u 1 ; : : : ; u N is an occurrence of an IID sequence U 1 ; : : : ; U N of random vectors (8i D 1; : : : ; N W P U i D P U ). Although the IID hypothesis does not apply to our initial problem concerning the measure of TE on stationary random sequences (that are generally not IID), the new methods presented in this thesis are extended from existing ones assuming this IID hypothesis (without relaxing it).</p>I .X; Y / of I .X; Y / from a sequence .x i ; y i /, i D 1; : : : ; N , of</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>Suppose that the total number of data points drawn independently from p X . / is N and that k points fall into the region L.x/, if N is large enough, this probability can be Moreover, if the diameter of L.x/ is small enough, p X .x/ can be considered as constant in this region. So Equ. (3.50) can be rewritten as As shown in Fig.3.3, in order to estimate p X .x/, there are two possible starting</figDesc><table><row><cell></cell><cell cols="2">P .X 2 L.x// D</cell><cell>Z</cell><cell cols="2">p X .y/dy</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">L.x/</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">p X .u/</cell><cell>Z</cell><cell>dy</cell><cell>(3.52)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">L.x/</cell></row><row><cell></cell><cell></cell><cell cols="4">D p X .u/V; u 2 L.x/;</cell></row><row><cell cols="6">where V is the volume of the region L.x/. Combining Equ. (3.51) and (3.52), we obtain</cell></row><row><cell></cell><cell>p X .u/</cell><cell cols="2">k N V</cell><cell cols="2">; u 2 L.x/:</cell><cell>(3.53)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>y/dy:</cell><cell>(3.50)</cell></row><row><cell>written</cell><cell cols="4">P .X 2 L.x//</cell><cell>k N</cell><cell>:</cell><cell>(3.51)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head></head><label></label><figDesc>], Zuo et al. tried to improve this method by introducing an adaptive bandwidth [Hwang 1994] to estimate the joint density probability with the highest dimension, 4 Inspired from the MI estimators proposed by Kraskov et al. (Equ. (3.63) and (3.64)), two different kNN TE estimators have been proposed afterwards. Applying the same strategy to estimate TE, the number of neighbors in the joint space S X p ;X ;Y is first fixed. Then, for each i , the resulting distance " i d .x p i ;x i ;y i /;k between .x</figDesc><table><row><cell>p i ; x i ; y i /</cell></row><row><cell>and its kth NN is projected into the three other lower-dimensional spaces, leading to the</cell></row><row><cell>following TE estimator [Vicente 2011,Lindner 2011,Wibral 2013,Wibral 2014a,Wollstadt</cell></row><row><cell>2014, Gómez-Herrero 2015] (Box 8 ○ in Fig 3.6)</cell></row></table><note><p>p X p ;X ;Y .x p i ; x i ; y i /.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head></head><label></label><figDesc>To apply this strategy, Kraskov proposed to obtain the distance in the joint space S X;Y , and then project the distance obtained into the marginal spaces S X and S Y . It should be mentioned that, until now, this strategy has been only implemented with the maximum norm.</figDesc><table><row><cell></cell><cell>Chapter 3</cell></row><row><cell>ideas.</cell><cell></cell></row><row><cell cols="2">(1) First, we can choose proper parameters for the estimation of individual entropies,</cell></row><row><cell>and try to tend towards the following approximation</cell><cell></cell></row><row><cell>B H .X / C B H .Y / B H .X; Y /:</cell><cell>(3.71)</cell></row><row><cell cols="2">In this case, the individual estimation bias would be cancelled out, and B I .X; Y / would vanish to zero. The MI estimator in Equ. (3.63) and the TE estimator (Equ.</cell></row><row><cell>(3.67) follow this idea.</cell><cell></cell></row><row><cell>C 1 H.Y / 3 H.X; Y /;</cell><cell>(3.69)</cell></row><row><cell cols="2">if we calculate the marginal and joint entropies on the right-hand side of Equ. (3.69)</cell></row><row><cell cols="2">separately, this introduces estimation bias for each term, denoted by B H .X/, B H .Y / and B H .X; Y / respectively. Then the bias for 2 I.X; Y / is expressed as</cell></row><row><cell>B I .X; Y / D B H .X/ C B H .Y / B H .X; Y /:</cell><cell>(3.70)</cell></row></table><note><p>Our goal is to reduce B I .X; Y / as much as possible. To this end, there are two basic (2) A second basic idea is to reduce B H .X/, B H .Y / and B H .X; Y / as much as possible but in a separate manner. For this purpose, we can calculate the individual entropies using the (hyper-)rectangle region instead of a (hyper-)cube. For the MI estimator in Equ. (3.64) (resp. Equ. (3.68) for TE), this idea is applied together with the first one.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head></head><label></label><figDesc>.6).</figDesc><table><row><cell>Extended TE estimators.</cell><cell></cell></row><row><cell>Firstly, a new analytical form of bias for the plug-in entropy estimator is introduced (Box 10 ○), and using this result, a relation leading to an optimal distance is developed for the estimation of mutual information and transfer entropy for two norms (Euclidean norm and maximum norm, Boxes 12 ○ and 13 ○ respectively). In the case of the maximum norm, this relation (Box 13 ○) actually provides the theoretical explanation for the kNN MI and TE mentioned previously (Boxes 5 ○ and 8 ○). In Box 12 ○, this relation is extended to Euclidean norm. With the relations in Boxes 12 ○ and 13 ○, we developed the "basic" estimators for both MI (Box 14 ○) and TE (Box 15 ○), using the kNN density estimator (Box 16 bility density estimation and entropy non-parametric estimation introduce, around each data point, a minimal (hyper-)cube (Box 1 ○), which includes the first k NNs, as it is the case for two already developed entropy estimators, namely the well-known Kozachenko-Leonenko estimator (Box 2 ○) and the less commonly used Singh's estimator (Box 3 ○). The idea of rectangle extends the idea of the product of cubes (Box 6 ○). It consists in proposing a different construction of the neighborhoods, which are no longer minimal (hyper-)cubes, nor products of (hyper-)cubes, but minimal (hyper-)rectangles (Box 20 ○), with possibly a different length for each dimension, to get two novel entropy estimators (Boxes 21 ○ and 22 ○), respectively derived from the Kozachenko-Leonenko entropy estima-11 Density estimator Equ. (3.97) The idea of cube Kozachenko-Leonenko entropy estimator Equ. (3.60) Singh's entropy estimator Equ. (3.61) The idea of rectangle TE estimator Equ. (3.67) Standard Algorithm TRENTOOL Max. norm Entropy estimator Equ. (3.172) Entropy estimator Equ. (3.150) TE estimator Equ. (3.177) TE_p2 TE estimator Equ. (3.176) TE_p1 1 2 3 4 5 6 TE estimator Equ. (3.68) Extended Algorithm JIDT Max. norm The idea of projected distances The idea of product of cubes KSG MI estimator 1 Equ. (3.63) MILCA KSG MI estimator 2 Equ. (3.64) MI estimators TE estimators Max norm 7 8 9 10 12 13 14 15 New bias expression Equ. (3.92) and (3.95) Optimized distance relation for max. norm Equ. (3.104) and (3.111) Optimized distance relation for Euclid. norm Equ. (3.103) and (3.110) Density estimator Equ. (3.116) Basic MI estimator Equ. (3.106) Max./Euclid. norm Basic TE estimator Equ. (3.113) Max./Euclid. norm Mixed MI estimators Equ. (3.125) Max./Euclid. norm Mixed entropy estimator Equ. (3.124) 18 17 19 21 22 23 24 Mixed TE estimators Equ. (3.126) Max./Euclid. norm 20 16</cell><cell>Figure 3.6: Synthetic overview of the different concepts and estimators. References to section 3.2 are marked in green.</cell></row></table><note><p><p><p><p><p>○). Note that the optimized distance relations are developed only under the independence assumption. To futher eliminate the bias for dependent X and Y , novel MI and TE estimators, named "mixed" estimators, are also introduced (Boxes 18 ○ and 19 ○).</p>For point (2) (marked as a blue arrow in Fig.</p>3</p>.6), there is one extra question.</p>In [Kraskov 2004], the development for the idea of product of cubes is based on the Kozachenko-Leonenko entropy estimator (Equ. (3.60)) using the maximum norm. After some extra mathematical development, this idea can be extended to a more general case (idea of rectangle, Box 20 ○ in Fig. 3.6), where the determined region in the joint space is a rectangle (the side lengths in two different dimensions can be different). Is it possible to derive the new kNN entropy estimator with the idea of rectangle based on Singh's entropy estimator (Equ. (3.61))? This point is discussed in section 3.4 (marked as a red dotted box in Fig. 3.6). The standard kNN methods using maximum norm for probator and Singh's entropy estimator . These two new entropy estimators lead respectively to two new TE estimators (Boxes 23 ○ and 24 ○) to be compared with the Standard and</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_16"><head></head><label></label><figDesc>L.x// corresponds to an estimation of the probability that X belongs to the set L.x/. If we assume that P .X 2 L.x// is perfectly known (but not p X .x/), we</figDesc><table><row><cell>v.x/ D As mentioned in section 3.2.1, in most existing density estimation algorithms, includ-Z L.x/ du: (3.72) ing either KDE with the Parzen window or kNN, p X .x/ is estimated as c p X .x/ D 6 P .X 2 L.x// v.x/ D 7 R L.x/ p X .y/dy v.x/ ; (3.73) where 6 can use the following approximation log p X .x/ log Â P .X 2 L .x// v .x/ Ã D log R L.x/ p X .y/ dy v .x/ ! : (3.74) Given Equ. (3.73), an estimation 4 log p X .x/ of log p X .x/ is introduced 4 log p X .x/ D log c p X .x/ D log 6 P .X 2 L.x// v.x/ D log R L.x/ p X .y/dy v.x/ C " ! ; (3.75) where the random estimation error " given by " D 7 R L.x/ p X .y/dy v.x/ R L.x/ p X .y/dy v.x/ (3.76) P .X 2 Chapter 3 is zero mean when 6</cell></row></table><note><p>X , we introduce the volume (Lebesgue measure) of L.x/ P .X 2 L.x// is unbiased.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_20"><head></head><label></label><figDesc>the kNN entropy estimator proposed by Singh (Equ. (3.61)), Equ. (3.96) could</figDesc><table><row><cell cols="4">be directly derived. Therefore, using Equ. (3.75), (3.79) and (3.96), we consider the</cell></row><row><cell cols="3">following generic density estimator (Box 11 ○ in Fig. 3.6)</cell><cell></cell></row><row><cell>2 p U .u i / D</cell><cell>e .k/ N v i</cell><cell>;</cell><cell>(3.97)</cell></row><row><cell cols="4">and the kNN entropy estimators mentioned previously (Equ. (3.60) and (3.61)) can</cell></row><row><cell cols="4">be considered as estimators with the same structure as in Equ. (3.79), and so can be</cell></row><row><cell>discussed under our bias analysis framework.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_21"><head></head><label></label><figDesc>X , d Y and d X;Y are the dimensions of the signals X, Y and .X; Y /, and R.x i /, R.y i / and R.x i ; y i / are the distances used for the estimation of c p X .x i /, c p Y .y i / and 1 p X;Y .x i ; y i /, respectively. Here, p X .x i /, p Y .y i / and their corresponding second-order derivatives are unknown, so to get Equ. (3.101) equal to zero, the following sufficient</figDesc><table><row><cell>pair of conditions is derived</cell></row><row><cell>8</cell></row></table><note><p><p><p>y 2 .y i / Á p Y .y i / ;</p>(3.101)</p>where d</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_23"><head></head><label></label><figDesc>log 1 NN, also denoted R k .x i ; y i /), this estimator is denoted by 2</figDesc><table><row><cell>3.3. First Improvement</cell><cell></cell><cell>Chapter 3</cell></row><row><cell cols="2">I.X; Y / k basic with</cell><cell></cell></row><row><cell>3</cell><cell></cell><cell></cell></row><row><cell>p X;Y .x i ; y i /</cell><cell>Ã</cell><cell>(3.105)</cell></row><row><cell cols="3">with an (approximately) zero bias by choosing R.x i ; y i / and properly defining R.x i / and R.y i / using Equ. (3.103) or (3.104). When R.x 63</cell></row></table><note><p>i ; y i / results from the kNN approach (i.e. when R.x i ; y i / D kkNN.x i ; y i / .x i ; y i /k is the distance from .x i ; y i / to its kth</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_24"><head></head><label></label><figDesc>.109) where R.x i ; y i /, R.x ; y i / and R.x i / are the distances used for the estimation of 2 p X i ;Y i .x i ; y i /, 2 X i .x i / at the i th point, d . / is the dimension of the corresponding space. After simplification,</figDesc><table><row><cell>p i ; x i /, R.x i ; x p X p p i ;X i i ; x Equ. (3.109) leads to (Box 12 .x p i ; x i /, 5 p X p i ;X i ;Y i .x p ○ in Fig. 3.6)</cell></row><row><cell>8</cell></row><row><cell>&lt; : R.x i ; y</cell></row></table><note><p>i i ; y i / and b p</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_25"><head></head><label></label><figDesc>Until now, L.x/ was built either from a KDE approach or a kNN approach. In the first case, R.x/ depends on the imposed bandwidth, and, in the second case, R.x/ is deduced from the kth NN. Hereafter, to carry on with the conjecture proposed in [Kraskov 2004], we only consider the kNN approach integrating two steps, (i) the choice of two different numbers of neighbors k 1 and k 2 , (ii)</figDesc><table><row><cell>/ .x i / where ˛i , i D 1; : : : ; N is a sequence of weighting coefficients to be determined, c Á ! ; (3.114) p X .1/ . / and c p X .2/ . / are two density estimations with the same structure 5 R L.x/ p X .y/dy v.x/ ! ob-3.3. First Improvement the definition of the probability density estimators, 8 &lt; : c p X .1/ .x i / D b p k 1 .x i / c p X .2/ .x i / D b p k 2 .x i /; (3.115) where b p k j .x/ D k j 1 N v k j .x/ ; j D 1; 2 (3.116) is the standard kNN density estimator as defined in [Fukunaga 2013]. The volume v k .x/ is equal to the Lebesgue measure of L k .x/ D fy W ky xk Ä R k .x/g ; (3.117) tained from two distinct definitions of L. /. Chapter 3 and R</cell></row></table><note><p>k .x i / is the distance between x i and its kth NN.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_28"><head></head><label></label><figDesc>123) with u replaced by x, y or .x; y/, are identical if we choose R 2 k 1 , R 2 k 2 with the constraint imposed by Equ. (3.103) (or Equ. (3.104)).</figDesc><table><row><cell>Developing Equ. (3.121) with the substitution ˛x i D ˛y i D</cell><cell>˛.x;y/</cell></row></table><note><p>i D ˛i , we have (Box 17 ○ in Fig. 3.6)</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_29"><head></head><label></label><figDesc>using Equ. (3.103) or (3.104) (depending on the norm) and determine the numbers of points kk 1 .x i /, k k 1 .y i /, k k 2 .x i /and k k 2 .y i / falling into the corresponding regions (iii) Estimate H.X / and H.Y / with Equ. (3.124), where ˛i is given by Equ. (3.123), H.X; Y / being calculated similarly (with k k 1 .x i ; y i / D k 1 and k k 2 .x i ; y i / D k 2 )</figDesc><table><row><cell>Chapter 3</cell><cell>3.4. Second Improvement</cell></row><row><cell cols="2">Note that 3 I .X; Y / k basic is obtained by replacing Equ. (3.125) by Equ. (3.106) in step (iii).</cell></row><row><cell cols="2">and then calculate 2 I.X; Y / k 1 ;k 2 mixed by Equ. (3.125). The resulting estimator is named "mixed estimator" and denoted by 2</cell></row></table><note><p>I.X; Y / mixed;E for the Euclidean norm and 2 I.X; Y / mixed;M for the maximum norm.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_31"><head></head><label></label><figDesc>[Kraskov 2004]  the equality E log h x i D x i ;k</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">D .k/</cell><cell>.N /</cell><cell>(3.152)</cell></row><row><cell cols="7">obtained for a (hyper-)cube is extended for the case d &gt; 2 to</cell></row><row><cell></cell><cell></cell><cell cols="5">E log h x i . 1 ; : : : ; d / D .k/</cell><cell>d 1 k</cell><cell>.N /:</cell><cell>(3.153)</cell></row><row><cell cols="6">So, if p X is approximately constant on D " 1 ;:::;" d x i</cell><cell>we get</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">h x i ." 1 ; : : : ; " d / ' v i p X .x i / ;</cell><cell>(3.154)</cell></row><row><cell>where v i D</cell><cell>R D</cell><cell>" 1 ;:::;" d x i</cell><cell cols="4">d d . / is the volume of the (hyper-)rectangle, and we obtain</cell></row><row><cell></cell><cell></cell><cell></cell><cell>log p X .x i /</cell><cell>.k/</cell><cell>.N /</cell><cell>d 1 k</cell><cell>log .v i / :</cell><cell>(3.155)</cell></row></table><note><p>Finally, by taking the experimental mean of the right term in Equ. (3.155) we obtain an estimation of the expectation E OElog p X .X/, i.e. Equ. (3.150).</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_33"><head></head><label></label><figDesc>and the pth component x i .p/ of x i . For each p, let us introduce J i .p/ 2 f1; : : : ; kg defined by D i .p; J i .p// D max .D .e. when the kth NN is systematically the farthest from the central point x i for each of the d directions, then all the entries in the last column of B i are equal to one while all other entries are equal to zero: we have only one column including values different from zero and, so, only one point on the border ( i D 1), what generalizes the case depicted in Fig. 3.7(b) for d D 2.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>j i .p/</cell></row><row><cell cols="4">j i (2) We can define i as the number of non-null column vectors in B i . For example, if of x</cell></row><row><cell>the kth NN x k i is such that</cell><cell></cell><cell></cell><cell></cell></row><row><cell>8j ¤ k; 8p D 1; : : : ; d W</cell><cell>ˇxj i .p/ x i .p/</cell><cell>ˇ&lt; ˇxk i .p/ x i .p/ ˇ;</cell><cell>(3.175)</cell></row></table><note><p>j D 1; : : : ; k, the k NNs of x i 2 R d and let us consider the d k array D i such that for any .p; j / 2 f1; : : : ; d g f1; : : : ; kg, D i .p; j / D ˇxj i .p/ x i .p/ ˇis the distance (in R) between the pth component x i .p; 1/; : : : ; D i .p; k// (3.173) and which is the value of the column index of D i for which the distance D i .p; j / is maximum in the row number p. Now, if there exists more than one index J i .p/ which fulfills this equality, we select arbitrary the lowest one, hence avoiding the max. / function to be multi-valued. The MATLAB implementation of the max. / function selects such a unique index value. Then, let us introduce the d k Boolean array B i defined by 8 &lt; : B i .p; j / D 1; if j D J i .p/; B i .p; j / D 0; otherwise: (3.174) Then (1) The d sizes " p , p D 1; : : : ; d of the (hyper-)rectangle D " 1 ;:::;" d x i are equal respectively to " p D 2D i .p; J i .p//, p D 1; : : : ; d . i</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_34"><head></head><label></label><figDesc>and, with Equ. (3.172), our second proposed estimator named TE p2 (Box 24 ○ in Fig. 3.6) is written</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_35"><head></head><label></label><figDesc>For the comparison of different mutual information estimators, we first consider the independence situation i.e. we first generated two independent d -dimensional IID random sequences .X t / t and .Y t / t such that both X t and Y t followed a zero mean Gaussian distribution N .0; C/, where C was a Toeplitz matrix with first line OE1; ˛; : : : ; ˛d 1 . For our simulations, we used sequences of N independent samples .X t ; Y t / t D1;:::;N . This model is named as Model 1:Clearly, whatever the value of ˛2 OE0; 1OE, the mutual information I.X t ; Y t / is theoretically equal to zero.Additionally, in order to briefly investigate the effect of non-independence on the bias of MI estimation when applying the different strategies, we also considered two dependence situations. First, we replaced the independent pairs .X t ; Y t / mentioned above (see Model 1) by dependent pairs, .X t ; Y 1t /, where X t was the same as previously and Y t was replaced by Y 1t :The parameter Â , Â 2 0; 2 , allowed to tune the dependence between X t and Y 1t .Note that, for Â D 2 , X t and Y 1t are independent. This model is named as Model 2 and the theoretical value of I.X t ; Y 1t / is equal to d log .sin Â/. The derivation of this theoretical value can be found in Appendix F.In a second dependence situation, data samples .X t / t D1;:::;N and .Y t / t D1;:::;N were generated by the following linear model more specifically to focus on the impact of the increase in dimension of the simulated random vectors X t and Y t . To this end, for both of them, their components were mutually independent. This model is denoted Model 3: Random sequences X and e independent; .X t / t D1;:::;N W IID; .e t / t D1;:::;N W IID and e t were two independent d -dimensional random vectors, and both of them followed a zero mean Gaussian distribution N .0; I / (I is the identity matrix, and in this case, it is easier to test the effect of dimensionality). ˇis a scalar coefficient. Clearly, when ˇdecreases, the dependence between X t and Y t increases. The theoretical value of mutual information I.X t ; Y t / is equal to d</figDesc><table><row><cell>Model 3</cell><cell></cell></row><row><cell>X t N .0; I /; e t N .0; I /</cell><cell></cell></row><row><cell>Y t D X t C ˇ e t ; ˇ2 R</cell><cell></cell></row><row><cell>4.1.1.1. Models for MI Estimation</cell><cell></cell><cell>(4.3)</cell></row><row><cell>where X t 2 log 1Cˇ2 ˇ2 Á</cell><cell>as detailed in Appendix G.</cell></row><row><cell>Model 1 4.1.1.2. Models for TE Estimation</cell><cell></cell></row><row><cell cols="2">X and Y independent; .X t / t D1;:::;N W IID; .Y t / t D1;:::;N W IID X t N .0; C/; Y t N .0; C/; C D toeplitz 1; ˛; : : : ; ˛d 1 Á</cell><cell>(4.1)</cell></row><row><cell>Model 2</cell><cell></cell></row><row><cell cols="2">X and Y as in Model 1 Y 1t D cos Â X t C sin Â Y t ; t D 1; : : : ; N</cell><cell>(4.2)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_36"><head></head><label></label><figDesc>With this model, we aimed at estimating H.X t jY t / H.X t jY t ; Z t / to test if the knowledge of Y t and Z t could improve the prediction of X t compared to only the knowledge of Y t . The triplet .X t ; Y t ; Z t / corresponds to the triplet X p t ; X t ; Y t introduced previously in chapter 3 to define transfer entropy. Here the theoretical value of TE is H .X t jY t / H .X t jY t ; Z t / D H .bZ t C W t / H .W t /, i.e. TE D H N 0; bC Z b T C 2 either two or three one-dimensional signals. For both models, there exists a bidirectional relation between each pair of signals. The PSD of these signals can be found in Appendix H. The models coefficients have been tuned in order to obtain signals displaying narrow bounds PSD shapes that can be retrieved in real epileptic signals. to illustrate the interest of entropic methods versus Granger causality in nonlinear situations. An a priori natural approach should have been to consider nonlinear VAR processes. Now, with this type of model the theoretical computation of TE values should have been cumbersome. With the proposed model we were able to obtain a precise numerical approximation of the theoretical value of TE. The parameter K allowed to weight the influences of .Y t ; Z t /. The parameter rep was a weighting coefficient to modify the influences of Y t and Z t . In this model, .X t ; Y t ; Z t / An illustration of the nonlinear statistical link between X t and Z t is given in Fig. 4.1. The visualized dimensional distribution clearly illustrates that a strong statistical dependence does not prevent from a null correlation, leading to a Granger causality index equal to zero.</figDesc><table><row><cell>4.1. Database</cell><cell>Chapter 4</cell></row><row><cell cols="2">The first vectorial AR model (marked as Model 5) was as follows: Model 5 e X ; e Y ; X; Y W random real scalar sequences e X ; e Y W independent N .0; 1/ white sequences 8 &lt; : X t D 0:45 p 2X t 1 0:9X t 2 0:6Y t 2 C e X;t Y t D 0:6X t 2 0:175 p 2Y t 1 C 0:55 p 2Y t 2 C e Y;t ; t D 1; : : : ; N The second vectorial AR model (marked as Model 6) was given by: Model 6 e X ; e Y ; e Z ; X; Y; Z W random real scalar sequences e X ; e Y ; e Z W 3 independent N .0; 1/ white sequences 8 &lt; : X t D 0:25X t 2 0:35Y t 2 C 0:35Z t 2 C e X;t Y t D 0:5X t 1 C 0:25Y t 1 0:5Z t 3 C e Y;t Z t D 0:6X t 2 0:7Y t 2 0:2Z t 2 C e Z;t ; t D 1; : : : ; N In Section 4.2, in order to estimate both TE and Granger causality index, the predic-(4.5) (4.6) tion orders m and n will be equal to the corresponding regression orders of the AR models. For example, when estimating TE Y !X , we set m D 2, n D 2 and X p t ; X t ; Y t corre-sponds to X t C1 ; X .2/ t ; Y .2/ t Á . As the 3 stochastic processes X; Y; Z are jointly Gaussian distributed the 6 theoretical TE values can be obtained from theoretical calculation of the corresponding Granger causality indexes. These latter can be computed from theoretical covariances of .X; Y; Z/ or from their estimation on a sufficiently large time interval. The following Model 7 was introduced as an example of nonlinear causal contribution of Y i onto X p W; Y; Z W 3 independent N .0; 1/ scalar real white sequences Y t N .0; 1/ ; Z t N .0; 1/ ; W t N .0; 1/ uniformly distributed on OE0; Â R X t D K Â rep Y t C q 1 rep 2 Z 2 t Ã C W t ; t D 1; : : : ; N; K 2 R; rep 2 R (4.7) corresponds to X p t ; X t ; Y t in chapter 3, i.e. to X t C1 ; X .n/ t ; Y .m/ t Á with m D n D 1. -3 -2 -1 0 1 2 3 -2 0 2 6 8 10 Z values i Model 7 4 X values</cell></row><row><cell></cell><cell>W</cell></row><row><cell>H N 0; 2 W</cell><cell>and can be easily computed.</cell></row><row><cell cols="2">The two following models (Model 5 and Model 6) are two VAR models made up of</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_37"><head></head><label></label><figDesc>To compute H.U t / we first obtain an approximation Q p U t of the probability density function of U t by convolving numerically the respective density functions of W t and K Then, given a large number N MC of realizations .z t ; w t / of .Z t ; W t / we computed the following Monte Carlo approximation:</figDesc><table><row><cell>4.1. Database</cell><cell></cell><cell>Chapter 4</cell></row><row><cell>(the theoretical density function of Z 2 t is known as Z t</cell><cell>N .0; 1/).</cell></row><row><cell></cell><cell>q</cell><cell>1 rep 2 Z 2 t</cell></row><row><cell>91</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_38"><head></head><label></label><figDesc>2.2)On remarque que ce système est d'ordre 11. Or si on dénombre les fonctions de transfert d'ordre 2 des filtres pré-somatiques dans la figure 2.9 on en trouve 7, ce qui devrait mener à l'introduction de 14 composantes d'état. Cependant on constate que les 3 fonctions de transfert Ah e apparaissant sur la droite de la figure 2.9 peuvent être fusionnées à la simple condition de les commuter avec les gains C 1 , C 3 , C 5 , ce qui économise 4 composantes d'état. Les vecteurs θ et γ regroupent respectivement les paramètres inconnus et les paramètres connus. Les paramètres liés aux efficacités de l'excitation et des deux types d'inhibition (A, B, G) sont à placer dans θ mais il faut évidemment leur</figDesc><table><row><cell>La fonction de transfert du filtre passe-haut de sortie étant d'ordre 1 elle ne nécessite qu'une composante d'état complémentaire et on arrive ainsi à 14-4+1=11 pour la dimension du vec-teur d'état et celle du système différentiel. En notant W (t)dt = dβ(t), où β(t) désigne un processus brownien de coefficient de diffusion égal à un (voir aussi section 3.2), le système peut alors s'écrire sous la forme d'une équation différentielle stochastique à 11 composantes</cell></row><row><cell>dX(t) = f (X(t), θ, γ) + D(γ)d(β(t)).</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_39"><head></head><label></label><figDesc>a second population Pop Y . In addition, this connection from population Pop X to Pop Y Table 4.1: Example of model constants of hippocampus.</figDesc><table><row><cell>Chapter 4</cell><cell></cell><cell>4.1. Database</cell></row><row><cell></cell><cell cols="2">Synaptic time constants</cell></row><row><cell>1=a</cell><cell>Excitatory</cell><cell>1/100</cell></row><row><cell cols="2">1=b Slow inhibitory</cell><cell>1/30</cell></row><row><cell cols="2">1=g Fast inhibitory</cell><cell>1/350</cell></row><row><cell></cell><cell cols="2">Connectivity constants</cell></row><row><cell>C 1</cell><cell>p e p e</cell><cell>135</cell></row><row><cell>C 2</cell><cell>p e p e</cell><cell>108</cell></row><row><cell>C 3</cell><cell>p e p si</cell><cell>33.8</cell></row><row><cell>C 4</cell><cell>p si p e</cell><cell>33.8</cell></row><row><cell>C 5</cell><cell>p e p f i</cell><cell>40.5</cell></row><row><cell>C 6</cell><cell>p si p f i</cell><cell>13.5</cell></row><row><cell>C 7</cell><cell>p f i p e</cell><cell>121.5</cell></row><row><cell cols="3">White Gaussian noise (input)</cell></row><row><cell>m p</cell><cell>mean</cell><cell>90</cell></row><row><cell></cell><cell>diffusion</cell><cell>30</cell></row><row><cell></cell><cell>Sigmoid</cell><cell></cell></row><row><cell>e 0</cell><cell></cell><cell>2.5 s 1</cell></row><row><cell>v 0</cell><cell></cell><cell>6 mV</cell></row><row><cell>r</cell><cell></cell><cell>0.56 mV 1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_44"><head></head><label></label><figDesc>D 1024). For MI and TE, and for each pair festimator type, connectivity caseg boxplots were obtained for both original data and surrogate data as displayed in Fig.</figDesc><table><row><cell></cell><cell>1</cell><cell></cell></row><row><cell></cell><cell>0.8</cell><cell>Theoretical value</cell></row><row><cell>Transfer Entropy</cell><cell>0.4 0.6 0.2</cell><cell>GCi/2 Standard TE algorithm Extended TE algorithm Basic TE estimator (Max. norm) Basic TE estimator (Euclid. norm) TE p1 TE p2</cell></row><row><cell></cell><cell>0</cell><cell></cell></row><row><cell cols="3">For the physiology-based model, simulated signals with a Runge-Kutta numerical time step corresponding to a 512 Hz sampling frequency were generated. Surrogate data were obtained following the strategy described in section 4.1.2.2. For each set of surrogate data, both dependency and information flow between two populations were considered. Firstly, basic MI estimators with both maximum and Euclidean norms (Box 14 ○ in Fig. 3.6), together with the MILCA toolbox, were tested for independency (see Fig. 4.14 and Tab. 4.3). Secondly, Granger causality, Standard algorithm, Extended algorithm and the proposed algorithm TE p1 were tested to detect information flow (see Fig. 4.15 and Tab. 4.4). All indexes in this section were calculated on 100 trials with 1024-point length 362 512 724 1024 1448 2048 -0.2 signal length (a) 256 362 512 724 1024 1448 2048 -0.1 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 signal length Transfer Entropy Theoretical value GCi/2 Standard TE algorithm Extended TE algorithm Basic TE estimator (Max. norm) Basic TE estimator (Euclid. norm) TE p1 TE p2 (b) 0 0.2 0.4 0.6 0.8 1 -0.2 0 0.2 0.4 0.6 0.8 1 1.2 1.4 rep value Transfer Entropy Theoretical value GCi/2 Standard TE algorithm Extended TE algorithm Basic TE estimator (Max. norm) Basic TE estimator (Euclid. norm) TE p1 TE p2 signals (N 256 (c)</cell></row></table><note><p><p><p>4.</p>14 and Fig. 4.15.  </p>In cases 1 and 2 (introduced in section 4.1.2), Pop X and Pop Y are</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_46"><head>Table 4</head><label>4</label><figDesc>Compared to Standard and Extended algorithms, both TE p1 and TE p2 involve supple-</figDesc><table><row><cell>mentary information, such as the maximum distance of the first kth nearest neighbor in</cell></row><row><cell>each dimension and the number of points on the border. However, most currently used</cell></row><row><cell>neighbor searching algorithms, such as kd tree (k-dimensional tree), ATRIA [Merkwirth</cell></row></table><note><p><p>2000], provide not only information on the kth neighbor, but also on the first .k 1/ nearest neighbors. So, in terms of computation cost, there is no significant difference .5: Computational cost of the different algorithms (seconds). This table refers to Model 5 (X ! Y ) using N D 1024 and 50 trials. Our implementation of Granger causality involves matrix inversion and an implementation based on MATLAB system identification toolbox would reduce the computation time. Theoretically, except for the Standard algorithm, TE algorithms require comparable computation times, their implementation being based on "for" loop structures, explaining slow process compared to the Standard algorithm.</p>among these kNN TE estimators. A practical order of magnitude of the relative computation times is given in Tab. 4.5 which provides the computation times required by the different algorithms according to the number of neighbors (except for Granger causality).</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_47"><head></head><label></label><figDesc>The mixed MI estimators provide good performance on dependent signals, but suffer from large bias in independence situations. It should be noted that the mixed estimators are sensitive to the choice of the number of neighbors and require some empirical tuning for practical use. To merge the respective advantages of basic MI estimators and mixed estimators a two-step procedure could be proposed to measure MI: in a first step the independence hypothesis would be tested with a basic type statistic and in case of independence rejection MI would be measured with a mixed type statistic.Concerning transfer entropy, for Gaussian distributions, experimental results show the effectiveness of the new estimators on IID data as well as on correlated AR signals in comparison with the standard KSG algorithm estimator. This conclusion still holds when comparing the new algorithms with the extended KSG estimator. Globally, all TE estimators satisfactorily converge to the theoretical TE value, i.e. to half the value of the Granger causality index, while the newly proposed TE estimators show lower bias for sufficiently large k (in comparison with the reference TE estimators) and comparable variance estimation errors. Now, one of the new TE estimators, TE p2 , suffers from noticeable error when the number of neighbors is small. Some experiments allowed us to verify that this issue already existed when estimating entropy of a random vector:</figDesc><table /><note><p><p><p>when the number of neighbors k falls below the value of dimension d , the bias drastically increases. As expected, experiments with Model 4 showed that all the TE estimators under examination suffer from "curse of dimensionality", which makes it difficult to obtain accurate estimation of TE with high dimension data. When tested on a model introducing a strong nonlinearity in the statistical link between the two simulated signals, the new algorithms did not longer display better performance than classical algorithms. This point would require deeper investigations, perhaps in line with existing works as</p><ref type="bibr" target="#b93">[Gao 2015</ref></p>]. When testing on the physiology-based model, the experiments did not show a clear advantage of TE approaches compared to the Granger causality. Now, as the nonlinearities included in this type of model are smooth nonlinearities, the performance of linear approach is not surprising.</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Comme mentionné précédemment, les méthodes basées sur la théorie de l'information, et plus particulièrement l'entropie de transfert, jouent un rôle essentiel dans la détection d'influences causales à partir de signaux d'observation. Lorsque ces derniers sont non stationnaires et que chaque mesure doit s'effectuer sur un intervalle d'observation relativement court (typiquement de l'ordre d'une à quatre secondes), la question qui se pose d'ores et déjà est la précision des estimations produites. Plus exactement, la pierre</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1"><p>This first improvement was the subject of our contribution in [Zhu</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_2"><p>2014, Zhu 2015b].</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_3"><p>This second improvement was the subject of our contribution in<ref type="bibr" target="#b278">[Zhu 2015a</ref>].</p></note>
		</body>
		<back>

			
			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="bibr" target="#b254">[Wendling 2005</ref><p>]. The symbols "+" and "-" represent excitatory and inhibitory afferences respectively.</p><p>constant effects are represented by linear transfer functions corresponding to 3 types of time continuous impulse responses: h e .t/ for the excitatory kinetics, h f i .t/ for the fast somatic inhibitory kinetics, h si .t / for the slow inhibitory kinetics. Sigmoidal functions S. / are also included in the subpopulations models, as a conversion law from mean neuronal membrane potential to mean rate of axonal action potentials. The impulse response G PH h ph (where G PH is the static gain) is that of an instrumentation high-pass filter, whose output is sampled at 256 Hz. Its transfer function is sG PH =.1 C s/ where s is the Laplace variable. The coefficients C i , i D 1; : : : ; 7, represent the average numbers of synaptic connections from a subpopulation to another. All impulse responses are of the form h.t / D ˛t exp. ˛t/, t 0, ˛being the inverse of a time constant, noted a for the excitation, b for the slow inhibition and g for the fast one. Their input-output relations can be characterized by a pair of first-order differential equations. The coefficients A, B, G represent synaptic efficiency (synaptic gains) for excitation, dendritic inhibition and somatic inhibition respectively. Only these three parameters are supposed to vary during a transition from a normal process to the epileptic seizure and are the ones to be tuned to simulate different types of activity, from healthy activity to paroxystic one.</p><p>Chapter 4</p><p>For the physiology-based model with two populations Pop X and Pop Y , we consider 3 situations: 1) Pop X has an influence on Pop Y (or the inverse, which corresponds to an unidirectional propagation), 2) both Pop X and Pop Y have influence on the other one (bidirectional propagation), 3) Pop X and Pop Y are independent. For these 3 cases, the values of the parameters A, B, G and g can be found in the following table.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2.2.">Surrogate Strategy</head><p>For the physiology-based model, for which the theoretical value of TE is not available for two connected populations, the situations are more complicated. As a matter of fact, for two populations Pop X and Pop Y generating signals X and Y for which we measure transfer entropy TE Y !X (given that a model of connectivity is available for these two populations), the problem is to evaluate the deviation from the H 0 hypothesis (X and Y independent), since it is difficult to obtain a theoretical distribution of TE Y !X under H 0 . This difficulty can be overcome using surrogate data synthesized from the original data and guaranteeing their independence to get a reference statistics under H 0 . To this end, we must develop a strategy to modify the observed signals to make them independent while preserving their marginal frequential characteristics (the variance of any statistics computed from these observations depends on these characteristics). In this work, surrogate pairs were generated with the following strategy. Let us give a series of M independent realizations .x m ; y m /, m D 1; : : : ; M , obtained with the same model (same structure, same parameters). To build independent pairs .X; Y 0 / preserving marginal laws, we introduced new pairs .X m ; Y m 0 / where m ¤ m 0 . According to this strategy, two different sets of TE values can be obtained, TE Y !X and TE Y 0 !X . If there is an influence from signal Y to signal X, the distributions of TE Cp9, Pp8, Dp1, Dp5, Tp1, Fp2 Not-involved group (group N) Ap11, Bp6, Bp11, Tp8, Hp2, Ip2, Fp8 <ref type="table">Table 5</ref>.1: Categories of the channels in the database. Each channel corresponds to a bipolar signal. For instance, Api represents the difference between the potentials recorded respectively from the i th sensor of electrode A' and its .i C 1/th sensor. The two red vertical lines separate this recording into three segments: pre-ictal phase (0s 20s), ictal phase (20s 52s) and post-ictal phase (52s 72s).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Method</head><p>The idea is to determine from the 20 recorded signals and a current observation time interval if a given channel can be considered as belonging to either a brain region implied in the seizure initialization mechanisms, or a secondary region relaying the seizure propagation, or a region not involved in the epileptic activities. To this end, we must introduce tal 1, ictal 2 and ictal 3 epochs (see Tab. 5.2)), T e including all the time indices necessary to cover the complete duration of e. The first argument of BXP , CI , indicates the type of causality index (Granger index, one of the TE indexes or DTF index). Concerning the analysis of DTF results, the corresponding boxplots are denoted BXPDTF .s; T e /. For TE and DTF, we choose to use an adaptive threshold to retain the same number of significant links as for Granger causality for each t value. Fig. <ref type="figure">5</ref>.3 gives an example on this point. This method is clearly not completely satisfying but it avoids making surrogate replications, which is time consuming for TE estimators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2.">Experimental Protocol</head><p>After building the boxplots, a Student's t-test is used to determine if the result obtained on each interval (i.e. pre-ictal, ictal 1, ictal 2, ictal 3, post-ictal) is significantly different from that on the baseline or not.</p><p>To compute the sizes of the past values vectors in Granger and TE causality index, and also to choose the order of the multivariate AR model in DTF we must estimate the Markovian memory length of the bivariate observed signal (suppose that the model is approximately a Markov process). For the selection of this model order, it is not obvious to find an optimal order using the widely used AIC and BIC criteria, as displayed in Fig.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.4.</head><p>Theoretically the estimated order corresponds to the value on the x axis for which the criterion value is minimum but here this x value can be too large to be used in the estimation algorithms. However we observe that the curves slopes are very small when the When we analyze Fig. <ref type="figure">5</ref>.5 and Tab. 5.3, we come to the following conclusions:</p><p>(i) the Standard algorithm fails in reflecting any variation during the first ictal epoch (ictal 1) for group O, and, on the contrary, there is a significant LCI values decrease for ictal 2 and ictal 3 epochs. Besides, it shows important changes for group N (significant decrease for pre-ictal, ictal 1, ictal 3 and post-ictal epochs); (ii) the Extended algorithm is globally not better than the Standard one for group O (no significant changes during the ictal epochs), and it leads to poor performance on group N (unexpected significant decrease on all epochs); (iii) among the three TE algorithms, the proposed algorithm demonstrates the best performance. Firstly, there is a significant increase for group O on the three ictal epochs. Secondly, for group N, the decrease for all epochs is not globally</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>A. Derivation of Equ. (3.89)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Proof of Property 1</head><p>Let us introduce the (hyper-)rectangle D is imposed to be equal to v r ). Finally, we can write Here, for the sake of convenience, we assume c D 0.</p><p>Given an observed N -length data sequence .X 1 ; X 2 ; : : : ; X N /, using the least-squares method [Brockwell 2013], we can fit a q-order VAR model, leading to an estimation of this model,</p><p>Following the model (Equ. (E.1)), we write the predicted value c X t as</p><p>Then, the experimental residual covariance matrix of the input noise of the VAR model can be calculated as</p><p>where T stands for the transpose operator.</p><p>Note that the above procedure can be carried out with any selected order q. For a given q, the AIC value is defined as</p><p>where log. / is the natural logarithm, and det. / stands for the matrix determinant.</p><p>Similarly, the BIC value can be calculated as</p><p>For practical use, we compute the AIC (or BIC) values for a given limited set of q values, and the selected value of q is the one leads to the minimum AIC (or BIC) value. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Time in seconds</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. Comparison between Entropy Estimators</head><p>Here, we try to explain the behavior of the estimator TE p2 in Fig. <ref type="figure">4</ref>.12(b).  Appendix some apparently surprising results obtained with this estimator in the estimation of TE, as reported in Fig. <ref type="figure">4</ref>.12(b). TE estimation is a sum of four separate vector entropy estimations, 3</p><p>Here, the dimensions of the four vectors are d H .X ; Y /, 5 H .X p ; X /, 2 H .X /, the numbers of neighbors to consider are generally larger than 3 (as a consequence of Kraskov's technique which introduces projected distances) and d Ä 5, so that we do not expect any underestimation of these terms. So, globally, when summing the four entropy estimations, the resulting positive bias observed in Fig. <ref type="figure">4</ref>.12(b) is understandable.</p><p>J. DTF Algorithm Used in Chapter 5</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>J. DTF Algorithm Used in Chapter 5</head><p>In this appendix, we give brief mathematical descriptions of the DTF index used as a reference index in chapter 5. Let X .t / D OEx 1 .t/ ; x 2 .t / ; : : : ; x d .t/ T denote a d -dimensional (d channels) vector process. An order p VAR model of X is given by</p><p>where E stands for an innovation process vector (which is ideally a random white process).</p><p>Representing Equ. (J.1) in the frequency domain, we get</p><p>where A .f / D P p nD0 A .n/ e i2 n.f =f s / , f s is the sampling frequency, and A .0/ D I (I being the identity matrix). Equ. (J.2) can be we rewritten as</p><p>where H is the transfer matrix from the innovation process to the observed process. The basic DTF from channel i to channel j at frequency f was defined in <ref type="bibr" target="#b123">[Kamiński 1991]</ref> as follows</p><p>To compare this frequency domain index with Granger causality and transfer entropy, we sum it over a chosen frequency band OEf 1 ; f 2 and then normalize the sum. Finally, the DTF index (from channel i to channel j ) we used in chapter 5 is defined and denoted as</p><p>For the real signals (sampled at 256 Hz), OEf 1 ; f 2 f0; : : : ; f s =2g is set to OE0; 128 and DTF i !j is calculated on a frequency grid with a 1 Hz step. Note that 0 Ä DTF i !j Ä 1, and DTF i !j can be considered as a causality index which reveals a direct effective connectivity which is not sensitive to spurious indirect causality paths (in the connectivity graph built on the d nodes, corresponding to the d channels). Indeed, Equ. (J.1) corresponds to a global model which takes into account the contextual channels set including the d components except for the two components i and j .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>K. Independence Test for Granger Causality</head><p>In this appendix, we develop the chi-square threshold for Granger causality. First of all, we make an introduction to the notations used in this appendix:</p><p>• N is the number of sample points extracted from each of the 2 time series X and Y .</p><p>• b 2 X jX is the empirical prediction error obtained with an L order AR model for X alone (marked as model ( <ref type="formula">1</ref>)).</p><p>• b 2 X jX;Y is the empirical prediction error obtained when X is predicted from both its own L order past and the L order past of Y (marked as model ( <ref type="formula">2</ref>)).</p><p>XjX;Y .</p><p>• GC log . / is the Granger causality index, while GW N . 1/ defines another statistic.</p><p>Under the H 0 hypothesis, i.e. if X and Y are stochastically independent, then the probability distribution of the statistic GW can be approximated by a 2 L distribution (centered chi-square distribution with parameter L) when N is sufficiently large (see for example <ref type="bibr" target="#b110">[Hlaváčková-Schindler 2007]</ref>). Note that it is easy to compute GW from GC as, clearly, we have GW D N e GC 1 .</p><p>It is easy to test the hypothesis H 0 corresponding to the acceptation of model ( <ref type="formula">1</ref> Finally the following procedure can be proposed to test H 0 from K non-overlapping time windows:</p><p>• Determine K .˛0/ such that P 2 LK &gt; K .˛0/ D ˛0;</p><p>• Compute GW </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Information theory and coding. McGraw-Hill electronic sciences series</title>
		<author>
			<persName><forename type="first">N</forename><surname>Abramson ; Abramson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1963">1963. 1963</date>
			<publisher>McGraw-Hill</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A resilient, low-frequency, small-world human brain functional network with highly connected association cortical hubs</title>
		<author>
			<persName><forename type="first">S</forename><surname>Achard ; Achard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salvador</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Whitcher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Suckling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bullmore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of neuroscience</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="63" to="72" />
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Dynamics of activity and connectivity in physiological neuronal networks</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Adhikari ; Adhikari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Epstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dhamala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aertsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Preissl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Aho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Derryberry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Peterson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Nonlinear dynamics and neuronal networks</title>
		<meeting><address><addrLine>Aho</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1991">2013. 2013. Aertsen 1991. 1991. 2014. 2014</date>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page" from="631" to="636" />
		</imprint>
	</monogr>
	<note>Ecology</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Maximum likelihood identification of gaussian autoregressive moving average models</title>
		<author>
			<persName><forename type="first">H</forename><surname>Akaike ; Akaike</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="255" to="265" />
			<date type="published" when="1973">1973. 1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Comprendre les réseaux cérébraux</title>
		<author>
			<persName><forename type="first">F</forename><surname>Alexandre ; Alexandre</surname></persName>
		</author>
		<idno>RR-8219</idno>
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
		<respStmt>
			<orgName>INRIA</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Research Report</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The relation between Granger causality and directed information theory: a review</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">O</forename><surname>Amblard ; Amblard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">J</forename><surname>Michel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Entropy</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="113" to="143" />
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Causal conditioning and instantaneous coupling in causality graphs</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">O</forename><surname>Amblard ; Amblard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">J</forename><surname>Michel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">264</biblScope>
			<biblScope unit="page" from="279" to="290" />
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Radial basis function approach to nonlinear Granger causality of time series</title>
		<author>
			<persName><forename type="first">N</forename><surname>Ancona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Marinazzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Stramaglia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">56221</biblScope>
			<date type="published" when="2004">2004. 2004</date>
			<pubPlace>Ancona</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Granger causality for circular variables</title>
		<author>
			<persName><forename type="first">L</forename><surname>Angelini ; Angelini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pellicoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Stramaglia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physics Letters A</title>
		<imprint>
			<biblScope unit="volume">373</biblScope>
			<biblScope unit="issue">29</biblScope>
			<biblScope unit="page" from="2467" to="2470" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Redundant variables and Granger causality</title>
		<author>
			<persName><forename type="first">L</forename><surname>Angelini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>De Tommaso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Marinazzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Nitti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pellicoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Stramaglia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">37201</biblScope>
			<date type="published" when="2010">Angelini 2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Causality estimates among brain cortical areas by partial directed coherence: simulations and application to real data</title>
		<author>
			<persName><forename type="first">L</forename><surname>Astolfi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Cincotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mattia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>De Vico Fallani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Salinari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Nocchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Baccalà</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ursino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zavaglia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Bioelectromagn</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2005">Astolfi 2005. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Assessing cortical functional connectivity by partial directed coherence: simulations and application to real data</title>
		<author>
			<persName><forename type="first">L</forename><surname>Astolfi ; Astolfi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Cincotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mattia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Marciani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Baccala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Fallani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Salinari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ursino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zavaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Babiloni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1802" to="1812" />
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Partial directed coherence: a new concept in neural structure determination</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Baccalá ; Baccalá</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sameshima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological cybernetics</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="463" to="474" />
			<date type="published" when="2001">2001. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Generalized partial directed coherence</title>
		<author>
			<persName><forename type="first">L</forename><surname>Baccald ; Baccald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>De Medicina</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">15th IEEE International Conference on Digital Signal Processing (DSP)</title>
		<imprint>
			<date type="published" when="2007">2007. 2007</date>
			<biblScope unit="page" from="163" to="166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Entropy of interval maps via permutations</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bandt ; Bandt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Pompe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nonlinearity</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1595" to="1602" />
			<date type="published" when="2002">2002. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Permutation entropy: a natural complexity measure for time series</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bandt ; Bandt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Pompe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review Letters</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">17</biblScope>
			<biblScope unit="page">174102</biblScope>
			<date type="published" when="2002">2002. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Granger causality and transfer entropy are equivalent for gaussian variables</title>
		<author>
			<persName><forename type="first">L</forename><surname>Barnett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Seth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical review letters</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="issue">23</biblScope>
			<biblScope unit="page">238701</biblScope>
			<date type="published" when="2009">Barnett 2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Behaviour of Granger causality under filtering: theoretical invariance and practical application</title>
		<author>
			<persName><forename type="first">L</forename><surname>Barnett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Seth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of neuroscience methods</title>
		<imprint>
			<biblScope unit="volume">201</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="404" to="419" />
			<date type="published" when="2011">Barnett 2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Transfer entropy as a loglikelihood ratio</title>
		<author>
			<persName><forename type="first">L</forename><surname>Barnett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Bossomaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical review letters</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page">138105</biblScope>
			<date type="published" when="2012">Barnett 2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The mvgc multivariate Granger causality toolbox: a new approach to Granger-causal inference</title>
		<author>
			<persName><forename type="first">L</forename><surname>Barnett ; Barnett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Seth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of neuroscience methods</title>
		<imprint>
			<biblScope unit="volume">223</biblScope>
			<biblScope unit="page" from="50" to="68" />
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multivariate Granger causality and generalized variance</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Barnett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Seth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">41907</biblScope>
			<date type="published" when="2010">Barrett 2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Granger causality is designed to measure effect, not mechanism</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Barrett ; Barrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Barnett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in neuroinformatics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Directed spectral methods</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Seth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Encyclopedia of Computational Neuroscience</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Small-world brain networks</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Bassett ; Bassett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bullmore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The neuroscientist</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="512" to="523" />
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Time delay estimation via minimum entropy</title>
		<author>
			<persName><forename type="first">J</forename><surname>Benesty ; Benesty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="157" to="160" />
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">On the directionality of cortical interactions studied by structural analysis of electrophysiological recordings</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bernasconi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Koènig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological cybernetics</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="199" to="210" />
			<date type="published" when="1999">1999. 1999</date>
			<pubPlace>Bernasconi</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Bidirectional interactions between visual areas in the awake behaving cat</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bernasconi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Von Stein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Koènig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroreport</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="689" to="692" />
			<date type="published" when="2000">2000. 2000</date>
			<pubPlace>Bernasconi</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">B</forename><surname>Bertrand ; Bertrand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Gérard</surname></persName>
		</author>
		<ptr target="http://www.anatomie-humaine.com/Le-Cerveau-1.html/" />
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Causal relationships between frequency bands of extracellular signals in visual cortex revealed by an information theoretic analysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Besserve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">K</forename><surname>Logothetis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Panzeri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Besserve 2010]</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="547" to="566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Identification of functional information subgraphs in complex networks</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Bettencourt ; Bettencourt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Gintautas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Ham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical review letters</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="issue">23</biblScope>
			<biblScope unit="page">238701</biblScope>
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Modeling nonlinear oscillatory systems and diagnostics of coupling between them using chaotic time series analysis: applications in neurophysiology</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">P</forename><surname>Bezruchko ; Bezruchko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">I</forename><surname>Ponomarenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Prokhorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Smirnov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Tass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physics-Uspekhi</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="304" to="310" />
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Nonlinear system identification: NARMAX methods in the time, frequency, and spatio-temporal domains</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Billings ; Billings</surname></persName>
		</author>
		<editor>C. M.</editor>
		<imprint>
			<date type="published" when="1995">2013. 2013. 1995</date>
			<publisher>Oxford university press</publisher>
		</imprint>
	</monogr>
	<note>Neural networks for pattern recognition</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Simultaneous assessment of flow and bold signals in resting-state functional connectivity maps</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">B</forename><surname>Biswal ; Biswal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">V</forename><surname>Kylen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Hyde</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NMR in Biomedicine</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">45</biblScope>
			<biblScope unit="page" from="165" to="170" />
			<date type="published" when="1997">1997. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Energy consumption and economic growth for selected oecd countries: Further evidence from the Granger causality test in the frequency domain</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bozoklu ; Bozoklu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Yilanci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Energy Policy</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="877" to="881" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Top-down control of human visual cortex by frontal and parietal cortex in anticipatory visual spatial attention</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Breedlove ; Breedlove</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">V</forename><surname>Watson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Rosenzweig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Bressler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Sylvester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">L</forename><surname>Shulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Corbetta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Bressler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Seth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">40</biblScope>
			<biblScope unit="page" from="323" to="329" />
			<date type="published" when="2007">2007. 2007. 2008. 2008. 2011</date>
			<publisher>Sinauer Associates, Incorporated Publishers</publisher>
		</imprint>
	</monogr>
	<note>Neuroimage</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Science and information theory</title>
		<author>
			<persName><forename type="first">L</forename><surname>Brillouin ; Brillouin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Brockwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Time series: theory and methods</title>
		<imprint>
			<publisher>Springer Science &amp; Business Media</publisher>
			<date type="published" when="2013">2013. 2013. Brockwell 2013. 2013</date>
		</imprint>
	</monogr>
	<note>Courier Corporation</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Beta oscillations in a large-scale sensorimotor cortical network: directional influences revealed by Granger causality</title>
		<author>
			<persName><forename type="first">A</forename><surname>Brovelli ; Brovelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ledberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nakamura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Bressler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences of the United States of America</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">26</biblScope>
			<biblScope unit="page" from="9849" to="9854" />
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Handbook of epilepsy</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Browne ; Browne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">L</forename><surname>Holmes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008. 2008</date>
			<publisher>Jones &amp; Bartlett Learning</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Model selection and multimodel inference: a practical information-theoretic approach</title>
		<author>
			<persName><forename type="first">A</forename><surname>Buehlmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Deco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Burnham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS computational biology</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">1000934</biblScope>
			<date type="published" when="2002">2010. Burnham 2002. 2002</date>
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
	<note>Optimal information transfer in the cortex through synchronization</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Feedback between stationary stochastic processes</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Caines ; Caines</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Automatic Control</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="498" to="508" />
			<date type="published" when="1975">1975. 1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Automatic lateralization of temporal lobe epilepsy based on scalp eeg</title>
		<author>
			<persName><forename type="first">M</forename><surname>Caparos ; Caparos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Louis Dorr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wendling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Maillard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Clinical neurophysiology</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2414" to="2423" />
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Survey and comparative analysis of entropy and relative entropy thresholding techniques</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">I</forename><surname>Chang ; Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Thouin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEE Proceedings on Vision, Image and Signal Processing</title>
		<imprint>
			<date type="published" when="2006">2006. 2006</date>
			<biblScope unit="volume">153</biblScope>
			<biblScope unit="page" from="837" to="850" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Statistical assessment of nonlinear causality: application to epileptic eeg signals</title>
		<author>
			<persName><forename type="first">M</forename><surname>Chávez ; Chávez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Martinerie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Van Quyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of neuroscience methods</title>
		<imprint>
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="113" to="128" />
			<date type="published" when="2003">2003. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Dynamic causal modelling of induced responses</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chen ; Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Kiebel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Friston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1293" to="1312" />
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Analyzing multiple nonlinear time series with extended Granger causality</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen ; Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Rangarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physics Letters A</title>
		<imprint>
			<biblScope unit="volume">324</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="26" to="35" />
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Frequency decomposition of conditional Granger causality and application to multivariate neural field potential data</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen ; Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Bressler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of neuroscience methods</title>
		<imprint>
			<biblScope unit="volume">150</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="228" to="237" />
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">On the spectral formulation of Granger causality</title>
		<author>
			<persName><forename type="first">D</forename><surname>Chicharro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological cybernetics</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="issue">5-6</biblScope>
			<biblScope unit="page" from="331" to="347" />
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Economic growth and energy consumption revisited-evidence from linear and nonlinear Granger causality</title>
		<author>
			<persName><surname>Chiou ; Chiou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Energy Economics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3063" to="3076" />
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">The stock market as a leading indicator: An application of Granger causality</title>
		<author>
			<persName><forename type="first">B</forename><surname>Comincioli ; Comincioli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">University Avenue Undergraduate Journal of Economics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="1996">1996. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Stereoelectroencephalography in the presurgical evaluation of focal epilepsy: a retrospective analysis of 215 procedures</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cossu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Cardinale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Castana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Citterio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Francione</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Benabid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">L</forename><surname>Russo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurosurgery</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="706" to="718" />
			<date type="published" when="2005">Cossu 2005. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Geodesic entropic graphs for dimension and entropy estimation in manifold learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Costa ; Costa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">O</forename><surname>Hero</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2210" to="2221" />
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Cover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Thomas</surname></persName>
		</author>
		<title level="m">Elements of information theory</title>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Do stock prices contain predictive power for the future economic activity? a Granger causality analysis in the frequency domain</title>
		<author>
			<persName><forename type="first">C</forename><surname>Croux ; Croux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Reusens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Macroeconomics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="93" to="103" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Bsmart: a matlab/c toolbox for analysis of multichannel neural time series</title>
		<author>
			<persName><forename type="first">J</forename><surname>Cui ; Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Bressler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1094" to="1104" />
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Graphical interaction models for multivariate time series1</title>
		<author>
			<persName><forename type="first">R</forename><surname>Dahlhaus ; Dahlhaus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Metrika</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="172" />
			<date type="published" when="2000">2000. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Dynamic causal modelling of distributed electromagnetic responses</title>
		<author>
			<persName><forename type="first">J</forename><surname>Daunizeau ; Daunizeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Kiebel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Friston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="590" to="601" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">The equivalence of linear gaussian connectivity techniques</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Davey ; Davey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Grayden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gavrilescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">F</forename><surname>Egan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Johnston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Human brain mapping</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1999" to="2014" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Identifying neural drivers with functional mri: an electrophysiological validation</title>
		<author>
			<persName><forename type="first">O</forename><surname>David ; David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Guillemain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Saillet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Reyt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Deransart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Segebarth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Depaulis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS Biol</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">315</biblScope>
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">The long-term outcome of adult epilepsy surgery, patterns of seizure remission, and relapse: a cohort study</title>
		<author>
			<persName><forename type="first">;</forename><surname>De Tisi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>De Tisi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">S</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Peacock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Mcevoy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">F</forename><surname>Harkness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Duncan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Lancet</title>
		<imprint>
			<biblScope unit="volume">378</biblScope>
			<biblScope unit="issue">9800</biblScope>
			<biblScope unit="page" from="1388" to="1395" />
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Multivariate Granger causality analysis of fmri data</title>
		<author>
			<persName><forename type="first">G</forename><surname>Deshpande ; Deshpande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Laconte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Peltier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Human brain mapping</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1361" to="1373" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Assessing and compensating for zero-lag correlation effects in time-lagged Granger causality analysis of fmri</title>
		<author>
			<persName><forename type="first">G</forename><surname>Deshpande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sathian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1446" to="1456" />
			<date type="published" when="2010">2010a</date>
		</imprint>
	</monogr>
	<note>Deshpande 2010a</note>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Effect of hemodynamic variability on Granger causality analysis of fmri</title>
		<author>
			<persName><forename type="first">G</forename><surname>Deshpande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sathian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="884" to="896" />
			<date type="published" when="2010">2010b</date>
		</imprint>
	</monogr>
	<note>Deshpande 2010b</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Multivariate conditional Granger causality analysis for lagged response of soil respiration in a temperate forest</title>
		<author>
			<persName><forename type="first">M</forename><surname>Detto ; Detto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bohrer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Nietz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">D</forename><surname>Maurer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Vogel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Gough</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Curtis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Entropy</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="4266" to="4284" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Epilepsy and driving: Current status of research</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Devlin ; Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Odell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Charlton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Koppel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Epilepsy research</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="135" to="152" />
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Information theory in neuroscience</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Dewey ; Dewey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russ</forename><surname>Dewey ; Dimitrov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Lazar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Victor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of computational neuroscience</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="5" />
			<date type="published" when="2007">2007. 2007. Dimitrov 2011. 2011</date>
		</imprint>
	</monogr>
	<note>Psychology: an introduction</note>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Ictal source analysis: localization and imaging of causal interactions in humans</title>
		<author>
			<persName><forename type="first">L</forename><surname>Ding ; Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Worrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Lagerlund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="575" to="586" />
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Short-window spectral analysis of cortical event-related potentials by adaptive multivariate autoregressive modeling: data preprocessing, model validation, and variability assessment</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Bressler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ding 2000]</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="page" from="35" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Granger causality: basic theory and application to neuroscience. Handbook of time series analysis: recent theoretical developments and applications</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ding ; Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Bressler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006. 2006</date>
			<biblScope unit="page">437</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Extraction of reproducible seizure patterns based on eeg scalp correlations</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">L</forename><surname>Dorr ; Dorr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Caparos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wendling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-P</forename><surname>Vignal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biomedical Signal Processing and Control</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="154" to="162" />
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Extraction of reproducible seizure patterns based on eeg scalp correlations</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">L</forename><surname>Dorr ; Dorr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Caparos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wendling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Vignal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biomedical Signal Processing and Control</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="154" to="162" />
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Adult epilepsy</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Duncan ; Duncan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Sisodiya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Lancet</title>
		<imprint>
			<biblScope unit="volume">367</biblScope>
			<biblScope unit="issue">9516</biblScope>
			<biblScope unit="page" from="1087" to="1100" />
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Using mutual information to measure order in model glass formers</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Dunleavy ; Dunleavy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wiesner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>Royall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">41505</biblScope>
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Shortcomings in the current treatment of epilepsy</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Eadie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert review of neurotherapeutics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1419" to="1427" />
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Graphical modelling of multivariate time series</title>
		<author>
			<persName><surname>Eichler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Probability Theory and Related Fields</title>
		<imprint>
			<biblScope unit="volume">153</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="233" to="268" />
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">A</forename><surname>Ellen ; Ellen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>David</surname></persName>
		</author>
		<ptr target="http://www.mayfieldclinic.com/PE-EpilepsySurg.htm/" />
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">Epilepsy: a comprehensive textbook</title>
		<author>
			<persName><forename type="first">J</forename><surname>Engel ; Engel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Pedley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Aicardi</surname></persName>
		</author>
		<idno>EPFL 2015] EPFL</idno>
		<ptr target="http://bluebrain.epfl.ch/" />
		<imprint>
			<date type="published" when="2008">2008. 2008. 2015</date>
			<publisher>Lippincott Williams &amp; Wilkins</publisher>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Application of high-frequency Granger causality to analysis of epileptic seizures and surgical decision making</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Epstein ; Epstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Adhikari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Willie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Dhamala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Epilepsia</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2038" to="2047" />
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Eric</surname></persName>
		</author>
		<ptr target="https://faculty.washington.edu/chudler/facts.html/" />
		<title level="m">Brain facts and figures</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Bivariate nonlinear prediction to quantify the strength of complex dynamical interactions in short-term cardiovascular variability</title>
		<author>
			<persName><forename type="first">L</forename><surname>Faes ; Faes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Nollo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Medical and Biological Engineering and Computing</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="383" to="392" />
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Extended causal modeling to assess partial directed coherence in multiple time series with significant instantaneous interactions</title>
		<author>
			<persName><forename type="first">L</forename><surname>Faes ; Faes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Nollo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological cybernetics</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="387" to="400" />
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Information-based detection of nonlinear Granger causality in multivariate processes via a nonuniform embedding technique</title>
		<author>
			<persName><forename type="first">L</forename><surname>Faes ; Faes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Nollo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Porta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">51112</biblScope>
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Non-uniform multivariate embedding to assess the information transfer in cardiovascular and cardiorespiratory variability series</title>
		<author>
			<persName><forename type="first">L</forename><surname>Faes ; Faes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Nollo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Porta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers in biology and medicine</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="290" to="297" />
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Conditional entropy-based evaluation of information dynamics in physiological systems</title>
		<author>
			<persName><forename type="first">L</forename><surname>Faes ; Faes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Porta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Directed information measures in neuroscience</title>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="page" from="61" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Epileptic seizures and epilepsy: definitions proposed by the international league against epilepsy (ilae) and the international bureau for epilepsy (ibe)</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Fisher ; Fisher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">V E</forename><surname>Boas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Blume</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Elger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Genton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Engel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Epilepsia</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="470" to="472" />
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Lower bounds on mutual information</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">V</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Grassberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">10101</biblScope>
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Testing nonlinearity and directedness of interactions between neural groups in the macaque inferotemporal cortex</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">A</forename><surname>Freiwald ; Freiwald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Valdes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bosch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Biscay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Jimenez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Kreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of neuroscience methods</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="105" to="119" />
			<date type="published" when="1999">1999. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Partial mutual information for coupling analysis of multivariate time series</title>
		<author>
			<persName><forename type="first">S</forename><surname>Frenzel ; Frenzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Pompe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Friston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical review letters</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="issue">20</biblScope>
			<biblScope unit="page" from="56" to="78" />
			<date type="published" when="1994">2007. 2007. 1994. 1994</date>
			<pubPlace>Friston</pubPlace>
		</imprint>
	</monogr>
	<note>Human brain mapping</note>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Dynamic causal modelling</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Friston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Penny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1273" to="1302" />
			<date type="published" when="2003">2003. 2003</date>
			<pubPlace>Friston</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Causal modelling and brain connectivity in functional magnetic resonance imaging</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Friston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS biology</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">220</biblScope>
			<date type="published" when="2009">2009. 2009</date>
			<pubPlace>Friston</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Friston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Bastos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Oswal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Wijk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Litvak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Granger causality revisited</title>
		<meeting><address><addrLine>Friston</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page" from="796" to="808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Modélisation et identification en épilepsie: De la dynamique des populations neuronales aux signaux EEG</title>
		<author>
			<persName><forename type="first">P</forename><surname>Frogerais ; Frogerais</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Université Rennes</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<title level="m" type="main">Introduction to statistical pattern recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Fukunaga ; Fukunaga</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
			<publisher>Academic press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">Evaluation of effective connectivity of motor areas during motor imagery and execution using conditional Granger causality</title>
		<author>
			<persName><forename type="first">K</forename><surname>Fukunaga ; Fukunaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Hostetler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1280" to="1288" />
			<date type="published" when="1973">1973. 1973. Gao 2011. 2011</date>
		</imprint>
	</monogr>
	<note>Neuroimage</note>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Efficient estimation of mutual information for strongly dependent variables</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">V</forename><surname>Steeg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Galstyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics, AISTATS 2015</title>
		<meeting>the Eighteenth International Conference on Artificial Intelligence and Statistics, AISTATS 2015<address><addrLine>Gao; San Diego, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-05-09">2015. 2015. May 9-12, 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Evaluation of the performance of information theory-based methods and cross-correlation to estimate the functional connectivity in cortical networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Garofalo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nieus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Massobrio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Martinoia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="304" to="313" />
			<date type="published" when="1982">2009. 2009. 1982</date>
			<publisher>Geweke</publisher>
		</imprint>
	</monogr>
	<note>PloS one</note>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Measures of conditional linear dependence and feedback between time series</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Geweke ; Geweke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Giovannoni</surname></persName>
		</author>
		<ptr target="http://multiple-sclerosis-research.blogspot.com/2015/01/education-whats-mri.html/" />
	</analytic>
	<monogr>
		<title level="m">a brief beginner&apos;s guide to the brain and mri</title>
		<imprint>
			<date type="published" when="1984">1984. 1984. Giovannoni 2015. 2015</date>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page" from="907" to="915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">Assessing coupling dynamics from an ensemble of time series</title>
		<author>
			<persName><forename type="first">Gómez-Herrero ;</forename><surname>Gómez-Herrero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Rutanen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Soriano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Pipa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Vicente</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Entropy</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">1958</biblScope>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Lexical influences on speech perception: a Granger causality analysis of meg and eeg source estimates</title>
		<author>
			<persName><forename type="first">B</forename><surname>Gourévitch ; Gourévitch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Eggermont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Gow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Segawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Ahlfors</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">H</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Neurophysiology</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="614" to="623" />
			<date type="published" when="2007">2007. 2007. Gow 2008. 2008</date>
		</imprint>
	</monogr>
	<note>Neuroimage</note>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Parallel versus serial processing dependencies in the perisylvian speech network: a Granger analysis of intracranial eeg data</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Gow ; Gow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Keller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Eskandar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Cash</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brain and language</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="43" to="48" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Network analysis of motor system connectivity in parkinson&apos;s disease: modulation of thalamocortical interactions after pallidotomy</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Grafton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Couldwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Waters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Human Brain Mapping</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="45" to="55" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<analytic>
		<title level="a" type="main">Investigating causal relations by econometric models and cross-spectral methods</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Granger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econometrica: Journal of the Econometric Society</title>
		<imprint>
			<biblScope unit="page" from="424" to="438" />
			<date type="published" when="1969">1969</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">A bivariate causality between stock prices and exchange rates: evidence from recent asianflu</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Granger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">N</forename><surname>Huangb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Quarterly Review of Economics and Finance</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="337" to="354" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Partial Granger causality-eliminating exogenous inputs and latent variables</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Greene ; Greene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Seth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Kendrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Econometric Analysis</title>
		<imprint>
			<date type="published" when="2003">2003. 2003. 2008a</date>
			<biblScope unit="volume">172</biblScope>
			<biblScope unit="page" from="79" to="93" />
		</imprint>
	</monogr>
	<note>Prentice Hall, 5. edition</note>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">Uncovering interactions in the frequency domain</title>
		<author>
			<persName><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS Comput Biol</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">1000087</biblScope>
			<date type="published" when="2008">2008. 2008b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Haines ; Haines</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Mihailoff</surname></persName>
		</author>
		<ptr target="http://clinicalgate.com/the-telencephalon/" />
		<title level="m">The telencephalon</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">A nonlinear generalization of spectral Granger causality</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Hamilton ; Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Billings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">G</forename><surname>Sarrigiannis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on bio-medical engineering</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1693" to="1701" />
			<date type="published" when="1994">1994. 1994. 2014. 2014</date>
			<publisher>Princeton university press Princeton</publisher>
		</imprint>
	</monogr>
	<note>Time series analysis</note>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">Small-world anatomical networks in the human brain revealed by cortical thickness from mri</title>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Evans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cerebral cortex</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2407" to="2419" />
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<monogr>
		<title level="m" type="main">The organization of behavior: A neuropsychological theory</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">O</forename><surname>Hebb</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">Hebb 2005. 2005</date>
			<publisher>Psychology Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">eeg spike and wave modelled by a stochastic limit cycle</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Hernández ; Hernández</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Valdés</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroReport</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page">2246</biblScope>
			<date type="published" when="1996">1996. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">The use of time-variant eeg Granger causality for inspecting directed interdependencies of neural assemblies</title>
		<author>
			<persName><forename type="first">W</forename><surname>Hesse ; Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Möller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Arnold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of neuroscience methods</title>
		<imprint>
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="27" to="44" />
			<date type="published" when="2003">2003. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Causality detection based on information-theoretic approaches in time series analysis</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hlaváčková-Schindler ; Hlaváčková-Schindler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Paluš</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vejmelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bhattacharya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physics Reports</title>
		<imprint>
			<biblScope unit="volume">441</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="46" />
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">Equivalence of Granger causality and transfer entropy: a generalization</title>
		<author>
			<persName><forename type="first">K</forename><surname>Hlaváčková-Schindler ; Hlaváčková-Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Mathematical Sciences</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">73</biblScope>
			<biblScope unit="page" from="3637" to="3648" />
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Reliability of inference of directed climate networks using conditional mutual information</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hlinka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hartman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Vejmelka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Runge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Marwan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kurths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Paluš</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Entropy</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2023" to="2045" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Network structure of cerebral cortex shapes functional connectivity on multiple time scales</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Honey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kötter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Breakspear</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Sporns</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">24</biblScope>
			<biblScope unit="page" from="10240" to="10245" />
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">The elusive concept of brain connectivity</title>
		<author>
			<persName><forename type="first">B</forename><surname>Horwitz ; Horwitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="466" to="470" />
			<date type="published" when="2003">2003. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Causality analysis of neural connectivity: critical examination of existing methods and advances of new methods</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hu ; Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Worrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="829" to="844" />
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Causality analysis of neural connectivity: New tool and limitations of spectral Granger causality</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hu ; Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="44" to="47" />
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">Nonparametric multivariate density estimation: a comparative study</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Hwang ; Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Lay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lippman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2795" to="2810" />
			<date type="published" when="1994">1994. 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">Comparison of nonlinear Granger causality extensions for low-dimensional systems</title>
		<author>
			<persName><forename type="first">K</forename><surname>Ishiguro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Otsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lungarella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kuniyoshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">36217</biblScope>
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Mutual information analysis of the eeg in patients with alzheimer&apos;s disease</title>
		<author>
			<persName><forename type="first">J</forename><surname>Jeong ; Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Gore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Peterson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Clinical Neurophysiology</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="827" to="835" />
			<date type="published" when="2001">2001. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">Linear and nonlinear information flow based on time-delayed mutual information method and its application to corticomuscular interaction</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hallett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Clinical Neurophysiology</title>
		<imprint>
			<biblScope unit="volume">121</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="392" to="401" />
			<date type="published" when="2010">Jin 2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Localization of ictal onset zones in lennox-gastaut syndrome using directional connectivity analysis of intracranial electroencephalography</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Jung ; Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">J</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">C</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">O</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Im</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">D</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="449" to="457" />
			<date type="published" when="2008">2008. 2008. Jung 2011. 2011</date>
		</imprint>
	</monogr>
	<note>Seizure</note>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<author>
			<persName><forename type="first">A</forename><surname>Kaiser ; Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schreiber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Information transfer in continuous processes</title>
		<imprint>
			<date type="published" when="2002">2002. 2002</date>
			<biblScope unit="volume">166</biblScope>
			<biblScope unit="page" from="43" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">A new method of the description of the information flow in the brain structures</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kamiński ; Kamiński</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Blinowska</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological cybernetics</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="203" to="210" />
			<date type="published" when="1991">1991. 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">Evaluating causal relations in neural systems: Granger causality, directed transfer function and statistical assessment of significance</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kamiński ; Kamiński</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">A</forename><surname>Truccolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Bressler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Kaufmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">I</forename><surname>Stern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological cybernetics</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="39" to="44" />
			<date type="published" when="1997">2001. 2001. Kaufmann 1997. 1997</date>
		</imprint>
	</monogr>
	<note>Nature</note>
</biblStruct>

<biblStruct xml:id="b125">
	<monogr>
		<title level="m" type="main">Anatomy and physiology: The unity of form and function</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Kenneth ; Kenneth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Carol</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998. 1998</date>
			<publisher>McGraw-Hill</publisher>
			<pubPlace>Boston, Massachusetts, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">Dynamic causal modelling of evoked responses in eeg/meg with lead field parameterization</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Kiebel ; Kiebel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Friston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1273" to="1284" />
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">Dynamic causal modeling: a generative model of slice timing in fmri</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Kiebel ; Kiebel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Klöppel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Weiskopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Friston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1487" to="1496" />
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">Entropybased analysis and bioinformatics-inspired integration of global economic information transfer</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">K</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">51986</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">Time-series dimensionality reduction via Granger causality</title>
		<author>
			<persName><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="611" to="614" />
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">Sample estimate of the entropy of a random vector</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">F</forename><surname>Kozachenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">N</forename><surname>Leonenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Problemy Peredachi Informatsii</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="9" to="16" />
			<date type="published" when="1987">1987. 1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">Estimating mutual information</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kraskov ; Kraskov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Stögbauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Grassberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical review E</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">66138</biblScope>
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title level="a" type="main">A partial Granger causality approach to explore causal networks derived from multi-parameter data</title>
		<author>
			<persName><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Lecture Notes in Computer Science</title>
		<imprint>
			<biblScope unit="volume">5307</biblScope>
			<biblScope unit="page" from="9" to="27" />
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">Direct-coupling information measure from nonuniform embedding</title>
		<author>
			<persName><forename type="first">D</forename><surname>Kugiumtzis ; Kugiumtzis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">62918</biblScope>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<monogr>
		<title level="m" type="main">How to Create a Mind: The Secret of Human Thought Revealed</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kurzweil ; Kurzweil</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
			<publisher>Penguin Books</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">Information flow between stock indices</title>
		<author>
			<persName><forename type="first">O</forename><surname>Kwon ; Kwon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-S</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Europhysics Letters</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">68003</biblScope>
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<analytic>
		<title level="a" type="main">Beyond elementwise interactions: identifying complex interactions in biological processes</title>
		<author>
			<persName><forename type="first">C</forename><surname>Ladroue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kendrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS One</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">6899</biblScope>
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main">Brain connectivity analysis: a short survey</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">W</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tomé</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">R</forename><surname>Keck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Górriz-Sáez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Puntonet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational intelligence and neuroscience</title>
		<imprint>
			<biblScope unit="page">8</biblScope>
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">E</forename><surname>Latham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Roudi</surname></persName>
		</author>
		<title level="m">Mutual information. Scholarpedia</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">1658</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<analytic>
		<title level="a" type="main">A class of rényi information estimators for multidimensional densities</title>
		<author>
			<persName><forename type="first">N</forename><surname>Leonenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pronzato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Savani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2153" to="2182" />
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<analytic>
		<title level="a" type="main">Time-varying linear and nonlinear parametric model for Granger causality analysis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li ; Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Billings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">F</forename><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">41906</biblScope>
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<analytic>
		<title level="a" type="main">Causal influences in primate cerebral cortex during visual pattern discrimination</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liang ; Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nakamura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Bressler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroreport</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="2875" to="2880" />
			<date type="published" when="2000">2000. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<analytic>
		<title level="a" type="main">Kernel Granger causality mapping effective connectivity on fmri data</title>
		<author>
			<persName><forename type="first">W</forename><surname>Liao ; Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Marinazzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1825" to="1835" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main">Evaluating the effective connectivity of resting state networks using conditional Granger causality</title>
		<author>
			<persName><forename type="first">W</forename><surname>Liao ; Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mantini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological cybernetics</title>
		<imprint>
			<biblScope unit="volume">102</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="57" to="69" />
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<analytic>
		<title level="a" type="main">Small-world directed networks in the human brain: multivariate Granger causality analysis of resting-state fmri</title>
		<author>
			<persName><forename type="first">W</forename><surname>Liao ; Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Marinazzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lindner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vicente</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Priesemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wibral</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Trentool: A matlab open source toolbox to analyse information flow in time series data with transfer entropy</title>
		<imprint>
			<date type="published" when="2011">2011. 2011. 2011</date>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page">119</biblScope>
		</imprint>
	</monogr>
	<note>Neuroimage</note>
</biblStruct>

<biblStruct xml:id="b145">
	<analytic>
		<title level="a" type="main">Local information transfer as a spatiotemporal filter for complex systems</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Lizier ; Lizier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Prokopenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Zomaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">26110</biblScope>
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b146">
	<analytic>
		<title level="a" type="main">Information modification and particle collisions in distributed computation</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Lizier ; Lizier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Prokopenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Zomaya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chaos: An Interdisciplinary Journal of Nonlinear Science</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">37109</biblScope>
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b147">
	<analytic>
		<title level="a" type="main">Multivariate information-theoretic measures reveal directed information structure and task relevant changes in fmri connectivity</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Lizier ; Lizier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heinzle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Horstmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-D</forename><surname>Haynes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Prokopenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational Neuroscience</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="85" to="107" />
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b148">
	<analytic>
		<title level="a" type="main">Jidt: An information-theoretic toolkit for studying the dynamics of complex systems</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Lizier ; Lizier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Robotics and AI</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b149">
	<analytic>
		<title level="a" type="main">Phase transfer entropy: a novel phase-based measure for directed connectivity in networks coupled by oscillatory interactions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lobier ; Lobier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Siebenhühner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Palva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Palva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="page" from="853" to="872" />
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b150">
	<analytic>
		<title level="a" type="main">Seizure source imaging by means of fine spatio-temporal dipole localization and directed transfer function in partial epilepsy patients</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu ; Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Worrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Clinical Neurophysiology</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1275" to="1283" />
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b151">
	<analytic>
		<title level="a" type="main">Testing methodologies for the nonlinear analysis of causal relationships in neurovascular coupling</title>
		<author>
			<persName><forename type="first">N</forename><surname>Lüdtke ; Lüdtke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">K</forename><surname>Logothetis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Panzeri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Magnetic resonance imaging</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1113" to="1119" />
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b152">
	<analytic>
		<title level="a" type="main">Methods for quantifying the causal structure of bivariate time series</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lungarella ; Lungarella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ishiguro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kuniyoshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Otsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of bifurcation and chaos</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">03</biblScope>
			<biblScope unit="page" from="903" to="921" />
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b153">
	<analytic>
		<title level="a" type="main">Information transfer at multiple scales</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lungarella ; B] Lungarella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pitti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kuniyoshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">56117</biblScope>
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b154">
	<monogr>
		<title level="m" type="main">New introduction to multiple time series analysis</title>
		<author>
			<persName><forename type="first">H</forename><surname>Lütkepohl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">Lütkepohl 2005. 2005</date>
			<publisher>Springer Publishing Company</publisher>
		</imprint>
	</monogr>
	<note>Incorporated</note>
</biblStruct>

<biblStruct xml:id="b155">
	<analytic>
		<title level="a" type="main">Hallmarks in the history of epilepsy: epilepsy in antiquity</title>
		<author>
			<persName><forename type="first">E</forename><surname>Magiorkinis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sidiropoulou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Diamantis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Epilepsy and Behavior</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="103" to="108" />
			<date type="published" when="2010">Magiorkinis 2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b156">
	<analytic>
		<title level="a" type="main">Harrison&apos;s principles of internal medicine</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Malani ; Malani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAMA</title>
		<imprint>
			<biblScope unit="volume">308</biblScope>
			<biblScope unit="issue">17</biblScope>
			<biblScope unit="page" from="1813" to="1814" />
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b157">
	<analytic>
		<title level="a" type="main">Kernel-Granger causality and the analysis of dynamical networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Marinazzo ; Marinazzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pellicoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Stramaglia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical review E</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">56215</biblScope>
			<date type="published" when="2008">2008. 2008a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b158">
	<analytic>
		<title level="a" type="main">Kernel method for nonlinear Granger causality</title>
		<author>
			<persName><forename type="first">D</forename><surname>Marinazzo ; Marinazzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pellicoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Stramaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Marinazzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Stramaglia</surname></persName>
		</author>
		<idno>144103.</idno>
	</analytic>
	<monogr>
		<title level="j">Physical Review Letters</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="330" to="338" />
			<date type="published" when="2008">2008. 2008b. Marinazzo 2011. 2011</date>
		</imprint>
	</monogr>
	<note>Neuroimage</note>
</biblStruct>

<biblStruct xml:id="b159">
	<analytic>
		<title level="a" type="main">Information transfer in the brain: Insights from a unified approach</title>
		<author>
			<persName><forename type="first">D</forename><surname>Marinazzo ; Marinazzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pellicoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Stramaglia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Directed Information Measures in Neuroscience</title>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
			<biblScope unit="page" from="87" to="110" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b160">
	<analytic>
		<title level="a" type="main">Dynamic causal modelling for fmri: a two-state model</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Marreiros ; Marreiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Kiebel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Friston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="269" to="278" />
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b161">
	<analytic>
		<title level="a" type="main">Population dynamics under the laplace assumption</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Marreiros ; Marreiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Kiebel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Daunizeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Friston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="701" to="714" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b162">
	<analytic>
		<title level="a" type="main">Intrinsically multivariate predictive genes</title>
		<author>
			<persName><forename type="first">;</forename><surname>Martins</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Martins</surname><genName>Jr</genName></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">M</forename><surname>Braga-Neto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">F</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Bittner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Dougherty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="424" to="439" />
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b163">
	<analytic>
		<title level="a" type="main">Determining the verse of magnetic turbulent cascades in the earth&apos;s magnetospheric cusp via transfer entropy analysis: preliminary results</title>
		<author>
			<persName><forename type="first">M</forename><surname>Materassi ; Materassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wernik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yordanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Nonlinear Processes in Geophysics</title>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Mccracken</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2007">2007. 2007. 2007</date>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="719" to="752" />
		</imprint>
	</monogr>
	<note>Asymptotics for out of sample tests of Granger causality</note>
</biblStruct>

<biblStruct xml:id="b164">
	<analytic>
		<title level="a" type="main">Structural modeling of functional neural pathways mapped with 2-deoxyglucose: effects of acoustic startle habituation on the auditory system</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mcintosh ; Mcintosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gonzalez-Lima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brain research</title>
		<imprint>
			<biblScope unit="volume">547</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="295" to="302" />
			<date type="published" when="1991">1991. 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b165">
	<analytic>
		<title level="a" type="main">Structural modeling of functional visual pathways mapped with 2-deoxyglucose: effects of patterned light and footshock</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mcintosh ; Mcintosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gonzalez-Lima</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brain Research</title>
		<imprint>
			<biblScope unit="volume">578</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="75" to="86" />
			<date type="published" when="1992">1992. 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b166">
	<analytic>
		<title level="a" type="main">Network analysis of cortical visual pathways mapped with pet</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mcintosh ; Mcintosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Grady</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">G</forename><surname>Ungerleider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Haxby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rapoport</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Horwitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The journal of neuroscience</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="655" to="666" />
			<date type="published" when="1994">1994. 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b167">
	<analytic>
		<title level="a" type="main">Symbolic transfer entropy analysis of the dust interaction in the presence of wakefields in dusty plasmas</title>
		<author>
			<persName><forename type="first">A</forename><surname>Melzer ; Melzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Schella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">41103</biblScope>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b168">
	<analytic>
		<title level="a" type="main">Fast nearestneighbor searching for nonlinear signal processing</title>
		<author>
			<persName><forename type="first">C</forename><surname>Merkwirth ; Merkwirth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Parlitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lauterborn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">2089</biblScope>
			<date type="published" when="2000">2000. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b169">
	<monogr>
		<title level="m" type="main">The effects of cryopreservation on the cat</title>
		<author>
			<persName><forename type="first">D</forename><surname>Michael ; Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jerry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hugh</surname></persName>
		</author>
		<ptr target="http://chronopause.com/chronopause.com/index.php/2012/02/21/the-effects-of-cryopreservation-on-the-cat-part-3/index.html/" />
		<imprint>
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b170">
	<analytic>
		<title level="a" type="main">Accurate epileptogenic focus localization through timevariant functional connectivity analysis of intracranial electroencephalographic signals</title>
		<author>
			<persName><forename type="first">P</forename><surname>Mierlo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Carrette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hallez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Vonck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Van Roost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Boon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Staelens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1122" to="1133" />
			<date type="published" when="2011">2011. 2011</date>
			<pubPlace>Mierlo</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b171">
	<analytic>
		<title level="a" type="main">Ictal-onset localization through connectivity analysis of intracranial eeg signals in patients with refractory epilepsy</title>
		<author>
			<persName><forename type="first">P</forename><surname>Mierlo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Carrette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hallez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Raedt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Meurs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vandenberghe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Roost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Boon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Staelens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Vonck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Epilepsia</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1409" to="1418" />
			<date type="published" when="2013">2013. 2013</date>
			<pubPlace>Mierlo</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b172">
	<analytic>
		<title level="a" type="main">Variation with age in the volumes of grey and white matter in the cerebral hemispheres of man: measurements with an image analyser</title>
		<author>
			<persName><forename type="first">A</forename><surname>Miller ; Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Alston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Corsellis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuropathology and applied neurobiology</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="119" to="132" />
			<date type="published" when="1980">1980. 1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b173">
	<analytic>
		<title level="a" type="main">Analysis of mutual information content for eeg responses to odor stimulation for subjects classified by occupation</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Min ; Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">H</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sakamoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chemical senses</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="741" to="749" />
			<date type="published" when="2003">2003. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b174">
	<analytic>
		<title level="a" type="main">Mute: a matlab toolbox to compare established and novel estimators of the multivariate transfer entropy</title>
		<author>
			<persName><forename type="first">A</forename><surname>Montalto ; Montalto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Faes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Marinazzo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">109462</biblScope>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b175">
	<analytic>
		<title level="a" type="main">Anatomy of the corpus callosum reveals its function</title>
		<author>
			<persName><forename type="first">E</forename><surname>Mooshagian ; Mooshagian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1535" to="1536" />
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b176">
	<analytic>
		<title level="a" type="main">Dynamic causal models of steady-state responses</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Moran ; Moran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">E</forename><surname>Stephan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Seidenbecher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">C</forename><surname>Pape</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Friston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="796" to="811" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b177">
	<analytic>
		<title level="a" type="main">Granger causality of coupled climate processes: Ocean feedback on the north atlantic oscillation</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Mosedale ; Mosedale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Stephenson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">C</forename><surname>Mills</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Climate</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1182" to="1194" />
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b178">
	<analytic>
		<title level="a" type="main">eeg in schizophrenic patients: mutual information analysis</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Na</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Ham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Clinical Neurophysiology</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1954" to="1960" />
			<date type="published" when="2002">2002. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b179">
	<analytic>
		<title level="a" type="main">Global, regional, and national age-sex specific all-cause and cause-specific mortality for 240 causes of death, 1990-2013: a systematic analysis for the global burden of disease study</title>
		<author>
			<persName><forename type="first">M</forename><surname>Naghavi ; Naghavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Lozano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Vollset</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Ozgoren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Abdalla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Abd-Allah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lancet</title>
		<imprint>
			<biblScope unit="volume">385</biblScope>
			<biblScope unit="issue">9963</biblScope>
			<biblScope unit="page" from="117" to="171" />
			<date type="published" when="2013">2015. 2015. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b180">
	<analytic>
		<title level="a" type="main">Synaptic information transfer in computer models of neocortical columns</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Neymotin ; Neymotin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Fenton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W</forename><surname>Lytton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of computational neuroscience</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="69" to="84" />
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b181">
	<analytic>
		<title level="a" type="main">Detecting nonlinearity in structural systems using the transfer entropy</title>
		<author>
			<persName><forename type="first">J</forename><surname>Nichols</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Seaver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Trickey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Todd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Olson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Overbey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">46217</biblScope>
			<date type="published" when="2005">Nichols 2005. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b182">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Noback</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">L</forename><surname>Strominger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Demarest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Ruggiero</surname></persName>
		</author>
		<title level="m">The human nervous system: structure and function</title>
		<imprint>
			<publisher>Springer Science &amp; Business Media</publisher>
			<date type="published" when="2005">Noback 2005. 2005</date>
			<biblScope unit="volume">744</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b183">
	<analytic>
		<title level="a" type="main">Diagnosis and management of the epilepsies in adults and children: summary of updated nice guidance</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">D</forename><surname>Nunes ; Nunes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sawyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Neilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sarri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Cross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMJ</title>
		<imprint>
			<biblScope unit="page">344</biblScope>
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b184">
	<analytic>
		<title level="a" type="main">Measuring time-varying information flow in scalp eeg signals: orthogonalized partial directed coherence</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Omidvarnia ; Omidvarnia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Azemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Boashash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>O'toole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Colditz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vanhatalo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Omidvarnia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Azemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Boashash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>O'toole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">B</forename><surname>Colditz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vanhatalo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing</title>
		<imprint>
			<date type="published" when="2012">2012. 2012. Omidvarnia 2014. 2014</date>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="680" to="693" />
		</imprint>
	</monogr>
	<note>Orthogonalized partial directed coherence for functional connectivity analysis of newborn eeg</note>
</biblStruct>

<biblStruct xml:id="b185">
	<analytic>
		<title level="a" type="main">The concept of the epileptogenic zone: a modern look at penfield and jasper&apos;s views on the role of interictal spikes</title>
		<author>
			<persName><forename type="first">A</forename><surname>Palmini ; Palmini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Epileptic disorders</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="10" to="15" />
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b186">
	<analytic>
		<title level="a" type="main">Synchronization as adjustment of information rates: detection from bivariate time series</title>
		<author>
			<persName><forename type="first">M</forename><surname>Paluš ; Paluš</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Komárek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hrnčíř</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Štěrbová</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">46211</biblScope>
			<date type="published" when="2001">2001. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b187">
	<analytic>
		<title level="a" type="main">Transfer entropy as a tool for reconstructing interaction delays in neural signals</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">C</forename><surname>Pampu ; Pampu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vicente</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C</forename><surname>Muresan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Priesemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Siebenhuhner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wibral</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE International Symposium on Signals, Circuits and Systems (ISSCS)</title>
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b188">
	<analytic>
		<title level="a" type="main">Multivariate Granger causality between co2 emissions, energy consumption, fdi (foreign direct investment) and gdp (gross domestic product): evidence from a panel of bric (brazil, russian federation, india, and china) countries</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Pao ; Pao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Tsai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Energy</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="685" to="693" />
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b189">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Papoulis ; Papoulis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">U</forename><surname>Pillai</surname></persName>
		</author>
		<title level="m">Probability, random variables, and stochastic processes</title>
		<imprint>
			<publisher>McGraw-Hill</publisher>
			<date type="published" when="1985">1985. 1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b190">
	<monogr>
		<title level="m" type="main">On estimation of a probability density function and mode. The annals of mathematical statistics</title>
		<author>
			<persName><forename type="first">E</forename><surname>Parzen ; Parzen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1962">1962. 1962</date>
			<biblScope unit="page" from="1065" to="1076" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b191">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<title level="m">Causality: models, reasoning, and inference</title>
		<imprint>
			<publisher>Cambridge university press</publisher>
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b192">
	<analytic>
		<title level="a" type="main">Modelling functional integration: a comparison of structural equation and dynamic causal models</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">D</forename><surname>Penny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">E</forename><surname>Stephan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mechelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Friston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="264" to="S274" />
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b193">
	<analytic>
		<title level="a" type="main">Dynamic causal models for phase coupling</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">D</forename><surname>Penny</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Litvak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fuentemilla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Duzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Friston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of neuroscience methods</title>
		<imprint>
			<biblScope unit="volume">183</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="30" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b194">
	<analytic>
		<title level="a" type="main">Nonlinear multivariate analysis of neurophysiological signals</title>
		<author>
			<persName><forename type="first">E</forename><surname>Pereda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">Q</forename><surname>Quiroga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bhattacharya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Progress in neurobiology</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="37" />
			<date type="published" when="2005">Pereda 2005. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b195">
	<analytic>
		<title level="a" type="main">Different forms of effective connectivity in primate frontotemporal pathways</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">I</forename><surname>Petkov ; Petkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kikuchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Milne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Rauschecker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">K</forename><surname>Logothetis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature communications</title>
		<imprint>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b196">
	<analytic>
		<title level="a" type="main">Fast algorithms for mutual information based independent component analysis</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">T</forename><surname>Pham ; Pham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2690" to="2700" />
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b197">
	<analytic>
		<title level="a" type="main">Causal influences in the human brain during face discrimination: a short-window directed transfer function approach</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Philiastides ; Philiastides</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sajda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2602" to="2605" />
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b198">
	<monogr>
		<title level="m" type="main">Quantitative evaluation of eeg signals in epilepsy</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pijn ; Pijn</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990">1990. 1990</date>
			<pubPlace>University of Amsterdam, Amsterdam</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b199">
	<analytic>
		<title level="a" type="main">Functional integration within the human pain system as revealed by Granger causality</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ploner ; Ploner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Schoffelen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Schnitzler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Human brain mapping</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4025" to="4032" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b200">
	<analytic>
		<title level="a" type="main">Momentary information transfer as a coupling measure of time series</title>
		<author>
			<persName><forename type="first">B</forename><surname>Pompe ; Pompe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Runge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">51122</biblScope>
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b201">
	<analytic>
		<title level="a" type="main">Small-world networks and epilepsy: graph theoretical analysis of intracerebrally recorded mesial temporal lobe seizures</title>
		<author>
			<persName><forename type="first">S</forename><surname>Ponten ; Ponten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bartolomei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Stam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Clinical neurophysiology</title>
		<imprint>
			<biblScope unit="volume">118</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="918" to="927" />
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b202">
	<monogr>
		<author>
			<persName><surname>Rch ; Rch</surname></persName>
		</author>
		<ptr target="http://www.rch.org.au/neurology/patient_information/antiepileptic_medications/" />
		<title level="m">Antiepileptic medications</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b203">
	<analytic>
		<title level="a" type="main">Mapping directed influence over the brain using Granger causality and fmri</title>
		<author>
			<persName><forename type="first">A</forename><surname>Roebroeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Formisano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Goebel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="230" to="242" />
			<date type="published" when="2005">Roebroeck 2005. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b204">
	<analytic>
		<title level="a" type="main">The identification of interacting networks in the brain using fmri: model selection, causality and deconvolution</title>
		<author>
			<persName><forename type="first">A</forename><surname>Roebroeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Formisano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Goebel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="296" to="302" />
			<date type="published" when="2011">2011a</date>
		</imprint>
	</monogr>
	<note>Roebroeck 2011a</note>
</biblStruct>

<biblStruct xml:id="b205">
	<analytic>
		<title level="a" type="main">Does partial Granger causality really eliminate the influence of exogenous inputs and latent variables</title>
		<author>
			<persName><forename type="first">A</forename><surname>Roebroeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Seth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Valdes-Sosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Roelstraete</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rosseel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Causality in Time Series Challenges in Machine Learning</title>
		<imprint>
			<date type="published" when="2011">2011b. 2012</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="73" to="77" />
		</imprint>
	</monogr>
	<note>Causal time series analysis of functional magnetic resonance imaging data</note>
</biblStruct>

<biblStruct xml:id="b206">
	<monogr>
		<title level="m" type="main">Some modern mathematics for physicists and other outsiders : an introduction to algebra, topology, and functional analysis</title>
		<author>
			<persName><forename type="first">P</forename><surname>Roman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1974">1974</date>
			<publisher>Pergamon Press</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b207">
	<analytic>
		<title level="a" type="main">The phase of thalamic alpha activity modulates cortical gamma-band activity: evidence from resting-state meg recordings</title>
		<author>
			<persName><forename type="first">F</forename><surname>Roux ; Roux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wibral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Aru</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Uhlhaas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Neuroscience</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">45</biblScope>
			<biblScope unit="page" from="17827" to="17835" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b208">
	<analytic>
		<title level="a" type="main">Escaping the curse of dimensionality in estimating multivariate transfer entropy</title>
		<author>
			<persName><forename type="first">J</forename><surname>Runge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heitzig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Petoukhov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kurths</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical review letters</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="issue">25</biblScope>
			<biblScope unit="page">258701</biblScope>
			<date type="published" when="2012">2012. 2012</date>
			<pubPlace>Runge</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b209">
	<analytic>
		<title level="a" type="main">Information flow in coupled nonlinear systems: Application to the epileptic human brain</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sabesan ; Sabesan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Narayanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Iasemidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Spanias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Tsakalis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Mining in Biomedicine</title>
		<imprint>
			<date type="published" when="2007">2007. 2007</date>
			<biblScope unit="page" from="483" to="503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b210">
	<analytic>
		<title level="a" type="main">Information flow and application to epileptogenic focus localization from intracranial eeg</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sabesan ; Sabesan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">B</forename><surname>Good</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Tsakalis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Spanias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Treiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">D</forename><surname>Iasemidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Systems and Rehabilitation Engineering</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="244" to="253" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b211">
	<analytic>
		<title level="a" type="main">Using partial directed coherence to describe neuronal ensemble interactions</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sameshima ; Sameshima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Baccalá</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of neuroscience methods</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="93" to="103" />
			<date type="published" when="1999">1999. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b212">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Sameshima ; Sameshima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Baccala</surname></persName>
		</author>
		<title level="m">Methods in Brain Connectivity Inference through Multivariate Time Series Analysis</title>
		<imprint>
			<publisher>CRC press</publisher>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b213">
	<analytic>
		<title level="a" type="main">A method to produce evolving functional connectivity maps during the course of an fmri experiment using wavelet-based time-varying Granger causality</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Sato ; Sato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Junior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Y</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>De Maria Felix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Brammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Morettin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="187" to="196" />
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b214">
	<analytic>
		<title level="a" type="main">Frequency domain connectivity identification: an application of partial directed coherence in fmri</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Sato ; Sato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">Y</forename><surname>Takahashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Arcuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sameshima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Morettin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Baccalá</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Human brain mapping</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="452" to="461" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b215">
	<analytic>
		<title level="a" type="main">Testing for directed influences among neural signals using partial directed coherence</title>
		<author>
			<persName><forename type="first">B</forename><surname>Schelter ; Schelter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Winterhalder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Eichler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Peifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hellwig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Guschlbauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Lücking</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dahlhaus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Timmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of neuroscience methods</title>
		<imprint>
			<biblScope unit="volume">152</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="210" to="219" />
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b216">
	<analytic>
		<title level="a" type="main">Assessing the strength of directed influences among neural signals using renormalized partial directed coherence</title>
		<author>
			<persName><forename type="first">B</forename><surname>Schelter ; Schelter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Timmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Eichler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of neuroscience methods</title>
		<imprint>
			<biblScope unit="volume">179</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="121" to="130" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b217">
	<analytic>
		<title level="a" type="main">Measuring information transfer</title>
		<author>
			<persName><forename type="first">T</forename><surname>Schreiber ; Schreiber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical review letters</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">461</biblScope>
			<date type="published" when="2000">2000. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b218">
	<analytic>
		<title level="a" type="main">Estimating the dimension of a model</title>
		<author>
			<persName><forename type="first">G</forename><surname>Schwarz ; Schwarz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The annals of statistics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="461" to="464" />
			<date type="published" when="1978">1978. 1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b219">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Sergey ; Sergey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Harald</surname></persName>
		</author>
		<ptr target="http://www.ucl.ac.uk/ion/departments/sobell/Research/RLemon/MILCA/MILCA/" />
		<title level="m">Milca toolbox</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b220">
	<analytic>
		<title level="a" type="main">A matlab toolbox for Granger causal connectivity analysis</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Seth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Seth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Granger causality. Scholarpedia</title>
		<imprint>
			<date type="published" when="2007">2007. 2007. Seth 2010. 2010</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="262" to="273" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b221">
	<analytic>
		<title level="a" type="main">Granger causality analysis of fmri bold signals is invariant to hemodynamic convolution but not downsampling</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Seth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chorley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C</forename><surname>Barnett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="540" to="555" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b222">
	<analytic>
		<title level="a" type="main">A mathematical theory of communities</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Shannon ; Shannon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Techn. J</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="379" to="423" />
			<date type="published" when="1948">1948. 1948</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b223">
	<monogr>
		<title level="m" type="main">Kernel methods for pattern analysis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shawe-Taylor ; Shawe-Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cristianini</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004. 2004</date>
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b224">
	<analytic>
		<title level="a" type="main">Nearest neighbor estimates of entropy</title>
		<author>
			<persName><forename type="first">H</forename><surname>Singh ; Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Hnizdo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fedorowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Demchuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American journal of mathematical and management sciences</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="301" to="321" />
			<date type="published" when="2003">2003. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b225">
	<analytic>
		<title level="a" type="main">From Granger causality to long-term causality: Application to climatic data</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Smirnov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">I</forename><surname>Mokhov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Smirnov</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">42917</biblScope>
			<date type="published" when="2009">2009. 2009. 2013. 2013</date>
			<pubPlace>Smirnov; Smirnov</pubPlace>
		</imprint>
	</monogr>
	<note>Physical Review E</note>
</biblStruct>

<biblStruct xml:id="b226">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Smithson ; Smithson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Walker</surname></persName>
		</author>
		<title level="m">ABC of Epilepsy</title>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="2012">2012. 2012</date>
			<biblScope unit="volume">201</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b227">
	<analytic>
		<title level="a" type="main">The human connectome: a structural description of the human brain</title>
		<author>
			<persName><forename type="first">O</forename><surname>Sporns ; Sporns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Sporns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tononi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kötter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Brain connectivity. Scholarpedia</title>
		<imprint>
			<date type="published" when="2005">2007. 2007. Sporns 2005. 2005</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">42</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b228">
	<analytic>
		<title level="a" type="main">Ensemble estimators for multivariate entropy estimation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sricharan ; Sricharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iii</forename><surname>Hero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">O</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">59</biblScope>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b229">
	<analytic>
		<title level="a" type="main">Symbolic transfer entropy</title>
		<author>
			<persName><forename type="first">M</forename><surname>Staniek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lehnertz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Staniek 2008]</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="page">158101</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b230">
	<analytic>
		<title level="a" type="main">Symbolic transfer entropy: inferring directionality in biosignals</title>
		<author>
			<persName><forename type="first">M</forename><surname>Staniek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lehnertz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Staniek 2009]</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="323" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b231">
	<analytic>
		<title level="a" type="main">Nonlinear dynamic causal models for fmri</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">E</forename><surname>Stephan ; Stephan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Weiskopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Drysdale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Friston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">E</forename><surname>Stephan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kasper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Daunizeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">E</forename><surname>Den Ouden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Breakspear</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Friston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="649" to="662" />
			<date type="published" when="2007">2007. 2007. Stephan 2008. 2008</date>
		</imprint>
	</monogr>
	<note>Neuroimage</note>
</biblStruct>

<biblStruct xml:id="b232">
	<analytic>
		<title level="a" type="main">Least-dependent-component analysis based on mutual information</title>
		<author>
			<persName><forename type="first">H</forename><surname>Stögbauer ; Stögbauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kraskov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Astakhov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Grassberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">66123</biblScope>
			<date type="published" when="2004">2004. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b233">
	<analytic>
		<title level="a" type="main">Nonlinear Granger causality for brain connectivity</title>
		<author>
			<persName><forename type="first">S</forename><surname>Stramaglia ; Stramaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Angelini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pellicoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Marinazzo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE International Workshop on Medical Measurements and Applications Proceedings (MeMeA)</title>
		<imprint>
			<date type="published" when="2011">2011. 2011</date>
			<biblScope unit="page" from="197" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b234">
	<analytic>
		<title level="a" type="main">Expanding the transfer entropy to identify information circuits in complex systems</title>
		<author>
			<persName><forename type="first">S</forename><surname>Stramaglia ; Stramaglia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pellicoro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Marinazzo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">66211</biblScope>
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b235">
	<analytic>
		<title level="a" type="main">Causation entropy identifies indirect influences, dominance of neighbors and anticipatory couplings</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Bollt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physica D: Nonlinear Phenomena</title>
		<imprint>
			<biblScope unit="volume">267</biblScope>
			<biblScope unit="page" from="49" to="57" />
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
		<respStmt>
			<orgName>Sun</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b236">
	<analytic>
		<title level="a" type="main">Assessing nonlinear Granger causality from multivariate time series</title>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<date type="published" when="2008">2008. 2008</date>
			<biblScope unit="page" from="440" to="455" />
		</imprint>
		<respStmt>
			<orgName>Sun</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b237">
	<analytic>
		<title level="a" type="main">Early cortical connective network relating to audiovisual stimulation by partial directed coherence analysis</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2721" to="2724" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
		<respStmt>
			<orgName>Sun</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b238">
	<analytic>
		<title level="a" type="main">The asymmetric Granger-causality analysis between energy consumption and income in the united states</title>
		<author>
			<persName><forename type="first">T</forename><surname>Suzuki ; Suzuki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kanamori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">K</forename><surname>Tiwari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Renewable and Sustainable Energy Reviews</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="362" to="369" />
			<date type="published" when="2008">2008. 2008. Tiwari 2014. 2014</date>
		</imprint>
	</monogr>
	<note>FSDM</note>
</biblStruct>

<biblStruct xml:id="b239">
	<analytic>
		<title level="a" type="main">On the use of Granger causality to investigate the human influence on climate</title>
		<author>
			<persName><forename type="first">U</forename><surname>Triacca ; Triacca</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theoretical and Applied Climatology</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="137" to="138" />
			<date type="published" when="2001">2001. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b240">
	<analytic>
		<title level="a" type="main">Learning curves for mutual information maximization</title>
		<author>
			<persName><forename type="first">R</forename><surname>Urbanczik ; Urbanczik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">16106</biblScope>
			<date type="published" when="2003">2003. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b241">
	<analytic>
		<title level="a" type="main">Exploring transient transfer entropy based on a group-wise ica decomposition of eeg data</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">A</forename><surname>Vakorin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kovacevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Mcintosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1593" to="1600" />
			<date type="published" when="2010">Vakorin 2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b242">
	<analytic>
		<title level="a" type="main">Confounding effects of indirect connections on causality estimation</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">A</forename><surname>Vakorin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">A</forename><surname>Krakovska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Mcintosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of neuroscience methods</title>
		<imprint>
			<biblScope unit="volume">184</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="152" to="160" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b243">
	<analytic>
		<title level="a" type="main">Empirical and theoretical aspects of generation and transfer of information in a neuromagnetic source network</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">A</forename><surname>Vakorin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mišić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Krakovska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">R</forename><surname>Mcintosh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in systems neuroscience</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<date type="published" when="2011">Vakorin 2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b244">
	<analytic>
		<title level="a" type="main">Effective connectivity: influence, causality and biophysical modeling</title>
		<author>
			<persName><forename type="first">;</forename><surname>Valdes-Sosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Valdes-Sosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roebroeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Daunizeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Friston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="339" to="361" />
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b245">
	<analytic>
		<title level="a" type="main">Edgeworth approximation of multivariate differential entropy</title>
		<author>
			<persName><forename type="first">;</forename><surname>Van Hulle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Van Hulle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1903" to="1910" />
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b246">
	<analytic>
		<title level="a" type="main">Epileptogenic networks of type ii focal cortical dysplasia: a stereo-eeg study</title>
		<author>
			<persName><forename type="first">G</forename><surname>Varotto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Franceschetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Spreafico</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Panzica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="591" to="598" />
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b247">
	<analytic>
		<title level="a" type="main">Efficient entropy estimation for mutual information analysis using b-splines</title>
		<author>
			<persName><forename type="first">A</forename><surname>Venelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Information Security Theory and Practices. Security and Privacy of Pervasive Systems and Smart Devices</title>
		<imprint>
			<publisher>Venelli</publisher>
			<date type="published" when="2010">2010. 2010</date>
			<biblScope unit="page" from="17" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b248">
	<analytic>
		<title level="a" type="main">Information transfer in social media</title>
		<author>
			<persName><forename type="first">G</forename><surname>Steeg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Galstyan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st international conference on World Wide Web</title>
		<meeting>the 21st international conference on World Wide Web</meeting>
		<imprint>
			<publisher>Ver Steeg</publisher>
			<date type="published" when="2012">2012. 2012</date>
			<biblScope unit="page" from="509" to="518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b249">
	<analytic>
		<title level="a" type="main">The curse of dimensionality in data mining and time series prediction</title>
		<author>
			<persName><forename type="first">M</forename><surname>Verleysen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>François</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Intelligence and Bioinspired Systems</title>
		<imprint>
			<date type="published" when="2005">Verleysen 2005. 2005</date>
			<biblScope unit="page" from="758" to="770" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b250">
	<analytic>
		<title level="a" type="main">Transfer entropy-a model-free measure of effective connectivity for the neurosciences</title>
		<author>
			<persName><forename type="first">R</forename><surname>Vicente</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wibral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lindner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pipa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of computational neuroscience</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="45" to="67" />
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b251">
	<analytic>
		<title level="a" type="main">Epileptic seizure detection based on partial directed coherence analysis</title>
		<author>
			<persName><forename type="first">G</forename><surname>Wang ; Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Biomedical and Health Informatics</title>
		<imprint>
			<biblScope unit="issue">99</biblScope>
			<biblScope unit="page" from="1" to="1" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
	<note>PP</note>
</biblStruct>

<biblStruct xml:id="b252">
	<analytic>
		<title level="a" type="main">Divergence estimation for multidimensional densities via-nearest-neighbor distances</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wang ; Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Verdú</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2392" to="2405" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b253">
	<analytic>
		<title level="a" type="main">Synchronization in small-world dynamical networks</title>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">F</forename><surname>Wang ; Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Bifurcation and Chaos</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">01</biblScope>
			<biblScope unit="page" from="187" to="192" />
			<date type="published" when="2002">2002. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b254">
	<analytic>
		<title level="a" type="main">Interictal to ictal transition in human temporal lobe epilepsy: insights from a computational model of intracerebral eeg</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wendling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Bellanger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chauvel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bartolomei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Clinical Neurophysiology</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">343</biblScope>
			<date type="published" when="2005">Wendling 2005. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b255">
	<analytic>
		<title level="a" type="main">Relevance of nonlinear lumped-parameter models in the analysis of depth-eeg epileptic signals</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wendling ; Wendling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Bellanger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bartolomei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chauvel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biological cybernetics</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="367" to="378" />
			<date type="published" when="2000">2000. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b256">
	<analytic>
		<title level="a" type="main">Interpretation of interdependencies in epileptic signals using a macroscopic physiological model of the eeg</title>
		<author>
			<persName><forename type="first">F</forename><surname>Wendling ; Wendling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bartolomei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Bellanger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Chauvel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Clinical neurophysiology</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1201" to="1218" />
			<date type="published" when="2001">2001. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b257">
	<analytic>
		<title level="a" type="main">Atlas: epilepsy care in the world</title>
		<ptr target="http://www.who.int/mental_health/neurology/Epilepsy_atlas_r1.pdf/" />
	</analytic>
	<monogr>
		<title level="m">WHO 2005</title>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b258">
	<analytic>
		<title level="a" type="main">Transfer entropy in magnetoencephalographic data: Quantifying information flow in cortical and cerebellar networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wibral ; Wibral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Rahm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rieder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lindner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vicente</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kaiser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Progress in biophysics and molecular biology</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="80" to="97" />
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b259">
	<analytic>
		<title level="a" type="main">Trentool: an open source toolbox to estimate neural directed interactions with transfer entropy</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wibral ; Wibral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vicente</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Priesemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lindner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Neuroscience</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">200</biblScope>
			<date type="published" when="2011">2011. 2011b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b260">
	<analytic>
		<author>
			<persName><forename type="first">M</forename><surname>Wibral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Pampu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Priesemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Siebenhühner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Seiwert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lindner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Lizier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vicente</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Measuring information-transfer delays</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">55809</biblScope>
		</imprint>
	</monogr>
	<note>Wibral 2013]</note>
</biblStruct>

<biblStruct xml:id="b261">
	<analytic>
		<title level="a" type="main">Transfer entropy in neuroscience</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wibral ; Wibral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vicente</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lindner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Directed Information Measures in Neuroscience</title>
		<imprint>
			<date type="published" when="2014">2014. 2014a</date>
			<biblScope unit="page" from="3" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b262">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Wibral ; Wibral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vicente</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Lizier</surname></persName>
		</author>
		<title level="m">Directed information measures in neuroscience</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014. 2014b</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b263">
	<monogr>
		<title level="m" type="main">The theory of prediction. Modern mathematics for engineers</title>
		<author>
			<persName><forename type="first">N</forename><surname>Wiener ; Wiener</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1956">1956. 1956</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="125" to="139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b264">
	<analytic>
		<title level="a" type="main">An adaptive directed transfer function approach for detecting dynamic causal interactions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wilke ; Wilke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">29th Annual International IEEE Conference of Engineering in Medicine and Biology Society (EMBS)</title>
		<imprint>
			<date type="published" when="2007">2007. 2007</date>
			<biblScope unit="page" from="4949" to="4952" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b265">
	<analytic>
		<title level="a" type="main">Estimation of time-varying connectivity patterns through the use of an adaptive directed transfer function</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wilke ; Wilke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2557" to="2564" />
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b266">
	<analytic>
		<title level="a" type="main">Neocortical seizure foci localization by means of a directed transfer function method</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wilke ; Wilke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Van Drongelen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kohrman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Epilepsia</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="564" to="572" />
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b267">
	<analytic>
		<title level="a" type="main">Efficient transfer entropy analysis of non-stationary neural time series</title>
		<author>
			<persName><forename type="first">B</forename><surname>Wissman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mckay-Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-M ;</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wollstadt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Martínez Zarzuela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vicente</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">J</forename><surname>Díaz Pernas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wibral</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">102833</biblScope>
			<date type="published" when="2011">2011. 2014</date>
		</imprint>
	</monogr>
	<note>PloS one</note>
</biblStruct>

<biblStruct xml:id="b268">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">P</forename><surname>Wollstadt ; Wollstadt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lindner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Vicente</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wibral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Pampu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Martinez Zarzuela</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
	<note>Trentool toolbox. www.trentool.de/</note>
</biblStruct>

<biblStruct xml:id="b269">
	<analytic>
		<title level="a" type="main">Inferring topologies of complex networks with hidden variables</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wu ; Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">X</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">46106</biblScope>
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b270">
	<analytic>
		<title level="a" type="main">Assessing dynamic spectral causality by lagged adaptive directed transfer function and instantaneous effect factor</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xu ; Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1979" to="1988" />
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b271">
	<analytic>
		<title level="a" type="main">Contribution à l&apos;analyse de connectivité effective en épilepsie</title>
		<author>
			<persName><forename type="first">C</forename><surname>Yang ; Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Université Rennes</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b272">
	<analytic>
		<title level="a" type="main">A new strategy for model order identification and its application to transfer entropy for eeg signals analysis</title>
		<author>
			<persName><forename type="first">C</forename><surname>Yang ; Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Le Bouquin Jeannes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Bellanger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Biomedical Engineering</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1318" to="1327" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b273">
	<analytic>
		<title level="a" type="main">Root cause diagnosis of plant-wide oscillations using Granger causality</title>
		<author>
			<persName><forename type="first">T</forename><surname>Yuan ; Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Process Control</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="450" to="459" />
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b274">
	<analytic>
		<title level="a" type="main">Thalamocortical relationship in epileptic patients with generalized spike and wave discharges-a multimodal neuroimaging study</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Zhang ; Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mundahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">R</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage: Clinical</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="117" to="127" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b275">
	<analytic>
		<title level="a" type="main">A new narx-based Granger linear and nonlinear casual influence detection method with applications to eeg data</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao ; Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Billings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">G</forename><surname>Sarrigiannis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of neuroscience methods</title>
		<imprint>
			<biblScope unit="volume">212</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="79" to="86" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b276">
	<analytic>
		<title level="a" type="main">Analyzing brain networks with pca and conditional Granger causality</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhou ; Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Human brain mapping</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2197" to="2206" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b277">
	<analytic>
		<title level="a" type="main">Bias reduction in the estimation of mutual information</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu ; Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Bellanger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Bouquin Jeannès</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">52714</biblScope>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b278">
	<analytic>
		<title level="a" type="main">Contribution to transfer entropy estimation via the k-nearest-neighbors approach</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu ; Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Bellanger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><surname>Le Bouquin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jeannès</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Entropy</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="4173" to="4201" />
			<date type="published" when="2015">2015. 2015a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b279">
	<analytic>
		<title level="a" type="main">Investigating bias in non-parametric mutual information estimation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu ; Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Bellanger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><surname>Le Bouquin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jeannès</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2015">2015. 2015b</date>
			<biblScope unit="page" from="3971" to="3975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b280">
	<analytic>
		<title level="a" type="main">Adaptive kernels and transfer entropy for neural connectivity analysis in eeg signals</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Bellanger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Bouquin Jeannès</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IRBM</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="330" to="336" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
