<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FOUNDATIONS OF STRUCTURAL CAUSAL MODELS WITH CYCLES AND LATENT VARIABLES</title>
				<funder>
					<orgName type="full">NWO</orgName>
				</funder>
				<funder ref="#_qb2EnqJ">
					<orgName type="full">VILLUM FONDEN</orgName>
				</funder>
				<funder>
					<orgName type="full">Carlsberg Foundation</orgName>
				</funder>
				<funder ref="#_y9TuCGq">
					<orgName type="full">Netherlands Organization for Scientific Research</orgName>
				</funder>
				<funder ref="#_vTgWCDt">
					<orgName type="full">European Research Council</orgName>
					<orgName type="abbreviated">ERC</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2021-11-22">22 Nov 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Stephan</forename><surname>Bongers</surname></persName>
							<email>s.r.bongers@uva.nl</email>
							<affiliation key="aff0">
								<orgName type="department">Informatics Institute</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Patrick</forename><surname>Forr É1</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Informatics Institute</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jonas</forename><surname>Peters</surname></persName>
							<email>jonas.peters@math.ku.dk</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Mathematical Sciences</orgName>
								<orgName type="institution">University of Copenhagen</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Joris</forename><forename type="middle">M</forename><surname>Mooij</surname></persName>
							<email>j.m.mooij@uva.nl</email>
							<affiliation key="aff2">
								<orgName type="department">Korteweg-De Vries Institute</orgName>
								<orgName type="institution">University of Amsterdam</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">FOUNDATIONS OF STRUCTURAL CAUSAL MODELS WITH CYCLES AND LATENT VARIABLES</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-11-22">22 Nov 2021</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1214/21-AOS2064</idno>
					<idno type="arXiv">arXiv:1611.06221v6[stat.ME]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>MSC2020 subject classifications: Primary 62A09</term>
					<term>68T30; secondary 68T37 structural causal models</term>
					<term>causal graph</term>
					<term>cycles</term>
					<term>interventions</term>
					<term>counterfactuals</term>
					<term>solvability</term>
					<term>Markov properties</term>
					<term>marginalization</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Structural causal models (SCMs), also known as (nonparametric) structural equation models (SEMs), are widely used for causal modeling purposes. In particular, acyclic SCMs, also known as recursive SEMs, form a wellstudied subclass of SCMs that generalize causal Bayesian networks to allow for latent confounders. In this paper, we investigate SCMs in a more general setting, allowing for the presence of both latent confounders and cycles. We show that in the presence of cycles, many of the convenient properties of acyclic SCMs do not hold in general: they do not always have a solution; they do not always induce unique observational, interventional and counterfactual distributions; a marginalization does not always exist, and if it exists the marginal model does not always respect the latent projection; they do not always satisfy a Markov property; and their graphs are not always consistent with their causal semantics. We prove that for SCMs in general each of these properties does hold under certain solvability conditions. Our work generalizes results for SCMs with cycles that were only known for certain special cases so far. We introduce the class of simple SCMs that extends the class of acyclic SCMs to the cyclic setting, while preserving many of the convenient properties of acyclic SCMs. With this paper we aim to provide the foundations for a general theory of statistical causal modeling with SCMs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>1. Introduction Structural causal models (SCMs), also known as (nonparametric) structural equation models (SEMs), are widely used for causal modeling purposes <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b72">73]</ref>. They form the basis for many statistical methods that aim at inferring knowledge of the underlying causal structure from data [see, e.g., <ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b55">56]</ref>. In these models, the causal relationships between the variables are expressed in the form of deterministic, functional relationships, and probabilities are introduced through the assumption that certain variables are exogenous latent random variables. SCMs arose out of certain causal models that were first introduced in genetics <ref type="bibr" target="#b78">[79]</ref>, econometrics <ref type="bibr" target="#b24">[25]</ref>, electrical engineering <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b39">40]</ref> and the social sciences <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b22">23]</ref>.</p><p>Acyclic SCMs, also known as recursive SEMs, form a special well-studied subclass of SCMs that generalize causal Bayesian networks <ref type="bibr" target="#b50">[51]</ref>. They have many convenient properties [see, e.g., <ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b59">60,</ref><ref type="bibr" target="#b77">78]</ref>: (i) they induce a unique distribution over the variables; (ii) they are closed under perfect interventions; (iii) they are closed under marginalizations; (iv) their marginalization respects the latent projection; (v) they obey (various equivalent versions of) the Markov property and (vi) their graphs express the causal relationships encoded by the SCM in an intuitive manner.</p><p>One important limitation of acyclic SCMs is that they cannot model systems that involve causal cycles. In many systems occurring in the real world, there are feedback loops between observed variables. For example, in economics the price of a product may be a function of the demanded or supplied quantities, and vice versa, the demanded and supplied quantities may be functions of the price. The underlying dynamic processes describing such systems have an acyclic causal structure over time. However, causal cycles may arise when one approximates such systems over time <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b42">43]</ref> or when one describes the equilibrium states of these systems <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b56">57]</ref>. In particular, in <ref type="bibr" target="#b5">[6]</ref> it was shown that the equilibrium states of a system governed by (random) differential equations can be described by an SCM that represents their causal semantics, which gives rise to a plethora of SCMs that include cycles (we provide some examples of such feedback systems in Appendix D.1 of the Supplementary Material). In contrast to their acyclic counterparts, SCMs with cycles have enjoyed less attention in the literature and are not as well understood. In general, none of the above properties (i)-(vi) hold in the class of SCMs. However, some progress has been made in the case of discrete <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b51">52]</ref> and linear models <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b69">[70]</ref><ref type="bibr" target="#b70">[71]</ref><ref type="bibr" target="#b71">[72]</ref>, and more recently, for more general cyclic models the Markov properties have been elucidated <ref type="bibr" target="#b17">[18]</ref>.</p><p>Contributions The purpose of this paper is to provide the foundations for a general theory of statistical causal modeling with SCMs. We study properties of SCMs and allow for cycles, latent variables and nonlinear functional relationships between the variables. We investigate to which extent and under which sufficient conditions each of the properties (i)-(vi) holds, in particular, in the presence of cycles. In the next paragraphs, we describe our contributions in more detail.</p><p>When there are cyclic functional relationships between variables, one encounters various technical complications, which even arise in the linear setting. The structural equations of an acyclic SCM trivially have a unique solution. This unique solvability property ensures that the SCM gives rise to a unique, well-defined probability distribution on the variables. In the case of cycles, however, this property may be violated, and consequently, the SCM may not have a solution at all, or may allow for multiple different probability distributions <ref type="bibr" target="#b25">[26]</ref>. Even if one starts with a cyclic SCM that is uniquely solvable, performing an intervention on the SCM may lead to an intervened SCM that is not uniquely solvable. Hence, a cyclic SCM may not give rise to a unique, well-defined probability distribution corresponding to that intervention, and whether or not this happens may depend on the intervention. We provide sufficient conditions for the existence and uniqueness of these probability distributions after intervention. In general, it is not clear whether the solutions of the structural equations of an SCM are measurable if cycles are present. In addition, we provide sufficient and necessary conditions for the measurability of solution functions of cyclic SCMs.</p><p>SCMs provide a detailed modeling description of a system. Not all information may be necessary for a certain modeling task, which motivates to consider certain classes of SCMs to be equivalent. In this paper, we formally introduce several of such equivalence relations. For example, we consider two SCMs observationally equivalent if they cannot be distinguished based on observations alone. Observationally equivalent SCMs can often still be distinguished by interventions. We consider two SCMs interventionally equivalent if they cannot be distinguished based on observations and interventions. While these concepts have been around in implicit form for acyclic SCMs, we formulate them in such a way that they also apply to cyclic SCMs that have either no solution at all or have multiple different induced probability distributions on the variables. Finally, we consider two SCMs counterfactually equivalent if they cannot be distinguished based on observations and interventions and in addition encode the same counterfactual distributions, which are the distributions induced by the so-called twin SCM via the twin network method <ref type="bibr" target="#b0">[1]</ref>. These different equivalence relations formalize the different levels of abstraction in the so-called causal hierarchy <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b68">69]</ref>. In addition, we add another, strong version of equivalence, such that equivalent SCMs have the same solutions. This notion clarifies ambiguities when a function is constant in one of its arguments, for example.</p><p>Marginalization becomes useful if not all variables are observed: given a joint probability distribution on some variables, we obtain a marginal distribution on a subset of the variables by integrating out the remaining variables. Analogously, we can marginalize an acyclic SCM by substituting the solutions of the structural equations of a subset of the endogenous variables into the structural equations of the remaining endogenous variables. For acyclic SCMs, the induced observational and interventional distributions of the marginalized SCM coincide with the marginals of the distributions induced by the original SCM [see <ref type="bibr">15, 16, 75, 78, a.o.]</ref>. In other words, for acyclic SCMs the operation of marginalization preserves the probabilistic and causal semantics (restricted to the remaining variables). We show that for cyclic SCMs a marginalization does not always exist without further assumptions. In <ref type="bibr" target="#b17">[18]</ref> it is shown that for modular SCMs, which can be seen as an SCM together with an additional structure of a compatible system of solution functions, a marginalization can be defined that preserves the probabilistic and causal semantics. We prove that this additional structure is not necessary and use a local unique solvability condition instead. Under this condition, we show that an SCM and its marginalization are observationally, interventionally and counterfactually equivalent on the remaining endogenous variables. Analogously, we define a marginalization operation on the associated graph of an SCM, which generalizes the latent projection <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b75">76,</ref><ref type="bibr" target="#b77">78]</ref>. In general, the marginalization of an SCM does not respect the latent projection of its associated graph, but we show that it does so under an additional local ancestral unique solvability condition.</p><p>In graphical models, Markov properties allow one to read off conditional independencies in a distribution directly from a graph. Various equivalent formulations of Markov properties exist for acyclic SCMs <ref type="bibr" target="#b33">[34]</ref>, one prominent example being the d-separation criterion, also known as the directed global Markov property, which was originally derived for Bayesian networks <ref type="bibr" target="#b49">[50]</ref>. Markov properties have been of key importance to derive various central results regarding causal reasoning and causal discovery. For cyclic SCMs, however, the usual Markov properties do not hold in general, as was already pointed out by Spirtes <ref type="bibr" target="#b70">[71]</ref>. His solution in terms of collapsed graphs was recently generalized and reformulated for a general class of causal graphical models <ref type="bibr" target="#b17">[18]</ref> by adapting the notion of d-separation into what has been termed σ-separation. This resulted in a general directed global Markov property expressed in terms of σ-separation instead of d-separation. Here, we formulate these general Markov properties specifically within the framework of SCMs. Again, they only hold under certain unique solvability conditions.</p><p>In addition to its interpretation in terms of conditional independencies, the graph of an acyclic SCM also has a direct causal interpretation <ref type="bibr" target="#b50">[51]</ref>. As was already observed in <ref type="bibr" target="#b48">[49]</ref>, the causal interpretation of SCMs with cycles can be counterintuitive, as the causal semantics under interventions no longer needs to be compatible with the structure imposed by the functional relations between the variables. We resolve this issue by showing that under certain ancestral unique solvability conditions the causal interpretation of SCMs is consistent with its graph.</p><p>Cycles lead to several technical complications related to solvability issues. We introduce a special subclass of (possibly cyclic) SCMs, the class of simple SCMs, for which most of these technical complications are absent and which preserves much of the simplicity of the theory for acyclic SCMs. A simple SCM is an SCM that is uniquely solvable with respect to every subset of the variables. Because of this strong solvability assumption, simple SCMs have all the convenient properties (i)-(vi): they always have uniquely defined observational, interventional and counterfactual distributions; we can perform every perfect intervention and marginalization on them and the result is again a simple SCM; marginalization does respect Overview of the objects constructed from an SCM and the mappings between them. The numbers correspond to the definition, proposition or theorem of the corresponding object, mapping or result. When an arrow is dashed, the relation only holds under nontrivial assumptions that can be found in the corresponding definition or theorem. The symbol "⊆" stands for the subgraph of a directed mixed graph (see Definition A.1 in the Supplementary Material) and the symbol " " denotes that the surrounding diagram commutes. Table <ref type="table" target="#tab_1">1</ref> gives an overview of the commutativity results for each pair of mappings between the objects with the names in bold.  ). All results apply under the assumptions stated in the corresponding proposition. The entries denoted by dots are omitted due to symmetry. We do not consider the commutativity of the twin operation with itself in this paper. <ref type="bibr">Proposition 5.11</ref> (in parentheses) is not a commutativity result but a weaker relation. The graphical twin operator is only defined for directed graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SCMs</head><p>the latent projection; they obey the general directed global Markov property, and for special cases (including the acyclic, linear and discrete case) they obey the (stronger) directed global Markov property; their graphs have a direct and intuitive causal interpretation. The scope of this paper is limited to establishing the foundations for statistical causal modeling with cyclic SCMs (Figure <ref type="figure">7</ref> in Appendix A.4 of the Supplementary Material shows an overview of how SCMs relate to other causal graphical models). For a detailed discussion of causal reasoning, causal discovery and causal prediction with cyclic SCMs we refer the reader to other literature [e.g., <ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b57">58,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b60">61]</ref>. Several recent results (generalizations of the do-calculus, adjustment criteria and an identification algorithm) for modular SCMs <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20]</ref> directly apply to the subclass of simple SCMs, as well. Finally, many causal discovery algorithms that have been designed for the acyclic case also apply to simple SCMs with no or only minor changes <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b46">47]</ref>.</p><p>Overview Figure <ref type="figure" target="#fig_0">1</ref> gives an overview of the different objects that can be constructed from an SCM and the different mappings between them. For pairs of mappings between the objects with the names in bold, we prove commutativity results which are summarized in Table <ref type="table" target="#tab_1">1</ref>.</p><p>Outline This paper is structured as follows: In Section 2, we provide a formal definition of SCMs and a natural notion of equivalence between SCMs, define the (augmented) graph corresponding to an SCM, and describe perfect interventions and counterfactuals. In Section 3, we discuss the concept of (unique) solvability, its properties and how it relates to self-cycles. In Section 4, we define and relate various equivalence relations between SCMs. In Section 5, we define a marginalization operation that is applicable to cyclic SCMs under certain conditions. We discuss several properties of this marginalization operation and discuss the relation with a marginalization operation defined on directed mixed graphs. In Section 6, we discuss Markov properties of SCMs. In Section 7, we discuss the causal interpretation of the graphs of SCMs. Section 8 introduces and discusses the class of simple SCMs.</p><p>The Supplementary Material introduces causal graphical models in Appendix A. This section also contains details on Markov properties and modular SCMs. Appendix B provides additional (unique) solvability properties, some results for linear SCMs are discussed in Appendix C, other examples in Appendix D and the proofs of all the theoretical results are in Appendix E. Appendix F contains some lemmas and measurable selection theorems that are used in several proofs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Structural causal models</head><p>In this section, we provide the definition and properties of structural causal models (SCMs). Our definition of SCMs slightly deviates from existing definitions <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b72">73]</ref>, because we make the definition of the SCM independent of the random variables that solve it. This enables us to deal with the various technical complications that arise in the presence of cycles. M := I, J , X , E, f , P E , where 1. I is a finite index set of endogenous variables, 2. J is a disjoint finite index set of exogenous variables, 3. X = i∈I X i is the product of the domains of the endogenous variables, where each domain X i is a standard measurable space (see Definition F.1), 4. E = j∈J E j is the product of the domains of the exogenous variables, where each domain E j is a standard measurable space, 5. f : X × E → X is a measurable function that specifies the causal mechanism, <ref type="bibr" target="#b5">6</ref>. P E = j∈J P Ej is a product measure, the exogenous distribution, where P Ej is a probability measure on E j for each j ∈ J . <ref type="foot" target="#foot_1">2</ref>In SCMs, the functional relationships between variables are expressed in terms of deterministic equations, where each equation expresses an endogenous variable (on the left-hand side) in terms of a causal mechanism depending on endogenous and exogenous variables (on the right-hand side). This allows us to model interventions in an unambiguous way by changing the causal mechanisms that target specific endogenous variables (see Section 2.4). DEFINITION 2.2 (Structural equations). Let M = I, J , X , E, f , P E be an SCM. We call the set of equations</p><formula xml:id="formula_0">x i = f i (x, e)</formula><p>x ∈ X , e ∈ E for i ∈ I the structural equations of the structural causal model M.</p><p>Although it is common to assume the absence of cyclic functional relations (see Definition 2.9), we make no such assumption here. In particular, we allow for self-cycles, which we will discuss in more detail in Sections 2.2 and 3.3.</p><p>The solutions of an SCM in terms of random variables are defined up to almost sure equality. Random variables that are almost surely equal are generally considered to be equivalent to each other for all practical purposes. <ref type="bibr">DEFINITION 2.3 (Solution)</ref>. A pair (X, E) of random variables X : Ω → X , E : Ω → E, where Ω is a probability space, is a solution of the SCM M = I, J , X , E, f , P E if 1. P E = P E , that is, the distribution of E is equal to P E , 3 and 2. the structural equations are satisfied, that is,</p><formula xml:id="formula_1">X = f (X, E) a.s..</formula><p>For convenience, we call a random variable X a solution of M if there exists a random variable E such that (X, E) forms a solution of M.</p><p>Often, the endogenous random variables X can be observed, while the exogenous random variables E are treated as latent. Latent exogenous variables are often referred to as "disturbance terms" or "noise variables." For a solution X, we call the distribution P X the observational distribution of M associated to X. In general, there may be multiple different observational distributions associated to an SCM due to the existence of different solutions of the structural equations. This is a consequence of the allowance of cycles in SCMs, as the following simple example illustrates. EXAMPLE 2.4 (Cyclic SCMs). For brevity, we use throughout this paper the notation n := {1, 2, . . . , n} for n ∈ N. Let M = 2, 1, R 2 , R, f , P R be an SCM 4 with f 1 (x, e) = x 2 and f 2 (x, e) = x 1 , and P R an arbitrary probability measure on R. Then (X, X) is a solution of M for any arbitrary random variable X with values in R. Hence, any probability distribution on {(x, x) : x ∈ R} is an observational distribution associated to M. Now consider instead the same SCM but with f 1 (x, e) = x 2 + 1. This SCM has no solutions at all, and hence induces no observational distribution.</p><p>Due to the fact that the structural equations only need to be satisfied almost surely, there may exist many different SCMs representing the same set of solutions (see Example D.4). It therefore seems natural not to differentiate between structural equations that have different solutions on at most a P E -null set of exogenous variables. This leads to an equivalence relation between SCMs. To be able to state the equivalence relation concisely, we introduce the following notation: For subsets U ⊆ I and V ⊆ J , we write X U := i∈U X i and E V := j∈V E j . In particular, X ∅ and E ∅ are defined by the singleton 1. Moreover, for a subset W ⊆ I ∪ J , we use the convention that we write X W and E W instead of X W∩I and E W∩J , respectively and we adopt a similar notation for the (random) variables in those spaces, that is, we write x W and e W instead of x W∩I and e W∩J , respectively. This allows us to define the following natural equivalence relation for SCMs. 5,6   DEFINITION 2.5 (Equivalence). The two SCMs M = I, J , X , E, f , P E and M = I, J , X , E, f , P E are equivalent, denoted by M ≡ M, if for all i ∈ I, for P E -almost every e ∈ E and for all x ∈ X ,</p><formula xml:id="formula_2">x i = f i (x, e) ⇐⇒ x i = fi (x, e).</formula><p>Thus, two equivalent SCMs can only differ in terms of their causal mechanism. Importantly, equivalent SCMs have the same solutions and, as we will see in Sections 2.4 and 2.5, they have the same causal and counterfactual semantics (see Definitions 2.12 and 2.17, respectively). This equivalence relation on the set of all SCMs gives rise to the quotient set of equivalence classes of SCMs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">The (augmented) graph</head><p>We will now define two types of graphs that can be used for representing structural properties of the SCM. These graphical representations are related to Wright's path diagrams <ref type="bibr" target="#b78">[79]</ref>. The structural properties of the functional relations between variables modeled by an SCM are specified by the causal mechanism of the SCM and can be encoded in an (augmented) graph. For the graphical notation and standard terminology on directed (mixed) graphs that is used throughout this paper, we refer the reader to Appendix A.1.</p><p>We first define the parents of an endogenous variable.</p><p>DEFINITION 2.6 (Parent). Let M = I, J , X , E, f , P E be an SCM. We call k ∈ I ∪ J a parent of i ∈ I if and only if there does not exist a measurable function 7  fi : X \k × E \k → X i such that for P E -almost every e ∈ E and for all x ∈ X ,</p><formula xml:id="formula_3">x i = f i (x, e) ⇐⇒ x i = fi (x \k , e \k ).</formula><p>Exogenous variables have no parents by definition. These parental relations are preserved under the equivalence relation ≡ on SCMs. They can be represented by a directed graph or a directed mixed graph. 8   DEFINITION 2.7 (Graph and augmented graph). Let M = I, J , X , E, f , P E be an SCM. We define: almost every x ∈ X under the observational distribution P X " will not lead to a well-defined equivalence relation, since in general the observational distribution P X may be nonunique or even nonexistent. Refining it by replacing the quantifier "for P E -almost every e ∈ E" by "for all e ∈ E" would make it too fine for our purposes, since we assume the exogenous distribution to be fixed and we assume as usual that random variables that are almost surely identical are indistinguishable in practice. Note that the "for P E -almost every e ∈ E" and "for all x ∈ X " quantifiers do not commute in general (see Example D.5) 6 We may extend this definition to allow J = J and for a larger class of SCMs such that the exogenous distribution does not factorize. Then, for any M that satisfies Definition 2.1, except for that it may have a nonfactorizing exogenous distribution, there exists an equivalent SCM with a factorizing exogenous distribution (and a different J ); the latter can be obtained by partitioning the exogenous components into independent tuples. This motivates why we can restrict ourselves in Definition 2.1 to factorizing exogenous distributions only. For some more discussion on the representation of latent confounders, see also Example D.6. 7 For X = i∈I X i , I some index set, I ⊆ I and k ∈ I, we denote X \I = i∈I\I X i and X \k = i∈I\{k} X i , and similarly for their elements. 8 A directed mixed graph G = (V, E, B) consists of a set of nodes V, a set of directed edges E and a set of bidirected edges B (see Definition A.1 for a more precise definition). 1. the augmented graph G a (M) as the directed graph with nodes I ∪ J and directed edges u → v if and only if u ∈ I ∪ J is a parent of v ∈ I; 2. the graph G(M) as the directed mixed graph with nodes I, directed edges u → v if and only if u ∈ I is a parent of v ∈ I and bidirected edges u ↔ v if and only if there exists a j ∈ J that is a parent of both u ∈ I and v ∈ I.</p><formula xml:id="formula_4">E 1 E 2 E 3 X 1 X 2 X 3 X 4 X 5 G a (M) X 1 X 2 X 3 X 4 X 5 G(M) X 1 X 2 X 3 X 4 X 5 G(M do({3},<label>1</label></formula><p>We call the mappings G a and G, that map M to G a (M) and G(M), the augmented graph mapping and the graph mapping, respectively.</p><p>In particular, the augmented graph contains no directed edges pointing toward an exogenous variable, that is, u ∈ I ∪J cannot be a parent of v ∈ J , because they are not functionally related through the causal mechanism. We call a directed edge i → i in G a (M) and G(M) (here, i is a parent of itself) a self-cycle at i. By definition, the mappings G a and G are invariant under the equivalence relation ≡ on SCMs, and hence the equivalence class of an SCM M is mapped to a unique augmented graph G a (M) and a unique graph G(M). EXAMPLE 2.8 (Graphs of an SCM). Let M = 5, 3, R 5 , R 3 , f , P R 3 be an SCM with causal mechanism given by</p><formula xml:id="formula_5">f 1 (x, e) = x 1 -x 2 1 + αe 2 1 , f 3 (x, e) = -x 4 + e 2 , f 5 (x, e) = x 4 • e 3 , f 2 (x, e) = x 1 + x 3 + x 4 + e 1 , f 4 (x, e) = x 2 + e 2 ,</formula><p>where α = 0 and P R 3 is a product of three probability measures P R over R that are nondegenerate. The augmented graph G a (M) and the graph G(M) of M are depicted<ref type="foot" target="#foot_3">foot_3</ref> in Figure <ref type="figure" target="#fig_1">2</ref> (left and center). Observe that if α had been equal to zero, then the endogenous variable 1 would not have any parents in G a (M), that is, it would not have a self-cycle and directed edge from any exogenous variables in G a (M), and it would not have a self-cycle and bidirected edge from any other variable in G(M). Moreover, if one of the probability measures P R over R were degenerate, then some of the directed edges from the exogenous variables to the endogenous variables in the augmented graph G a (M) and bidirected edges in the graph G(M) would be missing.</p><p>As is illustrated in this example, the augmented graph provides a more detailed representation than the graph. Therefore, we use the augmented graph as the standard graphical representation for SCMs, unless stated otherwise. For an SCM M, we denote the sets pa G a (M) (U), ch G a (M) (U), an G a (M) (U), etc., for some subset U ⊆ I ∪ J , by respectively pa(U), ch(U), an(U), etc., when the notation is clear from the context. DEFINITION 2.9. We call an SCM M acyclic if G a (M) is a directed acyclic graph (DAG). Otherwise, we call M cyclic.</p><p>Equivalently, an SCM M is acyclic if G(M) is an acyclic directed mixed graph (ADMG) <ref type="bibr" target="#b59">[60]</ref>. Acyclic SCMs are also known as semi-Markovian SCMs <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b75">76]</ref>. A commonly considered class of acyclic SCMs are the Markovian SCMs, which are acyclic SCMs for which each exogenous variable has at most one child. Several Markov properties were first shown for these models <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b75">76]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Structurally minimal representations</head><p>We have discussed an equivalence relation between SCMs in Section 2.1. In this subsection, we show that for each SCM there exists a representative of the equivalence class of that SCM for which each component of the causal mechanism does not depend on its nonparents [see also <ref type="bibr" target="#b54">55]</ref>. DEFINITION 2.10 (Structurally minimal SCM). Let M = I, J , X , E, f , P E be an SCM. We call M structurally minimal if for all i ∈ I there exists a mapping fi : X pa(i) × E pa(i) → X i such that f i (x, e) = fi (x pa(i) , e pa(i) ) for all e ∈ E and all x ∈ X .</p><p>We already encountered a structurally minimal SCM M in Example 2.8. Taking instead α = 0 in that example gives an SCM M that is not structurally minimal, since the endogenous variable 1 is then not a parent of itself, while f 1 (x, e) depends on x 1 . However, the equivalent SCM where we have replaced the causal mechanism of 1 by f 1 (x, e) = 0 yields a structurally minimal SCM. In general, there always exists an equivalent structurally minimal SCM. PROPOSITION 2.11 (Existence of a structurally minimal SCM). For an SCM M = I, J , X , E, f , P E , there exists an equivalent SCM M = I, J , X , E, f , P E that is structurally minimal.</p><p>For a causal mechanism f : X × E → X and a subset U ⊆ I, we write f U : X × E → X U for the U components<ref type="foot" target="#foot_4">foot_4</ref> of f . A structurally minimal representation is compatible with the (augmented) graph, in the sense that for every U ⊆ I there exists a unique measurable mapping fU : X pa(U ) × E pa(U ) → X U such that f U (x, e) = fU (x pa(U ) , e pa(U ) ) for all e ∈ E and all x ∈ X . Moreover, for any U ⊆ I there exists a unique measurable mapping fan(U) : X an(U ) × E an(U ) → X an(U ) with f an(U ) (x, e) = fU (x an(U ) , e an(U ) ) for all e ∈ E and all x ∈ X .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4.">Interventions</head><p>To define the causal semantics of SCMs, we consider here an idealized class of interventions introduced by Pearl <ref type="bibr" target="#b50">[51]</ref> that we refer to as perfect interventions. Other types of interventions, like mechanism changes <ref type="bibr" target="#b76">[77]</ref>, fat-hand interventions <ref type="bibr" target="#b12">[13]</ref>, activity interventions <ref type="bibr" target="#b44">[45]</ref> and stochastic versions of all these are at least as relevant, but we do not consider them here. DEFINITION 2.12 (Perfect intervention on an SCM). Let M = I, J , X , E, f , P E be an SCM, I ⊆ I a subset of endogenous variables and ξ I ∈ X I a value. The perfect intervention do(I, ξ I ) maps M to the SCM M do(I,ξI ) := I, J , X , E, f , P E , where the intervened causal mechanism f is given by</p><formula xml:id="formula_6">fi (x, e) = ξ i i ∈ I f i (x, e) i ∈ I \ I .</formula><p>This operation do(I, ξ I ) preserves the equivalence relation (see Definition 2.5) on the set of all SCMs, and hence this mapping induces a well-defined mapping on the set of equivalence classes of SCMs. Previous work has considered interventions only on a specific subset of endogenous variables <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b66">67]</ref>. Instead, we assume that we can intervene on any subset of endogenous variables in the model.</p><p>We define an analogous operation do(I) on directed mixed graphs. DEFINITION 2.13 (Perfect intervention on a directed mixed graph). Let G = (V, E, B) be a directed mixed graph and I ⊆ V a subset. The perfect intervention do(I) maps G to the directed mixed graph do(I)(G) := (V, Ẽ, B), where</p><formula xml:id="formula_7">Ẽ = E \ {v → i : v ∈ V, i ∈ I} and B = B \ {v ↔ i : v ∈ V, i ∈ I}.</formula><p>This operation simply removes all incoming edges on the nodes in I. The two notions of intervention are compatible with the (augmented) graph mapping. PROPOSITION 2.14. Let M = I, J , X , E, f , P E be an SCM, I ⊆ I a subset of endogenous variables and</p><formula xml:id="formula_8">ξ I ∈ X I a value. Then G a • do(I, ξ I ) (M) = do(I) • G a (M) and G • do(I, ξ I ) (M) = do(I) • G (M).</formula><p>The two notions of perfect intervention satisfy the following elementary properties. PROPOSITION 2.15. For an SCM and a directed mixed graph, we have the following properties:</p><p>1. perfect interventions on disjoint subsets of variables commute; 2. acyclicity is preserved under perfect intervention.</p><p>The following example shows that an SCM with a solution may not have a solution anymore after performing a perfect intervention on the SCM, and vice versa that an SCM without a solution may yield an SCM with a solution after intervention. EXAMPLE 2.16 (Intervened SCM and its graphs). Consider the SCM M of Example 2.8 which has a solution if and only if α ≥ 0. Applying the perfect intervention do({3}, 1) to M gives the intervened model M do({3},1) with the intervened causal mechanism</p><formula xml:id="formula_9">f1 (x, e) = x 1 -x 2 1 + αe 2 1 , f3 (x, e) = 1 , f5 (x, e) = x 4 • e 3 , f2 (x, e) = x 1 + x 3 + x 4 + e 1 , f4 (x, e) = x 2 + e 2 ,</formula><p>for which the graph G(M do({3},1) ) is depicted in Figure <ref type="figure" target="#fig_1">2</ref> (right). This is an example where a perfect intervention leads to an intervened SCM M do({3},1) that does not have a solution anymore. In addition, performing a perfect intervention do({4}, 1) on M do({3},1) yields again an SCM with a solution for α ≥ 0.</p><p>Recall that for each solution X of an SCM M we call the distribution P X the observational distribution of M associated to X. For cyclic SCMs, the observational distribution is in general not unique. <ref type="foot" target="#foot_5">11</ref> For example, the SCM M of Example 2.8 has two different observational distributions if α &gt; 0. Similarly, an intervened SCM may induce a distribution that is not unique. Whenever the intervened SCM M do(I,ξI ) has a solution X we therefore call the distribution P X the interventional distribution of M under the perfect intervention do(I, ξ I ) associated to X. <ref type="foot" target="#foot_6">12</ref>2.5. Counterfactuals The causal semantics of an SCM are described by the interventions on the SCM. Adding another layer of complexity, one can describe the counterfactual semantics of an SCM by the interventions on the so-called twin SCM, an idea introduced in <ref type="bibr" target="#b0">[1]</ref>. DEFINITION 2.17 (Twin SCM). Let M = I, J , X , E, f , P E be an SCM. The twin operation maps M to the twin structural causal model (twin SCM)</p><formula xml:id="formula_10">M twin := I ∪ I , J , X × X , E, f , P E , where I = {i : i ∈ I} is a copy of I and the causal mechanism f : X × X × E → X × X is the measurable function given by f (x, x , e) = f (x, e), f (x , e) .</formula><p>The twin operation on SCMs preserves the equivalence relation ≡ on the set of all SCMs. We define an analogous twin operation twin(I) on directed graphs. DEFINITION 2.18 (Twin graph). Let G = (V, E) be a directed graph and I ⊆ V a subset such that J := V \ I is exogenous, that is, pa G (J ) = ∅. The twin(I) operation maps G to the twin graph w.r.t. I defined by twin(I)(G) := ( Ṽ, Ẽ), where:</p><formula xml:id="formula_11">1. Ṽ = V ∪ I , where I is a copy of I, 2. Ẽ = E ∪ E</formula><p>, where E is given by</p><formula xml:id="formula_12">E = {j → i : j ∈ J , i ∈ I, j → i ∈ E} ∪ { ĩ → i : ĩ, i ∈ I, ĩ → i ∈ E}</formula><p>with i , ĩ ∈ I the respective copies of i, ĩ ∈ I.</p><p>Twin operations are compatible with the augmented graph mapping and preserve acyclicity. <ref type="bibr">PROPOSITION 2.19</ref>.</p><formula xml:id="formula_13">Let M = I, J , X , E, f , P E be an SCM. Then (G a • twin)(M) = (twin(I) • G a )(M).</formula><p>PROPOSITION 2.20. For SCMs and directed graphs, we have that acyclicity is preserved under the twin operation.</p><p>The perfect intervention and the twin operation for SCMs and directed graphs commute with each other in the following way. PROPOSITION 2.21. Let M = I, J , X , E, f , P E be an SCM and G = (V, E) a directed graph. Then we have that perfect intervention commutes with the twin operation on both: where I is the copy of I in I and ξ I = ξ I .</p><p>Whenever the intervened twin SCM (M twin ) do( Ĩ,ξĨ ) , where Ĩ ⊆ I ∪ I and ξ Ĩ ∈ X Ĩ , has a solution (X, X ), we call the distribution P (X,X ) the counterfactual distribution of M under the perfect intervention do( Ĩ, ξ Ĩ ) associated to (X, X ). In Example D.3, we provide an example of how counterfactuals can be sensibly formulated for a well-known market equilibrium model described in terms of a cyclic SCM. The interpretation of counterfactual statements has received a lot of attention in the literature <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b65">66]</ref>. For acyclic graphs, an alternative graphical approach to counterfactuals is the framework of Single World Intervention Graphs (SWIGs) <ref type="bibr" target="#b63">[64]</ref>. One topic of discussion is that there exist SCMs that induce the same observational and interventional distributions, but differ in their counterfactual statements <ref type="bibr" target="#b10">[11]</ref> (see also Example D.7). This raises the question how one can estimate such SCMs from data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Solvability</head><p>In this section, we introduce the notions of solvability and unique solvability with respect to a subset of the endogenous variables of an SCM. They describe the existence and uniqueness of measurable solution functions for the subsystem of structural equations that correspond with a certain subset of the endogenous variables. These notions play a central role in formulating sufficient conditions under which several properties of acyclic SCMs may be extended to the cyclic setting. For example, we show that solvability of an SCM is a sufficient and necessary condition for the existence of a solution of an SCM. Further, unique solvability of an SCM implies the uniqueness of the induced observational distribution. DEFINITION 3.1 (Solvability). Let M = I, J , X , E, f , P E be an SCM. We call M solvable w.r.t. O ⊆ I if there exists a measurable mapping g O : X pa(O)\O × E pa(O) → X O such that for P E -almost every e ∈ E and for all x ∈ X ,</p><formula xml:id="formula_14">x O = g O (x pa(O)\O , e pa(O) ) =⇒ x O = f O (x, e) .</formula><p>We then call g O a measurable solution function w.r.t. O for M. We call M solvable if it is solvable w.r.t. I.</p><p>By definition, solvability w.r.t. a subset respects the equivalence relation ≡ on SCMs. The measurable solution functions w.r.t. a certain subset do not always exist, and if they exist, they are not always uniquely defined. For example, for the SCM M in Example 2.8, the measurable solution functions w.r.t. {1} are given by g ± 1 (e 1 ) = ± αe 2 1 if and only if α ≥ 0. The following theorem states that various possible notions of "solvability" are equivalent. THEOREM 3.2 (Sufficient and necessary conditions for solvability). For an SCM M = I, J , X , E, f , P E , the following are equivalent:</p><p>1. M has a solution (see Definition 2.3); 2. for P E -almost every e ∈ E the structural equations x = f (x, e) have a solution x ∈ X ; 3. M is solvable (see Definition 3.1). While in the acyclic case, the above theorem is almost trivial, in the cyclic case the measure-theoretic aspects are not that obvious. In particular, to prove the existence of a measurable solution function g : E pa(I) → X in case the structural equations have a solution for almost every e ∈ E, we make use of a strong measurable selection theorem (see Theorem F.8 or <ref type="bibr" target="#b29">[30]</ref>). This theorem implies that if there exists a solution X : Ω → X , then there necessarily exists a random variable E : Ω → E and a mapping g : E pa(I) → X such that g(E pa(I) ) is a solution. However, it does not imply that there necessarily exists a random variable E : Ω → E and a mapping g : E pa(I) → X such that X = g(E pa(I) ) holds a.s., for example, if X is a nontrivial mixture of such solutions (see Example D.8).</p><formula xml:id="formula_15">X 1 X 2 X 3 X 4 G(M) X 1 X 2 X 3 X 4 G( M) X 1 X 2 G( M) X 1 X 2 G( M)</formula><p>Solvability w.r.t. a strict subset of I is in general neither sufficient nor necessary for the existence of a (global) solution of the SCM. Consider, for example, the SCM M in Example 2.8 with α &lt; 0. Even though this SCM is solvable w.r.t. {2, 3, 4}, it is not (globally) solvable, and hence does not have any solution. In Proposition B.1, we provide a sufficient condition for solvability w.r.t. a strict subset of I that is similar to condition (2) in Theorem 3.2 in the sense that it is formulated in terms of the solutions of (a subset of) the structural equations without requiring measurability of the solutions. For the class of linear SCMs, we provide in Proposition C.2 a sufficient and necessary condition for solvability w.r.t. a subset of I.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Unique solvability</head><p>The notion of unique solvability w.r.t. a subset O ⊆ I is similar to the notion of solvability, but with the additional requirement that the measurable solution function g O : X pa(O)\O × E pa(O) → X O is unique up to a P E -null set. DEFINITION 3.3 (Unique solvability). Let M = I, J , X , E, f , P E be an SCM. We call M uniquely solvable w.r.t. O ⊆ I if there exists a measurable mapping g O : X pa(O)\O × E pa(O) → X O such that for P E -almost every e ∈ E and for all x ∈ X ,</p><formula xml:id="formula_16">x O = g O (x pa(O)\O , e pa(O) ) ⇐⇒ x O = f O (x, e) .</formula><p>We call M uniquely solvable if it is uniquely solvable w.r.t. I.</p><p>If M ≡ M and M is uniquely solvable w.r.t. O, then M is uniquely solvable w.r.t. O, too, and the same mapping g O is a measurable solution function w.r.t. O for both M and M.</p><p>The following result explains why the notions of (unique) solvability do not play an important role in the theory of acyclic SCMs. PROPOSITION 3.4. An acyclic SCM M = I, J , X , E, f , P E is uniquely solvable w.r.t. every subset O ⊆ I.</p><p>We now illustrate that also cyclic SCMs can be uniquely solvable w.r.t. every subset. EXAMPLE 3.5 (Cyclic SCM, uniquely solvable w.r.t. each subset). Consider the SCM M = 4, 4, R 4 , R 4 , f , P R 4 with causal mechanism given by f 1 (x, e) = e 1 , f 2 (x, e) = e 2 , f 3 (x, e) = x 1 x 4 + e 3 , f 4 (x, e) = x 2 x 3 + e 4 and P R 4 the standard-normal distribution on R 4 . This SCM M is uniquely solvable w.r.t. every subset and its (augmented) graph includes a cycle (see Figure <ref type="figure" target="#fig_4">3</ref>). Theorem 3.2 provides sufficient and necessary conditions for (global) solvability. The next theorem states that under the additional uniqueness requirement there exists a sufficient and necessary condition for unique solvability w.r.t. any subset (for solvability w.r.t. a subset we only have the sufficient condition provided in Proposition B.1), and moreover, that all solutions of a uniquely solvable SCM induce the same observational distribution. THEOREM 3.6 (Sufficient and necessary conditions for unique solvability). Let M = I, J , X , E, f , P E be an SCM and O ⊆ I a subset. The following are equivalent:</p><p>1. for P E -almost every e ∈ E and for all x \O ∈ X \O the structural equations 3.3. Self-cycles One can think of a structural equation of a single endogenous variable i ∈ I as describing a small subsystem that interacts with the rest of the system. If the output x i of this subsystem is uniquely determined by the input (x \i , e) from the rest of the system (up to a P E -null set), then i is not a parent of itself (see Definition 2.6). PROPOSITION 3.7 (Self-cycles). The SCM M = I, J , X , E, f , P E is uniquely solvable w.r.t. {i} for i ∈ I if and only if G a (M) (or G(M)) has no self-cycle i → i at i ∈ I.</p><formula xml:id="formula_17">x O = f O (x,</formula><p>A self-cycle at an endogenous variable denotes that that variable is not uniquely determined by its parents, up to a P E -null set. This implies that an SCM with a self-cycle at an endogenous variable in its graph can be either solvable, or not solvable, w.r.t. that variable. For the SCM M of Example 2.8, we have indeed that it is solvable w.r.t. {1} for α &gt; 0, while for α &lt; 0 it is not. For linear SCMs with structural equations X i = j∈I B ij X j + k∈J Γ ik E k , the endogenous variable i ∈ I has a self-cycle if and only if B ii = 1 (see also Appendix C).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Interventions</head><p>The property of (unique) solvability is in general not preserved under perfect intervention. For example, a (uniquely) solvable SCM can lead to a nonuniquely solvable SCM after intervention, which either has no solution or has solutions with multiple induced distributions (see, e.g., Examples 2.16 and D.9). A sufficient condition for the intervened SCM to be (uniquely) solvable is that the original SCM has to be (uniquely) solvable w.r.t. the subset of nonintervened endogenous variables. PROPOSITION 3.8. Let M = I, J , X , E, f , P E be an SCM that is (uniquely) solvable w.r.t. O ⊆ I. Then, for any set I such that pa(O) \ O ⊆ I ⊆ I \ O and value ξ I ∈ X I the intervened SCM M do(I,ξI ) is (uniquely) solvable w.r.t. O ∪ I. Proposition 3.4 shows that acyclic SCMs are uniquely solvable w.r.t. every subset and hence are uniquely solvable after every perfect intervention. This also directly follows from the fact that acyclicity is preserved under perfect intervention (see Proposition 2.15). Moreover, since acyclicity is preserved under the twin operation (see Proposition 2.20), an acyclic SCM induces unique observational, interventional and counterfactual distributions. </p><formula xml:id="formula_18">X 1 X 2 X 3 X 4 L X 1 X 4 marg(L)</formula><formula xml:id="formula_19">(M) on O. A subset A ⊆ O is called an ancestral subset in G(M) O if A = an G(M)O (A)</formula><p>, where an G(M)O (A) are the ancestors of A according to the induced subgraph<ref type="foot" target="#foot_7">foot_7</ref> G(M) O . DEFINITION 3.9 (Ancestral (unique) solvability). Let M = I, J , X , E, f , P E be an SCM. We call M ancestrally (uniquely) solvable w.r. </p><formula xml:id="formula_20">f 1 (x, e) = e , f 2 (x, e) = x 2 • (1 -1 {0} (x 1 -x 3 )) + 1 , f 3 (x, e) = x 3 , f 4 (x, e) = x 3</formula><p>and P R the standard-normal measure on R. This SCM is uniquely solvable w.r.t. the set {2, 3}, and thus solvable w.r.t. this set. Although it is solvable w.r.t., the ancestral subset {3} in G(M) {2,3} , depicted in Figure <ref type="figure" target="#fig_5">4</ref> (left), it is not uniquely solvable w.r.t. this subset, because the structural equation x 3 = x 3 holds for any x 3 ∈ R. Hence, it is not ancestrally uniquely solvable w.r.t. {2, 3}.</p><p>However, for the class of linear SCMs we have that unique solvability w.r.t. O always implies ancestral unique solvability w.r.t. O (see Proposition C.4).</p><p>Although in general unique solvability is not preserved under unions, in Proposition B.4 we show that if an SCM is uniquely solvable w.r.t. two ancestral subsets and w.r.t. their intersection, then it is uniquely solvable w.r.t. their union. In general, the property of ancestral unique solvability is not preserved under perfect intervention, as can be seen in Example D.9. The notion of ancestral unique solvability will appear in various results in Sections 5 and 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Equivalences</head><p>In Section 2, we already encountered an equivalence relation on the class of SCMs (see Definition 2.5). The (augmented) graph of an SCM, its solutions and its induced observational, interventional and counterfactual distributions are preserved under this equivalence relation. In this section, we give several coarser equivalence relations on the class of SCMs: observational, interventional and counterfactual equivalence. 4.1. Observational equivalence Observational equivalence is the property that two SCMs are indistinguishable on the basis of their observational distributions. DEFINITION 4.1 (Observational equivalence). Two SCMs M = I, J , X , E, f , P E and M = Ĩ, J , X , Ẽ, f , P Ẽ are observationally equivalent w.r.t. O ⊆ I ∩ Ĩ, denoted by</p><formula xml:id="formula_21">M ≡ obs(O) M, if X O = X O</formula><p>and for all solutions X of M there exists a solution X of M such that P XO = P XO and for all solutions X of M there exists a solution X of M such that P XO = P XO . M and M are called observationally equivalent if they are observationally equivalent w.r.t. I = Ĩ.</p><p>Equivalent SCMs have the same solutions, and hence they are observationally equivalent w.r.t. every subset O ⊆ I. However, observational equivalence does not imply equivalence. . This SCM M is observationally equivalent to the SCM M. Because both SCMs have a different (augmented) graph they are not equivalent to each other (see Figure <ref type="figure" target="#fig_4">3</ref>). This example shows that if two SCMs M and M are observationally equivalent, then their associated augmented graphs G a (M) and G a ( M) are not necessarily equal to each other. Although interventional equivalence is a finer notion than observational equivalence, we have that if two SCMs M and M are interventionally equivalent, then their associated augmented graphs G a (M) and G a ( M) are not necessarily equal to each other. EXAMPLE 4.4 (Interventionally equivalent SCMs with different graphs). Consider the SCM M = 2, 2, {-1, 1} 2 , {-1, 1} 2 , f , P E and the SCM M that is the same as M except for its causal mechanism f , where the causal mechanisms are given by f1 (x, e) = e 1 , f2 (x, e) = x 1 e 2 , f1 (x, e) = e 1 , f2 (x, e) = e 2 ,</p><p>and</p><formula xml:id="formula_22">P E = P E with E 1 , E 2 ∼ U({-1, 1}) uniformly distributed and E 1 ⊥ ⊥ E 2 .</formula><p>Then M and M are interventionally equivalent although G( M) is not equal to G( M) (see Figure <ref type="figure" target="#fig_4">3</ref>).</p><p>Example D.6 showcases an SCM with two endogenous and three exogenous variables, for which there is no interventionally equivalent SCM (satisfying smoothness constraints) with one exogenous variable taking values in R 2 whose first and second components enter in the first and second structural equation, respectively. In this sense, representing confounders with dependent exogenous variables can be nontrivial in nonlinear models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Counterfactual equivalence</head><p>We consider two SCMs to be counterfactually equivalent if their twin SCMs induce the same counterfactual distributions under every perfect intervention. Interventionally equivalent SCMs that have the same causal mechanism (that differ only in their exogenous distribution) may not be counterfactually equivalent (see, e.g., Example D.7). Although the notion of counterfactual equivalence is finer than the notion of observational and interventional equivalence, the (augmented) graphs for counterfactually equivalent SCMs are in general not equal to each other (see Example D.10).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Relations between equivalences</head><p>The definitions of observational, interventional and counterfactual equivalence provide equivalence relations on the set of all SCMs. For two SCMs to be observationally, interventionally or counterfactually equivalent w.r.t. O ⊆ I ∩ Ĩ, the domains of their endogenous variables O have to be equal, that is, X O = X O . Apart from that, the index sets of the endogenous and the exogenous variables, the spaces of the other endogenous and exogenous variables, the causal mechanism and the exogenous probability measure may all differ. The observational, interventional and counterfactual equivalence classes w.r. This hierarchy allows us to compare SCMs at different levels of abstraction and formally establishes the "ladder" of causation (last two implications) <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b68">69]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Marginalizations</head><p>In this section, we show how, and under which condition, one can marginalize an SCM over a subset L ⊆ I of endogenous variables (thereby "hiding" the variables L), to another SCM on the margin I \ L that is observationally, interventionally and even counterfactually equivalent with respect to I \ L. In other words, we provide a formal notion of marginalization and show that this preserves the probabilistic, causal and counterfactual semantics on the margin.</p><p>The problem of marginalization of directed graphical models has been addressed for acyclic graph structures, for example, ADMGs and mDAGs [see <ref type="bibr">15, 16, 60, 62, 78, a.</ref>o.], and more recently in <ref type="bibr" target="#b17">[18]</ref> for certain graph structures ("HEDGes") that may include cycles. Although in the acyclic setting it has been shown that the marginalization for some of these graph structures preserves the probabilistic and causal semantics, in the cyclic setting this has only been shown for modular SCMs <ref type="bibr" target="#b17">[18]</ref>. We show that without the additional structure of a compatible system of solution functions (see Appendix A.3) one can still define a marginalization for SCMs under certain local unique solvability conditions. Intuitively, the idea is that if the state of a subsystem of endogenous variables is uniquely determined by the parents outside of this subsystem, then one can ignore the internals of this subsystem by treating it as a "black box" that can be described by certain measurable solution functions (see Figure <ref type="figure" target="#fig_5">4</ref>). One can marginalize over this subsystem by substituting these measurable solution functions into the rest of the model, thereby removing the functional dependencies on the variables of the subsystem from the rest of the system, while preserving the probabilistic, causal and the counterfactual semantics of the rest of the system. We show that in general this marginalization operation defined on SCMs does not respect the latent projection on its associated (augmented) graph, where the latent projection is a similar marginalization operation defined on directed mixed graphs <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b75">76,</ref><ref type="bibr" target="#b77">78]</ref>. We show that under certain stronger local ancestral unique solvability conditions the marginalization does respect the latent projection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Marginalization of a structural causal model</head><p>Before we show how one can marginalize an SCM w.r.t. a subset of endogenous variables, we first point out that in general it is not always possible to find an SCM on the margin that preserves the causal semantics, as the following example illustrates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EXAMPLE 5.1 (No SCM on the margin preserves the causal semantics). Consider the SCM</head><formula xml:id="formula_23">M = 3, ∅, R 3 , 1, f , P 1 with causal mechanism f 1 (x) = x 1 + x 2 + x 3 , f 2 (x) = x 2 , f 3 (x) = 0 .</formula><p>Then there exists no SCM M on the endogenous variables {2, 3} that is interventionally equivalent to M w.r.t. {2, 3}. To see this, suppose there exists such an SCM M, then for every (ξ 2 , ξ 3 ) ∈ X {2,3} such that ξ 2 + ξ 3 = 0 the intervened model Mdo({2,3},(ξ2,ξ3)) has a solution but M do({2,3},(ξ2,ξ3)) does not.</p><p>More generally, for an SCM M that is not solvable w.r.t. a subset L ⊆ I there is no SCM M on the endogenous variables I \ L that is interventionally equivalent w.r.t. I \ L.</p><p>The following example illustrates that for an SCM that is uniquely solvable w.r.t. a subset there exists an SCM on the margin that preserves the causal semantics. EXAMPLE 5.2 (SCM on the margin that preserves the causal semantics). Consider the SCM M of Example 3.11 that is uniquely solvable w.r.t. the subset L = {2, 3} (depicted by the gray box in Figure <ref type="figure" target="#fig_5">4</ref>). Substituting the measurable solution functions g L into the causal mechanism components f 1 and f 4 for the remaining endogenous variables {1, 4} gives a "marginal" causal mechanism f1 (x, e) := e and f4 (x, e) := x 1 . This defines an SCM M on the margin I \ L = {1, 4} that is interventionally equivalent w.r.t. I \ L to M.</p><p>In general, for an SCM M and a given subset L ⊆ I of endogenous variables and its complement O = I \ L, we can consider the "subsystem" of structural equations</p><formula xml:id="formula_24">x L = f L (x L , x O , e). If M is uniquely solvable w.r.t. L with measurable solution function g L : X pa(L)\L × E pa(L) → X L , then for each input (x pa(L)\L , e pa(L) ) ∈ X pa(L)\L × E pa(L)</formula><p>of the subsystem, there exists an output x L ∈ X L , which is unique for P Epa(L) -almost every e pa(L) ∈ E pa(L) and for all x pa(L)\L ∈ X pa(L)\L . We can remove this subsystem of endogenous variables from the model by substitution. This leads to a marginal SCM that is observationally, interventionally and counterfactually equivalent to the original SCM w.r.t. the margin, as we prove in Theorem 5.6. DEFINITION 5.3 (Marginalization of an SCM). Let M = I, J , X , E, f , P E be an SCM that is uniquely solvable w.r.t. a subset L ⊆ I and let</p><formula xml:id="formula_25">O = I \ L. For g L : X pa(L)\L × E pa(L) → L, any measurable solution function of M w.r.t. L, we call the SCM M marg(L) := O, J , X O , E, f , P E with the marginal causal mechanism f : X O × E → X O given by f (x O , e) = f O (g L (x pa(L)\L , e pa(L) ), x O , e) ,</formula><p>a marginalization of M w.r.t. L. We denote by marg(L)(M) the equivalence class of the marginalizations of M w.r.t. L.</p><p>The marginalization of M w.r.t. L is defined up to the equivalence ≡ on SCMs, since the measurable solution functions g L are uniquely defined up to P E -null sets. With this definition at hand, we can always construct a marginal SCM over a subset of the endogenous variables of an acyclic SCM by mere substitution (see also <ref type="bibr">Proposition 3.4)</ref>. Moreover, this definition extends that notion to SCMs that are uniquely solvable w.r.t. a certain subset. For linear SCMs this condition translates into a matrix invertibility condition, and since substitution preserves linearity, marginalization yields a linear marginal SCM (see Proposition C.5).</p><p>In general, marginalization is not always defined for all subsets. For instance, the SCM of Example 3.11 cannot be marginalized over the variable 3 (due to the self-cycle at 3), but can be marginalized over the variables 2 and 3 together. It follows from Proposition 3.7 that we can only marginalize over a single variable if that variable has no self-cycle. Note that we may introduce new self-cycles if we marginalize over a subset of variables, as can be seen, for example, from the SCM M in Example 2.8. This SCM has only one self-cycle; however, marginalizing w.r.t. {2} gives a marginal SCM with another self-cycle at variable 4.</p><p>The definition of marginalization satisfies an intuitive property: if we can marginalize over two disjoint subsets after each other, then we can also marginalize over the union of those subsets at once, and the respective results agree. <ref type="bibr">PROPOSITION 5.4</ref>.</p><formula xml:id="formula_26">Let M = I, J , X , E, f , P E be an SCM that is uniquely solvable w.r.t. a subset L 1 ⊆ I and let L 2 ⊆ I be a subset disjoint from L 1 . Then M marg(L1) is uniquely solvable w.r.t. L 2 if and only if M is uniquely solvable w.r.t. L 1 ∪ L 2 , Moreover, marg(L 2 ) • marg(L 1 )(M) = marg(L 1 ∪ L 2 )(M).</formula><p>In this proposition, L 1 and L 2 have to be disjoint, since marginalizing first over L 1 gives a marginal SCM M marg(L1) with endogenous variables I \ L 1 .</p><p>Next, we show that the distributions of a marginal SCM are identical to the marginal distributions induced by the original SCM. A simple proof of this result proceeds by showing that both the intervention and the twin operation commute with marginalization. PROPOSITION 5.5. Let M be an SCM that is uniquely solvable w.r.t. a subset L ⊆ I. Then the marginalization marg(L) commutes with both:</p><p>1. the perfect intervention do(I, ξ I ) for a subset I ⊆ I \ L and a value ξ I ∈ X I , that is,</p><formula xml:id="formula_27">(marg(L) • do(I, ξ I ))(M) = (do(I, ξ) • marg(L))(M), and 2. the twin operation twin, that is, (marg(L ∪ L ) • twin)(M) = (twin • marg(L))(M),</formula><p>where L is the copy of L in I . With Proposition 5.5 at hand, we can prove the main result of this subsection. THEOREM 5.6 (Marginalization of an SCM preserves the observational, causal and counterfactual semantics). Let M be an SCM that is uniquely solvable w.r.t. a subset L ⊆ I. Then M and marg(L)(M) are observationally, interventionally and counterfactually equivalent w.r.t. I \ L.</p><p>This shows that our definition of marginalization (Definition 5.3) preserves the probabilistic, causal and counterfactual semantics, under a certain local unique solvability condition. Moreover, this allows us to marginalize SCMs w.r.t. a certain subset that do not satisfy the additional assumptions imposed by modular SCMs, for example, the SCM M of Example 3.11 does not have any additional structure of a compatible system of solution functions, but M can be marginalized w.r.t. the subset {2, 3} (see Appendix A.3).</p><p>In general, interventional equivalence does not imply counterfactual equivalence (see, e.g., Example D.7). However, for our definition of marginalization we arrive at a marginal SCM that is not only interventionally equivalent, but also counterfactually equivalent w.r.t. the margin.</p><p>For an SCM M, unique solvability w.r.t. a certain subset L ⊆ I is a sufficient, but not a necessary condition for the existence of an SCM M on the margin I \ L such that M and M are counterfactually equivalent w.r.t. I \ L (see, e.g., Example D.11). Hence, in certain cases it may be possible to relax the uniqueness condition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Marginalization of a graph</head><p>We now turn to a marginalization operation for directed mixed graphs, which we call the latent projection. This name is inspired from a similar construction on directed mixed graphs in <ref type="bibr" target="#b77">[78]</ref>. In <ref type="bibr" target="#b77">[78]</ref>, the authors concentrate on a mapping between directed mixed graphs and show that it preserves conditional independence properties [see also <ref type="bibr" target="#b75">76]</ref>. In this subsection, we provide a sufficient condition for the marginalization of an SCM to respect the latent projection, that is, that the augmented graph of the marginal SCM is a subgraph of the latent projection of the augmented graph of the original SCM. DEFINITION 5.7 (Marginalization/latent projection of a directed mixed graph). Let G = (V, E, B) be a directed mixed graph and L ⊆ V a subset. The marginalization of G w.r.t. L or the latent projection of G onto V \ L maps G to the marginal graph marg(L)(G) := ( Ṽ, Ẽ, B), where:</p><formula xml:id="formula_28">1. Ṽ = V \ L, 2. for i, j ∈ Ṽ: i → j ∈ Ẽ if and only if there exists a directed path i → 1 → • • • → n → j in G with n ≥ 0 and 1 , . . . , n ∈ L, 3. for i = j ∈ Ṽ: i ↔ j ∈ B if and only if a) there exist n, m ≥ 0, 1 , . . . , n ∈ L, ˜ 1 , . . . , ˜ m ∈ L such that i ← l 1 ← l 2 ← • • • ← n ↔ ˜ m → ˜ m-1 → • • • → ˜ 1 → j in G, or b) there exist n, m ≥ 1, 1 , . . . , n ∈ L, ˜ 1 , . . . , ˜ m ∈ L such that i ← l 1 ← l 2 ← • • • ← n and ˜ m → ˜ m-1 → • • • → ˜ 1 → j in G and n = ˜ m .</formula><p>Note that this gives G(M) = marg(J )(G a (M)) for any SCM M. Further, for a subgraph H ⊆ G we have marg(L)(H) ⊆ marg(L)(G) for any subset of nodes L. It does not matter in which order we project out the nodes or if we perform several projections at once. PROPOSITION 5.8. Let G = (V, E, B) be a directed mixed graph and</p><formula xml:id="formula_29">L 1 , L 2 ⊆ V two dis- joint subsets. Then (marg(L 1 ) • marg(L 2 ))(G) = (marg(L 2 ) • marg(L 1 ))(G) = marg(L 1 ∪ L 2 )(G).</formula><p>Similar to the definition of marginalization for SCMs, this definition of the latent projection commutes with both the (graphical) perfect intervention and the twin operation. PROPOSITION 5.9. Let G = (V, E, B) be a directed mixed graph and L, I, I ⊆ V subsets. Then the marginalization marg(L) commutes with both:</p><formula xml:id="formula_30">1. perfect intervention do(I) if I is disjoint from L, that is, (marg(L) • do(I))(G) = (do(I) • marg(L))(G), and 2. the twin operation twin(I) if B = ∅, J := V \ I is exogenous (i.e., pa G (J ) = ∅) and L ⊆ I, that is, (marg(L ∪ L ) • twin(I))(G) = (twin(I \ L) • marg(L))(G),</formula><p>where L is the copy of L in I .</p><p>An example of an SCM for which a marginalization respects the latent projection is the SCM M of Example 2.8. Marginalizing M w.r.t. L = {2} gives a marginal SCM M marg(L) with a graph that is a subgraph of the latent projection of the graph of the SCM M onto I \ L. In general, not all marginalizations respect the latent projection, as is illustrated in the following example. Under the local ancestral unique solvability condition, which is a stronger condition than the local unique solvability condition (i.e., ancestral unique solvability w.r.t. a subset implies unique solvability w.r.t. that subset), one can prove that the marginalization of an SCM respects the latent projection. PROPOSITION 5.11. Let M be an SCM that is ancestrally uniquely solvable w.r.t. a subset</p><formula xml:id="formula_31">L ⊆ I. Then G a • marg(L) (M) ⊆ marg(L) • G a (M) and G • marg(L) (M) ⊆ marg(L) • G (M).</formula><p>The (augmented) graph of a marginalized SCM can be a strict subgraph of the corresponding latent projection if, for example, certain paths cancel each other out after the substitution of the measurable solution function(s) into the causal mechanism(s) on the margin (see Example D.12). For acyclic SCMs, we recover with Proposition 5.11 the known result that this class is closed under marginalization (see Proposition 3.4) <ref type="bibr" target="#b14">[15]</ref>. For linear SCMs, we have that unique solvability w.r.t. a subset L holds if and only if ancestral unique solvability w.r.t. L holds (see Proposition C.4), and hence, a marginalization of a linear SCM always respects the latent projection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Markov properties</head><p>In this section, we give a short overview of Markov properties for SCMs with cycles. We make use of the Markov properties that were recently developed by Forré and Mooij <ref type="bibr" target="#b17">[18]</ref> for HEDGes, a graphical representation that is similar to the augmented graph of SCMs. We briefly summarize some of their main results and apply them to the class of SCMs. In Appendix A.2, we provide a more thorough introduction and give an intuitive derivation, which can act as an entry point for the reader into the more extensive discussion of Markov properties provided in <ref type="bibr" target="#b17">[18]</ref>.</p><p>Markov properties associate a set of conditional independence relations to a graph. The directed global Markov property for directed acyclic graphs (see Definitions A.4 and A.6), also known as the d-separation criterion <ref type="bibr" target="#b49">[50]</ref>, is one of the most widely used. It directly extends to a similar property for acyclic directed mixed graphs (ADMGs) <ref type="bibr" target="#b59">[60]</ref>. It does not hold in general for cyclic SCMs, however, as was already observed earlier <ref type="bibr" target="#b70">[71,</ref><ref type="bibr" target="#b71">72]</ref>. EXAMPLE 6.1 (Directed global Markov property does not hold for cyclic SCM). One can check that for every solution X of the SCM M of Example 3.5, X 1 is not independent of X 2 given {X 3 , X 4 }. However, the variables X 1 and X 2 are d-separated given {X 3 , X 4 } in G(M) (see Figure <ref type="figure" target="#fig_4">3</ref>). Hence the global directed Markov property does not hold here.</p><p>Although some progress has been made in the case of discrete <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b51">52]</ref> and linear models <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b69">[70]</ref><ref type="bibr" target="#b70">[71]</ref><ref type="bibr" target="#b71">[72]</ref>, only recently a general directed global Markov property has been introduced for more general cyclic models <ref type="bibr" target="#b17">[18]</ref>, that is based on σ-separation (see Definition A. <ref type="bibr">16 and A.20)</ref>, an extension of d-separation. This notion of σ-separation was derived from the notion of d-separation in the acyclification of the graph <ref type="bibr" target="#b17">[18]</ref> (see Definition A.13). The acyclification of a graph generalizes the idea of the collapsed graph developed by Spirtes <ref type="bibr" target="#b70">[71]</ref> and can, in particular, be applied to the graphs of SCMs. The main idea of the acyclification is that under the condition that the SCM is uniquely solvable w.r.t. each strongly connected component, we can replace the causal mechanisms of these strongly connected components by their measurable solution functions, which results in an acyclic SCM. This acyclified SCM (see Definition A.11) is observationally equivalent to the original SCM (see Proposition A.12). EXAMPLE 6.2 (Construction of an observationally equivalent acyclic SCM). The SCM M of Example 3.5 is uniquely solvable w.r.t. all its strongly connected components, that is, the subsets {1}, {2} and {3, 4}. Replacing the causal mechanisms of these strongly connected components by their measurable solution functions gives the observationally equivalent SCM M of Example 4.2. Because M is acyclic (see Figure <ref type="figure" target="#fig_4">3</ref>) we can apply the directed global Markov property to M. The fact that X 1 and X 2 are not d-separated given {X 3 , X 4 } in G( M) is in line with X 1 being dependent of X 2 given {X 3 , X 4 } for every solution X of M (and hence of M). This acyclification preserves solutions, and d-separation in the acyclification can directly be translated into σ-separation on the original graph (see Proposition A. <ref type="bibr" target="#b18">19</ref>). This leads to the general directed global Markov property. The following theorem summarizes the main results of <ref type="bibr" target="#b17">[18]</ref> applied to SCMs. THEOREM 6.3 (Global Markov properties for SCMs <ref type="bibr" target="#b17">[18]</ref>). Let M be a uniquely solvable SCM. Then its observational distribution P X exists, is unique and the following two statements hold: The general directed global Markov property is generally weaker than the directed global Markov property, since σ-separation implies d-separation. The acyclic case is well known and was first shown in the context of linear-Gaussian structural equation models <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b74">75]</ref>. The discrete case fixes the erroneous theorem by Pearl and Dechter <ref type="bibr" target="#b51">[52]</ref>, for which a counterexample was found by Neal <ref type="bibr" target="#b48">[49]</ref>, by adding the ancestral unique solvability condition, and extends it to allow for bidirected edges in the graph. The linear case is an extension of existing results for the linear-Gaussian setting without bidirected edges <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b71">72]</ref> to a linear (possibly non-Gaussian) setting with bidirected edges in the graph.</p><formula xml:id="formula_32">1. P X satisfies</formula><p>In constraint-based approaches to causal discovery, one usually assumes the converse of the (general) directed global Markov property to hold <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b72">73]</ref>, which is called σ-faithfulness respectively d-faithfulness (see Definition A.9 and A.23). Meek <ref type="bibr" target="#b40">[41]</ref> showed that for multinomial and linear-Gaussian DAG (i.e., acyclic and causally sufficient SCMs) models, dfaithfulness holds for all parameter values up to a measure zero set. Up to our knowledge no such results have been shown in more general parametric or nonparametric settings (neither for d-faitfhulness in acyclic or cyclic settings, nor for σ-faithfulness).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Causal interpretation of the graph of SCMs</head><p>In Example 4.4, we already saw that sometimes no information in the observational, interventional and even the counterfactual distributions suffices to decide whether a directed path or bidirected edge is present in the graph, or not. Here, we do not attempt to provide a complete characterization of the conditions under which the presence or absence of a directed path or bidirected edge in the graph can be identified from the observational and interventional distributions. Instead, we give sufficient conditions to detect a directed path and bidirected edge in the graph.</p><p>In general, cyclic SCMs may have none, one or multiple induced observational distributions, and this may change after intervening in the system. Here, we restrict ourselves to graphs of SCMs where the induced (marginal) observational and interventional distributions are uniquely defined.</p><p>7.1. Directed paths and edges For cyclic SCMs, the causal interpretation of the SCM is not always consistent with its graph. This can be illustrated with the SCM M of Example 5.10. Here, one sees a difference in the marginal distribution P Mdo({1},ξ 1 ) on X 4 for different values of ξ 1 , although variable 1 is not an ancestor of variable 4 and each marginal distribution P Mdo({1},ξ 1 ) on X 4 is uniquely defined. This counterintuitive behavior that an intervention on a nonancestor of a variable can change the distribution of that variable was already observed by Neal <ref type="bibr" target="#b48">[49]</ref>. However, under a specific unique solvability condition, we obtain a direct causal interpretation for the absence of a directed edge or directed path in the graph of an SCM. PROPOSITION 7.1 (Sufficient condition for detecting a directed edge in the latent projection of the graph of an SCM). Consider an SCM M = I, J , X , E, f , P E , a subset O ⊆ I and i, j ∈ O such that i = j. Let ξ I ∈ X I , where I := O \ {i, j}, such that M do(I,ξI ) is uniquely solvable w.r.t. an G(Mdo(I,ξ I ) )\i (j). If there exist values ξ i = ξi ∈ X i such that both (M do(I,ξI ) ) do({i},ξi) and (M do(I,ξI ) ) do({i}, ξi) induce unique marginal distributions on X j , and these two induced distributions do not coincide, that is, there exists a measurable set B j ⊆ X j such that</p><formula xml:id="formula_33">P (Mdo(I,ξ I ) )do({i},ξ i ) (X j ∈ B j ) = P (Mdo(I,ξ I ))do({i}, ξi ) (X j ∈ B j ) , the directed edge i → j is present in the latent projection marg(I \ O)(G(M)) of G(M) on O.</formula><p>Two cases are of special interest: O = I, which corresponds with a directed edge i → j in G(M), and O = {i, j}, which corresponds with a directed path i →</p><formula xml:id="formula_34">• • • → j in G(M).</formula><p>The condition in Proposition 7.1 is a sufficient condition for determining whether a directed edge or path is present in the graph. In general, not all directed edges and paths can be identified from the interventional distributions with this sufficient condition. For example, no interventional distribution satisfies the condition of Proposition 7.1 for the SCM M in Example 4.4, although there is a directed edge 1 → 2 in the graph G( M).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Bidirected edges</head><p>It is well known that there exists a similar sufficient condition for detecting bidirected edges in the graph of an acyclic SCM also known as the commoncause principle [see, e.g., 51]. In the two variables case, this criterion informally states that there exists a bidirected edge between the variables i and j in the graph of the SCM, if the marginal interventional distribution of X j under the intervention do({i}, x i ) differs from the conditional distribution of X j given X i = x i (see Example D.13). The following proposition provides a generalization of this sufficient condition for detecting bidirected edges in graphs of SCMs that may include cycles. PROPOSITION 7.2 (Sufficient condition for detecting a bidirected edge in the latent projection of the graph of an SCM). Consider an SCM M = I, J , X , E, f , P E , a subset O ⊆ I and i, j ∈ O such that i = j. Let ξ I ∈ X I , where I := O \ {i, j}, such that M do(I,ξI ) is uniquely solvable w.r.t. both an G(Mdo(I,ξ I ) ) (i) and an G(Mdo(I,ξ I ))\i (j). Assume that for every ξ i ∈ X i both M do(I,ξI ) and (M do(I,ξI ) ) do({i},ξi) induce a unique marginal distribution on X j × X i and X j , respectively. If j / ∈ an G(Mdo(I,ξ I ) ) (i) and there exists a measurable set B j ⊆ X j such that for every version of the regular conditional probability P Mdo(I,ξ I ) (X j ∈ B j | X i = ξ i ), there exists a value ξ i ∈ X i such that</p><formula xml:id="formula_35">P (Mdo(I,ξ I ))do({i},ξ i ) (X j ∈ B j ) = P Mdo(I,ξ I ) (X j ∈ B j | X i = ξ i ) , then there exists a bidirected edge i ↔ j in the latent projection marg(I \ O)(G(M)) of G(M) on O.</formula><p>This proposition gives a sufficient condition for determining that a bidirected edge is present in the graph. In general, not all bidirected edges in the graph can be identified from the observational, interventional and even the counterfactual distributions, as we saw in Example D.10. In this example, there exists a bidirected edge 1 ↔ 2 ∈ G(M) while the density p(x 2 | do(X 1 = x 1 )) = p(x 2 | X 1 = x 1 ) for all x 1 ∈ X 1 . For the acyclic setting, the above criterion is generally considered as a universal way to detect a confounder (note that then one can also deal with the case j ∈ an G(Mdo(I,ξ I )) (i) by swapping the roles of i and j). If i and j are part of a cycle, the above sufficient condition cannot be applied, and in that case, to the best of our knowledge, no simple sufficient conditions for detecting the presence of a bidirected edge are known.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Simple SCMs</head><p>In this section, we introduce the well-behaved class of simple SCMs. Simple SCMs satisfy all the local unique solvability conditions to ensure that this class is closed under both perfect intervention and marginalization. They extend the subclass of acyclic SCMs to the cyclic setting, while preserving many of their convenient properties. DEFINITION 8.1 (Simple SCM). Let M = I, J , X , E, f , P E be an SCM. We call M simple if it is uniquely solvable w.r.t. every subset O ⊆ I.</p><p>Loosely speaking, an SCM is simple if any subset of its structural equations can be solved uniquely for its associated variables in terms of the other variables that appear in these equations. An example of a simple SCM is given in Example D.1.</p><p>On simple SCMs one can perform any number of marginalizations (see Definition 5.3) in any order (see <ref type="bibr">Proposition 5.4)</ref>. All these marginalizations respect the latent projection (see Proposition 5.11) and each resulting marginal SCM is again simple. Moreover, we show that this class is closed under intervention and the twin operation. PROPOSITION 8.2. The class of simple SCMs is closed under marginalization, perfect intervention and the twin operation.</p><p>The class of simple SCMs contains the acyclic SCMs as a subclass (see Proposition 3.4). In particular, a simple SCM has no self-cycles (see Proposition 3.7), since a self-cycle denotes that that variable cannot be uniquely (up to a P E -null set) determined by its parents.</p><p>From Proposition 8.2, it follows that the results summarized in Theorem 6.3 also apply to all the observational, interventional and counterfactual distributions of simple SCMs. COROLLARY 8.3 (Global Markov properties for simple SCMs). Let M be a simple SCM. Then the:</p><p>1. observational distribution, 2. interventional distribution after perfect intervention on I ⊂ I, 3. counterfactual distribution after perfect intervention on Ĩ ⊆ I ∪ I , all exist, are unique and satisfy the general directed global Markov property relative to G(M), do(I)(G(M)) and do( Ĩ)(twin(G(M))), respectively. Moreover, if M satisfies at least one of the three conditions (1a), (1b), (1c) of Theorem 6.3, then they also obey the directed global Markov property relative to G(M), do(I)(G(M)) and do( Ĩ)(twin(G(M))), respectively.</p><p>Many of these properties are also shown to hold for the class of modular SCMs <ref type="bibr" target="#b17">[18]</ref>, which contains, in particular, the class of simple SCMs (see Appendix A.3 for more details).</p><p>Moreover, simple SCMs satisfy the unique solvability conditions of Proposition 7.1 and 7.2, which allows us to define the causal relationships for simple SCMs in terms of its graph. DEFINITION 8.4 (Causal relationships for simple SCMs). Let M be a simple SCM.</p><p>1. If there exists a directed edge i → j ∈ G(M), that is, i ∈ pa(j), then we call i a direct cause of j according to M; 2. If there exists a directed path i → • • • → j in G(M), that is, i ∈ an(j), then we call i a cause of j according to M; 3. If there exists a bidirected edge i ↔ j ∈ G(M), then we call i and j (latently) confounded according to M.</p><p>In summary, we have the following sufficient conditions for determining the different causal and confoundedness relationships according to a specific simple SCM M. COROLLARY 8.5 (Sufficient conditions for the presence of causal and confoundedness relationships for simple SCMs). Let M be a simple SCM and i, j ∈ I such that i = j and I := I \ {i, j}. Then:</p><p>1. If there exist values ξ I ∈ X I and ξ i = ξi ∈ X i and a measurable set B j ⊆ X j such that</p><formula xml:id="formula_36">P (Mdo(I,ξ I ))do({i},ξ i ) (X j ∈ B j ) = P (Mdo(I,ξ I ) ) do({i}, ξi ) (X j ∈ B j ) ,</formula><p>then i is a direct cause of j according to M, that is, i → j ∈ G(M); 2. If there exist values ξ i = ξi ∈ X i and a measurable set B j ⊆ X j such that</p><formula xml:id="formula_37">P Mdo({i},ξ i ) (X j ∈ B j ) = P M do({i}, ξi ) (X j ∈ B j ) , then i is a cause of j according to M, that is, i → • • • → j in G(M); 3. If j /</formula><p>∈ an G(Mdo(I,ξ I )) (i) and there exist a value ξ I ∈ X I and a measurable set B j ⊆ X j such that for every version of the regular conditional probability P Mdo(I,ξ I ) (X j ∈ B j | X i = ξ i ) there exists a value ξ i ∈ X i such that</p><formula xml:id="formula_38">P (Mdo(I,ξ I ))do({i},ξ i ) (X j ∈ B j ) = P Mdo(I,ξ I ) (X j ∈ B j | X i = ξ i ) ,</formula><p>then i and j are confounded according to M, that is, i ↔ j ∈ G(M).</p><p>For simple SCMs, it is in general not possible to identify all the causal and confoundedness relationships in the graph from the observational, interventional or even the counterfactual distributions. Examples 4.4 and D.10 show that this is already impossible for acyclic SCMs without further assumptions.</p><p>Finally, there is a connection between SCMs and potential outcomes <ref type="bibr" target="#b67">[68]</ref> that generalizes to the cyclic setting. One of the consequences of Proposition 8.2 is that all counterfactuals are defined for a simple SCM (even if it is cyclic). This allows us to define potential outcomes in terms of a simple SCM in the following way. DEFINITION 8.6 (Potential outcome). Let M = I, J , X , E, f , P E be a simple SCM, I ⊆ I a subset, ξ I ∈ X I a value and E a random variable such that P E = P E . The potential outcome under the perfect intervention do(I, ξ I ) is defined as X ξI := g Mdo(I,ξ I ) (E pa(I) ), where g Mdo(I,ξ I ) : E pa(I) → X is a measurable solution function for M do(I,ξI ) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.">Discussion</head><p>In this paper, we studied the basic properties of SCMs in the presence of cycles and latent variables without restricting to linear functional relationships between the variables. We saw that cyclic SCMs behave differently in many aspects than acyclic SCMs. Indeed, in the presence of cycles, many of the convenient properties of acyclic SCMs do not hold in general: SCMs do not always have a solution; they do not always induce unique observational, interventional and counterfactual distributions; a marginalization does not always exist, and if it exists the marginal model does not always respect the latent projection; they do not always satisfy a Markov property and their graphs are not always consistent with their causal semantics.</p><p>We introduced various notions of (unique) solvability and showed that under appropriate (unique) solvability conditions, many of the operations and results for the acyclic setting can be extended to SCMs with cycles. For example, we introduced several equivalence relations between SCMs to compare SCMs at different levels of abstraction, we showed how to define marginal SCMs on a subset of the variables that are (in various ways) equivalent to the original SCM, we discussed under which conditions the distributions satisfy the (general) directed global Markov property relative to their graphs and we showed under which conditions the graph of an SCM can be interpreted causally. Most of these results are shown under sufficient conditions that are not necessary (e.g., for the marginalization operation this was shown in Example D.11). It may therefore be possible to further relax some of the conditions.</p><p>These insights led us to introduce the more well-behaved class of simple SCMs, which forms an extension of the class of acyclic SCMs to the cyclic setting that preserves many of its convenient properties: simple SCMs induce unique observational, interventional and counterfactual distributions; the class of simple SCMs is closed under both perfect intervention and marginalization; the marginalization respects the latent projection; the induced distributions obey the general directed global Markov property and obey the directed global Markov property in the acyclic, discrete and linear case. This class does not contain SCMs that have self-cycles and graphs of simple SCMs have a direct and intuitive causal interpretation.</p><p>One key property of simple SCMs is that the solutions always satisfy the conditional independencies implied by σ-separation. By simply replacing d-separation with σ-separation it turns out that one can directly extend results and algorithms for acyclic SCMs to the more general class of simple SCMs. For example, adjustment criteria (including the back-door criterion), Pearl's do-calculus and Tian's ID algorithm for the identification of causal effects have been extended recently to the class of modular SCMs, which contains the class of simple SCMs <ref type="bibr" target="#b19">[20]</ref>. Several causal discovery algorithms have already been proposed that work with simple SCMs, for example, the first constraint-based causal discovery algorithm that can deal with cycles and nonlinear functional relationships <ref type="bibr" target="#b18">[19]</ref>. Also, Local Causal Discovery (LCD) <ref type="bibr" target="#b9">[10]</ref>, Y-structures <ref type="bibr" target="#b37">[38]</ref> and the Joint Causal Inference framework (JCI) all apply to simple SCMs <ref type="bibr" target="#b46">[47]</ref> even though they were originally developed for acyclic SCMs only. Recently, it has been shown that even the well-known Fast Causal Inference (FCI) algorithm <ref type="bibr" target="#b73">[74,</ref><ref type="bibr" target="#b79">80]</ref> is directly applicable to simple SCMs <ref type="bibr" target="#b43">[44]</ref> and provides a consistent estimate of the Markov equivalence class (under the faithfulness assumption). Moreover, a method for constructing nonlinear simple SCMs using neural networks and sampling from them has been proposed <ref type="bibr" target="#b18">[19]</ref>. This illustrates that the class of simple SCMs forms a convenient and practical extension of the class of acyclic SCMs that can be used for the purposes of causal modeling, reasoning, discovery and prediction.</p><p>We hope that this work will provide the foundations for a general theory of statistical causal modeling with SCMs. Future work might consist of reparametrizing and reducing the space of the exogenous variables of an SCM while preserving the causal and counterfactual semantics; extending and generalizing the identifiability results for (direct) causes and confounders; extending the graphs of SCMs to represent selection bias; proving completeness results for some Markov properties for a subclass of SCMs that contains cycles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.</head><p>A cycle through i ∈ V in a directed mixed graph G is a directed path from i to some node j extended with the edge j → i ∈ E. In particular, a self-cycle i → i ∈ E is a cycle. Note that a path cannot contain any cycles. A directed graph and a directed mixed graph are said to be acyclic if they contain no cycles, and are then referred to as a directed acyclic graph (DAG) and an acyclic directed mixed graph (ADMG), respectively. 6. For a directed mixed graph G and a node i ∈ V we define the set of parents of i by pa G (i) := {j ∈ V : j → i ∈ E}, the set of children of i by ch G (i) := {j ∈ V : i → j ∈ E}, the set of ancestors of i by an G (i) := {j ∈ V : there is a directed path from j to i in G} and the set of descendants of i by de G (i) := {j ∈ V : there is a directed path from i to j in G} .</p><p>Note that we have {i} ∪ pa</p><formula xml:id="formula_39">G (i) ⊆ an G (i) and {i} ∪ ch G (i) ⊆ de G (i).</formula><p>We can apply all these definitions to subsets U ⊆ V by taking unions, for example pa <ref type="figure">E,</ref><ref type="figure">B</ref>) be a directed mixed graph. We call G strongly connected if for every pair of distinct nodes i, j ∈ V, the graph contains a cycle that passes through both i and j. The strongly connected component of i ∈ V, denoted by sc G (i), is the maximal subset S ⊆ V such that i ∈ S and the induced subgraph G S is strongly connected. Equivalently,</p><formula xml:id="formula_40">G (U) := ∪ i∈U pa G (i). A subset A ⊆ V is called an ancestral subset in G if A = an G (A), that is, A is closed under taking ancestors of A in G. 7. Let G = (V,</formula><formula xml:id="formula_41">sc G (i) = an G (i) ∩ de G (i). 8. A loop in a directed mixed graph G = (V, E, B) is a subset O ⊆ V that is strongly con- nected in the induced subgraph G O of G on O. 9.</formula><p>For a directed graph G = (V, E), we define the graph of strongly connected components of G as the directed graph G sc := (V sc , E sc ), where V sc are the strongly connected components of G, that is, V sc are the equivalence classes in V/∼ with the equivalence relation i ∼ j if and only if i ∈ sc G (j), and E sc = (E \ {i → i : i ∈ V})/∼ with the equivalence relation (i → j) ∼ (i → j ) if and only if i ∼ i and j ∼ j .</p><p>We omit the subscript G whenever it is clear which directed (mixed) graph G we are referring to. LEMMA A.2 (DAG of strongly connected components). Let G = (V, E) be a directed graph. Then G sc , the graph of strongly connected components of G, is a DAG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Markov properties</head><p>In this subsection, we give a short overview of Markov properties for SCMs with cycles. We will make use of the Markov properties that were recently developed by Forré and Mooij <ref type="bibr" target="#b17">[18]</ref> for HEDGes, a graphical representation that is similar to the augmented graph of SCMs. We briefly summarize some of their main results and apply them to the class of SCMs. We also provide a shorter and more intuitive derivation so that this subsection can act as an entry point for the reader into the more extensive discussion of Markov properties provided in <ref type="bibr" target="#b17">[18]</ref>.</p><p>Markov properties associate a set of conditional independence relations to a graph. The directed global Markov property for directed acyclic graphs, also known as the d-separation criterion <ref type="bibr" target="#b49">[50]</ref>, is one of the most widely used. It directly extends to a similar property for acyclic directed mixed graphs (ADMGs) <ref type="bibr" target="#b59">[60]</ref>. It does not hold in general for cyclic SCMs, however, as was already observed earlier <ref type="bibr" target="#b70">[71,</ref><ref type="bibr" target="#b71">72]</ref>. Under some conditions (roughly speaking, linearity or discrete variables) the directed global Markov property can be shown to hold also in the presence of cycles <ref type="bibr" target="#b17">[18]</ref>.</p><p>Inspired by work of Spirtes <ref type="bibr" target="#b70">[71]</ref>, Forré and Mooij <ref type="bibr" target="#b17">[18]</ref> recognized that in the general cyclic case a different extension of d-separation, termed σ-separation, is needed, leading to the general directed global Markov property. One key result in <ref type="bibr" target="#b17">[18]</ref> implies that under the assumption of unique solvability w.r.t. each strongly connected component of its graph, the observational distribution of an SCM satisfies the general directed global Markov property w.r.t. its graph. The solvability assumptions are in general not preserved under interventions. Under the stronger assumption of simplicity, however, they are, and one obtains the corollary that also all interventional and counterfactual distributions of a simple SCM satisfy the general directed global Markov property w.r.t. to their corresponding graphs.</p><p>For a more extensive study of different Markov properties that can be associated to SCMs we refer the reader to <ref type="bibr" target="#b17">[18]</ref>.</p><p>A.2.1. The directed global Markov property Conditional independencies in the observational distribution of an acyclic SCM can be read off from its graph by using the graphical criterion called d-separation <ref type="bibr" target="#b50">[51]</ref>. The directed global Markov property associates a conditional independence relation in the observational distribution of the SCM to each d-separation entailed by the graph. Here, we use a formulation of d-separation that generalizes d-separation for DAGs <ref type="bibr" target="#b49">[50]</ref> and m-separation for ADMGs <ref type="bibr" target="#b59">[60]</ref> and mDAGs <ref type="bibr" target="#b14">[15]</ref>.</p><formula xml:id="formula_42">DEFINITION A.3 (Collider). Let π = (i 0 , 1 , i 1 , 2 , i 2 , . . . , n , i n ) be a walk (path) in a directed mixed graph G = (V, E, B). A node i k on π is called a collider on π if it is a non-endpoint node (1 ≤ k &lt; n) and the two edges k , k+1 meet head-to-head on i k (i.e., if the subwalk (i k-1 , k , i k , k+1 , i k+1 ) is of the form i k-1 → i k ← i k+1 , i k-1 ↔ i k ← i k+1 , i k-1 → i k ↔ i k+1 or i k-1 ↔ i k ↔ i k+1 ). The node i k is called a non-collider on π otherwise, that is, if it is an endpoint node (k = 0 or k = n) or if the subwalk (i k-1 , k , i k , k+1 , i k+1 ) is of the form i k-1 → i k → i k+1 , i k-1 ← i k ← i k+1 , i k-1 ← i k → i k+1 , i k-1 ↔ i k → i k+1 or i k-1 ← i k ↔ i k+1 .</formula><p>Note in particular that the end points of a walk are non-colliders on the walk. DEFINITION A.4 (d-separation). Let G = (V, E, B) be a directed mixed graph and let C ⊆ V be a subset of nodes. A walk (path</p><formula xml:id="formula_43">) π = (i 0 , 1 , i 1 , . . . , i n ) in G is said to be C-d- blocked or d-blocked by C if 1. it contains a collider i k / ∈ an G (C), or 2. it contains a non-collider i k ∈ C. The walk (path) π is said to be C-d-open if it is not d-blocked by C. For two subsets of nodes A, B ⊆ V, we say that A is d-separated from B given C in G if</formula><p>all paths between any node in A and any node in B are d-blocked by C, and write</p><formula xml:id="formula_44">A d ⊥ G B | C .</formula><p>The next lemma is a straightforward generalization of Lemma 3.3 in <ref type="bibr" target="#b21">[22]</ref> to the cyclic setting. It implies that it suffices to formulate d-separation in terms of paths rather than walks. LEMMA A.5. Let G = (V, E, B) be a directed mixed graph, C ⊆ V and i, j ∈ V. There exists a C-d-open walk between i and j in G if and only if there exists a C-d-open path between i and j in G. Let G = (V, E, B) be a directed mixed graph and P V a probability distribution on X V = i∈V X i , where each X i is a standard probability space. The probability distribution P V satisfies the directed global Markov property relative to G if for all subsets A, B, C ⊆ V we have</p><formula xml:id="formula_45">X 1 X 2 X 3 X 4 X 1 X 2 X 3 X 4</formula><formula xml:id="formula_46">A d ⊥ G B | C =⇒ X A ⊥ ⊥ PV X B | X C ,</formula><p>that is, (X i ) i∈A and (X i ) i∈B are conditionally independent given (X i ) i∈C under P V , where we take the canonical projections X i : X V → X i as random variables.</p><p>From the results in <ref type="bibr" target="#b17">[18]</ref> it directly follows that for the observational distribution of an SCM, the directed global Markov property w.r.t. the graph of the SCM (also known as the d-separation criterion), holds under one of the following assumptions. THEOREM A.7 (Directed global Markov property for SCMs <ref type="bibr" target="#b17">[18]</ref>). Let M be a uniquely solvable SCM that satisfies at least one of the following three conditions:</p><p>1. M is acyclic; 2. all endogenous spaces X i are discrete and M is ancestrally uniquely solvable; 3. M is linear (see Definition C.1), each of its causal mechanisms {f i } i∈I has a nontrivial dependence on at least one exogenous variable, and P E has a density w.r.t. the Lebesgue measure on R J .</p><p>Then its observational distribution P X exists, is unique and satisfies the directed global Markov property relative to G(M) (see Definition A.6).</p><p>The acyclic case is well known and was first shown in the context of linear-Gaussian structural equation models <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b74">75]</ref>. The discrete case fixes the erroneous theorem by Pearl and Dechter <ref type="bibr" target="#b51">[52]</ref>, for which a counterexample was found by Neal <ref type="bibr" target="#b48">[49]</ref>, by adding the ancestral unique solvability condition, and extends it to allow for bidirected edges in the graph. The linear case is an extension of existing results for the linear-Gaussian setting without bidirected edges <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b71">72]</ref> to a linear (possibly non-Gaussian) setting with bidirected edges in the graph.</p><p>The following counterexample of an SCM for which the directed global Markov property does not hold was already given in <ref type="bibr" target="#b70">[71,</ref><ref type="bibr" target="#b71">72]</ref>. EXAMPLE A.8 (Directed global Markov property does not hold for cyclic SCM). Consider the SCM M = 4, 4, R 4 , R 4 , f , P R 4 with causal mechanism given by</p><formula xml:id="formula_47">f 1 (x, e) = e 1 , f 2 (x, e) = e 2 , f 3 (x, e) = x 1 x 4 + e 3 , f 4 (x, e) = x 2 x 3 + e 4</formula><p>and P R 4 is the standard-normal distribution on R 4 . The graph of M is depicted in Figure <ref type="figure" target="#fig_6">5</ref> on the left. The model is uniquely solvable (it is even simple). One can check that for every solution X of M, X 1 is not independent of X 2 given {X 3 , X 4 }. However, the variables X 1 and X 2 are d-separated given {X 3 , X 4 } in G(M). Hence the global directed Markov property does not hold here.</p><p>In constraint-based approaches to causal discovery, one usually assumes the converse of the directed global Markov property to hold <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b72">73]</ref>. DEFINITION A.9 (d-Faithfulness). Let G = (V, E, B) be a directed mixed graph and P V a probability distribution on X V = i∈V X i , where each X i is a standard probability space. The probability distribution P V is d-faithful to G if for all subsets A, B, C ⊆ V we have</p><formula xml:id="formula_48">A d ⊥ G B | C ⇐= X A ⊥ ⊥ PV X B | X C ,</formula><p>where we take the canonical projections X i : X V → X i as random variables.</p><p>In other words, the d-faithfulness assumption states that the graph explains, via dseparation, all the conditional independencies that are present in the observational distribution. Meek <ref type="bibr" target="#b40">[41]</ref> showed that for multinomial and linear-Gaussian DAG (i.e., acyclic and causally sufficient SCMs) models, d-faithfulness holds for all parameter values up to a measure zero set (in a natural parameterization). Up to our knowledge no such results have been shown in more general parametric or nonparametric settings (neither in the acyclic case, nor in the cyclic one).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.2. The general directed global Markov property</head><p>In <ref type="bibr" target="#b17">[18]</ref> the general directed global Markov property is introduced, that is based on σ-separation, an extension of d-separation. This notion of σ-separation was derived from the notion of d-separation in the acyclification of the graph. The acyclification of a graph generalizes the idea of the collapsed graph for directed graphs, developed by Spirtes <ref type="bibr" target="#b70">[71]</ref>, to HEDGes. In particular, this notion can be applied to directed mixed graphs, and thus to the graphs of SCMs. The main idea of the acyclification is that under the condition that the SCM is uniquely solvable w.r.t. each strongly connected component, we can replace the causal mechanisms of these strongly connected components by their measurable solution functions, which results in an acyclic SCM. This acyclification preserves the solutions, and d-separation in the acyclification can directly be translated into σ-separation in the original graph. This then leads to the general directed global Markov property. We will discuss this now in more detail. EXAMPLE A.10 (Construction of an observationally equivalent acyclic SCM). Consider the SCM M of Example A.8 which is uniquely solvable w.r.t. all its strongly connected components, i.e., the subsets {1}, {2} and {3, 4}. Replacing the causal mechanisms of these strongly connected components by their measurable solution functions gives the SCM M that is the same as M except that its causal mechanism f is given by f1 (x, e) := e 1 , f2 (x, e) := e 2 , f3 (x, e) := One of the key insights in <ref type="bibr" target="#b17">[18]</ref> is that this example can easily be generalized as follows.</p><p>DEFINITION A.11 (Acyclification of an SCM). Let M = I, J , X , E, f , P E be an SCM that is uniquely solvable w.r.t. each strongly connected component of G(M). For each i ∈ I, let g i be the i th component of a measurable solution function g sc(i) : X pa(sc(i))\sc(i) × E pa(sc(i)) → X sc(i) of M w.r.t. sc(i), where pa and sc denote the parents and strongly connected components according to G a (M), respectively. We call the SCM M acy := I, J , X , E, f , P E with the acyclified causal mechanism f : X × E → X given by fi (x, e) = g i (x pa(sc(i))\sc(i) , e pa(sc(i)) ) , i ∈ I , an acyclification of M. We denote by acy(M) the equivalence class of the acyclifications of M.</p><formula xml:id="formula_49">X 1 X 2 G(M) X 1 X 2 G(acy(M)) X 1 X 2 acy(G(M))</formula><p>Note that acy(M) is well-defined: all acyclifications of an SCM M belong to the same equivalence class of SCMs. PROPOSITION A.12. Let M be an SCM that is uniquely solvable w.r.t. each strongly connected component of G(M). Then an acyclification M acy of M is acyclic and observationally equivalent to M.</p><p>We can also define a graphical acyclification for directed mixed graphs, which is a special case of the operation defined in <ref type="bibr" target="#b17">[18]</ref> for HEDGes. DEFINITION A.13 (Acyclification of a directed mixed graph). Let G = (V, E, B) be a directed mixed graph. The acyclification of G maps G to the acyclified graph G acy := (V, Ê, B) with directed edges j → i ∈ Ê if and only if j ∈ pa G (sc G (i)) \ sc G (i) and bidirected edges i ↔ j ∈ B if and only if there exist i ∈ sc G (i) and j ∈ sc G (j) with i = j or i ↔ j ∈ B.</p><p>The following compatibility result is immediate from the definitions. PROPOSITION A.14. Let M be an SCM that is uniquely solvable w.r.t. each strongly connected component of G(M). Then G a (acy(M)) ⊆ acy(G a (M)) and G(acy(M)) ⊆ acy(G(M)).</p><p>The following example illustrates that the graph of the acyclification of an SCM can be a strict subgraph of the acyclification of the graph of the SCM. EXAMPLE A.15 (Graph of the acyclification of the SCM is a strict subgraph of the acyclification of its graph). Consider the SCM M = 2, 1, R 2 , R, f , P R with the causal mechanism defined by</p><formula xml:id="formula_50">f 1 (x, e) = x 2 -e , f 2 (x, e) = 1</formula><p>2 x 1 + e and P R the standard Gaussian measure on R. The SCM M is uniquely solvable w.r.t. the (only) strongly connected component {1, 2}. An acyclification of M is the acyclified SCM M acy with the acyclified causal mechanism f defined by f1 (x, e) = 0 , f2 (x, e) = e .</p><p>The graph G(acy(M)) is a strict subgraph of acy(G(M)) as can be seen in Figure <ref type="figure" target="#fig_14">6</ref>.</p><p>Translating the notion of d-separation from the acyclified graph back to the original graph led to the notion of σ-separation. DEFINITION A.16 (σ-separation <ref type="bibr" target="#b17">[18]</ref>). Let G = (V, E, B) be a directed mixed graph and let C ⊆ V be a subset of nodes. A walk (path) π = (i 0 , 1 , i 1 , . . . , i n ) in G is said to be C-σblocked or σ-blocked by C if 1. its first node i 0 ∈ C or its last node i n ∈ C, or 2. it contains a collider i k / ∈ an G (C), or 3. it contains a non-endpoint non-collider i k ∈ C that points towards a neighboring node on π that lies in a different strongly connected component of G, that is, such that</p><formula xml:id="formula_51">i k-1 ← i k in π and i k-1 / ∈ sc G (i k ), or i k → i k+1 in π and i k+1 / ∈ sc G (i k ).</formula><p>The walk (path) π is said to be C-σ-open if it is not σ-blocked by C. For two subsets of nodes A, B ⊆ V, we say that A is σ-separated from B given C in G if all paths between any node in A and any node in B are σ-blocked by C, and write</p><formula xml:id="formula_52">A σ ⊥ G B | C .</formula><p>The only difference between σ-separation and d-separation is that d-separation does not have the extra condition on the non-collider that it has to point to a node in a different strongly connected component. It is therefore obvious that σ-separation reduces to d-separation for acyclic graphs, since sc G (i) = {i} for each i ∈ V in that case.</p><p>Although for proofs it is often easier to make use of walks, it suffices to formulate σseparation in term of paths rather than walks because of the following result, which is analogous to a similar result for d-separation (see Lemma A.5).</p><p>LEMMA A.17. Let G = (V, E, B) be a directed mixed graph, C ⊆ V and i, j ∈ V. There exists a C-σ-open walk between i and j in G if and only if there exists a C-σ-open path between i and j in G.</p><p>It is clear from the definitions that σ-separation implies d-separation. The other way around does not hold in general, as can be seen in the following example. EXAMPLE A.18 (d-separation does not imply σ-separation). Consider the directed graph G as depicted in Figure <ref type="figure" target="#fig_6">5</ref> (left). Here X 1 is d-separated from X 2 given {X 3 , X 4 }, but X 1 is not σ-separated from X 2 given {X 3 , X 4 }.</p><p>The following result in <ref type="bibr" target="#b17">[18]</ref> relates σ-separation to d-separation. PROPOSITION A. <ref type="bibr" target="#b18">19</ref>. Let G = (V, E, B) be a directed mixed graph. Then for A, B, C ⊆ V,</p><formula xml:id="formula_53">A σ ⊥ G B | C ⇐⇒ A d ⊥ acy(G) B | C .</formula><p>By replacing in Definition A.6 "d-separation" by "σ-separation", one obtains the formulation of what Forré and Mooij <ref type="bibr" target="#b17">[18]</ref> termed the general directed global Markov property. DEFINITION A.20 (General directed global Markov property <ref type="bibr" target="#b17">[18]</ref>). Let G = (V, E, B) be a directed mixed graph and P V a probability distribution on X V = i∈V X i , where each X i is a standard probability space. The probability distribution P V satisfies the general directed global Markov property relative to G if for all subsets A, B, C ⊆ V we have</p><formula xml:id="formula_54">A σ ⊥ G B | C =⇒ X A ⊥ ⊥ PV X B | X C ,</formula><p>that is, (X i ) i∈A and (X i ) i∈B are conditionally independent given (X i ) i∈C under P V , where we take the canonical projections X i : X V → X i as random variables.</p><p>The fact that σ-separation implies d-separation means that the directed global Markov property implies the general directed global Markov property. In other words, the general directed global Markov property is weaker than the directed global Markov property. It is actually strictly weaker, as we saw in Example A. <ref type="bibr" target="#b17">18</ref>.</p><p>The following fundamental result, also known as the σ-separation criterion, follows directly from the theory in <ref type="bibr" target="#b17">[18]</ref>. THEOREM A.21 (General directed global Markov property for SCMs). Let M be an SCM that is uniquely solvable w.r.t. each strongly connected component of G(M). Then its observational distribution P X exists, is unique and it satisfies the general directed global Markov property relative to G(M). 15   The proof is based on the reasoning that, for A, B, C ⊆ I, if A is σ-separated from B given C in G(M), then A is d-separated from B by C in acy(G(M)) and hence in G(acy(M)), and since acy(M) is acyclic and observationally equivalent to M, it follows from the directed global Markov property applied to acy(M) that X A ⊥ ⊥ P X X B | X C for every solution X of M. Note that the ancestral unique solvability condition for the discrete case is strictly weaker than the condition of unique solvability w.r.t. each strongly connected component in Theorem A.21. For the linear case, the condition of unique solvability is equivalent to the condition of unique solvability w.r.t. each strongly connected component (see Proposition C.4).</p><p>The results in Theorems A.7 and A.21 are not preserved under perfect intervention, because intervening on a strongly connected component could split it into several strongly connected components with different solvability properties. As the class of simple SCMs is preserved under perfect intervention and the twin operation (Proposition 8.2), we obtain the following corollary. COROLLARY A.22 (Global Markov properties for simple SCMs). Let M be a simple SCM. Then the:</p><p>1. observational distribution, 2. interventional distribution after perfect intervention on I ⊂ I, 3. counterfactual distribution after perfect intervention on Ĩ ⊆ I ∪ I , all exist, are unique and satisfy the general directed global Markov property relative to G(M), do(I)(G(M)) and do( Ĩ)(twin(G(M))), respectively. Moreover, if M satisfies at least one of the three conditions (1), ( <ref type="formula">2</ref>), (3) of Theorem A.7, then they also satisfies the directed global Markov property relative to G(M), do(I)(G(M)) and do( Ĩ)(twin(G(M))), respectively.</p><p>Similar to d-faithfulness, σ-faithfulness 16 is defined as follows. 15 Since <ref type="bibr" target="#b17">[18]</ref> also provides results under the weaker condition that an SCM is solvable (not necessarily uniquely) w.r.t. each strongly connected component of G(M), one might believe that Theorem A.21 could be generalized to stating that in that case, any of its observational distributions satisfies the general directed global Markov property. However, that is not true: consider for example the SCM M = 2, ∅, R 2 , 1, f , P 1 with f 1 (x) = x 1 and f 2 (x) = x 2 . Then M is solvable w.r.t. each of its strongly connected components {1} and {2}. The solution with X 1 = X 2 shows a dependence between X 1 and X 2 and thus X 1 ⊥ ⊥ X 2 does not hold. In general, all strongly connected components that admit multiple solutions may be dependent on any other variable(s) in the model. 16 In <ref type="bibr" target="#b62">[63]</ref> it is called "collapsed graph faithfulness".</p><p>DEFINITION A.23 (σ-Faithfulness). Let G = (V, E, B) be a directed mixed graph and P V a probability distribution on X V = i∈V X i , where each X i is a standard probability space. The probability distribution P V is σ-faithful to G if for all subsets A, B, C ⊆ V we have</p><formula xml:id="formula_55">A σ ⊥ G B | C ⇐= X A ⊥ ⊥ PV X B | X C ,</formula><p>where we take the canonical projections X i : X V → X i as random variables.</p><p>In other words, the graph explains, via σ-separation, all the conditional independencies that are present in the observational distribution. Although it has been conjectured <ref type="bibr" target="#b71">[72]</ref> that under certain conditions σ-faithfulness should hold, formulating and proving such completeness results is an open problem to the best of our knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Modular SCMs</head><p>In this subsection, we relate the class of (simple) SCMs to that of modular SCMs. Modular SCMs introduced by Forré and Mooij <ref type="bibr" target="#b17">[18]</ref> are causal graphical models on which marginalizations and interventions are defined and they satisfy the general directed global Markov property. For a comprehensive account on modular SCMs we refer the reader to <ref type="bibr" target="#b17">[18]</ref>. A.3.1. Definition of a modular SCM In contrast to an SCM from which a graph can be derived, a modular SCM is defined in terms of a graphical object, which Forré and Mooij <ref type="bibr" target="#b17">[18]</ref> call a directed graph with hyperedges (HEDG). The hyperedges of a HEDG are described in terms of a simplicial complex. DEFINITION A.24 (Simplicial complex). Let V be a finite set. A simplicial complex H over V is a set of subsets of V such that 1. all single element sets {v} are in H for v ∈ V, and 2. if F ∈ H, then also all subsets F ⊆ F are elements of H. DEFINITION A.25 (Directed graph with hyperedges (HEDGes) <ref type="bibr" target="#b17">[18]</ref>). A directed graph with hyperedges (HEDG) is a triple G = (V, E, H), where (V, E) is a directed graph and H a simplicial complex over the set of nodes V. The elements F of H are called hyperedges of G. The elements F of H that are inclusion-maximal elements of H are called maximal hyperedges and are denoted by Ĥ.</p><p>A HEDG G = (V, E, H) can be represented as a directed graph Ḡ := (V, E) consisting of nodes V and directed edges E, with additional maximal hyperedges F ∈ Ĥ with |F| ≥ 2 (i.e., not corresponding to single element sets {v} ∈ Ĥ), that point to their target nodes v ∈ F . For a HEDG G, we define pa G , ch G , etc., in terms of the underlying directed graph Ḡ, that is, pa Ḡ , ch Ḡ , etc., respectively.</p><p>A loop in a HEDG G = (V, E, H) is a subset O ⊆ V that is a loop in the underlying directed graph Ḡ = (V, E). In other words, a loop of G is a set of nodes O ⊆ V such that for every two nodes v, w ∈ O there are directed paths v → • • • → w and w → • • • → v in G for which all the intermediate nodes lie in O (if any exist). In particular, a loop may consist of a single element {v} for v ∈ V. The set of loops in G is denoted by L(G).</p><p>In order to define a modular SCM one needs the notion of a compatible system of solution functions, which assigns to each loop a separate solution function such that all these solution functions are "compatible" with each other. DEFINITION A.26 (Compatible system of solution functions <ref type="foot" target="#foot_9">17</ref> ). Let G = (V, E, H) be a HEDG. For every v ∈ V and maximal hyperedge F in Ĥ, let X v and E F be standard measurable spaces. For a subset O ⊆ V we define<ref type="foot" target="#foot_10">foot_10</ref> </p><formula xml:id="formula_56">X O := v∈O X v and E O := F ∈ Ĥ F ∩O =∅ E F .</formula><p>Consider a family of measurable mappings (g O ) O∈L(G) indexed by L(G) which are of the form</p><formula xml:id="formula_57">g O : X pa G (O)\O × E O → X O .</formula><p>We call the family of measurable mappings (g O ) O∈L(G) a compatible system of solution functions, if for all O, Õ ∈ L(G) with Õ ⊆ O and for all e O ∈ E O and x pa G (O)∪O ∈ X pa G (O)∪O we have</p><formula xml:id="formula_58">x O = g O (x pa G (O)\O , e O ) =⇒ x Õ = g Õ(x pa G ( Õ)\ Õ, e Õ) .</formula><p>This structure of a compatible system of solution functions is at the heart of the defnition of a modular SCM. DEFINITION A.27 (Modular structural causal model (mSCM) <ref type="bibr" target="#b17">[18]</ref>). A modular structural causal model (mSCM) is a tuple</p><formula xml:id="formula_59">M := G, X , E, (g O ) O∈L(G) , P E , where 1. G = (V, E, H) is a HEDG, 2. X = v∈V X v is the product of standard measurable spaces X v , 3. E = F ∈ Ĥ E F is the product of standard measurable spaces E F , 4. (g O ) O∈L(G)</formula><p>is a compatible system of solution functions, 5. P E = F ∈ Ĥ P EF is a product measure, where P EF is a probability measure on E F for each F ∈ Ĥ.</p><p>Let M = G, X , E, (g O ) O∈L(G) , P E be a modular SCM and O 1 , . . . , O r ∈ L(G) the strongly connected components of G ordered according to a topological order of the DAG of strongly connected components of G. Then for any random variable E : Ω → E such that P E = P E one can inductively define the random variables</p><formula xml:id="formula_60">X v := (g Oi ) v (X pa G (Oi)\Oi , E Oi ) for all v ∈ O i for all i ≥ 1, starting at X v := (g O1 ) v ( E O1 ) for all v ∈ O 1 . Because (g O ) O∈L(G)</formula><p>is a compatible system of solution functions, we have for every O ∈ L(G)</p><formula xml:id="formula_61">X O = g O (X pa G (O)\O , E O ) .</formula><p>We call the random variable X a solution of the modular SCM M. Note that the solution X depends on the choice of the random variable E : Ω → E.</p><p>The causal semantics of modular SCMs can be defined in terms of perfect interventions, which is defined as follows.</p><p>DEFINITION A.28 (Perfect intervention on an mSCM). Consider a modular SCM M = G, X , E, (g O ) O∈L(G) , P E , a subset I ⊆ V of endogenous variables and a value ξ I ∈ X I . The perfect intervention do(I, ξ I ) maps M to the modular SCM M do(I,ξI ) := G do , X , E do , (g do O ) O∈L(G do ) , P E do , where</p><formula xml:id="formula_62">1. G do = (V, E do , H do )</formula><p>, where</p><formula xml:id="formula_63">E do = E \ {v → w : v ∈ V, w ∈ I} H do = {F \ I : F ∈ H} ∪ {{v} : v ∈ I} , 2. φ : {F ∈ Ĥ : F \ I = ∅} → Ĥdo \ {{v} : v ∈ I} is a mapping such that φ(F) ⊇ F \ I for all F ∈ Ĥ for which F \ I = ∅, 3. E do = F ∈ Ĥdo E do F</formula><p>, where</p><formula xml:id="formula_64">E do F = X v if F = {v} for v ∈ I F =φ -1 ( F ) E F if F ∈ Ĥdo \ {{v} : v ∈ I} , 4. for every O ∈ L(G do ) g do O = I {v} if O = {v} for v ∈ I g O otherwise, (note that if O is a loop in G do , then it is a loop in G), 5. P E do = F ∈ Ĥdo P E do F ,</formula><p>where</p><formula xml:id="formula_65">P E do F = δ ξv if F = {v} for v ∈ I F =φ -1 ( F ) P EF if F ∈ Ĥdo \ {{v} : v ∈ I} .</formula><p>In contrast to SCMs, these perfect interventions on modular SCMs are directly defined on the underlying HEDG and depend on the choice of the mapping φ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3.2. Relation between SCMs and modular SCMs</head><p>The solutions of a modular SCM can be described by an SCM that is loop-wisely solvable. DEFINITION A.29 (Underlying SCM). Let M = G, X , E, (g O ) O∈L(G) , P E be a modular SCM. Then the mapping ι maps M to the underlying SCM M := Ĩ, J , X , Ẽ, f , P Ẽ , where</p><formula xml:id="formula_66">1. Ĩ = V, 2. J = Ĥ, 3. X = X , 4. Ẽ = E, 5. f is given by fv = (g {v} ) v for all v ∈ V, 6. P Ẽ = P E .</formula><p>Every solution X of a modular SCM M is also a solution of the underlying SCM ι( M). Observe that for the modular SCM M we have that the induced subgraph G a (ι( M)) Ĩ , of the augmented graph of the underlying SCM G a (ι( M)) on Ĩ, is a subgraph of the underlying HEDG G, that is, G a (ι( M)) Ĩ ⊆ G. This implies that, in general, the underlying HEDG G of M may have more loops than the loops in G(ι( M)). For a subset O ⊆ Ĩ, we have for the exogenous parents of the underlying SCM ι( M)</p><formula xml:id="formula_67">pa(O) ∩ J ⊆ {F ∈ J : F ∩ O = ∅} ,</formula><p>where pa(O) denotes the set of parents of O in G a (ι( M)). Hence, in general, not all the hyperedges F ∈ H such that |F| = 2 (i.e., bidirected edges) are in the set of bidirected edges B of the graph of the underlying SCM G(ι( M)) = (V, E, B). We conclude that the graph of the underlying SCM is, in general, a sparser graph than the HEDG of the modular SCM.</p><p>Next, we show that the compatible system of solution functions of a modular SCM induces a compatible system of solution functions on the underlying SCM. For this we need the notion of loop-wise solvability for SCMs. DEFINITION A.30 (Loop-wise (unique) solvability for SCMs). We call an SCM M 1. loop-wisely solvable, if M is solvable w.r.t. every loop O ∈ L(G(M)), and 2. loop-wisely uniquely solvable, if M is uniquely solvable w.r.t. every loop O ∈ L(G(M)).</p><p>DEFINITION A.31 (Compatible system of solution functions for SCMs). For a loopwisely solvable SCM M, we call a family of measurable solution functions (g O ) O∈L(G(M)) , where g O is a measurable solution function of M w.r.t. O, a compatible system of solution functions, if for all O, Õ ∈ L(G(M)) with Õ ⊆ O and for P E -almost every e ∈ E and for all x ∈ X we have</p><formula xml:id="formula_68">x O = g O (x pa(O)\O , e pa(O) ) =⇒ x Õ = g Õ(x pa( Õ)\ Õ, e pa( Õ) ) .</formula><p>The underlying SCM of a modular SCM always has a compatible system of solution functions, by construction. PROPOSITION A.32. Let M = G, X , E, (g O ) O∈L(G) , P E be a modular SCM. Then the underlying SCM M := ι( M) is loop-wisely solvable. Moreover, it has a compatible system of solution functions (g O ) O∈L(G( M)) , where g O is a measurable solution function of M w.r.t.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>O.</head><p>This shows that a modular SCM can be seen as an SCM together with an additional structure of a compatible system of solution functions, and is, in particular, loop-wisely solvable.</p><p>Moreover, the class of simple SCMs corresponds exactly with those SCMs that are loopwisely uniquely solvable. LEMMA A. <ref type="bibr" target="#b32">33</ref>. An SCM M is simple if and only if it is loop-wisely uniquely solvable.</p><p>In particular, for simple SCMs, or loop-wisely uniquely solvable SCMs, there always exists a compatible system of solution functions. PROPOSITION A.34. Let M = I, J , X , E, f , P E be a simple SCM. Then every family of measurable solution functions (g O ) O∈L(G(M)) , where g O is a measurable solution function of M w.r.t. O, is a compatible system of solution functions. Overview of causal graphical models. The "gray" and "dark gray" areas contain all the causal graphical models that can be modeled by an SCM and an acyclic SCM, respectively.</p><p>A.4. Overview of causal graphical models Figure <ref type="figure">7</ref> gives an overview of the causal graphical models related to SCMs. The "gray" area contains all the causal graphical models that can be modeled by an SCM, by which we mean, that there exists an SCM that can describe all its observational and interventional distributions. The "dark gray" area contains all the causal graphical models which can be modeled by an acyclic SCM. Acyclic SCMs generalize causal Bayesian networks (causal BNs) <ref type="bibr" target="#b50">[51]</ref> to allow for latent confounders and to derive counterfactuals. Simple SCMs form a subclass of SCMs that extends acyclic SCMs to the cyclic setting, while preserving many of their convenient properties. Modular SCMs <ref type="bibr" target="#b17">[18]</ref> can be seen as SCMs that have an additional structure of compatible system of solution functions and contain, in particular, the class of simple SCMs. Forré and Mooij <ref type="bibr" target="#b17">[18]</ref> showed that modular SCMs satisfy various convenient properties, like marginalization and the general directed global Markov property. We show that for SCMs in general various of those properties still hold under certain solvability conditions. A generalization of SCMs, known as causal constraints models (CCMs), has been proposed <ref type="bibr" target="#b2">[3]</ref> in order to completely model the causal semantics of the equilibrium solutions of a dynamical system given the initial conditions. This class of CCMs is rich enough to model the causal semantics of SCMs, but does not come with a single graphical representation that provides both a Markov property and a causal interpretation <ref type="bibr" target="#b3">[4]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX B: (UNIQUE) SOLVABILITY PROPERTIES</head><p>In this appendix, we provide additional (unique) solvability properties for SCMs. In Appendix B.1 we provide a sufficient condition of solvability w.r.t. (strict) subsets. In Appendix B.2 we discuss how (unique) solvability is preserved under strict super-and subsets. In Appendix B.3 we discuss how (unique) solvability is preserved under unions and intersections. The proofs of the theoretical results in this appendix are given in Appendix E.</p><p>B.1. Sufficient condition for solvability w.r.t. subsets For solvability w.r.t. a (strict) subset of I there exists a sufficient condition that is similar to the sufficient (and necessary) condition (2) in Theorem 3.2 in the sense that it is formulated in terms of the solutions of (a subset of) the structural equations, but no measurability is required. PROPOSITION B.1 (Sufficient condition for solvability w.r.t. a subset). Let M = I, J , X , E, f , P E be an SCM and O ⊆ I a subset. If for P E -almost every e ∈ E and for all x \O ∈ X \O the topological space</p><formula xml:id="formula_69">S (e,x\O) := {x O ∈ X O : x O = f O (x, e)} ,</formula><p>with the subspace topology induced by X O is nonempty and σ-compact, <ref type="foot" target="#foot_11">19</ref> then M is solvable w.r.t. O.</p><p>For many purposes, this condition of σ-compactness suffices since it contains for example all countable discrete spaces, every interval of the real line, and moreover all the Euclidean spaces. In particular, it suffices to prove a sufficient and necessary condition for unique solvability w.r.t. a subset, in terms of the solutions of a subset of the structural equations (see <ref type="bibr">Theorem 3.6)</ref>. For larger solution spaces, we refer the reader to <ref type="bibr" target="#b29">[30]</ref>. For the class of linear SCMs (see Definition C.1), we provide in Proposition C.2 a sufficient and necessary condition for solvability w.r.t. a (strict) subset of I. EXAMPLE B.2 (Solvability is not preserved under strict sub-or supersets). Consider the SCM M = 3, ∅, R 3 , 1, f , P 1 where the causal mechanism is given by</p><formula xml:id="formula_70">f 1 (x) = x 1 • (1 -1 {1} (x 2 )) + 1 , f 2 (x) = x 2 , f 3 (x) = x 3 • (1 -1 {-1} (x 2 )) + 1 .</formula><p>This SCM is (uniquely) solvable w.r.t. the subsets {1, 2}, {2, 3}, however it is not (uniquely) solvable w.r.t. the subsets {1}, {3} and {1, 2, 3}, and not uniquely solvable w.r.t. {2}. However, in Proposition 3.10 we show that solvability w.r.t. O implies solvability w.r.t. every ancestral subset in G(M) O .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. (Unique) solvability w.r.t. unions and intersections</head><p>In general, (unique) solvability is not preserved under unions and intersections. The following example illustrates that (unique) solvability is in general not preserved under intersections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EXAMPLE B.3 (Solvability is not preserved under intersections).</head><p>Consider the SCM M = 3, ∅, R 3 , 1, f , P 1 where the causal mechanism is given by</p><formula xml:id="formula_71">f 1 (x) = 0 , f 2 (x) = x 2 • (1 -1 {0} (x 1 • x 3 )) + 1 , f 3 (x) = 0 .</formula><p>Then M is (uniquely) solvable w.r.t. {1, 2} and {2, 3}, however it is not (uniquely) solvable w.r.t. their intersection.</p><p>Example B.2 gives an example where (unique) solvability is not preserved under unions. Even, if we take the union of disjoint subsets, (unique) solvability is not preserved (see Example 2.4). Although, in general, unique solvability is not preserved under unions, we show next that unique solvability is preserved under the union of ancestral subsets, under the following assumptions. A consequence of this property is that in order to check whether an SCM is ancestrally uniquely solvable w.r.t. O, it suffices to check that it is uniquely solvable w.r.t. the ancestral subsets for each node in O. COROLLARY B.5. Let M = I, J , X , E, f , P E be an SCM and O ⊆ I a subset. Then M is ancestrally uniquely solvable w.r.t. O if and only if M is uniquely solvable w.r.t. an G(M)O (i) for every i ∈ O.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX C: LINEAR SCMS</head><p>In this appendix, we provide some results about (unique) solvability and marginalization for linear SCMs. Linear SCMs form a special class of SCMs that has seen much attention in the literature [see, e.g., <ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b26">27]</ref>. The proofs of the theoretical results in this appendix are given in Appendix E. DEFINITION C.1 (Linear SCM). We call an SCM M = I, J , R I , R J , f , P R J linear if each component of the causal mechanism is a linear combination of the endogenous and exogenous variables, that is</p><formula xml:id="formula_72">f i (x, e) = j∈I B ij x j + k∈J Γ ik e k ,</formula><p>where i ∈ I, B ∈ R I×I and Γ ∈ R I×J are matrices, and P R J is a product probability measure<ref type="foot" target="#foot_12">foot_12</ref> on R J .</p><p>For a subset O ⊆ I we also use the shorthand vector-notation</p><formula xml:id="formula_73">f O (x, e) = B OI x + Γ OJ e .</formula><p>A nonzero coefficient B ij for i, j ∈ I such that i = j corresponds with a directed edge j → i in the (augmented) graph, and a coefficient B ii = 1 for i ∈ I corresponds with a self-cycle i → i in the (augmented) graph of the SCM. A nonzero coefficient Γ ij for i ∈ I, j ∈ J with P Ej a nondegenerate probability distribution over R corresponds with a directed edge j → i in the augmented graph. A nonzero entry (ΓΓ T ) ij for i, j ∈ I with i = j such that there exists a k ∈ J for which Γ ik , Γ jk = 0 and P Ek a nondegenerate probability distribution over R corresponds with a bidirected edge i ↔ j in the graph of the SCM.</p><p>For linear SCMs, the solvability condition w.r.t. a subset, Definition 3.1, translates into a matrix condition. In order to state this condition we need to define the pseudoinverse (or the Moore-Penrose inverse) A + of a real matrix A <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b53">54]</ref>. The pseudoinverse of the matrix A is defined by A + := V Σ + U * , where A = U ΣV * is the singular value decomposition of A and Σ + is obtained by replacing each nonzero entry on the diagonal of Σ by its reciprocal <ref type="bibr" target="#b23">[24]</ref>. One of its useful properties is that AA + A = A. </p><formula xml:id="formula_74">A LL A + LL (B LO x O + Γ LJ e) = B LO x O + Γ LJ e</formula><p>is satisfied, where A + LL is the pseudoinverse of A LL . Moreover, if M is solvable w.r.t. L, then for every vector v ∈ R L the mapping</p><formula xml:id="formula_75">g v L : R O × R J → R L given by g v L (x O , e) = A + LL (B LO x O + Γ LJ e) + [I L -A + LL A LL ]v , is a measurable solution function for M w.r.t. L.</formula><p>For linear SCMs, the unique solvability condition w.r.t. a subset translates into a matrix invertibility condition, as was already shown in <ref type="bibr" target="#b26">[27]</ref>. </p><formula xml:id="formula_76">A LL = I L -B LL is invertible. Moreover, if M is uniquely solvable w.r.t. L, then the mapping g L : R O × R J → R L given by g L (x O , e) = A -1 LL (B LO x O + Γ LJ e) , is a measurable solution function for M w.r.t. L. Note that if A LL is invertible, then A + LL = A -1 LL (see Lemma 1.3 in [54]</formula><p>), and the matrix condition of Proposition C.2 is always satisfied and all the measurable solution functions g v L of Proposition C.2 are (up to a P E -null set) equal to the solution function g L of Proposition C.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>REMARK.</head><p>A sufficient condition for A LL to be invertible is that the spectral radius of B LL is less than one. If that is the case, then A -1 LL = ∞ n=0 (B LL ) n . Note that the nonzero nondiagonal entries of the matrix B LL represent the directed edges in the induced subgraph G(M) L . In particular, if the diagonal entries of the matrix B LL are zero, then for n ∈ N, the coefficients of the matrix (B LL ) n in the sum represent the sum of the product of the edge weights B ij over directed paths of length n in the induced subgraph G(M) L .</p><p>From Proposition 3.10 we know that an SCM is solvable w.r.t. L if and only if it is ancestrally solvable w.r.t. L. In particular, this result also holds for linear SCMs. We saw in Example 3.11 that a similar result for unique solvability does not hold, that is, in general, it does not hold that unique solvability w.r.t. L implies ancestral unique solvability w.r.t. L. For the class of linear SCMs we do have the following positive result. PROPOSITION C.4 (Equivalent unique solvability conditions for linear SCMs). For a linear SCM M and a subset L ⊆ I the following are equivalent:</p><formula xml:id="formula_77">1. M is uniquely solvable w.r.t. L; 2. M is ancestrally uniquely solvable w.r.t. L; 3. M is uniquely solvable w.r.t. each strongly connected component in G(M) L .</formula><p>Under the condition of unique solvability w.r.t. a subset L we can define the marginalization w.r.t. L of a linear SCM by mere substitution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PROPOSITION C.5 (Marginalization of a linear SCM).</head><p>Let M be a linear SCM and L ⊆ I a subset of endogenous variables such that I L -B LL is invertible. Then there exists a marginalization M marg(L) that is linear and with marginal causal mechanism f : From Theorem 5.6 we know that M and its marginalization M marg(L) over L are observationally, interventionally and counterfactually equivalent w.r.t. O. A similar result can also be found in <ref type="bibr" target="#b26">[27]</ref>. In contrast to nonlinear SCMs, this class of linear SCMs has the convenient property that every marginalization of a model of this class respects the latent projection. Moreover, the subclass of simple linear SCMs is even closed under marginalization.</p><formula xml:id="formula_78">R O × R J → R O given by f (x O , e) = [B OO + B OL A -1 LL B LO ]x O + [B OL A -1 LL Γ LJ + Γ OJ ]e , where A LL = I L -B LL . Moreover, this marginalization respects the latent projection, that is, G a • marg(L) (M) ⊆ marg(L) • G a (M). m 1 m 2 m 3 m 4 m 5 0 1 2 3 4 5 Q 0 = 0 Q 6 = L Q 1 Q 2 Q 3 Q 4 Q 5</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX D: EXAMPLES</head><p>In this appendix, we provide additional examples. In Appendix D.1 we provide some examples of SCMs that describe the equilibrium states of certain feedback systems governed by (random) differential equations <ref type="bibr" target="#b5">[6]</ref> that motivated our study of cyclic SCMs. In Appendix D.2 we provide additional examples that support the main text.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1. SCMs as equilibrium models</head><p>In many systems occurring in the real world feedback loops between observed variables are present. For example, in economics, the price of a product may be a function of the demanded or supplied quantities, and vice versa; or in physics, two masses that are connected by a spring may exert forces on each other. Such systems are often described by a system of (random) differential equations. In <ref type="bibr" target="#b5">[6]</ref> it was shown that SCMs are capable of modeling the causal semantics of the equilibrium states of such systems. For illustration purposes we provide the following toy example of interacting masses that are attached to springs. EXAMPLE D.1 (Damped coupled harmonic oscillator). Consider a one-dimensional system of d point masses m i ∈ R (i = 1, . . . , d) with positions Q i , which are coupled by springs, with spring constants k i &gt; 0 and equilibrium lengths i &gt; 0 (i = 0, . . . , d), under influence of friction with friction coefficients b i ∈ R (i = 1, . . . , d) and with fixed endpoints Q 0 = 0 and Q d+1 = L &gt; 0 (see Figure <ref type="figure" target="#fig_20">8</ref> (top)). The equations of motion of this system are provided by the following differential equations</p><formula xml:id="formula_79">d 2 Q i dt 2 = k i m i (Q i+1 -Q i -i ) + k i-1 m i (Q i-1 -Q i + i-1 ) - b i m i dQ i dt (i = 1, . . . , d) .</formula><p>The dynamics of the masses, in terms of the position, velocity and acceleration, is described by a single and separate equation of motion for each mass. Under friction, that is, b i &gt; 0 (i = 1, . . . , d), there is a unique equilibrium position, where the sum of forces vanishes for each mass. If one starts out of equilibrium, for example, by moving one or several masses out of equilibrium, then the masses will start to oscillate and converge to their unique equilibrium position. At equilibrium (i.e., for t → ∞) the velocity dQi dt and acceleration d 2 Qi dt 2 of the masses vanish (i.e., dQi dt , d 2 Qi dt 2 → 0), and thus the following equation holds at equilibrium</p><formula xml:id="formula_80">0 = k i m i (Q i+1 -Q i -i ) + k i-1 m i (Q i-1 -Q i + i-1 ) ,</formula><p>for each mass (i = 1, . . . , d). Hence, for each mass i = 1, . . . , d its equilibrium position Q i is given by</p><formula xml:id="formula_81">Q i = k i (Q i+1 -i ) + k i-1 (Q i-1 + i-1 ) k i + k i-1 .</formula><p>By considering the i and k i and L as fixed parameters, we arrive at a linear SCM (see <ref type="bibr" target="#b5">[6]</ref> for more details about constructing an SCM from a dynamical system)</p><formula xml:id="formula_82">M = {1, . . . , d}, ∅, R d , 1, f , P 1 ,</formula><p>where the causal mechanism f is given by</p><formula xml:id="formula_83">f i (q) = k i (q i+1 -i ) + k i-1 (q i-1 + i-1 ) k i + k i-1 .</formula><p>Alternatively, (some of) the parameters could be treated as exogenous variables instead. Its graph is depicted in Figure <ref type="figure" target="#fig_20">8</ref> (bottom). This SCM allows us to describe the equilibrium behavior of the system under perfect intervention. For example, when forcing the mass j to a fixed position Q j = ξ j with 0 ≤ ξ j ≤ L, the equilibrium positions of the masses correspond to the solutions of the intervened model M do({j},ξj) . It is an easy exercise to show that M is a simple SCM by using Proposition C.3.</p><p>Next, we show that the well known market equilibrium model from economics, which has been thoroughly discussed in the literature [see, e.g., 65], can be described by a (non-simple) SCM. This example illustrates how self-cycles enrich the class of SCMs. EXAMPLE D.2 (Price, supply and demand). Let X D denote the demand and X S the supply of a quantity of a product. The price of the product is denoted by X P . The following system of differential equations describes how the demanded and supplied quantities are determined by the price, and how price adjustments occur in the market:</p><formula xml:id="formula_84">X D = β D X P + E D X S = β S X P + E S dX P dt = X D -X S ,</formula><p>where E D and E S are exogenous random influences on the demand and supply, respectively, β D &lt; 0 is the reciprocal of the slope of the demand curve, and β S &gt; 0 is the reciprocal of the slope of the supply curve. At the situation known as a "market equilibrium", the price is determined implicitly by the condition that demanded and supplied quantities should be equal, since dXP dt = 0 at equilibrium. Applying the results in <ref type="bibr" target="#b5">[6]</ref> gives rise to a linear SCM M = {P, S, D}, {S, D}, R 3 , R 2 , f , P E at equilibrium with the causal mechanism defined by</p><formula xml:id="formula_85">f D (x, e) := β D x P + e D f S (x, e) := β S x P + e S f P (x, e) := x P + (x D -x S ) .</formula><p>Note how we use a self-cycle for P in order to implement the equilibrium equation X D = X S as the causal mechanism for the price P . <ref type="foot" target="#foot_13">21</ref> Moreover, M is uniquely solvable. Its augmented graph is depicted in Figure <ref type="figure" target="#fig_21">9</ref> (left). Next, we provide an example of how counterfactuals can be sensibly formulated for cyclic SCMs, namely for the price, supply and demand model at equilibrium. EXAMPLE D.3 (Price, supply and demand at equilibrium). Consider the price, supply and demand model at equilibrium of Example D.2 given by the SCM M. As an example of a counterfactual query, consider P(X P | do(X S = s, X S = s ), X P = p) , which denotes the conditional distribution of X P given X P = p of a solution of the intervened twin model M twin do({S,S },(s,s )) . In words: how would-ceteris paribus-price have been distributed, had we intervened to set supplied quantities equal to s , given that actually we intervened to set supplied quantities equal to s and observed that this led to price p? A straightforward calculation shows that this counterfactual distribution of price is the Dirac measure on x P = p + (s -s)/β D . The augmented graphs of the SCM, its twin graph, and its intervened twin graph are depicted in Figure <ref type="figure" target="#fig_21">9</ref>. </p><formula xml:id="formula_86">E D X D E S X S X P G a (M) E D X D X D E S X S X S X P X P G a (M twin ) E D X D X D E S X S X S X P X P G a (M twin ) do({S,S })</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2. Additional examples</head><formula xml:id="formula_87">= 1, 1, X , E, f, P E with X = E = {-1, 0, 1}, P E ({-1}) = P E ({1}) = 1</formula><p>2 and f (x, e) = e 2 + e -1. Let M be the SCM M but with a different causal mechanism f (x, e) = e. Then the sets of solutions of the structural equations agree for both SCMs for e ∈ {-1, +1}, while they differ only for e = 0, which occurs with probability zero. Hence, a pair of random variables (X, E) is a solution of M if and only if it is a solution of M.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EXAMPLE D.5 (The for-all and for-almost-every quantifier do not commute in general).</head><p>Consider the SCM M = 2, 1, X , E, f , P E with X = (0, 1) 2 , E = (0, 1), the causal mechanism f given by</p><formula xml:id="formula_88">f 1 (x, e) = x 1 , f 2 (x, e) = 1 {0} (x 1 -e) • (x 2 + 1) ,</formula><p>and P E = P E with E ∼ U(0, 1). Define the property</p><formula xml:id="formula_89">P (x, e) := 1 if x = f (x, e) holds, 0 otherwise. X 1 X 2 E 1 E 2 E 3 X 1 X 2 E Fig 10:</formula><p>Augmented graphs of the SCMs M (left) and M * (right) in Example D.6. For SCM M * , the exogenous variable E consists of two real-valued components; the structural equation for X 1 depends only on the first, while the structural equation for X 2 depends only on the second component.</p><p>Then, for all x ∈ X and for P E -almost every e ∈ E the property P (x, e) holds, however for P E -almost every e ∈ E and for all x ∈ X the property P (x, e) does not hold, since for P Ealmost every e ∈ E the equation x = f (x, e) does not hold for x 1 = e. Hence, in general, for a property P (x, e) we have that for all x ∈ X and for P E -almost every e ∈ E P (x, e) does not imply for P E -almost every e ∈ E for all x ∈ X P (x, e) (see Lemma F.11 for additional properties of the for-almost-every quantifier).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EXAMPLE D.6 (Representation of latent confounders).</head><p>Consider the SCM M = 2, 3, R 2 , R 3 , f , P R 3 with causal mechanism given by f 1 (e 1 , e 3 ) = e 1 + e 3 f 2 (x 1 , e 2 , e 3 ) = x 1 e 3 + e 2 and P R 3 the standard-normal distribution on R 3 ; Figure <ref type="figure" target="#fig_13">10</ref> (left) shows the corresponding augmented graph. Then there exists no SCM M * = 2, 1, R 2 , R 2 , f * , P * R 2 that satisfies the following conditions:</p><p>1. M * is interventionally equivalent to M, 2. its structural equations have the form</p><formula xml:id="formula_90">x 1 = f * 1 (e * 1 ) x 2 = f * 2 (x 1 , e * 2 )</formula><p>, where e * 1 , e * 2 are the two components of e * = (e * 1 , e * 2 ) ∈ R 2 , 3. the function e * 2 → f * 2 (x 1 , e * 2 ) is strictly monotonically increasing for all x 1 ∈ R, 4. the cumulative distribution function F * 2 of the second component of P * R 2 is continuous and strictly monotonically increasing.</p><p>The augmented graph of such an SCM is shown in Figure <ref type="figure" target="#fig_13">10 (right)</ref>.</p><p>The proof of this statement proceeds by contradiction. Assume that such an SCM M * exists. For any uniquely solvable SCM M and any endogenous variable i appearing in M, we denote with F M Xi the marginal cumulative distribution function of the i th component of the observational distribution of M. For all ξ ∈ R, we have for all</p><formula xml:id="formula_91">x 2 ∈ R (1) F Mdo({1},ξ) X2 (x 2 ) = P(ξE 3 + E 2 ≤ x 2 ) = Φ x 2 / 1 + ξ 2 ,</formula><p>where Φ denotes the (invertible) cdf of the standard-normal distribution. Now define φ : R → R with φ(e 2 ) := Φ -1 (F * 2 (e 2 )) and define the SCM</p><formula xml:id="formula_92">M := 2, 1, R 2 , R 2 , f , PR 2 such that the causal mechanism f is given by f1 (e 1 ) = f * 1 (e 1 ), f2 (x 1 , e 2 ) = f * 2 (x 1 , φ -1 (e 2 )),</formula><p>and PR 2 is the push-forward measure of P * R 2 using (I R , φ). Then, M is interventionally equivalent to M * by construction, and the second component of PR 2 has a standard-normal distribution. Let ( X1 , X2 , Ẽ) be a solution of M and let us write Ẽ = ( Ẽ1 , Ẽ2 ). Then, for all ξ ∈ R and ẽ2 ∈ R,</p><formula xml:id="formula_93">F Mdo({1},ξ) X2 ( f2 (ξ, ẽ2 )) = P( f2 (ξ, Ẽ2 ) ≤ f2 (ξ, ẽ2 )) = P( Ẽ2 ≤ ẽ2 ) = Φ(ẽ 2 ),</formula><p>using that ẽ2 → f2 (ξ, ẽ2 ), too, is strictly monotonically increasing for all ξ. This implies that, for all ξ ∈ R and ẽ2 ∈ R,</p><formula xml:id="formula_94">f2 (ξ, ẽ2 ) = (F Mdo({1},ξ) X2 ) -1 Φ(ẽ 2 ) = 1 + ξ 2 ẽ2 ,</formula><p>where we used interventional equivalence of M and M, and (1) for the second equality.</p><p>Furthermore, X2 = f2 ( X1 , Ẽ2 ) = 1 + X2 1 Ẽ2 a.s., so Ẽ2 = X2 / 1 + X2 1 a.s.. Now let (X 1 , X 2 , E 1 , E 2 , E 3 ) be a solution of M. By observational equivalence, ( X1 , X2 ) has the same distribution as (X 1 , X 2 ), and thus Ẽ2 is distributed as</p><formula xml:id="formula_95">X 2 1 + X 2 1 = (E 1 + E 3 )E 3 + E 2 1 + (E 1 + E 3 ) 2 a.s..</formula><p>This contradicts the fact that Ẽ2 has a standard-normal distribution as, for example, the mean of the right-hand side is nonzero.</p><p>EXAMPLE D.7 (Counterfactual density unidentifiable from observational and interventional densities <ref type="bibr" target="#b10">[11]</ref>). Let ρ ∈ R and</p><formula xml:id="formula_96">M ρ = 2, 2, {0, 1} × R, {0, 1} × R 2 , f , P E</formula><p>be the SCM with causal mechanism given by</p><formula xml:id="formula_97">f 1 (x, e) = e 1 , f 2 (x, e) = e 21 (1 -x 1 ) + e 22 x 1</formula><p>and P E = P (E1,E2) with E 1 ∼ Bernoulli(1/2),</p><formula xml:id="formula_98">E 2 := E 21 E 22 ∼ N 0, 1 ρ ρ 1 normally distributed and E 1 ⊥ ⊥ E 2 .</formula><p>In an epidemiological setting, this SCM could be used to model whether a patient was treated or not (X 1 ) and the corresponding outcome for that patient (X 2 ). Suppose in the actual world we did not assign treatment to a patient (X 1 = 0) and the outcome was X 2 = c ∈ R. Consider the counterfactual query "What would the outcome have been, if we had assigned treatment to this patient?". We can answer this question by introducing a parallel counterfactual world that is modeled by the twin SCM M twin ρ , as depicted in Figure <ref type="figure" target="#fig_23">11</ref>. The counterfactual query then asks for p(</p><formula xml:id="formula_99">X 2 = x 2 | do(X 1 = 1, X 1 = 0), X 2 = c). One can calculate that X 2 X 2 | do(X 1 = 1, X 1 = 0) ∼ N 0, 1 ρ ρ 1 and hence X 2 | do(X 1 = 1, X 1 = 0), X 2 = c ∼ N (ρc, 1 -ρ 2 ).</formula><p>Note that the answer to the counterfactual query depends on a quantity ρ that we cannot identify from the observational density p(X 1 , X 2 ) or the interventional densities p(X 2 | do(X 1 = 0)) and p(X 2 | do(X 1 = 1)), none of which depends on ρ. Therefore, even data from randomized controlled trials combined with observational data would not suffice to determine the value of this particular counterfactual query. Indeed, SCMs M ρ and M ρ with ρ = ρ are interventionally equivalent, but not counterfactually equivalent.  Section 3 EXAMPLE D.8 (Mixtures of solutions are solutions). Let M = 1, ∅, R, 1, f, P 1 be an SCM with causal mechanism f : X × E → X defined by f (x, e) = x -x 2 + 1. There exist only two measurable solution functions g ± : E → X for M, defined by g ± (e) = ±1. Let X : Ω → R be a random variable that is a nontrivial mixture of point masses on {-1, +1}. Then X is a solution of M, however neither g + (E) = X a.s., nor g -(E) = X a.s., for any random variable E such that P E = P E . EXAMPLE D.9 (Solvability is not preserved under perfect intervention). Consider the SCM M = 2, ∅, R 2 , 1, f , P 1 with the following causal mechanism</p><formula xml:id="formula_100">E 2 E 1 X 1 X 2 E 2 E 1 X 1 X 2 X 1 X 2 E 2 X 1 X 2 X 1 X 2</formula><formula xml:id="formula_101">X 1 X 2 E 1 E 2 G a ( M) X 1 X 2 E 1 E 2 G a ( M) X 1 X 2 E 1 E 2 G a (M) X 1 X 2 E 1 E 2 G a ( M)</formula><formula xml:id="formula_102">f 1 (x) = x 1 + x 2 1 -x 2 + 1 , f 2 (x) = x 2 (1 -1 {0} (x 1 )) + 1 .</formula><p>This SCM has a unique solution (0, 1). Doing a perfect intervention do({1}, ξ 1 ) for some ξ 1 = 0, however, leads to an intervened model M do({1},ξ1) that is not solvable. Performing instead the perfect intervention do({2}, ξ 2 ) for some ξ 2 &gt; 1 leads also to a nonuniquely solvable SCM M do({2},ξ2) which has solutions with multiple induced distributions, for example,</p><formula xml:id="formula_103">(X 1 , X 2 ) = (φ(ξ 2 ) √ ξ 2 -1, ξ 2 )</formula><p>with some measurable φ : R → {-1, +1}, but also mixtures of those. Section 4 EXAMPLE D.10 (Counterfactually equivalent SCMs with different graphs). Consider the SCM M = 2, 2, {-1, 1} 2 , {-1, 1} 2 , f , P E with causal mechanism given by f1 (x, e) = e 1 and f2 (x, e) = e 2 , and</p><formula xml:id="formula_104">P E = P E with E 1 , E 2 ∼ U({-1, 1}) uniformly distributed and E 1 ⊥ ⊥ E 2 .</formula><p>Consider also the SCM M that is the same as M except for its causal mechanism, which is given by f 1 (x, e) = e 1 and f 2 (x, e) = e 1 e 2 . Then M and M are counterfactually equivalent although G(M) is not equal to G( M) (see Figure <ref type="figure" target="#fig_24">12</ref>). Section 5</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>EXAMPLE D.11 (Marginalization condition of an SCM is not a necessary condition).</head><p>Consider the SCM M = 4, 1, R 4 , R, f , P R with causal mechanism given by</p><formula xml:id="formula_105">f 1 (x, e) = e , f 2 (x, e) = x 1 , f 3 (x, e) = x 2 , f 4 (x, e) = x 4</formula><p>and P R is the standard-normal measure on R. This SCM is solvable w.r.t. L = {2, 4}, but not uniquely solvable w.r.t. L, and hence we cannot apply Definition 5.3 to L. However, the SCM M on the endogenous variables {1, 3} with the causal mechanism f given by f1 (x, e) = e and f3 (x, e) = x 1 is counterfactually equivalent to M w.r.t. {1, 3}, which can be checked easily. EXAMPLE D.12 (Graph of the marginal SCM is a strict subgraph of the latent projection). Consider the SCM M = 3, 1, R 3 , R, f , P R with causal mechanism given by</p><formula xml:id="formula_106">f 1 (x, e) = e 1 , f 2 (x, e) = x 1 -x 3 , f 3 (x, e) = x 1</formula><p>and take for P R the standard-normal measure on R. In contrast, to the (augmented) graph of M, there is no directed path in the (augmented) graph of the marginal SCM M marg({3}) .</p><p>Section 7 EXAMPLE D.13 (Detecting a bidirected edge in the graph of an SCM). Consider the SCM M = 2, 2, {-1, 1} 2 , {-1, 1} 2 , f , P E with causal mechanism given by f1 (x, e) = e 1 and f2 (x, e) = x 1 e 2 , and P E = P E with E 1 , E 2 ∼ U({-1, 1}) uniformly distributed and E 1 ⊥ ⊥ E 2 . Consider also the SCM M that is the same as M except for its causal mechanism, which is given by f1 (x, e) = e 1 and f2 (x, e) = x 1 e 1 . See Figure <ref type="figure" target="#fig_24">12</ref> for their augmented graphs. For the SCM M we observe that the marginal interventional distribution P Mdo({1},ξ 1 ) (X 2 = -1) is not equal to the conditional distribution P M(X 2 = -1 | X 1 = ξ 1 ) for both ξ 1 = -1 and ξ 1 = 1. This observation suffices to identify the presence of the bidirected edge 1 ↔ 2 in the graph G( M). For the SCM M, whose graph does not contain the bidirected edge 1 ↔ 2, the marginal interventional distribution and conditional distribution coincide.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX E: PROOFS</head><p>This appendix contains the proofs of all the theoretical results in the appendices A, B and C, and the main text. Some of the proofs will rely on the measure theoretic terminology and results of Appendix F.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1. Proofs of the appendices</head><p>Appendix A PROOF OF LEMMA A.5. It suffices to show that for every C-d-open walk between i and j in G, there exists a C-d-open path between i and j in G. Take a C-d-open walk π = (i = i 0 , . . . , i n = j). If a node occurs more than once in π, let i j be the first occurrence of in π and i k the last occurrence of in π. We now construct a new walk π from π by removing the subwalk between i j and i k of π from π. It is easy to check that the new walk π is still C-d-open. If is an endpoint on π , then i j or i k must be endpoint of π, and hence / ∈ C. If is a non-endpoint non-collider on π , then also i j or i k must have been a non-endpoint non-collider on π, and hence / ∈ C. If is a collider on π , then either (i) i j or i k are both colliders on π, and hence is ancestor of C in G, or (ii) on the subwalk between i j and i k that was removed, there must be a directed path in G from i j or i k to a collider in an G (C), and hence, is in an G (C). The other nodes on π cannot be responsible for C-d-blocking the walk, since they also occur (together with their adjacent edges) on π and they do not C-d-block π.</p><p>In π , the number of nodes that occur multiple times is at least one less than in π. Repeat this procedure until no repeated nodes are left. PROOF OF THEOREM A.7. The first case is a well known result. An elementary proof is obtained by noting that an acyclic system of structural equations trivially satisfies the local directed Markov property, and then apply <ref type="bibr" target="#b34">[35,</ref><ref type="bibr">Proposition 4]</ref>, followed by applying the stability of d-separation with respect to (graphical) marginalization <ref type="bibr" target="#b17">[18,</ref><ref type="bibr">Lemma 2.2.15]</ref>. Alternatively, the result also follows from sequential application of Theorems 3.8.2, 3.8.11, 3.7.7, 3.7.2 and 3.3.3 (using Remark 3.3.4) in <ref type="bibr" target="#b17">[18]</ref>.</p><p>The discrete case is proved by the series of results Theorem 3.8.12, Remark 3.7.2, Theorem 3.6.6 and 3.5.2 in <ref type="bibr" target="#b17">[18]</ref>.</p><p>The linear case is proved in Example 3.8.17 in <ref type="bibr" target="#b17">[18]</ref>. To connect the assumptions made there with the ones we state here, observe that under the linear transformation rule for Lebesgue measures, the image measure of P E under the linear mapping R J → R I : e → Γ IJ e gives a measure on X = R I with a density w.r.t. the Lebesgue measure on R I , as long as the image of the linear mapping is the entire R I . This is guaranteed if each causal mechanism has a nontrivial dependence on some exogenous variable(s), that is, for each i ∈ I there is some j ∈ J with Γ ij = 0. PROOF OF PROPOSITION A.12. This follows directly from the fact that the strongly connected components of G a (M) form a DAG by Lemma A.2 and that the directed edges in G a (acy(M)) by construction respect every topological ordering of that DAG. Both SCMs are observationally equivalent by construction. PROOF OF PROPOSITION A.14. This follows immediately from the Definitions A.11 and A.13. PROOF OF LEMMA A.17. It suffices to show that for every C-σ-open walk between i and j in G, there exists a C-σ-open path between i and j in G. Let π = (i = i 0 , . . . , i n = j) be a C-σ-open walk in G. If a node occurs more than once in π, let i j be the first node in π and i k the last node in π that are in the same strongly connected component as . Since i j and i k are in the same strongly connected component, there are directed paths</p><formula xml:id="formula_107">i j → • • • → i k and i k → • • • → i j in G.</formula><p>We now construct a new walk π from π by replacing the subwalk between i j and i k of π by a particular directed path between i j and i k : (i) If k = n, or if k &lt; n and i k → i k+1 on π, we replace it by a shortest directed path i j → • • • → i k , otherwise (ii) we replace it by a shortest directed path i j ← • • • ← i k . We now show that the new walk π is still C-σ-open.</p><p>π cannot become C-σ-blocked through one of the initial nodes i 0 . . . i j-1 or one of the final nodes i k+1 . . . i n on π , since these nodes occur in the same local configuration on π and do not C-σ-block π by assumption. Furthermore, π cannot become C-σ-blocked through one of the nodes strictly between i j and i k on π (if there are any), since these nodes are all non-endpoint non-colliders that only point to nodes in the same strongly connected component on π . Because</p><formula xml:id="formula_108">π is C-σ-open, i k / ∈ C if k = n or if i k → i k+1 on π.</formula><p>This holds in particular in case (i). Similarly, i j / ∈ C if j = 0 or i j-1 ← i j on π. In case (i), π is not C-σ-blocked by i k because i k is a non-collider on π but i k / ∈ C. Also i j does not C-σ-block π . Assume i j = i k (otherwise there is nothing to prove). If j = 0, or if j &gt; 0 and i j-1 ← i j on π , then the same holds for π and hence i j / ∈ C; i j is then a noncollider on π , but i j / ∈ C. If j &gt; 0 and i j-1 ↔ i j or i j-1 → i j on π then i j is a non-endpoint non-collider on π that does not point to a node in another strongly connected component. Now consider case (ii). If j = 0 or i j-1 ← i j on π then this case is analogous to case (i). So assume j &gt; 0 and i j-1 → i j or i j-1 ↔ i j on π . If i j is an endpoint of π , then i j = i k and k = n and therefore i k / ∈ C, and hence i j and i k do not C-σ-block π . Otherwise, i j must be a collider on π (whether i j = i k or not). Then on the subwalk of π between i j and i k there must be a directed path from i j to a collider that is ancestor of C, which implies that i j is itself ancestor of C, and hence i j does not C-σ-block π . Also i k cannot C-σ-block π . Assume i j = i k (otherwise there is nothing to prove). Since i k ← i k+1 or i k ↔ i k+1 on π , i k is a non-endpoint non-collider on π that does not point to a node in another strongly connected component. Now in π , the number of nodes that occurs more than once is at least one less than in π. Repeat this procedure until no nodes occur more than once. PROOF OF PROPOSITION A.19. This follows directly as a special case of Corollary 2.8.4 in <ref type="bibr" target="#b17">[18]</ref>. PROOF OF THEOREM A.21. An SCM M that is uniquely solvable w.r.t. each strongly connected component is uniquely solvable and hence, by Theorem 3.6, all its solutions have the same observational distribution. The last statement follows from the series of results Theorem 3.8.2, 3.8.11, Lemma 3.7.7 and Remark 3.7.2 in <ref type="bibr" target="#b17">[18]</ref>. Alternatively, we give here a shorter proof: Under the stated conditions one can always construct the acyclification acy(M) which is observationally equivalent to M and is acyclic (see Proposition A.12) and hence we can apply Theorem A.7 to acy(M). Together with Proposition A. <ref type="bibr">14 and A.19</ref> this gives This, together with the fact that the family of mappings (g O ) O∈L(G) is a compatible system of solution functions, implies that for P E -almost every e ∈ E and for all x ∈ X we have</p><formula xml:id="formula_109">A σ ⊥ G(M) B | C ⇐⇒ A d ⊥ acy(G(M)) B | C =⇒ A d ⊥ G(acy(M)) B | C =⇒ X A ⊥ ⊥ P X M X B | X C ,</formula><formula xml:id="formula_110">x O = gO (x pa(O)\O , e pa(O) ) =⇒ x O = fO (x, e) .</formula><p>The second measurable selection theorem, Theorem F.9, now implies that there exists a measurable Hence for P E -almost every e ∈ E and for all x ∈ X</p><formula xml:id="formula_111">x D = f D (x, e) ⇐⇒          x A\C = f A\C (x, e) x C = f C (x, e) x C = f C (x, e) x Ã\C = f Ã\C (x, e) ⇐⇒         </formula><p>x A\C = (g A ) A\C (x pa(A)\A , e pa(A) )</p><p>x C = (g A ) C (x pa(A)\A , e pa(A) ) x C = (g Ã) C (x pa( Ã)\ Ã, e pa( Ã) )</p><p>x Ã\C = (g Ã) Ã\C (x pa( Ã)\ Ã, e pa( Ã) )</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>⇐⇒</head><p>x A = g A (x pa(A)\A , e pa(A) )</p><p>x Ã = g Ã(x pa( Ã)\ Ã, e pa( Ã) ) . PROOF OF COROLLARY B.5. It suffices to show the implication to the left. We have to show that M is uniquely solvable w.r.t. each ancestral subset of G(M) O . The proof proceeds via induction with respect to the size of the ancestral subset. For ancestral subsets of size 0, the claim is trivially true. Ancestral subsets of size 1 must be of the form {i} = an G(M)O (i) for i ∈ O and hence the claim is true by assumption. Assume that the claim holds for all the set of nodes V is a strongly connected component in G(M) L . Since an upper triangular block matrix A LL is invertible if and only if every diagonal block in A LL is invertible, we have that M is uniquely solvable w.r.t. L if and only if M is uniquely solvable w.r.t. each strongly connected component in G(M) L . PROOF OF PROPOSITION C.5. By the definition of marginalization and Proposition C.3 the marginal causal mechanism f is given by</p><formula xml:id="formula_112">f (x O , e) := f O (x O , g L (x O , e), e) = B OO x O + B OL g L (x O , e) + Γ OJ e = [B OO + B OL A -1 LL B LO ]x O + [B OL A -1 LL Γ LJ + Γ OJ ]e .</formula><p>From Propositions C.4 and 5.11 it follows that the marginalization respects the latent projection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2. Proofs of the main text</head><p>Section 2 PROOF OF PROPOSITION 2.11. Let i ∈ I. Note that Definition 2.6 can alternatively be formulated as follows: for k ∈ I ∪ J , k ∈ pa(i) if and only if there exists a measurable mapping fi : X × E → X i such that for P E -almost every e ∈ E and for all x ∈ X ,</p><formula xml:id="formula_113">x i = f i (x, e) ⇐⇒ x i = fi (x, e)</formula><p>and either k ∈ I and there exists xk ∈ X k such that fi (x, e) = fi (x \k , xk , e) for all x ∈ X , e ∈ E, or k ∈ J and there exists êk ∈ E k such that fi (x, e) = fi (x, e \k , êk ) for all x ∈ X , e ∈ E. By repeatedly applying (this formulation of) Definition 2.6 to all k / ∈ pa(i), we obtain the existence of a measurable mapping fi : X × E → X i and x\pa(i) ∈ X \pa(i) , ê\pa(i) ∈ E \pa(i) such that for P E -almost every e ∈ E and for all x ∈ X ,</p><formula xml:id="formula_114">x i = f i (x, e) ⇐⇒ x i = fi (x, e),</formula><p>and for all e ∈ E and all x ∈ X , fi (x, e) = fi (x pa(i) , x\pa(i) , e pa(i) , ê\pa(i) ).</p><p>Define the SCM M as M except that its causal mechanism is f instead of f . Then M is structurally minimal and equivalent to M. PROOF OF PROPOSITION 2.14. The do(I, ξ I ) operation on M completely removes the functional dependence on x and e from the f i components for i ∈ I and hence the corresponding incoming directed and bidirected edges on nodes in I from the (augmented) graph.</p><p>PROOF OF PROPOSITION 2.15. The first statement follows from Definitions 2.12 and 2.13. For the second statement, note that a perfect intervention can only remove parental relations, and therefore will never introduce a cycle. PROOF OF <ref type="bibr">PROPOSITION 2.19</ref>. This follows directly from Definitions 2.17 and 2.18.</p><p>PROOF OF PROPOSITION 2.20. The additional edges introduced by the twin operation cannot lead to a directed cycle involving both copied and original nodes, because there are no edges pointing from copied nodes to original nodes (i.e., of the form i → v with i ∈ I and v ∈ V). Directed cycles involving only original nodes are absent by assumption, and directed cycles involving only copied nodes as well since they would correspond with a directed cycle in the original directed graph. PROOF OF PROPOSITION 2.21. It suffices to prove the property for directed graphs, since the property for SCMs follows directly from Definitions 2.12 and 2.17.</p><p>Applying the intervention do(I) on the graph G removes all the incoming edges from the nodes in I. Now, if we perform the twin operation w.r.t. I on this graph do(I)(G), then we copy the same edges as if we had twinned the graph G w.r.t. I, except those edges that do point to one of the nodes in I. Hence, if we apply the intervention do(I ∪ I ) on the graph twin(I)(G), which removes all incoming edges of both I and its copy I , then we clearly obtain the same graph. This is a measurable set, since S(M) = h -1 (∆), where h : E × X → X × X is the measurable mapping defined by h(e, x) = (x, f (x, e)) and ∆ is the set defined by {(x, x) : x ∈ X }, which is measurable since X is Hausdorff. Note that</p><formula xml:id="formula_115">A := pr E (S(M)) = {e ∈ E : ∃x ∈ X s.t. x = f (x, e)} ,</formula><p>is an analytic set because the projection pr E : X × E → E is a measurable mapping between standard measurable spaces (Lemma F.3).</p><p>Suppose that (1) holds, that is, M has a solution. Then there exists a pair of random variables (E, X) : Ω → E × X such that X = f (X, E) P-a.s.. Note that {ω ∈ Ω :</p><formula xml:id="formula_116">X(ω) = f X(ω), E(ω) } ⊆ {ω ∈ Ω : ∃x ∈ X s.t. x = f x, E(ω) } ⊆ E -1 {e ∈ E : ∃x ∈ X s.t. x = f (x, e)} = E -1 (A).</formula><p>By Lemma F.6, A is P E -measurable because it is analytic, and we can write</p><formula xml:id="formula_117">A = B ∪ N with B ⊆ E measurable and N a P E -null set. Hence E -1 (A) = E -1 (B) ∪ E -1 (N ) where E -1 (N ) is a P-null set. Therefore, E -1 (B) ⊇ {ω ∈ Ω : X(ω) = f X(ω), E(ω) } \ E -1 (N )</formula><p>which implies that P(E -1 (B)) = 1. Hence, E \ A is a P E -null set. In other words, for P Ealmost every e ∈ E the structural equations x = f (x, e) have a solution x ∈ X , that is, (2) holds.</p><p>Suppose that (2) holds. Then E \ pr E (S(M)) is a P E -null set. By application of the measurable selection theorem F.8, there exists a measurable g : E → X such that for P Ealmost all e ∈ E, g(e) = f (g(e), e). Hence, there exists a measurable mapping g : E → X such that for P E -almost every e ∈ E and for all x ∈ X x = g(e) =⇒ x = f (x, e) , which we call property (A). Let f : E × X → X be the causal mechanism of a structurally minimal SCM that is equivalent to M (see Proposition 2.11). In particular, for any \pa(I) ∈ E \pa(I) , we have that f (x, e) = f (x, e pa(I) , \pa(I) ) for all x ∈ X and all e ∈ E. This means that we may also consider f as a mapping f : X × E pa(I) → X . By applying Lemma F.10 to the canonical projection pr Epa(I) : E → E pa(I) and using the equivalence of f and f , we obtain that for P Epa(I) -almost all e pa(I) ∈ E pa(I) there exists x ∈ X with x = f (x, e pa(I) ). By applying the implication (2) =⇒ (A) to E pa(I) and f , we conclude the existence of a measurable g : E pa(I) → X such that for P Epa(I) -almost all e pa(I) ∈ E pa(I) , g(e pa(I) ) = f (g(e pa(I) ), e pa(I) ). Once more using Lemma F.10, we obtain that for P E -almost all e ∈ E, g(e pa(I) ) = f (g(e pa(I) ), e). In other words, (3) holds.</p><p>Lastly, suppose that (3) holds, that is there exists a measurable solution function g : E pa(I) → X . Then the measurable mappings E : E → E and X : E → X , defined by E(e) := e and X(e) := g(e pa(I) ), respectively, define a pair of random variables (X, E) such that X = f (X, E) holds a.s. and hence (X, E) is a solution. Hence (1) holds. PROOF OF PROPOSITION 3.4. Let f : E × X → X be the causal mechanism of a structurally minimal SCM M that is equivalent to M (see Proposition 2.11). For a subset O ⊆ I consider the induced subgraph G a (M) O of the augmented graph G a (M) on O. Then the acyclicity of G a (M) implies that the induced subgraph G a (M) O is acyclic, and hence there exists a topological ordering on the nodes O. We can substitute the components fi of the causal mechanism f for i ∈ O into each other along this topological ordering. This gives a measurable solution function g O : X pa(O)\O × E pa(O) → X O for M, and hence for M. It is clear from the acyclic structure that this mapping g O is independent of the choice of the topological ordering and is the only solution function for M. Therefore, M is uniquely solvable w.r. </p><formula xml:id="formula_118">X x O = g O (x pa(O)\O , e pa(O) ) ⇐⇒ x O = f O (x, e) .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>This implies (1).</head><p>For the last statement, assume that M is uniquely solvable. Let g : E pa(I) → X be a measurable solution function. Then there exists a measurable set B ⊆ E with P E (B) = 1 and for all e ∈ B, ∀x ∈ X : x = f (x, e) =⇒ x = g(e pa(I) ).</p><p>The existence of a solution for M follows directly from Theorem 3.2. Each solution (X, E) : Ω → X × E of M satisfies X(ω) = f (X(ω), E(ω)) P-a.s.. In addition, it satisfies E(ω) ∈ B P-a.s., since P • E -1 = P E . Hence, it satisfies X(ω) = g(E(ω) pa(I) ) P-a.s.. Thus for every solution (X, E) the associated observational distribution is the push-forward of P E under g • pr pa(I) . PROOF OF <ref type="bibr">PROPOSITION 3.8</ref> </p><formula xml:id="formula_119">x ∈ X x O = g O (x pa(O)\O , e pa(O) ) =⇒ x O = f O (x, e) .</formula><p>Let f : E × X → X be the causal mechanism of a structurally minimal SCM M that is equivalent to M (see Proposition 2.11). Let P := an G(M)O (A) for some A ⊆ O. Then for P E -almost every e ∈ E and for all x ∈ X</p><formula xml:id="formula_120">x P = (g O ) P (x pa(O)\O , e pa(O) ) x O\P = (g O ) O\P (x pa(O)\O , e pa(O) )</formula><p>=⇒ x P = fP (x pa(P) , e pa(P) ) x O\P = fO\P (x pa(O\P) , e pa(O\P) ) .</p><p>Since pa(P) \ P ⊆ pa(O) \ O, we have that in particular for P E -almost every e ∈ E and for all x ∈ X x P = (g O ) P (x pa(O)\O , e pa(O) ) =⇒ x P = fP (x pa(P) , e pa(P) ) .</p><p>This implies that the mapping (g O ) P cannot depend on elements different from pa(P). Moreover, it follows from the definition of P that (pa(O) \ O) ∩ pa(P) = pa(P) \ P and thus we have pa(O) \ O = (pa(P) \ P) ∪ (pa(O) \ (O ∪ pa(P))). Now, pick an element xpa(O)\(O∪pa(P)) ∈ X pa(O)\(O∪pa(P)) and define the mapping gP : X pa(P)\P × E pa(P) → X P by gP (x pa(P)\P , e pa(P) ) := (g O ) P (x pa(P)\P , xpa(O)\(O∪pa(P)) , e pa(O) ) .</p><p>Then, for P E -almost every e ∈ E and for all x ∈ X x P = gP (x pa(P)\P , e pa(P) ) ⇐⇒ x P = (g O ) P (x pa(O)\O , e pa(O) ) .</p><p>Together this gives that for P E -almost every e ∈ E and for all x ∈ X x P = gP (x pa(P)\P , e pa(P) ) =⇒ x P = fP (x pa(P) , e pa(P) ) .</p><p>which is equivalent to the statement that M is solvable w.r.t. an G(M)O (A). Section 4 LEMMA E.1. Let M be an SCM that is uniquely solvable w.r.t. two subsets A, B ⊆ I that satisfy A ⊆ B and pa(A) \ A ⊆ pa(B) \ B. Let g A : X pa(A)\A × E pa(A) → X A and g B : X pa(B)\B × E pa(B) → X B be measurable solution functions for M w.r.t. A and B, respectively. Then for P E -almost every e ∈ E and for all x ∈ X g A (x pa(A)\A , e pa(A) ) = (g B ) A (x pa(B)\B , e pa(B) ) .</p><p>PROOF. Without loss of generality, we assume that M is structurally minimal (see Proposition 2.11). Let Ē ⊆ E be a measurable set with P E ( Ē) = 1 such that for all e ∈ Ē for all x ∈ X :</p><formula xml:id="formula_121">x A = g A (x pa(A)\A , e pa(A) ) ⇐⇒ x A = f A (x pa(A) , e pa(A) ) and x B = g B (x pa(B)\B , e pa(B) ) ⇐⇒ x B = f B (x pa(B) , e pa(B) ) .</formula><p>Now let e ∈ Ē and let x A∪pa(B)\B ∈ X A∪pa(B)\B . Then</p><formula xml:id="formula_122">x A = (g B ) A (x pa(B)\B , e pa(B) ) =⇒ x A = (g B ) A (x pa(B)\B , e pa(B) ) ∃x B\A ∈ X B\A : x B\A = (g B ) B\A (x pa(B)\B , e pa(B) ) =⇒ ∃x B\A ∈ X B\A : x B = g B (x pa(B)\B , e pa(B) ) =⇒ ∃x B\A ∈ X B\A : x B = f B (x pa(B) , e pa(B) ) =⇒ ∃x B\A ∈ X B\A : x A = f A (x pa(A) , e pa(A) ) =⇒ x A = f A (x pa(A) , e pa(A) ) =⇒ x A = g A (x pa(A)\A , e pa(A) ) ,</formula><p>where the exists-quantifier could be omitted because the expression it binds to does not depend on x B\A (from the assumptions it follows that (A ∪ pa(A)) ∩ (B \ A) = ∅). Hence, for all e ∈ Ē and all x A∪pa(B)\B ∈ X A∪pa(B)\B</p><formula xml:id="formula_123">x A = (g B ) A (x pa(B)\B , e pa(B) ) =⇒ x A = g A (x pa(A)\A , e pa(A) ) .</formula><p>Hence, for all e ∈ Ē and all x A∪pa(B)\B ∈ X A∪pa(B)\B (g B ) A (x pa(B)\B , e pa(B) ) = g A (x pa(A)\A , e pa(A) ) .</p><p>Since this expression does not depend on x (B\A)∪I\(B∪pa(B)) , from Lemma F.11.(2) we conclude that for all e ∈ Ē and all x ∈ X (g B ) A (x pa(B)\B , e pa(B) ) = g A (x pa(A)\A , e pa(A) ) .</p><p>LEMMA E.2. An SCM M is observationally equivalent to M twin w.r.t. O ⊆ I.</p><p>PROOF. Let (X, E) be a solution of M, then ((X, X), E) is a solution of M twin . Conversely, let ((X, X ), E) be a solution of M twin , then (X, E) is a solution of M. PROOF OF <ref type="bibr">PROPOSITION 5.4</ref>. From unique solvability of M w.r.t. L 1 it follows that there exists a mapping g L1 : X pa(L1)\(L1) × E pa(L1) → X L1 such that for P E -almost every e ∈ E and for all x ∈ X x L1 = g L1 (x pa(L1)\L1 , e pa(L1) ) ⇐⇒ x L1 = f L1 (x, e) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Let pa denotes the parents in</head><formula xml:id="formula_124">G a (M marg(L1) ). Note that pa(L 2 ) \ L 2 ⊆ pa(L 1 ∪ L 2 ) \ (L 1 ∪ L 2 )</formula><p>. Let f denote the marginal causal mechanism of a structurally minimal SCM that is equivalent to the marginalization M marg(L1) constructed from g L1 (see <ref type="bibr">Proposition 2.11)</ref>.</p><p>=⇒ : If M marg(L1) is uniquely solvable w.r.t. L 2 , then there exists a mapping gL2 : X pa(L2)\L2 ×E pa(L2) → X L2 such that for P E -almost every e ∈ E and for all x I\L1 ∈ X I\L1</p><p>x L2 = gL2 (x pa(L2)\L2 , e pa(L2) ) ⇐⇒ x L2 = f L2 (g L1 (x pa(L1)\L1 , e pa(L1) ), x I\L1 , e) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Define the mapping</head><formula xml:id="formula_125">h : X pa(L1∪L2)\(L1∪L2) × E pa(L1∪L2) → X L1∪L2 by (h L1 , h L2 )(x pa(L1∪L2)\(L1∪L2) , e pa(L1∪L2) ) := g L1 (g L2 ) pa(L1) (x pa(L2)\L2</formula><p>, e pa(L2) ), x pa(L1)\(L1∪L2) , e pa(L1) , gL2 (x pa(L2)\L2 , e pa(L2) ) .</p><p>Then for P E -almost every e ∈ E and for all x ∈ X</p><formula xml:id="formula_126">x L1 = f L1 (x, e) x L2 = f L2 (x, e) ⇐⇒ x L1 = g L1 (x pa(L1)\L1 , e pa(L1) ) x L2 = f L2 (x, e) ⇐⇒ x L1 = g L1 (x pa(L1)\L1 , e pa(L1) ) x L2 = f L2 (g L1 (x pa(L1)\L1 , e pa(L1) ), x I\L1 , e) ⇐⇒ x L1 = g L1 (x pa(L1)\L1 , e pa(L1) ) x L2 = gL2 (x pa(L2)\L2 , e pa(L2) ) ⇐⇒ x L1 = g L1 (g L2 ) pa(L1) (x pa(L2)\L2 , e pa(L2) ), x pa(L1)\(L1∪L2) , e pa(L1) x L2 = gL2 (x pa(L2)\L2 , e pa(L2) ) ⇐⇒ x L1 = h L1 (x pa(L1∪L2)\(L1∪L2) , e pa(L1∪L2) ) x L2 = h L2 (x pa(L1∪L2)\(L1∪L2) , e pa(L1∪L2) ) ,</formula><p>where in the first equivalence we used unique solvability w.r.t. L 1 of M, in the second we used substitution, in the third we used unique solvability w.r.t. L 2 of M marg(L1) , in the fourth we used again substitution and in the last equivalence we used the definition of h. From this we conclude that M is uniquely solvable w.r.t. L 1 ∪ L 2 . Hence, by definition it follows that marg</p><formula xml:id="formula_127">(L 2 ) • marg(L 1 )(M) = marg(L 1 ∪ L 2 )(M).</formula><p>⇐= : If M is uniquely solvable w.r.t. L 1 ∪ L 2 , then there exists a mapping h : X pa(L1∪L2)\(L1∪L2) × E L1∪L2 → X L1∪L2 such that for P E -almost every e ∈ E for all x ∈ X x L1∪L2 = h(x pa(L1∪L2)\(L1∪L2) , e pa(L1∪L2) ) ⇐⇒ x L1∪L2 = f L1∪L2 (x, e) .</p><p>Then, for P E -almost every e ∈ E for all x ∈ X x L1 = h L1 (x pa(L1∪L2)\(L1∪L2) , e pa(L1∪L2) )</p><formula xml:id="formula_128">x L2 = h L2 (x pa(L1∪L2)\(L1∪L2) , e pa(L1∪L2) ) ⇐⇒ x L1 = f L1 (x, e) x L2 = f L2 (x, e) ⇐⇒ x L1 = g L1 (x pa(L1)\L1 , e pa(L1) ) x L2 = f L2 (x, e) ⇐⇒ x L1 = g L1 (x pa(L1)\L1 , e pa(L1) ) x L2 = f L2 (g L1 (x pa(L1)\L1 , e pa(L1) ), x I\L1 , e) ⇐⇒ x L1 = g L1 (x pa(L1)\L1 , e pa(L1) )</formula><p>x L2 = fL2 (x pa(L2) , e pa(L2) ) .</p><p>This gives for P E -almost every e ∈ E for all x I\L1 ∈ X I\L1</p><p>x L2 = h L2 (x pa(L1∪L2)\(L1∪L2) , e pa(L1∪L2) )</p><p>⇐⇒ x L2 = fL2 (x pa(L2) , e pa(L2) ) .</p><p>Now apply Lemma E.3 to conclude that M marg(L1) is uniquely solvable w.r.t. L 2 .</p><p>PROOF OF PROPOSITION 5.5. The commutation relation with the perfect intervention follows straightforwardly from the definitions of perfect intervention and marginalization and the fact that if M is uniquely solvable w.r.t. L, then M do(I,ξI ) is also uniquely solvable w.r.t. L, since the structural equations for the variables L are the same for M and M do(I,ξI ) .</p><p>The commutation relation with the twin operation follows straightforwardly from the definition of the twin operation and marginalization and the fact that if M is uniquely solvable w.r.t. L, then twin(M) is uniquely solvable w.r.t. L ∪ L , where L is the copy of L in I . </p><formula xml:id="formula_129">x L = f L (x, e) x O = f O (x, e) ⇐⇒ x L = g L (x pa(L)\L , e pa(L) ) x O = f O (g L (x pa(L)\L , e pa(L) ), x O , e) ⇐⇒ x L = g L (x pa(L)\L , e pa(L) ) x O = f (x O , e) ,</formula><p>where f is the marginal causal mechanism of M marg(L) constructed from a measurable solution function g PROOF OF PROPOSITION 5.8. A similar proof as for Theorem 1 in <ref type="bibr" target="#b14">[15]</ref> works. PROOF OF PROPOSITION 5.9. First we prove the commutation relation of the perfect intervention. Observe that applying the do(I) operation to the latent projection marg(L)(G) removes all the incoming edges on the nodes I. Such an incoming edge at a node in I in marg(L)(G) corresponds to a path in G that points to that node. But since do(I)(G) is just G with all the incoming edges on I removed, the graph (marg(L) • do(I))(G) also has all the incoming edges on the nodes I removed.</p><formula xml:id="formula_130">L : X pa(L)\L × E pa(L) → X L for M w.r.t. L. Hence, a solution (X, E) of M satisfies X O = f (X O , E) a.s.. Conversely, if ( XO , E)</formula><p>Next, we will prove the commutation relation of the twin operation. We will denote the copy in I of any node i ∈ I by i , that is, I = {i : i ∈ I}. The edges in (twin(I \ L) • marg(L))(G) can be partitioned into three cases:</p><formula xml:id="formula_131">     v → w v ∈ J ∪ I \ L, w ∈ J ∪ I \ L, v → w ∈ marg(L)(G) , v → w v ∈ J , w ∈ I \ L, v → w ∈ marg(L)(G) , v → w v ∈ I \ L, w ∈ I \ L, v → w ∈ marg(L)(G) ,</formula><p>where J := V \ I.</p><p>Note that in twin(I)(G), there are no directed edges of the form v → w by definition. Therefore, the edges in (marg(L ∪ L ) • twin(I))(G) can be partitioned into three cases:</p><formula xml:id="formula_132">     v → w v ∈ J ∪ I \ L, w ∈ J ∪ I \ L, v → 1 → • • • → n → w ∈ twin(I)(G) , v → w v ∈ J , w ∈ I \ L, v → 1 → • • • → n → w ∈ twin(I)(G) , v → w v ∈ I \ L, w ∈ I \ L, v → 1 → • • • → n → w ∈ twin(I)(G) ,</formula><p>where all 1 , . . . , n ∈ L and 1 , . . . , n ∈ L . Thus, the non-endpoint nodes on the directed paths in twin(I)(G) must either all lie in L or in L . With the definition of twin(I)(G) we can rewrite this as follows:</p><formula xml:id="formula_133">     v → w v ∈ J ∪ I \ L, w ∈ J ∪ I \ L, v → 1 → • • • → n → w ∈ G , v → w v ∈ J , w ∈ I \ L, v → 1 → • • • → n → w ∈ G , v → w v ∈ I \ L, w ∈ I \ L, v → 1 → • • • → n → w ∈ G ,</formula><p>where all intermediate 1 , . . . , n must lie in L. This corresponds exactly with the edges in (twin(I \ L) • marg(L))(G).</p><p>PROOF OF <ref type="bibr">PROPOSITION 5.11</ref>. Without loss of generality, we assume that M is structurally minimal (see <ref type="bibr">Proposition 2.11)</ref>. Let g L be a measurable solution function for M w.r.t. L and denote by M marg(L) the marginal SCM constructed from g L . For j ∈ I \ L, define A j := an G(M)L (pa(j) ∩ L) ⊆ L and let gAj be a measurable solution function for M w.r.t. A j . Because A j ⊆ L and pa(A j ) \ A j ⊆ pa(L) \ L, by Lemma E.1, for P E -almost every e ∈ E and for all x ∈ X (g L ) Aj (x pa(L)\L , e pa(L) ) = gAj (x pa(Aj)\Aj , e pa(Aj) ) .</p><p>Therefore, the component fj of the marginal causal mechanism f of M marg(L) satisfies for P E -almost every e ∈ E and for all x ∈ X fj (x I\L , e) := f j (g L ) pa(j) (x pa(L)\L , e pa(L) ), x pa(j)\L , e pa(j)</p><p>= f j (g Aj ) pa(j)∩L (x pa(Aj)\Aj , e pa(Aj) ), x pa(j)\L , e pa(j) .</p><p>Hence, the endogenous parents of j in M marg(L) are a subset of (pa(A j ) \ A j ) ∪ (pa(j) \ L) ∩ I and the exogenous parents of j in M marg(L) are a subset of (pa(A j ) ∪ pa(j)) ∩ J . Hence, all parents of j in M marg(L) are a subset of those k ∈ (I \L)∪J such that there exists a path k → 1 → • • • → n → j ∈ G a (M) for n ≥ 0 and 1 , . . . , n ∈ L. Therefore, the augmented graph G a marg(L)(M) is a subgraph of the latent projection marg(L) G a (M) . and we conclude that also the graph G marg(L)(M) is a subgraph of the latent projection marg(L) G(M) .</p><p>Section 6</p><p>PROOF OF THEOREM 6.3. This follows directly from Theorems A.7 and A.21.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Section 7</head><p>PROOF OF PROPOSITION 7.1. We define M := M do(I,ξI ) , pa := pa G a ( M) and A := an G( M)\i (j). Suppose that i → j / ∈ marg(I \ O)(G(M)) and assume that the two induced distributions do not coincide. Because i → j / ∈ marg(I \ O)(G(M)) it follows that ( pa(A) \ A) ∩ I = ∅. Let now gA : E pa(A) → X A be a measurable solution function for M w.r.t. A, that is, we have for P E -almost every e ∈ E and for all x ∈ X x A = fA (x, e) ⇐⇒ x A = gA (e pa(A) ) , where f is the ausal mechanism of M. Because i / ∈ A and j ∈ A, it follows that for the intervened model (M do(I,ξI ) ) do({i},ξi) the marginal solution X j is also a marginal solution of (M do(I,ξI ) ) do({i}, ξi) and vice versa, which is in contradiction with the assumption. PROOF OF PROPOSITION 7.2. Let's define M := M do(I,ξI ) , pa := pa G a ( M) , A i := an G( M) (i) and A \i j := an G( M)\i (j). Suppose that there does not exist a bidirected edge i ↔ j in the latent projection marg(I \ O)(G(M)). Because i ↔ j / ∈ marg(I \ O)(G( M)), where here M is the intervened model M do(I,ξI ) , we have that an G a ( M)\j (i) ∩ an G a ( M)\i (j) ∩ J = ∅. From j / ∈ an G( M) (i) it follows that an G( M)\j (i) = an G( M) (i), and hence an G a ( M) (i) ∩ an G a ( M)\i (j) ∩ J = ∅. Observe that pa(A i ) ⊆ an G a ( M) (i) and pa(A \i j ) ⊆ an G a ( M)\i (j) ∪ {i}, and thus pa(A i ) ∩ pa(A \i j ) ∩ J = ∅. Let g Ai : E pa(Ai) → X Ai be a measurable solution function for M w.r.t. A i , that is, we have for P E -almost every e ∈ E and for all x ∈ X x Ai = fAi (x, e) ⇐⇒ x Ai = g Ai (e pa(Ai) ) , where f is the intervened causal mechanism of M. Because pa(A i ) ∩ pa(A \i j ) ∩ J = ∅ and i ∈ A i , we have that X i ⊥ ⊥ E pa(A \i j ) for every solution (X, E) of M. Assume for the moment that i ∈ pa(A PROOF OF COROLLARY 8.3. This follows from Corollary A.22.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX F: MEASURABLE SELECTION THEOREMS</head><p>In this appendix, we derive some lemmas and state two measurable selection theorems that are used in several proofs in Appendix E. First, we introduce the measure theoretic notation and terminology needed to understand the results (see <ref type="bibr" target="#b29">[30]</ref> for more details). DEFINITION F.1 (Standard measurable space). A measurable space (X , Σ) is a standard measurable space if it is isomorphic to (Y, B(Y)), where Y is a Polish space, that is, a separable completely metrizable space, 22 and B(Y) are the Borel subsets of Y, that is, the σalgebra generated by the open sets in Y. A measure space (X , Σ, µ) is a standard probability space if (X , Σ) is a standard measurable space and µ is a probability measure.</p><p>Examples of standard measurable spaces are the open and closed subsets of R d , and the finite sets with the usual complete metric. If we say that X is a standard measurable space, then we implicitly assume that there exists a σ-algebra Σ such that (X , Σ) is a standard measurable space. Similarly, if we say that X is a standard probability space with probability measure P X , then we implicitly assume that there exists a σ-algebra Σ such that (X , Σ, P X ) is a standard probability space. DEFINITION F.2 (Analytic set). Let X be a Polish space. A set A ⊆ X is called analytic if there exist a Polish space Y and a continuous mapping f : Y → X with f (Y) = A.</p><p>LEMMA F.3. Let X and Y be standard measurable spaces and f : X → Y a measurable mapping. Then 1. every measurable set A ⊆ X is analytic; 2. if the subsets A ⊆ X and Ã ⊆ Y are analytic, then the sets f (A) and f -1 ( Ã) are analytic.</p><p>PROOF. From Proposition 13.7 in <ref type="bibr" target="#b29">[30]</ref> it follows that every measurable set A ⊆ X is analytic. From Proposition 14.4.(ii) in <ref type="bibr" target="#b29">[30]</ref> it follows that the image and the preimage of an analytic set is an analytic set. DEFINITION F.4 (µ-measurability). Let (X , Σ, µ) be a measure space. A set E ⊆ X is called a µ-null set if there exists a A ∈ Σ with E ⊆ A and µ(A) = 0. We denote the class of µ-null sets by N , and we denote the σ-algebra generated by Σ ∪ N by Σ, and its members are called the µ-measurable sets. Note that each member of Σ is of the form A ∪ E with A ∈ Σ and E ∈ N . The measure µ is extended to a measure μ on Σ, by μ(A ∪ E) = µ(A) for every A ∈ Σ and E ∈ N , and is called its completion. A mapping f : X → Y between measurable spaces is called µ-measurable if the inverse image f -1 (C) of every measurable set C ⊆ Y is µ-measurable. 22 A metrizable space is a topological space X for which there exists a metric d such that (X , d) is a metric space and induces the topology on X . For a metric space (X , d), a Cauchy sequence is a sequence (xn) n∈N of elements of X such that for every &gt; 0 there exists an N ∈ N such that for all natural numbers p, q &gt; N we have d(xn, xm) &lt; . We call (X , d) complete if every Cauchy sequence has a limit in X . A completely metrizable space is a topological space X for which there exists a metric d such that (X , d) is a complete metric space that induces the topology on X . A topological space X is called separable if it contains a countable dense subset, that is, there exists a sequence (xn) n∈N of elements in X such that every nonempty open subset of X contains at least one element of the sequence. A separable completely metrizable space is called a Polish space (see <ref type="bibr" target="#b8">[9]</ref> and <ref type="bibr" target="#b29">[30]</ref> for more details). DEFINITION F.5 (Universal measurability). Let (X , Σ) be a standard measurable space. A set A ⊆ X is called universally measurable if it is µ-measurable for every σ-finite measure 23 µ on X (i.e., in particular every probability measure). A mapping f : X → Y between standard measurable spaces is universally measurable if it is µ-measurable for every σ-finite measure µ.</p><p>LEMMA F.6. Let E be a standard probability space with probability measure P E and A ⊆ E an analytic set. Then A is P E -measurable and there exist measurable sets S, T ⊆ E such that S ⊆ A ⊆ T and P E (S) = PE (A) = P E (T ), where PE is the completion of P E .</p><p>PROOF. Let A ⊆ E be an analytic set. Since every analytic set in a standard measurable space is a universally measurable set (see Theorem 21.10 in <ref type="bibr" target="#b29">[30]</ref>), we know that A is a universally measurable set, and hence it is in particular a P E -measurable set. Thus, there exist a measurable set S ⊆ E and a P E -null set C ⊆ E such that A = S ∪ C and PE (A) = P E (S), where PE is the completion of P E . Moreover, there exists a measurable set C ⊆ E such that C ⊆ C and P E ( C) = 0. Let T := S ∪ C, then A ⊆ T and P E (T ) = P E (S).</p><p>LEMMA F.7. Let f : X → Y be a µ-measurable mapping. If Y is countably generated, then there exists a measurable mapping g : X → Y such that f (x) = g(x) holds µ-a.e.. where for y 0 we can take an arbitrary point in Y. This mapping g is measurable since for each generator C n we have</p><formula xml:id="formula_134">g -1 (C n ) = Ân if y 0 / ∈ C n , Ân ∪ B otherwise.</formula><p>is in Σ. Moreover, f (x) = g(x) µ-almost everywhere.</p><p>With this result at hand we can now prove the first measurable selection theorem. THEOREM F.8 (Measurable selection theorem). Let E be a standard probability space with probability measure P E , X a standard measurable space and S ⊆ E × X a measurable set such that E \ pr E (S) is a P E -null set, where pr E : E × X → E is the projection mapping on E. Then there exists a measurable mapping g : E → X such that (e, g(e)) ∈ S for P Ealmost every e ∈ E.</p><p>PROOF. Take the subset Ê := E \ B, for some measurable set B ⊇ E \ pr E (S) and P E (B) = 0, and note that Ê is a standard measurable space (see Corollary 13.4 in <ref type="bibr" target="#b29">[30]</ref>) and 23 A measure µ on a measurable space (X , Σ) is called σ-finite if X = ∪ n∈N An, with An ∈ Σ, µ(An) &lt; ∞.</p><p>Ê ⊆ pr E (S). Let Ŝ = S ∩ ( Ê × X ). Because the set Ŝ is measurable, it is in particular analytic (see Lemma F.3). It follows by the Jankov-von Neumann Theorem (see Theorem 18.8 or 29.9 in <ref type="bibr" target="#b29">[30]</ref>) that Ŝ has a universally measurable uniformizing function, that is, there exists a universally measurable mapping ĝ : Ê → X such that for all e ∈ Ê, (e, ĝ(e)) ∈ Ŝ. Hence, in particular, it is P E Ê -measurable, where P E Ê is the restriction of P E to Ê. Now define the mapping g * : E → X by g * (e) := ĝ(e) if e ∈ Ê</p><p>x 0 otherwise, where for x 0 we can take an arbitrary point in X . Then this mapping g * is P E -measurable.</p><p>To see this, take any measurable set C ⊆ X , then</p><formula xml:id="formula_135">g * -1 (C) = ĝ-1 (C) if x 0 / ∈ C ĝ-1 (C) ∪ B otherwise.</formula><p>Because ĝ-1 (C) is P E Ê -measurable it is also P E -measurable and thus g * -1 (C) is P Emeasurable.</p><p>By Lemma F.7 and the fact that standard measurable spaces are countably generated (see Proposition 12.1 in <ref type="bibr" target="#b29">[30]</ref>), we prove the existence of a measurable mapping g : E → X such that g * = g P E -a.e. and thus it satisfies (e, g(e)) ∈ S for P E -almost every e ∈ E.</p><p>This theorem rests on the assumption that the standard measurable space E has a probability measure P E . If this space becomes the product space Y ×E, for some standard measurable space Y where only the space E has a probability measure, then in general this theorem does not hold anymore. However, if we assume in addition that the fibers of S in Y are σ-compact for P E -almost every e ∈ E and for all x ∈ X , then we can prove a second measurable selection theorem. A topological space is σ-compact if it is the union of countably many compact subspaces. For example, all countable discrete spaces, every interval of the real line, and moreover all the Euclidean spaces are σ-compact spaces. THEOREM F.9 (Second measurable selection theorem). Let E be a standard probability space with probability measure P E , X and Y standard measurable spaces and S ⊆ X × E × Y a measurable set such that E \ K σ is a P E -null set, where K σ := {e ∈ E : ∀x ∈ X (S (x,e) is nonempty and σ-compact)} , with S (x,e) denoting the fiber over (x, e), that is S (x,e) := {y ∈ Y : (x, e, y) ∈ S} .</p><p>Then there exists a measurable mapping g : X × E → Y such that for P E -almost every e ∈ E and for all x ∈ X we have (x, e, g(x, e)) ∈ S.</p><p>PROOF. Take the subset Ê := E \ B, for some measurable set B ⊇ E \ K σ and P E (B) = 0. Note that Ê is a standard measurable space, Ê ⊆ K σ and Ŝ = S ∩(X × Ê ×Y) is measurable. By assumption, for each (x, e) ∈ X × Ê the fiber Ŝ(x,e) is nonempty and σ-compact and hence by applying the Theorem of Arsenin-Kunugui (see <ref type="bibr">Theorem 35.46 in [30]</ref>) it follows that the set Ŝ has a measurable uniformizing function, that is, there exists a measurable mapping ĝ : X × Ê → Y such that for all (x, e) ∈ X × Ê, (x, e, ĝ(x, e)) ∈ Ŝ. Now define the mapping g : X × E → Y by g(x, e) := ĝ(x, e) if e ∈ Ê y 0 otherwise, where for y 0 we can take an arbitrary point in Y. This mapping g inherits the measurability from ĝ and it satisfies for P E -almost every e ∈ E and for all x ∈ X that (x, e, g(x, e)) ∈ S.</p><p>The next two lemmas provide some useful properties for the "for P E -almost every e ∈ E" quantifier.</p><p>LEMMA F.10. Let φ : E → Ẽ be a measurable map between two standard measurable spaces. Let P E be a probability measure on E and let P Ẽ = P E • φ -1 be its push-forward under φ. Let P : Ẽ → {0, 1} be a property, that is, a (measurable) boolean-valued function on Ẽ. Then the property P = P • φ on E holds P E -a.e. if and only if the property P holds P Ẽ -a.e.. PROOF. Assume the property P = P • φ holds P E -a.e., then C = {e ∈ E : P (e) = 1} contains a measurable set C * with P E -measure 1, that is, C * ⊆ C and P E (C * ) = 1. By Lemma F.3, φ(C * ) is analytic. By Lemma F.6, there exist measurable sets A, B such that A ⊆ φ(C * ) ⊆ B and P Ẽ (A) = P Ẽ (B). Because φ is measurable, φ -1 (A) and φ -1 (B) are both measurable. Also, φ -1 (A) ⊆ φ -1 (φ(C * )) ⊆ φ -1 (B). As C * ⊆ φ LEMMA F.11 (Some properties for the for-almost-every quantifier). Let X = X × X and E = E × Ẽ be products of nonempty standard measurable spaces and P E = P E × P Ẽ be the product measure of probability measures P E and P Ẽ on E and Ẽ, respectively. Denote by "∨ ∼ e" the quantifier "for P E -almost every e ∈ E" and by "∀x" the quantifier "for all x ∈ X ", and similarly for their components, for example, "∨ ∼ e" for "for P E -almost every e ∈ E" and "∀x" for "for all x ∈ X ". Then we have the following properties: where P denotes a property, that is, a measurable boolean-valued function, on the corresponding measurable spaces and we write e and x for (e, ẽ) and (x, x), respectively.</p><p>PROOF. We only prove the statements that may not be immediately obvious. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>2. 1 .</head><label>1</label><figDesc>Structural causal models and their solutions DEFINITION 2.1 (Structural causal model). A structural causal model (SCM) is a tuple 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>) )Fig 2 :</head><label>2</label><figDesc>Fig 2: The augmented graph (left) and the graph (center) of the SCM M of Example 2.8 and the graph of the intervened SCM M do({3},1) of Example 2.16 (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>1 .</head><label>1</label><figDesc>the SCM M: for a subset I ⊆ I and value ξ I ∈ X I , (do(I ∪ I , ξ I∪I )) • twin)(M) = (twin • do(I, ξ I ))(M), and 2. the directed graph G: for subsets I ⊆ I ⊆ V such that J := V \ I is exogenous, (do(I ∪ I ) • twin(I))(G) = (twin(I) • do(I))(G),</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>3. 1 .</head><label>1</label><figDesc>Definition of solvability Intuitively, one can think of the structural equations corresponding to a subset of endogenous variables O ⊆ I as a description of how the subsystem formed by the variables O interacts with the rest of the system I \ O through the variables pa(O) \ O. A solution function w.r.t. O assigns each input value (x pa(O)\O , e pa(O) ) of this subsystem to a specific output value x O of the subsystem. This is formalized as follows.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig 3 :</head><label>3</label><figDesc>Fig 3: Left: The graphs of the observationally equivalent SCMs M and M of Examples 3.5 and 4.2, respectively. Right: The graphs of the interventionally equivalent SCMs M and M of Example 4.4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig 4 :</head><label>4</label><figDesc>Fig 4: The graphs of the SCM M (left) of Example 3.11 and the marginal SCM M marg({2,3}) (right) of Example 5.10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>3. 5 .</head><label>5</label><figDesc>Ancestral (unique) solvability We saw that, in general, solvability w.r.t. O ⊆ I does not imply solvability w.r.t. a strict subset of O. Here we show that it does imply solvability w.r.t. the ancestral subsets in G(M) O , that is, in the induced subgraph of the graph G</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>EXAMPLE 4 . 2 (</head><label>42</label><figDesc>Observational equivalence does not imply equivalence). Consider the SCM M that is the same as M of Example 3.5 but with the causal mechanism f given by f1 (x, e) := e 1 , f2 (x, e) := e 2 , f3 (x, e) := x1e4+e3 1-x1x2 , f4 (x, e) := x2e3+e4 1-x1x2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>4. 2 .</head><label>2</label><figDesc>Interventional equivalence We consider two SCMs to be interventionally equivalent if they induce the same interventional distributions under all perfect interventions. DEFINITION 4.3 (Interventional equivalence). Two SCMs M = I, J , X , E, f , P E and M = Ĩ, J , X , Ẽ, f , P Ẽ are interventionally equivalent w.r.t. O ⊆ I ∩ Ĩ, denoted by M ≡ int(O) M, if X O = X O and for every I ⊆ O and every value ξ I ∈ X I their intervened models M do(I,ξI ) and Mdo(I,ξI) are observationally equivalent with respect to O. M and M are called interventionally equivalent if they are interventionally equivalent w.r.t. I = Ĩ. Equivalent SCMs have the same solutions under every perfect intervention, and hence they are interventionally equivalent w.r.t. every subset O ⊆ I. SCMs that are interventionally equivalent w.r.t. a subset O ⊆ I are interventionally equivalent w.r.t. every strict subset W O. But in general, they are not interventionally equivalent w.r.t. a strict superset O V ⊆ I, as can be seen in Example 4.2, where the SCMs M and M are interventionally equivalent w.r.t. {1, 2} but are not interventionally equivalent. Interventional equivalence w.r.t. O ⊆ I implies observational equivalence w.r.t. O, since the empty perfect intervention (I = ∅) is a special case of a perfect intervention. However, observational equivalence w.r.t. O ⊆ I does not imply interventional equivalence w.r.t. O in general, as can be seen in Example 4.2, where the SCMs M and M are observationally equivalent but not interventionally equivalent.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>DEFINITION 4 . 5 (</head><label>45</label><figDesc>Counterfactual equivalence). Two SCMs M = I, J , X , E, f , P E and M = Ĩ, J , X , Ẽ, f , P Ẽ are counterfactually equivalent with respect to O ⊆ I ∩ Ĩ, denoted by M ≡ cf(O) M, if M twin and Mtwin are interventionally equivalent with respect to O ∪ O , where O corresponds to the copy of O in I ∩ Ĩ . M and M are called counterfactually equivalent if they are counterfactually equivalent with respect to I = Ĩ. The notion of counterfactual equivalence is coarser than equivalence and finer than interventional equivalence. PROPOSITION 4.6. For SCMs, we have that equivalence implies counterfactual equivalence w.r.t. O, which in turn implies interventional equivalence w.r.t. O, for any O ⊆ I.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>t. O ⊆ I ∩ Ĩ are related in the following way (see Proposition 4.6): M and M are equivalent =⇒ M and M are counterfactually equivalent w.r.t. O =⇒ M and M are interventionally equivalent w.r.t. O =⇒ M and M are observationally equivalent w.r.t. O .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>EXAMPLE 5 . 10 (</head><label>510</label><figDesc>Marginalization does not respect the latent projection). Consider the SCM M of Example 3.11. Although M and its marginalization M marg(L) with L = {2, 3} are interventionally equivalent w.r.t. I \ L = {1, 4}, the graph G(M marg(L) ) is not a subgraph of the latent projection of G(M) onto I \ L, as can be verified from the graphs depicted in Figure 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig 5 :</head><label>5</label><figDesc>Fig 5: The graphs of the observationally equivalent SCMs M (left) and M (right) of Example A.8 and A.10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>x1e4+e3 1 -</head><label>1</label><figDesc>x1x2 , f4 (x, e) := x2e3+e4 1-x1x2 . By construction, M and M are observationally equivalent. Because M is acyclic (see Figure 5 on the right) we can apply the directed global Markov property to M. The fact that X 1 and X 2 are not d-separated given {X 3 , X 4 } in G( M) is in line with X 1 being dependent of X 2 given {X 3 , X 4 } for every solution X of M (and hence of M).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig 6 :</head><label>6</label><figDesc>Fig 6: The graphs of the original SCM M (left), of the acyclified SCM (center), and of the acyclification of the graph of M (right) corresponding to Example A.15.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head></head><label></label><figDesc>Fig 7:Overview of causal graphical models. The "gray" and "dark gray" areas contain all the causal graphical models that can be modeled by an SCM and an acyclic SCM, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>B. 2 .</head><label>2</label><figDesc>(Unique) solvability w.r.t. strict super-and subsets In general, (unique) solvability w.r.t. O ⊆ I does not imply (unique) solvability w.r.t. a strict superset O V ⊆ I nor w.r.t. a strict subset W O, as can be seen in the following example.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>PROPOSITION B. 4 (</head><label>4</label><figDesc>Combining measurable solution functions on different sets). Let M = I, J , X , E, f , P E be an SCM, O ⊆ I a subset and A, Ã ⊆ O two ancestral subsets in G(M) O . If M is uniquely solvable w.r.t. A, Ã and A ∩ Ã, then M is uniquely solvable w.r.t. A ∪ Ã.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>PROPOSITION C. 2 (</head><label>2</label><figDesc>Sufficient and necessary condition for solvability w.r.t. a subset for linear SCMs). Let M be a linear SCM and L ⊆ I and O = I \ L. Then M is solvable w.r.t. L if and only if for the matrix A LL = I L -B LL , for P E -almost every e ∈ E and for all x O ∈ X O the identity</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>PROPOSITION C. 3 (</head><label>3</label><figDesc>Sufficient and necessary condition for unique solvability w.r.t. a subset for linear SCMs). Let M be a linear SCM, L ⊆ I and O = I \ L. Then M is uniquely solvable w.r.t. L if and only if the matrix</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head>Fig 8 :</head><label>8</label><figDesc>Fig 8: Damped coupled harmonic oscillator (top) and the graph of the SCM M that describes the positions of the masses at equilibrium (bottom) of Example D.1 for d = 5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head>Fig 9 :</head><label>9</label><figDesc>Fig 9: The augmented graph of the SCM M (left), its twin SCM M twin (center) and the intervened twin SCM (M twin ) do({S,S },(s,s )) (right) of Examples D.2 and D.3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>2 EXAMPLE D. 4 (</head><label>24</label><figDesc>In this subsection, we provide additional examples that support the main text.Section Structural equations up to almost sure equality). Consider the SCM M</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>Fig 11 :</head><label>11</label><figDesc>Fig 11: The augmented graph of the SCM Mρ (left), its twin SCM M twin ρ (center) and the intervened twin SCM (M twin ρ ) do({1 ,1},(1,0)) (right) of Example D.7.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>Fig 12 :</head><label>12</label><figDesc>Fig 12: The augmented graphs of SCMs M, M, M, and M that appear in Examples 4.4, D.10, and D.13.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head></head><label></label><figDesc>for A, B, C ⊆ I and X a solution of M. PROOF OF COROLLARY A.<ref type="bibr" target="#b21">22</ref>. First observe that simplicity is preserved under both perfect intervention and the twin operation (see Proposition 8.2). Now the first statement follows from Theorem A.21 if one takes into account the identities of Proposition 2.14 and 2.19. Similarly, the last statement follows from Theorem A.7. PROOF OF PROPOSITION A.32. Let M =: V, Ĥ, X , E, f , P E be the induced SCM. Observe that every loop O ∈ L(G( M)) is a loop in L(G). Fix x ∈ X and ě ∈ E. For every O ∈ L(G( M)), defineI O := (pa G (O) \ O) \ (pa(O) \ O) ⊆ Ĩ and J O := {F ∈ J : F ∩ O = ∅} \ pa(O) ⊆ J .Now, define the family of measurable mappings(g O ) O∈L(G( M)), where the mapping gO :X pa(O)\O × E pa(O) → X O isgiven by gO (x pa(O)\O , e pa(O) ) := g O (x pa(O)\O , xIO , e pa(O) , ěJO ) where x pa G (O)\O = (x pa(O)\O , xIO ) and e O = (e pa(O) , ěJO ). Observe that from the definition of the parents (see Definition 2.6) it follows that for P E -almost every e ∈ E and for all x ∈ X we have x O = fO (x \IO , xIO , e \JO , ěJO ) ⇐⇒ x O = fO (x, e) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_26"><head></head><label></label><figDesc>g O : X pa(O)\O × E pa(O) → X O such that for P Epa(O) -almost every e pa(O) ∈ E pa(O) and for all x pa(O)\O ∈ X pa(O)\O g O (x pa(O)\O , e pa(O) ) = fO x pa(O)\O , g O (x pa(O)\O , e pa(O) ), e pa(O) .Once more applying Lemma F.10, we obtain that for P E -almost every e ∈ E and for all x ∈ Xx O = g O (x pa(O)\O , e pa(O) ) =⇒ x O = f O (x, e).Hence M is solvable w.r.t. O. PROOF OF PROPOSITION B.4. Without loss of generality, we assume that M is structurally minimal (see Proposition 2.11). Define C := A ∩ Ã and D := A ∪ Ã. Let g A , g Ã be measurable solution functions for M w.r.t. A and Ã, respectively. Note that pa(C) \ C ⊆ pa(A) \ A and similarly pa(C) \ C ⊆ pa( Ã) \ Ã. Indeed, for c ∈ pa(C): if c ∈ O then c ∈ C because A and Ã are both ancestral in G(M) O , while if c / ∈ O then c / ∈ A and c / ∈ Ã. Hence by Lemma E.1, for P E -almost all e ∈ E and for all x ∈ X (g A ) C (x pa(A)\A , e pa(A) ) = (g Ã) C (x pa( Ã)\ Ã, e pa( Ã) ) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_27"><head></head><label></label><figDesc>Now pa(A) \ A ⊆ pa(D) \ D, and similarly, pa( Ã) \ Ã ⊆ pa(D) \ D. Hence, we conclude that the mapping h D :X pa(D)\D × E pa(D) → X D defined by h D (x pa(D)\D , e pa(D) ) := (g A ) A\C (x pa(A)\A , e pa(A) ), (g A ) C (x pa(A)\A , e pa(A) ), (g Ã) Ã\C (x pa( Ã)\ Ã, e pa( Ã) )is a measurable solution function for M w.r.t. D, and that M is uniquely solvable w.r.t. D.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_28"><head>Section 3 PROOF</head><label>3</label><figDesc>OF THEOREM 3.2. First we define the solution space S(M) of M by S(M) := {(e, x) ∈ E × X : x = f (x, e)} .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_29"><head>PROOF OF PROPOSITION 4 . 6 . 5 LEMMA E. 3 .</head><label>4653</label><figDesc>First we show that equivalence implies counterfactual equivalence w.r.t. O. The twin operation preserves the equivalence relation on SCMs and since equivalent SCMs are interventionally equivalent w.r.t. every subset, the two equivalent twin SCMs have to be interventionally equivalent w.r.t. O ∪ O for every O ⊆ I with O the copy of O in I . Now, let M and M be counterfactually equivalent w.r.t. O. Then M twin and Mtwin are interventionally equivalent w.r.t. O ∪ O . Thus for I ⊆ O, I ⊆ O the copy of I and ξ I = ξ I ∈ X I , M twin do(I∪I ,ξ I∪I ) and Mtwin do(I∪I ,ξ I∪I ) are observationally equivalent w.r.t. O ∪ O . In particular, they are observationally equivalent w.r.t. O. From Proposition 2.21 we have that M twin do(I∪I ,ξ I∪I ) = (M do(I,ξI ) ) twin and Mtwin do(I∪I ,ξ I∪I ) = ( Mdo(I,ξI) ) twin , and together with Lemma E.2 this gives that M do(I,ξI ) and Mdo(I,ξI) are observationally equivalent w.r.t. O.Section Let M be an SCM. Let B ⊆ I and A ⊆ I ∪ J such that (pa(B) \ B) ⊆ A and B ∩ A = ∅. Assume that g B : X A × E A → X B is a measurable function such that for P E -almost every e ∈ E and for all x ∈ Xx B = f B (x pa(B) , e pa(B) ) ⇐⇒ x B = g B (x A , e A ) .Then M is uniquely solvable w.r.t. B. PROOF. Assume that for P E -almost every e ∈ E and for all x ∈ Xx B = f B (x pa(B) , e pa(B) ) ⇐⇒ x B = g B (x A , e A ) .Let C := A \ (pa(B) \ B), then by Lemma F.11.<ref type="bibr" target="#b6">(7)</ref> we have that there exists êC ∈ E C and xC ∈ X C such that for P EJ \C -almost every e J \C ∈ E J \C and for all x I\C ∈ X I\C x B = f B (x pa(B) , e pa(B) ) ⇐⇒ x B = g B (x pa(B)\B , xC , e pa(B) , êC ) . Defining the mapping h B : X pa(B)\B × E pa(B) → X B by h B (x pa(B)\B , e pa(B) ) := g B (x pa(B)\B , xC , e pa(B) , êC ) , where we picked êC ∈ E C and xC ∈ X C such that the above equivalence holds, and applying Lemma F.11.(6) we get that for P E -almost every e ∈ E and for all x ∈ X x B = f B (x pa(B) , e pa(B) ) ⇐⇒ x B = h B (x pa(B)\B , e pa(B) ) holds. Thus, M is uniquely solvable w.r.t. B.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_30"><head>LEMMA E. 4 .</head><label>4</label><figDesc>Given an SCM M and a subset L ⊆ I such that M is uniquely solvable w.r.t. L. Then M and marg(L)(M) are observationally equivalent w.r.t. I \ L. PROOF. Let O := I \ L. From unique solvability w.r.t. L it follows that for P E -almost every e ∈ E and for all x ∈ X</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_31"><head></head><label></label><figDesc>is a solution of the marginal SCM M marg(L) then with XL := g L ( Xpa(L)\L , E pa(L) ), the random variables (X, E) := ( XO , XL , E) are a solution of M. PROOF OF THEOREM 5.6. The observational equivalence follows from Lemma E.4. Using both Lemma E.4 and Proposition 5.5 we can prove the interventional equivalence. Observe that from Proposition 5.5 we know that for a subset I ⊆ I \ L and a value ξ I ∈ X I , (marg(L) • do(I, ξ I ))(M) exists. By Lemma E.4 we know that do(I, ξ I )(M) and (marg(L) • do(I, ξ I ))(M) are observationally equivalent w.r.t. O and hence by applying again Proposition 5.5, do(I, ξ I )(M) and (do(I, ξ) • marg(L))(M) are observationally equivalent w.r.t. O. This implies that M and marg(L)(M) are interventionally equivalent w.r.t. O. Lastly, we need to show that twin(M) and (twin • marg(L))(M) are interventionally equivalent w.r.t. (I ∪ I ) \ (L ∪ L ), where L is the copy of L in I . From Proposition 5.5 (twin • marg(L))(M) is equivalent to (marg(L ∪ L ) • twin)(M) and since we proved that (marg(L ∪ L ) • twin)(M) and twin(M) are interventionally equivalent w.r.t. (I ∪ I ) \ (L ∪ L ) the result follows.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_32"><head>G</head><label></label><figDesc>marg(L)(M) = marg(J ) G a marg(L)(M) ⊆ marg(J ) marg(L) G a (M) = marg(L) marg(J ) G a (M) = marg(L) G(M)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_33"><head></head><label></label><figDesc>I = {i}. Let g A \i j : X i × E pa(A \i j ) → X A \ij be a measurable solution function for M w.r.t. A \i j , that is, we have for P E -almost every e ∈ E and for all x ∈ X x A \i j = fA \i j (x, e) ⇐⇒ x A \i j = g A \i j (x i , e pa(A \i j ) ) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_34"><head></head><label></label><figDesc>PROOF. Let the σ-algebra of Y be generated by the countable generating set{C n } n∈N . The µ-measurable set f -1 (C n ) = A n ∪ E n for some A n ∈ Σ and some E n ∈ N and hence there is some E n ⊆ B n ∈ Σ such that µ(B n ) = 0. Let B = ∪ n∈N B n , Ân = A n \ B and Â = ∪ n∈N Ân , then µ( B) = 0,Â and B are disjoint and X = Â ∪ B. Now define the mapping g : X → Y by g(x) := f (x) if x ∈ Â, y 0 otherwise,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_35"><head></head><label></label><figDesc>-1 (φ(C * )), we must have that P E (φ -1 (B))≥ P E (C * ) = 1. Hence P Ẽ (A) = P Ẽ (B) = 1. Note that as C * ⊆ C, A ⊆ φ(C * ) ⊆ φ(C) ⊆ {ẽ ∈ Ẽ : P (ẽ) = 1}.Hence the set C := {ẽ ∈ Ẽ : P (ẽ) = 1} contains a measurable set of P Ẽ -measure 1, in other words, P holds P Ẽ -a.s..The converse is easier to prove. Suppose C = {ẽ ∈ Ẽ : P (ẽ) = 1} contains a measurable set C * with P Ẽ -measure 1, that is, C * ⊆ C and P Ẽ ( C * ) = 1. Because φ is measurable, the set φ -1 ( C * ) is measurable and P E (φ -1 ( C * )) = 1, and furthermore, φ -1 ( C * ) ⊆ φ -1 ( C) = C.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_36"><head></head><label></label><figDesc>1. ∨ ∼ e : P (e) =⇒ ∃e : P (e) (similarly to ∀x : P (x) =⇒ ∃x : P (x)); 2. ∨ ∼ e : P (e) ⇐⇒ ∨ ∼ e : P (e) (similarly to ∀x : P (x) ⇐⇒ ∀x : P (x)); 3. ∃x∨ ∼ e : P (x, e) =⇒ ∨ ∼ e∃x : P (x, e) (similarly to ∃x∀e : P (x, e) =⇒ ∀e∃x : P (x, e)); 4. ∨ ∼ e∀x : P (x, e) =⇒ ∀x∨ ∼ e : P (x, e) (similarly to ∀e∀x : P (x, e) =⇒ ∀x∀e : P (x, e)); 5. ∨ ∼ e : P (e) =⇒ ∃ẽ∨ ∼ e : P (e) (similarly to ∀x : P (x) =⇒ ∃x∀x : P (x)); 6. ∨ ∼ e∀x : P (x, e) ⇐⇒ ∨ ∼ e∀x : P (x, e); 7. ∨ ∼ e∀x : P (x, e) =⇒ ∃ẽ∃x∨ ∼ e∀x : P (x, e),</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_37"><head>Property 2 .</head><label>2</label><figDesc>Let pr E : E → E be the projection mapping on E. Then by Lemma F.10 we have ∨ ∼ e : P (e) ⇐⇒ ∨ ∼ e : P • pr E (e) ⇐⇒ ∨ ∼ e : P (e) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Overview of the commutativity results of different pairs of mappings, defined on SCMs (left table) and on graphs (right table</figDesc><table><row><cell></cell><cell>do</cell><cell>twin</cell><cell>marg</cell><cell>Graphs</cell><cell>do</cell><cell>twin</cell><cell>marg</cell></row><row><cell>G, G a</cell><cell>2.14</cell><cell>2.19</cell><cell>(5.11)</cell><cell>do</cell><cell cols="3">2.15.(1) 2.21.(2) 5.9.(1)</cell></row><row><cell>do</cell><cell cols="3">2.15.(1) 2.21.(1) 5.5.(1)</cell><cell>twin</cell><cell>• • •</cell><cell>-</cell><cell>5.9.(2)</cell></row><row><cell>twin</cell><cell>• • •</cell><cell>-</cell><cell>5.5.(2)</cell><cell>marg</cell><cell>• • •</cell><cell>• • •</cell><cell>5.8</cell></row><row><cell>marg</cell><cell>• • •</cell><cell>• • •</cell><cell>5.4</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>t. O ⊆ I if M is (uniquely) solvable w.r.t. every ancestral subset in G(M) O . We call M ancestrally (uniquely) solvable if it is ancestrally (uniquely) solvable w.r.t. I. PROPOSITION 3.10 (Solvability is equivalent to ancestral solvability). The SCM M = I, J , X , E, f , P E is solvable w.r.t. the subset O ⊆ I if and only if M is ancestrally solvable w.r.t. O. Consider the SCM M = 4, 1, R 4 , R, f , P R with causal mechanism given by</figDesc><table><row><cell>A similar result does not hold for unique solvability. Although ancestral unique solvability</cell></row><row><cell>w.r.t. O ⊆ I implies unique solvability w.r.t. O, the converse does not hold in general, as the</cell></row><row><cell>following example illustrates.</cell></row><row><cell>EXAMPLE 3.11 (Unique solvability w.r.t. O does not imply ancestral unique solvability</cell></row><row><cell>w.r.t. O).</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>the directed global Markov property ("d-separation criterion") relative to G(M) (see Definition A.6) if M satisfies at least one of the following conditions: X satisfies the general directed global Markov property ("σ-separation criterion") relative to G(M) (see Definition A.20) if M is uniquely solvable w.r.t. each strongly connected component of G(M).14   </figDesc><table /><note><p><p>a) M is acyclic; b) all endogenous spaces X i are discrete and M is ancestrally uniquely solvable; c) M is linear (see Definition C.1), each of its causal mechanisms {f i } i∈I has a nontrivial dependence on at least one exogenous variable, and P E has a density w.r.t. the Lebesgue measure on R J .</p>2. P</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>t. O, and so is M. PROOF OFPROPOSITION 3.7. This follows immediately from Definitions 2.7 and 3.3. PROOF OF THEOREM 3.6. Suppose that (1) holds. By Proposition B.1 there exists a measurable solution function g O : X pa(O)\O × E pa(O) → X O for M w.r.t. O. Then for P Ealmost every e ∈ E and for all x \O ∈ X \O we have that g O (x pa(O)\O , e pa(O) ) is a solution of x O = f O (x, e). Hence, because of (1), for P E -almost every e ∈ E and for all x \O ∈ X \O we have thatx O = f O (x, e) implies x O = g O (x pa(O)\O , e pa(O) ).Thus, M is uniquely solvable w.r.t. O, that is, (2) holds. Suppose that (2) holds. Let g O : X pa(O)\O × E pa(O) → X O be a measurable solution function for M w.r.t. O. Then, for P E -almost every e ∈ E and for all x ∈</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>. Let g O : X pa(O)\O × E pa(O) → X O be a measurable solution function for M w.r.t. O. Then the mapping gO∪I : E pa(O) → X O∪I defined by gO∪I (e pa(O) ) := (g O (ξ pa(O)\O , e pa(O) ), ξ I ) is a measurable solution function for the SCM M do(I,ξI ) w.r.t. O ∪ I. If M is (uniquely) solvable w.r.t. O, then it follows that M do(I,ξI ) is (uniquely) solvable w.r.t. O ∪ I. PROOF OF PROPOSITION 3.10. It suffices to show that solvability of M w.r.t. O implies ancestral solvability w.r.t. O. Solvability of M w.r.t. O implies that there exists a measurable mapping g O : X pa(O)\O × E pa(O) → X O such that for P E -almost every e ∈ E and for all</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>We often use boldface for variables that have multiple components, for example, vectors in a Cartesian product.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>For the case J = ∅, we have that E is the singleton 1 and P E is the degenerate probability measure P 1 .</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2"><p>An attempt at coarsening this notion of equivalence by replacing the quantifier "for all x ∈ X " by "for</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_3"><p>For visualizing an (augmented) graph, we adapt the common convention of using random variables, with the index set as a subscript, instead of using the index set itself. With a slight abuse of notation, we still use the random variables notation in the (augmented) graph in the case that the SCM has no solution at all.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_4"><p>For U = ∅, we always consider the trivial mapping f ∅ : X × E → X ∅ where X ∅ is the singleton 1.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_5"><p>In order to assure the existence of a unique observational distribution it is common to consider only SCMs for which the structural equations have a unique solution (see, e.g., Definition 7.1.1 in<ref type="bibr" target="#b50">[51]</ref>). Although these SCMs induce a unique observational distribution, they generally do not induce a unique distribution after a perfect intervention.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="12" xml:id="foot_6"><p>In the literature, one often finds the notation p(x) and p(x | do(X I = x I )) for the densities of the observational and interventional distribution, respectively, in case these are uniquely defined by the SCM [e.g., 51].</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="13" xml:id="foot_7"><p>Here, one can also use the augmented graph G a (M) on O since an G(M) O (A) = an G a (M) O (A) for every subset A ⊆ O ⊆ I.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="14" xml:id="foot_8"><p>Since<ref type="bibr" target="#b17">[18]</ref> also provides results under the weaker condition that an SCM is solvable (not necessarily uniquely) w.r.t. each strongly connected component of G(M), one might believe that Theorem 6.3.(2) could be generalized to stating that in that case, any of its observational distributions satisfies the general directed global Markov property. However, that is not true: consider, for example, the SCM M = 2, ∅, R 2 , 1, f , P 1 with f 1 (x) = x 1 and f 2 (x) = x 2 . Then M is solvable w.r.t. each of its strongly connected components {1} and {2}. The solution with X 1 = X 2 , where X 2 has a nondegenerate distribution, shows a dependence between X 1 and X 2 , and thus X 1 ⊥ ⊥ X 2 does not hold. In general, all strongly connected components that admit multiple solutions may be dependent on any other variable(s) in the model.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="17" xml:id="foot_9"><p>We deviate from the terminology in<ref type="bibr" target="#b17">[18]</ref> where this is called a "compatible system of structural equations".</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="18" xml:id="foot_10"><p>We use the "hat" notation E O to distinguish it from the ordinary subscript convention that E O = F ∈O E F for some subset O ⊆ Ĥ.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="19" xml:id="foot_11"><p>A topological space X is called σ-compact if it is the union of a countable set of compact topological spaces.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="20" xml:id="foot_12"><p>Note that we do not assume that the probability measure P R J is Gaussian.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="21" xml:id="foot_13"><p>Richardson and Robins [65]  argue that this market equilibrium model cannot be modeled as an SCM. We observe that it can, as long as one allows for self-cycles.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments <rs type="person">S. Bongers</rs> and <rs type="person">J.M. Mooij</rs> are supported in part by <rs type="funder">NWO</rs>, the <rs type="funder">Netherlands Organization for Scientific Research</rs> (<rs type="grantName">VIDI grant</rs> <rs type="grantNumber">639.072.410</rs> and <rs type="grantName">VENI grant</rs> <rs type="grantNumber">639.031.036</rs>). <rs type="person">P. Forré</rs> and <rs type="person">J.M. Mooij</rs> are supported in part by the <rs type="funder">European Research Council (ERC)</rs> under the European Union's <rs type="programName">Horizon 2020 research and innovation programme</rs> (grant agreement n o 639466). J. Peters is supported by research grants from <rs type="funder">VILLUM FONDEN</rs> (<rs type="grantNumber">18968</rs>) and the <rs type="funder">Carlsberg Foundation</rs>.</p><p>The authors are grateful to <rs type="person">Bernhard Schölkopf</rs> and <rs type="person">Robin Evans</rs> for stimulating discussions, and to <rs type="person">Noud de Kroon</rs>, <rs type="person">Tineke Blom</rs> and <rs type="person">Alexander Ly</rs> for providing helpful comments on earlier drafts. We thank two anonymous reviewers and the associate editor for helpful comments.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_y9TuCGq">
					<idno type="grant-number">639.072.410</idno>
					<orgName type="grant-name">VIDI grant</orgName>
				</org>
				<org type="funding" xml:id="_vTgWCDt">
					<idno type="grant-number">639.031.036</idno>
					<orgName type="grant-name">VENI grant</orgName>
					<orgName type="program" subtype="full">Horizon 2020 research and innovation programme</orgName>
				</org>
				<org type="funding" xml:id="_qb2EnqJ">
					<idno type="grant-number">18968</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FOUNDATIONS OF STRUCTURAL CAUSAL MODELS WITH CYCLES AND LATENT VARIABLES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head><p>This Supplementary Material contains a summary of the basic terminology and results for causal graphical models (Appendix A), additional (unique) solvability properties (Appendix B), some results for linear SCMs (Appendix C), other examples (Appendix D), the proofs of all the theoretical results (Appendix E) and the measurable selection theorems (Appendix F) that are used in several proofs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX A: CAUSAL GRAPHICAL MODELS</head><p>In this appendix, we provide a summary of the basic terminology and results for causal graphical models. In Appendix A.1 we provide the terminology for directed (mixed) graphs. In Appendix A.2 we give an introduction and an intuitive derivation of Markov properties for SCMs with cycles. In Appendix A.3 we provide a definition of modular SCMs and show how they relate to SCMs. In Appendix A. <ref type="bibr" target="#b3">4</ref> we provide an overview of the causal graphical models related to SCMs. The proofs of the theoretical results in this appendix are given in Appendix E.</p><p>A.1. Directed (mixed) graphs In this subsection, we introduce the terminology for directed (mixed) graphs, where we do allow for cycles <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b59">60]</ref>. DEFINITION A.1 (Directed (mixed) graph).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1.</head><p>A directed graph is a pair G = (V, E), where V is a set of nodes and E is a set of directed edges, which is a subset E ⊆ V × V of ordered pairs of nodes. Each element (i, j) ∈ E can be represented by the directed edge i → j or equivalently j ← i. In particular, (i, i) ∈ E represents a self-cycle i → i.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>A directed mixed graph is a triple G = (V, E, B), where the pair (V, E) forms a directed graph and B is a set of bidirected edges, which is a subset B ⊆ {{i, j} : i, j ∈ V, i = j} of unordered (distinct) pairs of nodes. Each element {i, j} ∈ B can be represented by the bidirected edge i ↔ j or equivalently j ↔ i. Note that a directed graph can be considered as a directed mixed graph without bidirected edges.</p><p>, in which case we write G ⊆ G. For a subset W ⊆ V, we define the induced subgraph of G on W by G W := (W, Ẽ, B), where Ẽ and B are the set of directed and bidirected edges in E and B, respectively, that lie in W × W and {{i, j} : i, j ∈ W, i = j}, respectively. 4. A walk between i, j ∈ V in a directed mixed graph G is a tuple (i 0 , 1 , i 1 , 2 , i 2 , . . . , n , i n )</p><p>of alternating nodes and edges in G for some n ≥ 0, where all i 0 , . . . , i n ∈ V, all</p><p>and it starts with node i 0 = i and ends with node i n = j. Note that n = 0 corresponds with a trivial walk consisting of a single node. If all nodes i 0 , . . . , i n are distinct, it is called a path. A walk (path) of the form i →</p><p>Hence, ι( M) is loop-wisely solvable and thus (g O ) O∈L(G( M)) is a family of measurable solution functions. In particular, for all O, Õ ∈ L(G( M)) with Õ ⊆ O and for P E -almost every e ∈ E and for all x ∈ X we have</p><p>From this we conclude that (g O ) O∈L(G( M)) is a compatible system of solution functions.</p><p>PROOF OF LEMMA A.33. Suppose M is loop-wisely uniquely solvable and consider a subset </p><p>This implies that for P E -almost every e ∈ E and for all x ∈ X</p><p>PROOF OF COROLLARY 8.5. This follows directly from Proposition 7.1 and 7.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B</head><p>PROOF OF PROPOSITION B.1. Let f : E × X → X be the causal mechanism of a structurally minimal SCM that is equivalent to M (see Proposition 2.11). In particular, for any \pa(O) ∈ E \pa(O) and ξ \pa(O) ∈ X \pa(O) , we have that for all x ∈ X and all e ∈ E, f (x, e) = f (x pa(O) , ξ \pa(O) , e pa(O) , \pa(O) ). This means that we may also consider f as a mapping f :</p><p>Consider the set</p><p>By similar reasoning as in the proof of Theorem 3.2, S is measurable.</p><p>By assumption, for P E -almost every e ∈ E and for all x \O ∈ X \O the space {x O ∈ X O : j=1 an G(M)O (i j ) and an G(M)O (i k ), thereby noting that the intersection of these two sets is an ancestral subset of size ≤ n and making use of the induction hypothesis, we arrive at the conclusion that M is uniquely solvable w.r.t. A. </p><p>has a unique solution x L ∈ X L . Hence, M is uniquely solvable w.r.t. L if and only if A LL is invertible.</p><p>PROOF OF PROPOSITION C.4. It suffices to show (1) =⇒ ( <ref type="formula">2</ref>) and (1) ⇐⇒ (3). We start by showing that (1) =⇒ <ref type="bibr" target="#b1">(2)</ref>. Let V ⊆ L and denote U := an G(M)L (V), then we need to show that M is uniquely solvable w.r.t. U . From Proposition C.3 we know that M is uniquely solvable w.r.t. L if and only if the matrix A LL = I L -B LL is invertible. The matrix A LL is invertible if and only if the rows of A LL are all linearly independent. In particular, the rows of A U L are all linearly independent. Because</p><p>where Z U L is the zero matrix, we know that the rows of A U U = I U -B U U are also all linearly independent, and hence A U U is invertible.</p><p>Next, we show that (1) ⇐⇒ (3). Observe that the strongly connected components of G(M) L form a partition of the set L and that the directed mixed graph G(M) L and the directed graph G a (M) L have the same strongly connected components. Because, by Lemma A.2, the graph of strongly connected components G sc of the directed graph G a (M) L is a DAG, the square matrix B LL can be permuted to an upper triangular block matrix BLL , where for each diagonal block BVV of BLL the set of nodes V is a strongly connected component in G(M) L .</p><p>Without loss of generality we assume now that B LL is an upper triangular block matrix. From Proposition C.3 it follows that M is uniquely solvable w.r.t. L if and only if the matrix A LL = I L -B LL is invertible. Because B LL is an upper triangular block matrix, we know that A LL is an upper triangular block matrix, where for each diagonal block A VV of A LL For every measurable set B j ⊆ X j there exists a version of the regular conditional probability P Mdo(I,ξ I ) (X j ∈ B | X i = ξ i ) such that for every value ξ i ∈ X i it satisfies</p><p>where we used X i ⊥ ⊥ E pa(A \i j ) in the fourth equality. If we assume i / ∈ pa(A Next, we show that the class of simple SCMs is closed under the twin operation. Let </p><p>where we define pa := pa G a (M twin ) as the parents w.r.t. the twin graph G a (M twin ). Then by construction this mapping h Õ is a measurable solution function for M twin w.r.t. Õ, and it is clear that M twin is uniquely solvable w.r.t. Õ.</p><p>Lastly, it follows that the observational and all the intervened models of M and M twin are uniquely solvable. From Theorem 3.6 we conclude that M induces unique observational, interventional and counterfactual distributions. Property 5: Let N be a measurable P E -null set such that P (e) holds for all e ∈ E \ N . Define for ẽ ∈ Ẽ the set N ẽ := {e ∈ E : (e, ẽ) ∈ N }. Note that the sets N ẽ are measurable. From Fubini's theorem it follows that for P Ẽ -almost every ẽ ∈ Ẽ we have P E (N ẽ) = 0. That is, there exists a measurable P Ẽ -null set Ñ such that P E (N ẽ) = 0 for all ẽ ∈ Ẽ \ Ñ . Hence, there exists ẽ ∈ Ẽ \ Ñ such that P E (N ẽ) = 0; for all e ∈ E \ N ẽ, P (e) then holds. This means ∃ẽ∨ ∼ e : P (e). </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Probabilistic Evaluation of Counterfactual Queries</title>
		<author>
			<persName><forename type="first">A</forename><surname>Balke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth National Conference on Artificial Intelligence (AAAI-94</title>
		<meeting>the Twelfth National Conference on Artificial Intelligence (AAAI-94</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="1994">1994</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="230" to="237" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Abstracting Causal Models</title>
		<author>
			<persName><forename type="first">S</forename><surname>Beckers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Halpern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence (AAAI-19</title>
		<meeting>the Thirty-Third AAAI Conference on Artificial Intelligence (AAAI-19</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="2678" to="2685" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Beyond Structural Causal Models: Causal Constraints Models</title>
		<author>
			<persName><forename type="first">T</forename><surname>Blom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bongers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Mooij</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th Conference on Uncertainty in Artificial Intelligence (UAI-19</title>
		<editor>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">V</forename><surname>Gogate</surname></persName>
		</editor>
		<meeting>the 35th Conference on Uncertainty in Artificial Intelligence (UAI-19</meeting>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Conditional Independences and Causal Relations implied by Sets of Equations</title>
		<author>
			<persName><forename type="first">T</forename><surname>Blom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Van Diepen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Mooij</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.07183[cs.AI</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv.org preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Bollen</surname></persName>
		</author>
		<title level="m">Structural Equations with Latent Variables</title>
		<meeting><address><addrLine>New York, USA</addrLine></address></meeting>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Bongers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Blom</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Mooij</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.08784v3[cs.AI</idno>
		<title level="m">Causal Modeling of Dynamical Systems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv.org preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">CAM: Causal Additive Models, high-dimensional order search and penalized regression</title>
		<author>
			<persName><forename type="first">P</forename><surname>Ühlmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ernest</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="2526" to="2556" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M J</forename><surname>Byrne</surname></persName>
		</author>
		<title level="m">The Rational Imagination: How People Create Alternatives to Reality. A Bradford Book</title>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Cohn</surname></persName>
		</author>
		<title level="m">Measure Theory, 2nd ed. Birkhäuser</title>
		<meeting><address><addrLine>Boston, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A Simple Constraint-Based Algorithm for Efficiently Mining Observational Databases for Causal Relationships</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">F</forename><surname>Cooper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="203" to="224" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Influence Diagrams for Causal Modelling and Inference</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Dawid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Statistical Review</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="161" to="189" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Introduction to Structural Equation Models</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">D</forename><surname>Duncan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1975">1975</date>
			<publisher>Academic Press</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Exact Bayesian structure learning from uncertain interventions</title>
		<author>
			<persName><forename type="first">D</forename><surname>Eaton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Meila</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">X</forename><surname>Shen</surname></persName>
		</editor>
		<meeting>the Eleventh International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="107" to="114" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Combining Experiments to Discover Linear Cyclic Models with Latent Variables</title>
		<author>
			<persName><forename type="first">F</forename><surname>Eberhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Scheines</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</title>
		<editor>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Titterington</surname></persName>
		</editor>
		<meeting>the Thirteenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="185" to="192" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Graphs for Margins of Bayesian Networks</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Evans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scandinavian Journal of Statistics</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="625" to="648" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Margins of discrete Bayesian networks</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Evans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="2623" to="2656" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A Correspondence Principle For Simultaneous Equation Models</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">M</forename><surname>Fisher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econometrica</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="73" to="92" />
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Markov Properties for Graphical Models with Cycles and Latent Variables</title>
		<author>
			<persName><forename type="first">P</forename><surname>Forr É</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Mooij</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.08775</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv.org preprint</note>
	<note>math.ST</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Constraint-based Causal Discovery for Non-Linear Structural Causal Models with Cycles and Latent Confounders</title>
		<author>
			<persName><forename type="first">P</forename><surname>Forr É</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Mooij</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th Conference on Uncertainty in Artificial Intelligence (UAI-18</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Globerson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Silva</surname></persName>
		</editor>
		<meeting>the 34th Conference on Uncertainty in Artificial Intelligence (UAI-18</meeting>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Causal Calculus in the Presence of Cycles, Latent Confounders and Selection Bias</title>
		<author>
			<persName><forename type="first">P</forename><surname>Forr É</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Mooij</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th Conference on Uncertainty in Artificial Intelligence (UAI-19</title>
		<editor>
			<persName><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">V</forename><surname>Gogate</surname></persName>
		</editor>
		<meeting>the 35th Conference on Uncertainty in Artificial Intelligence (UAI-19</meeting>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Half-trek Criterion for Generic Identifiability of Linear Structural Equation Models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Foygel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Draisma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Drton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="1682" to="1713" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Graphoids: A Qualitative Framework for Probabilistic Inference</title>
		<author>
			<persName><forename type="first">D</forename><surname>Geiger</surname></persName>
		</author>
		<idno>No. R-142</idno>
		<imprint>
			<date type="published" when="1990">1990</date>
			<pubPlace>Los Angeles, USA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Computer Science Department, University of California</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">D</forename><surname>Duncan</surname></persName>
		</author>
		<title level="m">Structural Equation Models in the Social Sciences</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Seminar Press</publisher>
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Calculating the Singular Values and Pseudo-Inverse of a Matrix</title>
		<author>
			<persName><forename type="first">G</forename><surname>Golub</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Kahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Society for Industrial and Applied Mathematics: Series B</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="205" to="224" />
			<date type="published" when="1965">1965</date>
		</imprint>
	</monogr>
	<note>Numerical Analysis</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The Statistical Implications of a System of Simultaneous Equations</title>
		<author>
			<persName><forename type="first">T</forename><surname>Haavelmo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econometrica</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="1943">1943</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Axiomatizing Causal Reasoning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Halpern</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth Conference on Uncertainty in Artificial Intelligence (UAI-98</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Cooper</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Moral</surname></persName>
		</editor>
		<meeting>the Fourteenth Conference on Uncertainty in Artificial Intelligence (UAI-98<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="202" to="210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning Linear Cyclic Causal Models with Latent Variables</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hyttinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Eberhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">O</forename><surname>Hoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="3387" to="3439" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Discovering Cyclic Causal Models with Latent Variables: A General SAT-based Procedure</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hyttinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">O</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Eberhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ärvisalo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth Conference on Uncertainty in Artificial Intelligence (UAI-13</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Nicholson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Smyth</surname></persName>
		</editor>
		<meeting>the Twenty-Ninth Conference on Uncertainty in Artificial Intelligence (UAI-13<address><addrLine>Corvallis, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="301" to="310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Causality and model abstraction</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Iwasaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">A</forename><surname>Simon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page" from="143" to="194" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Classical Descriptive Set Theory</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Kechris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Graduate Texts in Mathematics</title>
		<imprint>
			<biblScope unit="volume">156</biblScope>
			<date type="published" when="1995">1995</date>
			<publisher>Springer-Verlag</publisher>
			<pubPlace>New York, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Markov Properties of Nonrecursive Causal Models</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T A</forename><surname>Koster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="2148" to="2177" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">On the Validity of the Markov Interpretation of Path Diagrams of Gaussian Structural Equations Systems with Correlated Errors</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T A</forename><surname>Koster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scandinavian Journal of Statistics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="413" to="431" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Discovering cyclic causal models by independent components analysis</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lacerda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">L</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ramsey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">O</forename><surname>Hoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fourth Conference on Uncertainty in Artificial Intelligence (UAI-08</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Myllymaki</surname></persName>
		</editor>
		<meeting>the Twenty-Fourth Conference on Uncertainty in Artificial Intelligence (UAI-08<address><addrLine>Corvallis, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="366" to="374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Graphical Models</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Lauritzen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Oxford Statistical Science Series</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<date type="published" when="1996">1996</date>
			<publisher>Clarendon Press</publisher>
			<pubPlace>Oxford</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Independence Properties of Directed Markov Fields</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Lauritzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Dawid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">N</forename><surname>Larsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">G</forename><surname>Leimer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="491" to="505" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Lewis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Counterfactual Dependence and Time&apos;s Arrow. Noûs</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="455" to="476" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Estimating High-Dimensional Intervention Effects from Observational Data</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Maathuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Colombo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kalisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ühlmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="3133" to="3164" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">A Bayesian Local Causal Discovery Framework</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
		<respStmt>
			<orgName>University of Pittsburg</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Feedback Theory -Some Properties of Signal Flow Graphs</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Mason</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IRE</title>
		<meeting>the IRE</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1953">1953</date>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="1144" to="1156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Feedback Theory -Further Properties of Signal Flow Graphs</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Mason</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IRE</title>
		<meeting>the IRE</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1956">1956</date>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="920" to="926" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Strong Completeness and Faithfulness in Bayesian Networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Meek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence (UAI-95</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Besnard</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Hanks</surname></persName>
		</editor>
		<meeting>the Eleventh Conference on Uncertainty in Artificial Intelligence (UAI-95<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="411" to="418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Markov equivalence of marginalized local independence graphs</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Mogensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">R</forename><surname>Hansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Statist</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="539" to="559" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Causal Learning for Partially Observed Stochastic Dynamical Systems</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">W</forename><surname>Mogensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Malinsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">R</forename><surname>Hansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Fourth conference on Uncertainty in Artificial Intelligence (UAI-18</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Globerson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Silva</surname></persName>
		</editor>
		<meeting>the Thirty-Fourth conference on Uncertainty in Artificial Intelligence (UAI-18</meeting>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Constraint-Based Causal Discovery using Partial Ancestral Graphs in the presence of Cycles</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Mooij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Claassen</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th Conference on Uncertainty in Artificial Intelligence</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Sontag</surname></persName>
		</editor>
		<meeting>the 36th Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="1159" to="1168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Cyclic Causal Discovery from Continuous Equilibrium Data</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Mooij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Heskes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th Conference on Uncertainty in Artificial Intelligence (UAI-13</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Nicholson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Smyth</surname></persName>
		</editor>
		<meeting>the 29th Conference on Uncertainty in Artificial Intelligence (UAI-13<address><addrLine>Corvallis, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="431" to="439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">From Ordinary Differential Equations to Structural Causal Models: the deterministic case</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Mooij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th Conference on Uncertainty in Artificial Intelligence (UAI-13</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Nicholson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Smyth</surname></persName>
		</editor>
		<meeting>the 29th Conference on Uncertainty in Artificial Intelligence (UAI-13</meeting>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="440" to="448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Joint Causal Inference from Multiple Contexts</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Mooij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Magliacane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Claassen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1" to="108" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Distinguishing Cause from Effect using Observational Data: Methods and Benchmarks</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Mooij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zscheischler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="1" to="102" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">On Deducing Conditional Independence from d-Separation in Causal Graphs with Feedback</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Neal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="87" to="91" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">A Constraint Propagation Approach to Probabilistic Reasoning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Conference on Uncertainty in Artificial Intelligence (UAI-85</title>
		<editor>
			<persName><forename type="first">L</forename><surname>Kanal</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Lemmer</surname></persName>
		</editor>
		<meeting>the First Conference on Uncertainty in Artificial Intelligence (UAI-85<address><addrLine>Corvallis, Oregon, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="1985">1985</date>
			<biblScope unit="page" from="31" to="42" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<title level="m">Causality: Models, Reasoning, and Inference</title>
		<meeting><address><addrLine>New York, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>nd ed.</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Identifying Independence in Causal Graphs with Feedback</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Dechter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth Conference on Uncertainty in Artificial Intelligence (UAI-96</title>
		<editor>
			<persName><forename type="first">E</forename><surname>Horvitz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Jensen</surname></persName>
		</editor>
		<meeting>the Twelfth Conference on Uncertainty in Artificial Intelligence (UAI-96<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="420" to="426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">The Book of Why: The New Science of Cause and Effect, 1st ed</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mackenzie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>Basic Books</publisher>
			<pubPlace>New York, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">A generalized inverse for matrices</title>
		<author>
			<persName><forename type="first">R</forename><surname>Penrose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Proceedings of the Cambridge Philosophical Society</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="406" to="413" />
			<date type="published" when="1955">1955</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Elements of Causal Inference: Foundations and Learning Algorithms</title>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ölkopf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Causal Discovery with Continuous Additive Noise Models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Mooij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="2009" to="2053" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Learning Stable and Predictive Structures in Kinetic Systems</title>
		<author>
			<persName><forename type="first">N</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Academy of Sciences</title>
		<meeting>the National Academy of Sciences</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="page" from="25405" to="25411" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">A Discovery Algorithm for Directed Cyclic Graphs</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth Conference on Uncertainty in Artificial Intelligence (UAI-96</title>
		<editor>
			<persName><forename type="first">E</forename><surname>Horvitz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Jensen</surname></persName>
		</editor>
		<meeting>the Twelfth Conference on Uncertainty in Artificial Intelligence (UAI-96<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="454" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Discovering Cyclic Causal Structure</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Richardson</surname></persName>
		</author>
		<idno>No. CMU-PHIL-68</idno>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Markov Properties for Acyclic Directed Mixed Graphs</title>
		<author>
			<persName><forename type="first">T</forename><surname>Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scandinavian Journal of Statistics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="145" to="157" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Automated Discovery of Linear Feedback Models</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<editor>Computation, Causation, and Discovery (C. Glymour and G. F. Cooper</editor>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>MIT Press</publisher>
			<biblScope unit="page" from="253" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Ancestral Graph Markov Models</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="962" to="1030" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Richardson</surname></persName>
		</author>
		<title level="m">Models of Feedback: Interpretation and Discovery</title>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Single World Intervention Graphs (SWIGs): A Unification of the Counterfactual and Graphical Approaches to Causality</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Robins</surname></persName>
		</author>
		<idno>No. 128</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>Center for Statistics and the Social Sciences</publisher>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">ACE Bounds; SEMs with Equilibrium Conditions</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Robins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistical Science</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="363" to="366" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Counterfactual Thinking</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Roese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Bulletin</title>
		<imprint>
			<biblScope unit="volume">121</biblScope>
			<biblScope unit="page" from="133" to="148" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Causal Consistency of Structural Equation Models</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Rubenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Weichwald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bongers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Mooij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Grosse-Wentrup</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd Conference on Uncertainty in Artificial Intelligence (UAI-17</title>
		<editor>
			<persName><forename type="first">G</forename><surname>Elidan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Kersting</surname></persName>
		</editor>
		<meeting>the 33rd Conference on Uncertainty in Artificial Intelligence (UAI-17</meeting>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Estimating Causal Effects of Treatments in Randomized and Nonrandomized Studies</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Educational Psychology</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="688" to="701" />
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Complete Identification Methods for the Causal Hierarchy</title>
		<author>
			<persName><forename type="first">I</forename><surname>Shpitser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1941" to="1979" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<title level="m" type="main">Directed Cyclic Graphs, Conditional Independence, and Non-recursive Linear Structural Equation Models</title>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<idno>No. CMU-PHIL-35</idno>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Conditional Independence in Directed Cyclic Graphical Models for Feedback</title>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<idno>No. CMU-PHIL-54</idno>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Directed Cyclic Graphical Representations of Feedback Models</title>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence (UAI-95</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Besnard</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Hanks</surname></persName>
		</editor>
		<meeting>the Eleventh Conference on Uncertainty in Artificial Intelligence (UAI-95<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="499" to="506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Scheines</surname></persName>
		</author>
		<title level="m">Causation, Prediction, and Search, 2nd ed. Adaptive Computation and Machine Learning</title>
		<meeting><address><addrLine>Cambridge, Massachusetts</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">An Algorithm for Causal Inference in the Presence of Latent Variables and Selection Bias</title>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Meek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computation, Causation and Discovery</title>
		<editor>
			<persName><forename type="first">C</forename><surname>Glymour</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">F</forename><surname>Cooper</surname></persName>
		</editor>
		<imprint>
			<publisher>The MIT Press</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="211" to="252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Using Path Diagrams as a Structural Equation Modelling Tool</title>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Meek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Scheines</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Glymour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sociological Methods &amp; Research</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="182" to="225" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Tian</surname></persName>
		</author>
		<idno>No. R-309</idno>
		<title level="m">Studies in Causal Reasoning and Learning</title>
		<meeting><address><addrLine>Los Angeles, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
		<respStmt>
			<orgName>Cognitive Systems Laboratory, University of California</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Causal Discovery from Changes</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<idno>UAI-01</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th Conference in Uncertainty in Artificial Intelligence</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Breese</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</editor>
		<meeting>the 17th Conference in Uncertainty in Artificial Intelligence<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="512" to="521" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Graphical Aspects of Causal Models</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Verma</surname></persName>
		</author>
		<idno>No. R-191</idno>
		<imprint>
			<date type="published" when="1993">1993</date>
			<pubPlace>Los Angeles, USA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Computer Science Department, University of California</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Correlation and Causation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Agricultural Research</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="557" to="585" />
			<date type="published" when="1921">1921</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">On the Completeness of Orientation Rules for Causal Discovery in the Presence of Latent Confounders and Selection Bias</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">172</biblScope>
			<biblScope unit="page" from="1873" to="1896" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
