<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Image Segmentation with a Unified Graphical Model</title>
				<funder ref="#_3d9hpXA">
					<orgName type="full">US National Science Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2009-07-09">9 July 2009.</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><roleName>Member, IEEE</roleName><forename type="first">Lei</forename><surname>Zhang</surname></persName>
							<email>leizhang2009@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Rensselaer Polytechnic Institute</orgName>
								<address>
									<postCode>12180</postCode>
									<settlement>Troy</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Image Segmentation with a Unified Graphical Model</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2009-07-09">9 July 2009.</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1109/TPAMI.2009.145</idno>
					<note type="submission">received 2 Dec. 2008; revised 5 Mar. 2009; accepted 18 May 2009;</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:24+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Image segmentation</term>
					<term>probabilistic graphical model</term>
					<term>Conditional Random Field</term>
					<term>Bayesian Network</term>
					<term>factor graph</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a unified graphical model that can represent both the causal and noncausal relationships among random variables and apply it to the image segmentation problem. Specifically, we first propose to employ Conditional Random Field (CRF) to model the spatial relationships among image superpixel regions and their measurements. We then introduce a multilayer Bayesian Network (BN) to model the causal dependencies that naturally exist among different image entities, including image regions, edges, and vertices. The CRF model and the BN model are then systematically and seamlessly combined through the theories of Factor Graph to form a unified probabilistic graphical model that captures the complex relationships among different image entities. Using the unified graphical model, image segmentation can be performed through a principled probabilistic inference. Experimental results on the Weizmann horse data set, on the VOC2006 cow data set, and on the MSRC2 multiclass data set demonstrate that our approach achieves favorable results compared to state-of-the-art approaches as well as those that use either the BN model or CRF model alone.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ç</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>I MAGE segmentation has been an active area of research in computer vision for more than 30 years. Many approaches have been proposed to solve this problem. They can be roughly divided into two groups: the deterministic approach and the probabilistic approach. The former formulates the segmentation problem as a deterministic optimization problem. This approach includes the clustering method <ref type="bibr" target="#b0">[1]</ref>, "snakes" or active contours <ref type="bibr" target="#b1">[2]</ref>, the graph partitioning method <ref type="bibr" target="#b2">[3]</ref>, the level set-based method <ref type="bibr" target="#b3">[4]</ref>, etc. The probabilistic approach, on the other hand, formulates the segmentation problem as a stochastic optimization problem. It can be further divided into two groups. One group uses various graphical models (such as Markov Random Fields and Bayesian Network) to model the joint probability distribution of the related image entities <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>. The other group directly models the probability distribution of the image entities either parametrically or nonparametrically, without using graphical models. It includes the discriminative approach <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>, the generative approach <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>, <ref type="bibr" target="#b22">[23]</ref>, and the hybrid approach, combining the discriminative model and the generative model <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b24">[25]</ref>. Our work belongs to the category of using graphical models for image segmentation. Specifically, we develop a unified graphical model that can incorporate various types of probabilistic relationships and apply it to the image segmentation problem. However, this framework is general enough to be applied to other computer vision problems.</p><p>Much progress has been made in the image segmentation field so far. As a result of the progress, computer vision is able to segment increasingly more complex images. Image segmentation, however, is still far from being resolved. One factor that prevents this from happening is the lack of ability by the existing methods to incorporate information/knowledge other than image data itself. Many existing image segmentation methods are data driven. These methods tend to fail when image contrast is low or in the presence of occlusion, the clutter of other objects. The fact of the matter is that the image itself may not contain enough information for an effective segmentation, no matter what algorithms we use and how sophisticated the algorithms are.</p><p>If we study human segmentation, we will quickly realize that humans tend to exploit additional knowledge besides the image data to perform this task. Humans segment an image not only based on the image itself, but also based on their plentiful knowledge, such as the contour smoothness, connectivity, local homogeneity, the object shape, the contextual information, etc. A human's capability to combine image data with additional knowledge plays an important role for effective and robust image segmentation. Many researchers have realized this aspect and have proposed different model-based approaches for image segmentation. The model is used to capture certain prior knowledge and to guide the image segmentation.</p><p>Despite these efforts, what is lacking is an image segmentation model that can systematically integrate different types of prior knowledge and the image data. Many existing approaches can only exploit very limited information, such as the image data and the local homogeneity of image labels. One of the reasons is due to the lack of a systematic way to incorporate various types of knowledge into a single framework.</p><p>A desirable image segmentation framework may be the one that is able to flexibly incorporate various types of information and constraints, and solve image segmentation in a probabilistic way. We notice that Probabilistic Graphical Models (PGMs) <ref type="bibr" target="#b25">[26]</ref>, <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b27">[28]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref> in the Machine Learning community are very powerful statistical models that are potentially able to satisfy all of these requirements. They provide an effective way to model various types of image entities, their uncertainties, and the related prior knowledge.</p><p>There are two basic types of graphical models: the undirected graphical model and the directed acyclic graphical model. The undirected graphical model can represent noncausal relationships among the random variables. The Markov Random Field (MRF) <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b25">[26]</ref> is a type of well-studied undirected graphical model. MRF models have been widely used for image segmentation. They incorporate the spatial relationships among neighboring labels as a Markovian prior. This prior can encourage (or discourage) the adjacent pixels to be classified into the same group. As an extension to MRFs, the Conditional Random Field (CRF) <ref type="bibr" target="#b26">[27]</ref> is another type of undirected graphical model that has become increasingly popular. The differences between MRF models and CRF models will be elaborated in Section 2.</p><p>While both MRF and CRF models can effectively capture noncausal relationships among the random variables (i.e., the nodes in a graphical model), such as the spatial homogeneity, they cannot model some directed relationships (e.g., the causalities) that extensively exist and are also important <ref type="bibr" target="#b30">[31]</ref>. Fortunately, this problem can be complementarily solved by another type of graphical model, i.e., the directed acyclic graphical model such as Bayesian Network (BN) <ref type="bibr" target="#b28">[29]</ref>  <ref type="bibr" target="#b29">[30]</ref>. BN can conveniently model the causal relationships between random variables using directed links and conditional probabilities. It has been successfully applied to medical diagnosis systems, expert systems, decision-making systems, etc. For image segmentation, there are some relationships that can be naturally modeled as causal relationships. For example, two adjacent regions with significantly different characteristics can lead to a highcontrast edge between them. In another example, the mutual exclusion, co-occurrence, or intercompetition relationships among the intersecting edges can also be modeled as causal relationships. These relationships are useful when one wants to impose some constraints on the edges. For example, two adjacent edges initially might have no relationships with each other when deciding which edge is part of the object boundary. However, knowing one edge is part of the object boundary and that the object boundary should be smooth, the probability of the other edge on the object boundary will reduce if the angle between the two edges is small. In this case, two edges become dependent on each other now. Such a relationship can be modeled by BN as the "explainingaway" relationship, but is hard to model by the undirected graphical models.</p><p>The existing graphical models for image segmentation tend to be either directed or undirected models alone. While they can effectively capture one type of image relationship, they often fail to capture the complex image relationships of different types. To overcome this limitation, we propose a probabilistic framework that unifies an undirected graphical model (i.e., the CRF model) with a directed graphical model (i.e., the BN model). It can flexibly incorporate image measurements, both noncausal relationships and causal relationships, and various types (both quantitative and qualitative) of human prior knowledge. In addition, the unified model systematically integrates the region-based image segmentation with the edge-based image segmentation. With this framework, image segmentation is performed through a principled probabilistic inference. The proposed framework is powerful, flexible, and also extendable to other computer vision problems.</p><p>Our main contributions lie in the introduction of a unified probabilistic framework for effective and robust image segmentation by incorporating various types of contextual/prior knowledge and image measurements under uncertainties. Our model captures the natural causal relationships among three entities in image segmentation: the regions, edges, and vertices (i.e., the junctions) as well as the noncausal spatial relationships among image labels and their measurements. Besides, various constraints are also modeled as either directed relationships or undirected relationships in the model.</p><p>The remainder of this paper is organized as follows: In Section 2, we review the related works that use graphical models for image segmentation. In Section 3, we give an overview of the proposed unified graphical model. In Section 4, we describe the region-based CRF image segmentation model. In Section 5, we describe the edgebased BN image segmentation model. In Section 6, we explain how we combine the CRF model with the BN model into a unified graphical model. In Section 7, we introduce the experiments on the Weizmann horse data set <ref type="bibr" target="#b31">[32]</ref>, on the Microsoft multiclass segmentation data set (MSRC2) <ref type="bibr" target="#b32">[33]</ref>, and the comparisons with the state-of-the-art techniques. In addition, we also introduce the experiments on the cow images from the VOC2006 database <ref type="bibr" target="#b33">[34]</ref>. This paper concludes in Section 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Various graphical models have been used for image segmentation. In particular, the Markov Random Field has been used for a long time <ref type="bibr" target="#b4">[5]</ref>, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b8">[9]</ref>. In the simplest case, the MRF model formulates the joint probability distribution of the image observation and the label random variables on the 2D regular lattice. It is therefore a generative model. According to the Bayes' rule, the joint probability can be decomposed into the product of the likelihood of the image observation and the prior distribution of the label random variables. An apriori Markovian field is normally assumed as the prior distribution, which normally encourages the adjacent labels to be the same (i.e., locally homogeneous). In order to reduce the computational complexity, the MRF model often assumes the observations to be conditionally independent given the label of each site.</p><p>The simple MRF model has been extended to more complex structures. In <ref type="bibr" target="#b5">[6]</ref>, the authors propose a multiscale hierarchical model. The label random variables at two adjacent layers form a Markov chain. The links between them enforce the consistency of the labels at adjacent layers. This model can partially model the long-range relationships between spatially faraway nodes. One problem of this model is due to its model structure. The commonly used quadtree model leads to the problem that spatially adjacent nodes may be far from each other in the quadtree structure. Another problem is that there are no direct interactions among adjacent labels at the same layer. These issues can lead to imprecise segmentation at the boundary <ref type="bibr" target="#b34">[35]</ref>.</p><p>Several works extend the idea of multiscale random fields to make it more powerful. Cheng and Bouman <ref type="bibr" target="#b35">[36]</ref> extend it by introducing a more complex transition conditional probability using a class probability tree. The advantage of using such an approach is that it can use a relatively larger size (e.g., 5 Â 5) of the neighborhood at the next coarser layer. By considering more parents, complex context information can be taken into account in the multiscale random fields. <ref type="bibr">Wilson and Li [37]</ref> also try to improve the interactions of the label fields at two adjacent layers. The neighborhood of a site is extended to include two types of sites: the parent sites at the next coarser layer and the adjacent sites at the same layer. In <ref type="bibr" target="#b37">[38]</ref>, Irving et al. alleviate the drawback of the quadtree model due to its fixed structure. Instead of assuming the nodes at one layer to be nonoverlapped, they propose an overlapping tree model where the sites at each layer correspond to overlapping parts in the image. This approach is also demonstrated to reduce the problem of imprecise segmentation.</p><p>The conditional independence assumption of the image observations in a MRF model is normally invalid for texture image segmentation. A double Markov Random Fields (DMRF) model is exploited <ref type="bibr" target="#b6">[7]</ref> in order to overcome this limitation. In this model, one MRF is used to represent the labeling process and another MRF is used to model the textured regions. With the DMRF model, the textured image segmentation can be performed by simultaneously estimating the MRF texture models and the MRF label fields. In <ref type="bibr" target="#b7">[8]</ref>, the authors propose a pairwise Markov Random Fields (PMF) to overcome the strong conditional independence assumption. They relax this assumption by directly assuming the joint random fields of the labels and the observations follow the Markovian property. Although the PMF model has weaker assumptions than the traditional MRF model, the authors shift the problem to directly model the joint probability, which is generally a difficult problem.</p><p>Differently from the MRF model, the CRF <ref type="bibr" target="#b26">[27]</ref> directly models the posteriori probability distribution of the label random variables, given the image observation. This posteriori probability is assumed to follow the Markovian property. The CRF is therefore a discriminative model that focuses on discriminating image observations at different sites. Since the CRF model does not try to describe the probability distribution of the observation, it may require fewer resources for training, as pointed out by <ref type="bibr" target="#b26">[27]</ref>, <ref type="bibr" target="#b38">[39]</ref>. Compared to the traditional MRF model, the CRF model relaxes the conditional independence assumption of the observations. The CRF model allows arbitrary relationships among the observations, which is obviously more natural in reality. The CRF model also makes the Markovian assumption of the labels conditioned on the observation. As a result, the CRF model can relax the apriori homogeneity constraint based on the observation. For example, this constraint may not be enforced where there is a strong edge. This characteristic makes the CRF model able to handle the discontinuity of the image data and labels in a natural way.</p><p>Several previous works have demonstrated the success of CRF models in image segmentation. He et al. <ref type="bibr" target="#b9">[10]</ref> have used CRF for segmenting static images. By introducing the hidden random variables, they can incorporate additional contextual knowledge (i.e., the scene context) to facilitate image segmentation. In <ref type="bibr" target="#b10">[11]</ref>, Ren et al. have used CRF for figure/ground labeling. They first oversegment the image into a set of triangles using the constrained Delaunay triangulation (CDT). A CRF model is then constructed based on these triangles. Their CRF model integrates various cues (e.g., similarity, continuity, and familiarity) for image segmentation by adding additional energy functions. This model, however, treats all hidden random variables as in the same layer and ignores the fact that they may come from different levels of abstraction. Our model is different from Ren et al.'s model because we model different image entities at the hierarchical layers and capture their natural causal relationships within this hierarchical framework.</p><p>Besides the simple CRF model, more complex CRF models have also been proposed. A hierarchical CRF model was proposed by Kumar and Hebert to exploit different levels of contextual information for object detection <ref type="bibr" target="#b39">[40]</ref>. This model can capture both pixelwise spatial interactions and relative configurations between objects. Another hierarchical tree-structured CRF model was developed in <ref type="bibr" target="#b11">[12]</ref>. These models are in spirit similar to the Multiscale Random Field model in <ref type="bibr" target="#b34">[35]</ref>. The main difference is that all links are now represented by the undirected links and the pairwise relationships are modeled by the potential functions conditioned on the image observation. In <ref type="bibr" target="#b40">[41]</ref>, the authors developed a complex CRF model for object detection. They introduced additional hidden layers to represent the locations of the detected object parts. The object parts are constrained by their relative locations with respect to the object center. Based on a model similar to <ref type="bibr" target="#b40">[41]</ref>, Winn and Shotton further introduced the layout consistency relationships among parts for recognizing and segmenting partially occluded objects in both 2D <ref type="bibr" target="#b41">[42]</ref> and 3D <ref type="bibr" target="#b42">[43]</ref> cases.</p><p>Although not as popular as those undirected graphical models (MRF or CRF), directed graphical models such as the Bayesian Network (BN) have also been exploited in solving computer vision problems <ref type="bibr" target="#b43">[44]</ref>, <ref type="bibr" target="#b44">[45]</ref>, <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b14">[15]</ref>. BN provides a systematic way to model the causal relationships among the random variables. It simplifies the modeling of a possibly complex joint probability distribution by explicitly exploiting the conditional independence relationships (known as prior knowledge) encoded in the BN structure. Based on the BN structure, the joint probability is decomposed into the product of a set of local conditional probabilities, which is much easier to specify because of their semantic meanings.</p><p>For image segmentation, the BN model can be used to represent the prior knowledge of the statistical relationships among different entities, such as the regions, edges, and their measurements. Several previous works have exploited BN for image segmentation. Feng et al. <ref type="bibr" target="#b12">[13]</ref> combine BN with Neural Networks (NN) for scene segmentation. They train neural networks as the classifiers that can produce the "scaled-likelihood" of the data. The BN models the prior distribution of the label fields. The local predictions of pixelwise labels generated from NN are fused with the prior to form a hybrid segmentation approach. Since Feng's BN model has the quadtree structure, it inherits the drawback of the quadtree model due to its fixed structure. In order to overcome such a problem, Todorovic and Nechyba <ref type="bibr" target="#b45">[46]</ref> develop a dynamic multiscale tree model that simultaneously infers the optimal structure as well as the random variable states. Although they show some successful experiments, their model is very complex and there are many random variables to be inferred. Good initialization is required for their variational inference approach. In <ref type="bibr" target="#b13">[14]</ref>, Mortensen and Jia proposed a semiautomatic segmentation technique based on a two-layer BN. Given a user-input seed path, they use the minimum-path spanning tree graph search to find the most likely object boundaries. In <ref type="bibr" target="#b14">[15]</ref>, Alvarado et al. use a BN model to capture all available knowledge about the real composition of a scene for segmenting a handheld object. Their BN model combines high-level cues such as the possible locations of the hand in the image to infer the probability of a region belonging to the object.</p><p>Besides those segmentation approaches based on either an undirected graphical model or a directed graphical model, several hybrid approaches have been previously proposed to combine different types of segmentation techniques. Huang et al. <ref type="bibr" target="#b46">[47]</ref> couple an MRF with a deformable model for image segmentation. To make the inference tractable, they require decoupling the model into two separate parts and using different techniques to perform inference in each part. The inference in the MRF part is performed using belief propagation, while the estimation of the deformable contour is based on the variational approaches. In contrast, our model unifies two types of graphical models (i.e., CRF and BN) and belief propagation can be applied to both models. Therefore, we can use a consistent inference approach in our model. Lee et al. <ref type="bibr" target="#b47">[48]</ref> combine Discriminative Random Field (DRF) <ref type="bibr" target="#b38">[39]</ref> with Support Vector Machines (SVMs) to segment brain tumors. They use the SVM to train a classifier that predicts a label based on the local feature. The local prediction is then combined with a prior distribution modeled by a DRF model for image labeling. In this approach, the two model parts (i.e., SVM and DRF) are not functioning at the same level. The DRF serves as the backbone of the whole model, while the SVM serves as the local classifier to collect image features for labeling. Moreover, the two parts are operated under different principles and cannot be unified in a consistent way.</p><p>As discussed before, the undirected graphical model (e.g., MRF and CRF) and the directed graphical model (e.g., BN) are suitable for representing different types of statistical relationships among the random variables. Their combination can create a more powerful and flexible probabilistic graphical model that can easily model various apriori knowledge to facilitate image segmentation. This is the basic motivation of our work in this paper.</p><p>Little previous work focuses on modeling image segmentation by unifying different types of graphical models. Liu et al. <ref type="bibr" target="#b48">[49]</ref> combine a BN with a MRF for image segmentation. A naive BN is used to transform the image features into a probability map in the image domain. The MRF enforces the spatial relationships of the labels. The use of a naive BN greatly limits the capability of this method because it is hard to model the complex relationships between the label random variables and the image measurements using a naive BN. In contrast, we use a hierarchical BN to capture the complex causalities among multiple image entities and their measurements, as well as the local constraints. Murino et al. <ref type="bibr" target="#b49">[50]</ref> formulate a Bayesian Network of Markov Random Field model for image processing. A BN is used to represent the apriori constraints between different abstraction levels. A coupled MRF is used to solve the coupled restoration and segmentation problem at each level. This approach only uses the BN to model the set of apriori constraints between the same entities at different levels. The image information is only exploited in the MRF model for inferring the hidden random variables, while, in our unified model, we exploit image measurements both in the CRF part and in the BN part to perform image segmentation based on two complementary principles: the region-based segmentation and the edge-based segmentation. In <ref type="bibr" target="#b50">[51]</ref>, Kumar et al. combined an MRF with a layered pictorial structures (LPS) model for object detection and segmentation. The LPS model represents the global shape of the object and restrains the relative location of different parts of the object. They formulate the LPS model using a fully connected MRF. The whole model is therefore an extended MRF model, which is different from our unified model which combines different types of graphical models. Hinton et al. <ref type="bibr" target="#b51">[52]</ref> studied the learning issue for a hybrid model. Their hybrid model differs from ours in several aspects. First, Hinton et al.'s model is constructed by connecting several MRFs at different layers using directed links. The configuration of a top-level MRF provides the biases that influence the configuration of the next level MRF through the directed links, while, in our model, the directed links capture the causalities among multiple image entities and the undirected links capture the spatial correlation conditioned on the observation. Second and most important, Hinton et al. exploit an approximation of the true posterior probability distribution of the hidden nodes by implicitly assuming the posterior of each hidden node is independent of each other. In contrast, we derive the factored joint probability distribution using the global Markov property based on the graphical model structure, and therefore, do not have such an assumption as Hinton et al.'s. Third, based on their approximation, Hinton et al. apply variational approach to approximately perform inference and parameter learning. In contrast, we convert our model into a factor graph to perform inference as well as learning through principled factor graph inference.</p><p>Compared to the aforementioned works, our unified model differs from them in several aspects. First, our model can capture both the causal and noncausal relationships that extensively and naturally exist among different image entities. Second, our unified model consists of two parts: a CRF part and a BN part. It can therefore systematically combine the region-based image segmentation (i.e., the CRF part) with the edge-based image segmentation (i.e., the BN part). Finally, via the factor graph, we can perform image segmentation under the unified framework through a consistent probabilistic inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">OVERVIEW OF THE APPROACH</head><p>The structure of the proposed graphical model is illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>. It consists of two parts: the CRF part and the BN part. The CRF part in Fig. <ref type="figure" target="#fig_0">1a</ref> performs region-based image segmentation, while the BN part performs edge-based segmentation. The two parts are connected through the region nodes fy i g.</p><p>Given an image, it is first oversegmented to produce an edge map. We can use any suitable segmenter to produce this oversegmentation. Specifically, we use the Edgeflowbased anisotropic diffusion method <ref type="bibr" target="#b52">[53]</ref> for this purpose. The region node y i correspond to each oversegmented region (referred to as superpixel thereafter) in the edge map. We assume that the superpixel region nodes y ¼ fy i g and the image observation x form a CRF. They are used to construct the CRF part of the model. Conditioned on the image observation x, the region nodes y follow the Markovian property, i.e., P ðy i jfy j g j6 ¼i ; xÞ ¼ P ðy i jN ðy i Þ; xÞ, where N ðy i Þ represents the spatial neighborhood of y i .</p><p>The CRF image segmentation can be thought of as a labeling problem, i.e., to assign a label to the ith superpixel. In the figure/ground image segmentation problem, the node y i has two possible labels, i.e., þ1 (the foreground) and À1 (the background). Let fy i g n i¼1 be the label random variables corresponding to all superpixels, where n is the total number of superpixels in an image. fx i g n i¼1 are the corresponding image measurements. x i is represented as a local feature vector extracted from the image. Different types of cues such as intensity, color, and textures can be included in this feature vector for image segmentation. Using the CRF model, we can infer the label for each superpixel from image measurements.</p><p>While the CRF model can effectively capture the spatial relationships among image entities, they cannot capture other causal relationships that naturally exist among different image entities. Due to this reason, a multilayer BN is constructed to capture the causalities among the regions, edges, vertices (or junctions), and their measurements. In Fig. <ref type="figure" target="#fig_0">1a</ref>, the e nodes represent all the edge nodes in the constructed BN model. These edge nodes correspond to the edge segments in the edge map. The v nodes represent all the vertex nodes. They are automatically detected from the edge map (see more details in Section 5). The nodes M e and M v represent the measurements of edges and vertices, respectively. Given the BN model, the goal is to infer the edge labels from various image measurements.</p><p>Combining the CRF model with the BN model yields a unified probabilistic graphical model that captures both the causal and noncausal relationships, as shown in Fig. <ref type="figure" target="#fig_0">1a</ref>. The unified graphical model is further converted into a Factor Graph representation for performing the joint inference of image labels. Based on the Factor Graph theory, principled algorithms such as the sum-product algorithm and the maxproduct algorithm can be used to perform consistent inference in the unified model. We therefore formulate a unified graphical model that can exploit both the regionbased information and the edge-based information, and more importantly, the causal and noncausal relationships among random variables for image segmentation. Specifically, in Fig. <ref type="figure" target="#fig_0">1a</ref>, the region nodes fy i g act as the parents of an edge node. The parents of the edge node correspond to the two regions that intersect to form this edge. The links between the parents and the child represent their causal relationships. If the parent region nodes have different labels, it is more likely that there is an object boundary (i.e., e j ¼ 1) between them. In this way, the proposed model systematically combines the CRF model and the BN model in a unified probabilistic framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">REGION-BASED IMAGE SEGMENTATION USING CONDITIONAL RANDOM FIELD</head><p>Our CRF model is a superpixel-based model. We choose the superpixel-based model because the relationship between two regions naturally provides the clue for inferring the state of the edge between them. In addition, the superpixel CRF model reduces the computational problem that is a common issue in the undirected graphical model. Using the CRF model, we want to decide if the superpixels representing the oversegmented regions should be merged. As shown in Fig. <ref type="figure" target="#fig_2">2</ref>, the image is first oversegmented into superpixel regions. Each superpixel region is a relatively homogenous region and it corresponds to one region node in the CRF model. Based on the oversegmentation, we automatically detect the topological relationships among these region nodes. The CRF model is then constructed based on these topological relationships. For the computational concern, we only consider the pairwise relationships among the region nodes. If two superpixel regions are adjacent to each other in the oversegmentation, an undirected link will be added between their corresponding nodes in the CRF model. This link means that there is an interaction between them, which is represented by the pairwise potential. From the example in Fig. <ref type="figure" target="#fig_2">2</ref>, it is obvious that different region nodes may have a different number of neighbors, which means their interactions with other nodes can be stronger or weaker.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">CRF Model</head><p>Our CRF model directly models the posteriori probability distribution of the region nodes y as</p><formula xml:id="formula_0">P ðyjxÞ ¼ 1 Z Y i2V ðy i ; x i Þ Y i2V Y j2N i expðy i y j T g ij ðxÞÞ;<label>ð1Þ</label></formula><p>where V is the set of all superpixel region nodes and y is the joint labeling of all region nodes. N i denotes the neighborhood of the ith region, which is automatically detected from the topological relationships. is the parameter vector. g ij ðÁÞ represents the feature vector for a pair of nodes i and j. The symbol Z denotes the normalization term (i.e., the partition function).</p><p>There are two parts in <ref type="bibr" target="#b0">(1)</ref>. The first part, ðy i ; x i Þ, is the unary potential, which tries to label the ith node according to its local features. It indicates how likely it is that the ith node will be assigned the label y i given the local features x i . For this purpose, we use a discriminative classifier based on a multilayer perceptron (MLP). We actually use a three-layer perceptron classifier in this work. Let netðx i Þ denotes the output of the perceptron when the feature vector x i is the input. The output is further converted into a probabilistic interpretation using a logistic function,</p><formula xml:id="formula_1">ðy i ; x i Þ ¼ 1 1 þ exp À À y i netðx i Þ Á ;<label>ð2Þ</label></formula><p>where is a constant that can adjust the curve of the logistic function. Similar definitions of the unary potentials were first proposed in <ref type="bibr" target="#b53">[54]</ref> and have been used in <ref type="bibr" target="#b9">[10]</ref>, <ref type="bibr" target="#b38">[39]</ref>. The three-layer perceptron is automatically trained with a set of training data.</p><p>The second part expðy i y j T g ij ðxÞÞ in ( <ref type="formula" target="#formula_0">1</ref>) is the pairwise potential. It can be seen as a measure of how the adjacent region nodes y i and y j should interact with each other, given the measurements x. We use a log-linear model to define this pairwise potential, which depends on the inner product of the weight vector and the pairwise feature vector g ij ðxÞ. The weight vector will be learned during a training process (see more details in Section 4.2). The pairwise feature vector g ij ðxÞ can be defined based on the whole image measurements to consider the arbitrary relationships among the observations. For simplicity, it is currently defined based on the difference of the feature vectors x i and x j . An additional bias term (fixed as 1) is also added into the feature vector g ij ðxÞ. The feature vector g ij ðxÞ is defined as g ij ðxÞ ¼ ½1; jx i À x j j T , where T is the transpose of a vector. The operator j Á j represents the absolute value of each component in the difference between x i and x j . Similar pairwise potentials have been used in <ref type="bibr" target="#b38">[39]</ref>.</p><p>The pairwise potential expðy i y j T g ij ðxÞÞ will have different values when the labels y i and y j are same or different. Moreover, when the measurements of the ith and jth nodes are same, the pairwise potential will only depend on the bias term. The bias term determines how the model prefers the neighboring nodes to have same or different labels. If the measurements at nodes i and j are significantly different, the pairwise potential will depend on the difference of measurements and the corresponding weights. This definition has incorporated the data adaptive capability, which is important for segmenting regions with discontinuity (e.g., the regions near strong edges).</p><p>The partition function Z in (1) can be calculated by summing out all the possible configurations of y, i.e., </p><formula xml:id="formula_2">Z ¼ X y exp X i2V logðy i ; x i Þ þ X j2N i y i y j T g ij<label>ðxÞ</label></formula><p>Direct calculation of the partition function in (3) is computationally difficult. However, the Bethe free energy <ref type="bibr" target="#b54">[55]</ref> can be used to approximate the logarithm of the partition function and to alleviate this computational problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Parameter Estimation</head><p>The three-layer perceptron classifier and the parameter vector of the pairwise potential are automatically learned from the training data. Assuming that we have x ð1Þ ; x ð2Þ ; . . . ; x ðKÞ training images and their ground truth labeling y ð1Þ ; y ð2Þ ; . . . ; y ðKÞ , where K is the number of training images, the aim of parameter estimation is to automatically learn the three-layer preceptron classifier and the parameter vector from these data. First, we train the three-layer perceptron classifier. The structure of our three-layer perceptron depends on the dimension of the input feature vector and the number of available training data. In the training stage, the target output of the three-layer perceptron is þ1 (the foreground) or À1 (the background). Given the input and the desired output, the three-layer perceptron is trained using the standard BFGS quasi-Newton backpropagation method.</p><p>Next, we fix the three-layer perceptron classifier and use the conditional Maximum Likelihood Estimation (MLE) method to learn the parameter vector . Assuming all of the training data are identically independently sampled, the log-likelihood of the parameter is calculated as</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LðÞ</head><formula xml:id="formula_4">¼ X K k¼1 X i2V log À y ðkÞ i ; x ðkÞ i Á þ X j2N i y ðkÞ i y ðkÞ j T g ij À x ðkÞ Á 2 4 3 5 À z ðkÞ 8 &lt; : 9 = ; ;<label>ð4Þ</label></formula><p>where z ðkÞ is the log-partition function, i.e., z ðkÞ ¼ logZ ðkÞ . As mentioned before, this log-partition function can be approximated by the Bethe free energy.</p><p>The optimal parameters Ã are estimated according to the MLE estimation, i.e.,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ã ¼ arg max</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LðÞ ¼ arg min</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ÀLðÞ: ð5Þ</head><p>We use the stochastic gradient descent method <ref type="bibr" target="#b54">[55]</ref> to find the optimal parameters Ã in <ref type="bibr" target="#b4">(5)</ref>. The gradient of the log-likelihood LðÞ is calculated as follows:</p><formula xml:id="formula_5">@LðÞ @ ¼ X K k¼1 " X i X j2N i y ðkÞ i y<label>ðkÞ</label></formula><formula xml:id="formula_6">j g ij À x ðkÞ Á À E P ðyjx ðkÞ ;Þ X i X j2N i y i y j g ij À x ðkÞ Á !# ;<label>ð6Þ</label></formula><p>where E P ½Á denotes the expectation with respect to the distribution P . For example,</p><formula xml:id="formula_7">E P ðyjx ðkÞ ;Þ X i X j2N i y i y j g ij À x ðkÞ Á ! ¼ X y P À yjx ðkÞ ; Á X i X j2N i y i y j g ij À x ðkÞ Á :<label>ð7Þ</label></formula><p>The summation in <ref type="bibr" target="#b6">(7)</ref> is performed over all possible configurations of the region nodes y.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Labeling Inference</head><p>When all of the parameters are known, the CRF model can be used to infer the region labels corresponding to the superpixel regions. The optimal labeling can be found by the Maximum Posterior Marginal (MPM) criterion <ref type="bibr" target="#b55">[56]</ref>. Each node i is assigned a label that maximizes its marginal posteriori probability, i.e.,</p><formula xml:id="formula_8">y Ã i ¼ arg max yi2f1;À1g P ðy i jx; Þ:<label>ð8Þ</label></formula><p>The marginal probability P ðy i jx; Þ is calculated by the sum-product loopy belief propagation (LBP) <ref type="bibr" target="#b29">[30]</ref>, <ref type="bibr" target="#b56">[57]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EDGE-BASED IMAGE SEGMENTATION USING BAYESIAN NETWORK</head><p>The CRF naturally models the region-based image segmentation. For the edge-based image segmentation, we use a multilayer BN to model it. Bayesian Network <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b29">[30]</ref> is a directed acyclic graph (DAG) that consists of a set of random variables and a set of directed links between these random variables. Each variable (node) has a set of mutually exclusive states. The directed links represent the causal dependence between the random variables. The probability distribution of a node is defined by its conditional probability distribution, given the states of its parents. With a BN, the joint probability of a set of random variables can be factored into the product of a set of local conditional probabilities that are easier to compute/estimate. We use the BN to explicitly capture the causal relationships that naturally exist among image entities such as regions, edges, vertices, and their image measurements. For example, intersections of regions naturally produce edges, while interactions of edges yield vertices. The BN provides a probabilistic framework that can systematically combine the image observations and various causal relationships so that image segmentation can be performed through a principled probabilistic inference. Besides, the BN provides a direct analogy to the human reasoning process. It can straightforwardly represent the causalities that have been extensively exploited by human. This advantage is unique for the BN compared to those undirected graphical models (e.g., CRF and MRF).</p><p>The whole structure of our BN model consists of multiple layers. Each layer will be described in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Region, Edge, and Vertex Nodes</head><p>We build the BN model based on an oversegmented edge map that is also used to construct the superpixel CRF model in Section 4. The edge map consists of edge segments fe j g m j¼1 and vertices fv t g l t¼1 , where m is the number of edges and l is the number of vertices. In this work, a vertex is the place where three or more edges intersect, i.e., a junction.</p><p>The BN model represents the relationships among the superpixel region nodes fy i g, the edge segments fe j g, and the vertices fv t g. We use a synthetic edge map in Fig. <ref type="figure">3a</ref> to explain how to construct the BN model. The basic BN model consists of three layers as shown in Fig. <ref type="figure">3b</ref>. Specifically, the region node layer contains all of the superpixel region nodes. The edge node layer contains all the edge segments. The vertex node layer contains all vertices that are the intersections of edges.</p><p>The parents of an edge node are the two regions that intersect to form this edge. If the parents of an edge e j have different labels, it is more likely that there is a true object boundary between them, i.e., e j ¼ 1. The relationship between the edge node e j and its parent region nodes paðe j Þ is defined by the conditional probability P ðe j jpaðe j ÞÞ. The conditional probability P ðe j jpaðe j ÞÞ is defined as follows: </p><formula xml:id="formula_9">P ðe j ¼</formula><p>This definition basically means that the edge segment e j has a high probability of being a true boundary when the two adjacent regions are assigned different region labels. The edge nodes and the vertex nodes are causally linked, too. The parents of a vertex node are those edges that intersect to form this vertex. Each edge node is a binary node. Its true state represents that this edge segment belongs to the object boundary. The vertex node also assumes binary values (true or false) and it is true if the vertex is actually a corner on the object boundary.</p><p>Both the edge nodes and the vertex nodes have image measurements (i.e., the shaded nodes in Fig. <ref type="figure">3b</ref>). We can use complex edge features (e.g., edgelet) or the edge probability estimated by other approaches as the edge measurements. For simplicity, we use the average gradient magnitude as the edge measurement in this work. We denote the measurement of the edge node e j as M ej . The measurement nodes fM ej g are continuous nodes. The conditional probability P ðM ej je j Þ is parameterized using Gaussian distributions defined with mean and variance 2 , which are learned from the training data.</p><p>Similarly, each vertex node is also associated with an image measurement. The M vt node in Fig. <ref type="figure">3b</ref> is the measurement of a vertex node v t . We use the Harris corner detector <ref type="bibr" target="#b57">[58]</ref> to calculate the measurement. Let Iðr; cÞ denote the corresponding gray-scale image. The Harris matrix A is given by A ¼ @I @r 2 @I @r @I @c @I @r @I @c @I @c </p><p>where r and c denote the row and column coordinates, respectively. Given the Harris matrix A, the strength of a corner is determined by a corner response function</p><formula xml:id="formula_12">R ¼ detðAÞ À k Á traceðAÞ 2 ,</formula><p>where k is set as the suggested value 0.04 <ref type="bibr" target="#b57">[58]</ref>.</p><p>The vertex measurement M v t is currently discretized according to the corner response R. If the corner response R is above a threshold (fixed as 1,000) and it is a local maximum, a corner is detected and the measurement node M vt becomes true. If no corner is detected at the location of the vertex v t , the measurement node M v t becomes false. The conditional probability P ðM vt jv t Þ quantifies the statistical relationship between the vertex node v t and its measurement M v t . It describes the uncertainty between the state of a vertex and its measurement. This conditional probability is defined based on the empirical distribution of the measurements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Local Smoothness Constraint</head><p>In the real world, the boundary of a natural object is locally smooth. We enforce the local smoothness constraint in our BN model by penalizing sharp corners between the intersecting edges. A sharp corner is defined as an angle between two intersecting edges that is less than 6 . In order to impose this constraint, the angular node ! j;s is introduced to model the relationship between two adjacent edges e j and e s . The parent nodes (e j and e s ) of the angular node ! j;s correspond to the edge segments that intersect to form this angle. The angular node ! j;s is a binary node, with its true state meaning that the local smoothness constraint is violated by these two edges. Fig. <ref type="figure">4</ref> illustrates how the angular nodes are added into the BN model. The measurement M ! j;s of an angle node is currently discretized according to a small angle. If the angle ! j;s is smaller than 6 , the measurement node becomes 1 (true). The conditional probability table (CPT) between an angular node and its measurement can be set according to the empirical distribution of angle measurements.</p><p>To enforce the smoothness constraint, a CPT is defined to specify the relationship between the angular node ! j;s and the edges e j and e s that intersect to form this angle, i.e., P ð! j;s ¼ 1je j ; e s Þ ¼ 0:2; if both e j and e s are 1; 0:5; otherwise:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>&amp; ð11Þ</head><p>This conditional probability definition effectively reduces the probability that both e j and e s are true boundary edges if the angle between them is too small (i.e., ! j;s ¼ 1). In other words, it penalizes the existence of a sharp corner in the object boundary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Connectivity Constraint</head><p>In general, the boundary of an object should be simply connected, i.e., an edge segment should connect with at most one edge segment at its end points. This constraint is imposed by defining a CPT between the edge nodes and the related vertex node as follows:</p><formula xml:id="formula_13">P ðv t ¼ 1jpaðv t ÞÞ ¼ 1;</formula><p>if exactly two parent edge nodes are true; 0:3; if none of the parent edge nodes is true; 0; otherwise;</p><formula xml:id="formula_14">8 &gt; &lt; &gt; :<label>ð12Þ</label></formula><p>where paðv t Þ denotes all of the parent edge nodes of the vertex node v t . If a corner is detected, the measurement M vt becomes true. The vertex node v t will have a high probability of being true. In such a case, it is most likely that exactly two parent edge nodes are true (i.e., on the boundary), which corresponds to the first case of the CPT definition in <ref type="bibr" target="#b11">(12)</ref>. This implies the simple connectivity of edges at this vertex. In the second case of the CPT definition in <ref type="bibr" target="#b11">(12)</ref>, we set the entry 0.3 to account for the case when corners are detected in the background. In such a case, it is possible that none of the parent edge segments is the true object boundary. However, the conditional probability for this case shall be smaller than the case that exactly two parent edge nodes are true.</p><p>Given the CPT definition in <ref type="bibr" target="#b11">(12)</ref>, the connectivity constraint is imposed into the BN model because it favors the case that exactly two intersecting edges are the true object boundaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Bayesian-Network-Based Image Segmentation</head><p>The complete BN model for the synthetic example in Fig. <ref type="figure">3a</ref> is shown in Fig. <ref type="figure" target="#fig_6">5</ref>. Based on the BN model, the goal of image segmentation is to infer the states of the edge nodes fe j g m j¼1 , given various measurements and constraints.</p><p>Let e represent all the edge nodes fe j g m j¼1 and y represent all the region nodes fy i g n i¼1 . Similarly, ! represents all the angular nodes f! j;s g and v represents all the vertex nodes fv t g l t¼1 .</p><p>Let M e represent all of the measurements for the edge nodes. M v represents all of the measurements for the vertex nodes and M ! represents all the measurements for the angular nodes. Image segmentation can be performed by searching for the most probable explanation (MPE) of all hidden nodes in the BN model given various measurements, i.e., e Ã ; y Ã ; ! Ã ; v Ã ¼ arg max </p><p>In the MPE results, the edge nodes with true states form the object boundary that we are looking for.</p><p>We can calculate the joint probability of all nodes in the BN model as follows: </p><p>where paðe j Þ denotes the parent nodes of e j . paðv t Þ denotes the parent nodes of v t . j denotes the set of edges that intersect with the edge e j . The factorization of this joint probability is based on the conditional independence relationships among all nodes, which are implied by the constructed BN model. Among the factored probabilities, P ðy i Þ is the prior probability of the region nodes. Without additional prior information, it is modeled as a uniform distribution. Other terms in ( <ref type="formula" target="#formula_16">14</ref>) are already defined in Sections 5.1, 5.2, and 5.3. Given all of these terms, the joint probability in ( <ref type="formula" target="#formula_16">14</ref>) can be calculated. The most probable states of the edge nodes can be found using a probabilistic inference approach to find the MPE solution. Specifically, the Junction Tree method <ref type="bibr" target="#b28">[29]</ref> is used to find the exact MPE solution in the BN model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Parameter Estimation</head><p>Since each node in a BN is conditionally independent of other nodes that are not within its Markov blanket <ref type="bibr" target="#b28">[29]</ref>, we can locally learn the conditional probability distributions (CPDs) of each node. For example, we can learn the Gaussian distributions of the edge measurements, i.e., the likelihood model P ðM e j je j Þ. Given the training images and their manual labeling, we classify the edge segments in the manually labeled images into two sets: the boundary edges and the nonboundary edges. We fit Gaussian distributions for the edge measurements in each set to learn P ðM e j je j Þ. In a similar way, we also learned the likelihood models of other measurements (i.e., P ðM vt jv t Þ and P ðM !j;s j! j;s Þ). The learned conditional probabilities are then used in <ref type="bibr" target="#b13">(14)</ref> to calculate the joint probability.</p><p>It is possible to learn the remaining CPDs in a similar way. However, the training on a specific data set tends to skew the BN model only for that specific set of training data. Such a trained BN model cannot generalize well to other unseen data. As a result, we opt for a soft BN parameterization instead of a hard BN parameterization. We empirically set the fixed values for some conditional probability parameters (as shown in previous equations) due to several considerations. First, we can directly define those CPDs according to the semantic meaning of their conditional probabilities. Second, some previous work <ref type="bibr" target="#b58">[59]</ref> shows the performance of BNs for diagnosis is not very sensitive to the accurate parameter setting. Third, we have changed the CPDs (e.g., P ðe j jpaðe j ÞÞ) for all edge nodes within a range of AE10 to 20 percent relative to the preset values. The segmentation results did not change very much, which agrees with the observations in <ref type="bibr" target="#b58">[59]</ref>. In Section 7.1, we will show a set of experiments to demonstrate this phenomenon. Fourth, we applied the model using this parameterization strategy on different data sets and found it generally performed well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">A UNIFIED GRAPHICAL MODEL COMBINING THE CRF MODEL WITH THE BN MODEL</head><p>The complete segmentation model unifies the CRF part and the BN part through the causal relationships between the region nodes y and the edge nodes e, as shown in Fig. <ref type="figure" target="#fig_7">6</ref>. The unified graphical model in Fig. <ref type="figure" target="#fig_7">6</ref> consists of both directed links and undirected links. To perform a consistent inference, it is necessary to convert the unified model into a Factor Graph (FG) representation <ref type="bibr" target="#b59">[60]</ref>, <ref type="bibr" target="#b60">[61]</ref> since it is very difficult to directly perform inference in such a graphical model. A factor graph is a bipartite graph that expresses the structure of the factorization of a global function over a set of variables. The FG consists of two types of nodes: the variable nodes and the factor nodes. The variable node corresponds to a random variable, while the factor node represents the factored local function. There is an edge connecting a variable node to a factor node if and only if the variable is an argument of the factored function. Following the convention, each variable node will be represented as a circle and each factor node will be represented as a filled square in the factor graph.</p><p>Since both the undirected graphical model (e.g., MRF) and the directed graphical model (e.g., BN) represent the factored joint probability distribution of a set of variables, they can be easily converted into a factor graph representation <ref type="bibr" target="#b59">[60]</ref>, <ref type="bibr" target="#b60">[61]</ref>, <ref type="bibr" target="#b30">[31]</ref>. Fig. <ref type="figure" target="#fig_9">7a</ref> is a BN that represents a joint probability distribution P ðx 1 ; x 2 ; x 3 Þ ¼ P ðx 1 ÞP ðx 2 jx 1 ÞP ðx 3 jx 1 ; x 2 Þ. Based on this factorization, a factor graph can be constructed to model the same distribution, as shown in Fig. <ref type="figure" target="#fig_9">7b</ref>. The variables in the FG correspond to those variables in the BN. Factor nodes are added to correspond to the factored probabilities in the joint probability. Edges are added to link a factor node and a variable node if and only if the factor is a function of this variable. We can convert an undirected graphical model into a factor graph in a similar way. Fig. <ref type="figure" target="#fig_9">7c</ref> is a simple MRF that represents a joint distribution P ðx 1 ; x 2 ; x 3 ; x 4 Þ ¼ ðx 1 ; x 2 Þðx 1 ; x 3 Þðx 3 ; x 4 Þðx 2 ; x 4 Þ, where the normalization constant can be merged into one factor such as ðx 1 ; x 2 Þ. Based on this factorization, a factor graph is constructed in a similar way to model the same distribution, as shown in Fig. <ref type="figure" target="#fig_9">7d</ref>. Each factor node corresponds to the factored potential function.</p><p>Based on the graphical structure of the unified model in Fig. <ref type="figure" target="#fig_7">6</ref>, we can factorize the joint probability distribution of all the variables according to the global Markov property in the graphical model (cf. <ref type="bibr" target="#b27">[28,</ref><ref type="bibr">Chapter 3]</ref>). According to the graphical structure, x are conditionally independent of other random variables given y. Hence, we have </p><formula xml:id="formula_17">P ðyjxÞP ðxÞ ¼ 1 Z Z Y m j¼1 P À M ej je j Á Y m j¼1;s2j P À M !j;s j! j;s Á Y l t¼1 P À M vt jv t Á Y m j¼1;s2 j P ð! j;s je j ; e s Þ Y l t¼1 P ðv t jpaðv t ÞÞ Y m j¼1 P ðe j jpaðe j ÞÞ Y i2V ðy i ; x i Þ Y i2V Y j2N i expðy i y j T g ij ðxÞÞ;<label>ð15Þ</label></formula><p>where Z Z is the normalization constant. Note that P ðxÞ is a constant since x are observed. It can therefore be merged into the normalization constant Z Z. Among these factored functions, P ðM e j je j Þ, P ðM ! j;s j! j;s Þ and P ðM v t jv t Þ are the likelihood models of the measurements of edges, angles, and vertices, respectively. P ð! j;s je j ; e s Þ, P ðv t jpaðv t ÞÞ, and P ðe j jpaðe j ÞÞ are the conditional probabilities of angular nodes, vertex nodes, and edge nodes, respectively. All of these conditional probabilities are defined in the BN part of the unified model (Section 5). The remaining factored functions ðy i ; x i Þ and expðy i y j T g ij ðxÞÞ are the potential functions defined in the CRF part of the unified model (Section 4). Based on the factored joint probability distribution in <ref type="bibr" target="#b14">(15)</ref>, we convert the unified model into a factor graph representation in order to perform joint inference. The converted FG is shown in Fig. <ref type="figure" target="#fig_11">8</ref>, where each factor node corresponds to one factored function in the above equation.</p><p>Given the factor graph representation, there are different principled ways to perform probabilistic inference. First, the sum-product algorithm can be used to efficiently calculate various marginal probabilities for either a single variable or a subset of variables <ref type="bibr" target="#b59">[60]</ref>, <ref type="bibr" target="#b30">[31]</ref>. The sum-product algorithm operates via a local "message passing" scheme. It can perform exact inference of the marginal probabilities for singly connected graphs. Let R denotes all the variables in the above factor graph and r i 2 R is one of the variable. We can use the sum-product algorithm to calculate the marginal probability of r i , i.e.,</p><formula xml:id="formula_18">P ðr i Þ ¼ X Rnri P ðRÞ;</formula><p>where the summation is over all variables excluding r i . P ðRÞ is the joint probability in <ref type="bibr" target="#b14">(15)</ref>. Given the marginal probability of each variable, the optimal state of the variable can be found by the MPM criterion <ref type="bibr" target="#b55">[56]</ref>, i.e.,</p><formula xml:id="formula_19">r Ã i ¼ arg max ri P ðr i Þ:</formula><p>Second, the max-product algorithm <ref type="bibr" target="#b30">[31]</ref> can be used to find a setting of all of the variables that corresponds to the largest joint probability, i.e.,</p><formula xml:id="formula_20">R Ã ¼ arg max R P ðRÞ:</formula><p>This optimal solution has the same meaning as the MPE solution mentioned in the Bayesian Network inference (Section 5.4). The max-product algorithm works identically to the sum-product algorithm except that the summation in the sum-product algorithm is replaced by the maximization when we calculate the messages.</p><p>Third, besides the max-product algorithm, there are other algorithms that can also find the MPE solution given the evidence. The stochastic local search (SLS) <ref type="bibr" target="#b61">[62]</ref> is one such algorithm. In <ref type="bibr" target="#b62">[63]</ref>, Hutter et al. improve Park's algorithm <ref type="bibr" target="#b61">[62]</ref> to achieve a more efficient algorithm for MPE solving. They also extend this algorithm and provide a public available software to deal with various types of graphical models, including the factor graph. Given the FG model in Fig. <ref type="figure" target="#fig_11">8</ref> where the joint probability is calculated by <ref type="bibr" target="#b14">(15)</ref>. In the MPE solution, the region nodes with the foreground labels form the final segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Figure/Ground Image Segmentation</head><p>We first tested the proposed model for figure/ground image segmentation using the Weizmann horse data set <ref type="bibr" target="#b31">[32]</ref>. This data set includes the side views of many horses that have different appearances and poses, which makes it challenging to segment these images. On the other hand, several related works <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b63">[64]</ref>, <ref type="bibr" target="#b64">[65]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b65">[66]</ref>, <ref type="bibr" target="#b10">[11]</ref> also did experiments on this data set. We can compare our results with these state-of-the-art works.</p><p>Our approach requires a learning process to train the model. For this purpose, we used 60 horse images as the training data. The test images include 120 images from the Weizmann horse data set. Compared to the training images, the foreground horses and the background scenes in the test images are more complex. The appearances of the horses have a much larger range of variations. The background includes more different kinds of scenes, many of which have never been seen in the training set. Due to these reasons, the segmentation is a challenging problem.</p><p>We have performed the experiments using both color images and gray-scale images of the same training set and the testing set. It demonstrates that our approach is flexible enough to segment different types of images. Different features have been used in the region-based CRF part for segmenting the color images and the gray-scale images. For the color images, we use the average CIELAB color and their standard deviations as the local features x i for each superpixel region. In this case, the length of the feature vector is 6. The three-layer perceptron has a structure with 6 nodes in the input layer, 35 nodes in the hidden layer, and 1 node in the output layer.</p><p>For the gray-scale images, we use the average intensity and 12 Gabor textures as the features for each superpixel region. The Gabor textures are calculated by filtering the gray-scale image with a set of Gabor filter banks. The average magnitude of the filtered image in each superpixel region is used as the Gabor feature. We use the Gabor filter banks with three scales and four orientations. In this case, the length of the feature vector x i is 13. The three-layer perceptron has a structure with 13 nodes in the input layer, 25 nodes in the hidden layer, and 1 node in the output layer. We are using fewer hidden nodes because the number of nodes in the input layer is increased but the total number of training data remains the same.</p><p>All of the training images and the test images are first oversegmented using the Edgeflow-based anisotropic diffusion method. Given the training images and their ground truth labeling, we automatically train the unified graphical model using the process described in the Sections 4.2 and 5.5.</p><p>After learning the model, we perform image segmentation on the test images using the inference process described in Section 6. Fig. <ref type="figure" target="#fig_12">9</ref> shows some examples of the color horse images and their segmentation masks. Fig. <ref type="figure" target="#fig_0">10</ref> shows examples of the gray-scale horse images and their segmentation masks. We achieved encouraging results on these images. Most small errors happen on the horse's feet, where the appearances of these parts are different from the horse's body. Another kind of error is caused by the clutter. When the background (e.g., the shadow) has a similar appearance as the foreground, the proposed model may not be able to completely separate them.</p><p>We first qualitatively compare our segmentation results with some results produced by other state-of-the-art approaches. Cour and Shi <ref type="bibr" target="#b63">[64]</ref> segment an image by finding the optimal combination of the superpixel regions in an oversegmentation produced by the Normalized Cuts <ref type="bibr" target="#b2">[3]</ref>. The optimal segmentation in their approach shall have a similar shape as the shape template that is generated from the manually labeled training data. Borenstein et al. <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b22">[23]</ref> combine the top-down and bottom-up process to perform image segmentation. Levin and Weiss <ref type="bibr" target="#b64">[65]</ref> propose a learning-based approach for image segmentation. They perform image segmentation based on matching patch templates with the images. Winn and Jojic <ref type="bibr" target="#b19">[20]</ref> learn the object class model from the unlabeled images. Their approach combines the bottom-up cues of color and edge with top-down cues of shape and pose. Zhu et al. <ref type="bibr" target="#b65">[66]</ref> propose an unsupervised structure learning method to learn the hierarchical compositional model for deformable objects and apply it to segment articulated objects. All of these works have been tested on (a subset of) the Weizmann horse data set. Fig. <ref type="figure" target="#fig_0">11</ref> shows several example segmentation results of these approaches on color images. Our results can compete with these results according to the visual inspection.</p><p>In order to quantitatively evaluate our segmentation results and compare with the aforementioned approaches, we calculate the average percentage of correctly labeled pixels (i.e., segmentation consistency <ref type="bibr" target="#b66">[67]</ref>) in all test images. The quantitative results are summarized in Table <ref type="table" target="#tab_1">1</ref>. In this table, we also list the segmentation consistency achieved by other related works.</p><p>From the quantitative results in Table <ref type="table" target="#tab_1">1</ref>, we conclude that our results are comparable to (or better than) the results produced by other state-of-the-art approaches. Note that we only use very simple image features (e.g., the color) for segmentation and have not performed any feature selection. Besides, we have not utilized the additional object shape information as some works have done <ref type="bibr" target="#b63">[64]</ref>, <ref type="bibr" target="#b22">[23]</ref>. We notice that the work <ref type="bibr" target="#b64">[65]</ref> has performed the feature selection from a pool of 2,000 features, which may be crucial to increasing its performance. In addition, they only show segmentation results on eight testing images in their paper. They have not mentioned how many images they have actually used for testing. Therefore, the performance of their approach on a relatively large data set is unknown. It is also difficult to directly compare our results with the results reported in <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b15">[16]</ref> because they did not give the segmentation consistency measurement.</p><p>In Table <ref type="table" target="#tab_1">1</ref>, we also list the performance using a CRF model alone and using a BN model alone. The CRF model is exactly the same as what is described in Section 4. The BN model is basically the same as what is described in Section 5. We further incorporate the CIELAB color as the measurements of the region nodes. The likelihood of the region measurements given the region label is simply modeled as Mixture of Gaussians, which are learned from the training data. Compared with the performance of the unified model, it is apparent that the unified model combining both the CRF part and the BN part performs much better than either using the CRF model alone or using the BN model alone. These results demonstrate the usefulness of both parts in our unified model. For the figure/ground image segmentation, we have also performed the experiments on a set of cow images from the VOC2006 database <ref type="bibr" target="#b33">[34]</ref>. This database is primarily used for object categorization. In this work, we use it to test our image segmentation approach. Since there are no original ground truth segmentations, we manually segment a set of cow images from this database. We use about a half set of the cow images (57 images) for training our unified model and use the rest half set of images (51 images) for testing. Some examples of the image segmentation results are shown in Fig. <ref type="figure" target="#fig_14">12</ref>. We have achieved reasonable segmentation results. Although those cows have different appearances and sizes and there might be multiple cows in the image, our approach successfully segments them out. Besides the qualitative evaluation of these results, we manually segment the test images and use the manual segmentation as the ground truth to calculate the segmentation consistency. We achieve a good segmentation consistency of 96 percent on these cow images. We also use the CRF part in our unified model to segment these images. The CRF model alone achieved a segmentation consistency of 93.9 percent, which is apparently inferior to the unified model. This also demonstrates the benefits of unifying two parts for improved performance. Specifically, we observed that the CRF model alone tended to oversmooth the labeling without the help of the BN part.</p><p>Besides providing the segmentation consistency on the two sets of figure/ground segmentation experiments, we also summarize the quantitative results using the two-class confusion matrices in Table <ref type="table" target="#tab_2">2</ref>. Each row is normalized w.r.t. the total number of pixels in the foreground or in the background, respectively. Therefore, the summation of each row will be equal to 1. These confusion matrices show that most of the foreground and background are correctly labeled.</p><p>In Section 5.5, we have mentioned that the performance of the model is not very sensitive to the accurate parameter setting in the Bayesian Network part. We perform a set of The average percentage of correctly labeled pixels (i.e., segmentation consistency) is used as the quantitative measurement. experiments to validate this. We change the conditional probability P ðejpaðeÞÞ for all edge nodes and redo the segmentation of color images in the Weizmann data set and VOC2006 cow data set. Specifically, we change P ðe ij ¼ 1jy i 6 ¼ y j Þ (and the related CPT entries) to six different values but retain all other configurations. The segmentation consistencies of this set of experiments are summarized in Table <ref type="table" target="#tab_3">3</ref>. We observe that the overall performance only changes about one percent, which shows the performance is not very sensitive to the accurate parameter setting in the BN part. However, we also notice that extremely inappropriate parameter setting may decrease the performance.</p><p>In addition, although we use the anisotropic segmentation software to produce the initial oversegmentation, we can also use other approaches to produce the oversegmentation. For example, we use the public available Normalized Cuts software to oversegment the image into 50 segments. We then redid the segmentation on the VOC2006 cow data set and found that the segmentation consistency just slightly changed, as shown in Table <ref type="table">4</ref>. It demonstrates that our approach does not depend on a specific method for the initial oversegmentation.</p><p>Finally, since an oversegmentation is required to produce the input edge map for constructing the proposed model, we did a set of experiments to study the influence of the initial oversegmentation on the overall segmentation performance. We use Normalized Cuts software to produce the oversegmentation of the VOC2006 cow images with 50, 60, 80, 100, and 120 number of oversegmentations. Then, we use the same model with the same parameters to segment these images. The quantitative results of these experiments are summarized in Fig. <ref type="figure" target="#fig_15">13</ref>. We observed that the initial oversegmentation with different numbers only has marginal influence on the overall segmentation performance. However, if the initial oversegmentation is too coarse, there will be segments crossing the object boundary and leading to incorrect segmentation. In practice, we observed that the anisotropic segmentation software can produce oversegmentation where the edges align with the true object boundary well. On the other hand, if the oversegmentation is too fine grained, the number of neighbors of a superpixel will significantly increase. This might influence the spatial interaction between adjacent superpixels and slightly change the segmentation performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Multiclass Segmentation</head><p>To further test the capability of the proposed model, we apply it to a difficult multiclass segmentation problem on the Microsoft data set (MSRC2) <ref type="bibr" target="#b32">[33]</ref>. This data set includes 591 images with 21 object classes, 1 nonsense class, and 2 notconsidered classes. There are significant overlaps between the appearances of different object classes (e.g., building and road). In addition, the within-class appearances also have significant variations. These reasons make the multiclass Note that P ðe ij ¼ 1jy i 6 ¼ y j Þ shall be larger than 0.5 because there is more likely a boundary (e ij ¼ 1) when the adjacent regions have different labels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE 4</head><p>The Segmentation Consistency on the VOC2006 Data Set When Different Methods Are Used for the Initial Oversegmentation segmentation very challenging. Even the state-of-the-art approach <ref type="bibr" target="#b67">[68]</ref> can only achieve a 75.1 percent overall pixelwise labeling accuracy.</p><p>Although our unified model is designed as a figure/ ground segmentation model, it can still be applied to multiclass segmentation. We first train 21 figure/ground segmentation models for all the object classes. We roughly divided the whole data set into two halves and use one half for training (296 images) and the other half for testing (295 images). Since some object classes rarely exist in the whole data set, we use more positive samples to train the corresponding model but ensure no overlap between the training and testing images. After the model training, we sequentially apply these models to each testing image to achieve multiclass segmentation.</p><p>In addition, since there is significant between-class overlap of object appearances and within-class variation of object appearances, we use more local features for the multiclass segmentation. Specifically, we use the color features together with 38 features calculated from Maximum Response (MR) filter sets <ref type="bibr" target="#b68">[69]</ref>. Fig. <ref type="figure" target="#fig_17">14</ref> shows a few examples of the multiclass segmentation results, where different color corresponds to different object classes. We successfully segmented the multiple object classes in these images.</p><p>To quantitatively evaluate the performance, we calculate the confusion matrix of the multiclass segmentation results and the overall labeling accuracy. We achieve an overall pixelwise labeling accuracy of 75.4 percent. There are some state-of-the-art approaches that have also been tested on this data set <ref type="bibr" target="#b32">[33]</ref>, <ref type="bibr" target="#b67">[68]</ref>, <ref type="bibr" target="#b69">[70]</ref>. We summarize the overall labeling accuracy of these approaches in Table <ref type="table" target="#tab_4">5</ref>. Our overall performance is slightly better than these approaches. We notice that <ref type="bibr" target="#b67">[68]</ref> uses some implicit shape information and <ref type="bibr" target="#b69">[70]</ref> exploits a feature selection process. These additional processes are important to achieve their performance. In contrast, we have not exploited these optional processes yet.</p><p>The confusion matrix of our multiclass segmentation results is shown in Fig. <ref type="figure" target="#fig_19">15</ref>. Each number is the percentage of labeling normalized w.r.t. the total pixels in each class. We summarize the diagonal elements of the confusion matrix in Table <ref type="table">6</ref> and compare them with those from the state-of-theart approach <ref type="bibr" target="#b67">[68]</ref>. The bold numbers highlight those object classes where our approach performs better. Compared to <ref type="bibr" target="#b67">[68]</ref>, our approach achieves better performance on 11 classes and ties on 1 class (i.e., water). We notice that our approach performs better in some difficult classes, such as chair, sign, etc. This may be due to the fact that our model is (potentially) capable of capturing very complex relationships between multiple image entities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Computational Time</head><p>We implement the whole model using Matlab software. The segmentation speed mainly depends on the complexity of the constructed graphical model. The constructed factor graph usually consists of 700 to 1,500 nodes. It may take several seconds to half a minute to segment an image using the efficient factor graph inference <ref type="bibr" target="#b62">[63]</ref> in a Pentium M 1.7 GHz laptop. We summarize the typical segmentation time required by other related works in Table <ref type="table" target="#tab_5">7</ref>. Compared to these works, our approach is relatively more efficient on segmenting a normal size image. This can be attributed to using superpixels as the basic units in an image and the fast MPE inference using factor graph.    In this work, we use a combination of supervised parameter learning and manual parameter setting for the model parameterization. Although this method works generally well in our experiments, in the long run, it may be more desirable to directly perform joint parameter learning in the unified model, i.e., to simultaneously learn the BN and CRF parameters automatically from the training data. This is not a trivial task and requires further theoretical derivations and in-depth study of the unified graphical model. We will study this issue as part of future work.</p><p>Finally, we want to point out that the application of the unified graphical model is not limited to image or video segmentation. It can find applications in many different computer vision problems including object tracking, object recognition, activity modeling and recognition, etc.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>TABLE 6</head><p>The Comparison of the Diagonal Elements in the Confusion Matrices between Our Approach and the State-of-the-Art Approach <ref type="bibr" target="#b67">[68]</ref> The bold numbers indicate where our approach performs better than the compared approach.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. (a) The graphical structure of the unified image segmentation model. It combines the Conditional Random Field with the Bayesian Network to unify the region-based image segmentation and the edge-based image segmentation. This example includes four superpixel region nodes fy i g 4 i¼1 , three edge nodes fe j g 3 j¼1 and their measurement fM ej g 3 j¼1 , one vertex node v 1 and its measurement M v1 . (b) The process to construct the CRF model and the BN model from the oversegmentation of the original image.</figDesc><graphic coords="5,41.50,69.17,483.48,179.89" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Illustration of a part of the CRF model built on the superpixel regions. The oversegmentation is superimposed on the original image for an easy view. Each small region in the oversegmented image corresponds to a region node in the CRF model. The correspondence between the region nodes and the superpixel regions are indicated by the white dotted links. The black nodes are the region nodes. The gray node represents the whole image observation.</figDesc><graphic coords="6,29.48,69.17,244.46,111.86" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 3 .Fig. 4 .</head><label>34</label><figDesc>Fig. 3. A synthetic edge map and the BN that models the statistical relationships among superpixel regions, edge segments, vertices, and their measurements: (a) The synthetic edge map. There are six superpixel regions fy i g 6 i¼1 , seven edge segments fe j g 7 j¼1 , and two vertices fv t g 2 t¼1 . (b) The corresponding basic BN structure. The shaded circles represent the measurement nodes.</figDesc><graphic coords="8,34.47,69.17,497.54,153.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>e;y;!;v P ðe; y; !; vjM e ; M ! ; M v Þ ¼ arg max e;y;!;v P ðe; y; !; v; M e ; M ! ; M v Þ:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. The complete Bayesian Network model for the example in Fig. 3a. It consists of six region nodes fy i g 6i¼1 , seven edge nodes fe j g 7 j¼1 , 12 angular nodes, two vertex nodes fv t g 2 t¼1 , and the measurements of edge nodes, angular nodes, and vertex nodes.</figDesc><graphic coords="9,65.71,69.17,435.06,240.49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. The unified graphical model corresponding to the example in Fig. 3a. It combines the CRF model with the BN model through the region nodes.</figDesc><graphic coords="10,69.62,69.17,427.24,287.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>P</head><label></label><figDesc>ðy; e; !; v; M e ; M ! ; M v ; xÞ ¼ P ðe; !; v; M e ; M ! ; M v jy; xÞP ðy; xÞ ¼ P ðe; !; v; M e ; M ! ; M v jyÞP ðyjxÞP ðxÞ ¼ P ðM e ; M ! ; M v je; !; v; yÞP ðe; !; vjyÞP ðyjxÞP ðxÞ ¼ P ðM e jeÞP ðM ! j!ÞP ðM v jvÞP ð!; vje; yÞP ðejyÞP ðyjxÞP ðxÞ ¼ P ðM e jeÞP ðM ! j!ÞP ðM v jvÞP ð!jeÞP ðvjeÞP ðejyÞ</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Examples of different types of graphical models and their corresponding factor graph representations: (a) Bayesian Network and (b) its corresponding factor graph representation; (c) Markov Random Field and (d) its corresponding factor graph representation.</figDesc><graphic coords="11,116.50,69.17,333.52,95.41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>, we use the inference package provided by Hutter et al. to perform MPE inference in the factor graph, i.e., y Ã ; e Ã ; ! Ã ; v Ã ¼ arg max y;e;!;v P ðy; e; !; v; M e ; M ! ; M v ; xÞ; ð16Þ</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. The factor graph representation of the unified graphical model in Fig. 6. The circles represent the variables, while the filled squares represent the factors. The shaded circles are observed variables. For clarity, not all factors are drawn. The symbol "..." represents the undrawn factors.</figDesc><graphic coords="12,66.67,69.17,433.13,286.13" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Examples of the color image segmentation results arranged in two groups of two rows. In each group, the first row includes the color horse images. The second row includes the segmentation masks produced by the proposed approach.</figDesc><graphic coords="13,49.04,69.17,468.40,245.37" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Fig. 11 .Fig. 10 .</head><label>1110</label><figDesc>Fig.11. Qualitative comparison of our segmentation results with the results produced by other state-of-the-art approaches<ref type="bibr" target="#b22">[23]</ref>,<ref type="bibr" target="#b64">[65]</ref>,<ref type="bibr" target="#b63">[64]</ref>,<ref type="bibr" target="#b19">[20]</ref>.(a)-(g) The test images. (h)-(n)The screen copies of some results from the related works: (h) and (i) are the results from<ref type="bibr" target="#b22">[23]</ref>. (j) and (k) are the results from<ref type="bibr" target="#b64">[65]</ref>. The segmentation is superimposed as red contours. (l) and (m) are the results from<ref type="bibr" target="#b63">[64]</ref>. (n) is the result from<ref type="bibr" target="#b19">[20]</ref>. The segmentation is superimposed as blue contours. (o)-(u) The corresponding results produced by our approach. This figure should be viewed in color.</figDesc><graphic coords="14,56.98,342.31,452.52,200.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Fig. 12 .</head><label>12</label><figDesc>Fig. 12. Examples of the segmentation results of the VOC2006 cow images arranged in two groups of two rows. In each group, the first row includes the color test images. The second row includes the segmentation masks produced by the proposed approach.</figDesc><graphic coords="15,41.61,268.84,483.31,244.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Fig. 13 .</head><label>13</label><figDesc>Fig. 13. The influence of the initial oversegmentation on the overall segmentation performance on the VOC2006 cow data set. Normalized Cuts software is used to produce the initial oversegmentation with different number of segments.</figDesc><graphic coords="16,292.65,570.61,244.12,149.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>8 CONCLUSIONS</head><label>8</label><figDesc>To summarize, we present a new image segmentation framework based on a unified probabilistic graphical model. The proposed unified model can systematically capture the complex and heterogenous relationships among different image entities and combine the captured relationships with various image measurements to perform effective image segmentation. An image is first oversegmented to produce an edge map, from which a superpixel-based CRF and a multilayer BN are automatically constructed. The superpixel CRF model performs region-based image segmentation based on the local features and the conditioned Markovian</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Fig. 14 .</head><label>14</label><figDesc>Fig. 14. Examples of the segmentation results of MSRC2 images arranged in three rows. (a) The color test images. (b) The multiclass labeling results produced by the proposed approach. Each color represents a different object class. (c) The ground truth labeling, where the black color indicates the nonsense class. This figure should be viewed in color.</figDesc><graphic coords="17,34.75,69.17,496.97,139.47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head></head><label></label><figDesc>property. The BN model performs edge-based image segmentation based on the measurements of edges, vertices, angles, and local constraints such as the smoothness and connectivity constraints. The CRF model and the BN model are then integrated through factor graph theories to produce a unified graphical model. Image segmentation under the unified graphical model is solved through an efficient MPE inference given various image measurements. The unified graphical model systematically combines the CRF model with the BN model. These models represent different types of graphical models. The proposed unified model can therefore flexibly model both causal and noncausal relationships among different entities in the image segmentation problem. By using the unified graphical model, both region-based information and edge-based information are also seamlessly integrated into the segmentation process. The proposed approach represents a new direction for developing image segmentation methods. The experimental results on the Weizmann horse data set, the VOC2006 cow data set, and the MSRC2 mutliclass segmentation data set show that our approach achieves the segmentation performance that can rival the competing state-of-the-art approaches. It demonstrates the promising capability of the proposed unified framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Fig. 15 .</head><label>15</label><figDesc>Fig. 15. The confusion matrix of our multiclass segmentation results on the MSRC2 data set. The rows correspond to the ground truth classes, while the columns correspond to the labeled classes by the proposed approach. The overall pixelwise labeling accuracy is 75.4 percent.</figDesc><graphic coords="18,48.13,69.17,470.15,230.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 1 The</head><label>1</label><figDesc>Quantitative Comparison of Our Approach with Several Related Works for Segmenting the Weizmann Horse Images</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE 2</head><label>2</label><figDesc>Two-Class Confusion Matrices of the Figure/Ground Segmentation on (a) the Weizmann Data Set and (b) the VOC2006 Cow Data Set</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>TABLE 3 The</head><label>3</label><figDesc>Segmentation Consistencies When the Conditional Probability Table P ðe ij jy i ; y j Þ in the Model Changes</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE 5 The</head><label>5</label><figDesc>Quantitative Comparison of Our Approach with Several Related Works for Segmenting the MSRC2 Data Set</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>TABLE 7 The</head><label>7</label><figDesc>Typical Time Required for Segmenting a Normal Size Image</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>The average percentage of correctly labeled pixels is used as the quantitative measurement.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>This project is supported in part by a grant from the <rs type="funder">US National Science Foundation</rs> under award number <rs type="grantNumber">0241182</rs>. The authors also want to thank the anonymous reviewers and the editors who gave them valuable comments on this work.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_3d9hpXA">
					<idno type="grant-number">0241182</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>. For more information on this or any other computing topic, please visit our Digital Library at www.computer.org/publications/dlib.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Mean Shift: A Robust Approach Toward Feature Space Analysis</title>
		<author>
			<persName><forename type="first">D</forename><surname>Comaniciu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Meer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="603" to="619" />
			<date type="published" when="2002-05">May 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Snakes: Active Contour Models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Witkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Terzopoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int&apos;l J. Computer Vision</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="321" to="331" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Normalized Cuts and Image Segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="888" to="905" />
			<date type="published" when="2000-08">Aug. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Active Contours without Edges</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Vese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="266" to="277" />
			<date type="published" when="2001-02">Feb. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Stochastic Relaxation, Gibbs Distributions, and the Bayesian Restoration of Images</title>
		<author>
			<persName><forename type="first">S</forename><surname>Geman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Geman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="721" to="741" />
			<date type="published" when="1984-11">Nov. 1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Multiple Resolution Segmentation of Textured Images</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bouman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="99" to="113" />
			<date type="published" when="1991-02">Feb. 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Double Markov Random Fields and Bayesian Image Segmentation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Melas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wilson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Signal Processing</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="357" to="365" />
			<date type="published" when="2002-02">Feb. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Pairwise Markov Random Fields and Segmentation of Textured Images</title>
		<author>
			<persName><forename type="first">W</forename><surname>Pieczynski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tebbache</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Graphics and Vision</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="705" to="718" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A Tree-Structured Markov Random Field Model for Bayesian Image Segmentation</title>
		<author>
			<persName><forename type="first">C</forename><surname>D'elia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Poggi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Scarpa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1259" to="1273" />
			<date type="published" when="2003-10">Oct. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multiscale Conditional Random Fields for Image Labeling</title>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Carreira Perpinan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CS Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE CS Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="695" to="702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Cue Integration in Figure/ Ground Labeling</title>
		<author>
			<persName><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="1121" to="1128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Image Modeling Using Tree Structured Conditional Random Fields</title>
		<author>
			<persName><forename type="first">P</forename><surname>Awasthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gagrani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ravindran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Joint Conf</title>
		<meeting>Int&apos;l Joint Conf</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="2054" to="2059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Combining Belief Networks and Neural Networks for Scene Segmentation</title>
		<author>
			<persName><forename type="first">X</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Felderhof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="467" to="483" />
			<date type="published" when="2002-04">Apr. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Real-Time Semi-Automatic Segmentation Using a Bayesian Network</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">N</forename><surname>Mortensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CS Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE CS Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="1007" to="1014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Combination of High-Level Cues in Unsupervised Single Image Segmentation Using Bayesian Belief Networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Alvarado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Akyol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Conf. Imaging Science, Systems, and Technology</title>
		<meeting>Int&apos;l Conf. Imaging Science, Systems, and Technology</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="675" to="681" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Detecting Object Boundaries Using Low-, Mid-, and High-Level Information</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CS Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE CS Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Combined Object Categorization and Segmentation with an Implicit Shape Model</title>
		<author>
			<persName><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. Computer Vision Workshop Statistical Learning in Computer Vision</title>
		<meeting>European Conf. Computer Vision Workshop Statistical Learning in Computer Vision</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="17" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Bayesian Hyperspectral Image Segmentation with Discriminative Class Learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Borges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Bioucas</forename><surname>Dias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Marcal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Third Iberian Conf. Pattern Recognition and Image Analysis</title>
		<meeting>Third Iberian Conf. Pattern Recognition and Image Analysis</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="22" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Image Segmentation Using MAP-MRF Estimation and Support Vector Machine</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hosaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kobayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Otsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Interdisciplinary Information Sciences</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="33" to="42" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Locus: Learning Object Classes with Unsupervised Segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Jojic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int&apos;l Conf. Computer Vision</title>
		<meeting>IEEE Int&apos;l Conf. Computer Vision</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="756" to="763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Object Based Segmentation of Video Using Color, Motion and Spatial Information</title>
		<author>
			<persName><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CS Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE CS Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="746" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Soft Color Segmentation and Its Applications</title>
		<author>
			<persName><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-K</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1520" to="1537" />
			<date type="published" when="2007-09">Sept. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Shape Guided Object Segmentation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Borenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CS Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE CS Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="969" to="976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Image Parsing: Unifying Segmentation, Detection, and Recognition</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int&apos;l Conf. Computer Vision</title>
		<meeting>IEEE Int&apos;l Conf. Computer Vision</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="18" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Brain Anatomical Structure Segmentation by Hybrid Discriminative/Generative Models</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Narr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Dinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Toga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Medical Imaging</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="495" to="508" />
			<date type="published" when="2008-04">Apr. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<title level="m">Markov Random Field Modeling in Image Analysis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Conf. Machine Learning</title>
		<meeting>Int&apos;l Conf. Machine Learning</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Lauritzen</surname></persName>
		</author>
		<title level="m">Graphical Models</title>
		<imprint>
			<publisher>Oxford Univ. Press</publisher>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Bayesian Networks and Decision Graphs</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">V</forename><surname>Jensen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>Springer-Verlag</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<title level="m">Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference</title>
		<imprint>
			<publisher>Morgan-Kaufmann Publishers</publisher>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<title level="m">Pattern Recognition and Machine Learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Combining Top-Down and Bottom-Up Segmentation</title>
		<author>
			<persName><forename type="first">E</forename><surname>Borenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Sharon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ullman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CS Conf. Computer Vision and Pattern Recognition Workshop Perceptual Organization in Computer Vision</title>
		<meeting>IEEE CS Conf. Computer Vision and Pattern Recognition Workshop Perceptual Organization in Computer Vision</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page">46</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Textonboost: Joint Appearance, Shape and Context Modeling for Multi-Class Object Recognition and Segmentation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Criminisi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. Computer Vision</title>
		<meeting>European Conf. Computer Vision</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<author>
			<persName><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<ptr target="http://www.pascal-network.org/challenges/VOC/databases.html#VOC2006" />
	</analytic>
	<monogr>
		<title level="m">The VOC2006 Database</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A Multiscale Random Field Model for Bayesian Image Segmentation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bouman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="162" to="177" />
			<date type="published" when="1994-03">Mar. 1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Multiscale Bayesian Segmentation Using a Trainable Context Model</title>
		<author>
			<persName><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bouman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="511" to="525" />
			<date type="published" when="2001-04">Apr. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A Class of Discrete Multiresolution Random Fields and Its Application to Image Segmentation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="42" to="56" />
			<date type="published" when="2003-01">Jan. 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">An Overlapping Tree Approach to Multiscale Stochastic Modeling and Estimation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fieguth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Willsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Processing</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1517" to="1529" />
			<date type="published" when="1997-11">Nov. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Discriminative Random Fields</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int&apos;l J. Computer Vision</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="179" to="201" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">A Hierarchical Field Framework for Unified Context-Based Classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int&apos;l Conf. Computer Vision</title>
		<meeting>IEEE Int&apos;l Conf. Computer Vision</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="1284" to="1291" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Located Hidden Random Fields: Learning Discriminative Parts for Object Detection</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. Computer Vision</title>
		<meeting>European Conf. Computer Vision</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="302" to="315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">The Layout Consistent Random Field for Recognizing and Segmenting Partially Occluded Objects</title>
		<author>
			<persName><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CS Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE CS Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="37" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">3D LayoutCRF for Multi-View Object Class Recognition and Segmentation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CS Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE CS Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Integration, Inference, and Management of Spatial Information Using Bayesian Networks: Perceptual Organization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Boyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993-03">Mar. 1993</date>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="256" to="274" />
		</imprint>
	</monogr>
	<note>IEEE Trans. Pattern Analysis and Machine Intelligence, special section on probabilistic reasoning</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Region Competition: Unifying Snake/ Balloon, Region Growing and Bayes/MDL/Energy for Multiband Image Segmentation</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="884" to="900" />
			<date type="published" when="1996-09">Sept. 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Dynamic Trees for Unsupervised Segmentation and Matching of Image Regions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nechyba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1762" to="1777" />
			<date type="published" when="2005-11">Nov. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A Graphical Model Framework for Coupling MRFs and Deformable Models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pavlovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CS Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE CS Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="739" to="746" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Segmenting Brain Tumors with Conditional Random Fields and Support Vector Machines</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Murtha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bistritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Greiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Computer Vision for Biomedical Image Applications: Current Techniques and Future Trends (within Int&apos;l Conf. Computer Vision)</title>
		<meeting>Computer Vision for Biomedical Image Applications: Current Techniques and Future Trends (within Int&apos;l Conf. Computer Vision)</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="469" to="478" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Image Segmentation Based on Bayesian Network-Markov Random Field Model and its Application on in vivo Plaque Composition</title>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Kerwin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. IEEE Int&apos;l Symp. Biomedical Imaging</title>
		<imprint>
			<biblScope unit="page" from="141" to="144" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Distributed Propagation of A-Priori Constraints in a Bayesian Network of Markov Random Fields</title>
		<author>
			<persName><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">S</forename><surname>Regazzoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Vernazza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEE Proc. I Comm., Speech and Vision</title>
		<imprint>
			<date type="published" when="1993-02">Feb. 1993</date>
			<biblScope unit="volume">140</biblScope>
			<biblScope unit="page" from="46" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">OBJ CUT</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CS Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE CS Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="18" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Learning Causally Linked Markov Random Fields</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 10th Int&apos;l Workshop Artificial Intelligence and Statistics</title>
		<meeting>10th Int&apos;l Workshop Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Edgeflow-Driven Variational Image Segmentation: Theory and Performance Evaluation</title>
		<author>
			<persName><forename type="first">B</forename><surname>Sumengen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Manjunath</surname></persName>
		</author>
		<ptr target="http://barissumengen.com/seg/" />
		<imprint>
			<date type="published" when="2005">2005</date>
			<pubPlace>Santa Barbara</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Univ. of California</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">technical report</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Discriminative Random Fields: A Discriminative Framework for Contextual Interaction in Classification</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int&apos;l Conf. Computer Vision</title>
		<meeting>IEEE Int&apos;l Conf. Computer Vision</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="1150" to="1157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Accelerated Training of Conditional Random Fields with Stochastic Gradient Methods</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">V N</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">N</forename><surname>Schraudolph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Conf. Machine Learning</title>
		<meeting>Int&apos;l Conf. Machine Learning</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="volume">148</biblScope>
			<biblScope unit="page" from="969" to="976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Probabilistic Solution of Ill-Posed Problems in Computational Vision</title>
		<author>
			<persName><forename type="first">J</forename><surname>Marroquin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mitter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Am. Statistical Assoc</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="page" from="76" to="89" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Understanding Belief Propagation and Its Generalizations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yedidia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Joint Conf</title>
		<meeting>Int&apos;l Joint Conf</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">A Combined Corner and Edge Detector</title>
		<author>
			<persName><forename type="first">C</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Stephens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Fourth Alvey Vision Conf</title>
		<meeting>Fourth Alvey Vision Conf</meeting>
		<imprint>
			<date type="published" when="1988">1988</date>
			<biblScope unit="page" from="147" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<author>
			<persName><forename type="first">M</forename><surname>Pradhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Henrion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Provan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Favero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Sensitivity of Belief Networks to Imprecise Probabilities: An Experimental Investigation</title>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="page" from="363" to="397" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Factor Graphs and the Sum-Product Algorithm</title>
		<author>
			<persName><forename type="first">F</forename><surname>Kschischang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Frey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-A</forename><surname>Loeliger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Information Theory</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="498" to="519" />
			<date type="published" when="2001-02">Feb. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Extending Factor Graphs so as to Unify Directed and Undirected Graphical Models</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Frey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 19th Conf. Uncertainty in Artificial Intelligence</title>
		<meeting>19th Conf. Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="257" to="264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Using Weighted Max-Sat Engines to Solve MPE</title>
		<author>
			<persName><forename type="first">J</forename><surname>Park</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 18th Nat&apos;l Conf. Artificial Intelligence</title>
		<meeting>18th Nat&apos;l Conf. Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="682" to="687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Efficient Stochastic Local Search for MPE Solving</title>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Hoos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Stutzle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int&apos;l Joint Conf</title>
		<meeting>Int&apos;l Joint Conf</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="169" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Recognizing Objects by Piecing Together the Segmentation Puzzle</title>
		<author>
			<persName><forename type="first">T</forename><surname>Cour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CS Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE CS Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Learning to Combine Bottom-Up and Top-Down Segmentation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. Computer Vision</title>
		<meeting>European Conf. Computer Vision</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="581" to="594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Unsupervised Structure Learning: Hierarchical Recursive Composition, Suspicious Coincidence and Competitive Exclusion</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. Computer Vision</title>
		<meeting>European Conf. Computer Vision</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="759" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Learning to Segment</title>
		<author>
			<persName><forename type="first">E</forename><surname>Borenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ullman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conf. Computer Vision</title>
		<meeting>European Conf. Computer Vision</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Multiple Class Segmentation Using a Unified Framework over Mean-Shift Patches</title>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Meer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Foran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CS Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE CS Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">A Statistical Approach to Texture Classification from Single Images</title>
		<author>
			<persName><forename type="first">M</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int&apos;l J. Computer Vision</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">1/2</biblScope>
			<biblScope unit="page" from="61" to="81" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Auto-Context and Its Application to High-Level Vision Tasks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE CS Conf. Computer Vision and Pattern Recognition</title>
		<meeting>IEEE CS Conf. Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
