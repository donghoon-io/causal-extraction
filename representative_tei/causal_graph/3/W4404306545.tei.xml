<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Causal Graph-Enhanced Gaussian Process Regression for Modeling Engine-out NOx</title>
				<funder ref="#_ezuzExh">
					<orgName type="full">Cummins Inc</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2025-08-19">19 Aug 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Shrenik</forename><surname>Zinage</surname></persName>
							<email>szinage@purdue.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Mechanical Engineering</orgName>
								<orgName type="institution">Purdue University</orgName>
								<address>
									<settlement>West Lafayette</settlement>
									<region>IN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">School of Mechanical Engineering</orgName>
								<orgName type="institution">Purdue University</orgName>
								<address>
									<settlement>West Lafayette</settlement>
									<region>IN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ilias</forename><surname>Bilionis</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Mechanical Engineering</orgName>
								<orgName type="institution">Purdue University</orgName>
								<address>
									<settlement>West Lafayette</settlement>
									<region>IN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Peter</forename><surname>Meckl</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Mechanical Engineering</orgName>
								<orgName type="institution">Purdue University</orgName>
								<address>
									<settlement>West Lafayette</settlement>
									<region>IN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Causal Graph-Enhanced Gaussian Process Regression for Modeling Engine-out NOx</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-08-19">19 Aug 2025</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2410.18424v2[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Gaussian process regression</term>
					<term>causal graph</term>
					<term>graph neural networks</term>
					<term>convolutional neural networks</term>
					<term>deep kernel</term>
					<term>engineout NOx</term>
					<term>diesel compression ignition engine</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The stringent regulatory requirements on nitrogen oxides (NOx) emissions from diesel compression ignition engines require accurate and reliable models for real-time monitoring and diagnostics. Although traditional methods such as physical sensors and virtual engine control module (ECM) sensors provide essential data, they are only used for estimation. Ubiquitous literature primarily focuses on deterministic models with little emphasis on capturing the various uncertainties. The lack of probabilistic frameworks restricts the applicability of these models for robust diagnostics. The objective of this paper is to develop and validate a probabilistic model to predict engine-out NOx emissions using Gaussian process regression. Our approach is as follows. We employ three variants of Gaussian process models: the first with a standard radial basis function kernel with input window, the second incorporating a deep kernel using convolutional neural networks to capture temporal dependencies, and the third enriching the deep kernel with a causal graph derived via graph convolutional networks. The causal graph embeds physics knowledge into the learning process. All models are compared against a virtual ECM sensor using both quantitative and qualitative metrics. We conclude that our model provides an improvement in predictive performance when using an input window and a deep kernel structure. Even more compelling is the further enhancement achieved by the incorporation of a causal graph into the deep kernel. These findings are corroborated across different verification and validation datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>Given the detrimental impact of nitrogen oxides (NOx) on environmental and human health, stringent regulations <ref type="bibr" target="#b0">[1]</ref> are essential to mitigate these effects and ensure sustainable urban air quality. In this context, developing robust predictive models is crucial for guiding the evolution of internal combustion engine technology, enabling manufacturers to comply with evolving global environmental legislation and effectively reducing air pollutants, including those contributing to greenhouse gas emissions. Research on predictive models for engine-out NOx emissions remains a dynamic and ongoing area of focus within the diesel powertrain research community. Although extensive research in this field is documented in the open literature <ref type="bibr" target="#b1">[2]</ref>, more studies need to be conducted, particularly in analyzing uncertainties associated with these models. Typically, such models are intended for real time use, with their predictions commonly used for control or diagnostic applications. Therefore, it is critical to develop probabilistic models. These models should not only predict emissions but also clearly indicate the expected range of errors or provide a distribution of possible prediction errors.</p><p>Given the extensive research on modeling engine-out NOx, various methods have been explored to address the complexities and uncertainties inherent in these systems. Early research developed physics based models to describe the chemical and thermal mechanisms that produce NOx in diesel engines. For example, <ref type="bibr" target="#b2">[3]</ref> presented a fast physics based approach for estimating NOx levels. Another study <ref type="bibr" target="#b3">[4]</ref> used chemical kinetics with finite reaction rates to simulate NOx formation, demonstrating how combustion dynamics impact emission levels. Although these approaches provide deep understanding, they often demand significant computational resources and require precise knowledge of engine conditions. In recent years, data driven techniques have gained traction for NOx emission prediction, as these models can learn patterns directly from observed data. For instance, <ref type="bibr" target="#b4">[5]</ref> applied artificial neural networks to estimate transient NOx outputs in high speed diesel engines. In another case, <ref type="bibr" target="#b5">[6]</ref> combined deep learning with Bayesian hyperparameter optimization to forecast NOx emissions during rapidly changing engine operations. Other researchers have turned to machine learning algorithms such as support vector machines and used optimization techniques like particle swarm optimization to model emissions in homogeneous charge compression ignition engines <ref type="bibr" target="#b6">[7]</ref>. Hybrid modeling strategies that bring together physical principles and machine learning have also been investigated. For example, <ref type="bibr" target="#b7">[8]</ref> explored machine learning methods that incorporate physical insights to predict soot emissions. However, these deterministic models often overlook the uncertainties associated with sensor data and engine dynamics. To address this limitation, probabilistic frameworks have been proposed. <ref type="bibr" target="#b8">[9]</ref> used Bayesian inference techniques along with uncertainty quantification methods specifically targeting hydrogen enriched and lean premixed combustion processes. Similarly, <ref type="bibr" target="#b9">[10]</ref> introduced a systematic methodology for performing uncertainty analysis in predictive models focused on engine-out NOx, highlighting the need for robust uncertainty quantification.</p><p>Gaussian processes (GPs) <ref type="bibr" target="#b10">[11]</ref> are popular tools in Bayesian analysis because they offer clear interpretations and robust ways to measure uncertainty. GPs usually depend on a limited set of kernel parameters, which are adjusted to maximize the marginal likelihood. Most practical implementations, however, use a predetermined kernel, which restricts the model's capacity to learn complex patterns from the data. As a result, GPs often serve as smoothers and may not perform well with data that has intricate structures or lies in high-dimensional spaces.</p><p>On the other hand, deep neural networks (DNNs) <ref type="bibr" target="#b11">[12]</ref> have shown exceptional ability to learn useful representations, making them highly effective at predicting new data. Yet, traditional neural networks are usually deterministic, which can lead to overconfident outputs <ref type="bibr" target="#b12">[13]</ref> and unreliable uncertainty estimates. Although Bayesian neural networks (BNNs) attempt to address these shortcomings by treating model weights as random variables, they bring their own challenges. Inference in BNNs is often difficult because the prior choices are not always intuitive, the posterior distributions are complex, and the models have many parameters. In addition, BNNs frequently require repeated forward passes to estimate predictive distributions, resulting in significant computational overhead.</p><p>To tackle these issues, recent research has focused on blending the strengths of GPs and neural networks. Deep kernel learning (DKL) <ref type="bibr" target="#b13">[14]</ref> is one such approach, where a neural network transforms the input data into a new feature space, and a GP then operates on these features for prediction. This framework allows the model to optimize both the kernel and the neural network parameters together, using techniques like variational inference or marginal likelihood maximization. Experiments reported in <ref type="bibr" target="#b13">[14]</ref> demonstrate that DKL surpasses standard kernel approaches, such as those using the radial basis function (RBF) kernel, and also performs better than conventional neural networks on various benchmark datasets.</p><p>Graph neural networks (GNNs) have become an effective tool for learning from graph structured data (i.e., a structured network of interconnected nodes linked by edges, representing causal relationships between the nodes), evolving through several stages to address increasingly complex data representation and learning tasks. The inception of GNNs <ref type="bibr" target="#b14">[15]</ref> marked a significant shift in how data structured in graphs could be processed by learning algorithms. Graph convolutional networks (GCNs) <ref type="bibr" target="#b15">[16]</ref> introduced the concept of applying convolutional operations directly on graphs. These networks extend the convolutional paradigm to graph data by considering the graph's structure in the convolution operation, allowing for the aggregation of neighbor features through a form of weighted average. This process allows the model to capture the local connectivity and feature patterns in the graph, which has proven effective for node and graph classification tasks. Following GCNs, the development of graph attention networks (GATs) <ref type="bibr" target="#b16">[17]</ref> introduced an attention based approach. In GATs, nodes learn to assign different weights to the features of their neighbors, allowing the network to focus on the most relevant information in the local graph structure. GATs have shown significant improvements in various tasks by allowing more nuanced feature aggregation from neighbors, compared to the more uniform aggregation in GCNs. Gated graph sequence networks (GGSNs) <ref type="bibr" target="#b17">[18]</ref> represent a further evolution in the processing of graph structured data, incorporating elements of sequence modeling into graph networks. By using gated recurrent units <ref type="bibr" target="#b18">[19]</ref> or similar mechanisms, GGSNs can model graph dynamics and temporal changes, making them particularly suited for tasks involving sequences of graphs or graphs with evolving structures. This approach has expanded the applicability of GNNs to a broader range of tasks, including those that involve time series data on graphs.</p><p>Despite advances in modeling techniques, many current methods still largely neglect the use of causal knowledge in predictive models. Causal knowledge <ref type="bibr" target="#b19">[20]</ref>, involves understanding and representing cause and effect relationships rather than merely detecting correlations or patterns in data. Most existing data driven and hybrid models primarily identify associations between input features and predicted outcomes, but they often fail to express the underlying causal mechanisms clearly. This lack of causal understanding can significantly limit a model's interpretability, robustness, and ability to generalize, particularly when faced with conditions that differ substantially from those encountered during training. Explicitly integrating causal knowledge into modeling frameworks is, therefore, crucial. It allows models to move beyond simple predictions and actively reason about potential interventions and hypothetical (counterfactual) scenarios. Such an approach provides deeper insights into the underlying physical processes, improving the reliability of predictions. This is especially valuable in complex, dynamic systems such as diesel engines, where accurately distinguishing between mere correlation and genuine causation is essential for accurate diagnostics and effective emission control strategies.</p><p>The objective of this paper is to develop and validate a probabilistic model to predict engine-out NOx emissions using Gaussian process regression. Our approach is as follows. We employ three variants of GP models: the first with a standard RBF kernel with input window, the second incorporating a deep kernel using convolutional neural networks (CNNs) to capture temporal dependencies, and the third enriching the deep kernel with a causal graph derived via GCNs. The causal information derived from the causal graph serves to embed physics-informed knowledge into the learning process.</p><p>We have organized our paper as follows. We begin by delving into the physical principles underlying the formation of NOx. We then define the problem statement before presenting the methodology. Following this, we verify our approach on an illustrative example. This is followed by the experimental setup, after which we present and discuss our main findings. Finally, we conclude the paper with a summary of the key results and their implications.</p><p>This research has been conducted in collaboration with Cummins Inc with the data from a Cummins medium-duty diesel engine. In compliance with Cummins policies, all plots in this study have been normalized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Engine-out NO x formation</head><p>The formation of NO x , consisting of nitric oxide (NO) and nitrogen dioxide (NO 2 ), in diesel engines involves several mechanisms. Notably, thermal NO, fuel NO, and prompt NO play crucial roles <ref type="bibr" target="#b20">[21]</ref>. Prompt NO predominantly forms under fuel-rich conditions and exhibits minimal temperature dependency. Conversely, fuel NO relies on nitrogenous compounds in the fuel. The dominant process in diesel engines for NO generation within the combustion chamber is thermal NO, as described by the extended Zeldovich mechanism <ref type="bibr" target="#b21">[22]</ref>. This process results from the oxidation of atmospheric nitrogen. The key reactions for thermal NO are as follows:</p><formula xml:id="formula_0">N 2 + O → NO + N, N + O 2 → NO + O, N + OH → NO + H.</formula><p>These above reactions become significant at high temperatures (above 2000 K), and are influenced by the amount of available oxygen (O 2 ) and the time the gases spend at peak temperatures, especially in lean mixtures <ref type="bibr" target="#b22">[23]</ref>. In comparison, NO 2 is mainly produced outside the cylinder, as a result of the partial oxidation of NO through the reaction:</p><formula xml:id="formula_1">NO + HO 2 → NO 2 + OH.</formula><p>The final level of NO x leaving the engine depends mostly on the temperature of the burnt gases and the oxygen content in the cylinder. These outcomes are affected by several engine operating conditions, including the flow rate of intake air, the rate of fuel injection, and changes in engine speed and load. Consequently, these engine variables are chosen for modeling engine-out NO x . A widely used technique for reducing NO x emissions in diesel engines is exhaust gas recirculation (EGR). EGR works by redirecting a portion of the exhaust back into the combustion chamber. The recirculated gases consist mostly of nitrogen (N 2 ), carbon dioxide (CO 2 ), and water vapor (H 2 O), which take the place of some of the fresh intake air. This process lowers both the oxygen content and the peak combustion temperature because the triatomic molecules in EGR have higher specific heat capacities. As a result, less NO x is produced <ref type="bibr" target="#b23">[24]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Problem Statement</head><p>Despite substantial progress in modeling engine-out NOx, most existing approaches operate within deterministic frameworks that fall short of capturing the various uncertainties present in sensor measurements, engine dynamics, and operating conditions. This limits their robustness and effectiveness for real-time diagnostics. Traditional physicsbased models are computationally demanding and require detailed engine knowledge, while data-driven approaches often provide overconfident predictions without uncertainty quantification. Hybrid models partially address these issues, but typically ignore underlying causal mechanisms, reducing interpretability and generalizability.</p><p>To overcome these limitations, it is important to develop a new modeling approach that is both reliable and suitable for real world use. First, the model should be able to estimate how certain or uncertain its predictions are, which will make diagnostics more trustworthy and useful. Second, it needs to understand and learn how engine behaviors change over time, since engine processes are dynamic and affected by many factors. Finally, the model should include knowledge about the real causes behind engine operations, so that its predictions are not only accurate but also easier to explain and understand.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Gaussian Process Regression</head><p>GP is a flexible, probabilistic model often used for regression problems. Instead of assuming a fixed functional form, GPs define a prior directly over possible functions. This prior is completely described by two functions: the mean function m(x) and the covariance function k(x, x ′ ). The defining property of a GP is that, for any set of input points, the function values follow a joint Gaussian distribution. In GPR, the core idea is to treat the unknown function as a random draw from a GP. The mean function is given by:</p><formula xml:id="formula_2">m(x) = E[f (x)],</formula><p>while the covariance function is given by:</p><formula xml:id="formula_3">k(x, x ′ ) = E[(f (x) -m(x))(f (x ′ ) -m(x ′ ))].</formula><p>In our paper, we use a zero mean function. The kernel function measures how similar two points are. One popular choice is the RBF kernel with automatic relevance determination. Assuming l d as the characteristic length scale corresponding to each dimension d, and D denoting the dimensionality, the RBF kernel is defined as:</p><formula xml:id="formula_4">k rbf α (x, x ′ ) = σ 2 exp - D d=1 (x d -x ′ d ) 2 2l 2 d ,</formula><p>where σ 2 represents the signal variance, and α = (l 1 , l 2 , . . . , l d , σ 2 ). The length scale parameters l d dictate the sensitivity of the kernel to changes along each dimension. A lower value of l d increases the influence of variations in the corresponding dimension on the covariance.</p><p>Consider a dataset D consisting of n input observations represented by the matrix X = (x 1 , . . . , x n ), each with dimensionality D, and corresponding targets denoted by the vector y = (y 1 , . . . , y n ) ⊤ . Assuming additive Gaussian noise, the likelihood for an individual observation is expressed as:</p><formula xml:id="formula_5">y i | f (x i ) ∼ N (y i ; f (x i ), σ 2 y ),</formula><p>where σ 2 y denotes the variance associated with observational noise. Observations are considered statistically independent.</p><p>For n * new test inputs X * , the GP provides the following predictive distribution:</p><formula xml:id="formula_6">f * | X * , X, y, θ, σ 2 y ∼ N (E[f * ], C(f * )) , E[f * ] = m(X * ) + k θ (X * , X)[k θ (X, X) + σ 2 y I] -1 (y -m(X)), C[f * , f * ] = k θ (X * , X * ) -k θ (X * , X)[k θ (X, X) + σ 2 y I] -1 k θ (X, X * ),</formula><p>where,</p><formula xml:id="formula_7">f * = [f * (x 1 ), . . . , f * (x n * )] ⊤ .</formula><p>In these equations, k θ (X * , X) is the covariance matrix between test and training points, k θ (X, X) is the covariance among training points, and m(X * ) is the mean at the test inputs. The model parameters θ can be determined by maximizing the marginal log likelihood of the observed targets:</p><formula xml:id="formula_8">log p(y | θ, X) ∝ -   y ⊤ (k θ (X, X) + σ 2 y I) -1 y model fit + log |k θ (X, X) + σ 2 y I| complexity penalty    .</formula><p>The parameters θ are found by maximizing this quantity.</p><p>Deep Kernel A deep kernel <ref type="bibr" target="#b13">[14]</ref> combines the strengths of neural networks and classical kernel methods to improve a model's ability to capture complicated relationships in data. The primary function of the neural network component is to perform dimensionality reduction and feature extraction, thereby generating a compact and informative representation suitable for the GP model. Unlike conventional kernels such as the RBF, which inherently assume smoothness and isotropy <ref type="bibr" target="#b24">[25]</ref>, deep kernels can model complex, irregular, and anisotropic functional relationships. Moreover, the neural network's ability to project high dimensional input spaces onto lower dimensional latent spaces helps mitigate issues associated with the curse of dimensionality. This reduction in dimensionality not only improves computational tractability, but also improves the predictive capability of the GP by focusing on the most informative features of the input data. Let ϕ(x; w) denote a nonlinear transformation executed by a neural network characterized by parameters w, and k rbf α (x i , x ′ j ) represent the standard RBF kernel. The input transformations are then expressed as:</p><formula xml:id="formula_9">k deep θ (x i , x ′ j ) → k rbf α (ϕ(x i ; w), ϕ(x ′ j ; w)),</formula><p>where θ = (α, w) collects all the parameters of both the kernel and the neural network. These parameters are learned together by maximizing the log marginal likelihood of the GP model.</p><p>To capture temporal or sequential dependencies effectively, especially when dealing with time-series data, a sliding window approach is often used. The sliding window technique involves using a fixed window to extract overlapping segments from the input data, allowing the model to capture both recent and historical patterns in the sequence. If we consider a sliding input window, we would have the input space dimension to be mapped from a number of input features (N inputs ) × window size (W s ) to a user-defined input space before feeding into the RBF kernel. In this study, we used CNNs to capture these temporal dependencies as depicted in Fig. <ref type="figure" target="#fig_0">1</ref>. CNNs are particularly well-suited to capture temporal dependencies due to their translation invariance, local receptive fields, and efficient parameter sharing. Translation invariance allows the model to detect patterns regardless of their position in the sequence, while local receptive fields enable learning of localized temporal dependencies. Shared weights reduce the number of parameters, improving computational efficiency.</p><formula xml:id="formula_10">Input: N samples × N inputs × W s Conv1D: N samples × 256 × W s ReLU: N samples × 256 × W s MaxPool1D: N samples × 256 × (W s //2) Conv1D: N samples × 128 × (W s //2) Flatten: N samples × 128 * (W s //2) Linear: N samples × 100 Linear: N samples × 50 Linear: N samples × 25 Linear: N samples × 10 Linear: N samples × 3 RBF Kernel k rbf α (ϕ(x i ; w), ϕ(x ′ j ; w))</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deep kernel while incorporating causal information</head><p>GCNs <ref type="bibr" target="#b15">[16]</ref> extend the principles of CNNs to work with graph data. In our approach, we use GCNs to incorporate causal information from the causal graph (Fig. <ref type="figure" target="#fig_2">3</ref>) into the DKL framework. A graph is denoted as G = (V, E), where V is the set of nodes and E is the set of edges connecting pairs of nodes, with E ⊆ V × V . Each node v ∈ V is associated with a feature vector x v , which, in our case, contains the time series data for each variable. The collection of all node features forms a matrix M ∈ R N ×F , where N is the number of nodes and F is the number of features per node. The relationships between nodes are described by the adjacency matrix A ∈ {0, 1} N ×N , which encodes the causal graph structure: A ij = 1 indicates a direct causal link from node i to node j, and A ij = 0 otherwise. Let H (l) be the matrix of node features at the l-th GCN layer, starting with H (0) = M . The trainable weight matrix for layer l is W (l) . To incorporate each node's own features, we add self-loops to the adjacency matrix, yielding Ã = A + I, where I is the identity matrix. Assuming D represents the diagonal degree matrix, the graph convolution operation in its simplest form can be expressed as:</p><formula xml:id="formula_11">H (l+1) = σ D-1 2 Ã D-1 2 H (l) W (l) ,</formula><p>where σ(•) is a nonlinear activation function. This operation effectively updates each node's representation by considering its own features and those of its causal predecessors, as defined by the graph structure. The normalization with D-1 2 Ã D-1 2 ensures that the scale of the feature representations is maintained, preventing vanishing or exploding gradients during training. The use of multiple stacked GCN layers allows the model to effectively represent higher order dependencies, thus aiding the learning of rich feature representations that incorporate causal information.</p><p>The output of the GCN serves as the input to the GP's kernel function. By transforming the input features using the GCN, we provide the GP with rich representations that incorporate both the features and the causal information between the input variables. The RBF kernel is then applied to these transformed features. Fig. <ref type="figure" target="#fig_1">2</ref> shows the schematic of a deep kernel learning while encoding causal information. This means that the GCN learns not just the static structure of the graph but also how changes in one part of the graph (e.g., one node or a set of nodes) might influence other parts. Once it's trained, even though the output graph may structurally resemble the input, it contains a deeper understanding of the causal relationship within the graph. Please refer to <ref type="bibr" target="#b25">[26]</ref> for further insights into connections between GNNs and structural causal models (SCMs). SCMs are frameworks that combine causal graphs with structural equations to model the relationships between variables. Incorporating these structural equations into predictive models via SCMs offers several advantages over traditional statistical or purely correlational models. Traditional models often rely on correlation between variables, which may not reflect true causal relationships. SCMs on the other hand explicitly model causation, ensuring that the influence of each variable on others is accurately represented. Due to this, models based on causation are more likely to generalize well to new unseen data, especially under interventions or distribution shifts. The predictions remain reliable even when the underlying data distribution changes, as causal relationships are invariant to such changes.</p><formula xml:id="formula_12">Input: N samples × N inputs × W s GCNConv: N samples × N inputs × 32 ReLU: N samples × N inputs × 32 GCNConv: N samples × N inputs × 16 ReLU: N samples × N inputs × 16 GCNConv: N samples × N inputs × 8 ReLU: N samples × N inputs × 8 GCNConv: N samples × N inputs × 4 Subsampling: N samples × 3 × 4 Flatten: N samples × 12 RBF Kernel k rbf α (ϕ(x i ; w), ϕ(x ′ j ; w))</formula><p>The causal graph for engine-out NOx was constructed based on expert guidance from Cummins, reflecting a physical understanding of how engine-out NOx is caused due to other variables in a diesel compression ignition (CI) engine. Fig. <ref type="figure">4</ref> shows the causal graph for engine-out NOx with the arrows indicating the hypothesized causal link. In Fig. <ref type="figure">4</ref>, certain variables are unmeasured. Consequently, in this study, it is postulated that all causal influences directed towards these unmeasured variables are transmitted without integrating the information of the corresponding variable. One notable exception is the peak in-cylinder temperature, a variable that is critical in influencing the changes in engineout NOx emissions. However, due to the unavailability of data for peak in-cylinder temperature, it was substituted with turbine inlet temperature, the nearest available analogous variable. Given that the ambient temperature remained constant throughout the data collection phase, all causal relationships involving ambient temperature have been disregarded in this analysis. Also, due to the absence of data for charge temperature, it was substituted with the most closely related measured variable available, namely the intercooler outlet temperature. The input variables for modeling engine-out NOx were chosen primarily on the basis of the fundamental chemical and thermodynamic principles governing NOx formation.</p><p>The inputs we have considered are as follows:</p><p>1 It is important to note that while GCNs incorporate causal graph information into feature representations, they do not perform explicit causal inference or enforce causal constraints in predictions. Instead, it improves the model's ability to learn complex dependencies by embedding structural information from the causal graph <ref type="bibr" target="#b26">[27]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Verifying the approach on an illustrative example</head><p>To validate our proposed method, we apply it to a synthetic illustrative example based on a predefined SCM. Consider a system with k input variables and one output variable. We denote the input variables as x 1 , x 2 , . . . , x k , and the output variable as y. Specifically, let y t represent the target variable at time step t. The SCM for this example defines the following causal relationships:</p><formula xml:id="formula_13">x 5 = x 2 1 , x 6 = x 2 log(1 + |x 4 |) + x 3 ,</formula><p>x 7 = sin(x 5 ) cos(x 5 ),</p><formula xml:id="formula_14">x 8 = x 2 6 + |x 5 |, y = tanh(x 7 ) + cos(x 8 ) + sin(x 7 -x 8 ) + x 7 + x 8 .</formula><p>In this model, each equation represents how a child variable is causally influenced by its parent variables. For example, x 5 is directly influenced by x 1 , while y is directly influenced by x 7 and x 8 . This hierarchical and nonlinear structure allows us to assess how well our causal graph-enhanced GP captures the underlying causal dependencies and nonlinear relationships within the data, as depicted in Fig 5 . To evaluate our approach, we generate synthetic data that adheres to the defined SCM, ensuring the preservation of causal relationships. We begin by sampling the input variables x 1,t , x 2,t , x 3,t , x 4,t for each time step t from predefined distributions. Using these sampled inputs, we compute the intermediate variables x 5,t , x 6,t , x 7,t , x 8,t according to the SCM equations. The output variable y t is then defined as follows, with the additive Gaussian noise ϵ t ∼ N (0, ζ 2 ) to account for measurement uncertainty:</p><formula xml:id="formula_15">y t = tanh(x 7,t ) + cos(x 8,t ) + sin(x 7,t -x 8,t ) + x 7,t + x 8,t + ϵ t .</formula><p>Furthermore, we model the measured input variables x i,t,meas with uncertainty by introducing measurement noise:</p><formula xml:id="formula_16">x i,t,meas | x i,t ∼ N x i,t , τ 2 , i = 1, 2, . . . , k,</formula><p>where τ denotes the standard deviation. The GP model is then trained to predict y t using the inputs while incorporating the information of the causal graph through the deep kernel. We evaluate the performance of our model by comparing it with a standard GP and a GP equipped with a conventional deep kernel using multilayer perceptron (MLP). Additionally, we assess the predictive accuracy of our model against scenarios where the causal information is either partially correct or entirely incorrect, in order to validate the effectiveness of our approach.</p><p>To generate synthetic data, we set ζ = 0.01 and τ = 0.01. A total of 10,000 samples are generated, with the first 9000 samples used for training and the last 1000 samples reserved for testing.</p><p>We train three GP models: the first with a standard RBF kernel (GP (RBF)), the second with a conventional deep kernel using MLP (GP (DRBF-MLP)), and the third with a deep kernel based on GCN (GP (DRBF-GCN)). To compare the efficacy of our proposed model, we use the root mean squared error (RMSE) as the primary performance metric. Additionally, we assess the model's performance in scenarios with partially correct or entirely incorrect causal information by using RMSE, mean absolute error (MAE), and the coefficient of determination (R 2 ) as evaluation metrics.</p><p>Table <ref type="table" target="#tab_2">1</ref> presents the accuracy of the median predictions of these models, evaluated using RMSE as the number of training samples increases. Notably, the GP (DRBF-GCN) outperforms the other two models, especially in low-data regimes (i.e., when N = 1000) due to the inductive bias introduced by the causal information encoded in the deep kernel. As the number of training samples increases, the performance of all three models improves, and they converge to similar levels of accuracy.  <ref type="table" target="#tab_3">2</ref> summarizes the computational training and test times for each of these models when trained and tested on a desktop equipped with an Intel(R) Core(TM) i9-10900K CPU @3.70GHz and an NVIDIA RTX A4000 GPU with 16 GB of GDDR6 memory. Comparable computational efficiency can be observed across methods. Although the GP (DRBF-GCN) model requires slightly more time due to additional graph computations, this modest increase in computational cost is justified by the substantial improvement in accuracy achieved under low-data regimes. To further demonstrate that the model effectively encodes causal information, we train three additional GP models: one that encodes the correct causal relationships, one that encodes partially correct causal relationships, and one that encodes incorrect causal relationships. Table <ref type="table" target="#tab_4">3</ref> shows the accuracy of these model's median predictions on the test dataset, evaluated using RMSE, MAE, and R 2 . We can observe that the highest performance is achieved by the model which encoded the correct causal information with the performance degrading progressively as the encoding of causal information becomes partially correct to completely incorrect. This verifies our proposed approach. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experimental Setup</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Generation</head><p>The experimental data used in this research was obtained from a Cummins mid-range multi-pulse B6.7L inline 6cylinder diesel CI engine equipped with high pressure common rail fuel injection system, a high pressure EGR system, and a VGT. This engine was tested at Cummins Technical Center in Columbus, IN. The data is a mixture of lab-grade instrumentation and on-engine production sensors. Key specifications of the engine from which the experimental data were derived are presented in Table <ref type="table" target="#tab_5">4</ref>. The performance of the trained models is then tested on six different validation datasets that were collected on various duty cycles by intentionally running at varying engine-out NOx levels. In our discourse, we consider the experimental data to be the ground truth and the modeling results are compared with it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Normalization</head><p>Prior to training the GP model, we normalized all datasets using the empirical cumulative distribution function (ECDF) method (also known as the quantile transform method) <ref type="bibr" target="#b27">[28]</ref>. This normalization converts the data into a uniform/Gaussian distribution. In our study, we specifically transformed the data into a uniform distribution. This approach is especially beneficial when the original distribution is unknown or does not conform to the Gaussian distribution often assumed by many machine learning algorithms. Additionally, the ECDF approach improves the robustness to outliers by ranking data points instead of directly scaling their values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Metrics</head><p>In order to assess the ability of the GP models to accurately model NOx, we employed several quantitative metrics. These include the RMSE, the 90th, 95th, and 98th percentiles of the absolute errors in NOx emissions, which provide a comprehensive view of the error distribution. RMSE was selected as the primary statistic due to its sensitivity to large deviations, which aligns with the critical importance of accurately modeling peak emission events in combustion systems. Unlike MAE, which treats all errors uniformly, RMSE disproportionately penalizes larger errors, a particularly relevant factor in emissions modeling, where infrequent but extreme deviations can have regulatory implications. Similarly, R 2 while informative, is less interpretable in scenarios involving non-linear, heteroskedastic data distributions and may offer limited insight into absolute prediction quality. We also included the 90th, 95th, and 98th percentiles of absolute errors to better understand how the model performs under the most challenging conditions. These values tell us the error below which 90%, 95%, and 98% of the predictions fall. In other words, they show how large the biggest errors are for a small portion of the predictions. This is especially important in emissions modeling, where even a few large errors can lead to violations of strict environmental regulations. By looking at these high percentiles, we can better judge whether the model is reliable in situations where accuracy matters the most. Additionally, to examine the consistency of NOx emissions under identical input conditions, we used the Quantile-Quantile (Q-Q) plot, which provides a graphical representation of the empirical distribution of the model's output compared to a theoretical distribution, thereby allowing a thorough assessment of the model's ability to replicate the observed NOx distribution under repeated experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>The GP models and deep kernel using GCN are implemented using the GPyTorch <ref type="bibr" target="#b28">[29]</ref> and PyTorch Geometric <ref type="bibr" target="#b29">[30]</ref> libraries, respectively, each using a PyTorch backend. The negative exact marginal log likelihood serves as the loss function, while optimization is performed using the Adam optimizer <ref type="bibr" target="#b30">[31]</ref>, with tuned hyperparameters. To prevent the models from overfitting, we used early stopping with a patience of 50 epochs.</p><p>In our comprehensive analysis of various GP models built for predicting engine-out NOx, we were provided with the readings from an ECM virtual sensor provided by Cummins for the purpose of comparison. This comparative study revealed distinct trends and performance nuances in our models. Firstly, there is a clear indication that increasing the complexity of GP models improves their predictive performance. This is particularly noticeable when comparing the performance of standard GP (RBF) models with different input window sizes against relatively complex models such as GP (Deep RBF) with CNN or GCN. These complex models often outperform both the standard GP models and the ECM virtual sensor provided by Cummins, especially in scenarios with cumulative NOx level 2 (as seen in Tables <ref type="table" target="#tab_7">5, 6</ref> and<ref type="table" target="#tab_11">10</ref>). However, this trend is not universal across all test cases, suggesting that while complexity contributes to performance, it is not the sole determinant. We can see that GP (DRBF-GCN) [W s = 5s] performs relatively better than all other models for validation 1, 2, and 6 (see Tables <ref type="table" target="#tab_7">5, 6</ref> and<ref type="table" target="#tab_11">10</ref>). However, we observe a relatively better performance of more simpler models for cumulative NOx level 1 scenarios (as seen in Tables <ref type="table" target="#tab_9">8</ref> and<ref type="table" target="#tab_10">9</ref>).</p><p>It is important to differentiate between epistemic and aleatory uncertainties in this context. Epistemic uncertainty originates from insufficient knowledge or data within the model, while aleatory uncertainty is due to the inherent variability in the system under study. In the cumulative NOx level 3 scenario of validation 6, all GP models including the more complex ones, underperform significantly compared to the ECM sensor. The ECM sensor's relatively better performance in this scenario can be linked to its training on a different dataset, likely encompassing a wider representation of NOx conditions with cumulative NOx level 3. This highlights a potential gap in our training dataset for scenarios with cumulative NOx level 3, particularly in terms of epistemic uncertainty. Fig. <ref type="figure" target="#fig_5">7</ref> further supports this, showing considerable epistemic uncertainty as compared to aleatory uncertainty in the GP (DRBF-CNN) [W s = 5s] model predictions for FTP with cumulative NOx level 3, suggesting encounters with scenarios not adequately captured in the training phase. Due to encoding of causal relationships in the deep kernel for GP (DRBF-GCN), we can observe a relative decrease in the offset between the median prediction and the measured data (Fig. <ref type="figure" target="#fig_7">8</ref>) as compared to GP (DRBF-CNN). Although incorporating physical laws did not help us remove the offset completely, it was at least able to decrease this offset compared to pure regression models. Please note that all the values presented in the tables are expressed in parts per million (ppm).  To analyze the variability of NOx conditioned on the same input, Fig. <ref type="figure" target="#fig_4">6</ref>   normal for all validation datasets. In an ideal case these distributions should be the same i.e., all the points must lie on the 45 degree dotted red line. We can see that except for FTP with cumulative NOx level 3, the intermediate NOx values closely follow the standard normal. There is a reasonable difference observed between the sample and theoretical quantiles for extreme low and high NOx values. This is indicative that our model is very sensitive to the changes in input values for predicting extreme low or high NOx values. Due to the gap between validation 6 and the training datasets, we can observe reasonable deviations between these two quantiles for FTP with cumulative NOx level 3. We could not compare these results with the ECM virtual sensor as it was not a probabilistic model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusions</head><p>This paper developed and validated a set of probabilistic models for predicting engine-out NOx using GPR. These models were compared against a virtual ECM sensor, providing a robust framework for assessing their predictive performance under different operating conditions. We systematically increased the complexity of the model in three stages:</p><p>• First, by incorporating memory through varying input window sizes. • Second, by using CNN within the deep kernel framework to improve the model's ability to learn complex temporal patterns. • Finally, by using GCN to incorporate causal information, embedding knowledge informed by physics into the learning process.</p><p>Key findings indicate that increased model complexity improved prediction accuracy in cumulative NOx level 2 scenarios, with the GP(DRBF-GCN) model consistently outperforming simpler models and the ECM sensor. The incorporation of causal information in the GP(DRBF-GCN) model reduced the offset between predictions and actual measurements for the FTP cycle with cumulative NOx level 3, although the performance in extreme NOx cases varied. Simpler models performed better in scenarios with cumulative NOx level 1, and NOx predictions (more specifically, the FTP cycle with cumulative NOx level 3) suffered from high epistemic uncertainty due to insufficient training data.</p><p>Although the GP(DRBF-GCN) model demonstrates superior performance in encoding and leveraging causal information for NOx prediction, several limitations must be considered.</p><p>• The efficacy of GP(DRBF-GCN) is heavily based on the accuracy of the encoded causal relationships. As illustrated in Table <ref type="table" target="#tab_4">3</ref>, the performance of the model significantly degrades when causal information is partially correct or incorrect, highlighting its sensitivity to the quality of causal encoding. This dependence requires precise identification and integration of causal factors, which may not always be feasible in complex engine systems. • While GCNs incorporate causal graph information into feature representation, they impose only a soft constraint. This approach makes GCNs relatively less sensitive to partially correct/incorrect causal information. • Additionally, given a sufficiently large dataset, a standard GP can learn all relevant feature interactions. Under such conditions, the standard GP sometimes may even outperform a GCN, especially if the GCN architecture is excessively rigid or is inadequately aligned with the underlying structure of the data.</p><p>Future work will address the current limitations of the modeling framework, specifically its inability to transfer effectively from one engine to another due to sensor biases. To improve the generalizability and applicability of these predictive models, we plan to develop a framework capable of inferring/estimating these sensor biases for each engine, allowing corrections to be made dynamically as models transition between engines. By incorporating such biascorrection mechanisms, the need for extensive retraining of the model is significantly reduced, allowing the use of a pretrained model with minimal adjustments. This approach will ensure consistent and accurate predictions across different engines, improving both the robustness and efficiency of the modeling process.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Deep kernel using CNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>FlatteningFigure 2 .</head><label>2</label><figDesc>Figure 2. Deep kernel learning while incorporating causal information.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Deep kernel using GCN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>7 yFigure 5 .</head><label>75</label><figDesc>Figure 5. Structural causal model (illustrative example).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Quantile-Quantile plots (GP(DRBF-GCN)[Ws = 5s]).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. GP(DRBF-CNN)[Ws = 5s] predictions on FTP with cumulative NOx level 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>shows the Q-Q plots between the sample quantiles of the GP (DRBF-GCN) [W s = 5s] model (in the scaled version) and the theoretical quantiles of standard Engine-out NOx conc. [Scaled]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. GP(DRBF-GCN)[Ws = 5s] predictions on FTP with cumulative NOx level 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>Accuracy of the GP models (illustrative example)</figDesc><table><row><cell>Models</cell><cell cols="3">RMSE (using N training samples) N = 1000 N = 5000 N = 9000</cell></row><row><cell>GP (RBF)</cell><cell>0.45</cell><cell>0.20</cell><cell>0.16</cell></row><row><cell>GP (DRBF-MLP)</cell><cell>0.54</cell><cell>0.21</cell><cell>0.15</cell></row><row><cell>GP (DRBF-GCN)</cell><cell>0.18</cell><cell>0.16</cell><cell>0.15</cell></row><row><cell>Table</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>Computational training and test time of GP models (illustrative example)</figDesc><table><row><cell>Models</cell><cell cols="2">Training time (s) (per epoch) Test time (s)</cell></row><row><cell>GP (RBF)</cell><cell>0.53</cell><cell>2.1</cell></row><row><cell>GP (DRBF-MLP)</cell><cell>0.52</cell><cell>2.4</cell></row><row><cell>GP (DRBF-GCN)</cell><cell>0.68</cell><cell>2.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Accuracy of the GP models with different causal information (illustrative example)</figDesc><table><row><cell>Models</cell><cell cols="2">RMSE MAE</cell><cell>R 2</cell></row><row><cell>Correct causal information</cell><cell>0.15</cell><cell>0.11</cell><cell>0.95</cell></row><row><cell>Partially correct causal information</cell><cell>0.52</cell><cell>0.31</cell><cell>0.54</cell></row><row><cell>Incorrect causal information</cell><cell>2.31</cell><cell>1.98</cell><cell>-9.68</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 .</head><label>4</label><figDesc>Specifications of the engine</figDesc><table><row><cell>Specification</cell><cell>Details</cell></row><row><cell>Engine type</cell><cell>Cummins B6.7L CI engine</cell></row><row><cell>Horsepower</cell><cell>200-325 hp (149-242 kW)</cell></row><row><cell>Peak torque</cell><cell>520-750 lb-ft (705-1017 Nm)</cell></row><row><cell>Governed speed</cell><cell>2600 rpm</cell></row><row><cell>Clutch engagement torque</cell><cell>400 lb-ft (542 Nm)</cell></row><row><cell>Number of cylinders</cell><cell>6</cell></row><row><cell>Engine weight (dry)</cell><cell>1150 lb (522 kg)</cell></row><row><cell>Fuel system</cell><cell>High pressure common rail</cell></row><row><cell>Turbocharger</cell><cell>VGT</cell></row><row><cell>Emissions control</cell><cell>High pressure EGR</cell></row><row><cell>Certification</cell><cell>EPA 2021</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 .</head><label>5</label><figDesc>Validation 1 (FTP cumulative NOx level 2) (in ppm)</figDesc><table><row><cell>Models</cell><cell>RMSE</cell><cell>NOx Error Percentiles 90th 95th 98th</cell></row><row><cell>GP (RBF) [W s = 1s]</cell><cell>61.39</cell><cell>75.86 111.10 174.29</cell></row><row><cell>GP (RBF) [W s = 2s]</cell><cell>63.30</cell><cell>68.81 118.17 193.81</cell></row><row><cell>GP (RBF) [W s = 3s]</cell><cell>61.68</cell><cell>73.78 113.68 187.26</cell></row><row><cell>GP (RBF) [W s = 4s]</cell><cell>63.08</cell><cell>80.54 117.73 174.86</cell></row><row><cell>GP (RBF) [W s = 5s]</cell><cell>66.58</cell><cell>77.56 120.20 203.28</cell></row><row><cell>GP (DRBF-CNN) [W s = 5s]</cell><cell>61.46</cell><cell>80.18 120.83 176.45</cell></row><row><cell>GP (DRBF-GCN) [W s = 5s]</cell><cell>60.73</cell><cell>74.58 110.33 170.36</cell></row><row><cell>ECM virtual sensor</cell><cell cols="2">102.53 142.88 189.14 361.77</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 .</head><label>6</label><figDesc>Validation 2 (SET cumulative NOx level 2) (in ppm)</figDesc><table><row><cell>Models</cell><cell>RMSE</cell><cell cols="3">NOx Error Percentiles 90th 95th 98th</cell></row><row><cell>GP (RBF) [W s = 1s]</cell><cell>46.20</cell><cell>81.78</cell><cell cols="2">97.09 105.62</cell></row><row><cell>GP (RBF) [W s = 2s]</cell><cell>46.55</cell><cell>64.25</cell><cell>69.62</cell><cell>91.48</cell></row><row><cell>GP (RBF) [W s = 3s]</cell><cell>42.87</cell><cell>74.09</cell><cell>82.32</cell><cell>86.66</cell></row><row><cell>GP (RBF) [W s = 4s]</cell><cell>44.44</cell><cell>74.90</cell><cell>83.69</cell><cell>86.88</cell></row><row><cell>GP (RBF) [W s = 5s]</cell><cell>45.78</cell><cell>75.07</cell><cell>82.55</cell><cell>85.91</cell></row><row><cell>GP (DRBF-CNN) [W s = 5s]</cell><cell>50.16</cell><cell>83.66</cell><cell>92.35</cell><cell>97.72</cell></row><row><cell>GP (DRBF-GCN) [W s = 5s]</cell><cell>42.86</cell><cell>74.18</cell><cell>82.45</cell><cell>86.95</cell></row><row><cell>ECM virtual sensor</cell><cell>58.59</cell><cell cols="3">105.38 111.58 140.42</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 .</head><label>7</label><figDesc>Validation 3 (Step cycle cumulative NOx level 2) (in ppm)</figDesc><table><row><cell>Models</cell><cell>RMSE</cell><cell>NOx Error Percentiles 90th 95th 98th</cell></row><row><cell>GP (RBF) [W s = 1s]</cell><cell cols="2">117.62 126.26 185.99 313.48</cell></row><row><cell>GP (RBF) [W s = 2s]</cell><cell>73.70</cell><cell>98.19 141.58 196.38</cell></row><row><cell>GP (RBF) [W s = 3s]</cell><cell>82.45</cell><cell>132.19 175.77 250.93</cell></row><row><cell>GP (RBF) [W s = 4s]</cell><cell>90.24</cell><cell>132.87 183.10 268.93</cell></row><row><cell>GP (RBF) [W s = 5s]</cell><cell cols="2">101.43 147.34 197.26 294.08</cell></row><row><cell>GP (DRBF-CNN) [W s = 5s]</cell><cell>81.33</cell><cell>101.54 153.63 254.07</cell></row><row><cell>GP (DRBF-GCN) [W s = 5s]</cell><cell>83.54</cell><cell>103.82 158.26 241.45</cell></row><row><cell>ECM virtual sensor</cell><cell cols="2">216.60 349.67 561.70 758.07</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 8 .</head><label>8</label><figDesc>Validation 4  (FTP cumulative NOx level 1) (in ppm)</figDesc><table><row><cell>Models</cell><cell>RMSE</cell><cell>NOx Error Percentiles 90th 95th 98th</cell></row><row><cell>GP (RBF) [W s = 1s]</cell><cell>89.56</cell><cell>100.05 132.69 199.25</cell></row><row><cell>GP (RBF) [W s = 2s]</cell><cell>80.38</cell><cell>103.20 134.09 169.11</cell></row><row><cell>GP (RBF) [W s = 3s]</cell><cell>77.94</cell><cell>94.85 124.80 171.09</cell></row><row><cell>GP (RBF) [W s = 4s]</cell><cell>77.37</cell><cell>88.78 116.60 148.06</cell></row><row><cell>GP (RBF) [W s = 5s]</cell><cell>83.37</cell><cell>98.78 125.67 152.97</cell></row><row><cell cols="3">GP (DRBF-CNN) [W s = 5s] 126.98 209.13 291.76 334.32</cell></row><row><cell>GP (DRBF-GCN) [W s = 5s]</cell><cell>93.71</cell><cell>110.48 143.31 205.34</cell></row><row><cell>ECM virtual sensor</cell><cell cols="2">152.79 225.43 287.62 400.98</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 9 .</head><label>9</label><figDesc>Validation 5 (Step cycle cumulative NOx level 1) (in ppm)</figDesc><table><row><cell>Models</cell><cell>RMSE</cell><cell>NOx Error Percentiles 90th 95th 98th</cell></row><row><cell>GP (RBF) [W s = 1s]</cell><cell cols="2">123.90 190.46 247.38 392.68</cell></row><row><cell>GP (RBF) [W s = 2s]</cell><cell>85.35</cell><cell>132.38 177.28 256.68</cell></row><row><cell>GP (RBF) [W s = 3s]</cell><cell>84.83</cell><cell>130.67 189.10 277.71</cell></row><row><cell>GP (RBF) [W s = 4s]</cell><cell>92.76</cell><cell>147.26 212.70 313.76</cell></row><row><cell>GP (RBF) [W s = 5s]</cell><cell>96.39</cell><cell>151.64 229.76 313.40</cell></row><row><cell>GP (DRBF-CNN) [W s = 5s]</cell><cell>89.66</cell><cell>138.24 186.94 265.52</cell></row><row><cell>GP (DRBF-GCN) [W s = 5s]</cell><cell>91.58</cell><cell>143.65 188.63 265.20</cell></row><row><cell>ECM virtual sensor</cell><cell cols="2">213.85 324.20 510.12 711.17</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 10 .</head><label>10</label><figDesc>Validation 6 (FTP cumulative NOx level 3) (in ppm)</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>Models</cell><cell></cell><cell cols="2">RMSE</cell><cell>NOx Error Percentiles 90th 95th 98th</cell></row><row><cell></cell><cell cols="4">GP (RBF) [W s = 1s]</cell><cell cols="3">438.44 691.64 703.77 731.37</cell></row><row><cell></cell><cell cols="4">GP (RBF) [W s = 2s]</cell><cell cols="3">473.84 791.03 819.27 848.78</cell></row><row><cell></cell><cell cols="4">GP (RBF) [W s = 3s]</cell><cell cols="3">345.26 569.79 584.68 616.90</cell></row><row><cell></cell><cell cols="4">GP (RBF) [W s = 4s]</cell><cell cols="3">289.50 468.79 481.60 515.96</cell></row><row><cell></cell><cell cols="4">GP (RBF) [W s = 5s]</cell><cell cols="3">285.97 467.26 484.28 527.75</cell></row><row><cell cols="8">GP (DRBF-CNN) [W s = 5s] 257.10 399.96 427.23 455.54</cell></row><row><cell cols="8">GP (DRBF-GCN) [W s = 5s] 138.18 123.43 183.87 380.44</cell></row><row><cell></cell><cell cols="4">ECM virtual sensor</cell><cell cols="3">107.55 108.35 169.16 349.40</cell></row><row><cell></cell><cell></cell><cell>1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Engine-out NOx conc. [Scaled]</cell><cell cols="2">0.29 0.43 0.57 0.72 0.86</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">0.14</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>8</cell><cell>0</cell><cell>0</cell><cell>200</cell><cell>Measured 400</cell><cell cols="2">ECM virtual sensor Time (s) 600 800 1000</cell><cell>1200</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">GP(DRBF-CNN)[W s = 5s] (median)</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">95% epistemic uncertainty</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">95% epistemic + aleatory uncertainty</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Prepared using sagej.cls</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p>The authors extend sincere gratitude to <rs type="person">Akash Desai</rs> from <rs type="affiliation">Cummins Inc.</rs> for his invaluable feedback and guidance throughout this research. Additionally, heartfelt thanks are due to <rs type="person">Dr. Lisa Farrell</rs> and <rs type="person">Clay Arnett</rs> from <rs type="affiliation">Cummins Inc.</rs>, not only for sponsoring this research but also for their crucial technical input and the provision of experimental data essential for conducting the simulations.</p></div>
			</div>
			<div type="funding">
<div><head>Funding</head><p>This work has been funded by <rs type="funder">Cummins Inc</rs> under grant number <rs type="grantNumber">00099056</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_ezuzExh">
					<idno type="grant-number">00099056</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Authors' contributions Shrenik Zinage: Methodology, Software, Validation, Visualization, Writing -original draft. Ilias Bilionis: Funding acquisition, Methodology, Validation, Writing -review and editing. Peter Meckl: Funding acquisition, Validation, Writing -review and editing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Declaration of conflicting interests</head><p>The author(s) declared no potential conflicts of interest with respect to the research, authorship, and/or publication of this article.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Regulations for emissions from vehicles and engines</title>
		<author>
			<persName><surname>Epa</surname></persName>
		</author>
		<ptr target="https://www.epa.gov/regulations-emissions-vehicles-and-engines/cleaner-trucks-initiative" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Modeling, diagnostics, optimization, and control of internal combustion engines via modern machine learning techniques: A review and future directions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Aliramezani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shahbakhti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Progress in Energy and Combustion Science</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="page">100967</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A fast and accurate physics-based model for the nox emissions of diesel engines</title>
		<author>
			<persName><forename type="first">J</forename><surname>Asprion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chinellato</forename><forename type="middle">O</forename><surname>Guzzella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied energy</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="page" from="221" to="233" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Modeling of nox formation in diesel engines using finite-rate chemical kinetics</title>
		<author>
			<persName><forename type="first">S</forename><surname>Aithal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Energy</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2256" to="2265" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Artificial neural network (ann) assisted prediction of transient nox emissions from a high-speed direct injection (hsdi) diesel engine</title>
		<author>
			<persName><forename type="first">X</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Papaioannou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Engine Research</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1201" to="1212" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep neural network model with bayesian hyperparameter optimization for prediction of nox at transient conditions in a diesel engine</title>
		<author>
			<persName><forename type="first">S</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Engineering Applications of Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="page">103761</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Support vector machine based emissions modeling using particle swarm optimization for homogeneous charge compression ignition engine</title>
		<author>
			<persName><forename type="first">D</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Blomeyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Engine Research</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="536" to="551" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Soot emission modeling of a compression ignition engine using machine learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Shahpouri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hayduk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IFAC-PapersOnLine</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">20</biblScope>
			<biblScope unit="page" from="826" to="833" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Bayesian inference and uncertainty quantification for hydrogen-enriched and lean-premixed combustion systems</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yousefian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Bourque</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">F</forename><surname>Monaghan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">international journal of hydrogen energy</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">46</biblScope>
			<biblScope unit="page" from="23927" to="23942" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A structured approach to uncertainty analysis of predictive models of engine-out nox emissions</title>
		<author>
			<persName><forename type="first">H</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Brewbaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Upadhyay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Engine Research</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="423" to="433" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Gaussian processes for machine learning</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>MIT press</publisher>
			<biblScope unit="volume">2</biblScope>
			<pubPlace>Cambridge, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">On calibration of modern neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<biblScope unit="page" from="1321" to="1330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep kernel learning</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Artificial intelligence and statistics</title>
		<imprint>
			<biblScope unit="page" from="370" to="378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">The graph neural network model</title>
		<author>
			<persName><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="61" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno>arXiv:160902907</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<idno>arXiv:171010903</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Gated graph sequence neural networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brockschmidt</surname></persName>
		</author>
		<idno>arXiv:151105493</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<idno>arXiv:14061078</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<author>
			<persName><surname>Causality</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Internal combustion engine fundamentals</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Heywood</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>McGraw-Hill Education</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Experimental and theoretical study of nitric oxide formation in internal combustion engines</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Lavoie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Heywood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Keck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Combustion science and technology</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="313" to="326" />
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Kinetics of pollutant formation and destruction in combustion</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">T</forename><surname>Bowman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Progress in energy and combustion science</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="33" to="45" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Prediction of nox reduction with exhaust gas recirculation using the flame temperature correlation technique</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">C</forename><surname>Sekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Balaji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the national conference on advances in mechanical engineering</title>
		<meeting>the national conference on advances in mechanical engineering</meeting>
		<imprint>
			<biblScope unit="page" from="18" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Manifold gaussian processes for regression</title>
		<author>
			<persName><forename type="first">R</forename><surname>Calandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Rasmussen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 International joint conference on neural networks (IJCNN)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="3338" to="3345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Relating graph neural networks to structural causal models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zečević</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Dhami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<idno>arXiv:210904173</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">V</forename><surname>Thost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><forename type="middle">J</forename></persName>
		</author>
		<idno>arXiv:210107965</idno>
		<title level="m">Directed acyclic graph neural networks</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Ordered quantile normalization: a semiparametric transformation built for the cross-validation era</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Peterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Cavanaugh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of applied statistics</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Gpytorch: Blackbox matrix-matrix gaussian process inference with gpu acceleration</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Fast graph representation learning with pytorch geometric</title>
		<author>
			<persName><forename type="first">Fey</forename><forename type="middle">M</forename><surname>Lenssen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename></persName>
		</author>
		<idno>arXiv:190302428</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Adam</surname></persName>
		</author>
		<idno>arXiv:14126980</idno>
		<title level="m">A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
