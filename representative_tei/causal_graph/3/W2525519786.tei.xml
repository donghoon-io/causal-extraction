<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Improving Fluency in Narrative Text Generation With Grammatical Transformations and Probabilistic Parsing</title>
				<funder ref="#_JBAb9C7">
					<orgName type="full">National Science Foundation</orgName>
				</funder>
				<funder ref="#_HTyHraU">
					<orgName type="full">Office of Naval Research</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Emily</forename><surname>Ahn</surname></persName>
							<email>eahn@wellesley.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Creative Technologies</orgName>
								<orgName type="institution">University of Southern</orgName>
								<address>
									<country>California</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fabrizio</forename><surname>Morbini</surname></persName>
							<email>morbini@ict.usc.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Creative Technologies</orgName>
								<orgName type="institution">University of Southern</orgName>
								<address>
									<country>California</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Andrew</forename><forename type="middle">S</forename><surname>Gordon</surname></persName>
							<email>gordon@ict.usc.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Institute for Creative Technologies</orgName>
								<orgName type="institution">University of Southern</orgName>
								<address>
									<country>California</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Improving Fluency in Narrative Text Generation With Grammatical Transformations and Probabilistic Parsing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In research on automatic generation of narrative text, story events are often formally represented as a causal graph. When serializing and realizing this causal graph as natural language text, simple approaches produce cumbersome sentences with repetitive syntactic structure, e.g. long chains of "because" clauses. In our research, we show that the fluency of narrative text generated from causal graphs can be improved by applying rule-based grammatical transformations to generate many sentence variations with equivalent semantics, then selecting the variation that has the highest probability using a probabilistic syntactic parser. We evaluate our approach by generating narrative text from causal graphs that encode 100 brief stories involving the same three characters, based on a classic film of experimental social psychology. Crowdsourced workers judged the writing quality of texts generated with ranked transformations as significantly higher than those without, and not significantly lower than human-authored narratives of the same situations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Narrative Text Generation</head><p>Across several academic disciplines, it has become common to represent narratives as causal graphs. In the causal network model of psychologists <ref type="bibr" target="#b8">Trabasso and van den Broek (1985)</ref>, vertices in the graph structure represent settings, events, goals, attempts and outcomes of a narrative, linked via directed edges that encode cause/effect relationships. In computer science, similar causal graphs have been used to model and manipulate narrative elements including suspense <ref type="bibr" target="#b1">(Cheong and Young, 2014)</ref>, conflict <ref type="bibr" target="#b9">(Ware and Young, 2011)</ref>, flashback and foreshadowing <ref type="bibr" target="#b0">(Bae and Young, 2008)</ref>. <ref type="bibr" target="#b2">Elson (2012)</ref> elaborates the causal network model by relating it to both the temporal ordering of story-world events and an author's textual realization, creating a three-layer Story Intention Graph.</p><p>Causal graph representations of narrative create new opportunities for natural language generation (NLG) of narrative text. For example, <ref type="bibr" target="#b6">Lukin et al. (2015)</ref> describe a narrative NLG pipeline for Story Intention Graphs, generating variations of an original text that can be parameterized for particular discourse goals. When serializing and realizing a causal graph structure as natural language text, some care must be taken to avoid the generation of cumbersome sentences with repetitive syntactic structure, e.g. as a long chain of "because" clauses. <ref type="bibr" target="#b6">Lukin et al. (2015)</ref> directly compared readers' overall preferences for certain causal connectives over others, finding that no single class of variations will produce sentences that are preferable to a human author's stylistic choices.</p><p>We hypothesize that the policies used by native speakers to select among lexical-syntactic variations are complex and content-dependent, and are best described in statistical models trained on natural language corpora. In this paper, we explore a new approach to narrative NLG that integrates rule-based and statistical methods to produce fluent realizations of storylines encoded as causal graphs. Beginning with the output of a simple baseline system, we show that the fluency of narrative text generated from causal graphs can be improved by applying rule-based grammatical transformations to generate many sentence variations with equivalent semantics, then selecting the variation that has the highest probability using a probabilistic syntactic parser. Our software implementation is available online. 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Triangle-COPA Causal Graphs</head><p>As a corpus of causal graphs for use as input for our NLG experiments, we used a set of solutions to the 100 interpretation problems in the Triangle-COPA evaluation. 2 This evaluation set is based on a film from 1958 created by Fritz Heider and Marianne Simmel for use in early social psychology experiments <ref type="bibr" target="#b5">(Heider and Simmel, 1944)</ref>, depicting the movements of geometric shapes (two triangles and a circle) moving in and around a box with a door. The Triangle-COPA evaluation consists of 100 short movies in the same style, with both natural language and formal descriptions of their content. The evaluation tests a system's ability to select the most humanlike interpretation of the observed behavior among two choices <ref type="bibr" target="#b7">(Maslan et al., 2015)</ref>.</p><p>Gordon ( <ref type="formula">2016</ref>) demonstrated that Triangle-COPA questions could be solved by automatically constructing causal graphs that explain the behavior of the moving shapes in terms of their underlying goals, emotions, and social relationships. The approach pairs a hand-authored knowledge base of commonsense causal axioms with an abductive reasoning engine that finds the most-probable set of assumptions that logically entail the observed behavior. 3 Forward-chaining from these assumptions us-1 <ref type="url" target="https://github.com/fmorbini/hsit-generation">https://github.com/fmorbini/hsit-generation</ref> 2 <ref type="url" target="https://github.com/asgordon/TriangleCOPA">https://github.com/asgordon/TriangleCOPA</ref> 3 <ref type="url" target="https://github.com/asgordon/EtcAbductionPy">https://github.com/asgordon/EtcAbductionPy</ref> ing the knowledge base axioms produces a directed causal graph, where the goals, emotions, and social relationships of the characters are intermediate inferences in proving the observations.</p><p>In our own research, we generated causal graphs for each of the 100 Triangle-COPA questions using the abduction engine and knowledge base of <ref type="bibr" target="#b4">Gordon (2016)</ref>, for use as input in our NLG experiments. An example causal graph solution appears in Figure <ref type="figure" target="#fig_0">1</ref> (for Triangle-COPA question 1). In our evaluations, we also used the human-authored narrative included with each question in the Triangle-COPA question set, which represents the realization of the writer's own interpretation of the events depicted in a given Triangle-COPA movie. For the same movie that produced the interpretation represented in Figure <ref type="figure" target="#fig_0">1</ref>, the following is the human-authored narrative:</p><p>A circle stealthily stalks a big triangle. It does not want to be seen so it moves very slowly and quietly and then suddenly startles the triangle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Baseline NLG System</head><p>We developed a baseline NLG system that transforms our causal graphs into narrative texts. Our approach was to first divide the input graph into sections containing exactly one timeline event (creepup-on and flinch, in Figure <ref type="figure" target="#fig_0">1</ref>) along with its causal antecedents. Each of these sections becomes a sentence in the generated output, ordered by any sequence information provided in the input (seq in Figure <ref type="figure" target="#fig_0">1</ref>). Each sentence is structured as a chain of "because" clauses, beginning with the timeline event and followed by each of its causal antecedents. These structures are then realized as text using the 71 SimpleNLG engine <ref type="bibr">(Gatt and Reiter, 2009)</ref> with the support of a custom lexicon for the specific predicates used in Triangle-COPA's representations.</p><p>Below is an example of the output of this Baseline NLG system, generated from the causal graph depicted in Figure <ref type="figure" target="#fig_0">1</ref>. As expected, this text exhibits cumbersome phrasing and repetitive structure.</p><p>The circle creeps up on the big triangle because the circle wants that the big triangle does not see the circle. The big triangle flinches because the circle startles the big triangle because the big triangle sees the circle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Grammatical Transformations</head><p>We sought to improve the fluency of our baseline NLG system by generating many variations of each sentence through domain-independent grammatical transformations, then ordering these variations to select the best one. In this section, we describe the set of 24 hand-authored rules for grammatical transformations used in our experiments, of 7 types: Lexical fixes: These transformations handle special cases of lexical-syntactic patterns not easily handled by the realization engine. Example input: A knocks the door in order to B. Output: A knocks on the door in order to B.</p><p>Each of the 24 hand-authored rules in our system is applied recursively and exhaustively to the syntactic structures used in our baseline NLG system, generating tens to hundreds of variations for each input sentence. We explored the use of a largescale paraphrase database <ref type="bibr" target="#b3">(Ganitkevitch et al., 2013)</ref> as a source of transformation rules, but found that it contained none that were equivalent to those in our hand-authored set. The advantage our handauthored rules is that they strictly preserve the semantics of the original input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Probabilistic Parsing</head><p>To select the best variation for each sentence in the output narrative, we parse each variant using a probabilistic syntactic parser and rank according to the probability of the generated parse tree. For this purpose, we use the constituency parser of Charniak and Johnson (2005) without the built-in reranker, using model SANCL2012-Uniform. Each variant is grammatically correct, so our interest is solely the assigned probability score for the typicality of the lexical-syntactic structure in the training data. Here the parser serves the same role as n-gram language models in machine translation or speech recognition systems, but should be better suited for our task where intra-sentence long-range dependencies are factors in the quality of the text. We investigated whether normalizing these scores by sentence length would improve rankings, but our evaluations here are based on unnormalized probability scores.</p><p>After selecting the most-probable variant for each sentence, we assembled a final narrative for each of the 100 causal graphs. For example, the graph in Figure <ref type="figure" target="#fig_0">1</ref> produces the following output:</p><p>The circle creeps up on the big triangle because she does not want him to see her. He flinches because he sees her.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Evaluation</head><p>We evaluated the quality of our narrative NLG approach by soliciting ratings of writing quality from crowdsourced annotators, comparing the output of our system, our baseline NLG system, and original human-authored narratives for each of the 100 questions in the Triangle-COPA question set. In each annotation task, the annotator watched the short movie associated with a given question, read the text associated with the question randomly selected from our three conditions, then rated the writing quality of the text on a 5-point Likert scale -from (1) Horrible gibberish to (5) Excellent, professional quality. In addition, we asked raters to answer a factual multiplechoice question about each movie to validate their effort on this crowdsourced task. After filtering annotators who failed this validation task, we analyzed 717 ratings evenly distributed across the three conditions and 100 questions, shown in Table <ref type="table" target="#tab_0">1</ref>. Significant gains in quality ratings were observed for our approach over the Baseline NLG system. The differences observed between human-authored narratives and our system were not significant. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>This research demonstrates that high-quality textual narratives can be generated from causal graph representations of stories. The use of hand-authored grammatical transformation rules helps ensure that all textual variations retain the semantics of the original input, while probabilistic parsing helps identify the variation that corresponds best to the structures produced by native speakers.</p><p>In our study, the input causal graphs were also automatically generated, identified as the mostprobable explanations of series of observable events using logical abduction. Having combined automated interpretation with automated narrative generation, we now wonder if automated perception algorithms could serve as the input to similar pipelines to enable future systems to generate human-like narratives of the events in real-world situations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Causal graph output for Triangle-COPA question 1, where a circle (C) creeps up on a big triangle (BT), who then flinches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Sentential arguments: These transformations improve the fluency of verb phrases with sentential arguments. Example input: A wants that B does C. Output: A wants B to do C. Causality: These transformations realize the causality relation in ways other than the default "because" connective. Example input: A does B because A wants C. (where the subject of C is A) Output: A does B to C. Conjunction introduction: These transformations simplify neighboring structures that share some components (e.g. a subject). Example input: A does B and A does C. Output: A does B and C. Repetitions: Identical timeline events in sequences are combined. Example input: A does B. A does B. A does B. Output: A does B repeatedly. Intermediate deletion: These transformations remove intermediate vertices in causal chains, under the assumption that some causal links are intuitive and can be left implicit. Example input: A ignores B because A dislikes B because B annoys A. Output: A ignores B because B annoys A. Pronoun introduction: These transformations replace proper nouns with pronouns when it is unambiguous to do so. Example input: A ignores B because B annoys A. Output: A ignores B because A annoys it.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Ratings of writing quality. (*) significant at p&lt;0.05</figDesc><table><row><cell cols="2">Condition Ratings Mean score (1-5)</cell></row><row><cell>Human authored 233</cell><cell>3.69</cell></row><row><cell>Our system 236</cell><cell>3.59 *</cell></row><row><cell>Baseline NLG 248</cell><cell>3.11</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This research was supported by the <rs type="funder">Office of Naval Research</rs>, grant <rs type="grantNumber">N00014-13-1-0286</rs>. This material is based upon work supported by the <rs type="funder">National Science Foundation</rs> under Grant No. <rs type="grantNumber">1263386</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_HTyHraU">
					<idno type="grant-number">N00014-13-1-0286</idno>
				</org>
				<org type="funding" xml:id="_JBAb9C7">
					<idno type="grant-number">1263386</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A use of flashback and foreshadowing for surprise arousal in narrative using a plan-based approach</title>
		<author>
			<persName><forename type="first">Byung-Chul</forename><surname>Bae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Young</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Interactive Digital Storytelling. Eugene Charniak and Mark Johnson</title>
		<imprint>
			<date type="published" when="2005">2008. 2005</date>
		</imprint>
	</monogr>
	<note>43rd Annual Meeting on Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Suspenser: A story generation system for suspense</title>
		<author>
			<persName><forename type="first">Yun-Gyung</forename><surname>Cheong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Young</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Computational Intelligence and Artificial Intelligence in Games</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">David</forename><surname>Elson</surname></persName>
		</author>
		<title level="m">Modeling Narrative Discourse</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
		<respStmt>
			<orgName>Columbia University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">SimpleNLG: A realisation engine for practical applications</title>
		<author>
			<persName><forename type="first">Juri</forename><surname>Ganitkevitch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Callison-Burch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 Meeting of the North American Association for Computational Linguistics. Albert Gatt and Ehud Reiter</title>
		<imprint>
			<date type="published" when="2009">2013. 2009</date>
			<biblScope unit="volume">12</biblScope>
		</imprint>
	</monogr>
	<note>PPDB: The paraphrase database. th European Workshop on Natural Language Generation</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Commonsense interpretation of triangle behavior</title>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">S</forename><surname>Gordon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">30th AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An experimental study of apparent behavior</title>
		<author>
			<persName><forename type="first">Fritz</forename><surname>Heider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marianne</forename><surname>Simmel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The American Journal of Psychology</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="243" to="259" />
			<date type="published" when="1944">1944</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Generating sentence planning variations for story telling</title>
		<author>
			<persName><forename type="first">Stephanie</forename><surname>Lukin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marilyn</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">16th Annual SIGdial meeting on Discourse and Dialogue</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">One hundred challenge problems for logical formalizations of commonsense psychology</title>
		<author>
			<persName><forename type="first">Nicole</forename><surname>Maslan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melissa</forename><surname>Roemmele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">S</forename><surname>Gordon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th International Symposium on Logical Formalizations of Commonsense Reasoning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Causal thinking and the representation of narrative events</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Trabasso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><surname>Broek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of memory and language</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="612" to="630" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Cpocl: A narrative planner supporting conflict</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Ware</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Young</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Artificial Intelligence and Interactive Digital Entertainment</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
