<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fairness Violations and Mitigation under Covariate Shift</title>
				<funder ref="#_jx5ucv4">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2021-01-22">22 Jan 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Harvineet</forename><surname>Singh</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Rina</forename><surname>Singh</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Vishwali</forename><surname>Mhasawade</surname></persName>
							<email>vishwalim@nyu.edu</email>
						</author>
						<author>
							<persName><forename type="first">Rumi</forename><surname>Chunara</surname></persName>
							<email>rumi.chunara@nyu.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Center for Data Science</orgName>
								<orgName type="institution">New York University New York City</orgName>
								<address>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Tandon School of Engineering</orgName>
								<orgName type="institution">New York University New York City</orgName>
								<address>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Tandon School of Engineering</orgName>
								<orgName type="institution">New York University New York City</orgName>
								<address>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department" key="dep1">Tandon School of Engineering</orgName>
								<orgName type="department" key="dep2">School of Global Public Health</orgName>
								<orgName type="institution">New York University New York City</orgName>
								<address>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="laboratory">Current affiliation is Fusemachines Inc</orgName>
								<orgName type="institution">New York University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Fairness Violations and Mitigation under Covariate Shift</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-01-22">22 Jan 2021</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3442188.3445865</idno>
					<idno type="arXiv">arXiv:1911.00677v2[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>algorithmic fairness</term>
					<term>domain adaptation</term>
					<term>covariate shift</term>
					<term>causal inference</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We study the problem of learning fair prediction models for unseen test sets distributed differently from the train set. Stability against changes in data distribution is an important mandate for responsible deployment of models. The domain adaptation literature addresses this concern, albeit with the notion of stability limited to that of prediction accuracy. We identify sufficient conditions under which stable models, both in terms of prediction accuracy and fairness, can be learned. Using the causal graph describing the data and the anticipated shifts, we specify an approach based on feature selection that exploits conditional independencies in the data to estimate accuracy and fairness metrics for the test set. We show that for specific fairness definitions, the resulting model satisfies a form of worst-case optimality. In context of a healthcare task, we illustrate the advantages of the approach in making more equitable decisions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>â€¢ Computing methodologies â†’ Learning under covariate shift; Causal reasoning and diagnostics.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Deployment of machine learning algorithms to aid consequential decisions, such as in medicine, criminal justice, and employment, require revisiting the dominant paradigms of training and testing such algorithms. Particularly, the assumption that the data distribution in training and deployment will be the same is not always warranted. Examples of the impact of distribution shift can be found in medical imaging tasks <ref type="bibr" target="#b56">[57,</ref><ref type="bibr" target="#b74">75]</ref>, where the algorithms trained on one chest radiography dataset performed poorly on other datasets. Similarly, Nestor et al. <ref type="bibr" target="#b45">[46]</ref> find that models for critical care tasks degraded in performance over time resulting from changes in the instrumentation of the electronic health records. Given the safetycritical nature of the decisions, the decision-making process should account for these shifts in distributions to ensure high predictive accuracy of the algorithms.</p><p>Many methods exist to learn under distribution shifts <ref type="bibr" target="#b57">[58]</ref>, including recent work from a causal inference perspective <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b67">68]</ref>. Such methods have significant appeal since they allow learning accurate models for arbitrary shifts, including those in unseen future data. This is achieved by exploiting causally-relevant factors in data that are generalizable to unseen test sets, as opposed to fitting to the factors specific to the training sets. However, the focus of the methods has been on average case prediction performance alone. In certain circumstances, while high predictive accuracy is a necessary requirement, decisions made using the algorithms should also not lead to or perpetuate past disparities among groups in the data. Without any design changes, algorithmic solutions for mitigating distribution shifts that do not account for disparities in training data can result in disparate impact while predicting under distribution shifts. We discuss a concrete example later.</p><p>At the same time, most work in algorithmic fairness addresses the setting with a single learning task (or domain) under the assumption that the data distribution does not change between train and test settings <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b22">23]</ref>. Under this assumption, minimizing classification risk along with constraints on the fairness metric in training data is likely to generalize to identically distributed test data. Thinking about shifts in fair machine learning is also important though, since deployment of a (fair) decision-making tool might affect what data is collected in future (e.g. selectively policing locations with high predicted risk <ref type="bibr" target="#b35">[36]</ref>), or might incentivize individuals to strategically adapt their features for favourable outcomes <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b34">35]</ref>, thus, causing distribution shift. In addition, due to data-scarcity, such as in medical decision-making <ref type="bibr" target="#b72">[73]</ref>, the models may be applied to newer settings (such as hospitals) than the ones seen during training. The issue of ensuring fairness when deployment environment differs from the training one has received little attention <ref type="bibr" target="#b62">[63]</ref>. Due to the variety of train-test shifts that can occur, conceptualizing and addressing the problem has been challenging.</p><p>Our contributions. We address the problem of learning fair models under mismatch in train-test distributions when either limited or no data is available from the test distribution. We consider the setup of causal domain adaptation where possible shifts are expressed using causal graphs with the goal of learning models with stable performance under the specified shifts. Our main contribution is to formulate the fair learning problem in this setup and provide sufficient conditions that enable estimation of model accuracy and fairness metrics in the test domain. For a subset of covariate shifts and for several well-known group-fairness metrics, we show that the resulting solution is worst-case optimal. We operationalize the sufficient conditions in an algorithm based on a reduction to the standard fair learning problem. Finally, we present a case study on a medical decision-making task which demonstrates applicability of the approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Domain adaptation and fair machine learning are both widely studied problems. Thus, we primarily focus the discussion on literature at their intersection.</p><p>Fairness. A number of fairness metrics have been proposed that make different normative statements on the machine learning models' output (see <ref type="bibr" target="#b41">[42]</ref> for a review). Depending on the application context, different metrics might be appropriate or mandatory by law <ref type="bibr" target="#b44">[45]</ref>. Consequently, fairness methods have been developed to build/modify models that satisfy different fairness criteria. We focus on a class of methods that pose the problem as that of constrained optimization <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b14">15]</ref>. Domain adaptation. The seminal work of Ben-David et al. <ref type="bibr" target="#b2">[3]</ref> relates the target domain error to the source domain error and the distance between the distributions. This inspired many domain adaptation methods based on adversarial training of representations to align the distributions <ref type="bibr" target="#b18">[19]</ref>. One drawback is that the methods require some data from test distribution while training. When causal structure of the domains is known, recent work on causal domain adaptation <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b60">61,</ref><ref type="bibr" target="#b65">66,</ref><ref type="bibr" target="#b67">68]</ref> identify predictors with stable accuracy under unseen changes in distribution. To accomplish this, the methods exploit the principle of invariance of causal mechanisms <ref type="bibr" target="#b49">[50,</ref><ref type="bibr">Sec. 1.3</ref>] that says -interventions (or shifts) in certain mechanisms in the graph keep the other mechanisms unchanged. The invariant mechanisms can be used to build stable predictors. Similar to <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b67">68]</ref>, we adopt a setting where a causal graph specifies anticipated distribution shifts and no target domain data is given (but can be used if available). The goal is to construct predictors that are invariant to all anticipated shifts, without necessarily observing the corresponding data. The setting is particularly well-suited for consequential decision-making where we want to proactively guard against shifts that may result in harm, before deploying the model and collecting target data. However, none of these methods consider the possibility of unfair outcomes after adaptation.</p><p>Fairness and domain adaptation. On multiple benchmark datasets, Friedler et al. <ref type="bibr" target="#b17">[18]</ref> found that fair machine learning methods showed high variance in achieved accuracy and fairness on randomly split train-test sets. To mitigate this, Huang and Vishnoi <ref type="bibr" target="#b26">[27]</ref> propose adding a regularization term to the constrained ERM problem that guarantees stability. However, the term stability is used for changes in the fairness metric as a training data sample is removed/added, as opposed to changes under different distributions. In <ref type="bibr" target="#b12">[13]</ref>, authors propose algorithms for generalisation of fairness constraints but to an i.i.d. test set. In <ref type="bibr" target="#b36">[37]</ref>, the authors propose learning feature representations, using adversarial training, which result in fair classifiers when trained on the representations. They do not address changes in distribution of the features (and their representations) across domains.</p><p>In the same setup as ours under the assumption of covariate shift but with the availability of unlabelled target data, <ref type="bibr" target="#b11">[12]</ref> give weighting-based estimators and <ref type="bibr" target="#b58">[59]</ref> take a robust optimization approach. Other works that assume some labelled data from the target domain include <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b62">63,</ref><ref type="bibr" target="#b64">65]</ref>. For instance, <ref type="bibr" target="#b48">[49]</ref> learns a representation from multiple domains with guarantees on generalization to the target domain, but requires labelled target data to fine-tune classifiers and a low-rank assumption that constrains dis-similarity between the domains. In <ref type="bibr" target="#b64">[65]</ref>, authors restrict to shifts in feature means and propose ways to flag a potentially unfair model under such shifts. Further, concurrent work <ref type="bibr" target="#b38">[39]</ref> posits a set of test distributions defined as weighted combinations of the training data, and find a fair classifier minimizing the worst loss across such distributions. Instead, we rely on distributional assumptions expressed using a causal graph. Considering the causal structure of the problem allows the modeller to express plausible distribution shifts more intuitively by denoting the mechanisms, instead of the statistical properties, that can change. It also guides the construction of estimators that are robust against shifts of arbitrary magnitude rather than only the shifts in the observed datasets.</p><p>Our work is related in spirit to <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b28">29]</ref> who consider building fair models from 'biased' training data. Here, we provide a complementary set of results on fairness under train-test distribution mismatch, avoiding assumptions on specific generative processes for the shift. Instead we use causal graphs to make weaker assumptions on where the mismatch is. This allows us to give a general characterization of the addressable mismatch settings. Moreover, at a conceptual level, our focus is on addressing mismatch with multiple future test sets rather than a biased training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PROBLEM SETUP</head><p>Let us denote all the variables associated with the system being modelled as V := (X, ğ´, ğ‘Œ ), where ğ´ is the sensitive attribute, X is a non-empty set of covariates other than ğ´, and ğ‘Œ is an outcome of interest. We will consider a binary sensitive attribute, ğ´ âˆˆ {a, d} (i.e. advantaged and disadvantaged group), and the binary classification case, thus, ğ‘Œ âˆˆ {0, 1}. For simplicity of exposition, consider the case with only two domains -a source and a target -with joint probability distributions ğ‘ƒ source and ğ‘ƒ target , respectively. Crucially, the two distributions may be different (e.g. data from two hospitals with different care practices). Bold letters are used for vectors, uppercase for random variables, and lowercase for instantiations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Fair classifier</head><p>Consider that the classifier is built from the feature (sub)set S âŠ† {X, ğ´} and outputs the binary prediction ğ‘“ (S) âˆˆ {0, 1}. <ref type="foot" target="#foot_0">1</ref> We will operate in the empirical risk minimization framework for learning classifiers and introduce additional fairness constraints in the objective to control the inter-group disparity, a commonly-used approach <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b73">74]</ref>. Each constraint is given by some function ğº of the prediction, outcome, and features. Denote the constraint by ğº (ğ‘“ (S), (ğ‘Œ, S)) â‰¤ ğœ– with (ğ‘Œ , S) âˆ¼ ğ‘ƒ target (ğ‘Œ, S) and some hyperparameter ğœ– â‰¥ 0 allowing for approximate fairness. If there are multiple constraints, we write the set of constraints succinctly as G(ğ‘“ , ğ‘ƒ target ) â‰¤ ğ. Note that the desired fairness constraint G is assumed to be the same in both the domains. The classification error i.e. probability of a misclassification is written as ğ‘ƒ (ğ‘“ (S) â‰  ğ‘Œ ). Then, the fair domain adaptation (DA) problem amounts to finding a minimizer</p><formula xml:id="formula_0">[Fair DA] ğ‘“ * target := arg min ğ‘“ âˆˆ F (S) {ğ‘ƒ target (ğ‘“ (S) â‰  ğ‘Œ ) : G(ğ‘“ , ğ‘ƒ target ) â‰¤ ğ }<label>(1)</label></formula><p>i.e. a function ğ‘“ * target in the set of learnable functions F (S) of features S that minimizes classification error as well as satisfies fairness constraints.</p><p>Fairness metrics. We will focus on group-fairness metrics defined based on some notion of parity across groups. These have received much attention in the fair machine learning literature <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b22">23]</ref> due to the relative ease of communicating their implications to stakeholders and the ease of computing them from observational data. We define two more metrics derived from EO. If we condition only on ğ‘Œ = 1, the resulting metric is known as true positive rate equality (TPR), or more commonly equality of opportunity <ref type="bibr" target="#b22">[23]</ref>. Similarly, for ğ‘Œ = 0, the metric is known as true negative rate equality (TNR).</p><p>Solving (1) requires estimating the error ğ‘ƒ target (ğ‘“ (S) â‰  ğ‘Œ ) and the fairness constraint G(ğ‘“ , ğ‘ƒ target ). Given enough samples from ğ‘ƒ target , standard fair learning methods e.g. <ref type="bibr" target="#b0">[1]</ref> return a solution. But, this is not possible in the Fair DA setting, as we do not have access to the complete target data. Thus, the central question we ask is: Under what assumptions can we still find ğ‘“ * target ? For arbitrary distribution shifts, it is not possible to answer this question in affirmative. With background knowledge of how the distributions differ, past work provides methods to bound the target domain error. Crucially, such methods still do not guarantee target domain fairness and using fairness constraints from the source domain, naturally, does not solve (1). Through the following example, we illustrate that these design choices can significantly affect accuracy and fairness of the models. It also shows how the causal inference framework for domain adaptation allows for the specification of shifts and design of predictors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">An illustrative example.</head><p>Consider a simplified version of the flu diagnosis task from <ref type="bibr" target="#b40">[41]</ref>. The associated data generating process is shown in Figure <ref type="figure" target="#fig_5">1a</ref>. Flu status ğ‘Œ of a person is to be predicted from three measurements {ğ‘‡ , ğ‘…, ğ´}. The disease has two known causes ğ‘… and ğ´, say virus-exposure risk and age group (indicating adult or child) respectively. In addition, a noisy yet predictive symptom of flu is observed as ğ‘‡ , say body temperature, which is expressed differently depending on the age group. A categorical variable ğ¶ indicates different data collection sites (the domains) which differ on (i) how well the temperature is measured, e.g. self-reported vs. clinician-tested (ğ¶ â†’ ğ‘‡ ), and (ii) the proportion of demographics across sites (ğ¶ â†’ ğ´). Suppose, a classifier Å¶ is to be built using data from a single site (source domain) and used in multiple sites (target domains) to allocate scarce healthcare resources (testing kits, medical consultation) to individuals. The model designer would like to mitigate differential error rates across age groups and chooses to use EO as the fairness constraint while learning Å¶ .</p><p>We compare three ways of designing the model that account differently for the possibility of shift and unfairness. Figures <ref type="figure" target="#fig_5">1b,</ref><ref type="figure" target="#fig_5">1c</ref> show results from a simulation, discussed in detail in Section 6.2. As we vary the magnitude of distribution shift between the sites, the Standard classifier, built by regressing ğ‘Œ on {ğ‘‡ , ğ‘…, ğ´} from source data degrades in accuracy (blue curve) on target data. By accounting for the shift, CausalDA, a domain adaptation approach <ref type="bibr" target="#b60">[61]</ref> that only uses the features {ğ‘…, ğ´}, remains stable (orange curve). Surprisingly, domain adaptation leads to higher levels of fairness violations, as shown in Figure <ref type="figure" target="#fig_5">1c</ref>. To mitigate this we would want to learn CausalDA with fairness constraints which is complicated, as discussed earlier, since we cannot evaluate the constraints for unseen target domains. However, following the method proposed in Section 5, learning CausalDA with fairness constraints on the source domain (red curve) retains both the desired propertiesconsistently high accuracy and low unfairness. Thus, the example illustrates the need to consider fairness constraints while adapting for the shifts.</p><p>Next, we describe the joint causal graphs in more detail that allow us to represent the potential shifts, followed by our main results on learning fair and stable predictors under specific shifts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">JOINT CAUSAL INFERENCE AND DOMAIN ADAPTATION</head><p>Following recent work <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b60">61]</ref>, we consider a joint causal graph which represents the data distribution for all domains. This allows us to reason about the invariant distributions under shifts, which is key to addressing the fair domain adaptation problem. Assume that all the source and the target domains are characterized by a set of variables V, which are observed under different    contexts (e.g. experimental settings) particular to each domain. Joint Causal Inference <ref type="bibr" target="#b42">[43,</ref><ref type="bibr">Sec. 3</ref>] framework provides a way of representing the data generating process for all domains as a single causal graph representing an underlying causal model. In addition to the system variables V, the framework introduces an additional set of exogenous variables, named context variables C , that represent the modeler's knowledge of how the domains differ from one another (given by the causal relations among the system and context variables). <ref type="foot" target="#foot_1">2</ref> We include the formal definition of JCI framework in Appendix D along with the necessary assumptions on faithfulness, and Markov property. For the example in Figure <ref type="figure" target="#fig_5">1a</ref>, system variables are {ğ‘‡ , ğ‘…, ğ´, ğ‘Œ }. With a binary context variable ğ¶, ğ‘ƒ (ğ‘‡ , ğ‘…, ğ´, ğ‘Œ | ğ¶ = 0) and ğ‘ƒ (ğ‘‡ , ğ‘…, ğ´, ğ‘Œ | ğ¶ = 1) correspond to joint distributions for the two domains, source and target. More generally, setting context variable to a particular value, say C = c, can be seen as an intervention that results in the data distribution for a domain ğ‘ƒ (V | C = c). <ref type="foot" target="#foot_2">3</ref>A class of causal domain adaptation problems is to learn a predictor that generalizes to different target data distributions which correspond to different settings of the context variables in the causal graph. In <ref type="bibr" target="#b37">[38]</ref>, authors propose learning a predictor using only a subset of the features that guarantee invariance of the outcome distribution conditional on the chosen feature subset. More specifically, if V = (X, ğ´, ğ‘Œ ) and C are the context variables, the desired subset of features S âŠ† {X, ğ´} satisfies ğ‘Œ âŠ¥ âŠ¥ C | S, implying that the conditional distribution of outcome ğ‘Œ given the features S is invariant to the effect of domain changes. The set S is referred to as a separating set as it d-separates ğ‘Œ and C in the joint causal graph. This criterion generalizes the covariate shift criterion <ref type="bibr" target="#b68">[69]</ref>, which assumes independence between ğ‘Œ and ğ¶ conditioned on all the features. Note that the separating set criterion excludes graphs where C directly causes ğ‘Œ , known as label shift. The predictor using the separating set satisfies a desirable optimality property. As shown in <ref type="bibr" target="#b60">[61]</ref>, it has the lowest mean squared loss against any distribution having the same outcome distribution ğ‘Œ | S as in the source.</p><p>However, using a separable set in itself does not guarantee fairness. For example, separating sets for Figure <ref type="figure" target="#fig_5">1a</ref> are S âˆˆ {{ğ´}, {ğ´, ğ‘…}}. But neither satisfies the condition required for EO, in general, i.e. ğ‘“ (S) âŠ¥ âŠ¥ ğ´ | ğ‘Œ . Thus, to ensure both invariance and fairness, we restrict our search space in Fair DA (1) to F (S), i.e. the set of predictors built using the separating set S. Next, we describe the assumptions that allow us to solve this problem. All proofs are included in Appendices A-C in the supplemental material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">FAIR DOMAIN ADAPTATION</head><p>Now, we return to our problem of finding fair classifiers for the target domain and describe how the joint causal graph helps in solving <ref type="bibr" target="#b0">(1)</ref>. In the context variable notation, we are interested in finding arg min</p><formula xml:id="formula_1">ğ‘“ âˆˆ F (S) {ğ‘ƒ (ğ‘“ (S) â‰  ğ‘Œ |ğ¶ = 1) : G(ğ‘“ , ğ‘ƒ (ğ‘Œ, S|ğ¶ = 1)) â‰¤ ğ }</formula><p>where ğ¶ = 1 represents the target domain. We start by noting the need for further assumptions.</p><p>Proposition 1. Fair DA problem (1) is not solvable in general without further assumptions.</p><p>Proposition <ref type="bibr" target="#b0">(1)</ref> follows by the impossibility results on domain adaptation <ref type="bibr" target="#b3">[4]</ref>. Even when domain adaptation is possible, i.e. target domain error is identifiable (uniquely estimable in terms of source domain distribution), the fairness constraint is not guaranteed to be identifiable. We make this point by constructing an example with group-specific measurement error in features.</p><p>Thus, the natural question is under what conditions on distributions and assumptions on data availability can we identify the error ğ‘ƒ (ğ‘“ (S) â‰  ğ‘Œ |ğ¶ = 1) and the fairness constraint G(ğ‘“ , ğ‘ƒ (ğ‘Œ , S|ğ¶ = 1)). We make the following two assumptions for the selected features S âŠ† {X, ğ´} for the classifier. For example, the condition for DP asserts that the characteristics (in terms of features S) of the sensitive groups are invariant across domains. Similarly, the condition for EO says that feature distribution for groups defined by the label and the sensitive attribute is invariant across domains. This ensures that we can evaluate (and hence balance) the corresponding fairness constraint irrespective of the domain.</p><p>Next, we consider two scenarios to state the quality of the solution that can be found under the two assumptions -(i) when labelled source and unlabelled target domain data is available, alternatively, (ii) when only the labelled source domain data is available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Fair domain adaptation with limited target domain data</head><p>Proposition 2. Given Assumptions 1 and 2 hold, then using only labelled source and unlabelled target data, the Fair DA problem (1) can be solved exactly by a data re-weighting method.</p><p>Proof sketch. This follows since the error is invariant, i.e. ğ‘ƒ (ğ‘“ (S) â‰  ğ‘Œ |S, ğ¶ = 1) = ğ‘ƒ (ğ‘“ (S) â‰  ğ‘Œ |S, ğ¶ = 0), due to Assumption 1. This implies that</p><formula xml:id="formula_2">E ğ‘Œ ,S (ğ‘ƒ (ğ‘“ (S) â‰  ğ‘Œ |S, ğ¶ = 1)) = E ğ‘Œ ,S (ğ‘¤ (S) Ã— ğ‘ƒ (ğ‘“ (S) â‰  ğ‘Œ |S, ğ¶ = 0))</formula><p>where weights, ğ‘¤ (S) = ğ‘ƒ (S|ğ¶ = 1)/ğ‘ƒ (S|ğ¶ = 0), are the ratio of feature densities. Under Assumption 2, the fairness constraint is invariant, i.e. G(ğ‘“ , ğ‘ƒ (ğ‘Œ, S|ğ¶ = 1)) = G(ğ‘“ , ğ‘ƒ (ğ‘Œ, S|ğ¶ = 0)). To solve (1), we find arg min</p><formula xml:id="formula_3">ğ‘“ âˆˆ F (S) {ğ‘¤ (S)ğ‘ƒ (ğ‘“ (S) â‰  ğ‘Œ |ğ¶ = 0) : G(ğ‘“ , ğ‘ƒ (ğ‘Œ, S|ğ¶ = 0)) â‰¤ ğ } .</formula><p>Both the error and the constraint are estimable as we have labelled source data sampled from ğ‘ƒ (ğ‘Œ, S|ğ¶ = 0). The remaining term is the density ratio ğ‘¤ (S) used to re-weight the error. Since we have features from both source and target in this scenario, ğ‘¤ (S) can be computed, for instance, using a probabilistic classifier for discriminating between the domains <ref type="bibr" target="#b4">[5]</ref>. â–¡</p><p>This solution strategy is akin to the importance-weighting approach of addressing covariate shift <ref type="bibr" target="#b63">[64,</ref><ref type="bibr" target="#b68">69]</ref>, with the distinction being the use of the separating feature set instead of all the features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Fair domain adaptation with no target domain data</head><p>In the scenario when only the labelled source data is available, we cannot use Proposition (2) since we cannot estimate the weights. Instead, we use the source data with the selected features, f * âˆˆ arg min</p><formula xml:id="formula_4">ğ‘“ âˆˆ F (S) {ğ‘ƒ (ğ‘“ (S) â‰  ğ‘Œ |ğ¶ = 0) : G(ğ‘“ , ğ‘ƒ (ğ‘Œ, S|ğ¶ = 0)) â‰¤ ğ } ,</formula><p>with S satisfying Assumptions 1 and 2 (2) Next, we show that this solution minimizes the worst-case error under fairness constraints among target distributions satisfying the two assumptions with respect to the feature subset S. Such a property might be desirable for models aiding consequential decisionmaking as it guarantees good performance under the worst possible target distribution. In other words, the solution to (2) will perform well for any target distribution we may encounter, as long as the distribution adheres to the stated assumptions.</p><p>Denote the set of continuous functions which satisfy the fairness constraints G with respect to the distribution ğ‘ƒ by</p><formula xml:id="formula_5">F (G, ğ‘ƒ) := {ğ‘“ âˆˆ C 0 : G(ğ‘“ , ğ‘ƒ) â‰¤ ğ },</formula><p>where C 0 denotes the set of all continuous functions. Let P denote the distributions over (X, ğ´, ğ‘Œ ) that satisfy Assumptions 1 and 2 for some features S. Then, the set F (G, ğ‘ƒ) is the same for any distribution ğ‘ƒ âˆˆ P.</p><formula xml:id="formula_6">Lemma 1. F (ğº, ğ‘ƒ) = F (ğº, ğ‘„), âˆ€ ğ‘ƒ, ğ‘„ âˆˆ P</formula><p>By Assumption 2, if G(ğ‘“ , ğ‘„) holds then G(ğ‘“ , ğ‘ƒ) also holds. Thus, the two sets are the same. Therefore, we can denote the set of fair functions by F (G, P).</p><p>For the next result, we will restrict to three fairness definitions (DP, TPR, or TNR) and assume that the conditional outcome, i.e. the random variable ğ‘ƒ (ğ‘Œ = 1|X, ğ´, ğ¶ = 1), has strictly positive density on [0, 1]. This technical condition allows us to characterize the optimal predictors in F (G, P), following Corbett-Davies et al. <ref type="bibr" target="#b10">[11]</ref>.</p><p>Theorem 1 (Worst-case optimality). Consider the set of distributions P satisfying Assumptions 1 and 2 which are absolutely </p><formula xml:id="formula_7">f * âˆˆ arg min ğ‘“ âˆˆ F (G,P) sup ğ‘ƒ âˆˆ P ğ‘ƒ (ğ‘“ (X, ğ´) â‰  ğ‘Œ )<label>(3)</label></formula><p>That is, the proposed approach achieves minimum worst-case error amongst the fair predictors with respect to the distributions satisfying the two assumptions. We note that the assumption of absolute continuity in Theorem 1 is made to avoid cases where source and target distributions have disjoint support, which would make generalization challenging if some parts of the feature space are not observed at all in the source domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Practicality of assumptions</head><p>Assumptions 1 and 2 together describe the types of shifts that our approach can address. Graphically, these are characterized as (a) shifts with causal paths to ğ‘Œ which all include ğ´ (i.e. ğ¶â€¢ â€¢ â€¢ â†’ğ´â†’ â€¢ â€¢ â€¢ğ‘Œ with all arrows toward ğ‘Œ ), and (b) shifts with non-causal paths to ğ‘Œ (i.e. ğ¶â€¢ â€¢ â€¢ â†’ğ‘€â† â€¢ â€¢ â€¢ğ‘Œ for some feature ğ‘€ âˆˆ S). This means that any shift causing change in the distribution of the sensitive attribute as well as any shift in variables with a non-causal path to ğ‘Œ can be addressed. Figure <ref type="figure" target="#fig_6">2</ref> gives an example of both the cases (described in more detail in Appendix F). Shifts in distribution of sensitive attribute are common when there is sample selection bias e.g. patient demographics being different between rural and urban hospitals. In Section 6.4, we demonstrate a general class of shifts in medical diagnosis tasks where both the assumptions are satisfied. Finally, we note that the assumptions (barring those for DP) are untestable without access to labelled target data. The reason for untestability is the same as that for no unmeasured confoundingwe do not observe the (counterfactual) target data, and hence cannot test for conditional independence. Thus, background knowledge of plausible shifts are critical.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Proposed algorithm</head><p>The approach described in (2) suggests a simple algorithm based on feature selection followed by solving the standard fair learning problem. We assume that the following are given -a causal graph for the system of interest G and data from a source domain D source = {(X ğ‘– , ğ´ ğ‘– , ğ‘Œ ğ‘– )} ğ‘› ğ‘–=1 . The steps, outlined in Algorithm 1, are as follows. (a) Iterate over all feature subsets to rank them in increasing order of their empirical error on the source domain. (b) Starting from the feature set with the least error, check for Assumptions 1 and 2 using ğ‘‘-separation <ref type="bibr" target="#b49">[50]</ref> in G. (c) Solve the fair learning problem with D source limited to model class F (S). This can be achieved by a fair learning algorithm, such as <ref type="bibr" target="#b0">[1]</ref>, chosen based on the model class and the fairness definition. If there is no S satisfying the assumptions, we do not return a solution.</p><p>The time complexity is dominated by the search over feature subsets in (a) which is exponential in number of features. To reduce the combinatorial search, we can run a feature selection procedure, e.g. the lasso in case of linear models [24, Chapter 3], to prune non-predictive features. Another heuristic is to start with the set of causal parents of Y (which satisfy Assumption 1) and prune it to get a subset satisfying Assumption 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Extension to Counterfactual Fairness</head><p>Another set of fairness definitions based on the causal effect of the sensitive attribute on the prediction have been proposed <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b43">44]</ref>. We consider one version of these counterfactual fairness definitions. One method to build a classifier ğ‘“ (S) satisfying Ctf is to only use feature set S âˆˆ {X, ğ´} that does not contain any descendant of ğ´ in the causal graph [32, <ref type="bibr">Lemma 1]</ref>.</p><p>Thus, the counterpart of Assumption 2 for solving Fair DA under Ctf is that the selected feature set contains the non-descendants of ğ´. Combined with Assumption 1, we select non-descendants of ğ´ which form a separating set in order to solve Fair DA. Since, Ctf only requires change in feature subset and does not include any fairness constraints in the fair learning problem, we can show the worst-case optimality result as well (described in Appendix E).</p><p>However, we note that there are multiple ways of defining counterfactual fairness. For instance, <ref type="bibr" target="#b43">[44]</ref> require that causal effects of ğ´ on Å¶ through particular paths should be zero or small. Further work should explore approaches to solve Fair DA under broader definitions of counterfactual fairness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXPERIMENTS</head><p>The experiment settings explained next are designed to evaluate performance (accuracy and fairness) of the proposed classifier, trained using a source dataset, on unseen target datasets. The constrained learning problem in (2) is solved using the algorithm by <ref type="bibr" target="#b0">[1]</ref>, referred henceforth as FairLearn, which converts the problem into a sequence of weighted cost-sensitive classification problems. Predictive performance is measured using accuracy (percentage correct),  area under ROC and precision-recall curves (AUPRC). For the experiments presented here, we use EO as the desired fairness constraint. To evaluate (un)fairness, we report the maximum violation of the EO constraint, i.e. max ğ‘Œ âˆˆ {0,1},ğ´âˆˆ {a,d} ğ‘ƒ (ğ‘“ (S) | ğ‘Œ, ğ´) -ğ‘ƒ (ğ‘“ (S) | ğ‘Œ ) .</p><formula xml:id="formula_8">ğ´ âˆ¼ Bernoulli (ğœ (ğ›¾ â€¢ ğœ† 1 â€¢ ğ¶ + ğ‘¢ 1 )) ğ‘… âˆ¼ N (0, 1) + ğœ† 2 â€¢ ğ´ + ğ‘¢ 2 ğ‘Œ âˆ¼ Bernoulli (ğœ (ğœ† 3 â€¢ ğ´ + ğœ† 4 â€¢ ğ‘… + ğ‘¢ 3 )) ğ‘‡ = ğœ† 5 â€¢ ğ‘Œ + ğœ† 6 â€¢ ğ‘… + ğœ† 7 â€¢ ğ´ + N (0, ğ›¾ â€¢ ğœ† 8 â€¢ ğ¶) + ğ‘¢ 4 ğ‘¢ 1 , ğ‘¢ 2 , ğ‘¢ 3 âˆ¼ N (0, 0.8 2 ), ğ‘¢ 4 âˆ¼ N (0, 1.0) (ğœ† 1 , ğœ† 2 , ğœ† 3 , ğœ† 4 ) = (0.2, -0.1, -0.8, 0.8) (ğœ† 5 , ğœ† 6 , ğœ† 7 , ğœ† 8 ) = (0.8, 0.1, -0.8, 0.2) ğ›¾ âˆˆ [0, 15] ğœ (ğ‘¥) = 1/(1 + exp(-ğ‘¥))</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Baselines.</head><p>We consider five baselines which account for either distribution shift, unfairness, both, or none of these.</p><p>â€¢ Standard is the optimal un-constrained classifier with all available features, i.e. ğ‘“ (V \ ğ‘Œ ). â€¢ CausalDA is the classifier with the separating set, i.e. ğ‘“ (S) s.t. ğ¶ âŠ¥ âŠ¥ ğ‘Œ | S. â€¢ OTDA is an optimal transport-based method for unsupervised domain adaptation <ref type="bibr" target="#b13">[14]</ref>.</p><formula xml:id="formula_9">â€¢ Standard+FairLearn is ğ‘“ (V \ ğ‘Œ ) trained with FairLearn.</formula><p>â€¢ Finally, CausalDA+FairLearn is the proposed method i.e.</p><p>ğ‘“ (S) trained with FairLearn where S satisfies Assumptions 1 and 2.</p><p>Results on another method, anchor regression <ref type="bibr" target="#b61">[62]</ref>, are included in Appendix H. Since this method requires data from multiple sources, we evaluate it against the above methods in a separate experiment setting. Hyperparameters used for the methods are reported in Appendix I. <ref type="bibr" target="#b3">4</ref>. Code for reproducing results on synthetic data is at <ref type="url" target="https://github.com/ChunaraLab/fair_domain_adaptation">https://github.com/ChunaraLab/fair_domain_adaptation</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Synthetic data example</head><p>Setup. For the flu example in Figure <ref type="figure" target="#fig_5">1a</ref>, we generate data from a structural equation model described in Figure <ref type="figure" target="#fig_9">3a</ref> with linear relationships and logit link function for binary variables. To generate target domains, we perform soft interventions <ref type="bibr" target="#b39">[40]</ref> to shift distributions of ğ´ and ğ‘‡ . The shift magnitude is governed by a multiplier ğ›¾ in the linear equations. In total, 50 pairs of source and target datasets are simulated with ğ‘ = 2000 samples in each dataset. The proportion of disadvantaged group in source is kept at roughly 0.5. In target domains with an extreme value for ğ›¾ = 15, the ratio shifts to roughly 0.94. Class ratio is varied from 0.5 to 0.36 with increase in ğ›¾. From Figure <ref type="figure" target="#fig_5">1a</ref>, we observe that S={ğ´, ğ‘…} satisfies the two assumptions. Adding ğ‘‡ (a collider) to S makes the predictor dependent on ğ¶ and, thus, unstable. We use logistic regression models in all experiments. In Figure <ref type="figure" target="#fig_9">3b</ref>, the goal is to find a classifier performing well on both accuracy and fairness, i.e. one close to the right-hand bottom corner.</p><p>Results. For a high magnitude of shift, Figure <ref type="figure" target="#fig_9">3b</ref>, domain adaptation (CausalDA) leads to considerably higher accuracy than using all features (Standard), but results in high unfairness. Learning with fairness constraints (CausalDA+FairLearn) which results in low unfairness with a minimal loss in accuracy even when the domains differ significantly. As seen in Figure <ref type="figure" target="#fig_9">3c</ref>, for low magnitudes of shift, CausalDA+FairLearn still has low unfairness but results in a pessimistic accuracy estimate as it accounts for larger shifts than are seen in the target domain. Thus, in practice, the choice of method will depend on the expected magnitude of shift.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Synthetic data example: additional results</head><p>Varying magnitude of shift. To check robustness of different models to distribution shift, we generate target datasets with different values of ğ›¾ in the linear structural equations in Section 6.1. Figure <ref type="figure" target="#fig_18">5 (a,</ref><ref type="figure">b,</ref><ref type="figure">c</ref>), included at the end, shows two predictive performance metrics -Accuracy (percentage correct), AUROC -and one fairness metric -maximum fairness violation -for different magnitudes of shift. We observe the same trends as reported in Section 6.1, i.e. CausalDA (orange curve) achieves stable predictive performance but leads to high unfairness, whereas CausalDA+FairLearn (red curve) achieves both stable predictive performance and low unfairness.</p><p>Results with demographic parity.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Case study: diagnosing Acute Kidney Injury</head><p>Acute Kidney Injury (AKI) is a condition characterized by an acute decline in renal function, affecting 7-18% of hospitalized patients and more than 50% of patients in the intensive care unit (ICU) <ref type="bibr" target="#b7">[8]</ref>. The condition can develop over a few hours to days and early prediction can greatly reduce the fatalities associated with the condition. Hence, building models for predicting AKI risk from clinical data is an active area of research. Such models can be used to risk-stratify patients to screen them for close monitoring or to perform further diagnostics to guide course of treatment <ref type="bibr" target="#b25">[26]</ref>. Importantly, AKI incidence has well-documented disparities across groups defined by race and sex <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>. Thus, introduction of risk prediction tools for guiding clinical care has a potential to perpetuate such disparities, or alternatively, to address them through a more deliberate design of the prediction tools. A recent study <ref type="bibr" target="#b69">[70]</ref> showed good predictive performance for AKI based on patient data provided by the U.S. Department of Veteran Affairs. However, the female population was severely underrepresented in the data, which raises concern over differential error rates when deployed in a different population. Therefore, to analyze the fairness across sensitive groups, we conduct experiments on MIMIC III, a publicly-available critical care dataset <ref type="bibr" target="#b27">[28]</ref>. We extract variable types, mentioned in caption of Figure <ref type="figure" target="#fig_9">3</ref>, for around 24ğ¾ patients. Pre-processing steps are described in Appendix I.2. We use a simplified causal graph for the AKI diagnosis task, Figure <ref type="figure">4a</ref>, based on the one used by <ref type="bibr" target="#b65">[66]</ref> for a sepsis diagnosis task. The group sex=female is taken as the sensitive attribute to assess fairness of the predictions. In this case study, the AKI risk score is not intended to prescribe treatment, but to flag a patient for extra care resources e.g. by alerting clinical staff. Thus, the potential harm that we want to avoid is groups having unequal opportunity to such care resulting from group differences in prediction errors.</p><p>Setup. Patient encounters are randomly split (2:1) into source and target data. We artificially introduce two types of shifts -(a) change in female proportion, and (b) change in measurement policy, where a lab test is prescribed less often -some of the factors affecting model performance across clinical settings <ref type="bibr" target="#b59">[60]</ref>. We randomly downsample female population by rejecting each row in the source data from that group with probability 50%. This shifts the proportion of females from 40% to 25%. Also, we randomly choose 50% encounters in the target data and add missing values for the Blood Urea Nitrogen (BUN) test, a biomarker of AKI <ref type="bibr" target="#b15">[16]</ref>. Results with other missingness proportions are included in Appendix I.3.</p><p>From Figure <ref type="figure">4a</ref>, we note that S={D, M, X\BUN} satisfies the two assumptions. We report AUPRC in Figure <ref type="figure">4c</ref>, instead of accuracy, as it is less sensitive to class imbalance (class ratio is 0.21). All results are reported for classifiers trained with gradient boosting trees. We drop OTDA from comparison due to its low accuracy and high running time for this dataset.</p><p>Results. We find that classifiers with separating feature set perform significantly better in AUPRC compared to those with all features (exact numbers are reported in Appendix I.3). Further, CausalDA+FairLearn improves fairness in target domain, reducing fairness violation by 47% with 0.8% decrease in AUPRC. Thus, the experiments provide preliminary evidence that our method can learn stable classifiers while being fair for a class of shifts in diagnosis tasks denoted by Figure <ref type="figure">4a</ref>. Note that the setup has some limitations, namely, adding missing values to perturb target data conflates the effectiveness of the procedure for handling missing data (mean value imputation in our case) with the procedure for domain adaptation. We plan to validate the approach on datasets across multiple hospitals or time points to address these limitations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">LIMITATIONS AND DISCUSSION</head><p>Knowledge of causal graph. Our approach requires the causal graph for the system being studied to check whether the two assumptions are satisfied for any given subset. While this is a requirement made by multiple domain adaptation methods <ref type="bibr" target="#b65">[66,</ref><ref type="bibr" target="#b67">68]</ref>, this can be relaxed when data from multiple domains are available. In such settings, causal discovery methods <ref type="bibr" target="#b52">[53]</ref> can be used to posit a graph and validate with domain experts. Such a procedure is demonstrated in <ref type="bibr" target="#b66">[67]</ref>. An important direction for future work includes identifying the desired feature subsets with causal discovery algorithms. We recommend that the causal graph be postulated conservatively, i.e. only adding conditional independencies that are well-substantiated by domain knowledge. In this case, if the separating features are not found, our method will output that a fair predictor is not possible instead of incorrectly returning a model that will not be fair.</p><p>Addressable shifts. In Section 5.3, we described shifts that our approach can address and presented examples. However, these are only a part of the possible shifts that a modeller may worry about. For example, shifts in direct causes of the outcome are excluded due to Assumption 2. Such shifts can result in arbitrary changes to the outcome within each group, making it impossible to balance group-specific statistics in the fairness constraint (see Appendix A for an example). These are difficult to address without making strict assumptions on magnitude of the shift or assuming access to target data. Thus, for some joint causal graphs, Algorithm 1 might not yield any feature set. In such cases, an alternative is to return the set with the least source domain risk but such a set has no generalization guarantee.</p><p>Algorithmic fairness in healthcare to promote health equity. Disparities in health outcomes and healthcare access across different groups (e.g. based on race and gender) arise from multiple reasons such as socio-economic inequities (e.g. due to structural racism) <ref type="bibr" target="#b55">[56]</ref> and clinician bias <ref type="bibr" target="#b47">[48]</ref>. Such disparities can result in differential model performance across groups as Obermeyer et al. <ref type="bibr" target="#b46">[47]</ref> finds in context of a model for identifying patients who need extra care resources. Left unaddressed, allocating resources using 'biased' models may worsen health disparities. As a consequence, a growing body of work aims to develop algorithms embodying fairness principles specific to healthcare <ref type="bibr" target="#b8">[9]</ref>. This includes constraining prediction errors across groups for the tasks of predicting risk of cardiovascular events <ref type="bibr" target="#b53">[54]</ref> or predicting healthcare costs <ref type="bibr" target="#b77">[78]</ref>. However, such group-level fairness constraints, including the ones we consider, may not match ethical desiderata in all possible healthcare settings. Some alternative constraints have been defined, for example, using counterfactuals <ref type="bibr" target="#b54">[55]</ref> or preference between group or aggregate-level models <ref type="bibr" target="#b70">[71]</ref>. We plan to investigate fair domain adaptation under broader notions of fairness. We have motivated the approach on healthcare tasks due to the importance of ensuring reliable model performance under distribution shifts in this domain. We note that the approach is more broadly applicable to other domains involving high-stakes decisions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION AND FUTURE WORK</head><p>In absence of data from new environments in which a machine learning model will be deployed, giving performance guarantees regarding predictive performance and fairness is challenging. We find that methods to address distribution shift, while controlling for decay in accuracy, can result in fairness violations. As a countermeasure, we show that it is possible to obtain accurate and fair predictors for widely-studied fairness definitions and under a large class of shifts particularly prevalent in healthcare tasks. Future work includes studying fair domain adaptation under parametric assumptions on shifts, adaptation for counterfactual definitions of fairness, and finite sample properties of the estimators. We hope that the problem setup presented here will enable further work at the intersection of fairness and causal inference.       </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A PROPOSITION 1: EXAMPLE WITH GROUP-SPECIFIC MEASUREMENT ERROR</head><p>Proof of Proposition 1. Through a simple example, we show that fair domain adaptation (DA) is not possible without making assumptions, even in cases where domain adaptation is possible.</p><p>Consider the data generating process, represented by Figure <ref type="figure" target="#fig_19">6</ref>, with two sensitive groups ğ´ âˆˆ {0, 1}, a covariate ğ‘‹ , and an outcome ğ‘Œ âˆˆ R.</p><p>ğ´ âˆ¼ Bernoulli(0.5), The group-specific errors, governed by ğ‘ƒ (ğ‘‹ | ğ´, ğ¶) in case of DP, can change arbitrarily between the domains depending on the mechanism for ğ¶ â†’ ğ‘‹ . Thus, we cannot constrain the difference in group-specific errors to satisfy DP in target. For this example, the proposed method fails to find a feature set satisfying Assumptions 1 and 2.</p><formula xml:id="formula_10">ğ‘‹ = ğ´ + ğ›¾ğ¶ Ã— ğ´ + ğœ– ğ‘‹ , ğ‘Œ = ğ‘‹ + ğ´ + ğœ– ğ‘Œ , ğœ– ğ‘‹ , ğœ– ğ‘Œ âˆ¼ N (0, 1).</formula><p>That is, the feature ğ‘‹ for the subgroup ğ´ = 1 is corrupted in the target domain ğ¶ = 1. The magnitude of corruption is governed by a constant ğ›¾ that depends on the particular target domain of interest. Then, the fair DA problem requires finding a predictor for the target domain ğ¶ = 1, given data from the source domain ğ¶ = 0, s.t.</p><formula xml:id="formula_11">ğ‘“ * = arg min ğ‘“ âˆˆ F {ğ¿ ğ¶=1 (ğ‘“ ) : ğº ğ¶=1 (ğ‘“ ) â‰¤ ğœ–}.<label>(4)</label></formula><p>We will restrict to linear models as the true distribution lies in the class of linear models. Let, ğ‘“ (ğ‘‹, ğ´; ğ›½) = ğ›½ 0 + ğ›½ 1 ğ‘‹ + ğ›½ 2 ğ´, for some unknown vector ğ›½.</p><p>Then, the fairness constraint is given by ğº ğ¶=1 (ğ‘“ )</p><formula xml:id="formula_12">= |ğ›½ 1 + ğ›½ 1 ğ›¾ + ğ›½ 2 |.</formula><p>Note that ğº ğ¶=1 (ğ‘“ ) depends on the domain ğ¶ = 1 through ğ›¾. That is, the fairness constraint ğº ğ¶=1 (ğ‘“ ) can change arbitrarily depending on the value of ğ›¾ for the particular target domain, while the source data distribution ğ¶ = 0 available to us is fixed. In other words, the fairness constraint (thought of a function of the target distribution ğ‘ƒ (ğ‘‹, ğ´, ğ‘Œ |ğ¶ = 1)) is not identifiable by the observed distribution ğ‘ƒ (ğ‘‹, ğ´, ğ‘Œ |ğ¶ = 0) alone. Thus, we cannot solve (4) without further assumptions or without data from the target domain. â–¡</p><p>We constructed the example with a regression task but the same can be shown for a classification task e.g. a logistic function for ğ‘“ (ğ‘‹, ğ´). The DP constraint will again depend on ğ›¾, in general.</p><p>The example also motivates the Assumption 2 for DP which requires identifying a feature set S s.t. ğ¶ âŠ¥ âŠ¥ S | ğ´ as it guarantees ğº ğ¶=1 (ğ‘“ ) = ğº ğ¶=0 (ğ‘“ ). With this assumption along with the separating set assumption, we can find favourable solutions for Fair DA as shown in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B PROPOSITION 2: SOLUTION USING DATA RE-WEIGHTING</head><p>Proof of Proposition 2. To show this, we consider the terms involved in Fair DA <ref type="bibr" target="#b4">(5)</ref>, namely classification error and fairness constraint, separately. arg min</p><formula xml:id="formula_13">ğ‘“ âˆˆ F (S) {ğ‘ƒ (ğ‘“ (S) â‰  ğ‘Œ |ğ¶ = 1) : G(ğ‘“ , ğ‘ƒ (ğ‘Œ, S|ğ¶ = 1)) â‰¤ ğ } (5)</formula><p>We will show that the two terms can be computed by labelled source data, sampled from ğ‘ƒ (S, ğ´, ğ‘Œ |ğ¶ = 0), and unlabelled target data, sampled from ğ‘ƒ (S, ğ´|ğ¶ = 1). Thus, showing that (5) can be indirectly solved.</p><p>The classification error can be written as,</p><formula xml:id="formula_14">ğ‘ƒ (ğ‘“ (S) â‰  ğ‘Œ |ğ¶ = 1) = E Pr(V|ğ¶=1) (1(ğ‘“ (S) â‰  ğ‘Œ )) = E Pr(ğ‘Œ ,S |ğ¶=1) (1(ğ‘“ (S) â‰  ğ‘Œ )) (6) = E Pr(S |ğ¶=1) E Pr(ğ‘Œ |S,ğ¶=1) (1(ğ‘“ (S) â‰  ğ‘Œ ))<label>(7)</label></formula><formula xml:id="formula_15">= E Pr(S |ğ¶=1) E Pr(ğ‘Œ |S,ğ¶=0) (1(ğ‘“ (S) â‰  ğ‘Œ )) (8) = E Pr(S |ğ¶=1) (ğ‘ƒ (ğ‘“ (S) â‰  ğ‘Œ |S, ğ¶ = 0)) = E Pr(S |ğ¶=0)</formula><p>Pr(S|ğ¶ = 1) Pr(S|ğ¶ = 0)</p><formula xml:id="formula_16">ğ‘ƒ (ğ‘“ (S) â‰  ğ‘Œ |S, ğ¶ = 0)<label>(9)</label></formula><p>Here, (6) marginalizes out features V\{S, ğ‘Œ } from the target data distribution as they do not change the error. Step <ref type="bibr" target="#b6">(7)</ref> follows from the law of iterated expectations, (8) uses the conditional independence for the separating set (Assumption 1), and ( <ref type="formula" target="#formula_16">9</ref>) uses the importanceweighting identity. We observe that the expectation in ( <ref type="formula" target="#formula_16">9</ref>) can be estimated just from the available data consisting of ğ‘Œ in the source and S in both the domains. We can re-weight the per-sample source error by the density ratio, Pr(S|ğ¶=1)/Pr(S|ğ¶=0), and take the sample average. The density ratio can be computed, for example, using a probabilistic classifier to discriminate between the domains <ref type="bibr" target="#b4">[5]</ref>.</p><p>Next, consider the fairness constraint. We show the following for EO and the corresponding Assumption 2. The argument is similar for the other definitions i.e. DP, TPR, and TNR. Assumption 2 for EO, i.e. ğ¶ âŠ¥ âŠ¥ S|ğ‘Œ, ğ´, implies that ğ¶ âŠ¥ âŠ¥ ğ‘“ (S)|ğ‘Œ , ğ´, assuming that classifier ğ‘“ is a measurable function (which is the case for most classification functions and feature spaces used in machine learning). Thus, for ğ‘¦ âˆˆ {0, 1}, we can write the EO constraints as, Therefore, Assumption 2 guarantees that the evaluation of the fairness constraint does not change with the domain. The same is true for the other three fairness constraints. For DP, the conditioning on ğ‘Œ is dropped in Assumption 2. Similarly, TPR and TNR fix ğ‘Œ = 1 and ğ‘Œ = 0 respectively.</p><p>Since both the error and the constraint are estimable, we can minimize (5) without access to labelled target domain data, if we can find a feature subset satisfying the two assumptions. â–¡ C THEOREM 5.2: WORST-CASE OPTIMALITY Now, we show that the fair classifier learned in our setting is worstcase optimal. We will restrict to one of the following fairness constraints -demographic parity (DP), true positive rate equality (i.e. equality of opportunity or TPR), or true negative rate equality (TNR). This restriction enables us to characterize the Bayes optimal classifier under fairness constraints based on results from <ref type="bibr" target="#b10">[11]</ref>.</p><p>For brevity, we change the notation to represent sensitive attribute ğ´ as one of the features in X, i.e. system variables are V := {X, ğ‘Œ }. In addition, lowercase letters denote observations of random variables which are denoted by the corresponding uppercase letters. For example, ğ‘ is used to represent the observed value of ğ´ (which can be either disadvantaged or advantaged group).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Existing result on optimal fair classifier</head><p>The optimal classifier minimizing classification error under either of the fairness constraints (DP, TPR, or TNR) is a threshold function on the conditional outcome probability. In <ref type="bibr" target="#b10">[11]</ref>, authors use the term statistical parity for DP and predictive equality for false positive rate equality (or false negative rate equality) which is the same as ensuring TNR (or TPR). Formally, they prove the following.</p><p>Lemma 2 (Theorem 3.2 by Corbett-Davies et al. <ref type="bibr" target="#b10">[11]</ref>). Suppose we want to find a classifier ğ‘“ that maximizes immediate utility, defined as ğ‘ˆ (ğ‘“ , ğ‘) = E(ğ‘¦ ğ‘“ (x) -ğ‘ ğ‘“ (x)) for a constant ğ‘ âˆˆ (0, 1), and satisfies the fairness constraint (either DP, TPR, or TNR). Let, ğ‘ ğ‘¦ |x (x) = Pr(ğ‘Œ = 1|X = x). Assume that the distribution of the random variable ğ‘ ğ‘¦ |x (X) has positive density on [0, 1]. Then, the optimal classifier ğ‘“ * under the fairness constraint is a threshold function i.e.</p><formula xml:id="formula_17">ğ‘“ * (x) = 1[ğ‘ ğ‘¦ |x (x) â‰¥ ğ‘¡ ğ‘ ] for some constant thresholds ğ‘¡ ğ‘ âˆˆ [0, 1]</formula><p>that can optionally depend on the sensitive attribute ğ´ = ğ‘.</p><p>The result also holds when the fairness constraints are required to be only approximately satisfied.</p><p>To relate this lemma to our problem, note that our objective of minimizing classification error is same as maximizing immediate utility for ğ‘ = 0.5, as observed by Lipton et al. <ref type="bibr" target="#b33">[34,</ref><ref type="bibr">Lemma 1]</ref>. To see this, rewrite the error as</p><formula xml:id="formula_18">ğ‘ƒ (ğ‘“ (x) â‰  ğ‘¦) = E(1(ğ‘“ (x) â‰  ğ‘¦)) = E(1 - ğ‘¦ ğ‘“ (x) -(1 -ğ‘¦) (1 -ğ‘“ (x)</formula><p>)) for binary labels ğ‘¦ âˆˆ {0, 1}. Rearranging terms, we get</p><formula xml:id="formula_19">ğ‘ƒ (ğ‘“ (x) â‰  ğ‘¦) = E(1 -ğ‘¦ ğ‘“ (x) -(1 -ğ‘¦)(1 -ğ‘“ (x))) = E(-2ğ‘¦ ğ‘“ (x) + ğ‘“ (x)) + E(ğ‘¦) = -2ğ‘ˆ (ğ‘“ , 0.5) + E(ğ‘¦)</formula><p>where E(ğ‘¦) is a constant. Thus, we can use Lemma 2 to characterize the optimal classifier under classification error and the three fairness definitions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 New result on optimal fair classifier under distribution shift</head><p>Now we describe the setting in which the new result is established. Let P be the set of distributions over (X, ğ‘Œ ) that satisfy Assumptions 1 and 2. That is, for all distributions in P, there exists a feature subset S s.t. ğ‘ƒ (ğ‘Œ |S) and ğ‘ƒ (S|ğ´) (for DP, as an example) are invariant. Let the source distribution be denoted by Q âˆˆ P. In addition, assume that the distributions in P are absolutely continuous with respect to the same product measure.</p><p>The proposed classifier is f * that satisfies</p><formula xml:id="formula_20">f * âˆˆ arg min ğ‘“ ğ¸ Q (1[ğ‘¦ â‰  ğ‘“ (s)]), s.t. G(ğ‘“ , Q) â‰¤ ğ,</formula><p>Denote the set of continuous functions which satisfy the fairness constraint G w.r.t. the source distribution Q by F (G, Q) := {ğ‘“ âˆˆ C 0 |G(ğ‘“ , Q) holds}. Then, we show that the set is the same for all distributions in P.</p><formula xml:id="formula_21">Lemma 3. F (G, Q) = F (G, P), âˆ€ P âˆˆ P</formula><p>Proof. Under Assumption 2, the fairness constraints are invariant. Thus, if G(ğ‘“ , Q) holds then G(ğ‘“ , P) also holds for any distribution P âˆˆ P and vice versa. â–¡ Thus, we will denote the set of fair functions by F (G, P). The proposed estimator can be written as</p><formula xml:id="formula_22">f * âˆˆ arg min ğ‘“ âˆˆ F (G,P) ğ¸ Q (1[ğ‘¦ â‰  ğ‘“ (s)])</formula><p>We will show that the proposed predictor f * is optimal over the set of fair functions w.r.t. plausible target distributions in P and fairness constraint G in an adversarial setting.</p><p>Theorem 2. Consider the set of distributions P satisfying Assumptions 1 and 2 which are absolutely continuous with respect to the same product measure, and a set of fair functions F (G, P) satisfying either DP, TPR, or TNR. Assume that the conditional outcome has strictly positive density. Then, the proposed classifier f * satisfies</p><formula xml:id="formula_23">f * âˆˆ arg min ğ‘“ âˆˆ F (G,P) sup Pâˆˆ P E P (1[ğ‘¦ â‰  ğ‘“ (x)])<label>(10)</label></formula><p>i.e. the proposed predictor achieves minimum worst-case loss amongst the fair predictors w.r.t. distributions satisfying the two assumptions.</p><p>Proof. The proof follows the arguments made in [61, Theorem 4] that proves worst-case optimality of using invariant predictors without considering fairness. For a given source distribution, we will construct an adversarial target distribution which incurs more error for a fair predictor than that of the proposed predictor on the source distribution. This will prove the minmax property from <ref type="bibr" target="#b9">(10)</ref>.</p><p>Suppose the density of the source distribution Q is ğ‘(x, ğ‘¦). Construct a distribution P with density ğ‘ (x, ğ‘¦) = ğ‘(s, ğ‘¦)ğ‘(z) where Z := {X \ S}. From the construction of the density, it follows that P satisfies Assumptions 1 and 2. Thus, P âˆˆ P. To see this, observe that ğ‘ (ğ‘¦|s) = ğ‘(ğ‘¦|s) and ğ‘ (s|ğ‘) = ğ‘(s|ğ‘) (for DP, as an example), and we know that ğ‘(ğ‘¦|s), ğ‘(s|ğ‘) are invariant since Q âˆˆ P. Also, by construction, Z âŠ¥ âŠ¥ (S, ğ‘Œ ) in P.</p><p>Consider a function ğ‘“ âˆˆ F (G, P). Then,</p><formula xml:id="formula_24">E P (1[ğ‘¦ â‰  ğ‘“ (x)]) â‰¥ min ğ‘“ âˆˆ F (G,P) E P (1[ğ‘¦ â‰  ğ‘“ (x)])<label>(11)</label></formula><p>Using Lemma 2, the error minimizer in F (G, P) w.r.t. P is a threshold rule, i.e. f * (x</p><formula xml:id="formula_25">) = 1[ğ‘ ğ‘¦ |x (x) â‰¥ ğ‘¡ ğ‘ ] for a constant ğ‘¡ ğ‘ âˆˆ [0, 1].</formula><p>Thus, we can write min ğ‘“ âˆˆ F (G,P)</p><formula xml:id="formula_26">E P (1[ğ‘¦ â‰  ğ‘“ (x)]) = E P (1[ğ‘¦ â‰  f * (x)])<label>(12)</label></formula><formula xml:id="formula_27">= E P (1[ğ‘¦ â‰  f * (s, z)]) = E P (1[ğ‘¦ â‰  f * (s)])<label>(13)</label></formula><p>where <ref type="bibr" target="#b12">(13)</ref> follows from the construction of Z which satisfies Z âŠ¥ âŠ¥ (S, ğ‘Œ ). Since ğ‘ ğ‘¦ |s,z = ğ‘ ğ‘¦ |s , this implies f * (s, z) = f * (s). Plugging back in <ref type="bibr" target="#b10">(11)</ref>, we get</p><formula xml:id="formula_28">E P (1[ğ‘¦ â‰  ğ‘“ (x)]) â‰¥ E P (1[ğ‘¦ â‰  f * (s)])</formula><p>By construction, ğ‘ (x, ğ‘¦)=ğ‘(s, ğ‘¦)ğ‘(z). Thus, E P (ğ‘”(s, ğ‘¦))=E Q (ğ‘”(s, ğ‘¦)) for any function ğ‘”. With this,</p><formula xml:id="formula_29">E P (1[ğ‘¦ â‰  ğ‘“ (x)]) â‰¥ E Q (1[ğ‘¦ â‰  f * (s)]) = arg min ğ‘“ âˆˆ F (G,P) ğ¸ Q (1[ğ‘¦ â‰  ğ‘“ (s)])</formula><p>Thus, for the proposed predictor trained on the source distribution with fairness constraints, there always exists a distribution P âˆˆ P with larger error for a fair predictor ğ‘“ âˆˆ F (G, P). In other words, the proposed predictor is worst-case optimal against distributions satisfying the two assumptions. â–¡</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D JOINT CAUSAL INFERENCE (JCI) FRAMEWORK</head><p>JCI framework <ref type="bibr" target="#b42">[43]</ref> has been proposed, primarily, for causal discovery from multiple datasets, and for causal modelling of a system observed in different domains (or contexts). Following <ref type="bibr" target="#b37">[38]</ref>, we focus on the latter and use the framework for domain adaptation.</p><p>A joint causal model is used to model the data generated under different domains by introducing context variables to the standard structural causal model, defined as follows.</p><p>Definition D.1 (Joint Causal Model). A joint causal model M is a tuple âŸ¨I, J, K, H, F , ğ‘ƒ (K)âŸ© where (1) {ğ¶ ğ‘– } ğ‘– âˆˆI denote a set of context variables, assumed to be exogenous to the system of interest. (2) {ğ‘‹ ğ‘— } ğ‘— âˆˆ J denote a set of system variables, assumed to be endogenous. (3) {ğ‘ˆ ğ‘˜ } ğ‘˜ âˆˆK denote a set of independent exogenous variables, assumed to be unobserved. (4) {â„ ğ‘– } ğ‘– âˆˆH denote a set of functions that give functional dependence of context variables. Each variable ğ¶ ğ‘– is assigned its value as ğ¶ ğ‘– â† â„ ğ‘– (U pa(ğ¶ ğ‘– )âˆ©K ). (5) {ğ‘“ ğ‘— } ğ‘— âˆˆ J denote a set of functions that give functional dependence of system variables. Each variable ğ‘‹ ğ‘— is assigned its value as ğ‘‹ ğ‘— â† ğ‘“ ğ‘— (C pa(ğ‘‹ ğ‘— )âˆ©I , X pa(ğ‘‹ ğ‘— )âˆ© J , U pa(ğ‘‹ ğ‘— )âˆ©K ). (6) ğ‘ƒ (K) denotes a probability distribution over unobserved exogenous variables, ğ‘ƒ (U) = Î  ğ‘˜ âˆˆK ğ‘ƒ (ğ‘ˆ ğ‘˜ ).</p><p>Here, pa(â€¢) is a set of variables, referred to as parents of the variable.</p><p>A joint causal graph G(M) refers to the causal graph representing M. The graph only contains nodes from I âˆª J , a directed edge ğ‘ â†’ ğ‘ for nodes ğ‘, ğ‘ iff ğ‘ âˆˆ pa(ğ‘), and a bidirected edge ğ‘ â†” ğ‘ iff the set pa(ğ‘) âˆ© pa(ğ‘) âˆ© K is non-empty. That is, directed edges represent direct functional dependence and bidirected edges represent presence of unobserved common causes.</p><p>The functional dependencies along with distribution over the unobserved variables induces a distribution over context and system variables. Importantly, intervening on the context variables give distributions for different domains. <ref type="foot" target="#foot_3">4</ref> Thus, JCI provides a framework to reason about the distributions of data from multiple domains. To read the conditional independencies in the domain distributions using the graph, we need two assumptions. Causal Markov assumption requires that any d-separation of sets of nodes in G(M) imply the corresponding conditional independencies in the distribution. Conversely, faithfulness means that there are no other conditional independencies than the ones implied by d-separation. Please refer to <ref type="bibr" target="#b37">[38,</ref><ref type="bibr">Section 2.2]</ref> for details.</p><p>Thus, given any G(M) proposed using domain knowledge or discovered from data, we can check the conditional independence assumptions posited by Assumptions 1 and 2.</p><p>Remark 1. In case of TPR and TNR, which require ğ¶ âŠ¥ âŠ¥ ğ‘† |ğ‘Œ = 1, ğ´ and ğ¶ âŠ¥ âŠ¥ ğ‘† |ğ‘Œ = 0, ğ´, we need to consider causal graphs drawn for sub-populations (ğ‘Œ = 1 or ğ‘Œ = 0). The conditional independence can then be checked in such a graph using d-separation. This requires stronger assumptions that the modeler can express causal knowledge at the sub-population level and the distribution is faithful to such a graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E RESULTS AND DISCUSSION ON COUNTERFACTUAL FAIRNESS</head><p>Another set of fairness definitions have been proposed based on the causal effect of the protected attribute on the prediction <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b43">44]</ref>. We consider one strict version of these counterfactual fairness definitions.</p><p>Definition E.1. (Ctf) <ref type="bibr" target="#b31">[32]</ref> A classifier Å¶ = ğ‘“ (X, ğ´) is said to be counterfactually fair if the interventional distribution of Å¶ conditioned on all observed values is the same under ğ‘‘ğ‘œ (ğ´ = ğ‘) and ğ‘‘ğ‘œ (ğ´ = ğ‘‘), i.e. ğ‘ƒ ( Å¶ğ‘‘ğ‘œ (ğ´=ğ‘) = ğ‘¦|X = x, ğ´ = ğ‘–) = ğ‘ƒ ( Å¶ğ‘‘ğ‘œ (ğ´=ğ‘‘) = ğ‘¦|X = x, ğ´ = ğ‘–), for ğ‘¦ âˆˆ {0, 1} and ğ‘– âˆˆ {ğ‘, ğ‘‘ }.</p><p>One method to build a classifier ğ‘“ (S) satisfying Ctf is to only use feature set S âˆˆ {X, ğ´} that does not contain any descendant of ğ´ in the causal graph [32, <ref type="bibr">Lemma 1]</ref>.</p><p>Thus, the counterpart of Assumption 2 for solving Fair DA under Ctf is that the selected feature set contains the non-descendants of ğ´. Combined with Assumption 1, we select non-descendants of ğ´ which form a separating set in order to solve Fair DA.</p><p>Since, Ctf only requires change in feature subset and does not include any fairness constraints in the fair learning problem, we can show the worst-case optimality result as well. The arguments in the proof of Theorem 2 still hold. The only difference is that instead of using Lemma 2 at Step <ref type="bibr" target="#b11">(12)</ref>, we use the fact that Bayes classifier (without fairness constraints) is also a threshold function that depends on conditional outcome distribution.</p><p>However, we note that there are multiple ways of defining counterfactual fairness. For instance, <ref type="bibr" target="#b43">[44]</ref> require that causal effects of ğ´ on Å¶ through particular paths should be zero or small. Further work should explore approaches to solve Fair DA under broader definitions of counterfactual fairness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F EXAMPLES OF ADDRESSABLE SHIFTS</head><p>As remarked in Section 5.3, we can find at least one feature set satisfying Assumptions 1 and 2 in case of the following shiftsshifts with causal paths to ğ‘Œ which all include ğ´ (i.e. ğ¶â€¢â€¢ â†’ğ´â†’ â€¢â€¢ğ‘Œ with all arrows toward ğ‘Œ ) and shifts with non-causal paths to ğ‘Œ (i.e. ğ¶â€¢â€¢ â†’ğ‘€â† â€¢â€¢ğ‘Œ for some feature ğ‘€ âˆˆ S). This means that any shift causing change in distribution of the sensitive attribute as well as any measurement error in variables with a non-causal path to ğ‘Œ can be addressed. We present two examples of real-world tasks with such shifts. Medical diagnosis. Figure <ref type="figure" target="#fig_23">7a</ref> represents an example of sepsis prediction based on the causal graph presented in <ref type="bibr" target="#b65">[66]</ref>; ğ‘Œ : outcome (prevalence of sepsis condition), ğ´: age (sensitive attribute), ğ‘… represents a pre-existing condition like chronic liver condition. An indicator like the international normalized ratio (INR) which is a measure of blood clotting tendency is denoted by ğ‘‹ . ğ‘‡ represents treatment prescribed to the patient. The graph represents a case of population shift represented by ğ¶ 1 and treatment prescription policy shift denoted by ğ¶ 2 .</p><p>Credit scoring. Figure <ref type="figure" target="#fig_23">7b</ref> represents a causal graph posited in <ref type="bibr" target="#b9">[10]</ref> for the credit scoring task. The task is to predict ğ‘Œ : credit risk i.e. whether an applicant will repay the loan or not from features ğ‘…: credit amount and repayment duration, ğ´: sex (sensitive attribute), ğ¿: age, and ğ‘†: savings and housing status. The context variable ğ¶ denotes an anticipated shift which changes the distribution of females, for instance, in the target domain.</p><p>Recidivism risk assessment. Another example of risk assessment tools used to inform pre-trial bail decisions in the criminal justice system is presented in Figure <ref type="figure" target="#fig_23">7c</ref>. In the causal graph for this task, presented in <ref type="bibr" target="#b75">[76]</ref>, ğ´ denotes the sensitive attribute, here race, ğ‘ represents other demographic information (e.g., age, gender) of the defendant which can be confounded by race. Prior convictions are denoted by ğ‘Š and recidivism outcome (0 for no, 1 otherwise) is represented by ğ‘Œ . ğ¶ represents population shift where the distribution of the defendant's race changes across jurisdictions i.e. target domains.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ğ´</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G EXPERIMENT SETUP</head><p>Data generating process for synthetic example. We generate data using the following structural equation model.</p><formula xml:id="formula_30">ğ´ âˆ¼ Bernoulli (ğœ (ğ›¾ â€¢ ğœ† 1 â€¢ ğ¶ + ğ‘¢ 1 )) ğ‘… âˆ¼ N (0, 1) + ğœ† 2 â€¢ ğ´ + ğ‘¢ 2 ğ‘Œ âˆ¼ Bernoulli (ğœ (ğœ† 3 â€¢ ğ´ + ğœ† 4 â€¢ ğ‘… + ğ‘¢ 3 )) ğ‘‡ = ğœ† 5 â€¢ğ‘Œ + ğœ† 6 â€¢ ğ‘… + ğœ† 7 â€¢ ğ´ + N (0, ğ›¾ â€¢ ğœ† 8 â€¢ ğ¶) + ğ‘¢ 4 ğ‘¢ 1 , ğ‘¢ 2 , ğ‘¢ 3 âˆ¼ N (0, 0.8 2 ), ğ‘¢ 4 âˆ¼ N (0, 1.0) ğœ† 1 = 0.2, ğœ† 2 = -0.1, ğœ† 3 = -0.8, ğœ† 4 = 0.8 ğœ† 5 = 0.8, ğœ† 6 = 0.1, ğœ† 7 = -0.8, ğœ† 8 = 0.2 ğ›¾ âˆˆ [0, 15]</formula><p>Variables ğ´ and ğ‘Œ are binary, taking values in {0, 1} and are sampled from a Bernoulli distribution with respective means as per equations, where ğœ (ğ‘¥) = 1 1+exp(-ğ‘¥) . For source domain, ğ¶ = 0 and for target domain, ğ¶ = 1. Setting ğ¶ = 1 amounts to performing a soft intervention <ref type="bibr" target="#b39">[40]</ref> on ğ´ and ğ‘‡ , whose distributions change as a result in the target domain. The magnitude of the effect of ğ¶ on ğ´ and ğ‘‡ is scaled by a constant ğ›¾, which is varied from 0 to 15 for simulating distribution shifts of increasing magnitude.</p><p>More details on OTDA baseline. OTDA is an optimal transportbased method for unsupervised domain adaptation <ref type="bibr" target="#b13">[14]</ref>. Instead of making assumptions on covariate shift, it posits that some (nonlinear) transformation exists between source and target features. It finds the minimal-cost transformations for a given cost function. A classifier is trained on the transformed source data and applied directly to the target data. We use the squared ğ‘™ 2 cost and ğ‘™ ğ‘ -ğ‘™ 1 regularization-based transport method <ref type="bibr" target="#b13">[14]</ref>. 5    We compare with another causal domain adaptation method called anchor regression <ref type="bibr" target="#b61">[62]</ref> in a separate experiment setting where we assume access to data from multiple source domains. This method has been shown to be competitive among state-ofthe-art methods for multi-source domain adaptation which do not require target domain data <ref type="bibr" target="#b67">[68]</ref>.</p><p>Anchor regression requires specifying an anchor variable which indicates (in case of discrete anchors) homogeneous subsets of data. An example is the context variable ğ¶ which separates the two homogeneous datasets -source and target -based on their magnitude of shift. Anchors are exogenous sources of variation which can be exploited to regularize the predictor. Intuitively, the method avoids overfitting the predictor to the source dataset by decorrelating the residuals from the anchor. In addition, the method assumes that the anchor causes mean-shifts in a subset of variables, i.e. the linear structural equations for the variables have an added intercept term that is constant for a given homogeneous subset. The data generating process described in Section 6.1 does not have 5 Implemented in POT library <ref type="url" target="https://github.com/rflamary/POT">https://github.com/rflamary/POT</ref> an anchor that follows this assumption. The shift in feature ğ‘‡ is caused by a shift in the variance of ğ‘‡ , instead of being a constant mean-shift. Thus, we change the data generating process for the experiments reported here by changing the equation for ğ‘‡ (marked in red below).</p><formula xml:id="formula_31">ğ´ âˆ¼ Bernoulli (ğœ (ğ›¾ â€¢ ğœ† 1 â€¢ ğ¶ + ğ‘¢ 1 )) ğ‘… âˆ¼ N (0, 1) + ğœ† 2 â€¢ ğ´ + ğ‘¢ 2 ğ‘Œ âˆ¼ Bernoulli (ğœ (ğœ† 3 â€¢ ğ´ + ğœ† 4 â€¢ ğ‘… + ğ‘¢ 3 )) ğ‘‡ = ğœ† 5 â€¢ğ‘Œ + ğœ† 6 â€¢ ğ‘… + ğœ† 7 â€¢ ğ´ + ğ›¾ â€¢ ğœ† 8 â€¢ ğ¶ + ğ‘¢ 4 ğ‘¢ 1 , ğ‘¢ 2 , ğ‘¢ 3 âˆ¼ N (0, 0.8 2 ), ğ‘¢ 4 âˆ¼ N (0, 1.0) ğœ† 1 = 0.2, ğœ† 2 = -0.1, ğœ† 3 = -0.8, ğœ† 4 = 0.8 ğœ† 5 = 0.8, ğœ† 6 = 0.1, ğœ† 7 = -0.8, ğœ† 8 = 0.2 ğ›¾ âˆˆ [0, 9]</formula><p>Setup We generate multiple datasets by varying the magnitude of shift, specifically ğ›¾ = {0, 2, 4}, and combine them into a single source dataset. In case of anchor regression, dummy variables are added to encode the samples corresponding to the three different shift magnitudes. The dummy variables are the anchors. For rest of the methods, we simply concatenate the datasets without adding any dummy variables. All variables in source and features in target are mean centered using the corresponding datasets. We use square loss as the loss function for the classification problem with an additional regularization term which is minimized following a two-stage least squares procedure <ref type="bibr" target="#b61">[62]</ref>. We evaluate the resulting predictor on five target datasets ğ›¾ = {1, 3, 5, 7, 9}.</p><p>Results. Figure <ref type="figure" target="#fig_25">8</ref> shows fairness violation and accuracy for anchor regression along with other methods. We observe that anchor regression (labelled as AnchorReg) is robust to distribution shift as it has high accuracy even in case of large shifts (green curve in Figure <ref type="figure" target="#fig_25">8a</ref>). But it has higher unfairness than the proposed approach CausalDA+FairLearn, which is the case for other domain adaptation methods CausalDA and OTDA as well (Figure <ref type="figure" target="#fig_25">8b</ref>). Thus, this experiment also demonstrates the importance of constraining the fairness violations of the predictors while performing domain adaptation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I ACUTE KIDNEY INJURY: DATA DESCRIPTION, PRE-PROCESSING, AND ADDITIONAL RESULTS I.1 Dataset</head><p>The dataset is constructed from a large de-identified electronic healthcare record database, called MIMIC-III, for adult inpatients (Age &gt; 18 years) at a critical care unit of a tertiary care hospital, ranging from years 2001 to 2012 <ref type="bibr" target="#b27">[28]</ref>. The total number of encounters (admissions) in the dataset is 58, 976, with some patients having multiple encounters. In order to have a complete view of a patient's hospital stay from admission until discharge, we removed any patient who died during their hospital stay regardless of the cause. We have also excluded patients that were admitted with evidence of moderate or severe kidney dysfunction, as done by He et al. <ref type="bibr" target="#b24">[25]</ref>. Patients with estimated glomerular filtration rate (eGFR) less than 60 mL/min/1.73 ğ‘š 2 (calculated using the revised MDRD study equation <ref type="bibr" target="#b32">[33]</ref>) or Serum Creatinine (SCr) level more than 1.3 mg/dL within the first 24 hours of hospital admission are excluded. Patients admitted with null eGFR or null SCr (i.e., no SCr reading) within first 24 hours were also removed. The final analysis consisted of 24, 852 encounters.</p><p>According to Kidney Disease Improving Global Outcomes (KDIGO) <ref type="bibr" target="#b29">[30]</ref>, AKI was defined as either of the following two criteria being met: 1) greater than or equal to 50% increase from the baseline SCr value or 2) greater than or equal to 0.3 mg/dL change in SCr from the baseline creatinine. Baseline SCr level was defined as the first SCr measured after hospital admission. Out of a total of 24, 852 encounters in the final analysis, AKI events occurred in 5, 137 encounters and 19, 715 encounters had no AKI events. We consider a prediction window which ends 1 day before the onset of AKI. A summary of features used to train the prediction models can be found in Table <ref type="table">3</ref>, included at the end. Some of these are also used by <ref type="bibr" target="#b24">[25]</ref>. However, unlike this study, we exclude medications and medical history features as they are high-dimensional (around 1,000). Only patients who did not have AKI at the time of admission are included since we are interested in diagnosing AKI for in-hospital patients. Data for variables mentioned in Table <ref type="table">3</ref> is extracted for each encounter from time before the prediction window. Since the features are measured at multiple timepoints, we consider the last observed value of a feature until the end of the prediction window.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.2 Data pre-processing</head><p>In-hospital encounters which met the two KDIGO criteria were labeled as the positive class, while encounters in which the patient did not meet the above criteria was labeled as the negative class. Feature vectors were created for each encounter. Demographic information (age, gender, and race) were included in the feature vectors; age in years, and dummy variables for gender categories (female, male) and race categories (American Indian or Alaska Native, Asian, Black or African American, Native Hawaiian or Other Pacific Islander, White, multiple race, refuse to answer, no information, unknown, and other). A patient's vitals (BMI, diastolic BP, systolic BP, height, weight) were included as numerical features, where the most recent value associated with any vital was used in cases where multiple measurements were taken during an encounter. Any missing values were imputed using the mean value (calculated on the training data) for the given feature. Lab tests and comorbidities were included in the form of boolean features indicating whether the lab test/comorbidity was present. While only lab tests performed during a given in-hospital encounter were used, comorbidities up to one year prior to the hospital stay were included. Comorbidities included in the AKI predictive model by <ref type="bibr" target="#b69">[70]</ref> were used as a guide for selecting the nine comorbidities in Table <ref type="table">3</ref>. After adding dummy variables and missing value indicators, we obtained a dataset containing 24, 852 examples, each with 51 features (13 demographic, 5 vitals, 9 comorbidities, 12 lab tests, and 12 lab test missing value indicators). We removed features that had more than 90% values as missing, namely the lab tests for Troponin, Albumin, and WBC. Since the BUN lab test was one of the most predictive features, it was chosen for creating shifted datasets. We discarded both the BUN variable and missing value indicator when creating the invariant predictor.</p><p>Remark 2. We include race categories, constructed from race and ethnicity codes in the raw data, as a feature in the model following past work on AKI risk assessment <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b76">77]</ref>. However, we emphasize that race is an ill-defined and ill-measured feature and including it might cause discrimination in allocating healthcare resources based on the model predictions <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b71">72]</ref>. Thus, more careful examination of the decision to include race in the models is required.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.3 Different proportions of missingness</head><p>To evaluate robustness of models, distributional shifts are simulated in target data following the setup outlined in Section 6.2. Firstly, we add missing values in the BUN feature values for a proportion of the rows. Secondly, we downsample the female population by randomly keeping only 50% of the rows from the female group in the source data. Figure <ref type="figure">9</ref> shows two predictive performance metrics, namely AUPRC, AUROC, and one fairness metric, namely maximum fairness violation, for different proportions of missingness, ranging from 0.1 to 0.5. As mentioned in Section 6.2, the methods which use the invariant features (CausalDA and CausalDA+FairLearn) have stable accuracy as the distribution shifts. Moreover, CausalDA+FairLearn reduces unfairness consistently in the shifted data. Table <ref type="table" target="#tab_2">1</ref> reports the numerical values for evaluation metrics with missingness proportion set to 0.5. These are reported visually as well in Figure <ref type="figure">9</ref> here and Figure <ref type="figure">4c</ref> in the main text. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I.4 Implementation details</head><p>Hyperparameters used in solving the Fair DA problem are listed in Table <ref type="table" target="#tab_3">2</ref>. Rest of the hyperparameters are kept as defaults chosen in the packages cited. Classifiers (logistic regression and gradient boosting trees) are implemented using the package scikit-learn (v0.22.1) <ref type="bibr" target="#b51">[52]</ref> in Python. All experiments were ran on a single node of a compute cluster with a 3.0 GHz Intel processor and 8 GB memory.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Definition 3 . 1 .</head><label>31</label><figDesc>(DP) [7] A classifier ğ‘“ is said to satisfy demographic parity for some distribution ğ‘ƒ if ğ‘ƒ (ğ‘“ (S)|ğ´) = ğ‘ƒ (ğ‘“ (S)). Thus, the constraint ğº is |ğ‘ƒ (ğ‘“ (S)|ğ´ = a) -ğ‘ƒ (ğ‘“ (S)|ğ´ = d)| â‰¤ ğœ–. Definition 3.2. (EO) [23] A classifier ğ‘“ is said to satisfy equalized odds for some distribution ğ‘ƒ if ğ‘ƒ (ğ‘“ (S)|ğ‘Œ = ğ‘¦, ğ´) = ğ‘ƒ (ğ‘“ (S)|ğ‘Œ = ğ‘¦) for ğ‘¦ âˆˆ {0, 1}. Thus, the constraints G are |ğ‘ƒ (ğ‘“ (S)|ğ‘Œ = ğ‘¦, ğ´ = a) -ğ‘ƒ (ğ‘“ (S)|ğ‘Œ = ğ‘¦, ğ´ = d)| â‰¤ ğœ– for ğ‘¦ âˆˆ {0, 1}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Example causal graph annotated to show anticipated shifts in the distributions ğ‘ƒ (ğ´) and/or ğ‘ƒ (ğ‘‡ |ğ´, ğ‘Œ ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Fairness Violation vs. Shift.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Flu diagnosis example. (a) Data generating process for source and target domains represented as a causal graph where domains are indicated by the context variable ğ¶. Edges from ğ¶ represent shifts between the domains. {ğ‘‡ , ğ‘…, ğ´} are features, with sensitive attribute ğ´, and outcome ğ‘Œ . (b,c) Classification accuracy and fairness violation with varying magnitude of shifts for synthetic data (Section 6.2) for the example. Fairness violation is computed as the maximum violation of equalized odds constraint across ğ‘Œ and ğ´. Median values are plotted over 50 runs and error bars show first and third quartiles. Proposed approach (CausalDA+FairLearn) achieves both stable accuracy and fairness in the shifted target domains.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Assumption 1 (</head><label>1</label><figDesc>Invariance of classification error). Features S form a separating set, i.e. ğ¶ âŠ¥ âŠ¥ ğ‘Œ | S. Assumption 2 (Invariance of fairness constraint). Depending on the fairness metric, assume that â€¢ For demographic parity (DP), S satisfies ğ¶ âŠ¥ âŠ¥ S | ğ´, â€¢ For equalized odds (EO), S satisfies ğ¶ âŠ¥ âŠ¥ S | ğ‘Œ, ğ´, â€¢ For true positive rate equality (TPR), S satisfies ğ¶ âŠ¥ âŠ¥ S | ğ‘Œ = 1, ğ´, â€¢ For true negative rate equality (TNR), S satisfies ğ¶ âŠ¥ âŠ¥ S | ğ‘Œ = 0, ğ´.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Examples of addressable causal graphs. (a) Disease risk scoring under population shift and treatment policy shift [66] (b) Credit scoring under population shift<ref type="bibr" target="#b9">[10]</ref>. Following Assumptions 1 and 2, including ğ´ in the feature set blocks the effect of population shift (e.g. the paths in magenta) and excluding ğ¿ from the feature set blocks the effect of treatment policy shift (e.g. the path in green).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Definition 5 . 1 .</head><label>51</label><figDesc>(Ctf)  <ref type="bibr" target="#b31">[32]</ref> A classifier Å¶ = ğ‘“ (X, ğ´) is said to be counterfactually fair if the counterfactual distribution of Å¶ conditioned on all observed values is the same under ğ‘‘ğ‘œ (ğ´ = a) and ğ‘‘ğ‘œ (ğ´ = d), i.e. ğ‘ƒ ( Å¶ğ‘‘ğ‘œ (ğ´=a) = ğ‘¦|X = x, ğ´ = ğ‘–) = ğ‘ƒ ( Å¶ğ‘‘ğ‘œ (ğ´=d) = ğ‘¦|X = x, ğ´ = ğ‘–), for ğ‘¦ âˆˆ {0, 1} and ğ‘– âˆˆ {a, d}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>High shift magnitude, ğ›¾ = 15. Low shift magnitude, ğ›¾ = 1.67.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: (a) Data for the domains with shift governed by ğ›¾, highlighted in red. (b,c) Accuracy and fairness metrics on synthetic data example with different magnitude of shifts. Median values are reported over 50 runs and error bars show first and third quartiles. Proposed approach CausalDA+FairLearn is both accurate and fair under large shifts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 5 (</head><label>5</label><figDesc>d,e,f) report results on the synthetic example with demographic parity (DP) as the fairness constraint instead of EO. In case of DP, the fairness violation is quantified as |ğ‘ƒ (ğ‘“ (S) | ğ´ = ğ‘) -ğ‘ƒ (ğ‘“ (S) | ğ´ = ğ‘‘) . We observe similar trends as compared to the plots for EO.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>21 Figure 4 :</head><label>214</label><figDesc>Figure 4: (a) Postulated causal graph for AKI. Bi-directed edge denotes unmeasured confounding between disease outcome and lab test values due to unobserved common causes. (b) Legend for variables in the graph. (c) Accuracy and fairness metrics for AKI data. Median values are reported over 50 runs and error bars show first and third quartiles. Proposed approach improves fairness with small loss in accuracy, even on shifted target data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>Fairness Violation vs Shift.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head></head><label></label><figDesc>Fairness Violation vs Shift.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Accuracy, AUROC, and Fairness violation with varying magnitude of shifts for synthetic data. (a,b,c) With equalized odds (EO) as the fairness constraint. (d,e,f) With demographic parity (DP) as the fairness constraint. Median values are reported over 50 runs and error bars show first and third quartiles. Performance of the proposed approach is stable across different shifts and for the two fairness metrics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Example with group-specific measurement error.The group-specific errors, governed by ğ‘ƒ (ğ‘‹ | ğ´, ğ¶) in case of DP, can change arbitrarily between the domains depending on the mechanism for ğ¶ â†’ ğ‘‹ . Thus, we cannot constrain the difference in group-specific errors to satisfy DP in target. For this example, the proposed method fails to find a feature set satisfying Assumptions 1 and 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head></head><label></label><figDesc>Suppose, we want to build a predictor, Å¶ = ğ‘“ (ğ‘‹, ğ´), satisfying demographic parity (DP) as the fairness definition. DP requires ğ‘“ (ğ‘‹, ğ´) âŠ¥ âŠ¥ ğ´. Thus, the fairness constraint is ğº ğ¶=1 (ğ‘“ ) = |E(ğ‘“ (ğ‘‹, ğ´) | ğ´ = 1, ğ¶ = 1) -E(ğ‘“ (ğ‘‹, ğ´) | ğ´ = 0, ğ¶ = 1)|. Define the mean squared error in target domain as ğ¿ ğ¶=1 (ğ‘“ ) = E ğ‘ƒ (ğ‘‹ ,ğ´,ğ‘Œ |ğ¶=1) ((ğ‘“ (ğ‘‹, ğ´) -ğ‘Œ ) 2 ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head></head><label></label><figDesc>ğº (ğ‘“ , ğ‘ƒ (ğ‘Œ, S|ğ¶ = 1)) = |ğ‘ƒ (ğ‘“ (S) â‰  ğ‘¦|ğ‘Œ = ğ‘¦, ğ´ = ğ‘, ğ¶ = 1) -ğ‘ƒ (ğ‘“ (S) â‰  ğ‘¦|ğ‘Œ = ğ‘¦, ğ´ = ğ‘‘, ğ¶ = 1)| = |ğ‘ƒ (ğ‘“ (S) â‰  ğ‘¦|ğ‘Œ = ğ‘¦, ğ´ = ğ‘, ğ¶ = 0) -ğ‘ƒ (ğ‘“ (S) â‰  ğ‘¦|ğ‘Œ = ğ‘¦, ğ´ = ğ‘‘, ğ¶ = 0)| = ğº (ğ‘“ , ğ‘ƒ (ğ‘Œ, S|ğ¶ = 0))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head></head><label></label><figDesc>Recidivism risk assessment under population shift</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>Figure 7 :</head><label>7</label><figDesc>Figure7: Examples of addressable causal graphs. Following Assumptions 1 and 2, including ğ´ in the feature set blocks the effect of population shift (e.g. the paths in magenta) and excluding ğ¿ from the feature set blocks the effect of treatment policy shift (e.g. the path in green).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Accuracy and Fairness violation with varying magnitude of shifts in the new setting for synthetic data. Median values are reported over 50 runs and error bars show first and third quartiles.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Algorithm 1 Fair domain adaptation via reduction to standard fair learning Input Joint causal graph G, source data D source , fairness metric Output Classifier ğ‘“ * target (S) or No_solution Initialize ğ‘… val â† {}. for S âŠ† {X, ğ´} do Solve min ğ‘“ âˆˆ F (S) ğ‘ƒ source (ğ‘“ (S) â‰  ğ‘Œ ) and compute error ğ‘… val(S) on validation set ğ‘… val â† {ğ‘… val , ğ‘… val(S) } end for Sort ğ‘… val in increasing order Traverse ğ‘… val and select S satisfying Assumptions 1 and 2, say S * , by checking for d-separation in graph G if S * exists then Solve Fair DA problem (2) with features S * and return output else return No_solution end if continuous with respect to the same product measure, and a set of fair functions F (G, P) satisfying either DP, TPR, or TNR. Assume that the conditional outcome has strictly positive density. Then, the proposed classifier f * satisfies</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Note that we use target features as an input during training for only this baseline.</figDesc><table><row><cell cols="4">H SYNTHETIC DATA EXAMPLE:</cell></row><row><cell></cell><cell cols="3">COMPARISON WITH ANCHOR</cell></row><row><cell></cell><cell cols="2">REGRESSION</cell><cell></cell></row><row><cell></cell><cell>0.90</cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.85</cell><cell></cell><cell></cell></row><row><cell>Accuracy on Target Data</cell><cell>0.50 0.55 0.60 0.65 0.70 0.75 0.80</cell><cell>2 Standard 4 Magnitude of Shift 6 Causal DA OT DA Anchor Reg Standard w. FairLearn Causal DA w. FairLearn</cell><cell>8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Fairness Violation, AUPRC and AUROC (median across 50 runs) for AKI task, with missigness proportion set to 0.5</figDesc><table><row><cell>Model</cell><cell cols="3">Fairness Viol. AUPRC AUROC</cell></row><row><cell>Standard</cell><cell>0.0404</cell><cell>0.626</cell><cell>0.7906</cell></row><row><cell>CausalDA</cell><cell>0.037</cell><cell>0.6507</cell><cell>0.8104</cell></row><row><cell>Standard+FairLearn</cell><cell>0.0193</cell><cell>0.6197</cell><cell>0.7863</cell></row><row><cell>CausalDA+FairLearn</cell><cell>0.0193</cell><cell>0.6454</cell><cell>0.8075</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Hyperparameter settings used in synthetic and AKI experiments</figDesc><table><row><cell>Method</cell><cell>Hyperparameter</cell><cell>Value</cell></row><row><cell>FairLearn -</cell><cell cols="2">Allowed fairness constraint violation (eps) 10 -2</cell></row><row><cell>Exponentiated</cell><cell>Maximum number of iterations (T)</cell><cell>50</cell></row><row><cell>Gradient</cell><cell>Convergence threshold (nu)</cell><cell>10 -6</cell></row><row><cell>OTDA</cell><cell>Entropic regularization (reg_e) Class regularization (reg_cl)</cell><cell>10 10 -2</cell></row><row><cell>AnchorReg</cell><cell>Regularization coefficient (gamma)</cell><cell>1.5</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Note that S can contain ğ´ as we assume that disparate treatment is allowed in the problems of our interest.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>In a related concept, selection diagrams also add auxiliary variables to a causal graph to represent the distributions that can change across different domains<ref type="bibr" target="#b50">[51]</ref>. More discussion on the relationship between the two can be found in<ref type="bibr" target="#b42">[43]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>Under the assumptions of JCI framework, discussed in Appendix D, this is the same as ğ‘ƒ (V ğ‘‘ğ‘œ (C=c) ) where ğ‘‘ğ‘œ (C = c) denotes an intervention on ğ¶.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>Since the context variables C are assumed to be exogenous, intervening on them, ğ‘‘ğ‘œ (C), is same as conditioning on their value, C = c. This follows by Rule 2 of do-calculus.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_4"><p>FAccT '21, March 3-10, 2021, Virtual Event, Canada Singh, et al.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>We acknowledge funding from the <rs type="funder">NSF</rs> grant number <rs type="grantNumber">1845487</rs>. HS would like to thank <rs type="person">Sreyas Mohan</rs>, <rs type="person">Kunal Relia</rs>, <rs type="person">Margarita Boyarskaya</rs> and <rs type="person">Nabeel Abdur Rehman</rs> for helpful discussions.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_jx5ucv4">
					<idno type="grant-number">1845487</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A Reductions Approach to Fair Classification</title>
		<author>
			<persName><forename type="first">Alekh</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alina</forename><surname>Beygelzimer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miroslav</forename><surname>Dudik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanna</forename><surname>Wallach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="60" to="69" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">LÃ©on</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.02893</idno>
		<title level="m">Invariant risk minimization</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A theory of learning from different domains</title>
		<author>
			<persName><forename type="first">Shai</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koby</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><forename type="middle">Wortman</forename><surname>Vaughan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page" from="151" to="175" />
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Impossibility theorems for domain adaptation</title>
		<author>
			<persName><forename type="first">Shai</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tyler</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teresa</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">DÃ¡vid</forename><surname>PÃ¡l</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="129" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Discriminative learning for differing training and test distributions</title>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Bickel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>BrÃ¼ckner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Scheffer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th international conference on Machine learning</title>
		<meeting>the 24th international conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="81" to="88" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Recovering from Biased Data: Can Fairness Constraints Improve Accuracy?</title>
		<author>
			<persName><forename type="first">Avrim</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Stangl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">1st Symposium on Foundations of Responsible Computing</title>
		<imprint>
			<publisher>FORC</publisher>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note>Schloss Dagstuhl-Leibniz-Zentrum fÃ¼r Informatik</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Building classifiers with independency constraints</title>
		<author>
			<persName><forename type="first">Toon</forename><surname>Calders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faisal</forename><surname>Kamiran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mykola</forename><surname>Pechenizkiy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE International Conference on Data Mining Workshops</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="13" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Acute kidney disease and renal recovery: consensus report of the Acute Disease Quality Initiative (ADQI) 16 Workgroup</title>
		<author>
			<persName><forename type="first">Rinaldo</forename><surname>Lakhmir S Chawla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Azra</forename><surname>Bellomo</surname></persName>
		</author>
		<author>
			<persName><surname>Bihorac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><forename type="middle">D</forename><surname>Stuart L Goldstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><forename type="middle">M</forename><surname>Siew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Bagshaw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinna</forename><surname>Bittleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zoltan</forename><surname>Cruz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">L</forename><surname>Endre</surname></persName>
		</author>
		<author>
			<persName><surname>Fitzgerald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Nephrology</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page">241</biblScope>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Ethical Machine Learning in Health Care</title>
		<author>
			<persName><forename type="first">Irene</forename><forename type="middle">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emma</forename><surname>Pierson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherri</forename><surname>Rose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shalmali</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kadija</forename><surname>Ferryman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marzyeh</forename><surname>Ghassemi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.10576</idno>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Biomedical Data Science</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note>To appear in</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Path-specific counterfactual fairness</title>
		<author>
			<persName><forename type="first">Silvia</forename><surname>Chiappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="7801" to="7808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Algorithmic decision making and the cost of fairness</title>
		<author>
			<persName><forename type="first">Sam</forename><surname>Corbett-Davies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emma</forename><surname>Pierson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avi</forename><surname>Feller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharad</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aziz</forename><surname>Huq</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="797" to="806" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fair transfer learning with missing protected attributes</title>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Coston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthikeyan</forename><surname>Natesan Ramamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dennis</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Skyler</forename><surname>Varshney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zairah</forename><surname>Speakman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Supriyo</forename><surname>Mustahsan</surname></persName>
		</author>
		<author>
			<persName><surname>Chakraborty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society</title>
		<meeting>the 2019 AAAI/ACM Conference on AI, Ethics, and Society</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="91" to="98" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Training Well-Generalizing Classifiers for Fairness Metrics and Other Data-Dependent Constraints</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Cotter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maya</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heinrich</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Srebro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Sridharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serena</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Blake</forename><surname>Woodworth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seungil</forename><surname>You</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1397" to="1405" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Optimal transport for domain adaptation</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Courty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">RÃ©mi</forename><surname>Flamary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devis</forename><surname>Tuia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alain</forename><surname>Rakotomamonjy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1853" to="1865" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Empirical risk minimization under fairness constraints</title>
		<author>
			<persName><forename type="first">Michele</forename><surname>Donini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Oneto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shai</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">S</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Massimiliano</forename><surname>Pontil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2791" to="2801" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Biomarkers of Acute Kidney Injury</title>
		<author>
			<persName><surname>Charles L Edelstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Chronic Kidney Disease</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="222" to="234" />
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Reconsidering the Consequences of Using Race to Estimate Kidney Function</title>
		<author>
			<persName><forename type="first">Denise</forename><surname>Nwamaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Eneanya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reese</forename><surname>Philip</surname></persName>
		</author>
		<idno type="DOI">10.1001/jama.2019.5774</idno>
		<ptr target="https://doi.org/10.1001/jama.2019.5774" />
	</analytic>
	<monogr>
		<title level="j">JAMA</title>
		<imprint>
			<biblScope unit="volume">322</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="113" to="114" />
			<date type="published" when="2019-07">2019. 07 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A comparative study of fairness-enhancing interventions in machine learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sorelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Friedler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suresh</forename><surname>Scheidegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sonam</forename><surname>Venkatasubramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><forename type="middle">P</forename><surname>Choudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Fairness, Accountability, and Transparency</title>
		<meeting>the Conference on Fairness, Accountability, and Transparency</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="329" to="338" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evgeniya</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hana</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">FranÃ§ois</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2096" to="2030" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Explaining the racial difference in AKI incidence</title>
		<author>
			<persName><forename type="first">Kunihiro</forename><surname>Morgan E Grams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingying</forename><surname>Matsushita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michelle</forename><forename type="middle">M</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meredith</forename><forename type="middle">C</forename><surname>Estrella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrienne</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><surname>Tin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linda</forename><surname>Wh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josef</forename><surname>Kao</surname></persName>
		</author>
		<author>
			<persName><surname>Coresh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Society of Nephrology</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="1834" to="1841" />
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A meta-analysis of the association of estimated GFR, albuminuria, age, race, and sex with acute kidney injury</title>
		<author>
			<persName><forename type="first">Yingying</forename><surname>Morgan E Grams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shoshana</forename><forename type="middle">H</forename><surname>Sang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ron</forename><forename type="middle">T</forename><surname>Ballew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heejin</forename><surname>Gansevoort</surname></persName>
		</author>
		<author>
			<persName><surname>Kimm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Csaba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Kovesdy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cecilia</forename><surname>Naimark</surname></persName>
		</author>
		<author>
			<persName><surname>Oien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josef</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><surname>Coresh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Journal of Kidney Diseases</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="591" to="601" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Strategic classification</title>
		<author>
			<persName><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nimrod</forename><surname>Megiddo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Papadimitriou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mary</forename><surname>Wootters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 ACM conference on innovations in theoretical computer science</title>
		<meeting>the 2016 ACM conference on innovations in theoretical computer science</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="111" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Equality of opportunity in supervised learning</title>
		<author>
			<persName><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nati</forename><surname>Srebro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3315" to="3323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The elements of statistical learning: data mining, inference and prediction</title>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerome</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Franklin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Mathematical Intelligencer</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="83" to="85" />
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Multi-perspective predictive modeling for acute kidney injury in general hospital populations using electronic medical records</title>
		<author>
			<persName><forename type="first">Jianqin</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangzhou</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lijuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lemuel</forename><forename type="middle">R</forename><surname>Waitman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mei</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JAMIA open</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="115" to="122" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">The role of risk prediction models in prevention and management of AKI</title>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Luke E Hodgson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao-Min</forename><surname>Selby</surname></persName>
		</author>
		<author>
			<persName><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lui</surname></persName>
		</author>
		<author>
			<persName><surname>Forni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Seminars in nephrology</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="421" to="430" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Stable and Fair Classification</title>
		<author>
			<persName><forename type="first">Lingxiao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nisheeth</forename><surname>Vishnoi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2879" to="2890" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">MIMIC-III, a freely accessible critical care database</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">W</forename><surname>Alistair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><forename type="middle">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Pollard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengling</forename><surname>Lehman Li-Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Ghassemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Moody</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leo</forename><surname>Szolovits</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><forename type="middle">G</forename><surname>Anthony Celi</surname></persName>
		</author>
		<author>
			<persName><surname>Mark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific data</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">160035</biblScope>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Residual Unfairness in Fair Machine Learning from Prejudiced Data</title>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Kallus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<meeting>the 35th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">KDIGO clinical practice guidelines for acute kidney injury</title>
		<author>
			<persName><forename type="first">Arif</forename><surname>Khwaja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nephron Clinical Practice</title>
		<imprint>
			<biblScope unit="volume">120</biblScope>
			<biblScope unit="page" from="179" to="184" />
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Avoiding discrimination through causal reasoning</title>
		<author>
			<persName><forename type="first">Niki</forename><surname>Kilbertus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateo</forename><forename type="middle">Rojas</forename><surname>Carulla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giambattista</forename><surname>Parascandolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>SchÃ¶lkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="656" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Counterfactual fairness</title>
		<author>
			<persName><forename type="first">Matt</forename><forename type="middle">J</forename><surname>Kusner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Loftus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4066" to="4076" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Expressing the Modification of Diet in Renal Disease Study equation for estimating glomerular filtration rate with standardized serum creatinine values</title>
		<author>
			<persName><forename type="first">Josef</forename><surname>Andrew S Levey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Coresh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jane</forename><surname>Greene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lesley</forename><forename type="middle">A</forename><surname>Marsh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">W</forename><surname>Stevens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frederick</forename><surname>Kusek</surname></persName>
		</author>
		<author>
			<persName><surname>Van Lente</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Clinical chemistry</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="766" to="772" />
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
	<note>Chronic Kidney Disease Epidemiology Collaboration</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Does mitigating ML&apos;s impact disparity require treatment disparity?</title>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Chouldechova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="8125" to="8135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Delayed impact of fair machine learning</title>
		<author>
			<persName><forename type="first">Lydia</forename><forename type="middle">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Esther</forename><surname>Rolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Simchowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 28th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6196" to="6200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">To predict and serve?</title>
		<author>
			<persName><forename type="first">Kristian</forename><surname>Lum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Isaac</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Significance</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="14" to="19" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning Adversarially Fair and Transferable Representations</title>
		<author>
			<persName><forename type="first">David</forename><surname>Madras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elliot</forename><surname>Creager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toniann</forename><surname>Pitassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3381" to="3390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Domain adaptation by using causal inference to predict invariant conditional distributions</title>
		<author>
			<persName><forename type="first">Sara</forename><surname>Magliacane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Thijs Van Ommen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Claassen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Bongers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joris</forename><forename type="middle">M</forename><surname>Versteeg</surname></persName>
		</author>
		<author>
			<persName><surname>Mooij</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="10846" to="10856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Ensuring fairness beyond the training data</title>
		<author>
			<persName><forename type="first">Debmalya</forename><surname>Mandal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suman</forename><surname>Jana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Hsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Probabilistic soft interventions in conditional Gaussian networks</title>
		<author>
			<persName><forename type="first">Florian</forename><surname>Markowetz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Grossmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rainer</forename><surname>Spang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Tenth International Workshop on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="214" to="221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Population-aware hierarchical bayesian domain adaptation via multi-component invariant learning</title>
		<author>
			<persName><forename type="first">Nabeel</forename><surname>Vishwali Mhasawade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rumi</forename><surname>Abdur Rehman</surname></persName>
		</author>
		<author>
			<persName><surname>Chunara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Conference on Health, Inference, and Learning</title>
		<meeting>the ACM Conference on Health, Inference, and Learning</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="182" to="192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<author>
			<persName><forename type="first">Shira</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Potash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Solon</forename><surname>Barocas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D'</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristian</forename><surname>Amour</surname></persName>
		</author>
		<author>
			<persName><surname>Lum</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.07867</idno>
		<title level="m">Prediction-based decisions and fairness: A catalogue of choices, assumptions, and definitions</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Joint Causal Inference from Multiple Contexts</title>
		<author>
			<persName><forename type="first">M</forename><surname>Joris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sara</forename><surname>Mooij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Magliacane</surname></persName>
		</author>
		<author>
			<persName><surname>Claassen</surname></persName>
		</author>
		<ptr target="http://jmlr.org/papers/v21/17-123.html" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1" to="108" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Fair inference on outcomes</title>
		<author>
			<persName><forename type="first">Razieh</forename><surname>Nabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Shpitser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Translation tutorial: 21 fairness definitions and their politics</title>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Fairness Accountability Transp</title>
		<meeting>Conf. Fairness Accountability Transp<address><addrLine>New York, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1170</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Feature robustness in non-stationary health records: caveats to deployable model performance in common clinical machine learning tasks</title>
		<author>
			<persName><forename type="first">Bret</forename><surname>Nestor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Mcdermott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Willie</forename><surname>Boag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriela</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tristan</forename><surname>Naumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Hughes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marzyeh</forename><surname>Goldenberg</surname></persName>
		</author>
		<author>
			<persName><surname>Ghassemi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.00690</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Dissecting racial bias in an algorithm used to manage the health of populations</title>
		<author>
			<persName><forename type="first">Ziad</forename><surname>Obermeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Powers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christine</forename><surname>Vogeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sendhil</forename><surname>Mullainathan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">366</biblScope>
			<biblScope unit="page" from="447" to="453" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Unequal Treatment: Confronting Racial and Ethnic Disparities in Health Care</title>
		<idno type="DOI">10.17226/12875</idno>
		<ptr target="https://doi.org/10.17226/12875" />
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>The National Academies Press</publisher>
			<pubPlace>Washington, DC</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<author>
			<persName><forename type="first">Luca</forename><surname>Oneto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Donini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Maurer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Massimiliano</forename><surname>Pontil</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.10673</idno>
		<title level="m">Learning fair and transferable representations</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Causality</title>
		<author>
			<persName><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Transportability of causal and statistical relations: A formal approach</title>
		<author>
			<persName><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elias</forename><surname>Bareinboim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Fifth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine Learning in Python</title>
		<author>
			<persName><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Causal inference by using invariant prediction: identification and confidence intervals</title>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>BÃ¼hlmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolai</forename><surname>Meinshausen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Statistical Methodology)</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page" from="947" to="1012" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Creating Fair Models of Atherosclerotic Cardiovascular Disease Risk</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Pfohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Marafino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrien</forename><surname>Coulet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fatima</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Latha</forename><surname>Palaniappan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nigam</surname></persName>
		</author>
		<author>
			<persName><surname>Shah</surname></persName>
		</author>
		<idno type="DOI">10.1145/3306618.3314278</idno>
		<ptr target="https://doi.org/10.1145/3306618.3314278" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society</title>
		<meeting>the 2019 AAAI/ACM Conference on AI, Ethics, and Society<address><addrLine>Honolulu, HI, USA; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="271" to="278" />
		</imprint>
	</monogr>
	<note>) (AIES &apos;19)</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Counterfactual Reasoning for Fair Clinical Risk Prediction</title>
		<author>
			<persName><forename type="first">Tony</forename><surname>Stephen R Pfohl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daisy</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><surname>Yi Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nigam</surname></persName>
		</author>
		<author>
			<persName><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning for Healthcare Conference</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="325" to="358" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Is racism a fundamental cause of inequalities in health?</title>
		<author>
			<persName><forename type="first">C</forename><surname>Jo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruce</forename><forename type="middle">G</forename><surname>Phelan</surname></persName>
		</author>
		<author>
			<persName><surname>Link</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Sociology</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="311" to="330" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Can we trust deep learning models diagnosis? The impact of domain shift in chest radiograph classification</title>
		<author>
			<persName><forename type="first">Pedro</forename><forename type="middle">L</forename><surname>Eduardo Hp Pooch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rodrigo</forename><forename type="middle">C</forename><surname>Ballester</surname></persName>
		</author>
		<author>
			<persName><surname>Barros</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.01940</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<title level="m" type="main">Dataset shift in machine learning</title>
		<author>
			<persName><forename type="first">Joaquin</forename><surname>Quionero-Candela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Schwaighofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<author>
			<persName><forename type="first">Ashkan</forename><surname>Rezaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anqi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omid</forename><surname>Memarrast</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Ziebart</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.05166</idno>
		<title level="m">Robust Fairness under Covariate Shift</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">External validation of clinical prediction models using big datasets from e-health records or IPD meta-analysis: opportunities and challenges</title>
		<author>
			<persName><forename type="first">Joie</forename><surname>Richard D Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kym</forename><forename type="middle">Ie</forename><surname>Ensor</surname></persName>
		</author>
		<author>
			<persName><surname>Snell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">A</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doug</forename><forename type="middle">G</forename><surname>Debray</surname></persName>
		</author>
		<author>
			<persName><surname>Altman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Karel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gary</forename><forename type="middle">S</forename><surname>Moons</surname></persName>
		</author>
		<author>
			<persName><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">bmj</title>
		<imprint>
			<biblScope unit="volume">353</biblScope>
			<biblScope unit="page">3140</biblScope>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Invariant models for causal transfer learning</title>
		<author>
			<persName><forename type="first">Mateo</forename><surname>Rojas-Carulla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>SchÃ¶lkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Peters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1309" to="1342" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<author>
			<persName><forename type="first">Dominik</forename><surname>RothenhÃ¤usler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolai</forename><surname>Meinshausen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>BÃ¼hlmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Peters</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.06229</idno>
		<title level="m">Anchor regression: heterogeneous data meets causality</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<author>
			<persName><forename type="first">Candice</forename><surname>Schumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Beutel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jilin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hai</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><forename type="middle">H</forename><surname>Chi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.09688</idno>
		<title level="m">Transfer of Machine Learning Fairness across Domains</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Improving predictive inference under covariate shift by weighting the log-likelihood function</title>
		<author>
			<persName><forename type="first">Hidetoshi</forename><surname>Shimodaira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of statistical planning and inference</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page" from="227" to="244" />
			<date type="published" when="2000">2000. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Fairness warnings and fair-MAML: learning fairly with minimal data</title>
		<author>
			<persName><forename type="first">Dylan</forename><surname>Slack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sorelle</forename><forename type="middle">A</forename><surname>Friedler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emile</forename><surname>Givental</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency</title>
		<meeting>the 2020 Conference on Fairness, Accountability, and Transparency</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="200" to="209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Counterfactual Normalization: Proactively Addressing Dataset Shift Using Causal Mechanisms</title>
		<author>
			<persName><forename type="first">Adarsh</forename><surname>Subbaswamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suchi</forename><surname>Saria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="947" to="957" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">I-SPEC: An End-to-End Framework for Learning Transportable</title>
		<author>
			<persName><forename type="first">Adarsh</forename><surname>Subbaswamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suchi</forename><surname>Saria</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.08948</idno>
	</analytic>
	<monogr>
		<title level="m">Shift-Stable Models</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Preventing failures due to dataset shift: Learning predictive models that transport</title>
		<author>
			<persName><forename type="first">Adarsh</forename><surname>Subbaswamy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Schulam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suchi</forename><surname>Saria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 22nd International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3118" to="3127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Direct importance estimation with model selection and its application to covariate shift adaptation</title>
		<author>
			<persName><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shinichi</forename><surname>Nakajima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hisashi</forename><surname>Kashima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">V</forename><surname>Buenau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Motoaki</forename><surname>Kawanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1433" to="1440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">A clinically applicable approach to continuous prediction of future acute kidney injury</title>
		<author>
			<persName><forename type="first">Nenad</forename><surname>TomaÅ¡ev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Jack W Rae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harry</forename><surname>Zielinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andre</forename><surname>Askham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anne</forename><surname>Saraiva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Mottram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suman</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Ravuri</surname></persName>
		</author>
		<author>
			<persName><surname>Protsyuk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">572</biblScope>
			<biblScope unit="page">116</biblScope>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Fairness without Harm: Decoupled Classifiers with Preference Guarantees</title>
		<author>
			<persName><forename type="first">Berk</forename><surname>Ustun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Parkes</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="http://proceedings.mlr.press/v97/ustun19a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning (Proceedings of Machine Learning Research</title>
		<editor>
			<persName><forename type="first">Kamalika</forename><surname>Chaudhuri</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</editor>
		<meeting>the 36th International Conference on Machine Learning ( Machine Learning Research<address><addrLine>Long Beach, California, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="6373" to="6382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Hidden in Plain Sight -Reconsidering the Use of Race Correction in Clinical Algorithms</title>
		<author>
			<persName><forename type="first">A</forename><surname>Darshali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leo</forename><forename type="middle">G</forename><surname>Vyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">S</forename><surname>Eisenstein</surname></persName>
		</author>
		<author>
			<persName><surname>Jones</surname></persName>
		</author>
		<idno type="DOI">10.1056/NEJMms2004740</idno>
		<ptr target="https://doi.org/10.1056/NEJMms2004740" />
	</analytic>
	<monogr>
		<title level="j">New England Journal of Medicine</title>
		<imprint>
			<biblScope unit="volume">383</biblScope>
			<biblScope unit="page" from="874" to="882" />
			<date type="published" when="2020">2020. 2020. 2004740</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">A study in transfer learning: leveraging data from multiple hospitals to enhance hospital-specific predictions</title>
		<author>
			<persName><forename type="first">Jenna</forename><surname>Wiens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Guttag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Horvitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Medical Informatics Association</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="699" to="706" />
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Fairness Constraints: A Flexible Approach for Fair Classification</title>
		<author>
			<persName><forename type="first">Muhammad</forename><surname>Bilal Zafar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isabel</forename><surname>Valera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manuel</forename><surname>Gomez-Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krishna</forename><forename type="middle">P</forename><surname>Gummadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="1" to="42" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Variable generalization performance of a deep learning model to detect pneumonia in chest radiographs: A cross-sectional study</title>
		<author>
			<persName><forename type="first">Marcus</forename><forename type="middle">A</forename><surname>John R Zech</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manway</forename><surname>Badgeley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><forename type="middle">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">J</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Titano</surname></persName>
		</author>
		<author>
			<persName><surname>Karl Oermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS medicine</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">1002683</biblScope>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Equality of opportunity in classification: A causal approach</title>
		<author>
			<persName><forename type="first">Junzhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elias</forename><surname>Bareinboim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3671" to="3681" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Early prediction of acute kidney injury following ICU admission using a multivariate panel of physiological measurements</title>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">A</forename><surname>Lindsay P Zimmerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela Dr</forename><surname>Reyfman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zexian</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abel</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Nelson Sanchez-Pinto</surname></persName>
		</author>
		<author>
			<persName><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC medical informatics and decision making</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Fair regression for health care spending</title>
		<author>
			<persName><forename type="first">Anna</forename><surname>Zink</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherri</forename><surname>Rose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="page" from="973" to="982" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
