<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Fairness without the Sensitive Attribute via Causal Variational Autoencoder</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Vincent</forename><surname>Grari</surname></persName>
							<email>vincent.grari@isir.upmc.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Sorbonne Universit√©</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<postCode>F-75005</postCode>
									<settlement>Paris</settlement>
									<region>ISIR</region>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">AXA</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sylvain</forename><surname>Lamprier</surname></persName>
							<email>sylvain.lamprier@isir.upmc.fr</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Sorbonne Universit√©</orgName>
								<orgName type="institution" key="instit2">CNRS</orgName>
								<address>
									<postCode>F-75005</postCode>
									<settlement>Paris</settlement>
									<region>ISIR</region>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Marcin</forename><surname>Detyniecki</surname></persName>
							<email>marcin.detyniecki@axa.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">AXA</orgName>
								<address>
									<settlement>Paris</settlement>
									<country key="FR">France</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Polish Academy of Science</orgName>
								<orgName type="institution" key="instit2">IBS PAN</orgName>
								<address>
									<settlement>Warsaw</settlement>
									<country key="PL">Poland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Fairness without the Sensitive Attribute via Causal Variational Autoencoder</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In recent years, most fairness strategies in machine learning have focused on mitigating unwanted biases by assuming that the sensitive information is available. However, in practice this is not always the case: due to privacy purposes and regulations such as RGPD in EU, many personal sensitive attributes are frequently not collected. Yet, only a few prior works address the issue of mitigating bias in this difficult setting, in particular to meet classical fairness objectives such as Demographic Parity and Equalized Odds. By leveraging recent developments for approximate inference, we propose in this paper an approach to fill this gap. To infer a sensitive information proxy, we introduce a new variational auto-encoding-based framework named SRCVAE that relies on knowledge of the underlying causal graph. The bias mitigation is then done in an adversarial fairness approach. Our proposed method empirically achieves significant improvement over existing works in the field. We observe that the generated proxy's latent space correctly recovers sensitive information and that our approach achieves a higher accuracy while obtaining the same level of fairness on two real datasets.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Over the past few years, machine learning algorithms have emerged in many different fields of application. However, this development is accompanied with a growing concern about their potential threats, such as their ability to reproduce discrimination against a particular group of people based on sensitive characteristics (e.g., religion, race, gender, etc.). In particular, algorithms trained on biased data have been shown to be prone to learn, perpetuate or even reinforce these biases <ref type="bibr" target="#b0">[Bolukbasi et al., 2016]</ref>, leading to numerous incidents being reported in recent studies <ref type="bibr" target="#b0">[Angwin et al., 2016;</ref><ref type="bibr" target="#b6">Lambrecht and E. Tucker, 2016]</ref>. To address this issue, there has been a growing interest for fair machine learning in the academic community, and a high variety of bias mitigation strategies have been proposed in the last decade [Zhang et al.,    *  Contact Author   2018; Adel et al., 2019; Hardt et al., 2016; Grari et al., 2020b;  Chen et al., 2019; Zafar et al., 2015; Celis et al., 2019;  Wadsworth et al., 2018]. Currently, the vast majority of these state-of-the-art approaches rely on having access to the sensitive information to be mitigated during training (though sometimes encrypted as in <ref type="bibr" target="#b9">[Veale and Binns, 2017;</ref><ref type="bibr" target="#b6">Kilbertus et al., 2018]</ref>). However, in practice, it is often unrealistic to assume that this sensitive information is available or even collected. In Europe, for example, a car insurance company cannot ask a potential client about his/her origin or religion, as this is strictly regulated. Furthermore, in May 2018, the EU introduced the General Data Protection Regulation (GDPR), representing one of the most important changes in the regulation of data privacy in 20 years. It strictly regulates the collection and usage of sensitive personal data. Ignoring sensitive attributes as input of predictive models in order to achieve fairness is known as "fairness through unawareness" <ref type="bibr">[Pedreshi et al., 2008]</ref>, but was shown to be insufficient since complex correlations in the data may provide unexpected links to sensitive information <ref type="bibr">[Dwork et al., 2012]</ref>.</p><p>For this reason, some approaches have attempted to obtain a fair predictor model without the sensitive information. Most of them leverage the use of external data or prior knowledge on correlations <ref type="bibr" target="#b13">[Zhao et al., 2021;</ref><ref type="bibr">Madras et al., 2018;</ref><ref type="bibr" target="#b8">Schumann et al., 2019;</ref><ref type="bibr" target="#b4">Gupta et al., 2018]</ref>. Others pursue fairness implicitly, by ensuring local smoothness in the decision function, rather than explicitly focusing on subgroups to be protected <ref type="bibr" target="#b5">[Hashimoto et al., 2018;</ref><ref type="bibr" target="#b6">Lahoti et al., 2020]</ref>.</p><p>To overcome limitations of these approaches, we propose a novel approach that leverages a causal graph to reconstruct sensitive information using Bayesian variational autoencoders (VaEs). The inferred information is then used as a proxy for mitigating biases in a adversarial fairness training setting. We empirically show experiments that this approach, based on sensitive reconstruction, is significantly more effective for achieving usual fairness objectives than its competitors, with a more direct control on mitigated biases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background and Related Work</head><p>In this paper, we consider training data which consists of n examples (x i , y i ) n i=1 , where x i ‚àà R p is the feature vector of the i-th example and y i its binary outcome. In our context the training sample x i is decomposed into two feature vectors x ci ‚àà R pc and x di ‚àà R p d . In addition, we consider an -unobserved -binary sensitive attribute s i for all i. We study fairness under the two following definitions. Definition 1. Demographic Parity: A classifier is considered fair under the demographic parity criterion if the prediction Y from features X is independent from the protected attribute S <ref type="bibr">[Dwork et al., 2012]</ref>. The underlying idea is that each demographic group has the same chance for a positive outcome. The p-rule assessment considers the likelihood ratio for the unprivileged group (the higher the more fair):</p><formula xml:id="formula_0">P-rule( Y , S) = min( P ( Y = 1|S = 1) P ( Y = 1|S = 0) , P ( Y = 1|S = 0) P ( Y = 1|S = 1) )</formula><p>Definition 2. Equalized Odds: A classifier is considered fair according to this criterion if the outcome Y has equal false positive rates and false negative rates for both demographics S = 0 and S = 1 <ref type="bibr" target="#b5">[Hardt et al., 2016]</ref>. A metric to assess this is the disparate mistreatment (DM) <ref type="bibr">[Zafar et al., 2015]</ref>, which we report as the sum of the two following quantities:</p><formula xml:id="formula_1">‚àÜF P R : |P ( Y = 1|Y = 0, S = 1) -P ( Y = 1|Y = 0, S = 0)| ‚àÜF N R : |P ( Y = 0|Y = 1, S = 1) -P ( Y = 0|Y = 1, S = 0)|</formula><p>From the state-of-the-art literature, one possible way to achieve fairness despite the unavailability of sensitive attributes during training is to use transfer learning methods from external sources of data where the sensitive group labels are known. For example, <ref type="bibr">[Madras et al., 2018]</ref> proposed to learn fair representations via adversarial learning on a specific downstream task and transfer it to the targeted one. <ref type="bibr" target="#b8">[Schumann et al., 2019]</ref> and <ref type="bibr" target="#b2">[Coston et al., 2019]</ref> focus on domain adaptation. <ref type="bibr" target="#b6">[Mohri et al., 2019]</ref> considers an agnostic federated learning context by equalizing the performance of all participants through the lens of minimax optimization and fair resource allocation. However, this makes the actual desired bias mitigation highly dependent on the distribution of the external data. Other methods require prior knowledge on sensitive correlations. With prior assumptions, <ref type="bibr" target="#b4">[Gupta et al., 2018]</ref> and <ref type="bibr" target="#b13">[Zhao et al., 2021]</ref> mitigate the dependence of the predictions on the available features that are known to be likely correlated with the sensitive attribute. However, such strongly correlated features do not always exist in the data.</p><p>Finally, a few approaches address this objective without any prior knowledge on the sensitive information. Some of these works aim at improving the accuracy for the worstcase protected group (Rawlsian Max-Min objective) by leveraging techniques from distributionally robust optimization <ref type="bibr" target="#b5">[Hashimoto et al., 2018]</ref> or adversarial learning <ref type="bibr" target="#b6">[Lahoti et al., 2020]</ref>. Other works act on the input data using a clusterbased balancing strategy in order to minimize the biases locally <ref type="bibr" target="#b11">[Yan et al., 2020]</ref>. However, such methods are usually ineffective for traditional group fairness definitions such as demographic parity and equalized odds. Their blind way of mitigation affects non-sensitive information, likely implying a degradation of the predictor accuracy.</p><p>Our approach is inherently different from the aforementioned approaches. Based on minimal prior knowledge of causal relationships in the data, we perform Bayesian inference of latent sensitive proxies, whose dependencies with prediction outputs are mitigated in a second training step. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>In our approach, we first assume the existence and availability of a specific causal graph which underlies the training data, as discussed in subsection 3.1. The causal graph allows us to infer, through Bayesian inference, a latent representation containing as much information as possible about the sensitive feature. This process is described in subsection 3.2. Finally, we present in subsection 3.3 our methodology to mitigate fairness biases while preserving as much as possible prediction accuracy using this latent representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Causal Structure of SRCVAE</head><p>Our work relies on the assumption of having an underlying causal graph describing the data, where causal interactions are indicated as directed edges between subsets of features (nodes). We consider the training data</p><p>In particular, we suppose that the graph can be represented by the illustration shown in Figure <ref type="figure" target="#fig_0">1</ref>. This structure is aimed to be generic enough to fit with most real world settings (slightly different graphs are studied in appendix). In the leftmost graph, parents of the output y are split into three components x c , x d and s. The subsets x c and x d regroup together all of the features that are given as input x to the model. The distinction between the two is made depending on the existence or absence of a causal relationship with the missing sensitive information s: no interaction is assumed with x c , while some is with x d . In addition, some causal relationship may exist between x c and x d .</p><p>To illustrate the generic aspect of this framework, we apply it to the Adult UCI dataset. The assumed causal graph of this dataset, with Gender as the sensitive attribute s and Income as the expected output y, is shown in Figure <ref type="figure">2</ref>. In this context, x c is the set of variables Race, Age and Native Country which do not depend on the sensitive attribute, while x d corresponds to all remaining variables that are generated from x c and s (i.e.,</p><formula xml:id="formula_2">x d = {Education, W ork Class, ...}).</formula><p>Assuming all of the variables except s are available, our purpose is to recover all the hidden information not caused by the set x c but responsible of x d and y. In a real world scenario, it is noteworthy that the accuracy with which one can recover the real sensitive s depends on the right representation of the complementary set x c . Yet, it is possible that the set x c is under-represented. In such a case, there is a risk that the reconstruction of s may contain some of this missing additional information. For instance, assuming that the graph from Figure <ref type="figure">2</ref> is the exact causal graph that underlies the Adult UCI, let us consider a setting where the variable Race is hidden. Hence, this variable would be likely to leak in the sensitive variable reconstruction. In such a leakage setting, we argue that working with a binary sensitive proxy would strongly degrade the inferred sensitive information, by introducing noise in the reconstruction. This is what motivated us to rather consider the rightmost graph from Figure <ref type="figure" target="#fig_0">1</ref>. It considers a multivariate continuous intermediate confounder z that both causes the sensitive s and the observed variables in x d and y. As long as the confounder z contains the real sensitive information, removing the corresponding dependence with the output prediction is guaranteed to ensure fairness for the model (we prove this in 1). As we observe in the experiments section, such a multivariate proxy also allows for better generalization abilities for mitigated prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Reconstructing the Sensitive Attributes</head><p>We describe in this section the first step of our SRCVAE (Sensitive Retrieval Causal Variational Autoencoder) framework, which aims to generate a latent representation z that contains as much information as possible about the real sensitive feature s. As discussed above, our strategy is to use Bayesian inference approximation, using the pre-defined causal graph represented in Figure <ref type="figure" target="#fig_0">1</ref>. VAE Leveraging recent developments for approximate inference with deep learning, many different works proposed to use Variational Autoencoding methods (VAE) <ref type="bibr" target="#b6">[Kingma and Welling, 2013]</ref> to model exogenous variables in causal graphs. It has been shown to achieve successful results, in particular in the sub-field of counterfactual fairness <ref type="bibr">[Louizos et al., 2017;</ref><ref type="bibr">Grari et al., 2020a]</ref>. We propose to apply VAE for our setting of fairness with hidden sensitive attribute.</p><p>Following the rightmost causal graph from Figure <ref type="figure" target="#fig_0">1</ref>, the decoder distribution p Œ∏ (x c , x d , y|z) can be factorized as:</p><formula xml:id="formula_3">p Œ∏ (x c , x d , y|z) = p(x c )p Œ∏ (x d |x c , z)p Œ∏ (y|x c , x d , z</formula><p>) Given an approximate posterior q œï (z|x c , x d , y), we obtain the following variational lower bound:</p><formula xml:id="formula_4">log(p Œ∏ (x c ,x d , y)) ‚â• E (xc,x d ,y)‚àºD, z‚àºq œï (z|xc,x d ,y) [log p Œ∏ (x d , y|x c , z) + log(p(x c )) -D KL (q œï (z|x c , x d , y)||p(z)) (1)</formula><p>where D KL denotes the Kullback-Leibler divergence of the posterior q œï (z|x c , x d , y) from a prior p(z), typically a standard Gaussian distribution N (0, I). The posterior q œï (z|x c , x d , y) is estimated using a deep neural network with parameters œï, which typically outputs the mean ¬µ œï and the variance œÉ œï of a diagonal Gaussian distribution N (¬µ œï , œÉ œï I).</p><p>The likelihood term, which factorizes as p Œ∏ (x d , y|x c , z) = p Œ∏ (x d |x c , z)p Œ∏ (y|x c , x d , z), is defined as the output of a neural network with parameters Œ∏. Since attracted by a standard prior, the posterior is supposed to remove the probability mass for any information of z that is not involved in the reconstruction of x d and y. Since x c is given together with z as input of the likelihoods, all the information from x c should be removed from the posterior distribution of z. In this paper, we employ a variant of the ELBO optimization as done in <ref type="bibr" target="#b7">[Pfohl et al., 2019]</ref>, where the term D KL (q œï (z|x c , x d , y)||p(z)) is replaced by a Maximum Mean Discrepancy (MMD) term L M M D (q œï (z)||p(z)) between the aggregated posterior q œï (z) and the prior. This has been shown to be more powerful than the classical D KL for ELBO optimization in <ref type="bibr" target="#b12">[Zhao et al., 2017]</ref>, as the latter may be too restrictive <ref type="bibr" target="#b1">[Chen et al., 2016;</ref><ref type="bibr" target="#b9">S√∏nderby et al., 2016]</ref>, and also tends to overfit the data.</p><p>HGR Minimization To be accurate, inference must ensure that no dependence is created between x c and z (no arrow is linking x c to z in the rightmost graph in Figure <ref type="figure" target="#fig_0">1</ref>). This ensures the generation of a proper sensitive proxy that is not linked to the complementary x c . However, by optimizing the ELBO Equation <ref type="formula">1</ref>, some dependence may still be observed empirically between x c and z, as we show in Section 4. This is due to some information from x c leaking to the inferred z. In order to ensure some minimum independence level, we add a penalisation term in the proposed loss function. Leveraging recent research for mitigating the dependence between continuous variables, we extend the main idea of <ref type="bibr" target="#b4">[Grari et al., 2021;</ref><ref type="bibr">Grari et al., 2020b]</ref> by adapting this penalization to the case of variational autoencoders. Following this idea, we consider the Hirschfeld-Gebelein-Renyi (HGR) coefficient <ref type="bibr" target="#b8">[R√©nyi, 1959]</ref> to measure the (possibly non linear) dependence between two (possibly multidimensional) variables. In the following, we denote as HGR w f ,wg U ‚àºD U ,V ‚àºD V (U, V ) the neural estimation of HGR between two variables U and V , computed via two inter-connected neural networks f and g with parameters w f and w g <ref type="bibr">[Grari et al., 2020b;</ref><ref type="bibr" target="#b4">Grari et al., 2021]</ref>:</p><formula xml:id="formula_5">HGR w f ,wg U ‚àºD U ,V ‚àºD V (U, V ) = max w f ,wg E U ‚àºD U ,V ‚àºD V ( f w f (U ) g wg (V ))</formula><p>where D U (resp. D V ) is the distribution of U (resp. V ), and f (resp. g) refer to standardized outputs of network f (resp. g).</p><p>Reconstruction Objective Altogether, the final objective of our SRCVAE approach is given as: where Œª mmd , Œª inf are scalar hyperparameters. The additional MMD objective can be interpreted as minimizing the distance between all moments of each aggregated latent code distribution and the prior distribution. Note that giving y as input of the inference scheme q(z|x c , x d , y) is allowed since z is only used during training (see next section).</p><formula xml:id="formula_6">arg min Œ∏,œï max w f ,wg -E (xc,x d ,y)‚àºD, z‚àºq œï (z|xc,x d ,y) [log p Œ∏ (x d , y|x c , z) + Œª mmd L M M D (q œï (z)||p(z))] + Œª inf HGR w f ,wg (xc,x d ,y)‚àºD, z‚àºq œï (z|xc,x d ,y) (x c , z) y ùúô ùëì ! ! ùëî ! " ùê∏(ùëì ! ! x " ùëî ! " ùëß ) x " z x # Encoder q (a) Max Phase: HGR y Encoder q ùúô y ! ùëù ! ! ùëù ! " " ùëì # # ùëî # $ ùê∏(ùëì # # (x $ )ùëî # $ (ùëß)) Estimated HGR Decoder ùëù ! x ! x " x " z<label>(</label></formula><p>In Figure <ref type="figure">3</ref>, we represent the min-max structure of SRC-VAE. The left structure represents the max phase where the HGR between z and x c is estimated by gradient ascent with multiple iterations. The right graph represents the min phase where the reconstruction of x d and y is performed by the decoder p Œ∏ (red frame) via the generated latent space z from the encoder q œï . The adversarial HGR component (blue frame) ensures independence between the generated latent space z and x c . The network f takes the set x c as input, while g takes the continuous representation space z. This way, for each gradient iteration of SRCVAE we capture the estimated between the set x c and the generated proxy latent space z. At the end of each iteration, the algorithm updates the parameters of the decoder parameters Œ∏ as well as the encoder parameters œï by one step of gradient descent. Concerning the HGR adversary, the backpropagation of the parameters œâ f and œâ g is performed by multiple steps of gradient ascent. This allows for a more accurate estimation of the HGR at each step, leading to a far more stable learning process. Œª inf controls the importance of the dependence loss in the optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Mitigating the Unwanted Biases</head><p>The sensitive reconstruction model can now be used for training a fair predictive function h Œ∏ . Since z contains some continuous multidimensional information, we adopt an HGRbased approach inspired from <ref type="bibr">[Grari et al., 2020b;</ref><ref type="bibr" target="#b4">Grari et al., 2021]</ref> which have shown superior performance in this context. In our setting, we also verify this claim empirically as shown in appendix. We propose to mitigate the unwanted bias via an adversarial penalization during the training phase that depends on the targeted fairness objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Demographic Parity</head><p>We propose to find a mapping h Œ∏ (x) that both minimizes the deviation with the expected target y and does not imply much dependency with the representation z, inferred from q œï (z|x c , x d , y) as described in the previous section. We propose the following optimization, which considers a neural estimation of HGR as well, but this time applied to variables h Œ∏ (x) (the output of the classifier) and z (the inferred latent representation):</p><formula xml:id="formula_7">arg min Œ∏ max œà f ,œàg L(h Œ∏ (x), y)+Œª DP HGR œà f ,œàg (xc,x d ,y)‚àºD, z‚àºq œï (z|xc,x d ,y) (h Œ∏ (x), z)</formula><p>where L is the predictor loss function (the log-loss function in our experiments) of the output h Œ∏ (x) ‚àà R w.r.t. the target label y. The hyperparameter Œª DP controls the impact of dependence between the output prediction h Œ∏ (x) ‚âà p(y = 1|x d , x c ) and the sensitive proxy z. To assess this correlation, K different representations are sampled for each observation (x ci , x di , y i ) from the causal model (200 in our experiments). As in the inference phase, the backpropagation of the HGR adversary with parameters œà f and œà g is performed by multiple steps of gradient ascent. This allows to optimize a more accurate estimation of the HGR at each step, leading to a greatly more stable predictive learning process. Practice in real-world As mentioned in the first subsection, the assumed causal graph 1 requires the right representation of the complementary set x c . If the set x c is underrepresented, some specific hidden attributes can be integrated with the sensitive information in the inferred sensitive latent space z. The following Theorem 1 allows us to ensure that mitigating the HGR between z and y implies some upperbound for the targeted objective (proof in appendix). Theorem 1. For two nonempty index sets S and Z such that S ‚äÇ Z and ≈∂ the output prediction of the model, we have:</p><formula xml:id="formula_8">HGR( ≈∂ , Z) ‚â• HGR( ≈∂ , S)</formula><p>(2) Proof. in appendix Therefore, minimizing HGR( ≈∂ , Z) tends to reduce the real bias objective HGR( ≈∂ , S). Results on benchmark and realworld datasets demonstrate below in part 1 that such an assumed graph demonstrates good robustness properties. This property is also held for equalized-odds we consider below, with HGR( ≈∂ , Z|Y ) ‚â• HGR( ≈∂ , S|Y ). Equalized odds We extend the demographic parity optimization to the equalized odds task. The objective is to find a mapping h Œ∏ (x) which both minimizes the deviation with the expected target y and does not imply too much dependency with the representation z conditioned on the actual outcome y. For the decomposition of disparate mistreatment, we propose to divide the mitigation based on the two different values of y. Identification and mitigation of the specific non linear dependence for these two subgroups leads to the same false positive and the same false negative rates for each demographic. We propose the following optimization: with D 0 (resp. D 1 ) corresponding to the observations set (x, y) verifying y = 0 (resp. y = 1). The hyperparameters Œª 0 and Œª 1 control the impact of the dependence loss for the false positive and the false negative objective respectively. The first penalisation (controlled by Œª 0 ) enforces the independence between the output prediction h Œ∏ (x) ‚âà p Œ∏ (y = 1|x) and the sensitive proxy z only for the cases where y = 0. It enforces the mitigation of the difference of false positive rates between demographics, since at optimum for Œ∏ * with no trade-off (i.e., with infinite Œª 0 ) and (x, y) ‚àº D 0 , HGR(h Œ∏ * (x), z) = 0 and implies theoretically: h Œ∏ * (x) ‚ä• z|y = 0. The second one enforces the mitigation of the difference between the true positive rates, since the dependence loss is performed between the output prediction h Œ∏ (x) and the sensitive proxy only for cases where y = 1 (i.e., mitigation of ‚àÜ F N R ).</p><formula xml:id="formula_9">arg min Œ∏ max œà f 0 ,œàg 0 ,œà f 1 ,œàg 1 L(h Œ∏ (x), y)+ Œª0 HGR œà f 0 ,œàg 0 (x,y)‚àºD 0 , z‚àºq œï (z|x,y) (h Œ∏ (x), z) + Œª1 HGR œà f 1 ,œàg 1 (x,y)‚àºD 1 , z‚àºq œï (z|x,y) (h Œ∏ (x), z)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Results</head><p>For our experiments, we empirically evaluate the performance of our contribution on real-world data sets where the sensitive s is available. This allows to assess the fairness of the output prediction, obtained without the use of the sensitive attribute, w.r.t. this ground truth. For this purpose, we use the popular Adult UCI and Default datasets (descriptions in Appendix), often used in fair classification. Sensitive Reconstruction In order to understand the interest of mitigating the dependence between the latent space z and the complementary set x c during the inference phase, we plot the t-SNE of z with two different inference models for the Adult UCI dataset in Figure <ref type="figure" target="#fig_3">4</ref>. We consider a version of our model trained without the penalization term (Œª inf = 0.00) as a baseline. It is then compared to a version trained with a penalization term equal to 0.20. As expected, training the inference model without the penalization term results in a poor reconstruction of the z proxy, where the dependence on x c is observed. We can observe that the separation between the men (blue points) and women (red points) data is not significant. We also observe that increasing this hyper-parameter (Œª inf ) allows to decrease the HGR estimation from 81.7% to 22.6% and to greatly increase the separation between male and female data points. Bias Mitigation The dynamics of adversarial training for demographic parity is performed for Adult UCI with unfair (Œª DP = 0) and fair (Œª DP = 0.5) models as illustrated in Figure <ref type="figure">6</ref>. Other values are presented in appendix. We represent the accuracy of the model (top), the P-rule metric between the prediction and the real sensitive s (middle), and the HGR between the prediction and the latent space z (bottom). For the unfair model (leftmost graph) we observe that the convergence is stable and achieves a P-rule of 29.5%. As expected, the penalization loss decreases (measured with the HGR) when the hyperparameter Œª DP is increased. It allows to increase the fairness metric P-rule to 83.1% with a slight drop of accuracy.</p><p>In Figure <ref type="figure">5</ref> we plot the distribution of the predicted probabilities for each sensitive attribute s for three different models: an unfair model with Œª DP = 0, and two fair models with Œª DP = 0.45 and 0.50, respectively. For the leftmost graph (i.e. Œª DP = 0) the model appears to be very unfair, since the distribution between the sensitive groups differs importantly. As expected, we observe that the distributions are more aligned as Œª DP values increase.</p><p>For the two datasets, we test different models where, for each, we repeat five runs by randomly sampling two subsets, 80% for the training set and 20% for the test set. As different optimization objectives result in different algorithms, we run separate experiments for the two fairness objectives of our interest. As an optimal baseline to be reached, we consider the approach from <ref type="bibr" target="#b0">[Adel et al., 2019]</ref> using observations of the sensitive s during training, which we denote as  <ref type="bibr">et al., 2020]</ref>. The latter is only compared for the equalized odds task (i.e. discussion in <ref type="bibr" target="#b13">[Zhao et al., 2021]</ref>). We plot the performance of these approaches by displaying the Accuracy against the P-rule for Demographic Parity (Figure <ref type="figure">7</ref>) and the Disparate Mistreatment (DM) for Equalized Odds (Figure <ref type="figure" target="#fig_5">8</ref>). For all algorithms, we clearly observe that the Accuracy, or predictive performance, decreases when fairness increases. As expected, the baseline True S achieves the best performance for all the scenarios with the highest accuracy and fairness. We note that, for all levels of fairness (controlled by the mitigation weight in every approach), our method outperforms state-of-the-art algorithms for both fairness tasks (except some points for very low levels of fairness, on the left of the curves). We attribute this to the ability of SRCVAE to extract a useful sensitive proxy, while the approaches FairRF and ProxyFairness seem to greatly suffer from merely considering correlations present in the data for mitigating fairness. The approach FairBalance, which preprocessed the data with clustering, seems inefficient and degrades the predictive performance too significantly. The advantages of our approach are more pronounced on the Default dataset, where a less obvious correlation exists between observed variables and the sensitive attribute. In that setting, leveraging the knowledge of a causal graph appears to be crucial.</p><p>Proxy dimensions In figure <ref type="figure">9</ref>(a), we perform an additional experiment on the sensitive proxy. For the two datasets we observe that increasing z dimensions results in increased accuracy. Increasing the dimensions to 5 for Adult UCI (same experiment for Default in appendix) allows to obtain better results in terms of accuracy and this for all levels of P-rule. We claim that mitigating biases in larger spaces allows better generalisation abilities at test time, as already observed in  <ref type="bibr">S8)</ref>. From the results, our approach appears greatly robust to noise, with results in every scenario at least comparable to the best considered competitors (which all present settings where performances catastrophically drop as observed in Fig. <ref type="figure">7</ref> and<ref type="figure" target="#fig_5">8</ref>). This robustness is partly achieved thanks to the use of a multivariate continuous proxy z, which limits the possible lack of sensitive information that would occur with a scalar proxy of s, if non-sensitive information leaks in the reconstruction. While the inclusion of variables from x d to x c may induce the removal of some useful sensitive information from the proxy, the inclusion of variables from x c to x d may lead to optimize the independence of some non sensitive information with model outputs. If fairness needs to be guaranteed, the expert must thus tend to favor false x d variables rather than false x c , the former only inducing a slight accuracy loss in most cases (as demonstrated in Theorem 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>This paper proposed a new way to mitigate undesired bias without the availability of the sensitive demographic information in training. To generate a latent representation which is expected to contain the most sensitive information as possible, the approach relies on a new variational auto-encoding based framework named SRCVAE. In a second phase, inferred proxies serve to mitigate biases in an adversarial fairness training of a prediction model. Compared with other state-of-the-art algorithms, our method proves to be more efficient in terms of accuracy for similar levels of fairness. For further investigation, we are interested in extending this work to settings where the actual sensitive can be continuous (e.g. age or weight attribute) and/or multivariate.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: Causal graphs of SRCVAE: Left graph represents prior expert knowledge, where x is mapped into two components xc and x d . Right graph denotes the graph considered in our approach, with a multivariate confounder z inferred to be used as a proxy of the sensitive attribute s. Solid arrows denote causal links, red dashed arrows denote inference, grey circles denote missing attributes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>?</head><label></label><figDesc>Figure 2: Causal Graph -Adult UCI</figDesc><graphic coords="3,56.16,65.32,238.64,105.67" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Figure 3: Neural architecture of SRCVAE in max phase for the HGR estimation between xc and z via gradient ascent (a) and Variational autoencoder structure of SRCVAE in min phase (b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Inference phase for Adult UCI: t-SNE of the sensitive latent reconstruction Z. Blue points are males (S = 1), red ones are females (S = 0). Increasing Œª inf improves the independence of z from xc. This leads to a better separation between male and female data points, which indicates a proper sensitive proxy.</figDesc><graphic coords="5,54.00,63.97,126.00,126.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Figure 5: Distributions of the predicted probabilities given the real sensitive s (Adult UCI data set) for the Demographic Parity task.</figDesc><graphic coords="5,180.00,63.97,126.00,126.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Equalized odds task</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc><ref type="bibr" target="#b4">Grari et al., 2021]</ref>. It supports the choice of considering a multivariate sensitive proxy z, rather than directly acting on a reconstruction of s as a univariate variable. Noisy graph In figure9(b), we analyse the impact of noise in the causal graph. To do this, we focus on cases where the decomposition of x in sets x c and x d is noisy, or sets of variables are under-represented. For this purpose, we experimented 8 scenarios on the Adult UCI data set. First, we removed features from x c : the race (S1), the age (S2). Then, we removed features from x d : the education (S3) and the hour (S4). Finally, we moved features from x c to x d and reversely: membership inversion between race and education (S5), membership inversion between age and hour (S6), inclusion of age in x d (S7) and inclusion of hour in x c</figDesc><table><row><cell>Classification accuracy %</cell><cell>74 76 78 80 82 84</cell><cell>0.6 p-rule % (a) Dimension of z</cell><cell>0.8</cell><cell>Latent Z dim=1 dim=3 dim=5</cell><cell>Classification accuracy %</cell><cell>86 74 76 78 80 82 84 72</cell><cell>(b) Noisy Graph 0.4 0.6 p-rule %</cell><cell>0.8</cell><cell>Settings Original S1 S2 S3 S4 S5 S6 S7 S8</cell></row><row><cell></cell><cell></cell><cell cols="6">Figure 9: Additional Experiments</cell><cell></cell><cell></cell></row><row><cell cols="4">another context in [</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence </p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Classification with fairness constraints: A meta-algorithm with provable guarantees</title>
		<author>
			<persName><forename type="first">Adel</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Fairness, Accountability, and Transparency</title>
		<meeting>the Conference on Fairness, Accountability, and Transparency</meeting>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016-05-23">2019. 2019. 2016. May 23, 2016, 2016. 2016. 2016. 2019. 2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="319" to="328" />
		</imprint>
	</monogr>
	<note>Machine bias. ProPublica</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Chen</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1611.02731</idno>
		<title level="m">Prafulla Dhariwal, John Schulman, Ilya Sutskever, and Pieter Abbeel. Variational lossy autoencoder</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fairness under unawareness: Assessing disparity when protected class is unobserved</title>
		<author>
			<persName><forename type="first">Chen</forename></persName>
		</author>
		<idno type="arXiv">arXiv:2008.13122</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd innovations in theoretical computer science conference</title>
		<editor>
			<persName><forename type="first">Vincent</forename><surname>Grari</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sylvain</forename><surname>Lamprier</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Marcin</forename><surname>Detyniecki</surname></persName>
		</editor>
		<meeting>the 3rd innovations in theoretical computer science conference</meeting>
		<imprint>
			<date type="published" when="2012">2019. 2019. 2019. 2019. 2012. 2020</date>
			<biblScope unit="page" from="214" to="226" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Adversarial learning for counterfactual fairness</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Fairness-aware neural r√©nyi minimization for continuous features</title>
		<author>
			<persName><surname>Grari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI 2020</title>
		<editor>
			<persName><forename type="first">Christian</forename><surname>Bessiere</surname></persName>
		</editor>
		<meeting>the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI 2020</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="2262" to="2268" />
		</imprint>
	</monogr>
	<note>ijcai.org</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning unbiased representations via r√©nyi minimization</title>
		<author>
			<persName><surname>Grari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.11212</idno>
	</analytic>
	<monogr>
		<title level="m">ECML PKDD</title>
		<editor>
			<persName><forename type="first">Maya</forename><surname>Gupta</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Andrew</forename><surname>Cotter</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Mahdi</forename><surname>Milani Fard</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Serena</forename><surname>Wang</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2018">2021. 2021. 2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">Proxy fairness. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fairness without demographics in repeated loss minimization</title>
		<author>
			<persName><surname>Hardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2016">2016. 2016. 2018. 2018</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1929" to="1938" />
		</imprint>
	</monogr>
	<note>Advances in neural information processing systems</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Algorithmic Bias? An Empirical Study into Apparent Gender-Based Discrimination in the Display of STEM Career Ads</title>
		<author>
			<persName><surname>Kilbertus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<idno>arXiv:2006.13114</idno>
	</analytic>
	<monogr>
		<title level="m">Fairness without demographics through adversarially reweighted learning</title>
		<editor>
			<persName><surname>Madras</surname></persName>
		</editor>
		<meeting><address><addrLine>Uri Shalit, Joris M Mooij, David Sontag, Richard Zemel, and Max Welling</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">2018. 2018. 2013. 2013. 2020. 2016. 2016. 2017. 2018. 2019. 2019. 2008</date>
			<biblScope unit="page" from="3384" to="3393" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>International Conference on Machine Learning. Pedreshi et al., 2008] Dino Pedreshi, Salvatore Ruggieri, and Franco Turini. Discrimination-aware data mining. In KDD&apos;08, page 560</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><surname>Pfohl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.06260</idno>
		<title level="m">Counterfactual reasoning for fair clinical risk prediction</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">On measures of dependence</title>
		<author>
			<persName><forename type="first">Alfr√©d</forename><surname>R√©nyi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>R√©nyi</surname></persName>
		</author>
		<author>
			<persName><surname>Schumann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.09688</idno>
	</analytic>
	<monogr>
		<title level="m">Transfer of machine learning fairness across domains</title>
		<imprint>
			<date type="published" when="1959">1959. 1959. 2019. 2019</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="441" to="451" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Fairer machine learning in the real world: Mitigating discrimination without collecting sensitive data</title>
		<author>
			<persName><surname>S√∏nderby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS&apos;16</title>
		<imprint>
			<publisher>Big Data &amp; Society</publisher>
			<date type="published" when="2016">2016. 2016. 2017. 2017</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">2053951717743530</biblScope>
		</imprint>
	</monogr>
	<note>Ladder variational autoencoders</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Achieving fairness through adversarial learning: an application to recidivism prediction</title>
		<author>
			<persName><surname>Wadsworth</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1807.00199</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fair class balancing: enhancing model fairness without observing sensitive attributes</title>
		<author>
			<persName><forename type="first">Yan</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1507.05259</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Information &amp; Knowledge Management</title>
		<editor>
			<persName><forename type="first">Muhammad</forename><surname>Bilal Zafar</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Isabel</forename><surname>Valera</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Manuel</forename><forename type="middle">Gomez</forename><surname>Rodriguez</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Krishna</forename><forename type="middle">P</forename><surname>Gummadi</surname></persName>
		</editor>
		<meeting>the 29th ACM International Conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2015">2020. 2020. 2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Fairness constraints: Mechanisms for fair classification</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Mitigating unwanted biases with adversarial learning</title>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02262</idno>
	</analytic>
	<monogr>
		<title level="m">Shengjia Zhao, Jiaming Song, and Stefano Ermon. Infovae: Information maximizing variational autoencoders</title>
		<imprint>
			<date type="published" when="2017">2018. 2018. 2017. 2017</date>
			<biblScope unit="page" from="335" to="340" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>AAAI&apos;18</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">You can still achieve fairness without sensitive attributes: Exploring biases in non-sensitive features</title>
		<author>
			<persName><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.14537</idno>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
