<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Causal Graph Fuzzy LLMs: A First Introduction and Applications in Time Series Forecasting</title>
				<funder>
					<orgName type="full">Coordination for the Improvement of Higher Education Personnel (CAPES)</orgName>
				</funder>
				<funder ref="#_X6PHgnJ">
					<orgName type="full">R&amp;D&amp;I agreement between UFMG and Fundep -Research Development Foundation</orgName>
				</funder>
				<funder>
					<orgName type="full">Academic Excellence Program (PROEX)</orgName>
				</funder>
				<funder ref="#_QtNechP">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2025-07-22">22 Jul 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Omid</forename><surname>Orang</surname></persName>
							<email>omid.orang@dcc.ufmg.br</email>
							<idno type="ORCID">0000-0002-4077-3775</idno>
							<affiliation key="aff4">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="laboratory">Future Lab</orgName>
								<orgName type="institution">Federal University of Minas Gerais</orgName>
								<address>
									<settlement>Belo Horizonte</settlement>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Patrícia</forename><forename type="middle">O</forename><surname>Lucas</surname></persName>
							<email>patricia.lucas@ifnmg.edu.br</email>
							<idno type="ORCID">0000-0002-7334-8863</idno>
							<affiliation key="aff1">
								<orgName type="institution">Federal Institute of Northern Minas Gerais</orgName>
								<address>
									<settlement>Salinas</settlement>
									<region>MG</region>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Graduate Program in Electrical Engineering</orgName>
								<orgName type="institution" key="instit2">Universidade Federal de Minas Gerais</orgName>
								<address>
									<addrLine>Av. Antônio Carlos 6627 Belo Horizonte</addrLine>
									<postCode>31270-901</postCode>
									<region>MG</region>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="laboratory">Future Lab</orgName>
								<orgName type="institution">Federal University of Minas Gerais</orgName>
								<address>
									<settlement>Belo Horizonte</settlement>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Gabriel</forename><forename type="middle">I F</forename><surname>Paiva</surname></persName>
							<email>gabrielpaiva18@gmail.com</email>
							<affiliation key="aff3">
								<orgName type="department">Graduate Program in Computer Science</orgName>
								<orgName type="institution">Federal University of Minas Gerais</orgName>
								<address>
									<settlement>Belo Horizonte</settlement>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="laboratory">Future Lab</orgName>
								<orgName type="institution">Federal University of Minas Gerais</orgName>
								<address>
									<settlement>Belo Horizonte</settlement>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Petrônio</forename><forename type="middle">C L</forename><surname>Silva</surname></persName>
							<idno type="ORCID">0000-0002-1202-2552</idno>
							<affiliation key="aff0">
								<orgName type="institution">Federal Institute of Northern Minas Gerais</orgName>
								<address>
									<settlement>Januária</settlement>
									<region>MG</region>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="laboratory">Future Lab</orgName>
								<orgName type="institution">Federal University of Minas Gerais</orgName>
								<address>
									<settlement>Belo Horizonte</settlement>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Felipe</forename><forename type="middle">Augusto</forename><surname>Rocha Da Silva</surname></persName>
							<idno type="ORCID">0000-0003-4567-8504</idno>
							<affiliation key="aff2">
								<orgName type="institution" key="instit1">Graduate Program in Electrical Engineering</orgName>
								<orgName type="institution" key="instit2">Universidade Federal de Minas Gerais</orgName>
								<address>
									<addrLine>Av. Antônio Carlos 6627 Belo Horizonte</addrLine>
									<postCode>31270-901</postCode>
									<region>MG</region>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="laboratory">Future Lab</orgName>
								<orgName type="institution">Federal University of Minas Gerais</orgName>
								<address>
									<settlement>Belo Horizonte</settlement>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Adriano</forename><forename type="middle">Alonso</forename><surname>Veloso</surname></persName>
							<email>adrianov@dcc.ufmg.br</email>
							<idno type="ORCID">0000-0002-9177-4954</idno>
							<affiliation key="aff4">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="laboratory">Future Lab</orgName>
								<orgName type="institution">Federal University of Minas Gerais</orgName>
								<address>
									<settlement>Belo Horizonte</settlement>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Frederico</forename><surname>Gadelha Guimarães</surname></persName>
							<email>fredericoguimaraes@ufmg.br</email>
							<idno type="ORCID">0000-0001-9238-8839</idno>
							<affiliation key="aff4">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="laboratory">Future Lab</orgName>
								<orgName type="institution">Federal University of Minas Gerais</orgName>
								<address>
									<settlement>Belo Horizonte</settlement>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Causal Graph Fuzzy LLMs: A First Introduction and Applications in Time Series Forecasting</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-07-22">22 Jul 2025</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2507.17016v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Time Series Forecasting</term>
					<term>Large Language Models</term>
					<term>Fuzzy Time Series</term>
					<term>Causal Graph</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In recent years, the application of Large Language Models (LLMs) to time series forecasting (TSF) has garnered significant attention among researchers. This study presents a new frame of LLMs named CGF-LLM using GPT-2 combined with fuzzy time series (FTS) and causal graph to predict multivariate time series, marking the first such architecture in the literature. The key objective is to convert numerical time series into interpretable forms through the parallel application of fuzzification and causal analysis, enabling both semantic understanding and structural insight as input for the pretrained GPT-2 model. The resulting textual representation offers a more interpretable view of the complex dynamics underlying the original time series. The reported results confirm the effectiveness of our proposed LLMbased time series forecasting model, as demonstrated across four different multivariate time series datasets. This initiative paves promising future directions in the domain of TSF using LLMs based on FTS.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Time series forecasting (TSF) and analysis play a pivotal role in many real-world applications, such as energy, traffic, healthcare, finance, and meteorology <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b1">[2]</ref>. A variety of forecasting methods have been proposed in the literature, generally classified into three main categories: statistical, machine learning (ML), and deep learning (DL) approaches <ref type="bibr" target="#b2">[3]</ref>. Despite their successes, these methods still face challenges such as high dimensionality, missing values, limited data availability, and the need to capture long-term dependencies-all of which are critical for accurate temporal modeling <ref type="bibr" target="#b0">[1]</ref>.</p><p>Large Language Models (LLMs), built on transformers composed of billions of parameters, have emerged to revolutionize DL models <ref type="bibr" target="#b2">[3]</ref>. Although LLMs were originally developed for natural language processing (NLP) tasks, their application in time series (TS) analysis and forecasting has recently gained significant momentum. This shift is largely due to their ability to leverage self-attention mechanisms to capture temporal dependencies and complex dynamics, thereby effectively modeling input-output relationships <ref type="bibr" target="#b3">[4]</ref>. This surge is driven by their remarkable ability to capture longrange dependencies and complex sequential patterns through the attention mechanism inherent in the transformers <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b4">[5]</ref>.</p><p>According to the reviewed literature <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b5">[6]</ref>, a number of methods leveraging LLMs have been proposed for TSF, differing in input types, model integration strategies, and LLM architectures. For instance, Time-LLM <ref type="bibr" target="#b6">[7]</ref> uses LLaMA and GPT-2 to process multimodal TS (MTS) and relies on tokenization and fine-tuning strategies. TEMPO <ref type="bibr" target="#b7">[8]</ref>, which also employs GPT-2, handles univariate TS data using tokenization and prompt-based modeling without fine-tuning. In contrast, PromptCast <ref type="bibr" target="#b8">[9]</ref> uses BART and BERT, focusing on prompt engineering without modifying LLM weights. Chronos <ref type="bibr" target="#b9">[10]</ref> incorporates GPT-2 and T5, combining prompt design with model fine-tuning. LLMTIME <ref type="bibr" target="#b10">[11]</ref> utilizes GPT-3 and LLaMA-2 to handle multimodal inputs with prompt tuning and partial integration of LLMs into the downstream task. GPT4MTS <ref type="bibr" target="#b11">[12]</ref>, employing GPT-2, and UniTime <ref type="bibr" target="#b12">[13]</ref>, also with GPT-2, both use prompt-based methods without integrating the LLM as part of the model. Meanwhile, S 2 IP-LLM <ref type="bibr" target="#b13">[14]</ref> relies on GPT-2 and fully integrates it into the forecasting pipeline. Several methods, such as LAMP <ref type="bibr" target="#b14">[15]</ref> (using GPT-3 variants and LLaMA-2) and the model proposed in <ref type="bibr" target="#b15">[16]</ref> (with GPT-4 and Open-LLaMA), explore newer LLMs and finetuning for multivariate forecasting in specific domains. These diverse strategies reflect the flexibility of LLMs in modeling sequential dependencies in TS, originally designed for text but increasingly adapted to structured temporal data. Notably, methods like Time-LLM, TEMPO, Chronos, and S 2 IP-LLM provide open-source code, fostering reproducibility and further research.</p><p>In more domain-specific scenarios, the authors in <ref type="bibr" target="#b16">[17]</ref> use ChatGPT to query multimodal data for financial forecasting without fine-tuning or integration. In the healthcare domain, Liu et al. <ref type="bibr" target="#b17">[18]</ref> apply PaLM for MTS both forecasting and classification, although without integration or tokenization. For mobility forecasting, AuxMobLCast <ref type="bibr" target="#b18">[19]</ref> leverages LLMs such as BERT, RoBERTa, GPT-2, and XLNet, combining tokenization and fine-tuning strategies. LLM-Mob <ref type="bibr" target="#b19">[20]</ref> builds on GPT-3.5, using token-based modeling without integration. Finally, in traffic forecasting, ST-LLM <ref type="bibr" target="#b20">[21]</ref> employs LLaMA and GPT-2, utilizing tokenization and prompt engineering with full integration into the final model. These models demonstrate that LLMs can be effectively adapted beyond general domains, extending their capabilities to temporal, multimodal, and spatio-temporal forecasting tasks across sectors.</p><p>This research pioneers a new multiple-input single-output (MISO) LLM-based forecasting model termed CGF-LLM. This method combines the concepts of fuzzy time series (FTS) <ref type="bibr" target="#b21">[22]</ref>, causal graphs, and LLMs. The central objective of this work is to transform numerical time series into interpretable linguistic representations through the parallel application of fuzzification and causal analysis. This dual approach enables both semantic understanding of variable behavior and structural insight into their temporal dependencies. Specifically, the framework integrates FTS modeling with causal discovery using the PCMCI algorithm <ref type="bibr" target="#b22">[23]</ref>. By combining these two perspectives, the method constructs a fuzzy causal text that is both data-driven and interpretable, which serves as input for GPT-2. In other words, it extracts meaningful knowledge, providing a clearer understanding of the complex dynamics within the original time series and revealing the causal relationships among variables. The results confirm that the proposed CGF-LLM technique surpasses the standard LLM in both accuracy and computational efficiency.</p><p>The remainder of this paper is structured as follows: Section II provides the basics of LLMs, FTS, and causal graphs. Section III outlines the details of the proposed CGF-LLM method. Section IV covers the case studies, results, and discussion. Finally, Section V concludes the paper and highlights the future research avenues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. PRELIMINARY CONCEPTS A. Large Language Models</head><p>The advent of Large Language Models (LLMs) has significantly advanced natural language processing (NLP), with early landmark models such as BERT <ref type="bibr" target="#b23">[24]</ref>, GPT-2 <ref type="bibr" target="#b24">[25]</ref>, and RoBERTa <ref type="bibr" target="#b25">[26]</ref>. These models are built upon the Transformer architecture introduced in <ref type="bibr" target="#b26">[27]</ref>, which replaced recurrence with self-attention, enabling more effective handling of sequential data.</p><p>A major leap occurred with GPT-3 <ref type="bibr" target="#b27">[28]</ref>, a 175 B-parameter model that demonstrated remarkable few-shot capabilities across diverse tasks. Its success, however, also highlighted issues such as limited transparency, bias, and occasional unreliability. Subsequent models sought to improve on these limitations. Google's PaLM <ref type="bibr" target="#b28">[29]</ref> introduced the Pathways approach for better reasoning and coherence at scale (540B parameters), while DeepMind's Chinchilla <ref type="bibr" target="#b29">[30]</ref> showed that smaller models (70B) can outperform larger ones when trained more efficiently. Recent models such as GPT-4 <ref type="bibr" target="#b30">[31]</ref>, LaMDA <ref type="bibr" target="#b31">[32]</ref>, LLaMA <ref type="bibr" target="#b32">[33]</ref>, and DeepSeek <ref type="bibr" target="#b33">[34]</ref>, among others, further expand capabilities.</p><p>Alongside performance gains, efforts to enhance efficiency (e.g., pruning, quantization) and interpretability (e.g., attention maps, chain-of-thought prompting) have become central to LLM research, enabling broader and safer application across domains, including time series analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Fuzzy time series</head><p>The concept of Fuzzy Time Series (FTS) was originally proposed by <ref type="bibr" target="#b34">[35]</ref>- <ref type="bibr" target="#b36">[37]</ref>, based on the fuzzy set theory of <ref type="bibr" target="#b37">[38]</ref>. The main idea of the model is to convert numerical time series into linguistic representations to describe and forecast their behavior through fuzzy relation rules or matrices.</p><p>Consider a univariate time series Y ∈ R, composed of observations y(t) for t = 0, 1, . . . , T . Each value y(t) can be associated with a fuzzy set A i ∈ Ã through a membership function µ Ai : R → [0, 1], which measures the degree of membership of y(t) in A i . The most common membership functions are triangular, trapezoidal, sigmoid, and Gaussian.</p><p>According to the approach of <ref type="bibr" target="#b36">[37]</ref>, the training process of an FTS model is divided into three main steps: a) Partitioning: Universe of Discourse (UoD), represented by U = [min(Y ), max(Y )], is segmented into k overlapping subintervals. For each subinterval, a fuzzy set A i is defined with its respective membership function µ Ai and central point c i . From these sets, the linguistic variable Ã is constructed, whose terms are the A i , with i = 1, . . . , k. b) Fuzzification: transforms the series Y into a fuzzy sequence F , in which each element f (t) ∈ F corresponds to a k-component vector, representing the degrees of association of the value y(t) with the fuzzy sets A i of the linguistic variable Ã. c) Rule Generation: from the fuzzified series F , transition patterns are identified between consecutive pairs (f (t -1), f (t)), from which rules of the type A i → A j , A k , . . . are extracted. These rules represent relationships such as:</p><formula xml:id="formula_0">"IF f (t) is A i , THEN f (t + 1) is A j , A k , etc.".</formula><p>The FTS model, therefore, is composed of both the linguistic variable Ã and the extracted set of fuzzy rules. To perform forecasting, the model follows these steps: a) Input Fuzzification: the current value y(t) is converted into its fuzzy representation f (t) using the previously defined membership functions. b) Rule Activation: the subset of rules R is identified whose antecedents contain the fuzzy set corresponding to f (t).</p><p>The activation degree µ r of each rule r ∈ R is given by the corresponding membership value. c) Defuzzification: the predicted value y(t+1) is calculated based on a weighted average of the central points mp r of the activated rules. The central point of a rule is given by:</p><formula xml:id="formula_1">mp r = i∈consequent c i (1)</formula><p>The final forecast is then obtained by:</p><formula xml:id="formula_2">y(t + 1) = r∈R µ r • mp r r∈R µ r<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Causal discovery in multivariate time series</head><p>In this part the PCMCI method developed by <ref type="bibr" target="#b22">[23]</ref> is discussed. The PCMCI is a causal discovery method that generates a causal graph directly from multivariate time series. It consists of two steps: first, using the PC 1 algorithm to identify the parents P(X j t ) for all variables in the time series</p><formula xml:id="formula_3">X j t ∈ X 1 t , • • • , X N t ,</formula><p>and second, applying the momentary conditional independence (MCI) test to test for indirect links X i t-τ -→ X j t . PC 1 is an algorithm that uses iterative independence tests for discovering Markov sets. For each variable X j t , the initialization of the initial parents P(</p><formula xml:id="formula_4">X j t ) = (X t-1 , X t-2 , • • • , X t-τmax</formula><p>) is performed. First, unconditional independence tests are applied to remove X i t-τ from P(X j t ) if the null hypothesis X i t-τ ⊥ ⊥ X j t is not rejected given a certain significance level α P C . The initial parents are ranked by their absolute test statistic value.</p><p>Figure <ref type="figure" target="#fig_0">1</ref> illustrates PC1 for two variables X 1 and X 3 where the color intensity represents the absolute test statistic value of the dependent variables (darker color indicates higher value). Gray nodes represent independent variables. Then, conditional independence tests X i t-τ ⊥ ⊥ X j t |L are performed, where L represents the strongest parents in P(X j t ) ∖ X i t-τ , and the independent parents are removed from P(X j t ). In this way, PC 1 converges to only a few relevant conditions that include causally linked parents with high probability (dark pink/dark blue) and potentially some false positives (dashed arrows).</p><p>In step (2), the MCI test uses the parents estimated by PC 1 to identify indirect causes. In the example illustrated in Figure <ref type="figure" target="#fig_0">1</ref>, the conditions P(X 3 t ) are sufficient to establish conditional independence and test X 1 t-2 -→ X 3 t . Additionally, lagged parents from P(X 1 t-2 ) are included as additional conditions, and they are responsible for maintaining the false positive rate at an expected level.</p><p>The PCMCI method is based on the assumptions of causal sufficiency 1 , the Causal Markov Condition, and the faithfulness assumption. It also does not assume contemporaneous causal effects and assumes stationarity <ref type="bibr" target="#b22">[23]</ref>. The PCMCI has a polynomial complexity in the number of variables N and τ max . In the worst-case scenario, where the graph is fully connected, the computational complexity of the PC1 condition selection stage for N variables equates to N 3 τ max 2 . The MCI step involves additional tests of N 2 τ max (for τ &gt; 0). Therefore, the total worst-case computational complexity in terms of the number of variables is polynomial and given by N 3 τ 2 max + N 2 τ max .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. CAUSAL GRAPH FUZZY LLM</head><p>This section introduces CGF-LLM, a novel multivariate time series forecasting framework that integrates FTS, causal 1 Implying that all common drivers are among the observed variables. graphs, and LLMs. The method begins by converting numerical time series into linguistic variables through fuzzification. Next, a causal graph is constructed using the PCMCI algorithm <ref type="bibr" target="#b22">[23]</ref> to capture temporal and causal relationships among variables. Specifically, the framework integrates FTS modeling with causal discovery using the PCMCI algorithm, where PCMCI identifies which variables are causally relevant, and fuzzification captures how those variables evolve over time. The resulting fuzzy linguistic descriptions and causal relations are combined into interpretable textual representations, which are provided as input to a pretrained GPT-2 model. The LLM generates a linguistic forecast, which is then defuzzified to obtain the final numerical prediction.</p><p>As illustrated in Figure <ref type="figure">2</ref>, CGF-LLM frames the forecasting task as a MISO (Multiple Input Single Output) system, where the n time series variables are treated as inputs (Y 0 , Y 1 , Y 2 , . . . , Y n ), and the output corresponds to the endogenous variable Y 0 , which is to be predicted at time step t + 1.</p><p>The CGF-LLM methodology comprises four main steps:</p><p>1) Fuzzification: The Universe of Discourse of the n time series is divided into K overlapping intervals, with each fuzzy set f j i defined by its membership function µ f j i and central point c f j i , where i = 1, . . . , K and j = 0, . . . , n. Each time series is then converted into a fuzzy time series F j , where each element f j (t) ∈ F j is a K-dimensional vector representing the degrees of membership of the value y j (t) in the fuzzy sets f j i . 2) Causal Graph Construction: The PCMCI algorithm is used to identify causal relationships among variables, resulting in a causal graph G for the endogenous variable Y 0 . This graph is constructed based on all input variables, considering a maximum lag window of size τ max . 3) Text Generation: Based on the fuzzy series f j and the Fig. <ref type="figure">2</ref>. Schematic overview of the proposed CGF-LLM forecasting model. Y j denotes the input multivariate time series, where j = 0, . . . , n. Y 0 corresponds to the endogenous time series. τmax represents the size of the lag window observed by the PCMCI method. The fuzzification transforms each Y j into a FTS f j using K fuzzy sets. Then, the temporal linguistic patterns are generated using parallel application of fuzzification and PCMCI. For example, the causal relation f 0 1 , f 1 2 , f 1 3 → f 0 2 illustrates that the fuzzy representation f 0 1 (value of Y 0 in fuzzy set 1), f 1 2 and f 1 3 (values of Y 1 in fuzzy sets 2 and 3) influence f 0 2 , as identified by PCMCI. Arrows in the causal graph (G) denote directed causal relationships among fuzzy sets, such that f 0 1 to f 0 2 , f 1 2 to f 0 2 and others.</p><p>causal graph G, temporal linguistic patterns are extracted that reflect the identified dependencies. For example, if G contains the links Y 0 (t -1) → Y 0 (t) and Y 1 (t -1) → Y 0 (t), the resulting pattern will be:</p><formula xml:id="formula_5">F (Y 0 (t - 1)), F (Y 1 (t -1)) → F (Y 0 (t)).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Forecasting:</head><p>The linguistic patterns derived from the FTS and causal graph are used to fine tune the GPT-2 model, originally designed for text generation, for numerical TSF. In more details, pretrained GPT-2 is integrated with additional layers that convert its textual output into a numerical value. This conversion involves a self-attention-based pooling step to aggregate token embeddings into a single representative vector that captures the global context of the sequence. This vector is then processed through a series of linear layers to produce the final output: the numerical forecast of variable Y 0 at time t + 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. COMPUTATIONAL EXPERIMENTS</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Case studies</head><p>The datasets were named according to their respective application domains: economic, energy, Internet of Things (IoT), and climate. Table <ref type="table" target="#tab_0">I</ref> provides a summary of the main details of the datasets. Accordingly, the proposed method is tested on these datasets to predict solar radiation in Brazil, wind power generation, household electricity consumption in Mexico, and Bitcoin prices in the United States.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experimental Methodology</head><p>The experiments were conducted as an ablation study to assess the impact of the proposed technique (CGF-LLM) on the performance of the GPT-2 model in multivariate time series forecasting. To this end, we compared the methods in terms of prediction accuracy and the number of tokens generated. Three configurations were evaluated: (i) the complete proposed method (CGF-LLM), (ii) a variation without the fuzzyfication step (CG-LLM), and (iii) a baseline LLM, in which numerical time series values were simply converted into text.</p><p>Each configuration was trained using two strategies: with freezing, where GPT-2's internal parameters were frozen and only an additional output layer was trained, and no freezing, where both GPT-2 and the additional layers were fine-tuned.</p><p>CGF-LLM was configured with the following hyperparameters: lag window size τ max = 20, α P C = 0.1, number of partitions set to 30, grid-based partitioning method, and triangular membership function. These values were empirically defined, taking into account the computational cost of running the experiments. The CG-LLM and LLM approaches used the same hyperparameters, adjusted according to their respective architectures. In all three approaches, the GPT-2 model was fine-tuned for 20 epochs.</p><p>For the comparative analysis, four datasets from distinct domains and with varying dimensionalities were employed. The experiments were conducted over 10 time windows, each with a size of 0.3 • |D| and an overlap of 30% along the multivariate time series of length |D|. Each window was split into training and test subsets, with 20% of the data reserved for testing.</p><p>Prediction accuracy was assessed using the average Normalized Root Mean Square Error (NRMSE) computed across the ten windows, as defined in Equation (3). Here, y max and y min denote the maximum and minimum values within the test set, respectively.</p><formula xml:id="formula_6">NRMSE = n t=0 (y(t) -ŷ(t)) 2 y max -y min<label>(3)</label></formula><p>It is worth noting that all the experiments are implemented and tested in Python 3, using open source libraries including PyFTS <ref type="bibr" target="#b38">[39]</ref>, Tigramite<ref type="foot" target="#foot_0">foot_0</ref> and Hugging Face <ref type="foot" target="#foot_1">3</ref>To promote transparency and reproducibility of the proposed model, source code and datasets are available at <ref type="url" target="https://github.com">https://github.com</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Results and Discussion</head><p>Table <ref type="table" target="#tab_1">II</ref> summarizes the means and standard deviations of the NRMSE for one-step-ahead forecasts across all datasets using three models: CGF-LLM, CG-LLM, and LLM. Two training strategies are compared: no freezing, where all the parameters of the GPT-2 model are fine-tuned for the forecasting task, and freezing, where only the last two layers (including attention pooling and MLP head) are fine-tuned to reduce the computational demands.</p><p>Across all datasets, the no-freezing strategy consistently achieves lower values of average NRMSE compared to those with freezing, indicating that full fine-tuning of the GPT-2 model improves the forecasting accuracy. For example, in the Energy dataset, CGF-LLM records an NRMSE of 0.066 ± 0.006 without freezing versus 0.153 ± 0.009 with freezing, highlighting a substantial performance gap. This trend holds across the other three datasets as well. The superior performance of the no-freezing strategy likely stems from its potential to fully adjust the model to the dynamic patterns of datasets, though it needs greater computational resources.</p><p>Comparing the models, CGF-LLM outperforms CG-LLM and LLM in all scenarios, with or without freezing. The exception occurs in the CLIMATIC with freezing, where CG-LLM reports the lowest forecasting error in comparison to CGF-LLM and LLM. In the ECONOMIC dataset, performance differences among the models are minimal, with overlapping standard deviations (e.g., 0.094 ± 0.018 for CGF-LLM vs. 0.104 ± 0.018 for LLM without freezing), suggesting no statistically significant distinction.</p><p>Thus, the obtained results underscore the efficacy of the proposed CGF-LLM for MISO time series forecasting, particularly under the no-freezing strategy. CG-LLM ranks second in most cases, while LLM generally performs the least effectively. These findings highlight a trade-off between computational efficiency and forecasting precision, with CGF-LLM offering the most robust solution when resources permit finetuning the full model.</p><p>In addition to the accuracy enhancement, Table <ref type="table" target="#tab_1">III</ref> reveals the computational efficiency of the proposed CGF-LMM method regarding the number of tokens. Although the total text size produced by the combined use of causal graphs and FTS is greater than that of CG-LLM without fuzzy logic, CGF-LLM results in significantly fewer tokens compared to both CG-LLM and standard LLM approaches. The reason is that fuzzy labels tokenize into fewer units with the GPT-2 tokenizer due to their repetitive nature and simpler structure compared to numerical strings. For instance, a label like "f 1 " often results in a single token, whereas "23.5" may split into multiple tokens (e.g., "23" and ".5"). Thus, CGF-LLM substantially reduces the number of tokens. For instance, on the IoT dataset, CGF-LLM produces 36 times fewer tokens than the standard LLM (839,412 vs. 30,699,520), leading to a drastic reduction in both memory usage and computational requirements.</p><p>In summary, CGF-LLM not only minimizes tokenization overhead and computational costs but also delivers the highest prediction accuracy, leveraging FTS and causal graphs to capture essential temporal relationships effectively. This evidence suggests that strategic pre-processing, such as that used by the CGF-LLM method, is critical for optimizing the scalability and cost-effectiveness of LLM-based forecasting methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION</head><p>This research introduces an LLM-based TSF method named CGF-LLM, which combines the concepts of causal graphs, FTS, and LLMs (fine-tuned GPT-2), applied to MISO applications. The key innovation of this technique relies on converting numerical time series into fuzzy causal text as input for the GPT-2 model, achieved through the parallel use of fuzzification and causality. The generated text provides an interpretable representation of the complex dynamics within the original time series. This method was tested across four different datasets, and the obtained results highlight the efficacy of the proposed CGF-LLM in terms of accuracy and computational costs compared to CG-LLM and regular LLM.</p><p>Despite the success of CGF-LLM, the current study utilized predefined values of hyperparameters. Thus, one possible future direction would be to optimize the model's hyperparameters via auto ML or other advanced optimization techniques. Another research avenue is to explore other alternatives such as GPT-3, GPT-4, and others. Also, future work may extend CGF-LLM to support multiple-output forecasting tasks.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Illustration of the PCMCI method.</figDesc><graphic coords="3,319.31,50.53,236.40,199.20" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>ACKNOWLEDGMENT</head><label></label><figDesc>This work has been supported by the Brazilian agencies (i) National Council for Scientific and Technological Development (CNPq), Grant no. 304856/2025-8, "Aprendizado de Máquina Colaborativo e Protec ¸ão à Privacidade"; (ii)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="4,61.85,50.55,488.28,183.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I SUMMARY</head><label>I</label><figDesc>OF DATASETS USED IN THE EXPERIMENTS</figDesc><table><row><cell>Dataset</cell><cell>Description</cell><cell cols="2">Target Variable Samples</cell><cell cols="2">Variables Frequency</cell></row><row><cell cols="2">ECONOMIC Bitcoin Daily Price Index</cell><cell>AVG</cell><cell>2.970</cell><cell>6</cell><cell>Daily</cell></row><row><cell>ENERGY</cell><cell>Wind Power Production</cell><cell>Power</cell><cell>43.800</cell><cell>9</cell><cell>Hourly</cell></row><row><cell>IoT</cell><cell>Household Electricity</cell><cell>active power</cell><cell>100.000</cell><cell>14</cell><cell>Minutely</cell></row><row><cell>CLIMATIC</cell><cell>Solar Radiation</cell><cell>glo avg</cell><cell>35.000</cell><cell>12</cell><cell>Minutely</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II COMPARISON</head><label>II</label><figDesc>OF FUZZY LLM AND LLM MODELS, WITH AND WITHOUT PARAMETER FREEZING, ACROSS DATASETS. ALL RESULTS ARE BASED ON ONE-STEP-AHEAD FORECASTING USING THE NRMSE (NORMALIZED ROOT MEAN SQUARED ERROR) METRIC.</figDesc><table><row><cell></cell><cell>Datasets</cell><cell>CGF-LLM</cell><cell cols="2">No Freezing CG-LLM</cell><cell>LLM</cell><cell>CGF-LLM</cell><cell>With Freezing CG-LLM</cell><cell>LLM</cell></row><row><cell></cell><cell cols="2">ECONOMICS 0.094 ± 0.018</cell><cell cols="2">0.113 ± 0.017</cell><cell>0.104 ± 0.018</cell><cell>0.201 ± 0.032</cell><cell>0.238 ± 0.029</cell><cell>0.222 ± 0.034</cell></row><row><cell></cell><cell>ENERGY</cell><cell>0.066 ± 0.006</cell><cell cols="2">0.083 ± 0.003</cell><cell>0.104 ± 0.004</cell><cell>0.153 ± 0.009</cell><cell>0.212 ± 0.012</cell><cell>0.246 ± 0.004</cell></row><row><cell></cell><cell>IoT</cell><cell>0.027 ± 0.003</cell><cell cols="2">0.030 ± 0.008</cell><cell>0.075 ± 0.005</cell><cell>0.061 ± 0.006</cell><cell>0.077 ± 0.067</cell><cell>0.082 ± 0.004</cell></row><row><cell></cell><cell>CLIMATIC</cell><cell>0.029 ± 0.002</cell><cell cols="2">0.063 ± 0.006</cell><cell>0.077 ± 0.006</cell><cell>0.135 ± 0.015</cell><cell>0.116 ± 0.011</cell><cell>0.125 ± 0.019</cell></row><row><cell></cell><cell cols="2">TABLE III</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">COMPARISON OF TEXT SIZE AND TOKEN METRICS FOR CGF-LLM,</cell><cell></cell></row><row><cell></cell><cell cols="3">CG-LLM, AND LLM ACROSS DATASETS</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Dataset</cell><cell>Metric</cell><cell cols="2">CGF-LLM CG-LLM</cell><cell></cell><cell>LLM</cell><cell></cell></row><row><cell></cell><cell>Total Text Size</cell><cell>43,875</cell><cell>40,643</cell><cell cols="2">1,335,281</cell><cell></cell></row><row><cell></cell><cell>Train Text Size</cell><cell>35,034</cell><cell>32,627</cell><cell cols="2">1,069,999</cell><cell></cell></row><row><cell>ECONOMICS</cell><cell>Test Text Size Total Tokens</cell><cell>8,841 17,340</cell><cell>8,016 32,984</cell><cell cols="2">265,282 846,300</cell><cell></cell></row><row><cell></cell><cell>Train Tokens</cell><cell>13,880</cell><cell>26,410</cell><cell cols="2">677,625</cell><cell></cell></row><row><cell></cell><cell>Test Tokens</cell><cell>3,460</cell><cell>6,574</cell><cell cols="2">168,675</cell><cell></cell></row><row><cell></cell><cell>Total Text Size</cell><cell>1,276,952</cell><cell cols="3">1,190,523 23,194,823</cell><cell></cell></row><row><cell></cell><cell>Train Text Size</cell><cell>1,021,690</cell><cell>952,364</cell><cell cols="2">18,564,729</cell><cell></cell></row><row><cell>ENERGY</cell><cell>Test Text Size Total Tokens</cell><cell>255,262 380,451</cell><cell>238,159 839,680</cell><cell cols="2">4,630,094 13,434,880</cell><cell></cell></row><row><cell></cell><cell>Train Tokens</cell><cell>304,384</cell><cell>671,744</cell><cell cols="2">10,747,904</cell><cell></cell></row><row><cell></cell><cell>Test Tokens</cell><cell>76,067</cell><cell>167,936</cell><cell cols="2">2,686,976</cell><cell></cell></row><row><cell></cell><cell>Total Text Size</cell><cell>3,147,800</cell><cell cols="3">2,064,031 94,793,482</cell><cell></cell></row><row><cell></cell><cell>Train Text Size</cell><cell>2,518,273</cell><cell cols="3">1,651,208 75,825,454</cell><cell></cell></row><row><cell>IoT</cell><cell>Test Text Size Total Tokens</cell><cell>629,527 839,412</cell><cell cols="3">412,823 1,738,840 30,699,520 18,968,028</cell><cell></cell></row><row><cell></cell><cell>Train Tokens</cell><cell>671,552</cell><cell>810,007</cell><cell cols="2">24,559,616</cell><cell></cell></row><row><cell></cell><cell>Test Tokens</cell><cell>167,860</cell><cell>347,768</cell><cell cols="2">6,139,904</cell><cell></cell></row><row><cell></cell><cell>Total Text Size</cell><cell>786,258</cell><cell>660,437</cell><cell cols="2">29,934,389</cell><cell></cell></row><row><cell></cell><cell>Train Text Size</cell><cell>629,000</cell><cell>528,553</cell><cell cols="2">23,949,302</cell><cell></cell></row><row><cell>CLIMATIC</cell><cell>Test Text Size Total Tokens</cell><cell>157,258 461,076</cell><cell>131,884 576,400</cell><cell cols="2">5,985,087 10,731,520</cell><cell></cell></row><row><cell></cell><cell>Train Tokens</cell><cell>368,896</cell><cell>461,120</cell><cell cols="2">8,585,216</cell><cell></cell></row><row><cell></cell><cell>Test Tokens</cell><cell>92,180</cell><cell>115,280</cell><cell cols="2">2,146,304</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>https://jakobrunge.github.io/tigramite/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>https://huggingface.co/docs/transformers/model doc/gpt2</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p><rs type="funder">Coordination for the Improvement of Higher Education Personnel (CAPES)</rs> through the <rs type="funder">Academic Excellence Program (PROEX)</rs>.</p><p>Partially funded by the <rs type="funder">R&amp;D&amp;I agreement between UFMG and Fundep -Research Development Foundation</rs>: "<rs type="programName">Federated Machine Learning Program for Connected Vehicles</rs>".</p><p>The authors acknowledge partial support from <rs type="person">Kunumi and Embrapii</rs>, project <rs type="grantNumber">PDCC-2412.0030</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_X6PHgnJ">
					<orgName type="program" subtype="full">Federated Machine Learning Program for Connected Vehicles</orgName>
				</org>
				<org type="funding" xml:id="_QtNechP">
					<idno type="grant-number">PDCC-2412.0030</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Time-series large language models: A systematic review of state-of-the-art</title>
		<author>
			<persName><forename type="first">S</forename><surname>Abdullahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Usman Danyaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zakari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">Abdul</forename><surname>Aziz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Amila Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdullah</forename><surname>Zawawi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Adamu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="30" to="235" />
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">How can time series analysis benefit from multiple modalities? a survey and outlook</title>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kamarthi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hartvigsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Prakash</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2503.11835" />
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">A comprehensive survey of deep learning for time series forecasting: Architectural diversity and open challenges</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yoon</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2411.05793" />
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Towards time series reasoning with llms</title>
		<author>
			<persName><forename type="first">W</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gardiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">T</forename><surname>Hallgrímsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Ren</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2409.11376" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Tiny time mixers (ttms): Fast pre-trained models for enhanced zero/few-shot forecasting of multivariate time series</title>
		<author>
			<persName><forename type="first">V</forename><surname>Ekambaram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dayama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">H</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Gifford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kalagnanam</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2401.03955" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Empowering time series analysis with large language models: A survey</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Nevmyvaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.03182</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Time-llm: Time series forecasting by reprogramming large language models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.01728</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Tempo: Prompt-based generative pre-trained transformer for time series forecasting</title>
		<author>
			<persName><forename type="first">D</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">O</forename><surname>Arik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.04948</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Promptcast: A new prompt-based learning paradigm for time series forecasting</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">D</forename><surname>Salim</surname></persName>
		</author>
		<idno type="DOI">10.1109/TKDE.2023.3342137</idno>
		<ptr target="https://doi.org/10.1109/TKDE.2023.3342137" />
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. on Knowl. and Data Eng</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="6851" to="6864" />
			<date type="published" when="2024-11">Nov. 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Chronos: Learning the language of time series</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Ansari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Stella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Turkmen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mercado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Shchur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Rangapuram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Arango</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kapoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zschiegner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Maddix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Mahoney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Torkkola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bohlke-Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2403.07815" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Large language models are zero-shot time series forecasters</title>
		<author>
			<persName><forename type="first">N</forename><surname>Gruver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Finzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Wilson</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2310.07820" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Gpt4mts: prompt-based large language model for multimodal time-series forecasting</title>
		<author>
			<persName><forename type="first">F</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v38i21.30383</idno>
		<ptr target="https://doi.org/10.1609/aaai.v38i21.30383" />
	</analytic>
	<monogr>
		<title level="m">ser. AAAI&apos;24/IAAI&apos;24/EAAI&apos;24</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Unitime: A language-empowered unified model for cross-domain time series forecasting</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Diao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hooi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zimmermann</surname></persName>
		</author>
		<idno type="DOI">10.1145/3589334.3645434</idno>
		<ptr target="https://doi.org/10.1145/3589334.3645434" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Web Conference 2024, ser. WWW &apos;24</title>
		<meeting>the ACM Web Conference 2024, ser. WWW &apos;24<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="4095" to="4106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">S2ip-llm: semantic space informed prompt learning with llm for time series forecasting</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Nevmyvaka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 41st International Conference on Machine Learning</title>
		<meeting>the 41st International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note>ser. ICML&apos;24. JMLR.org</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Language models can improve event prediction by few-shot abductive reasoning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="29" to="532" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Temporal data meets llm-explainable financial time series forecasting</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.11025</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Can chatgpt forecast stock price movements? return predictability and large language models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lopez-Lira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2304.07619" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Large language models are few-shot health learners</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mcduff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Kovacs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Galatzer-Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sunshine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-Z</forename><surname>Poh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">Di</forename><surname>Achille</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Patel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.15525</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Leveraging language foundation models for human mobility forecasting</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">P</forename><surname>Voutharoja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">D</forename><surname>Salim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Advances in Geographic Information Systems</title>
		<meeting>the 30th International Conference on Advances in Geographic Information Systems</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Where would i go next? large language models as human mobility predictors</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Cheng</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2308.15197" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Spatialtemporal large language model for traffic prediction</title>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2024 25th IEEE International Conference on Mobile Data Management (MDM)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="31" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A tutorial on fuzzy time series forecasting models: recent advances and challenges</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">O</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Orang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mendes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">G</forename><surname>Guimaraes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Learn Nonlinear Models</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="29" to="50" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Detecting and quantifying causal associations in large nonlinear time series datasets</title>
		<author>
			<persName><forename type="first">J</forename><surname>Runge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nowack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kretschmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Flaxman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sejdinovic</surname></persName>
		</author>
		<idno type="DOI">10.1126/sciadv.aau4996</idno>
		<ptr target="https://www.science.org/doi/10.1126/sciadv.aau4996" />
	</analytic>
	<monogr>
		<title level="j">Science Advances</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4996" to="5023" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>in Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems, ser. NIPS&apos;17</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems, ser. NIPS&apos;17<address><addrLine>Red Hook, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6000" to="6010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Language models are few-shot learners</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Neural Information Processing Systems, ser. NIPS &apos;20</title>
		<meeting>the 34th International Conference on Neural Information Processing Systems, ser. NIPS &apos;20<address><addrLine>Red Hook, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Palm: Scaling language modeling with pathways</title>
		<author>
			<persName><forename type="first">A</forename><surname>Chowdhery</surname></persName>
		</author>
		<ptr target="https://www.jmlr.org/papers/volume24/21-1378/21-1378.pdf" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="240" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Training compute-optimal large language models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hoffmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mensch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">E</forename><surname>Rutherford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">D L</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.15556</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Gpt-4 technical report</title>
		<author>
			<persName><forename type="first">O</forename></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2303.08774" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Lamda: Language models for dialog applications</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2201.08239" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Llama: Open and efficient foundation language models</title>
		<author>
			<persName><forename type="first">T</forename></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2302.13971" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning</title>
		<author>
			<persName><forename type="first">D.-A</forename></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2501.12948" />
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Forecasting enrollments with fuzzy time series-part i</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">S</forename><surname>Chissom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fuzzy sets and systems</title>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Forecasting enrollments with fuzzy time series-part ii</title>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note>Fuzzy sets and systems</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Forecasting enrollments based on fuzzy time series</title>
		<author>
			<persName><forename type="first">S.-M</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fuzzy sets and systems</title>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page" from="311" to="319" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Fuzzy sets</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Zadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information and control</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="338" to="353" />
			<date type="published" when="1965">1965</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">C</forename><surname>Lima E Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A S</forename><surname>Júnior</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Alves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">C P</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">L</forename><surname>Vieira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">De</forename><surname>Oliveira E Lucas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">J</forename><surname>Sadaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">G</forename><surname>Guimarães</surname></persName>
		</author>
		<idno>PYFTS/pyFTS: Stable version 1.7</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
