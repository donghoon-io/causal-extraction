<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Backdoor Defense via Deconfounded Representation Learning</title>
				<funder ref="#_mcD7mFx">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2023-03-13">13 Mar 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zaixi</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">State Key Laboratory of Cognitive Intelligence</orgName>
								<address>
									<settlement>Hefei</settlement>
									<region>Anhui</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qi</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">State Key Laboratory of Cognitive Intelligence</orgName>
								<address>
									<settlement>Hefei</settlement>
									<region>Anhui</region>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhicai</forename><surname>Wang</surname></persName>
							<email>wangzhic@mail.ustc.edu.cn</email>
							<affiliation key="aff3">
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zepu</forename><surname>Lu</surname></persName>
							<email>zplu@mail.ustc.edu.cn</email>
							<affiliation key="aff3">
								<orgName type="institution">University of Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qingyong</forename><surname>Hu</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">University of Science and Technology</orgName>
								<address>
									<country key="HK">Hong Kong</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="laboratory">Anhui Province Key Lab of Big Data Analysis and Application</orgName>
								<orgName type="institution">University of Science and Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Backdoor Defense via Deconfounded Representation Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-03-13">13 Mar 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2303.06818v1[cs.AI]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Deep neural networks (DNNs) are recently shown to be vulnerable to backdoor attacks, where attackers embed hidden backdoors in the DNN model by injecting a few poisoned examples into the training dataset. While extensive efforts have been made to detect and remove backdoors from backdoored DNNs, it is still not clear whether a backdoor-free clean model can be directly obtained from poisoned datasets. In this paper, we first construct a causal graph to model the generation process of poisoned data and find that the backdoor attack acts as the confounder, which brings spurious associations between the input images and target labels, making the model predictions less reliable. Inspired by the causal understanding, we propose the Causality-inspired Backdoor Defense (CBD), to learn deconfounded representations for reliable classification. Specifically, a backdoored model is intentionally trained to capture the confounding effects. The other clean model dedicates to capturing the desired causal effects by minimizing the mutual information with the confounding representations from the backdoored model and employing a sample-wise re-weighting scheme. Extensive experiments on multiple benchmark datasets against 6 state-ofthe-art attacks verify that our proposed defense method is effective in reducing backdoor threats while maintaining high accuracy in predicting benign samples. Further analysis shows that CBD can also resist potential adaptive attacks. The code is available at <ref type="url" target="https://github.com/zaixizhang/CBD">https://github.com/ zaixizhang/CBD</ref>.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recent studies have revealed that deep neural networks (DNNs) are vulnerable to backdoor attacks <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b51">53]</ref>, * Qi Liu is the corresponding author. where attackers inject stealthy backdoors into DNNs by poisoning a few training data. Specifically, backdoor attackers attach the backdoor trigger (i.e., a particular pattern) to some benign training data and change their labels to the attacker-designated target label. The correlations between the trigger pattern and target label will be learned by DNNs during training. In the inference process, the backdoored model behaves normally on benign data while its prediction will be maliciously altered when the backdoor is activated. The risk of backdoor attacks hinders the applications of DNNs to some safety-critical areas such as automatic driving <ref type="bibr" target="#b37">[38]</ref> and healthcare systems <ref type="bibr" target="#b13">[14]</ref>.</p><p>On the contrary, human cognitive systems are known to be immune to input perturbations such as stealthy trigger patterns induced by backdoor attacks <ref type="bibr" target="#b16">[17]</ref>. This is because humans are more sensitive to causal relations than the statistical associations of nuisance factors <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b58">60]</ref>. In contrast, deep learning models that are trained to fit the poisoned datasets can hardly distinguish the causal relations and the statistical associations brought by backdoor attacks. Based on causal reasoning, we can identify causal relation <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b50">52]</ref> and build robust deep learning models <ref type="bibr" target="#b68">[70,</ref><ref type="bibr" target="#b69">71]</ref>. Therefore, it is essential to leverage causal reasoning to analyze and mitigate the threats of backdoor attacks.</p><p>In this paper, we focus on the image classification tasks and aim to train backdoor-free models on poisoned datasets without extra clean data. We first construct a causal graph to model the generation process of backdoor data where nuisance factors (i.e., backdoor trigger patterns) are considered. With the assistance of the causal graph, we find that the backdoor attack acts as the confounder and opens a spurious path between the input image and the predicted label (Figure <ref type="figure" target="#fig_0">1</ref>). If DNNs have learned the correlation of such a spurious path, their predictions will be changed to the target labels when the trigger is attached.</p><p>Motivated by our causal insight, we propose Causalityinspired Backdoor Defense (CBD) to learn deconfounded representations for classification. As the backdoor attack is stealthy and hardly measurable, we cannot directly block the backdoor path by the backdoor adjustment from causal inference <ref type="bibr" target="#b50">[52]</ref>. Inspired by recent advances in disentangled representation learning <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b64">66]</ref>, we instead aim to learn deconfounded representations that only preserve the causality-related information. Specifically in CBD, two DNNs are trained, which focus on the spurious correlations and the causal effects respectively. The first DNN is designed to intentionally capture the backdoor correlations with an early stop strategy. The other clean model is then trained to be independent of the first model in the hidden space by minimizing mutual information. The information bottleneck strategy and sample-wise re-weighting scheme are also employed to help the clean model capture the causal effects while relinquishing the confounding factors. After training, only the clean model is used for downstream classification tasks. In summary, our contributions are:</p><p>• From a causal perspective, we find the backdoor attack acts as the confounder that causes spurious correlations between the input images and the target label.</p><p>• With the causal insight, we propose a Causalityinspired Backdoor Defense (CBD), which learns deconfounded representations to mitigate the threat of poisoning-based backdoor attacks.</p><p>• Extensive experiments with 6 representative backdoor attacks are conducted. The models trained using CBD are of almost the same clean accuracy as they were directly trained on clean data and the average backdoor attack success rates are reduced to around 1%, which verifies the effectiveness of CBD.</p><p>• We explore one potential adaptive attack against CBD, which tries to make the backdoor attack stealthier by adversarial training. Experiments show that CBD is robust and resistant to such an adaptive attack.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Backdoor Attacks</head><p>Backdoor attacks are emerging security threats to deep neural network classifiers. In this paper, we focus on the poisoning-based backdoor attacks, where the attacker can only inject poisoned examples into the training set while cannot modify other training components (e.g., training loss). Note that backdoor attacks could occur in other tasks (e.g., visual object tracking <ref type="bibr" target="#b33">[34]</ref>, graph classification <ref type="bibr" target="#b71">[73]</ref>, federated learning <ref type="bibr" target="#b70">[72]</ref>, and multi-modal contrastive learning <ref type="bibr" target="#b5">[6]</ref>). The attacker may also have extra capabilities such as modifying the training process <ref type="bibr" target="#b40">[41]</ref>. However, these situations are out of the scope of this paper.</p><p>Based on the property of target labels, existing backdoor attacks can be divided into two main categories: dirty-label attacks <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b71">73]</ref> and clean-label attacks <ref type="bibr" target="#b51">[53,</ref><ref type="bibr" target="#b53">55,</ref><ref type="bibr" target="#b61">63,</ref><ref type="bibr" target="#b79">81,</ref><ref type="bibr" target="#b80">82]</ref>. Dirty-label attack is the most common backdoor attack paradigm, where the poisoned samples are generated by stamping the backdoor trigger onto the original images and altering the labels to the target label. BadNets <ref type="bibr" target="#b17">[18]</ref> firstly employed a black-white checkerboard as the trigger pattern. Furthermore, more complex and stealthier trigger patterns are proposed such as blending backgrounds <ref type="bibr" target="#b9">[10]</ref>, natural reflections <ref type="bibr" target="#b38">[39]</ref>, invisible noise <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b34">35]</ref> and sample-wise dynamic patterns <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b47">48]</ref>. On the other hand, Cleanlabel backdoor attacks are arguably stealthier as they do not change the labels. For example, Turner et al. <ref type="bibr" target="#b61">[63]</ref> leveraged deep generative models to modify benign images from the target class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Backdoor Defenses</head><p>Based on the target and the working stage, existing defenses can be divided into the following categories: (1) Detection based defenses aim to detect anomalies in input data <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b56">58,</ref><ref type="bibr" target="#b60">62,</ref><ref type="bibr" target="#b65">67]</ref> or whether a given model is backdoored <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b54">56,</ref><ref type="bibr" target="#b63">65]</ref>. For instance, Du et al. <ref type="bibr" target="#b12">[13]</ref> applies differential privacy to improve the utility of backdoor detection. (2) Model reconstruction based defenses aim to remove backdoors from a given poisoned model. For example, Mode Connectivity Repair (MCR) <ref type="bibr" target="#b78">[80]</ref> mitigates the backdoors by selecting a robust model in the path of loss landscape, while Neural Attention Distillation (NAD) <ref type="bibr" target="#b32">[33]</ref> leverages attention distillation to remove triggers. (3) Poison suppression based defenses <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b31">32]</ref> reduce the effectiveness of poisoned examples at the training stage and try to learn a clean model from a poisoned dataset. For instance, Decoupling-based backdoor defense (DBD) <ref type="bibr" target="#b24">[25]</ref> decouples the training of DNN backbone and fully-connected layers to reduce the correlation between triggers and target labels. Anti-Backdoor Learning (ABL) <ref type="bibr" target="#b31">[32]</ref> uses gradient ascent to unlearn the backdoored model with the isolated backdoor data.</p><p>In this paper, our proposed CBD is most related to the Poison suppression based defenses. Our goal is to train clean models directly on poisoned datasets without access to clean datasets or further altering the trained model. Different with ABL <ref type="bibr" target="#b31">[32]</ref>, CBD directly trains clean models on poisoned datasets without further finetuning the trained model (unlearning backdoor). In contrast with DBD, CBD does not require additional self-supervised pretraining stages and is much more efficient. In Sec. 5, extensive experimental results clearly show the advantages of CBD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Causal Inference</head><p>Causal inference has a long history in statistical research <ref type="bibr" target="#b49">[50]</ref><ref type="bibr">[51]</ref><ref type="bibr" target="#b50">[52]</ref>. The objective of causal inference is to analyze the causal effects among variables and mitigate spurious correlations. Recently, causal inference has also shown promising results in various areas of machine learning <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b57">59,</ref><ref type="bibr" target="#b66">68,</ref><ref type="bibr" target="#b68">70,</ref><ref type="bibr" target="#b69">71]</ref>. However, to date, causal inference has not been incorporated into the analysis and defense of backdoor attacks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Preliminaries</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Formulation</head><p>In this section, we first formulate the problem of poison suppression based defense, then provide a causal view on backdoor attacks and introduce our proposed Causalityinspired Backdoor Defense. Here, we focus on image classification tasks with deep neural networks. Threat Model. We follow the attack settings in previous works <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b31">32]</ref>. Specifically, we assume a set of backdoor examples has been pre-generated by the attacker and has been successfully injected into the training dataset. We also assume the defender has complete control over the training process but is ignorant of the distribution or the proportion of the backdoor examples in a given dataset. The defender's goal is to train a backdoor-free model on the poisoned dataset, which is as good as models trained on purely clean data. Robust learning strategies developed under such a threat model could benefit research institutes, companies, or government agencies that have the computational resources to train their models but rely on outsourced training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">A Causal View on Backdoor Attacks</head><p>Humans' ability to perform causal reasoning is arguably one of the most important characteristics that distinguish human learning from deep learning <ref type="bibr" target="#b52">[54,</ref><ref type="bibr" target="#b68">70]</ref>. The superiority of causal reasoning endows humans with the abil-ity to recognize causal relationships while ignoring nonessential factors in tasks. On the contrary, DNNs usually fail to distinguish causal relations and statistical associations and tend to learn "easier" correlations than the desired knowledge <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b45">46]</ref>. Such a shortcut solution could lead to overfitting to nuisance factors (e.g., trigger patterns), which would further result in the vulnerability to backdoor attacks. Therefore, here we leverage causal inference to analyze DNN model training and mitigate the risks of backdoor injection.</p><p>We first construct a causal graph as causal graphs are the keys to formalize causal inference. One approach is to use causal structure learning to infer causal graphs <ref type="bibr" target="#b49">[50]</ref>, but it is challenging to apply this kind of approach to highdimensional data like images. Here, following previous works <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b68">70,</ref><ref type="bibr" target="#b69">71]</ref>, we leverage domain knowledge (Figure <ref type="figure" target="#fig_0">1 (a)</ref>) to construct a causal graph G (Figure <ref type="figure" target="#fig_0">1 (b)</ref>) to model the generation process of poisoned data.</p><p>In the causal graph, we denote the abstract data variables by the nodes (X as the input image, Y as the label, and B as the backdoor attack), and the directed links represent their relationships. As shown in Figure <ref type="figure" target="#fig_0">1(b)</ref>, besides the causal effect of X on Y (X → Y ), the backdoor attacker can attach trigger patterns to images (B → X) and change the labels to the targeted label (B → Y ). Therefore, as a confounder between X and Y , backdoor attack B opens the spurious path X ← B → Y (let B = 1 denotes the images are poisoned and B = 0 denotes the images are clean). By "spurious", we mean that the path lies outside the direct causal path from X to Y , making X and Y spuriously correlated and yielding an erroneous effect when the trigger is activated. DNNs can hardly distinguish between the spurious correlations and causative relations <ref type="bibr">[51]</ref>. Hence, directly training DNNs on potentially poisoned dataset incurs the risk of being backdoored.</p><p>To pursue the causal effect of X on Y , previous works usually perform the backdoor adjustment in the causal intervention [51] with do-calculus: P (Y |do(X)) = B∈{0,1} P (Y |X, B)P (B). However, since the confounder variable B is hardly detectable and measurable in our setting, we can not simply use the backdoor adjustment to block the backdoor path. Instead, since the goal of most deep learning models is to learn accurate embedding representations for downstream tasks <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b64">66]</ref>, we aim to disentangle the confouding effects and causal effects in the hidden space. The following section illustrates our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Causality-inspired Backdoor Defense</head><p>Motivated by our causal insight, we propose the Causality-inspired Backdoor Defense (CBD). In practice, it may be difficult to directly identify the confounding and causal factors of X in the data space. We make an assumption that the confounding and causal factors will be reflected in the hidden representations. Based on the assumption, we illustrate our main idea in Figure <ref type="figure" target="#fig_1">2</ref>. Generally, two DNNs including f B and f C are trained, which focus on the spurious correlations and the causal relations respectively. We take the embedding vectors from the penultimate layers of f B and f C as R and Z. Note that R is introduced to avoid confusion with B. Without confusion, we use uppercase letters for variables and lowercase letters for concrete values in this paper. To generate high-quality variable Z that captures the causal relations, we get inspiration from disentangled representation learning <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b64">66]</ref> </p><formula xml:id="formula_0">l Graph r Adjustment (b) Model Framework of CBD X R Y Z f B f C L adv L wce l 2 norm</formula><formula xml:id="formula_1">L C = min βI(Z; X) 1 -I(Z; Y ) 2 + I(Z; R) 3 ,<label>(1)</label></formula><p>where I(•; •) denotes the mutual information. Term 1 and 2 constitute the information bottleneck loss <ref type="bibr" target="#b59">[61]</ref> that encourages the variable Z to capture the core information for label prediction ( 2 ) while constraining unrelated informa-tion from inputs ( 1 ). β is a weight hyper-parameter. Term 3 is a de-confounder penalty term, which describes the dependency degree between the backdoored embedding R and the deconfounded embedding Z. It encourages Z to be independent of R by minimizing mutual information so as to focus on causal effects. However, L C in Equation 1 is not directly tractable, especially for the de-confounder penalty term. In practice, we relax Equation 1 and optimize the upper bound of the term 1 &amp; 2 and the estimation of the term 3 . The details are shown below. Term 1 . Based on the definition of mutual information and basics of probability, I(Z; X) can be calculated as:</p><formula xml:id="formula_2">I(Z; X) = x z p(z, x)log p(z, x) p(z)p(x) = x z p(z|x)p(x)log p(z|x)p(x) p(z)p(x) = x z p(z|x)p(x)log p(z|x) - z p(z)log p(z).</formula><p>(2) However, the marginal probability p(z) = x p(z|x)p(x) is usually difficult to calculate in practice. We use variational approximation to address this issue, i.e., we use a variational distribution q(z) to approximate p(z). According to Gibbs' inequality <ref type="bibr" target="#b41">[42]</ref>, we know that the KL divergence is non-negative:</p><formula xml:id="formula_3">D KL (p(z)||q(z)) ≥ 0 ⇒ -z p(z)log p(z) ≤ -z p(z)log q(z)</formula><p>. By substitute such inequality into Equation <ref type="formula">2</ref>, we can derive an upper bound of I(Z; X):</p><formula xml:id="formula_4">I(Z; X) ≤ x p(x) z p(z|x)log p(z|x) - z p(z)log q(z) = x p(x) z p(z|x)log p(z|x) q(z) = x p(x)D KL (p(z|x)||q(z)).</formula><p>(3) Following previous work <ref type="bibr" target="#b0">[1]</ref>, we assume that the posterior p(z|x) = N (µ(x), diag{σ 2 (x)}) is a gaussian distribution, where µ(x) is the encoded embedding of variable x and diag{σ 2 (x)} = {σ 2 d } D d=1 is the diagonal matrix indicating the variance. On the other hand, the prior q(z) is assumed to be a standard Gaussian distribution, i.e., q(z) = N (0, I). Finally, we can rewrite the above upper bound as:</p><formula xml:id="formula_5">D KL (p(z|x)||q(z)) = 1 2 ||µ(x)|| 2 2 + 1 2 d (σ 2 d -logσ 2 d -1).</formula><p>(4) The detailed derivation is shown in Appendix C. For ease of optimization, we fix σ(x) to be an all-zero matrix. Then z = µ(x) becomes a deterministic embedding. The optimization of this upper bound is equivalent to directly applying the l 2norm regularization on the embedding vector z. (</p><formula xml:id="formula_6">)<label>5</label></formula><p>In experiments, H(Y |Z) can be calculated and optimized as the cross entropy loss (CE). To further encourage the independence between f C and f B , we fix the parameters of f B and train f C with the samples-wise weighted cross entropy loss (L wce ). The weight is calculated as:</p><formula xml:id="formula_7">w(x) = CE(f B (x), y) CE(f B (x), y) + CE(f C (x), y) . (<label>6</label></formula><formula xml:id="formula_8">)</formula><p>For samples with large losses on f B , w(x) are close to 1; while w(x) are close to 0 when the losses are very small. The intuition of the re-weighting scheme is to let f C focus on "hard" examples for f B to encourage independence. Term 3 . Based on the relationship between mutual information and Kullback-Leibler (KL) divergence <ref type="bibr" target="#b3">[4]</ref>, the term I(Z; R) is equivalent to the KL divergence between the joint distribution p(Z, R) and the product of two marginals p(Z)p(X) as:</p><formula xml:id="formula_9">I(Z; R) = D KL (p(Z, R)||p(Z)p(R)).</formula><p>Therefore, to minimize the de-confounder penalty term I(Z; R), we propose to use an adversarial process that minimizes the distance between the joint distribution p(Z, R) and the marginals p(Z)p(R). During the adversarial process, a discriminator D ϕ is trained to classify the sampled representations drawn from the joint p(Z, R) as the real, i.e., 1 and samples drawn from the marginals p(Z)p(R) as the fake, i.e., 0. The samples from the marginal distribution p(Z)p(R) are obtained by shuffling the individual representations of samples (z, r) in a training batch from p(Z, R).</p><p>On the other hand, the clean model f C tries to generate z that look like drawn from p(Z)p(R) when combined with r from f B . Specifically, we optimize such adversarial objective similar to WGAN <ref type="bibr" target="#b1">[2]</ref> with spectral normalization <ref type="bibr" target="#b44">[45]</ref> since it is more stable in the learning process:</p><formula xml:id="formula_10">L adv = min θ C max ϕ E p(z,r) [D ϕ (z, r)] -E p(z)p(r) [D ϕ (z, r)],<label>(7)</label></formula><p>where θ C and ϕ denote the parameters of f C and D ϕ respectively. To sum up, the training objective for f C is:</p><formula xml:id="formula_11">L C = L wce + L adv + β||µ(x)|| 2 2 . (<label>8</label></formula><formula xml:id="formula_12">)</formula><p>The overall objective can then be minimized using SGD. f B and f C are trained for T 1 and T 2 epochs respectively. The pseudo algorithm of CBD is shown in Algorithm 1. Further Discussions. Admittedly, it is challenging to disentangle causal factors and confounding factors thoroughly. This is because f B may still capture some causal relations. Train f C with loss function in Equation <ref type="formula" target="#formula_11">8</ref>9: end for Moreover, encouraging independence between Z and R may result in loss of predictive information for f C . However, with the well-designed optimization objectives and training scheme, CBD manages to reduce the confounding effects as much as possible while preserving causal relations. The following section shows the detailed verification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Experimental Settings</head><p>Datasets and DNNs. We evaluate all defenses on three classical benchmark datasets, CIFAR-10 <ref type="bibr" target="#b27">[28]</ref>, GTSRB <ref type="bibr" target="#b55">[57]</ref> and an ImageNet subset <ref type="bibr" target="#b10">[11]</ref>. As for model architectures, we adopt WideResNet (WRN-16-1) <ref type="bibr" target="#b67">[69]</ref> for CIFAR-10 and GTSRB and ResNet-34 <ref type="bibr" target="#b21">[22]</ref> for ImageNet subset. Note that our CBD is agnostic to the model architectures. Results with more models are shown in Appendix B. Attack Baselines. We consider 6 representative backdoor attacks. Specifically, we select BadNets <ref type="bibr" target="#b17">[18]</ref>, Trojan attack <ref type="bibr" target="#b37">[38]</ref>, Blend attack <ref type="bibr" target="#b9">[10]</ref>, Sinusoidal signal attack (SIG) <ref type="bibr" target="#b2">[3]</ref>, Dynamic attack <ref type="bibr" target="#b46">[47]</ref>, and WaNet <ref type="bibr" target="#b47">[48]</ref>. Bad-Nets, Trojan attack are patch-based visible dirty-label attacks; Blend is an invisible dirty-label attack; SIG belongs to clean-label attacks; Dynamic and WaNet are dynamic dirty-label attacks. More types of backdoor attacks are explored in Appendix B. The results when there is no attack and the dataset is completely clean is shown for reference. Defense Baselines. We compare our CBD with 5 stateof-the-art defense methods: Fine-pruning (FP) <ref type="bibr" target="#b36">[37]</ref>, Mode Connectivity Repair (MCR) <ref type="bibr" target="#b78">[80]</ref>, Neural Attention Distillation (NAD) <ref type="bibr" target="#b32">[33]</ref>, Anti-Backdoor Learning (ABL) <ref type="bibr" target="#b31">[32]</ref>, and Decoupling-based backdoor defense (DBD) <ref type="bibr" target="#b24">[25]</ref>. We also provide results of DNNs trained without any defense methods i.e., No Defense. Attack Setups. We trained DNNs on poisoned datasets for 100 epochs using Stochastic Gradient Descent (SGD) with an initial learning rate of 0.1 on CIFAR-10 and the Ima-geNet subset (0.01 on GTSRB), a weight decay of 0.0001, and a momentum of 0.9. The target labels of backdoor at- Overall, the lower the ASR and the higher the BA, the better the defense.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Effectiveness of CBD</head><p>Comparison to Existing Defenses. Table <ref type="table" target="#tab_0">1</ref> demonstrates the proposed CBD method on CIFAR-10, GTSRB, and Im-ageNet Subset. We consider 6 representative backdoor attacks and compare the performance of CBD with four other backdoor defense techniques. We omit some attacks on Im-ageNet dataset due to a failure of reproduction following their original papers. We can observe that CBD achieves the lowest average ASR across all three datasets. Specifically, the average ASRs are reduced to around 1% (1.60% for CIFAR-10, 1.82% for GTSRB, and 0.91% for ImageNet).</p><p>On the other hand, the CAs of CBD are maintained and are close to training DNNs on clean datasets without attacks. We argue that baseline methods which try to fine-prune or unlearn backdoors of backdoored models are sub-optimal and less efficient. For example, the ABL tries to unlearn backdoor after model being backdoored; DBD requires additional a self-supervised pre-training stage, which introduces around 4 times overhead <ref type="bibr" target="#b24">[25]</ref>. On the contrary, CBD directly trains a backdoor-free model, which achieves high clean accuracy while keeping efficiency.</p><p>When comparing the performance of CBD against different backdoor attacks, we find WaNet achieves higher ASR than most attacks consistently (4.24% for CIFAR-10, 3.41% for GTSRB). This may be explained by the fact that WaNet as one of the state-of-the-art backdoor attacks adopts image warping as triggers that are stealthier than patch-based backdoor attacks <ref type="bibr" target="#b47">[48]</ref>. Hence, the spurious correlations between backdoor triggers and the target label are more difficult to capture for f B . Then in the second step, f C struggles to distinguish the causal and confounding effects. We also notice that CBD is not the best when defending SIG on GT-SRB and ImageNet Subset. We guess the reason is similar to WaNet discussed above. SIG produces poisoned samples by mingling trigger patterns with the background. Moreover, SIG belongs to clean-label attacks, which are stealthier than dirty-label attacks <ref type="bibr" target="#b2">[3]</ref>. This is one limitation of our CBD to be improved in the future. Effectiveness with Different Poisoning Rate. In Table <ref type="table" target="#tab_1">2</ref>, we demonstrate that our CBD is robust and can achieve satisfactory defense performance with a poisoning rate ranging from 1% to 50%. Note that the results when poisoning rate equals 0% have been shown in Table <ref type="table" target="#tab_0">1</ref> (None). Here, we did experiments on CIFAR-10 against 4 attacks including BadNets, Trojan, Blend, and WaNet. Generally, with a higher poisoning rate, CBD has lower CA and higher ASR. We can find that even with a poisoning rate of up to 50%, our CBD method can still reduce the ASR from Visualization of the Hidden Space. In Figure <ref type="figure" target="#fig_6">3</ref>, we show the t-SNE <ref type="bibr" target="#b62">[64]</ref> visualizations of the embeddings to give more insights of our proposed method. We conduct the BadNets attack on CIFAR-10. First, in Figure <ref type="figure" target="#fig_6">3</ref>   <ref type="table" target="#tab_2">3</ref>. We also report the time costs of training vanilla DNNs for reference. The extra computational cost is around 10% -20% of the standard training time on CIFAR-10 and the ImageNet subset. This again shows the advantages of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Resistance to Potential Adaptive Attacks</head><p>While not our initial intention, our work may be used to help develop more advanced backdoor attacks. Here, we tentatively discuss the potential adaptive attacks on CBD. Typically, backdoor attacks are designed to be injected successfully in a few epochs even only a small portion of data is poisoned (e.g., less than 1%). Hence, the confounding bias of backdoor can be well captured by f B and R. The intuition of our adaptive attack strategy is to slow the in- </p><formula xml:id="formula_13">L(f θ (x), y) + x∈D ′ max δi L(f θ (x + δ i ), y) ,<label>(9)</label></formula><p>where the noise δ i is bounded by ∥δ i ∥ p ≤ ϵ with ∥ • ∥ p denoting the L p norm, and ϵ is set to be small such that the poisoned samples cannot be filtered by visual inspection. After optimization, the poisoned examples attached with the optimized noises δ i are injected to the training dataset. We adopt the first-order optimization method PGD <ref type="bibr" target="#b42">[43]</ref> to solve the constrained inner maximization problem:</p><formula xml:id="formula_14">x t+1 = Π ϵ (x t + α • ∇ x L(f θ (x t ), y)), (<label>10</label></formula><formula xml:id="formula_15">)</formula><p>where t is the current perturbation step (M steps in total), ∇ x L(f θ (x t ), y) is the gradient of the loss with respect to the input, Π ϵ is a projection function that clips the noise back to the ϵ-ball around the original example x when it goes beyond, and α is the step size. Pseudo codes of the adaptive attack are shown in Appendix B.</p><p>Experimental Settings. We adopt the CIFAR-10 dataset and WRN-16-1 to conduct the experiments. According to previous studies in adversarial attacks, small L ∞ -bounded noise within ∥δ∥ ∞ &lt;8/255 on images are unnoticeable to human observers. Therefore, we consider the same constraint in our experiments. We use the SGD to solve the above optimization problem for 10 epochs with the step size α of 0.002 and M = 5 perturbation steps.</p><p>Results. With BadNets, the adaptive attack works well when there is no defense (CA=84.55%, ASR=99.62%). However, this attack still fails to attack our CBD (CA=84.19%, ASR=4.31%). More detailed results are shown in Appendix B. We can conclude that our defense is resistant to this adaptive attack. The most probable reason is that the optimized noise becomes less effective when the model is retrained and the model parameters are randomly initialized. In another word, the optimized perturbations are not transferable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>Inspired by the causal perspective, we proposed Causality-inspired Backdoor Defense (CBD) to learn deconfounded representations for reliable classification. Extensive experiments against 6 state-of-the-art backdoor attacks show the effectiveness and robustness of CBD. Further analysis shows that CBD is robust against potential adaptive attacks. Future works include extending CBD to other domains such graph learning <ref type="bibr" target="#b72">[74]</ref><ref type="bibr" target="#b73">[75]</ref><ref type="bibr" target="#b74">[76]</ref><ref type="bibr" target="#b75">[77]</ref>, federated learning <ref type="bibr" target="#b4">[5]</ref>, and self-supervised pertaining <ref type="bibr" target="#b76">[78,</ref><ref type="bibr" target="#b77">79]</ref>. In summary, our work opens up an interesting research direction to leverage causal inference to analyze and mitigate backdoor attacks in machine learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. More Experimental Settings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Datasets and Classifiers</head><p>The datasets and DNN models used in our experiments are summarized in Table <ref type="table" target="#tab_4">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Details of Baseline Implementations</head><p>We implemented the baselines including FP 1 , MCR 2 , NAD 3 ABL 4 , and DBD 5 with their open-sourced codes. For Fine-pruning (FP), we pruned the last convolutional layer of the model. For model connectivity repair (MCR), we trained the loss curve for 100 epochs using the backdoored model as an endpoint and evaluated the defense performance of the model on the loss curve. As for the Neural Attention Distillation (NAD), we finetuned the backdoored student network for 10 epochs with 5% of clean data. The distillation parameter for CIFAR-10 was set to be identical to the value given in the original paper. We cautiously selected the value of distillation parameter for GTSRB and ImageNet to achieve the best trade-off between ASR and CA. For ABL, we unlearned the backdoored model using the L GGA loss with 1% isolated backdoor examples and a learning rate of 0.0001. For our DBD, we adopt SimCLR as the self-supervised method and MixMatch as the semisupervised method. The filtering rate is set to 50% as suggested by the original paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3. Details of CBD Implementations</head><p>In CBD, f C is trained on poisoned datasets for 100 epochs using Stochastic Gradient Descent (SGD) with an initial learning rate of 0.1 on CIFAR-10 and the ImageNet subset (0.01 on GTSRB), a weight decay of 0.0001, and a momentum of 0.9. The learning rate is divided by 10 at the 20th and the 70th epoch. D ϕ is set as a MLP with 2 layers. The dimensions of the embedding r and z are set as 64.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. More Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Results of Adaptive Attacks</head><p>The pseudo codes of adaptive attacks against CBD are shown in Algorithm 2. The results of adaptive attacks with different kinds of backdoor are shown in Table <ref type="table" target="#tab_5">5</ref>. We also show the curves of training losses on clean/backdoor examples in the optimization of added noise along with the vanilla training for reference. In Figure <ref type="figure" target="#fig_9">4</ref> </p><formula xml:id="formula_16">2: for t = 1, • • • , T do 3: Draw a mini-batch B = {(x (i) , y (i) )} n i=1 from D ∪ D ′ 4: θ ← θ -η∇ θ (x,y)∈B L(f θ (x), y) 5: for (x i , y i ) in D ′ do 6: for m = 1, • • • , M do 7: x i ← Π ϵ (x i + α • ∇ x L(f θ (x i ), y i )) 8:</formula><p>end for  <ref type="table" target="#tab_5">5</ref>). The reason is most probably that the optimized noise becomes less effective when the model is retrained and the model parameters are randomly initialized. In another word, the optimized noise is not transferable.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Results with Different Model Architectures</head><p>Note that our CBD is agnostic to the choice of model architectures. In the main text, we report the results with WideResNet-16-1 and ResNet-34. Here, in Table <ref type="table" target="#tab_6">6</ref> and 7, we show experimental results on CIFAR-10 with WideResNet-40-1 <ref type="bibr" target="#b67">[69]</ref> and th T2T-ViT <ref type="bibr" target="#b39">[40]</ref> under poisoning rate 10%. We can observe that CBD can still greatly   Table. 8 shows the total computational time of defense methods against BadNets. As the methods belong to different categories, we count the time to train backdoored models for FP, MCR, and NAD for a fair comparison. Generally, the time cost of CBD is acceptable. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Details of Derivations</head><p>Here we show the details of derivation with respect to Equ. 4. Since the p(z|x) = N (µ(x), diag{σ 2 (x)}) and p(x) = N (0, 1) are multivariate Gaussian distributions with independent components, we only need to derive the case with univariate Gaussian distributions. For the univariate case, we have:</p><formula xml:id="formula_17">D KL (N (µ, σ 2 )||N (0, 1)) = 1 √ 2πσ 2 e -(x-µ) 2 /2σ 2 log e -(x-µ) 2 /2σ 2 / √ 2πσ 2 e -x 2 /2 / √ 2π dx = 1 √ 2πσ 2 e -(x-µ) 2 /2σ 2 log 1 σ exp 1 2 [x 2 -(x -µ) 2 /σ 2 ] dx = 1 2 1 √ 2πσ 2 e -(x-µ) 2 /2σ 2 [-logσ 2 + x 2 -(x -µ) 2 /σ 2 ]dx = 1 2 (-logσ 2 + µ 2 + σ 2 -1).<label>(11)</label></formula><p>The final equation of Equ. 11 holds because -logσ 2 is a constant; the term x 2 is the second order moment of the Gaussian distribution and equals to µ 2 +σ 2 after integration; the (x -µ) 2 in the third term calculates the variance and equals to σ 2 after integration (-σ 2 σ 2 = -1). For the results of multivariate Gaussian distributions, we have: </p><p>Therefore, Equ. 4 in the main text has been proved.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. (a) A real example of the backdoor attack. The backdoored DNN classifies the "turtle" image with a trigger pattern as the target label "dog". (b) The causal graph represents the causalities among variables: X as the input image, Y as the label, and B as the backdoor attack. Besides the causal effect of X on Y (X → Y ), the backdoor attack can attach trigger patterns to images (B → X), and change the labels to the targeted label (B → Y ). Therefore, as a confounder, the backdoor attack B opens a spurious path between X and Y (X ← B → Y ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. The model framework of CBD that includes an adversaral loss L adv for mutual information minimization, a l2-norm regularization on z, and a weighted cross entropy loss Lwce to augment causal effects.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>. In the training phase, f B is firstly trained on the poisoned dataset to capture spurious correlations of backdoor. The other clean model f C is then trained to encourage independence in the hidden space i.e., Z ⊥ R with mutual information minimization and sample re-weighting. After training, only f C is used for downstream classification tasks. In the rest of this section, we provide details on each step of CBD. Training a backdoored model f B . Firstly, f B is trained on the poisoned dataset with cross entropy loss to capture the spurious correlations of backdoor. Since the poisoned data still contains causal relations, we intentionally strengthen the confounding bias in f B with an early stop strategy. Specifically, we train f B only for several epochs (e.g., 5 epochs) and freeze its parameters in the training of f C . This is because previous works indicate that backdoor associations are easier to learn than causal relations [32]. Experiments in Appendix B also verify that the losses on backdoor examples reach nearly 0 while f B has not converged on clean samples after 5 epochs. Training a clean model f C . Inspired by previous works [19, 26], we formulate the training objective of f C with information bottleneck and mutual information minimization:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Term 2 .</head><label>2</label><figDesc>With the definition of mutual information, we have I(Z; Y ) = H(Y ) -H(Y |Z), where H(•) and H(•|•) denote the entropy and conditional entropy respectively. Since H(Y ) is a positive constant and can be ignored, we have the following inequality, -I(Z; Y ) ≤ H(Y |Z).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Algorithm 1 1 : 3 :</head><label>113</label><figDesc>Causality-inspried Backdoor Defense (CBD) Input: β, number of training iterations T 1 , T 2 Output: Clean model f C ; Initialize f C , f B , and D ϕ 2: for t = 1, • • • , T 1 do Train f B on the poisoned dataset with SGD 4: end for 5: for t = 1, • • • , T 2 do</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>(a)&amp;(b), we show the embeddings of r and z when CBD is just initialized and when the training of CBD is completed. We observe that there is a clear separation between the confounding component r and the causal component z after training. Moreover, in Figure3(c)&amp;(d), we use t-SNE separately on r and z and mark samples with different labels with different colors. Interestingly, we find the embeddings of the poisoned samples form clusters in r, which indicates that the spurious correlation between backdoor trigger and the target label has been learned. In contrast, poisoned samples lie closely to samples with their ground-truth label in deconfounded embeddings z, which demonstrates CBD can effectively defend backdoor attacks. Computational Complexity. Compared with the vanilla SGD to train a DNN model, CBD only requires additionally training a backdoored model f B for a few epochs (e.g., 5 epochs) and a discriminator D ϕ , which introduces minimal extra overhead. Here, we report the training time cost of CBD on CIFAR-10 and the ImageNet subset in Table</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Visualization of the hidden space with t-SNE</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>end for our adaptive attack strategy managed to increase the losses of backdoor examples in the optimization process (second line). The above observation indicates that the backdoor examples are much easier to learn than clean examples in vanilla training. The adaptive attack can slow the injection of backdoor and try to make the backdoor attack stealthier to bypass CBD. However, our CBD can still defend the adaptive attack successfully (Table</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. The curve of training losses on clean/backdoor examples in the vanilla training (first line) and in the optimization of adaptive attacks (second line). This experiment is conducted with WideResNet-16-1 for CIFAR-10 under poisoning rate 10%.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>D</head><label></label><figDesc>KL (p(z|x)||q(z)) = D KL (N (µ(x), diag{σ 2 (x)})||N (0, 1))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>The attack success rate (ASR %) and the clean accuracy (CA %) of 5 backdoor defense methods against 6 representative backdoor attacks. None means the training data is completely clean. The best results are bolded. Trojan 100 84.<ref type="bibr" target="#b52">54</ref> 68.95 81.03 19.47 77.12 19.96 80.05 3.88 87.29 3.79 87.29 1.24 87.52 Blend 100 84.56 87.14 81.57 36.15 78.24 10.65 83.71 14.60 85.02 5.12 86.83 1.96 87.48 SIG 99.32 84.14 73.87 81.04 2.34 77.93 1.79 83.54 0.36 88.10 0.44 87.52 0.25 87.29 Dynamic 100 85.48 89.22 80.63 25.26 75.03 25.60 74.94 17.26 85.29 10.21 85.42 0.86 85.67 WaNet 98.55 86.77 73.12 81.58 28.59 77.12 24.15 79.50 22.24 75.74 5.86 84.60 4.24 86.55 Average 99.65 85.14 82.03 81.38 19.39 77.52 14.20 80.67 10.25 84.62 4.53 86.43 1.60 87.00 BadNets 100 96.58 99.48 88.57 1.27 93.30 0.31 89.90 0.05 96.01 0.24 96.05 0.16 96.21 Trojan 99.95 96.49 97.40 88.51 4.62 92.99 0.56 90.32 0.47 94.91 0.56 94.69 0.12 95.29 Blend 100 95.57 98.78 87.50 6.85 93.11 13.06 89.20 22.97 93.25 6.36 93.72 0.90 94.16 SIG 98.24 96.55 85.04 89.97 26.80 91.14 5.35 89.27 5.09 96.28 4.70 94.58 5.41 94.37 Dynamic 100 96.87 98.33 88.09 59.54 90.51 62.35 84.30 6.32 95.76 5.16 95.86 0.96 96.02 WaNet 99.92 95.94 97.93 90.13 55.25 91.24 34.16 83.09 5.56 93.83 3.47 94.71 3.13 95.64 Average 99.69 96.33 96.16 88.80 25.72 92.05 19.30 87.68 7.96 95.01 3.42 94.94 1.82 95.17 BadNets 100 85.24 98.03 82.76 25.14 77.90 7.38 82.11 1.02 87.47 1.27 87.61 0.66 88.12 Trojan 100 85.65 97.29 81.46 6.65 77.06 13.80 81.49 1.68 88.21 1.48 88.20 0.72 88.24 Blend 99.89 86.10 99.10 81.37 18.37 76.21 25.05 82.54 20.80 85.23 4.73 86.25 1.82 87.95 SIG 98.53 86.06 77.39 82.55 24.62 78.97 5.30 83.24 0.22 86.65 1.95 87.09 0.45 87.27 Average 99.61 85.74 92.95 82.04 18.70 77.54 12.88 82.35 5.93 86.89 2.36 87.29 0.91 87.90</figDesc><table><row><cell>Dataset</cell><cell>Types</cell><cell>No Defense ASR CA ASR CA ASR CA ASR CA ASR CA ASR CA ASR CA FP MCR NAD ABL DBD CBD (Ours)</cell></row><row><cell></cell><cell>None</cell><cell>0 89.14 0 85.17 0 87.55 0 88.21 0 88.49 0 88.63 0 88.95</cell></row><row><cell></cell><cell cols="2">BadNets 100 85.37 99.96 82.41 4.52 79.66 3.07 82.25 3.13 86.30 1.76 86.94 1.06 87.46</cell></row><row><cell>CIFAR-10</cell><cell></cell><cell></cell></row><row><cell></cell><cell>None</cell><cell>0 97.74 0 90.18 0 95.27 0 95.29 0 96.47 0 96.45 0 96.54</cell></row><row><cell>GTSRB</cell><cell></cell><cell></cell></row><row><cell></cell><cell>None</cell><cell>0 88.95 0 83.05 0 85.61 0 87.34 0 88.12 0 88.30 0 88.57</cell></row><row><cell>ImageNet</cell><cell></cell><cell></cell></row><row><cell>Subset</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Robustness test with the poisoning rate from 1% to 50% for 4 attacks including BadNets, Trojan, Blend, and WaNet on CIFAR10 dataset. We show ASR (%) and CA (%). Pr (x,y)∈Dtest {f (x) = y}, where y t is the target label and B(•) is the adversarial generator to add triggers into images.</figDesc><table><row><cell>Poisoning Rate</cell><cell>Defense</cell><cell cols="2">BadNets ASR CA ASR CA ASR CA ASR CA Trojan Blend WaNet</cell></row><row><cell>1%</cell><cell cols="3">None 100 85.67 100 85.15 100 85.22 97.56 86.55 CBD 0.62 88.83 1.13 88.56 0.67 87.52 1.06 86.59</cell></row><row><cell>5%</cell><cell cols="3">None 100 84.68 100 84.82 100 85.06 99.83 86.27 CBD 0.93 87.50 1.10 88.45 0.73 87.47 1.07 86.56</cell></row><row><cell>20%</cell><cell cols="3">None 100 83.42 100 79.32 100 82.08 100 74.41 CBD 1.16 84.35 1.57 81.71 5.17 86.53 5.72 74.25</cell></row><row><cell>50%</cell><cell cols="3">None 100 79.45 100 72.83 100 69.67 100 67.25 CBD 1.47 78.88 2.31 75.34 8.14 85.56 8.75 70.43</cell></row><row><cell cols="3">tacks are set to 0 for CIFAR-10 and ImageNet, and 1 for</cell><cell>5-fold cross-validation on the training set according to the</cell></row><row><cell cols="3">GTSRB. The default poisoning rate is set to 10%.</cell><cell>average classification accuracy on hold-out sets. T 2 is set</cell></row><row><cell cols="3">Defense Setups. For FP, MCR, NAD, ABL, and DBD, we</cell><cell>to 100 in the default setting. All experiments were run</cell></row><row><cell cols="3">follow the settings specified in their original papers, includ-</cell><cell>one NVIDIA Tesla V100 GPU. More details of settings are</cell></row><row><cell cols="3">ing the available clean data. Three data augmentation tech-</cell><cell>shown in Appendix A.</cell></row><row><cell cols="3">niques suggested in [32] including random crop, horizontal</cell><cell>Metrics. We adopt two commonly used performance met-</cell></row></table><note><p><p><p>flipping, and cutout, are applied for all defense methods. The hyper-parameter T 1 and β are searched in {3, 5, 8} and {1e -5 , 1e -4 , 1e -3 } respectively. Following the suggestion of the previous work</p><ref type="bibr" target="#b78">[80]</ref></p>, we choose hyperparameters with rics for the evaluation all methods: attack success rate (ASR) and clean accuracy (CA). Let D test denotes the benign testing set and f indicates the trained classifier, we have ASR ≜ Pr (x,y)∈Dtest {f (B(x)) = y t |y ̸ = y t } and CA ≜</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>The average training time (seconds) on CIFAR10 and the ImageNet subset with no defense and CBD. The percentages in parentheses indicate the relative increase of training time.</figDesc><table><row><cell>Dataset</cell><cell cols="2">CIFAR-10 No Defense</cell><cell>CBD</cell><cell cols="2">ImageNet subset No Defense CBD</cell></row><row><cell>BadNets</cell><cell>1152</cell><cell cols="2">1317(14.3%)</cell><cell>2640</cell><cell>2987(13.1%)</cell></row><row><cell>Trojan</cell><cell>1204</cell><cell cols="2">1356(12.6%)</cell><cell>2621</cell><cell>2933(11.9%)</cell></row><row><cell>Blend</cell><cell>1159</cell><cell cols="2">1311(13.1%)</cell><cell>2623</cell><cell>3076(17.3%)</cell></row><row><cell>WaNet</cell><cell>1164</cell><cell cols="2">1293(11.1%)</cell><cell>2647</cell><cell>3074(16.1%)</cell></row><row><cell cols="6">100% to 1.47%, 2.31%, 8.14%, and 8.75% for BadNets,</cell></row><row><cell cols="6">Trojan, Blend, and WaNet, respectively. Moreover, CBD</cell></row><row><cell cols="6">helps backdoored DNNs recover clean accuracies. For in-</cell></row><row><cell cols="6">stance, the CA of CBD for Blend and WaNet improves from</cell></row><row><cell cols="6">69.67% and 67.25% to 85.56% and 70.43% respectively at</cell></row><row><cell cols="2">50% poisoning rate.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>, we can observe that the training losses of backdoor examples reaches almost zero after several epochs of training (first line) while 1 https://github.com/kangliucn/Fine-pruning-defense 2 https://github.com/IBM/model-sanitization 3 https://github.com/bboylyg/NAD 4 https://github.com/bboylyg/ABL 5 https://github.com/SCLBD/DBD Algorithm 2 Adaptive Attack to CBD Input: Model f θ , poisoned dataset D ′ , clean dataset D, perturbation range ϵ, number of training iterations T , step size α, update steps M . Output: optimized poisoned dataset D ′ 1: Initialize f θ</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Details of datasets and classifiers in the paper</figDesc><table><row><cell>Dataset</cell><cell>Labels</cell><cell>Input Size</cell><cell>Training Images</cell><cell>Classifier</cell></row><row><cell>CIFAR-10</cell><cell>10</cell><cell>32 x 32 x 3</cell><cell>50000</cell><cell>WideResNet-16-1</cell></row><row><cell>GTSRB</cell><cell>43</cell><cell>32 x 32 x 3</cell><cell>39252</cell><cell>WideResNet-16-1</cell></row><row><cell>ImageNet subset</cell><cell>12</cell><cell>224 x 224 x 3</cell><cell>12406</cell><cell>ResNet-34</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Attack success rate (ASR %) and clean accuracy (CA %) of Adaptive Attacks. CA ASR CA ASR CA None 99.62 84.55 99.85 84.32 97.63 84.45 97.24 85.47 CBD 4.31 84.19 3.77 84.37 2.57 84.49 5.19 85.33 reduce the attack success rate and keep clean accuracy with different model architectures.</figDesc><table><row><cell>Defense</cell><cell>BadNets ASR CA ASR Trojan</cell><cell>Blend</cell><cell>WaNet</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 .</head><label>6</label><figDesc>Results on CIFAR10 with WideResNet-40-1. We show attack success rates (ASR %) and clean accuracy (CA %). CA ASR CA ASR CA None 100 92.96 100 93.21 99.83 92.69 98.35 92.88 CBD 0.95 92.54 1.04 92.70 1.32 92.17 2.54 92.26</figDesc><table><row><cell>Defense</cell><cell>BadNets ASR CA ASR Trojan</cell><cell>Blend</cell><cell>WaNet</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 .</head><label>7</label><figDesc>Results on CIFAR10 with T2T-ViT. We show the attack success rates (ASR %) and the clean accuracy (CA %). None 100 85.<ref type="bibr" target="#b63">65</ref> 100 85.86 99.42 85.66 99.30 86.27 CBD 0.89 86.05 0.97 86.61 1.59 85.82 3.80 85.94 B.3. The computational time of other defenses.</figDesc><table><row><cell>Defense</cell><cell>BadNets ASR CA ASR CA ASR CA ASR CA Trojan Blend WaNet</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 .</head><label>8</label><figDesc>The total computational time (seconds) on CIFAR10 with WRN-16-1. The percentages in parentheses indicate the relative increase compared to no defence (None).</figDesc><table><row><cell>None</cell><cell>FP</cell><cell>MCR</cell><cell>NAD</cell><cell>ABL</cell><cell>DBD</cell><cell>CBD (ours)</cell></row><row><cell cols="7">1152 1515(31.5%) 3445(127.4%) 1306(13.4%) 1383(20.0%) 5280(358.3%) 1317(14.3%)</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This research was partially supported by a grant from the <rs type="funder">National Natural Science Foundation of China</rs> (Grant No. <rs type="grantNumber">61922073</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_mcD7mFx">
					<idno type="grant-number">61922073</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep variational information bottleneck</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Alexander A Alemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">V</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Dillon</surname></persName>
		</author>
		<author>
			<persName><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Wasserstein generative adversarial networks</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="214" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A new backdoor attack in cnns by training set corruption without label poisoning</title>
		<author>
			<persName><forename type="first">Mauro</forename><surname>Barni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kassem</forename><surname>Kallas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benedetta</forename><surname>Tondi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Sai Rajeshwar, Sherjil Ozair, Yoshua Bengio, Aaron Courville, and Devon Hjelm. Mutual information neural estimation</title>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Ishmael Belghazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aristide</forename><surname>Baratin</surname></persName>
		</author>
		<idno>PMLR, 2018. 5</idno>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<biblScope unit="page" from="531" to="540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Flcert: Provably secure federated learning against poisoning attacks</title>
		<author>
			<persName><forename type="first">Xiaoyu</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zaixi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinyuan</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Zhenqiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gong</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3691" to="3705" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Poisoning and backdooring contrastive learning</title>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Terzis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Detecting backdoor attacks on deep neural networks by activation clustering</title>
		<author>
			<persName><forename type="first">Bryant</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wilka</forename><surname>Carvalho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathalie</forename><surname>Baracaldo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heiko</forename><surname>Ludwig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taesung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Molloy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Biplav</forename><surname>Srivastava</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Workshop</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deepinspect: A black-box trojan detection and mitigation framework for deep neural networks</title>
		<author>
			<persName><forename type="first">Huili</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jishen</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Farinaz</forename><surname>Koushanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In IJCAI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Invisible poisoning: Highly stealthy targeted poisoning attack</title>
		<author>
			<persName><forename type="first">Jinyin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haibin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengmeng</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changting</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shouling</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICISC</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Targeted backdoor attacks on deep learning systems using data poisoning</title>
		<author>
			<persName><forename type="first">Xinyun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kimberly</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.05526</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Februus: Input purification defense against trojan attacks on deep neural network systems</title>
		<author>
			<persName><forename type="first">Bao</forename><surname>Gia Doan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ehsan</forename><surname>Abbasnejad</surname></persName>
		</author>
		<author>
			<persName><surname>Damith C Ranasinghe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACSAC</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Robust anomaly detection and backdoor attack detection via differential privacy</title>
		<author>
			<persName><forename type="first">Min</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruoxi</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Yu</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benteng</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shanshan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2112.01148,2021.1</idno>
		<title level="m">Fiba: Frequency-injection based backdoor attack in medical image analysis</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Strip: A defence against trojan attacks on deep neural networks</title>
		<author>
			<persName><forename type="first">Yansong</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Change</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiping</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Surya</forename><surname>Damith C Ranasinghe</surname></persName>
		</author>
		<author>
			<persName><surname>Nepal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACSAC</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Shortcut learning in deep neural networks</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>Geirhos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jörn-Henrik</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claudio</forename><surname>Michaelis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wieland</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><forename type="middle">A</forename><surname>Wichmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">A theory of causal learning in children: causal maps and bayes nets. Psychological review</title>
		<author>
			<persName><forename type="first">Alison</forename><surname>Gopnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clark</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">M</forename><surname>Sobel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><forename type="middle">E</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tamar</forename><surname>Kushnir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Danks</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Badnets: Identifying vulnerabilities in the machine learning model supply chain</title>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brendan</forename><surname>Dolan-Gavitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddharth</forename><surname>Garg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.06733</idno>
		<imprint>
			<date type="published" when="2005">2017. 1, 2, 5</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning disentangled representation for cross-modal retrieval with deep mutual information estimation</title>
		<author>
			<persName><forename type="first">Weikuo</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huaibo</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangwei</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ran</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Multimedia</title>
		<meeting>the 27th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1712" to="1720" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Rare event detection using disentangled representation learning</title>
		<author>
			<persName><forename type="first">Ryuhei</forename><surname>Hamaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ken</forename><surname>Sakurada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryosuke</forename><surname>Nakamura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Spectre: Defending against backdoor attacks using robust statistics</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Hayase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raghav</forename><surname>Somani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sewoong</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Unlearnable examples: Making personal data unexploitable</title>
		<author>
			<persName><forename type="first">Hanxun</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingjun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><forename type="middle">Monazam</forename><surname>Erfani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yisen</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deconfounded visual grounding</title>
		<author>
			<persName><forename type="first">Jianqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxin</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qianru</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Backdoor defense via decoupling the training process</title>
		<author>
			<persName><forename type="first">Kunzhe</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baoyuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhan</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kui</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2007">2022. 2, 3, 5, 7</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Disenqnet: Disentangled representation learning for educational questions</title>
		<author>
			<persName><forename type="first">Zhenya</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enhong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianhui</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Universal litmus patterns: Revealing backdoor attacks in cnns</title>
		<author>
			<persName><forename type="first">Soheil</forename><surname>Kolouri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aniruddha</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamed</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heiko</forename><surname>Hoffmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Building machines that learn and think like people</title>
		<author>
			<persName><surname>Brenden M Lake</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tomer D Ullman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><forename type="middle">J</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><surname>Gershman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Behavioral and brain sciences</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Invisible backdoor attacks on deep neural networks via steganography and regularization</title>
		<author>
			<persName><forename type="first">Shaofeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minhui</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Zi Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haojin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinpeng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.02742</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Invisible backdoor attack with samplespecific triggers</title>
		<author>
			<persName><forename type="first">Yuezun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baoyuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Longkang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ran</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siwei</forename><surname>Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="16463" to="16472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Anti-backdoor learning: Training clean models on poisoned data</title>
		<author>
			<persName><forename type="first">Yige</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xixiang</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nodens</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingjuan</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingjun</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<date type="published" when="2006">2021. 2, 3, 4, 5, 6</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Neural attention distillation: Erasing backdoor triggers from deep neural networks</title>
		<author>
			<persName><forename type="first">Yige</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xixiang</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nodens</forename><surname>Koren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingjuan</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingjun</forename><surname>Ma</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In ICLR, 2021. 2, 5</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Few-shot backdoor attacks on visual object tracking</title>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoxiang</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingjun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu-Tao</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Backdoor embedding in convolutional neural network models via invisible perturbation</title>
		<author>
			<persName><forename type="first">Cong</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoti</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anna</forename><surname>Squicciarini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sencun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Miller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>CO-DASPY</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Mitigating confounding bias in recommendation via information bottleneck</title>
		<author>
			<persName><forename type="first">Dugang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengxiang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenhua</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiuqiang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weike</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhong</forename><surname>Ming</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">RecSys</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="351" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Finepruning: Defending against backdooring attacks on deep neural networks</title>
		<author>
			<persName><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brendan</forename><surname>Dolan-Gavitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddharth</forename><surname>Garg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In RAID</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Trojaning attack on neural networks</title>
		<author>
			<persName><forename type="first">Yingqi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiqing</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yousra</forename><surname>Aafer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Chuan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NDSS</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Reflection backdoor: A natural backdoor attack on deep neural networks</title>
		<author>
			<persName><forename type="first">Yunfei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingjun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Efficient training of visual transformers with small datasets</title>
		<author>
			<persName><forename type="first">Yahui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enver</forename><surname>Sangineto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruno</forename><surname>Lepri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Nadai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">12</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Latent backdoor attacks on deep neural networks</title>
		<author>
			<persName><forename type="first">Xiaofeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorenzo</forename><surname>Cavallaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Kinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Katz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGSAC</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Information theory, inference and learning algorithms</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David Jc Mac</forename><surname>Mackay</surname></persName>
		</author>
		<author>
			<persName><surname>Kay</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Towards deep learning models resistant to adversarial attacks</title>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Vladu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Representation learning via invariant causal mechanisms</title>
		<author>
			<persName><forename type="first">Jovana</forename><surname>Mitrovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Buesing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<author>
			<persName><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toshiki</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuichi</forename><surname>Yoshida</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.05957</idno>
		<title level="m">Spectral normalization for generative adversarial networks</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning from failure: De-biasing classifier from biased classifier</title>
		<author>
			<persName><forename type="first">Junhyun</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyuntak</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sungsoo</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaeho</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinwoo</forename><surname>Shin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Input-aware dynamic backdoor attack</title>
		<author>
			<persName><forename type="first">Anh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anh</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2005">2020. 2, 5</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Wanet-imperceptible warpingbased backdoor attack</title>
		<author>
			<persName><forename type="first">Anh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anh</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2007">2021. 2, 5, 7</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Counterfactual vqa: A causeeffect look at language bias</title>
		<author>
			<persName><forename type="first">Yulei</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaihua</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiwu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian-Sheng</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">The book of why: the new science of cause and effect</title>
		<author>
			<persName><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dana</forename><surname>Mackenzie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009. 2018</date>
			<publisher>Basic Books</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Elements of causal inference: foundations and learning algorithms</title>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>The MIT Press</publisher>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Akshayvarun Subramanya, and Hamed Pirsiavash. Hidden trigger backdoor attacks</title>
		<author>
			<persName><forename type="first">Aniruddha</forename><surname>Saha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<title level="m" type="main">Toward causal representation learning. Proceedings of the IEEE</title>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Locatello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><forename type="middle">Rosemary</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Poison frogs! targeted clean-label poisoning attacks on neural networks</title>
		<author>
			<persName><forename type="first">Ali</forename><surname>Shafahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronny</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mahyar</forename><surname>Najibi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Octavian</forename><surname>Suciu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Studer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tudor</forename><surname>Dumitras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Backdoor scanning for deep neural networks through k-arm optimization</title>
		<author>
			<persName><forename type="first">Guangyu</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingqi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guanhong</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengwei</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiuling</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyuan</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiqing</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Man vs. computer: Benchmarking machine learning algorithms for traffic sign recognition. Neural networks</title>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Stallkamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Schlipsing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Salmen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Igel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="323" to="332" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Demon in the variant: Statistical analysis of dnns for robust backdoor contamination detection</title>
		<author>
			<persName><forename type="first">Di</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaofeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haixu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kehuan</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">USENIX Security</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Longtailed classification by keeping the good and removing the bad momentum causal effect</title>
		<author>
			<persName><forename type="first">Kaihua</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">How to grow a mind: Statistics, structure, and abstraction. science</title>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Kemp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">L</forename><surname>Griffiths</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">D</forename><surname>Goodman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">331</biblScope>
			<biblScope unit="page" from="1279" to="1285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<author>
			<persName><forename type="first">Naftali</forename><surname>Tishby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando</forename><forename type="middle">C</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Bialek</surname></persName>
		</author>
		<idno>arXiv preprint physics/0004057</idno>
		<title level="m">The information bottleneck method</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Spectral signatures in backdoor attacks</title>
		<author>
			<persName><forename type="first">Brandon</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerry</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<ptr target="https://people.csail.mit.edu/madry/lab/,2019.2" />
		<title level="m">Clean-label backdoor attacks</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">86</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Neural cleanse: Identifying and mitigating backdoor attacks in neural networks</title>
		<author>
			<persName><forename type="first">Bolun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanshun</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shawn</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huiying</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bimal</forename><surname>Viswanath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haitao</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><forename type="middle">Y</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">S&amp;P</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Cross-domain face presentation attack detection via multidomain disentangled representation learning</title>
		<author>
			<persName><forename type="first">Guoqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiguang</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xilin</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Detecting ai trojans using meta neural analysis</title>
		<author>
			<persName><forename type="first">Xiaojun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huichen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Borisov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><forename type="middle">A</forename><surname>Gunter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In S&amp;P</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Qianru Sun, and Xian-Sheng Hua. Interventional few-shot learning</title>
		<author>
			<persName><forename type="first">Zhongqi</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Wide residual networks</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">A causal view on robustness of neural networks</title>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingzhen</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Adversarial robustness through the lens of causality</title>
		<author>
			<persName><forename type="first">Yonggang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinmei</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Fldetector: Defending federated learning against model poisoning attacks via detecting malicious clients</title>
		<author>
			<persName><forename type="first">Zaixi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyu</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinyuan</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Zhenqiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gong</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2545" to="2555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Backdoor attacks to graph neural networks</title>
		<author>
			<persName><forename type="first">Zaixi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinyuan</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Binghui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Zhenqiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gong</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM Symposium on Access Control Models and Technologies</title>
		<meeting>the 26th ACM Symposium on Access Control Models and Technologies</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="15" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Hierarchical graph transformer with adaptive node sampling</title>
		<author>
			<persName><forename type="first">Zaixi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingyong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chee-Kong</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Model inversion attacks against graph neural networks</title>
		<author>
			<persName><forename type="first">Zaixi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenya</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chee-Kong</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enhong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Graphmi: Extracting private graph data from graph neural networks</title>
		<author>
			<persName><forename type="first">Zaixi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenya</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqiang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuanren</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enhong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Protgnn: Towards self-explaining graph neural networks</title>
		<author>
			<persName><forename type="first">Zaixi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqiang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheekong</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="9127" to="9135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Motif-based graph self-supervised learning for molecular property prediction</title>
		<author>
			<persName><forename type="first">Zaixi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqiang</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chee-Kong</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="15870" to="15882" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Graph self-supervised learning for optoelectronic properties of organic semiconductors</title>
		<author>
			<persName><forename type="first">Zaixi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang-Yu</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chee-Kong</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML AI4Science workshop</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<monogr>
		<title level="m" type="main">Bridging mode connectivity in loss landscapes and adversarial robustness</title>
		<author>
			<persName><forename type="first">Pu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pin-Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Payel</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthikeyan</forename><surname>Natesan Ramamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xue</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note>In ICLR, 2020. 2, 5</note>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Clean-label backdoor attacks on video recognition models</title>
		<author>
			<persName><forename type="first">Shihao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingjun</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bailey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Gang</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="14443" to="14452" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Transferable cleanlabel poisoning attacks on deep neural nets</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronny</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hengduo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gavin</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Studer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
