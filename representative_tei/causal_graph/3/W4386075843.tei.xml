<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Context De-confounded Emotion Recognition</title>
				<funder ref="#_TfJbp58 #_mATtHju">
					<orgName type="full">National Key R&amp;D Program of China</orgName>
				</funder>
				<funder ref="#_2qzaSh6">
					<orgName type="full">China Postdoctoral Science Foundation</orgName>
				</funder>
				<funder ref="#_dQtkQ5X">
					<orgName type="full">Shanghai Municipal Science and Technology Major Project</orgName>
				</funder>
				<funder ref="#_6ECVXKb">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2023-03-26">26 Mar 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Dingkang</forename><surname>Yang</surname></persName>
							<email>dkyang20@fudan.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Academy for Engineering and Technology</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Meta-Medical</orgName>
								<address>
									<country>IPASS</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhaoyu</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Academy for Engineering and Technology</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuzheng</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Academy for Engineering and Technology</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shunli</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Academy for Engineering and Technology</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mingcheng</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Academy for Engineering and Technology</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Siao</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Academy for Engineering and Technology</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiao</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Academy for Engineering and Technology</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shuai</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Academy for Engineering and Technology</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhiyan</forename><surname>Dong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Academy for Engineering and Technology</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Peng</forename><surname>Zhai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Academy for Engineering and Technology</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lihua</forename><surname>Zhang</surname></persName>
							<email>lihuazhang@fudan.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Academy for Engineering and Technology</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Institute of Meta-Medical</orgName>
								<address>
									<country>IPASS</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Jilin Provincial Key Laboratory of Intelligence Science and Engineering</orgName>
								<address>
									<settlement>Changchun</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department" key="dep1">Engineering Research Center of AI and Robotics</orgName>
								<orgName type="department" key="dep2">Ministry of Education</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Context De-confounded Emotion Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-03-26">26 Mar 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2303.11921v2[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Context-Aware Emotion Recognition (CAER) is a crucial and challenging task that aims to perceive the emotional states of the target person with contextual information. Recent approaches invariably focus on designing sophisticated architectures or mechanisms to extract seemingly meaningful representations from subjects and contexts. However, a long-overlooked issue is that a context bias in existing datasets leads to a significantly unbalanced distribution of emotional states among different context scenarios. Concretely, the harmful bias is a confounder that misleads existing models to learn spurious correlations based on conventional likelihood estimation, significantly limiting the models' performance. To tackle the issue, this paper provides a causality-based perspective to disentangle the models from the impact of such bias, and formulate the causalities among variables in the CAER task via a tailored causal graph. Then, we propose a Contextual Causal Intervention Module (CCIM) based on the backdoor adjustment to de-confound the confounder and exploit the true causal effect for model training. CCIM is plug-in and model-agnostic, which improves diverse state-of-the-art approaches by considerable margins. Extensive experiments on three benchmark datasets demonstrate the effectiveness of our CCIM and the significance of causal insight.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>As an essential technology for understanding human intentions, emotion recognition has attracted significant attention in various fields such as human-computer interaction <ref type="bibr" target="#b0">[1]</ref>, medical monitoring <ref type="bibr" target="#b27">[28]</ref>, and education <ref type="bibr" target="#b39">[40]</ref>. Previous works have focused on extracting multimodal emotion cues from human subjects, including facial expressions <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b48">49]</ref>, acoustic behaviors <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b51">52]</ref>, and body § Corresponding Author.  postures <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b52">53]</ref>, benefiting from advances in deep learning algorithms <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b53">54,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b58">59]</ref>. Despite the impressive improvements achieved by subjectcentered approaches, their performance is limited by natural and unconstrained environments. Several examples in Figure <ref type="figure" target="#fig_0">1</ref> (left) show typical situations on a visual level. Instead of well-designed visual contents, multimodal representations of subjects in wild-collected images are usually indistinguishable (e.g., ambiguous faces or gestures), which forces us to exploit complementary factors around the subject that potentially reflect emotions.</p><p>Inspired by psychological study <ref type="bibr" target="#b2">[3]</ref>, recent works <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b55">56]</ref> have suggested that contextual information contributes to effective emotion cues for Context-Aware Emotion Recognition (CAER). The contexts are considered to include the place category, the place attributes, the objects, or the actions of others around the subject <ref type="bibr" target="#b19">[20]</ref>. The majority of such research typically follows a common pipeline: <ref type="bibr" target="#b0">(1)</ref> Obtaining the unimodal/multimodal representations of the recognized subject; (2) Building diverse contexts and extracting emotion-related representations; (3) Designing We show a toy experiment on the EMOTIC <ref type="bibr" target="#b19">[20]</ref> and CAER-S <ref type="bibr" target="#b21">[22]</ref> datasets for scene categories of angry and happy emotions. More scene categories with normalized zeroconditional entropy reveal a strong presence of the context bias. fusion strategies to combine these features for emotion label predictions. Although existing methods have improved modestly through complex module stacking <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b50">51]</ref> and tricks <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b28">29]</ref>, they invariably suffer from a context bias of the datasets, which has long been overlooked. Recalling the process of generating CAER datasets, different annotators were asked to label each image according to what they subjectively thought people in the images with diverse contexts were feeling <ref type="bibr" target="#b19">[20]</ref>. This protocol makes the preference of annotators inevitably affect the distribution of emotion categories across contexts, thereby leading to the context bias. Figure <ref type="figure" target="#fig_0">1</ref> illustrates how such bias confounds the predictions. Intrigued, most of the images in training data contain vegetated scenes with positive emotion categories, while negative emotions in similar contexts are almost nonexistent. Therefore, the baseline <ref type="bibr" target="#b18">[19]</ref> is potentially misled into learning the spurious dependencies between contextspecific features and label semantics. When given test images with similar contexts but negative emotion categories, the model inevitably infers the wrong emotional states.</p><p>More intrigued, a toy experiment is performed to verify the strong bias in CAER datasets. This test aims to observe how well emotions correlate with contexts (e.g., scene categories). Specifically, we employ the ResNet-152 <ref type="bibr" target="#b14">[15]</ref> pre-trained on Places365 <ref type="bibr" target="#b57">[58]</ref> to predict scene categories from images with three common emotion categories (i.e., "anger", "happy", and "fear") across two datasets. The top 200 most frequent scenes from each emotion category are selected, and the normalized conditional entropy of each scene category across the positive and negative set of a specific emotion is computed <ref type="bibr" target="#b29">[30]</ref>. While analyzing correlations between scene contexts and emotion categories in Figure 2 (e.g., "anger" and "happy"), we find that more scene categories with the zero conditional entropy are most likely to suggest the significant context bias in the datasets, as it shows the presence of these scenes only in the positive or negative set of emotions. Concretely, for the EMOTIC dataset <ref type="bibr" target="#b19">[20]</ref>, about 40% of scene categories for anger have zero conditional entropy while about 45% of categories for happy (i.e., happiness) have zero conditional entropy. As an intuitive example, most party-related scene contexts are present in the samples with the happy category and almost non-existent in the negative categories. These observations confirm the severe context bias in CAER datasets, leading to distribution gaps in emotion categories across contexts and uneven visual representations.</p><p>Motivated by the above observation, we attempt to embrace causal inference <ref type="bibr" target="#b30">[31]</ref> to reveal the culprit that poisons the CAER models, rather than focusing on beating them. As a revolutionary scientific paradigm that facilitates models toward unbiased prediction, the most important challenge in applying classical causal inference to the modern CAER task is how to reasonably depict true causal effects and identify the task-specific dataset bias. To this end, this paper attempts to address the challenge and rescue the bias-ridden models by drawing on human instincts, i.e., looking for the causality behind any association. Specifically, we present a causality-based bias mitigation strategy. We first formulate the procedure of the CAER task via a proposed causal graph. In this case, the harmful context bias in datasets is essentially an unintended confounder that misleads the models to learn the spurious correlation between similar contexts and specific emotion semantics. From Figure <ref type="figure" target="#fig_2">3</ref>, we disentangle the causalities among the input images X, subject features S, context features C, confounder Z, and predictions Y . Then, we propose a simple yet effective Contextual Causal Intervention Module (CCIM) to achieve context-deconfounded training and use the do-calculus P (Y |do(X)) to calculate the true causal effect, which is fundamentally different from the conventional likelihood P (Y |X). CCIM is plug-in and model-agnostic, with the backdoor adjustment <ref type="bibr" target="#b13">[14]</ref> to de-confound the confounder and eliminate the impact of the context bias. We comprehensively evaluate the effectiveness and superiority of CCIM on three standard and biased CAER datasets. Numerous experiments and analyses demonstrate that CCIM can significantly and consistently improve existing baselines, achieving a new state-of-the-art (SOTA).</p><p>The main contributions can be summarized as follows:</p><p>• To our best knowledge, we are the first to investigate the adverse context bias of the datasets in the CAER task from the causal inference perspective and identify that such bias is a confounder, which misleads the models to learn the spurious correlation.</p><p>• We propose CCIM, a plug-in contextual causal intervention module, which could be inserted into most CAER models to remove the side effect caused by the confounder and facilitate a fair contribution of diverse contexts to emotion understanding.</p><p>• Extensive experiments on three standard CAER datasets show that the proposed CCIM can facilitate existing models to achieve unbiased predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Context-Aware Emotion Recognition. As a promising task, Context-Aware Emotion Recognition (CAER) not only draws on human subject-centered approaches <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b51">52]</ref> to perceive emotion via the face or body, but also considers the emotion cues provided by background contexts in a joint and boosting manner. Existing CAER models invariably extract multiple representations from these two sources and then perform feature fusion to make the final prediction <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b21">[22]</ref><ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b55">56]</ref>. For instance, Kosti et al. <ref type="bibr" target="#b18">[19]</ref> establish the EMOTIC dataset and propose a baseline Convolutional Neural Network (CNN) model that combines the body region and the whole image as the context. Hoang et al. <ref type="bibr" target="#b15">[16]</ref> propose an extra reasoning module to exploit the images, categories, and bounding boxes of adjacent objects in background contexts to achieve visual relationship detection. For a deep exploration of scene context, Li et al. <ref type="bibr" target="#b22">[23]</ref> present a body-object attention module to estimate the contributions of background objects and a body-part attention module to recalibrate the channel-wise body feature responses. Although the aforementioned approaches achieve impressive improvements by exploring diverse contextual information, they all neglect the limitation on model performance caused by the context bias of the datasets. Instead of focusing on beating the latest SOTA, we identify the bias as a harmful confounder from a causal inference perspective and significantly improve the existing models with the proposed CCIM. Causal Inference. Causal inference is an analytical tool that aims to infer the dynamics of events under changing conditions (e.g., different treatments or external interventions) <ref type="bibr" target="#b30">[31]</ref>, which has been extensively studied in economics, statistics, and psychology <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b40">41]</ref>. Without loss of generality, causal inference follows two main ways: structured causal model <ref type="bibr" target="#b31">[32]</ref> and potential outcome framework <ref type="bibr" target="#b37">[38]</ref>, which assist in revealing the causality rather than the superficial association among variables. Benefiting from the great potential of the causal tool to provide unbiased estimation solutions, it has been gradually applied to various computer tasks, such as computer vision <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b44">45]</ref> and natural language processing <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b56">57]</ref>. Inspired by visual commonsense learning <ref type="bibr" target="#b44">[45]</ref>, to our best knowledge, this is the first investigation of the confounding effect through causal inference in the CAER task while exploiting causal intervention to interpret and address the confounding bias from contexts. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Causal View at CAER Task</head><p>Firstly, we formulate a tailored causal graph to summarize the CAER framework. In particular, we follow the same graphical notation in structured causal model <ref type="bibr" target="#b31">[32]</ref> due to its intuitiveness and interpretability. It is a directed acyclic graph G = {N , E} that can be paired with data to produce quantitative causal estimates. The nodes N denote variables and the links E denote direct causal effects. As shown in Figure <ref type="figure" target="#fig_2">3</ref>, there are five variables involved in the CAER causal graph, which are input images X, subject features S, context features C, confounder Z, and predictions Y . Note that our causal graph is applicable to a variety of CAER methods, since it is highly general, imposing no constraints on the detailed implementations. The details of the causal relationships are described below. Z → X. Different subjects are recorded in various contexts to produce images X. On the one hand, the annotators make subjective and biased guesses about subjects' emotional states and give their annotations <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b19">20]</ref>, e.g., subjects are usually blindly assigned positive emotions in vegetation-covered contexts. On the other hand, the data nature leads to an unbalanced representation of emotions in the real world <ref type="bibr" target="#b12">[13]</ref>. That is, it is much easier to collect positive emotions in contexts of comfortable atmospheres than negative ones. The context bias caused by the above situations is treated as the harmful confounder Z to establish spurious connections between similar contexts and specific emotion semantics. For the input images X, Z determines the biased content that is recorded, i.e., Z → X. Z → C → Y . C represents the total context representation obtained by contextual feature extractors. C may come from the aggregation of diverse context features based on different methods. The causal path Z → C represents the detrimental Z confounding the model to learn unreliable emotion-related context semantics of C. In this case, the impure C further affects the predictions Y of the emotion labels and can be reflected via the link C → Y . Although Z potentially provides priors from the training data to better estimation when the subjects' features are ambiguous, it misleads the model to capture spurious "context-emotion" mapping during training, resulting in biased predictions.</p><formula xml:id="formula_0">X → C → Y &amp; X → S → Y .</formula><p>S represents the total subject representation obtained by subject feature extractors. Depending on distinct methods, S may come from the face, the body, or the integration of their features. In CAER causal graph, we can see that the desired effect of X on Y follows from two causal paths: X → C → Y and X → S → Y . These two causal paths reflect that the CAER model estimates Y based on the context features C and subject features S extracted from the input images X. In practice, C and S are usually integrated to make the final prediction jointly, e.g., feature concatenation <ref type="bibr" target="#b28">[29]</ref>.</p><p>According to the causal theory <ref type="bibr" target="#b30">[31]</ref>, the confounder Z is the common cause of the input images X and corresponding predictions Y . The positive effects of context and subject features providing valuable semantics follow the causal paths X → C/S → Y , which we aim to achieve. Unfortunately, the confounder Z causes the negative effect of misleading the model to focus on spurious correlations instead of pure causal relationships. This adverse effect follows the backdoor causal path X ← Z → C → Y .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Causal Intervention via Backdoor Adjustment</head><p>In Figure <ref type="figure" target="#fig_2">3</ref>(a), existing CAER methods rely on the likelihood P (Y |X). This process is formulated by Bayes rule:</p><formula xml:id="formula_1">P (Y |X) = z P (Y |X, S = fs(X), C = fc(X, z))P (z|X),<label>(1)</label></formula><p>where f s (•) and f c (•) are two generalized encoding functions that obtain the total S and C, respectively. The confounder Z introduces the observational bias via P (z|X).</p><p>To address the confounding effect brought by Z and make the model rely on pure X to estimate Y , an intuitive idea is to intervene X and force each context semantics to contribute to the emotion prediction fairly. The process can be viewed as conducting a randomized controlled experiment by collecting images of subjects with any emotion in any context. However, this intervention is impossible due to the infinite number of images that combine various subjects and contexts in the real world. To solve this, we stratify Z based on the backdoor adjustment <ref type="bibr" target="#b30">[31]</ref> to achieve causal intervention P (Y |do(X)) and block the backdoor path between X and Y , where do-calculus is an effective approximation for the imaginative intervention <ref type="bibr" target="#b13">[14]</ref>. Specifically, we seek the effect of stratified contexts and then estimate the average causal effect by computing a weighted average based on the proportion of samples containing different context prototypes in the training data. In Figure <ref type="figure" target="#fig_2">3</ref>(b), the causal path from Z to X is cut-off, and the model will approximate causal intervention P (Y |do(X)) rather than spurious association P (Y |X). By applying the Bayes rule on the new graph, Eq. ( <ref type="formula" target="#formula_1">1</ref>) with the intervention is formulated as: As z is no longer affected by X, the intervention intentionally forces X to incorporate every z fairly into the predictions of Y , subject to the proportion of each z in the whole.</p><formula xml:id="formula_2">P (Y |do(X)) = z P (Y |X, S = fs(X), C = fc(X, z))P (z).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Context-Deconfounded Training with CCIM</head><p>To implement the theoretical and imaginative intervention in Eq. ( <ref type="formula">2</ref>), we propose a Contextual Causal Intervention Module (CCIM) to achieve the context-deconfounded training for the models. From a general pipeline of the CAER task illustrated in Figure <ref type="figure" target="#fig_3">4</ref>(b), CCIM is inserted in a plugin manner after the original integrated feature of existing methods. Then, the output of CCIM performs predictions after passing the final task-specific classifier. The implementation of CCIM is described below. Confounder Dictionary. Since the number of contexts is large in the real world and there is no ground-truth contextual information in the training set, we approximate it as a stratified confounder dictionary Z = [z 1 , z 2 , . . . , z N ], where N is a hyperparameter representing the size, and each </p><formula xml:id="formula_3">z i ∈ R d</formula><formula xml:id="formula_4">M = {m k ∈ R d } Nm k=1</formula><p>, where N m is the number of training samples. To compute context prototypes, we use the K-Means++ with principle component analysis to learn Z so that each z i represents a form of context cluster. Each cluster z i is set to the average feature of each cluster in the K-Means++, i.e., z i = 1 Ni Ni j=1 m i j , where N i is the number of context features in the i-th cluster. Instantiation of the Proposed CCIM. Since the calculation of P (Y |do(X)) requires multiple forward passes of all z, the computational overhead is expensive. To reduce the computational cost, we apply the Normalized Weighted Geometric Mean (NWGM) <ref type="bibr" target="#b47">[48]</ref>   <ref type="table" target="#tab_3">2</ref> and<ref type="table" target="#tab_4">3</ref> follow the same interpretation.</p><p>expectation at the feature level as:</p><formula xml:id="formula_5">P (Y |do(X)) ≈ P (Y |X, S = fs(X), C = z fc(X, z)P (z)).<label>(3)</label></formula><p>Inspired by <ref type="bibr" target="#b44">[45]</ref>, we parameterize a network model to approximate the above conditional probability of Eq. ( <ref type="formula" target="#formula_5">3</ref>) as follows:</p><formula xml:id="formula_6">P (Y |do(X)) = W h h + WgEz[g(z)],<label>(4)</label></formula><p>where W h ∈ R dm×d h and W g ∈ R dm×d are the learnable parameters, and h = ϕ(s, c) ∈ R d h ×1 . ϕ(•) is a fusion strategy (e.g., concatenation) that integrates s and c into the joint representation h. Note that the above approximation is reasonable, because the effect on Y comes from S, C, and the confounder Z. Immediately, we approximate E z [g(z)] as a weighted integration of all context prototypes:</p><formula xml:id="formula_7">Ez[g(z)] = N i=1 λiziP (zi),<label>(5)</label></formula><p>where λ i is a weight coefficient that measures the importance of each z i after interacting with the origin feature h, and P (z i ) = Ni Nm . In practice, we provide two implementations of λ i : dot product attention and additive attention:</p><formula xml:id="formula_8">Dot Product : λi = sof tmax( (Wqh) T (W k zi) √ d ),<label>(6)</label></formula><p>Additive</p><formula xml:id="formula_9">: λi = sof tmax(W T t • T anh(Wqh + W k zi)),<label>(7)</label></formula><p>where W t ∈ R dn×1 , W q ∈ R dn×d h , and W k ∈ R dn×d are mapping matrices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Evaluation Metrics</head><p>Datasets. Our experiments are conducted on three standard datasets for the CAER task, namely EMOTIC <ref type="bibr" target="#b19">[20]</ref>, CAER-S <ref type="bibr" target="#b21">[22]</ref>, and GroupWalk <ref type="bibr" target="#b28">[29]</ref>  Evaluation Metrics. Following <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b28">29]</ref>, we utilize the mean Average Precision (mAP) to evaluate the results on the EMOTIC and GroupWalk. For the CAER-S, the standard classification accuracy is used for evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Model Zoo</head><p>Limited by the fact that most methods are not open source, we select four representative models to evaluate the effectiveness of CCIM, which have different network structures and contextual exploration mechanisms. EMOT-Net <ref type="bibr" target="#b18">[19]</ref>  work (CNN) model with two branches. Its distinct branches capture foreground body features and background contextual information, respectively. GCN-CNN <ref type="bibr" target="#b55">[56]</ref> utilizes different context elements to construct an affective graph and infer the affective relationship according to the Graph Convolutional Network (GCN). CAER-Net <ref type="bibr" target="#b21">[22]</ref> is a twostream CNN model following an adaptive fusion module to reason emotions. The method focuses on the context of the entire image after hiding the face and the emotion cues provided by the facial region. EmotiCon <ref type="bibr" target="#b28">[29]</ref> introduces three context-aware streams. Besides the subject-centered multimodal extraction branch, they propose to use visual attention and depth maps to learn the scene and socio-dynamic contexts separately. For EMOT-Net, we re-implement the model following the available code. Meanwhile, we reproduce the results on the three datasets based on the details reported in the SOTA methods above (i.e., GCN-CNN, CAER-Net, and EmotiCon).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Implementation Details</head><p>Confounder Setup. Firstly, except for the annotated EMOTIC, we utilize the pre-trained Faster R-CNN <ref type="bibr" target="#b35">[36]</ref> to detect the bounding box of the target subject for each training sample on both CAER-S and GroupWalk. After that, the context images are generated by masking the target subjects on the training samples based on the bounding boxes. Then, we use the ResNet-152 <ref type="bibr" target="#b14">[15]</ref> pre-trained on Places365 <ref type="bibr" target="#b57">[58]</ref> dataset to extract the context feature set </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparison with State-of-the-art Methods</head><p>We comprehensively compare the CCIM-based models with recent SOTA methods, including RRLA <ref type="bibr" target="#b22">[23]</ref>, VRD <ref type="bibr" target="#b15">[16]</ref>, SIB-Net <ref type="bibr" target="#b23">[24]</ref>, and GRERN <ref type="bibr" target="#b11">[12]</ref>. The default setting uses the dot product attention of Eq. ( <ref type="formula" target="#formula_8">6</ref>). Results on the EMOTIC Dataset. In Table <ref type="table" target="#tab_1">1</ref>, we observe that CCIM significantly improves existing models and achieves the new SOTA. Specifically, the CCIM-based EMOT-Net, GCN-CNN, CAER-Net and EmotiCon improve the mAP scores by 2.95%, 3.56%, 2.66%, and 3.85%, respectively, outperforming the vanilla methods by large margins. In this case, these CCIM-based methods achieve competitive or better performance than the recent models RRLA and VRD. We also find that CCIM greatly improves the AP scores for some categories heavily persecuted by the confounder. For instance, CCIM helps raise the results of "Anticipation" and "Sympathy" in these CAER methods by 29%∼37% and 14%∼29%, respectively. Due to the adverse bias effect, the performance of most models is usually poor on infrequent categories, such as "Aversion" (AP scores of about 3%∼12%) and "Embarrassment" (AP scores of about 1%∼10%). Thanks to CCIM, the AP scores in these two categories are achieved at about 12%∼19% and 5%∼16%. Results on the GroupWalk Dataset. As shown in Table <ref type="table" target="#tab_3">2</ref>, our CCIM effectively improves the performance of EMOT-Net, GCN-CNN, CAER-Net, and EmotiCon on the Group-Walk dataset. The mAP scores for these models are increased by 2.41%, 2.99%, 2.25%, and 3.73%, respectively. Results on the CAER-S Dataset. The accuracy of different methods on the CAER-S dataset is reported in Table <ref type="table" target="#tab_4">3</ref>. The performance of EMOT-Net, GCN-CNN, and CAER-Net is consistently increased by CCIM, making each context prototype contribute fairly to the emotion classification results. These models are improved by 1.31%, 1.45%, and 1.34%, respectively. Moreover, the CCIM-based EmotiCon We argue that the essence of fine-grained modeling is the potential context stratification within the sample from the perspective of backdoor adjustment. Fortunately, CCIM can better refine this stratification effect and make the models focus on contextual causal intervention across samples to measure the true causal effect. (iii) According to Tables <ref type="table" target="#tab_1">1</ref> and<ref type="table" target="#tab_3">2</ref>, and Figure <ref type="figure">5</ref>, while the causal intervention brings gains for most emotions across datasets, the performance of some categories shows slight improvements or even deteriorations. A reasonable explanation is that the few samples and insignificant confounding effects of these categories result in over-intervention. However, the minor sacrifice is tolerable compared to the overall superiority of our CCIM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Ablation Studies</head><p>We conduct thorough ablation studies in Table <ref type="table" target="#tab_7">4</ref> to evaluate the implementation of the causal intervention. To explore the effectiveness of CCIM when combining methods that model context semantics at different granularities, we choose the baseline EMOT-Net and SOTA EmotiCon. Rationality of Confounder Dictionary Z. We first provide a random dictionary with the same size to replace the tailored confounder dictionary Z, which is initialized by  randomization rather than the average context features. Experimental results <ref type="bibr" target="#b2">(3,</ref><ref type="bibr" target="#b3">4)</ref> show that the random dictionary would significantly hurt the performance, proving the validity of our context prototypes. Moreover, we use the ResNet-152 pre-trained on ImageNet <ref type="bibr" target="#b7">[8]</ref> to replace the default settings (1, 2) for extracting context features. The decreased results <ref type="bibr" target="#b4">(5,</ref><ref type="bibr" target="#b5">6)</ref> suggest that context prototypes based on scene semantics are more conducive to approximating the confounder than those based on object semantics. It is reasonable as scenes usually include objects, e.g., in Figure <ref type="figure" target="#fig_0">1</ref>, "grass" is the child of the confounder "vegetated scenes". Robustness of Pre-trained Backbones. The experiments <ref type="bibr" target="#b6">(7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10)</ref> in Table <ref type="table" target="#tab_7">4</ref> show that the gain from CCIM increases as more advanced pre-trained backbone networks are used, which indicates that our CCIM is not dependent on a well-chosen pre-trained backbone φ(•). Effectiveness of Components of E z [g(z)]. First, we report the results in experiments <ref type="bibr" target="#b10">(11,</ref><ref type="bibr" target="#b11">12)</ref> using the additive attention weight λ i in Eq. <ref type="bibr" target="#b6">(7)</ref>. The competitive performance demonstrates that both attention paradigms are meaningful and usable. Furthermore, we evaluate the effectiveness of the weighted integration by separately removing the weights λ i and prior probabilities P (z i ) in the E z [g(z)].</p><p>The decreased results <ref type="bibr" target="#b12">(13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16)</ref> suggest that depict-</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Illustration of the context bias in the CAER task. GT means the ground truth. Most images contain similar contexts in the training data with positive emotion categories. In this case, the model learns the spurious correlation between specific contexts and emotion categories and gives wrong results. Thanks to CCIM, the simple baseline [19] achieves more accurate predictions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure2. We show a toy experiment on the EMOTIC<ref type="bibr" target="#b19">[20]</ref> and CAER-S<ref type="bibr" target="#b21">[22]</ref> datasets for scene categories of angry and happy emotions. More scene categories with normalized zeroconditional entropy reveal a strong presence of the context bias.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Illustration of our CAER causal graph. (a) The conventional likelihood P (Y |X). (b) The causal intervention P (Y |do(X)).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. (a) The generation process of the confounder dictionary Z. (b) A general pipeline for the context-deconfounded training. The red dotted box shows the core component that achieves the powerful approximation to causal intervention: our CCIM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>represents a context prototype. As shown in Figure 4(a), we first mask the target subject in each training image based on the subject's bounding box to generate the context image set I. Subsequently, the image set I is fed to the pre-trained backbone network φ(•) to obtain the context feature set</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Ablation study results for the size N of the confounder dictionary Z on three datasets. (a), (b), and (c) from the EMOTIC, CAER-S, and GroupWalk datasets, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>phase GT： GT： GT： GT： GT： Training phase Prediction Kosti et al.</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell cols="2">Testing Affection</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Anticipation</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Engagement</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Excitement</cell></row><row><cell>Engagement</cell><cell>Engagement</cell><cell>Disapproval</cell><cell>Pleasure</cell></row><row><cell></cell><cell></cell><cell>Disconnection Disquietment Doubt/Confusion Engagement Sadness</cell><cell>Kosti et al. + CCIM(ours) Disapproval Disconnection Disquietment</cell></row><row><cell>Excitement Happiness Pleasure</cell><cell>Affection Pleasure Happiness</cell><cell>Grass, trees, Similar Context outdoors,etc</cell><cell>Doubt/Confusion Sadness Engagement</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>to approximate the above Average precision (%) of different methods for each emotion category on the EMOTIC dataset.</figDesc><table><row><cell>Category</cell><cell>EMOT-Net [19]</cell><cell>EMOT-Net + CCIM</cell><cell>GCN-CNN [56]</cell><cell>GCN-CNN + CCIM</cell><cell>CAER-Net [22]</cell><cell>CAER-Net + CCIM</cell><cell cols="2">RRLA [23] VRD [16]</cell><cell>EmotiCon [29]</cell><cell>EmotiCon + CCIM</cell></row><row><cell>Affection</cell><cell>26.47</cell><cell>34.87</cell><cell>47.52</cell><cell>36.18</cell><cell>22.36</cell><cell>23.08</cell><cell>37.93</cell><cell>44.48</cell><cell>38.55</cell><cell>40.77</cell></row><row><cell>Anger</cell><cell>11.24</cell><cell>13.05</cell><cell>11.27</cell><cell>12.53</cell><cell>12.88</cell><cell>12.99</cell><cell>13.73</cell><cell>30.71</cell><cell>14.69</cell><cell>15.48</cell></row><row><cell>Annoyance</cell><cell>15.26</cell><cell>18.04</cell><cell>12.33</cell><cell>13.73</cell><cell>14.42</cell><cell>15.28</cell><cell>20.87</cell><cell>26.47</cell><cell>24.68</cell><cell>24.47</cell></row><row><cell>Anticipation</cell><cell>57.31</cell><cell>94.19</cell><cell>63.2</cell><cell>92.32</cell><cell>52.85</cell><cell>90.03</cell><cell>61.08</cell><cell>59.89</cell><cell>60.73</cell><cell>95.15</cell></row><row><cell>Aversion</cell><cell>7.44</cell><cell>13.41</cell><cell>6.81</cell><cell>15.41</cell><cell>3.26</cell><cell>12.96</cell><cell>9.61</cell><cell>12.43</cell><cell>11.33</cell><cell>19.38</cell></row><row><cell>Confidence</cell><cell>80.33</cell><cell>74.9</cell><cell>74.83</cell><cell>75.01</cell><cell>72.68</cell><cell>73.24</cell><cell>80.08</cell><cell>79.24</cell><cell>68.12</cell><cell>75.81</cell></row><row><cell>Disapproval</cell><cell>16.14</cell><cell>19.87</cell><cell>12.64</cell><cell>14.45</cell><cell>15.37</cell><cell>16.38</cell><cell>21.54</cell><cell>24.54</cell><cell>18.55</cell><cell>23.65</cell></row><row><cell>Disconnection</cell><cell>20.64</cell><cell>27.72</cell><cell>23.17</cell><cell>30.52</cell><cell>22.01</cell><cell>23.39</cell><cell>28.32</cell><cell>34.24</cell><cell>28.73</cell><cell>31.93</cell></row><row><cell>Disquietment</cell><cell>19.57</cell><cell>19.12</cell><cell>17.66</cell><cell>20.85</cell><cell>10.84</cell><cell>18.1</cell><cell>22.57</cell><cell>24.23</cell><cell>22.14</cell><cell>26.84</cell></row><row><cell>Doubt/Confusion</cell><cell>31.88</cell><cell>19.35</cell><cell>19.67</cell><cell>20.43</cell><cell>26.07</cell><cell>17.66</cell><cell>33.5</cell><cell>25.42</cell><cell>38.43</cell><cell>34.28</cell></row><row><cell>Embarrassment</cell><cell>3.05</cell><cell>6.23</cell><cell>1.58</cell><cell>9.21</cell><cell>1.88</cell><cell>5.86</cell><cell>4.16</cell><cell>4.26</cell><cell>10.31</cell><cell>16.73</cell></row><row><cell>Engagement</cell><cell>86.69</cell><cell>88.93</cell><cell>87.31</cell><cell>96.88</cell><cell>73.71</cell><cell>70.04</cell><cell>88.12</cell><cell>88.71</cell><cell>86.23</cell><cell>97.41</cell></row><row><cell>Esteem</cell><cell>17.86</cell><cell>21.69</cell><cell>12.05</cell><cell>22.72</cell><cell>15.38</cell><cell>16.67</cell><cell>20.5</cell><cell>17.99</cell><cell>25.75</cell><cell>27.44</cell></row><row><cell>Excitement</cell><cell>78.05</cell><cell>73.81</cell><cell>72.68</cell><cell>73.21</cell><cell>70.42</cell><cell>71.08</cell><cell>80.11</cell><cell>74.21</cell><cell>80.75</cell><cell>81.59</cell></row><row><cell>Fatigue</cell><cell>8.87</cell><cell>9.96</cell><cell>12.93</cell><cell>12.66</cell><cell>6.29</cell><cell>9.73</cell><cell>17.51</cell><cell>22.62</cell><cell>19.35</cell><cell>15.53</cell></row><row><cell>Fear</cell><cell>15.7</cell><cell>9.04</cell><cell>6.15</cell><cell>10.31</cell><cell>7.47</cell><cell>6.61</cell><cell>15.56</cell><cell>13.92</cell><cell>16.99</cell><cell>15.37</cell></row><row><cell>Happiness</cell><cell>58.92</cell><cell>78.09</cell><cell>72.9</cell><cell>75.64</cell><cell>53.73</cell><cell>62.34</cell><cell>76.01</cell><cell>83.02</cell><cell>80.45</cell><cell>83.55</cell></row><row><cell>Pain</cell><cell>9.46</cell><cell>14.71</cell><cell>8.22</cell><cell>15.36</cell><cell>8.16</cell><cell>9.43</cell><cell>14.56</cell><cell>16.68</cell><cell>14.68</cell><cell>17.76</cell></row><row><cell>Peace</cell><cell>22.35</cell><cell>22.79</cell><cell>30.68</cell><cell>23.88</cell><cell>19.55</cell><cell>20.21</cell><cell>26.76</cell><cell>28.91</cell><cell>35.72</cell><cell>38.94</cell></row><row><cell>Pleasure</cell><cell>46.72</cell><cell>46.59</cell><cell>48.37</cell><cell>45.52</cell><cell>34.12</cell><cell>35.37</cell><cell>55.64</cell><cell>55.47</cell><cell>67.31</cell><cell>64.57</cell></row><row><cell>Sadness</cell><cell>18.69</cell><cell>17.47</cell><cell>23.9</cell><cell>22.08</cell><cell>17.75</cell><cell>13.24</cell><cell>30.8</cell><cell>42.87</cell><cell>40.26</cell><cell>45.63</cell></row><row><cell>Sensitivity</cell><cell>9.05</cell><cell>7.91</cell><cell>4.74</cell><cell>8.02</cell><cell>6.94</cell><cell>4.74</cell><cell>9.59</cell><cell>15.89</cell><cell>13.94</cell><cell>17.04</cell></row><row><cell>Suffering</cell><cell>17.67</cell><cell>15.35</cell><cell>23.71</cell><cell>18.45</cell><cell>14.85</cell><cell>11.89</cell><cell>30.7</cell><cell>46.23</cell><cell>48.05</cell><cell>21.52</cell></row><row><cell>Surprise</cell><cell>22.38</cell><cell>13.12</cell><cell>8.44</cell><cell>13.93</cell><cell>17.46</cell><cell>11.7</cell><cell>17.92</cell><cell>16.27</cell><cell>19.6</cell><cell>26.81</cell></row><row><cell>Sympathy</cell><cell>15.23</cell><cell>32.6</cell><cell>19.45</cell><cell>33.95</cell><cell>14.89</cell><cell>28.59</cell><cell>15.26</cell><cell>15.37</cell><cell>16.74</cell><cell>47.6</cell></row><row><cell>Yearning</cell><cell>9.22</cell><cell>10.08</cell><cell>9.86</cell><cell>11.58</cell><cell>4.84</cell><cell>8.61</cell><cell>10.11</cell><cell>10.04</cell><cell>15.08</cell><cell>12.25</cell></row><row><cell>mAP</cell><cell>27.93  †</cell><cell>30.88  † (↑ 2.95 )</cell><cell>28.16  †</cell><cell>31.72  † (↑ 3.56 )</cell><cell>23.85  †</cell><cell>26.51  † (↑ 2.66 )</cell><cell>32.41  *</cell><cell>35.16  *</cell><cell>35.28  †</cell><cell>39.13  † (↑ 3.85 )</cell></row></table><note><p>* : results from the original reports. †: results from implementation. The footnotes * and † of Tables</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 .</head><label>2</label><figDesc>is a baseline Convolutional Neural Net-Average precision (%) of different methods for each emotion category on the GroupWalk dataset.</figDesc><table><row><cell cols="2">Category EMOT-Net [19]</cell><cell cols="2">EMOT-Net + CCIM</cell><cell cols="2">GNN-CNN [56]</cell><cell>GNN-CNN + CCIM</cell><cell>CAER-Net [22]</cell><cell>CAER-Net + CCIM</cell><cell>EmotiCon [29]</cell><cell>EmotiCon + CCIM</cell></row><row><cell>Angry</cell><cell>57.65</cell><cell cols="2">62.41</cell><cell>51.92</cell><cell></cell><cell>54.07</cell><cell>45.18</cell><cell>50.43</cell><cell>68.85</cell><cell>75.93</cell></row><row><cell>Happy</cell><cell>71.32</cell><cell cols="2">75.68</cell><cell>63.37</cell><cell></cell><cell>70.25</cell><cell>56.59</cell><cell>60.71</cell><cell>72.31</cell><cell>79.15</cell></row><row><cell>Neutral</cell><cell>43.1</cell><cell cols="2">41.03</cell><cell>40.26</cell><cell></cell><cell>39.49</cell><cell>39.32</cell><cell>37.84</cell><cell>50.34</cell><cell>48.66</cell></row><row><cell>Sad</cell><cell>61.24</cell><cell cols="2">63.84</cell><cell>58.15</cell><cell></cell><cell>61.85</cell><cell>52.96</cell><cell>54.06</cell><cell>70.8</cell><cell>73.48</cell></row><row><cell>mAP</cell><cell>58.33  †</cell><cell cols="2">60.74  † (↑ 2.41 )</cell><cell cols="2">53.43  †</cell><cell>56.42  † (↑ 2.99 )</cell><cell>48.51  †</cell><cell>50.76  † (↑ 2.25 )</cell><cell>65.58  †</cell><cell>69.31  † (↑ 3.73 )</cell></row><row><cell>Methods</cell><cell cols="2">Accuracy (%)</cell><cell cols="2">Methods</cell><cell cols="2">Accuracy (%)</cell><cell></cell><cell></cell></row><row><cell>CAER-Net [22]</cell><cell cols="2">73.47  †</cell><cell cols="2">EmotiCon [29]</cell><cell cols="2">88.65  †</cell><cell></cell><cell></cell></row><row><cell cols="7">CAER-Net + CCIM 74.81  † (↑ 1.34 ) EmotiCon + CCIM 91.17  † (↑ 2.52 )</cell><cell></cell><cell></cell></row><row><cell>EMOT-Net [19]</cell><cell cols="2">74.51  †</cell><cell cols="2">SIB-Net [24]</cell><cell cols="2">74.56  *</cell><cell></cell><cell></cell></row><row><cell cols="3">EMOT-Net + CCIM 75.82  † (↑ 1.31 )</cell><cell cols="2">GRERN [12]</cell><cell cols="2">81.31  *</cell><cell></cell><cell></cell></row><row><cell>GNN-CNN [56]</cell><cell cols="2">77.21  †</cell><cell cols="2">RRLA [23]</cell><cell cols="2">84.82  *</cell><cell></cell><cell></cell></row><row><cell cols="3">GNN-CNN + CCIM 78.66  † (↑ 1.45 )</cell><cell cols="2">VRD [16]</cell><cell cols="2">90.49  *</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 .</head><label>3</label><figDesc>Emotion classification accuracy (%) of different methods on the CAER-S dataset.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 4 .</head><label>4</label><figDesc>Ablation study results on all three datasets. w/ and w/o are short for with and without, respectively.</figDesc><table><row><cell></cell><cell></cell><cell>ID</cell><cell></cell><cell>Setting</cell><cell></cell><cell></cell><cell></cell><cell cols="6">EMOTIC mAP (%) Accuracy (%) CAER-S</cell><cell cols="2">GroupWalk mAP (%)</cell></row><row><cell></cell><cell></cell><cell>(1)</cell><cell cols="5">EMOT-Net + CCIM</cell><cell>30.88</cell><cell></cell><cell cols="3">75.82</cell><cell></cell><cell>60.74</cell><cell></cell></row><row><cell></cell><cell></cell><cell>(2)</cell><cell cols="5">EmotiCon + CCIM</cell><cell>39.13</cell><cell></cell><cell cols="3">91.17</cell><cell></cell><cell>69.31</cell><cell></cell></row><row><cell></cell><cell></cell><cell>(3)</cell><cell></cell><cell cols="4">(1) w/ Random Z</cell><cell>26.56</cell><cell></cell><cell cols="3">73.36</cell><cell></cell><cell>57.45</cell><cell></cell></row><row><cell></cell><cell></cell><cell>(4)</cell><cell></cell><cell cols="4">(2) w/ Random Z</cell><cell>35.12</cell><cell></cell><cell cols="3">87.34</cell><cell></cell><cell>65.62</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="6">(5) (1) w/ ImageNet Pre-training</cell><cell>28.72</cell><cell></cell><cell cols="3">74.75</cell><cell></cell><cell>58.96</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="6">(6) (2) w/ ImageNet Pre-training</cell><cell>37.48</cell><cell></cell><cell cols="3">90.46</cell><cell></cell><cell>68.28</cell><cell></cell></row><row><cell></cell><cell></cell><cell>(7)</cell><cell></cell><cell cols="4">(1) w/ ResNet-50</cell><cell>29.53</cell><cell></cell><cell cols="3">75.34</cell><cell></cell><cell>59.92</cell><cell></cell></row><row><cell></cell><cell></cell><cell>(8)</cell><cell></cell><cell cols="4">(2) w/ ResNet-50</cell><cell>38.86</cell><cell></cell><cell cols="3">90.41</cell><cell></cell><cell>68.85</cell><cell></cell></row><row><cell></cell><cell></cell><cell>(9)</cell><cell></cell><cell cols="3">(1) w/ VGG-16</cell><cell></cell><cell>28.78</cell><cell></cell><cell cols="3">74.95</cell><cell></cell><cell>59.47</cell><cell></cell></row><row><cell></cell><cell></cell><cell>(10)</cell><cell></cell><cell cols="3">(2) w/ VGG-16</cell><cell></cell><cell>37.93</cell><cell></cell><cell cols="3">89.82</cell><cell></cell><cell>68.11</cell><cell></cell></row><row><cell></cell><cell></cell><cell>(11)</cell><cell cols="5">(1) w/ Additive Attention</cell><cell>30.79</cell><cell></cell><cell cols="3">75.64</cell><cell></cell><cell>60.85</cell><cell></cell></row><row><cell></cell><cell></cell><cell>(12)</cell><cell cols="5">(2) w/ Additive Attention</cell><cell>39.16</cell><cell></cell><cell cols="3">91.08</cell><cell></cell><cell>69.26</cell><cell></cell></row><row><cell></cell><cell></cell><cell>(13)</cell><cell></cell><cell cols="2">(1) w/o λ i</cell><cell></cell><cell></cell><cell>30.05</cell><cell></cell><cell cols="3">75.21</cell><cell></cell><cell>59.83</cell><cell></cell></row><row><cell></cell><cell></cell><cell>(14)</cell><cell></cell><cell cols="2">(2) w/o λ i</cell><cell></cell><cell></cell><cell>38.53</cell><cell></cell><cell cols="3">89.67</cell><cell></cell><cell>68.75</cell><cell></cell></row><row><cell></cell><cell></cell><cell>(15)</cell><cell></cell><cell cols="3">(1) w/o P (z i )</cell><cell></cell><cell>30.63</cell><cell></cell><cell cols="3">75.59</cell><cell></cell><cell>59.94</cell><cell></cell></row><row><cell></cell><cell></cell><cell>(16)</cell><cell></cell><cell cols="3">(2) w/o P (z i )</cell><cell></cell><cell>39.05</cell><cell></cell><cell cols="3">90.06</cell><cell></cell><cell>69.15</cell><cell></cell></row><row><cell></cell><cell></cell><cell>(17)</cell><cell cols="5">(1) w/o Masking Strategy</cell><cell>29.86</cell><cell></cell><cell cols="3">74.84</cell><cell></cell><cell>59.22</cell><cell></cell></row><row><cell></cell><cell></cell><cell>(18)</cell><cell cols="5">(2) w/o Masking Strategy</cell><cell>38.06</cell><cell></cell><cell cols="3">90.57</cell><cell></cell><cell>67.79</cell><cell></cell></row><row><cell>mean Average Precision</cell><cell>42 30 32 34 36 38 40</cell><cell>28.35 30.23 30.88 36.14 37.81 39.13</cell><cell>30.52 38.56</cell><cell>28.79 37.24 EMOT-Net EmotiCon</cell><cell>Accuracy</cell><cell>76 78 80 82 84 86 88 90 92</cell><cell>75.25 75.8275.56 91.1791.15 89.49</cell><cell>75.8 90.52</cell><cell>75.46 89.78 EMOT-Net EmotiCon</cell><cell>mean Average Precision</cell><cell>60 62 64 66 68 70</cell><cell>58.85 59.56 66.73 68.34</cell><cell>60.74 69.31</cell><cell>60.75 69.12</cell><cell>59.37 67.86 EMOT-Net EmotiCon</cell></row><row><cell></cell><cell>28</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>58</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>64128 256</cell><cell>512</cell><cell>1024</cell><cell></cell><cell></cell><cell>64128 256</cell><cell>512</cell><cell>1024</cell><cell></cell><cell></cell><cell cols="2">64128 256</cell><cell>512</cell><cell>1024</cell></row><row><cell></cell><cell></cell><cell cols="3">Confounder Dictionary Size</cell><cell></cell><cell></cell><cell cols="3">Confounder Dictionary Size</cell><cell></cell><cell></cell><cell cols="4">Confounder Dictionary Size</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work is supported in part by the <rs type="funder">National Key R&amp;D Program of China</rs> (<rs type="grantNumber">2021ZD0113502</rs>, <rs type="grantNumber">2021ZD0113503</rs>), in part by the <rs type="funder">Shanghai Municipal Science and Technology Major Project</rs> (<rs type="grantNumber">2021SHZDZX0103</rs>), and in part by the <rs type="funder">China Postdoctoral Science Foundation</rs> under Grant (<rs type="grantNumber">BX20220071</rs>, <rs type="grantNumber">2022M720769</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_TfJbp58">
					<idno type="grant-number">2021ZD0113502</idno>
				</org>
				<org type="funding" xml:id="_mATtHju">
					<idno type="grant-number">2021ZD0113503</idno>
				</org>
				<org type="funding" xml:id="_dQtkQ5X">
					<idno type="grant-number">2021SHZDZX0103</idno>
				</org>
				<org type="funding" xml:id="_2qzaSh6">
					<idno type="grant-number">BX20220071</idno>
				</org>
				<org type="funding" xml:id="_6ECVXKb">
					<idno type="grant-number">2022M720769</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>ing the importance and proportion of each confounder is indispensable for achieving effective causal intervention.</p><p>Effect of Confounder Size. To justify the size N of the confounder Z, we set N to 64, 128, 256, 512, and 1024 on all datasets separately to perform experiments. The results in Figure <ref type="figure">6</ref> show that selecting the suitable size N for a dataset containing varying degrees of the harmful bias can well help the models perform de-confounded training.</p><p>Necessity of Masking Strategy. The masking strategy aims to mask the recognized subject to learn prototype representations using pure background contexts. Note that other subjects are considered as background to provide the sociodynamic context. The gain degradation from the experiments <ref type="bibr" target="#b16">(17,</ref><ref type="bibr" target="#b17">18)</ref> is observed when the target subject regions are not masked. It is unavoidable because the target subject-based feature attributes would impair the contextbased confounder dictionary Z along the undesirable link S → Z, affecting causal intervention performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Qualitative Results</head><p>Difference Between P (Y |X) and P (Y |do(X)). To visually show the difference between the models approximate P (Y |X) and P (Y |do(X)), we visualize the distribution of context features learned by EMOT-Net and EmotiCon in testing samples on the GroupWalk. These sample images contain four real-world contexts, i.e., park, market, hospital, and station. Figure <ref type="figure">7</ref> shows the following observations. In vanilla models, features with the same emotion categories usually cluster within similar context clusters (e.g., context features of the hospital with the sad category are closer), implying that biased models rely on context-specific semantics to infer emotions lopsidedly. Conversely, in the CCIMbased models, context-specific features form clusters containing diverse emotion categories. The phenomenon suggests that the causal intervention promotes models to fairly  incorporate each context prototype semantics when predicting emotions, alleviating the effect of harmful context bias.</p><p>Case Study of Causal Intervention. In Figure <ref type="figure">8</ref>, we select two representative examples from each dataset to show the performance of the model before and after the intervention. For instance, in the first row, the vanilla baseline is misled to predict entirely wrong results because the subjects in the dim scenes are mostly annotated with negative emotions. Thanks to causal intervention, CCIM corrects the bias in the model's prediction. Furthermore, in the fifth row, CCIM disentangles the spurious correlation between the context ("hospital entrance") and the emotion semantics ("sad"), improving the model's performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This paper proposes a causal debiasing strategy to reduce the harmful bias of uneven distribution of emotional states across diverse contexts in the CAER task. Concretely, we disentangle the causalities among variables via a tailored causal graph and present a Contextual Causal Intervention Module (CCIM) to remove the adverse effect caused by the context bias as a confounder. Numerous experiments prove that CCIM can consistently improve existing models and promote them to a new SOTA. The model-agnostic and plug-in CCIM undoubtedly has excellent superiority over complex module stacking in previous approaches.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Aseel Alhadlaq, Chitra Shashidhar, Wesam Atef Hatamleh, Hussam Tarazi, Prashant Kumar Shukla, and Rajnish Ratna. Humancomputer interaction with detection of speaker emotions using convolution neural networks</title>
		<author>
			<persName><forename type="first">Abeer</forename><surname>Ali Alnuaim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammed</forename><surname>Zakariah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Intelligence and Neuroscience</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep neural network for visual emotion recognition based on resnet50 using songspeech characteristics</title>
		<author>
			<persName><forename type="first">Souha</forename><surname>Ayadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zied</forename><surname>Lachiri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 5th International Conference on Advanced Systems and Emergent Technologies (IC ASET)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="363" to="368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Context in emotion perception. Current Directions in Psychological</title>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barrett</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Batja</forename><surname>Mesquita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Gendron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="286" to="290" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Step: Spatial temporal graph convolutional networks for emotion perception from gaits</title>
		<author>
			<persName><forename type="first">Uttaran</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trisha</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohan</forename><surname>Chandra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1342" to="1350" />
		</imprint>
	</monogr>
	<note>Tanmay Randhavane, Aniket Bera, and Dinesh Manocha</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Causal intervention for subject-deconfounded facial action unit recognition</title>
		<author>
			<persName><forename type="first">Yingjie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="374" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Shape matters: deformable patch attack</title>
		<author>
			<persName><forename type="first">Zhaoyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianghe</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shouhong</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenqiang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="529" to="548" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Towards practical certifiable patch defense with vision transformer</title>
		<author>
			<persName><forename type="first">Zhaoyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianghe</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shouhong</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenqiang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="15148" to="15158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning associative representation for facial expression recognition</title>
		<author>
			<persName><forename type="first">Yangtao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dingkang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingchen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lihua</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="889" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Facial expression recognition in the wild via deep attentive center loss</title>
		<author>
			<persName><forename type="first">Amir</forename><surname>Hossein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Farzaneh</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojun</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2402" to="2411" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Causal inference and developmental psychology</title>
		<author>
			<persName><forename type="first">Foster</forename><surname>Michael</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Developmental psychology</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Graph reasoning-based emotion recognition network</title>
		<author>
			<persName><forename type="first">Qinquan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanxin</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Shortcut learning in deep neural networks</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>Geirhos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jörn-Henrik</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claudio</forename><surname>Michaelis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wieland</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><forename type="middle">A</forename><surname>Wichmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="665" to="673" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Causal inference in statistics: A primer</title>
		<author>
			<persName><forename type="first">Madelyn</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><forename type="middle">P</forename><surname>Jewell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>John Wiley &amp; Sons</publisher>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Context-aware emotion recognition based on visual relationship detection</title>
		<author>
			<persName><forename type="first">Manh-Hung</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soo-Hyung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyung-Jeong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guee-Sang</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="90465" to="90474" />
			<date type="published" when="2006">2021. 2, 3, 5, 6</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Counterfactually-augmented snli training data does not yield better generalization than unaugmented data</title>
		<author>
			<persName><forename type="first">William</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haokun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><surname>Samuel R Bowman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.04762</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Bryan</surname></persName>
		</author>
		<author>
			<persName><surname>Jones</surname></persName>
		</author>
		<title level="m">Bounded rationality. Annual review of political science</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="297" to="321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Emotion recognition in context</title>
		<author>
			<persName><forename type="first">Ronak</forename><surname>Kosti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adria</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2006">2017. 1, 2, 3, 5, 6</date>
			<biblScope unit="page" from="1667" to="1675" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Context based emotion recognition using emotic dataset</title>
		<author>
			<persName><forename type="first">Ronak</forename><surname>Kosti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adria</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2755" to="2766" />
			<date type="published" when="2005">2019. 1, 2, 3, 5</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Towards simultaneous segmentation of liver tumors and intrahepatic vessels via crossattention mechanism</title>
		<author>
			<persName><forename type="first">Haopeng</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dingkang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shunli</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoying</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lihua</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.09785</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Context-aware emotion recognition networks</title>
		<author>
			<persName><forename type="first">Jiyoung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seungryong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunok</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jungin</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kwanghoon</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2006">2019. 1, 2, 3, 5, 6</date>
			<biblScope unit="page" from="10143" to="10152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Human emotion recognition with relational region-level analysis</title>
		<author>
			<persName><forename type="first">Weixin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuan</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunhong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<date type="published" when="2006">2021. 1, 2, 3, 5, 6</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Sequential interactive biased network for context-aware emotion recognition</title>
		<author>
			<persName><forename type="first">Xinpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changxing</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Joint Conference on Biometrics (IJCB)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">imigue: An identity-free video dataset for micro-gesture understanding and emotion analysis</title>
		<author>
			<persName><forename type="first">Xin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henglin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zitong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaobai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoying</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10631" to="10642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning appearance-motion normality for video anomaly detection</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengyang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dingkang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoguang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Multimedia and Expo (ICME)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Generalized video anomaly event detection: Systematic taxonomy and comparison of deep models</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dingkang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.05087</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Hybrid emotion-aware monitoring system based on brainwaves for internet of medical things</title>
		<author>
			<persName><forename type="first">Weizhi</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurence</forename><forename type="middle">T</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Yang</forename><surname>Chiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Internet of Things Journal</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">21</biblScope>
			<biblScope unit="page" from="16014" to="16022" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Emoticon: Context-aware multimodal emotion recognition using frege&apos;s principle</title>
		<author>
			<persName><forename type="first">Trisha</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pooja</forename><surname>Guhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uttaran</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohan</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aniket</forename><surname>Bera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinesh</forename><surname>Manocha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2006">2020. 1, 2, 3, 4, 5, 6</date>
			<biblScope unit="page" from="14234" to="14243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Contemplating visual emotions: Understanding and overcoming dataset bias</title>
		<author>
			<persName><forename type="first">Rameswar</forename><surname>Panda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoxiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joon-Young</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amit K Roy-Chowdhury</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="579" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Causal inference in statistics: An overview</title>
		<author>
			<persName><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics surveys</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Models, reasoning and inference</title>
		<author>
			<persName><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>CambridgeUniversityPress</publisher>
			<biblScope unit="volume">19</biblScope>
			<pubPlace>Cambridge, UK</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Automatic Differentiation In Pytorch</title>
	</analytic>
	<monogr>
		<title level="j">Pytorch</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Two causal principles for improving visual dialog</title>
		<author>
			<persName><forename type="first">Jiaxin</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulei</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="10860" to="10869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Counterfactual inference for text classification debiasing</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lijie</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunping</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengjun</forename><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="5434" to="5445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Context-aware generation-based net for multi-label visual emotion recognition</title>
		<author>
			<persName><forename type="first">Kun</forename><surname>Shulan Ruan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yijun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weidong</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangyi</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enhong</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Multimedia and Expo (ICME)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Causal inference using potential outcomes: Design, modeling, decisions</title>
		<author>
			<persName><surname>Donald B Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="issue">469</biblScope>
			<biblScope unit="page" from="322" to="331" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Unbiased scene graph generation from biased training</title>
		<author>
			<persName><forename type="first">Kaihua</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulei</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3716" to="3725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Shoelace pattern-based speech emotion recognition of the lecturers in distance education: Shoepat23</title>
		<author>
			<persName><forename type="first">Dahiru</forename><surname>Tanko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sengul</forename><surname>Dogan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fahrettin</forename><surname>Burak Demir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehmet</forename><surname>Baygin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sakir</forename><surname>Engin Sahin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Turker</forename><surname>Tuncer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Acoustics</title>
		<imprint>
			<biblScope unit="volume">190</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">108637</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Causal inference in economics and marketing</title>
		<author>
			<persName><surname>Hal R Varian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="issue">27</biblScope>
			<biblScope unit="page" from="7310" to="7315" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Caspacenet: Counterfactual analysis for 6d pose estimation in space</title>
		<author>
			<persName><forename type="first">Shunli</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuaibing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dingkang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liuzhen</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chixiao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lihua</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="10627" to="10634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Tsa-net: Tube self-attention network for action quality assessment</title>
		<author>
			<persName><forename type="first">Shunli</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dingkang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chixiao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lihua</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Multimedia (ACM MM)</title>
		<meeting>the 29th ACM International Conference on Multimedia (ACM MM)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4902" to="4910" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A survey of video-based action quality assessment</title>
		<author>
			<persName><forename type="first">Shunli</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dingkang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Suo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ka</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lihua</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Networking Systems of AI (INSAI)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Visual commonsense r-cnn</title>
		<author>
			<persName><forename type="first">Tan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qianru</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">Yuzheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaoyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dingkang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizhe</forename><surname>Qi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.08764</idno>
		<title level="m">Adversarial contrastive distillation with adaptive denoising</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Explicit and implicit knowledge distillation via unlabeled data</title>
		<author>
			<persName><forename type="first">Yuzheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zuhao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaoyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuangjia</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunquan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizhe</forename><surname>Qi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.08771</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Rich Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Disentangled representation learning for multimodal emotion recognition</title>
		<author>
			<persName><forename type="first">Dingkang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haopeng</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangtao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lihua</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th ACM International Conference on Multimedia (ACM MM)</title>
		<meeting>the 30th ACM International Conference on Multimedia (ACM MM)</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Contextual and cross-modal interaction for multi-modal speech emotion recognition</title>
		<author>
			<persName><forename type="first">Dingkang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lihua</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2022">2093-2097, 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Emotion recognition for multiple context awareness</title>
		<author>
			<persName><forename type="first">Dingkang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shunli</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liuzhen</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingcheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lihua</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13697</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Learning modality-specific and -agnostic representations for asynchronous multimodal language sequences</title>
		<author>
			<persName><forename type="first">Dingkang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haopeng</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lihua</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th ACM International Conference on Multimedia (ACM MM)</title>
		<meeting>the 30th ACM International Conference on Multimedia (ACM MM)</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Target and source modality co-reinforcement for emotion understanding from asynchronous multimodal sequences</title>
		<author>
			<persName><forename type="first">Dingkang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Can</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingcheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuzheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lihua</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge-Based Systems</title>
		<imprint>
			<biblScope unit="volume">265</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">110370</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">A novel ef-ficient multi-view traffic-related object detection framework</title>
		<author>
			<persName><forename type="first">Kun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dingkang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanni</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.11810</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Robust adversarial reinforcement learning with dissipation inequation constraint</title>
		<author>
			<persName><forename type="first">Peng</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyan</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lihua</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shunli</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dingkang</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="5431" to="5439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Contextaware affective graph reasoning for emotion recognition</title>
		<author>
			<persName><forename type="first">Minghui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yumeng</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huadong</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Multimedia and Expo (ICME)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006">2019. 1, 3, 5, 6</date>
			<biblScope unit="page" from="151" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Debiasing distantly supervised named entity recognition via causal intervention</title>
		<author>
			<persName><forename type="first">Wenkai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianpei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.09233</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Places: A 10 million image database for scene recognition</title>
		<author>
			<persName><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Direct field-to-pattern monolithic design of holographic metasurface via residual encoderdecoder convolutional neural network. Opto-Electronic Advances</title>
		<author>
			<persName><forename type="first">Ruichao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiafu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianshuo</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dingkang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zuntian</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tonghao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yajuan</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongya</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaobo</forename><surname>Qu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="220148" to="220149" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
