<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Ordering-Based Causal Discovery with Reinforcement Learning</title>
				<funder ref="#_48ZRtnA">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xiaoqiang</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Automation Science and Engineering</orgName>
								<orgName type="laboratory">State Key Laboratory for Manufacturing Systems Engineering</orgName>
								<orgName type="institution">Xi&apos;an Jiaotong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yali</forename><surname>Du</surname></persName>
							<email>yali.dux@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="institution">University College London</orgName>
								<address>
									<addrLine>3 Huawei Noah&apos;s Ark Lab</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shengyu</forename><surname>Zhu</surname></persName>
							<email>zhushengyu@huawei.com</email>
						</author>
						<author>
							<persName><forename type="first">Liangjun</forename><surname>Ke</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Automation Science and Engineering</orgName>
								<orgName type="laboratory">State Key Laboratory for Manufacturing Systems Engineering</orgName>
								<orgName type="institution">Xi&apos;an Jiaotong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhitang</forename><surname>Chen</surname></persName>
							<email>chenzhitang2@huawei.com</email>
						</author>
						<author>
							<persName><forename type="first">Jianye</forename><surname>Hao</surname></persName>
							<email>haojianye@huawei.com</email>
							<affiliation key="aff2">
								<orgName type="department">College of Intelligence and Computing</orgName>
								<orgName type="institution">Tianjin University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jun</forename><surname>Wang</surname></persName>
							<email>jun.wang@cs.ucl.ac.uk</email>
							<affiliation key="aff1">
								<orgName type="institution">University College London</orgName>
								<address>
									<addrLine>3 Huawei Noah&apos;s Ark Lab</addrLine>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Ordering-Based Causal Discovery with Reinforcement Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:17+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>It is a long-standing question to discover causal relations among a set of variables in many empirical sciences. Recently, Reinforcement Learning (RL) has achieved promising results in causal discovery from observational data. However, searching the space of directed graphs and enforcing acyclicity by implicit penalties tend to be inefficient and restrict the existing RL-based method to small scale problems. In this work, we propose a novel RL-based approach for causal discovery, by incorporating RL into the ordering-based paradigm. Specifically, we formulate the ordering search problem as a multi-step Markov decision process, implement the ordering generating process with an encoder-decoder architecture, and finally use RL to optimize the proposed model based on the reward mechanisms designed for each ordering. A generated ordering would then be processed using variable selection to obtain the final causal graph. We analyze the consistency and computational complexity of the proposed method, and empirically show that a pretrained model can be exploited to accelerate training. Experimental results on both synthetic and real data sets shows that the proposed method achieves a much improved performance over existing RL-based method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Identifying causal structure from observational data is an important but also challenging task in many practical applications. This task can be formulated as that of finding a Directed Acyclic Graph (DAG) that minimizes a score function defined w.r.t. the observed data. However, searching over the space of DAGs for the best DAG is known to be NP-hard, even if each node has at most two parents <ref type="bibr">[Chickering, 1996]</ref>. Consequently, traditional methods mostly rely on local heuristics to perform the search, including greedy hill-climbing and greedy equivalence search that explores the Markov equivalence classes <ref type="bibr">[Chickering, 2002]</ref>.</p><p>Along with various search strategies, existing methods have also cast causal structure learning problem as that of learning an optimal variable ordering, considering that the ordering space is significantly smaller than that of directed graphs and searching over the ordering space can avoid dealing with the acyclicity constraint <ref type="bibr" target="#b9">[Teyssier and Koller, 2005]</ref>. Many methods, such as genetic algorithm <ref type="bibr" target="#b6">[Larranaga et al., 1996]</ref>, Markov chain Monte Carlo <ref type="bibr" target="#b5">[Friedman and Koller, 2003</ref>] and greedy local hill-climbing <ref type="bibr" target="#b9">[Teyssier and Koller, 2005]</ref>, have been exploited as the search strategies to find desired orderings. In practice, however, these methods often cannot effectively find a globally optimal ordering for their heuristic nature.</p><p>Recently, with smooth score functions, several gradientbased methods have been proposed by exploiting a smooth characterization of acyclicity, including NOTEARS <ref type="bibr" target="#b12">[Zheng et al., 2018]</ref> for linear causal models and several subsequent works, e.g., <ref type="bibr" target="#b11">[Yu et al., 2019;</ref><ref type="bibr" target="#b6">Lachapelle et al., 2020;</ref><ref type="bibr">Ng et al., 2019b;</ref><ref type="bibr">Ng et al., 2019a;</ref><ref type="bibr" target="#b12">Zheng et al., 2020]</ref>, which use neural networks for modelling non-linear causal relationships. As another attempt, <ref type="bibr" target="#b13">[Zhu et al., 2020]</ref> utilize Reinforcement Learning (RL) to find the underlying DAG from the graph space without the need of smooth score functions. Unfortunately, <ref type="bibr" target="#b13">[Zhu et al., 2020]</ref> achieved good performance only with up to 30 variables, for at least two reasons: 1) the action space, consisting of directed graphs, is tremendous for large scale problems and is hard to be explored efficiently; and 2) it has to compute scores for many non-DAGs generated during training but computing scores w.r.t. data is generally time-consuming. It appears that the RL-based approach may not be able to achieve a close performance to other gradientbased methods that directly optimize the same score function for large causal discovery problems, due to its search nature.</p><p>By taking advantage of the reduced space of variable orderings and the strong search ability of modern RL methods, we propose Causal discovery with Ordering-based Reinforcement Learning (CORL), which incorporates RL into the orderingbased paradigm and is shown to achieve a promising empirical performance. In particular, CORL outperforms NOTEARS, a state-of-the-art gradient-based method for linear data, even with 150-node graphs. Meanwhile, CORL is also competitive with a strong baseline, Causal Additive Model (CAM) method <ref type="bibr" target="#b3">[Bühlmann et al., 2014]</ref>, on non-linear data models.</p><p>Contributions. We make the following contributions in this work: 1) We formulate the ordering search problem as a multistep Markov Decision Process (MDP) and propose to implement the ordering generating process in an effective encoderdecoder architecture, followed by applying RL to optimizing the proposed model based on specifically designed reward mechanisms. We also incorporate a pretrained model into CORL to accelerate training. 2) We analyze the consistency and computational complexity of the proposed method. 3) We conduct comparative experiments on synthetic and real data sets to validate the performance of the proposed methods. 4) An implementation has been made available at <ref type="url" target="https://github.com/huawei-noah/trustworthyAI/tree/master/gcastle">https://github. com/huawei-noah/trustworthyAI/tree/master/gcastle</ref>. 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Besides the aforementioned heuristic ordering search algorithms, <ref type="bibr" target="#b8">[Schmidt et al., 2007]</ref> proposed L1OBS to conduct variable selection using 1 -regularization paths based on the method from <ref type="bibr" target="#b9">[Teyssier and Koller, 2005]</ref>. <ref type="bibr" target="#b8">[Scanagatta et al., 2015]</ref> further proposed an ordering exploration method on the basis of an approximated score function so as to scale to thousands of variables. The CAM method <ref type="bibr" target="#b3">[Bühlmann et al., 2014]</ref> was specifically designed for non-linear additive models. Some recent ordering-based methods such as sparsest permutation <ref type="bibr" target="#b8">[Raskutti and Uhler, 2018]</ref> and greedy sparsest permutation <ref type="bibr" target="#b8">[Solus et al., 2017]</ref> can guarantee consistency of Markov equivalence class, relying on some conditional independence relations and certain assumptions like faithfulness. A variant of greedy sparest permutation was further proposed in <ref type="bibr" target="#b2">[Bernstein et al., 2020]</ref> for the setting with latent variables. In the present work, we mainly work on identifiable cases which have different assumptions from theirs.</p><p>In addition, exact algorithms such as dynamic programming <ref type="bibr">[Xiang and Kim, 2013]</ref> and integer or linear programming <ref type="bibr" target="#b0">[Bartlett and Cussens, 2017]</ref> are also used for causal discovery problem. However, these algorithms usually can only work on small graphs <ref type="bibr" target="#b4">[De Campos and Ji, 2011]</ref>, and to handle larger problems with hundreds of variables, they usually need to incorporate heuristics search <ref type="bibr">[Xiang and Kim, 2013]</ref> or limit the maximum number of parents of each node.</p><p>Recently, RL has been used to tackle several combinatorial problems such as the maximum cut and traveling salesman problem <ref type="bibr" target="#b1">[Bello et al., 2016;</ref><ref type="bibr" target="#b5">Khalil et al., 2017;</ref><ref type="bibr">Kool et al., 2019]</ref>. These works aim to learn a policy as a solver based on the particular type of combinatorial problems. However, causal discovery tasks generally have different relationships, data types, graph structures, etc., and moreover, are typically off-line with focus on a or a class of causal graph(s). As such, we use RL as a search strategy, similar to <ref type="bibr" target="#b14">[Zoph and Le, 2017;</ref><ref type="bibr" target="#b13">Zhu et al., 2020]</ref>. Nevertheless, a pretrained model or policy can offer a good starting point to speed up training, as shown in our evaluation results (cf. Figure <ref type="figure" target="#fig_1">3</ref>). 1 The extended version can be found at <ref type="url" target="https://arxiv.org/abs/2105.06631">https://arxiv.org/abs/2105. 06631</ref>. Figure <ref type="figure">1</ref>: An example of the correspondence between an ordering and a fully-connected DAG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Causal Structure Learning</head><p>Let G = (d, V, E) denotes a DAG, with d the number of nodes, V = {v 1 , • • • , v d } the set of nodes, and E = {(v i , v j )|i, j = 1, . . . , d} the set of directed edges from v i to v j . Each node v j is associated with a random variable X j . The probability model associated with G factorizes as p(X</p><formula xml:id="formula_0">1 , • • • , X d ) = d j=1 p(X j |Pa(X j ))</formula><p>, where p(X j |Pa(X j )) is the conditional probability distribution for X j given its parents Pa(X j ) := {X k |(v k , v j ) ∈ E}. We assume that the observed data x j is obtained by the Structural Equation Model (SEM) with additive noises: X j := f j (Pa(X j )) + j , j = 1, . . . , d, where f j represents the functional relationship between X j and its parents, and j 's denote jointly independent additive noise variables. We assume causal minimality, which is equivalent to that each f j is not a constant for any X k ∈ Pa(X j ) in this SEM <ref type="bibr" target="#b8">[Peters et al., 2014]</ref>.</p><p>Given a sample</p><formula xml:id="formula_1">X = [x 1 , • • • , x d ] ∈ R m×d</formula><p>where x j is a vector of m observations for random variable X j . The goal is to find a DAG G that optimizes the Bayesian Information Criterion (BIC) (or equivalently, minimum description length) score, defined as</p><formula xml:id="formula_2">S BIC (G)= d j=1 m k=1 log p(x k j |Pa(x k j ); θ j )- |θ j | 2 log m ,<label>(1)</label></formula><p>where x k j is the k-th observation of X j , θ j is the parameter associated with each likelihood, and |θ j | denotes the parameter dimension. For linear-Gaussian models, p(x k j |Pa(x k j ); θ j ) = N (x j |θ T j Pa(x j ), σ 2 ) and σ 2 can be estimated from the data. The problem of finding a directed graph that satisfies the ayclicity constraint can be cast as that of finding a variable ordering <ref type="bibr" target="#b9">[Teyssier and Koller, 2005;</ref><ref type="bibr" target="#b8">Schmidt et al., 2007]</ref>. Specifically, let Π denote an ordering of the nodes in V , where the length of the ordering</p><formula xml:id="formula_3">|Π| = |V | and Π is indexed from 1. If node v j ∈ V lies in the p-th position, then Π(p) = v j .</formula><p>Notation Π ≺vj denotes the set of nodes that precede node v j in Π. One can easily establish a canonical correspondence between an ordering Π and a fully-connected DAG G Π ; an example is presented in Figure <ref type="figure">1</ref>. A DAG G can be consistent with more than one orderings and the set of these orderings is denoted by</p><formula xml:id="formula_4">Φ(Π)={Π : fully-connected DAG G Π is a super-DAG of G},</formula><p>where a super-DAG of G is a DAG whose edge set is a superset of that of G. The the search for the true DAG G * can be decomposed to two phases: finding the correct ordering and performing variable selection; the latter is to find the optimal DAG that is consistent with the ordering found in the first step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Reinforcement Learning</head><p>Standard RL is usually formulated as an MDP over the environment state s ∈ S and agent action a ∈ A, under an (unknown) environmental dynamics defined by a transition probability T (s |s, a). Let π φ (a|s) denote the policy, parameterized by φ, which outputs a distribution used to select an action from action space A based on state s. For episodic tasks, a trajectory τ = {s t , a t } T t=0 , where T is the finite time horizon, can be collected by executing the policy repeatedly. In many cases, an immediate reward r(s, a) can be received when agent executes an action. The objective of RL is to learn a policy which can maximize the expected cumulative reward along a trajectory, i.e., J(φ</p><formula xml:id="formula_5">) = E π φ [R 0 ] with R 0 = T t=0 γ t r t (s t , a t</formula><p>) and γ ∈ (0, 1] being a discount factor. For some scenarios, the reward is only earned at the terminal time (also called episodic reward), and</p><formula xml:id="formula_6">J(φ) = E π φ [R(τ )] with R(τ ) = r T (s T , a T ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Method</head><p>In this section, we first formulate the ordering search problem as an MDP and then describe the proposed approach. We also discuss the variable selection methods to obtain DAGs from variable orderings, as well as the consistency and computational complexity regarding the proposed method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Ordering Search as Markov Decision Process</head><p>To incorporate RL into the ordering-based paradigm, we formulate the variable ordering search problem as a multi-step decision process with a variable as an action at each decision step, and the order of the selected actions (or variables) is treated as the searched ordering. The decision-making process is Markovian, and its elements are described as follows.</p><p>State. One can directly take the sample data x j as the state. However, preliminary experiments (see Appendix A.1) show that it is difficult for feed-forward neural network models to capture the underlying causal relationships directly using observed data as states, and that the data pre-processed by an encoder module is helpful to find better orderings. The encoder module embeds each x j to state s j and all the embedded states constitute the state space S := {s 1 , • • • , s d }. In our case, we also need an initial state, denoted by s 0 (detailed choice is given in Section 4.2), to select the first action. The complete state space would be Ŝ := S ∪ {s 0 }. We will use ŝt to denote the actual state encountered at the t-th decision step when generating a variable ordering.</p><p>Action. We select an action (variable) from the action space</p><formula xml:id="formula_7">A := {v 1 , • • • , v d }</formula><p>consisting of all the variables at each decision step, and the action space size is equal to the number of variables, i.e., |A| = d. Compared to the previous RL-based method that searches over the graph space with size O(2 d×d ) <ref type="bibr" target="#b13">[Zhu et al., 2020]</ref>, the resulting action space becomes much smaller. State transition. The state transition is related to the action selected at the current decision step. If the selected variable is v j at the t-th decision step, then the state is transferred to the state s j ∈ S which corresponds to x j embedded by the encoder, i.e., ŝt+1 = s j .</p><p>Reward. In ordering-based methods, only the variables selected in previous decision steps can be the potential parents of the currently selected variable. Hence, we design the rewards in the following cases: episodic reward and dense reward. In the former case, we calculate the score for a variable ordering Π with d variables as the episodic reward, i.e.,</p><formula xml:id="formula_8">R(τ ) = r T (ŝ T , a T ) = S BIC (G Π )<label>(2)</label></formula><p>where T = d -1 and S BIC has been defined in Equation ( <ref type="formula" target="#formula_2">1</ref>), with Pa(X j ) replaced by the potential parent variable set U (X j ); here U (X j ) denotes the variables associated with the nodes in Π ≺vj . If the score function is decomposable (e.g., the BIC score), we can calculate an immediate reward by exploiting the decomposability for the current decision step. That is, for v j selected at time step t, the immediate reward is</p><formula xml:id="formula_9">r t = m k=1 log p(x k j |U (x k j ); θ j ) - |θ j | 2 log m.<label>(3)</label></formula><p>This belongs the second case with dense rewards. Here we keep -|θ j |/2 log m to make Equation (3) consistent with the form of BIC score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation and Optimization with Reinforcement Learning</head><p>We briefly describe the neural network architectures implemented in our method, as shown in Figure <ref type="figure" target="#fig_0">2</ref>. More details can be found in Appendix A.</p><p>Encoder. f enc φe : X → S is used to map the observed data to the embedding space S = {s 1 , • • • , s d }. Similar to <ref type="bibr" target="#b13">[Zhu et al., 2020]</ref>, we adopt mini-batch training and randomly draw n samples from m samples of the data set X to construct X ∈ R n×d at each episode. We also set the embedding s j to be in the same dimension, i.e., s j ∈ R n . For encoder choice, we conduct an empirical comparison among several representative structures such as MLP, LSTM and the self-attention based encoder <ref type="bibr" target="#b9">[Vaswani et al., 2017]</ref>. Empirically, we validate that the self-attention based encoder in the Transformer structure performs the best (see Appendix A.1).</p><p>Decoder. f dec φ d : Ŝ → A maps the state space Ŝ to the action space A. Among several decoder choices (see also Appendix A.1 for an empirical comparison), we pick an LSTM based structure that proves effective in our experiments. Although the initial state is generated randomly in many applications, we pick it as s 0 = 1 d d i=1 s i , considering that the source node is fixed in a correct ordering. We restrict each node only be selected once by masking the selected nodes, in order to generate a valid ordering <ref type="bibr" target="#b10">[Vinyals et al., 2015]</ref>.</p><p>Optimization. The optimization objective is to learn a policy maximizing J(φ), where φ = {φ e , φ d } with φ e and φ d being parameters associated with encoder f enc and decoder f dec , respectively. Based on the above definition, policy gradient <ref type="bibr" target="#b8">[Sutton and Barto, 2018]</ref> is used to optimize the ordering generation model parameters. For the episodic reward case, we have the following policy gradient</p><formula xml:id="formula_10">∇J(φ) = E π φ R(τ ) T t=0 ∇ φ log π φ (a t |ŝ t )</formula><p>, and the algorithm in this case is denoted as CORL-1. For the dense reward case, policy gradient can be calculated as</p><formula xml:id="formula_11">∇J(φ) = E π φ T t=0 R t ∇ φ log π φ (a t |ŝ t ) , where R t =</formula><p>T -t l=0 γ l r t+l denotes the return at time step t. We denote the algorithm in this case as CORL-2. Using a parametric baseline to estimate the expected score typically improves learning <ref type="bibr" target="#b8">[Sutton and Barto, 2018]</ref>. Therefore, we introduce a critic network V φv (ŝ t ) parameterized by φ v , which learns the expected return given state ŝt and is trained with stochastic gradient descent using Adam optimizer on a mean squared error objective between its predicted value and the actual return. More details about the critic network are described in Appendix A.2.</p><p>Inspired by the benefits from pretrained models <ref type="bibr" target="#b1">[Bello et al., 2016]</ref>, we also consider to incorporate pretraining to our method to accelerate training. In practice, one can usually obtain some observed data with known causal graphs or correct orderings, e.g., by simulation or real data with labeled graphs. Hence, we can pretrain a policy model with such data in a supervised way and use the pretrained model as initialization for new tasks. Meanwhile, a sufficient generalization ability is desired and we hence include diverse data sets with different numbers of nodes, noise types, causal relationships, etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Variable Selection</head><p>One can obtain the causal graph from an ordering by conducting variable selection methods, such as sparse candidate <ref type="bibr" target="#b9">[Teyssier and Koller, 2005]</ref>, significance testing of covariates <ref type="bibr" target="#b3">[Bühlmann et al., 2014]</ref>, and group Lasso <ref type="bibr" target="#b8">[Schmidt et al., 2007]</ref>. In this work, for linear data models, we apply linear regression to the obtained fully-connected DAG and then use thresholding to prune edges with small weights, as similarly used by <ref type="bibr" target="#b12">[Zheng et al., 2018]</ref>. For the non-linear model, we adopt the CAM pruning used by <ref type="bibr" target="#b6">[Lachapelle et al., 2020]</ref>. For each variable X j , one can fit a generalized additive model against the current parents of X j and then apply significance testing of covariates, declaring significance if the reported p-values are lower that or equal to 0.001. The overall method is summarized in Algorithm 1. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Consistency Analysis</head><p>So far we have presented CORL in a general manner without specifying explicitly the distribution family for calculating the scores or rewards. In principle, any distribution family could be employed as long as its log-likelihood can be computed. However, whether the maximization of the accumulated reward recovers the correct ordering, i.e., whether consistency of the score function holds, depends on both the modelling choice of reward and the underlying SEM. If the SEM is identifiable, then the following proposition shows that it is possible to find the correct ordering with high probability in the large sample limit. Proposition 1. Suppose that an identifiable SEM with true causal DAG G * on X = {X j } d j=1 induces distribution P (X). Let G Π be the fully-connected DAG that corresponds to an ordering Π. If there is an SEM with G Π inducing the same distribution P (X), then G Π must be a super-graph of G * , i.e., every edge in G * is covered in G Π .</p><p>Proof. The SEM with G Π may not be causally minimal but can be reduced to an SEM satisfying the causal minimality condition <ref type="bibr" target="#b8">[Peters et al., 2014]</ref>. Let GΠ denotes the causal graph in the reduced SEM with the same distribution P (X). Since we have assumed that original SEM is identifiable, i.e., the distribution P (X) corresponds to a unique true graph, GΠ is then identical to G * . The proof is complete by noticing that G Π is a super-graph of GΠ . Thus, if the causal relationships fall into the chosen model functions and a right distribution family is assumed, then given infinite samples the optimal accumulated reward (e.g., the optimal BIC score) must be achieved by a super-DAG of the underlying graph. However, finding the optimal accumulated reward may be hard, because policy gradient methods only guarantee local convergence <ref type="bibr" target="#b8">[Sutton and Barto, 2018]</ref>, and we can only apply approximate model functions and also need to assume a certain distribution family for calculating the reward. Nevertheless, the experimental results in Section 5 show that the proposed method can achieve a better performance than those with consistency guarantee in the finite sample regime, thanks to the improved search ability of modern RL methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Computational Complexity</head><p>In contrast with typical RL applications, we treat RL here as a search strategy, aiming to find an ordering that achieves the best score. CORL requires the evaluation of the rewards at each episode with O(dm 2 + d 3 ) computational cost if linear functions are adopted to model the causal relations, which is same to RL-BIC2 <ref type="bibr" target="#b13">[Zhu et al., 2020]</ref>. Fortunately, CORL does not need to compute the matrix exponential term with O(d 3 ) cost due to the use of ordering search. We observe that CORL performs fewer episodes than RL-BIC2 before the episode reward converges (see Appendix C). The evaluation of Transformer encoder and LSTM decoder in CORL take O(nd 2 ) and O(dn 2 ), respectively. However, we find that computing rewards is dominating in the total running time (e.g., around 95% and 87% for 30and 100-node linear data models). Thus, we record the decomposed scores for each variable v j with different parental sets Π ≺vj to avoid repeated computations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>In this section, we conduct experiments on synthetic data sets with linear and non-linear causal relationships as well as a real data set. The baselines are ICA-LiNGAM <ref type="bibr">[Shimizu et al., 2006]</ref>, three ordering-based approaches L1OBS <ref type="bibr" target="#b8">[Schmidt et al., 2007]</ref>, CAM <ref type="bibr" target="#b3">[Bühlmann et al., 2014]</ref> and A* Lasso <ref type="bibr">[Xiang and Kim, 2013]</ref>, some recent gradient-based approaches NOTEARS <ref type="bibr" target="#b12">[Zheng et al., 2018]</ref>, DAG-GNN <ref type="bibr" target="#b11">[Yu et al., 2019]</ref> and GraN-DAG <ref type="bibr" target="#b6">[Lachapelle et al., 2020]</ref>, and the RL-based approach RL-BIC2 <ref type="bibr" target="#b13">[Zhu et al., 2020]</ref>. We use the original implementations (see Appendix B.1 for details) and pick the recommended hyper-parameters unless otherwise stated. We generate different types of synthetic data sets which vary along: level of edge sparsity, graph type, number of nodes, causal functions and sample size. Two types of graph sampling schemes, Erdös-Rényi (ER) and Scale-free (SF), are considered here. We denote d-node ER and SF graphs with on average hd edges as ERh and SFh, respectively. Two common metrics are considered: True Positive Rate (TPR) and Structural Hamming Distance (SHD). The former indicates the probability of correctly finding the positive edges among the discoveries. Hence, it can be used to measure the quality of an ordering, and the higher the better. The latter counts the total number of missing, falsely detected or reversed edges, and the smaller the better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Linear Models with Gaussian and Non-Gaussian Noise</head><p>We evaluate the proposed methods on Linear Gaussian (LG) with equal variance Gaussian noise and LiNGAM data models, and the true DAGs in both cases are known to be identifiable <ref type="bibr" target="#b3">[Peters and Bühlmann, 2014;</ref><ref type="bibr">Shimizu et al., 2006]</ref>. We set h ∈ {2, 5} and d ∈ {30, 50, 100} to generate observed data (see Appendix B.2 for details). For variable selection, we set the as 0.3 and apply it to the estimated coefficients, as similarly used by <ref type="bibr" target="#b12">[Zheng et al., 2018;</ref><ref type="bibr" target="#b13">Zhu et al., 2020]</ref>.</p><p>Table <ref type="table" target="#tab_1">1</ref> presents the results for 30and 100-node LG data models; the conclusions do not change with 50-node graphs, which are given in Appendix D. The performances of ICA-LiNGAM, GraN-DAG and CAM is also given in Appendix D, and they are almost never on par with the best methods presented in this section. CORL-1 and CORL-2 achieve consistently good results on LiNGAM data sets which are reported in Appendix E due to the space limit.</p><p>We now examine Table <ref type="table" target="#tab_1">1</ref> (the values in parentheses represent the standard deviation across data sets per task). Across all settings, CORL-1 and CORL-2 are the best performing methods in terms of both TPR and SHD, while NOTEARS and DAG-GNN are not too far behind. In Figure <ref type="figure" target="#fig_1">3</ref>, we further show the training reward curves of CORL-1 and CORL-2 on 100-node LG data sets, where CORL-2 converges faster to a better ordering than CORL-1. We conjecture that this is be-cause dense rewards can provide more guidance information for the training process than episodic rewards, which is beneficial to the learning of RL model and improves the training performance. Hence, CORL-2 is preferred in practice if the score function is decomposable for each variable. As discussed previously, RL-BIC2 only achieves satisfactory results on graphs with 30 nodes. The TPR of L1OBS is lower than that of A* Lasso, which indicates that L1OBS using greedy hill-climbing with tabu lists may not find a good ordering. Note that the SHD of L1OBS and A* Lasso reported here are the results after applying the introduced pruning method. We observe that the SHDs are greatly improved after pruning. For example, the SHDs of L1OBS decrease from 171.6 (29.5), 588.0 (66.2) and 1964.5 (136.6) to 85.2 (23.8), 215.4 (26.3) and 481.2 (49.9) for ER2 graphs with 30, 50 and 100 nodes, respectively, while the TPRs almost keep the same.</p><p>We have also evaluated our method on 150-node LG data models on ER2 graphs. CORL-1 has TPR and SHD being 0.95 (0.01) and 63.7 (9.1), while CORL-2 has 0.97 (0.01) and 38.3 (14.3), respectively. CORL-2 outperforms NOTEARS that achieves 0.94 (0.02) and 50.8 (21.8).</p><p>Pretraining. We show the training reward curve of CORL-2pretrain in Figure <ref type="figure" target="#fig_1">3</ref>, where the model parameters are pretrained in a supervised manner. The data sets used for pretraining contain 30-node ER2 and SF2 graphs with different causal relationships. Note that the data sets used for evaluation are different from those used for pretraining. Compared to that of CORL-2 using random initialization, a pretrained model can accelerate the model learning process. Although pretraining requires additional time, it is only carried out once and when finished, the pretrained model can be used for multiple causal discovery tasks. Similar conclusion can be drawn in terms of CORL-1, which is shown in Appendix G.</p><p>Running time. We also report the running time of all the methods on 30and 100-node linear data models: CORL-1, CORL-2, GraN-DAG and DAG-GNN ≈ 15 minutes for 30-node graphs; CORL-1 and CORL-2 ≈ 7 hours against GraN-DAG and DAG-GNN ≈ 4 hours for 100-node graphs; CAM ≈ 15 minutes for both 30and 100-node graphs, while L1OBS and A* Lasso ≈ 2 minutes for that tasks; NOTEARS ≈ 5 minutes and ≈ 1 hour for the two tasks respectively; RL-BIC2 ≈ 3 hours for 30-node graphs. We set the maximal running time up to 15 hours, but RL-BIC2 did not converge on 100-node graphs, hence we did not report its results. Note that the running time can be significantly reduced by paralleling the evaluation of reward. The neural network based learning methods generally take longer time, and the proposed method achieves the best performance among these methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Non-Linear Model with Gaussian Process</head><p>In this experiment, we consider causal relationships with f j being a function sampled from a Gaussian Process (GP) with radial basis function kernel of bandwidth one. The additive noise follows standard Gaussian distribution, which is known to be identifiable <ref type="bibr" target="#b8">[Peters et al., 2014]</ref>. We consider ER1 and ER4 graphs with different sample numbers (see Appendix B.2 for the generation of data sets), and we only report the results with m = 500 samples due to the space limit (the remaining results are given in Appendix F). For comparison, only the methods that have been shown competitive for this non-linear data model in existing works <ref type="bibr" target="#b13">[Zhu et al., 2020;</ref><ref type="bibr" target="#b6">Lachapelle et al., 2020]</ref> are included. For a given ordering, we follow <ref type="bibr" target="#b13">[Zhu et al., 2020]</ref> to use GP regression to fit the causal relationships. We also set a maximum time limit of 15 hours for all the methods for fair comparison and only graphs with up to 30 nodes are considered here, as using GP regression to calculate the scores is time-consuming. The variable selection method used here is the CAM pruning from <ref type="bibr" target="#b3">[Bühlmann et al., 2014]</ref>.</p><p>The results on 10and 30-node data sets with ER1 and ER4 graphs are shown in Figure <ref type="figure" target="#fig_2">4</ref>. Overall, both GraN-DAG and DAG-GNN perform worse than CAM. We conjecture that this is because the number of samples are not sufficient for GraN-DAG and DAG-GNN to fit neural networks well, as also shown by <ref type="bibr" target="#b6">[Lachapelle et al., 2020]</ref>. CAM, CORL-1, and CORL-2 have similar results, with CORL-2 performing the best on 10-node graphs and being slightly worse than CAM on 30node graphs. All of these methods have better results on ER1 graphs than on ER4 graphs, especially with 30 nodes. We also notice that CORL-2 only runs about 700 iterations on 30-node graphs and about 5000 iterations on 10-node graphs within the time limit, due to the increased time from GP regression. Nonetheless, the proposed method achieves a much improved performance compared with the existing RL-based method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Real Data</head><p>The Sachs data set <ref type="bibr" target="#b8">[Sachs et al., 2005]</ref>, with 11-node and 17-edge true graph, is widely used for research on graphical models. The expression levels of protein and phospholipid in the data set can be used to discover the implicit protein signal network. The observational data set has m = 853 samples and is used to discover the causal structure. We similary use Gaussian Process regression to model the causal relationships in calculating the score. In this experiment, CORL-1, CORL-2 and RL-BIC2 achieve the best SHD 11. CAM, GraN-DAG, and ICA-LiNGAM achieve SHDs 12, 13 and 14, respectively. Particularly, DAG-GNN and NOTEARS result in SHDs 16 and 19, respectively, whereas an empty graph has SHD 17.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this work, we have incorporated RL into the ordering-based paradigm for causal discovery, where a generated ordering can be pruned by variable selection to obtain the causal DAG. Two methods are developed based on the MDP formulation and an encoder-decoder framework. We further analyze the consistency and computational complexity for the proposed approach. Empirical results validate the improved performance over existing RL-based causal discovery approach.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Illustration of the policy model. The encoder embeds the observed data xj into the state sj. An action at can be selected by the decoder according to the given state ŝt and the pointer mechanism at each time step t. Note that T = d -1. See Appendix A for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Learning curves of CORL-1, CORL-2 and CORL-2pretrain on 100-node LG data sets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The empirical results on GP data models with 10 and 30 nodes.</figDesc><graphic coords="6,119.90,-9593.10,10000.30,10005.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Algorithm 1 Causal discovery with Ordering-based RL. Require: observed data X, initial parameters φ e , φ d and φ v , two empty buffers D and D score , initial value (negative infinite) BestScore and a random ordering BestOrdeing. 1: while not terminated do of data ŝt , a t , r t with π φ : D = D ∪ { ŝt , a t , r t } 5: if v t , Π ≺vt , r t is not in D score then</figDesc><table><row><cell>2:</cell><cell cols="2">draw a batch of samples from X, encode them to S and</cell></row><row><cell></cell><cell cols="2">calculate the initial state ŝ0</cell></row><row><cell>3:</cell><cell cols="2">for t = 0, 1, . . . , T do</cell></row><row><cell cols="3">4: collect a batch 6: store v t , Π ≺vt , r t in D score</cell></row><row><cell>7:</cell><cell></cell><cell>end if</cell></row><row><cell>8:</cell><cell cols="2">end for</cell></row><row><cell>9:</cell><cell cols="2">update φ e , φ d , and φ v as described in Section 4.2</cell></row><row><cell>10:</cell><cell>if</cell><cell>T t=0 r t &gt; BestScore then</cell></row><row><cell>11:</cell><cell></cell><cell>update the BestScore and BestOrdering</cell></row><row><cell>12:</cell><cell cols="2">end if</cell></row><row><cell cols="3">13: end while</cell></row><row><cell cols="3">14: get the final DAG by pruning the BestOrdering</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Empirical results for ER and SF graphs of 30 and 100 nodes with LG data.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>RANDOM</cell><cell>NOTEARS</cell><cell>DAG-GNN</cell><cell>RL-BIC2</cell><cell>L1OBS</cell><cell>A* Lasso</cell><cell>CORL-1</cell><cell>CORL-2</cell></row><row><cell></cell><cell>ER2</cell><cell>TPR SHD</cell><cell>0.41 (0.04) 140.4 (36.7)</cell><cell>0.95 (0.03) 14.2 (9.4)</cell><cell>0.91 (0.05) 26.5 (12.4)</cell><cell>0.94 (0.05) 17.8 (22.5)</cell><cell>0.78 (0.06) 85.2 (23.8)</cell><cell>0.88 (0.04) 35.3 (14.3)</cell><cell>0.99 (0.02) 5.2 (7.4)</cell><cell>0.99 (0.01) 4.4 (3.5)</cell></row><row><cell>30 nodes</cell><cell>ER5</cell><cell>TPR SHD</cell><cell>0.43 (0.03) 210.2 (43.5)</cell><cell>0.93 (0.01) 35.4 (7.3)</cell><cell>0.85 (0.11) 68.0 (39.8)</cell><cell>0.91 (0.03) 45.6 (13.3)</cell><cell>0.74 (0.04) 98.6 (32.7)</cell><cell>0.84 (0.05) 71.2 (21.5)</cell><cell>0.94 (0.03) 37.4 (16.9)</cell><cell>0.95 (0.03) 37.6 (14.5)</cell></row><row><cell></cell><cell>SF2</cell><cell>TPR SHD</cell><cell>0.58 (0.02) 118.4 (12.3)</cell><cell>0.98 (0.02) 6.1 (2.3)</cell><cell>0.92 (0.09) 36.8 (33.1)</cell><cell>0.99 (0.02) 3.2 (1.7)</cell><cell>0.83 (0.04) 49.7 (28.1)</cell><cell>0.93 (0.02) 27.3 (18.4)</cell><cell>1.0 (0.01) 0.0 (0.0)</cell><cell>1.0 (0.01) 0.0 (0.0)</cell></row><row><cell></cell><cell>SF5</cell><cell>TPR SHD</cell><cell>0.44 (0.03) 165.4 (10.6)</cell><cell>0.94 (0.03) 23.3 (6.9)</cell><cell>0.89 (0.09) 47.8 (35.2)</cell><cell>0.96 (0.03) 11.3 (5.2)</cell><cell>0.79 (0.04) 89.3 (25.7)</cell><cell>0.88 (0.03) 40.5 (19,8)</cell><cell>1.00 (0.00) 0.0 (0.0)</cell><cell>1.00 (0.00) 0.0 (0.0)</cell></row><row><cell></cell><cell>ER2</cell><cell>TPR SHD</cell><cell>0.33 (0.05) 491.4 (17.6)</cell><cell>0.93 (0.02) 72.6 (23.5)</cell><cell>0.93 (0.03) 66.2 (19.2)</cell><cell>0.02 (0.01) 270.8 (13.5)</cell><cell>0.54 (0.02) 481.2 (49.9)</cell><cell>0.86 (0.04) 128.5 (38.4)</cell><cell>0.98 (0.02) 24.8 (10.1)</cell><cell>0.98 (0.01) 18.6 (5.7)</cell></row><row><cell>100 nodes</cell><cell>ER5</cell><cell>TPR SHD</cell><cell>0.34 (0.04) 984.4 (35.7)</cell><cell>0.91 (0.01) 170.3 (34.2)</cell><cell>0.86 (0.16) 236.4 (36.8)</cell><cell>0.08 (0.03) 421.2 (46.2)</cell><cell>0.53 (0.02) 547.9 (63.4)</cell><cell>0.82 (0.05) 244.0 (42.3)</cell><cell>0.93 (0.02) 175.3 (18.9)</cell><cell>0.94 (0.03) 164.8 (17.1)</cell></row><row><cell></cell><cell>SF2</cell><cell>TPR SHD</cell><cell>0.48 (0.03) 503.4 (23.8)</cell><cell>0.98 (0.01) 2.3 (1.3)</cell><cell>0.89 (0.14) 156.8 (21.2)</cell><cell>0.04 (0.02) 281.2 (17.4)</cell><cell>0.57 (0.03) 377.3 (53.4)</cell><cell>0.92 (0.03) 54.0 (22.3)</cell><cell>1.00 (0.00) 0.0 (0.0)</cell><cell>1.00 (0.00) 0.0 (0.0)</cell></row><row><cell></cell><cell>SF5</cell><cell>TPR SHD</cell><cell>0.47 (0.04) 891.3 (19.4)</cell><cell>0.95 (0.01) 90.2 (34.5)</cell><cell>0.87 (0.15) (22.0)</cell><cell>0.05 (0.03) 405.2 (77.4)</cell><cell>0.55 (0.04) 503.7 (56.4)</cell><cell>0.89 (0.03) 114.0 (36.4)</cell><cell>0.97 (0.02) 19.4 (5.2)</cell><cell>0.98 (0.01) 10.8 (6.1)</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence </p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p><rs type="person">Xiaoqiang Wang</rs> and <rs type="person">Liangjun Ke</rs> were supported by the <rs type="funder">National Natural Science Foundation of China</rs> under Grant <rs type="grantNumber">61973244</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_48ZRtnA">
					<idno type="grant-number">61973244</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Integer linear programming for the bayesian network structure learning problem</title>
		<author>
			<persName><forename type="first">Cussens</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Cussens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">244</biblScope>
			<biblScope unit="page" from="258" to="271" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><surname>Bello</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.09940</idno>
		<title level="m">Neural combinatorial optimization with reinforcement learning</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Ordering-based causal structure learning in the presence of latent variables</title>
		<author>
			<persName><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics (AISTATS)</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="4098" to="4108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Chickering, 2002] David Maxwell Chickering. Optimal structure identification with greedy search</title>
		<author>
			<persName><surname>Bühlmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Learning from Data: Artificial Intelligence and Statistics V</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1996-11">2014. 2014. 1996. Nov. 2002</date>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="507" to="554" />
		</imprint>
	</monogr>
	<note>The Annals of Statistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Efficient structure learning of bayesian networks using constraints</title>
		<author>
			<persName><forename type="first">De</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji ; Cassio P De</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="663" to="689" />
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Being Bayesian about network structure. A Bayesian approach to structure discovery in Bayesian networks</title>
		<author>
			<persName><forename type="first">Koller</forename><surname>Friedman</surname></persName>
			<affiliation>
				<orgName type="collaboration">Max Welling</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Nir</forename><surname>Friedman</surname></persName>
			<affiliation>
				<orgName type="collaboration">Max Welling</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Daphne</forename><surname>Koller</surname></persName>
			<affiliation>
				<orgName type="collaboration">Max Welling</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Khalil</surname></persName>
			<affiliation>
				<orgName type="collaboration">Max Welling</orgName>
			</affiliation>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<publisher>Herke Van Hoof</publisher>
			<date type="published" when="2003">2003. 2003. 2017. 2017. 2019</date>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="95" to="125" />
		</imprint>
	</monogr>
	<note>Learning combinatorial optimization algorithms over graphs. Kool et al., 2019. Attention, learn to solve routing problems! In International Conference on Learning Representations (ICLR</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning bayesian network structures by searching for the best ordering with genetic algorithms</title>
		<author>
			<persName><surname>Lachapelle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="1996">2020. 2020. 1996. 1996</date>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="487" to="493" />
		</imprint>
	</monogr>
	<note>Gradient-based neural DAG learning</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Identifiability of gaussian structural equation models with equal error variances</title>
		<author>
			<persName><surname>Ng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.08527</idno>
		<idno>arXiv:1911.07420</idno>
	</analytic>
	<monogr>
		<title level="m">A graph autoencoder approach to causal structure learning</title>
		<imprint>
			<publisher>Jonas Peters and Peter Bühlmann</publisher>
			<date type="published" when="2014">2019. 2019. 2019. 2019. 2014. 2014</date>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page" from="219" to="228" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Masked gradient-based causal structure learning</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Causal protein-signaling networks derived from multiparameter single-cell data</title>
		<author>
			<persName><surname>Peters</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.03530</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</title>
		<editor>
			<persName><surname>Shimizu</surname></persName>
		</editor>
		<meeting>the AAAI Conference on Artificial Intelligence (AAAI)<address><addrLine>Uhler</addrLine></address></meeting>
		<imprint>
			<publisher>MIT press</publisher>
			<date type="published" when="2003">2014. January 2014. 2018. 2018. 2005. 2005. 2015. 2015. 2007. 2007. Oct. 2003-2030, 2006. 2017. 2017. 2018</date>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="523" to="529" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Consistency guarantees for greedy permutationbased causal inference algorithms. Sutton and Barto, 2018. Reinforcement Learning: An Introduction</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Ordering-based search: A simple and effective algorithm for learning bayesian networks</title>
		<author>
			<persName><forename type="first">Koller</forename><surname>Teyssier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Teyssier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2005">2005. 2005. 2017</date>
		</imprint>
	</monogr>
	<note>Conference on Uncertainty in Artificial Intelligence (UAI)</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Xiang and Kim, 2013] Jing Xiang and Seyoung Kim. A* lasso for learning a sparse bayesian network structure for continuous variables</title>
		<author>
			<persName><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2013">2015. 2015. 2013</date>
		</imprint>
	</monogr>
	<note>Advances in Neural Information Processing Systems (NeurIPS)</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">DAG-GNN: DAG structure learning with graph neural networks</title>
		<author>
			<persName><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">DAGs with NO TEARS: Continuous optimization for structure learning</title>
		<author>
			<persName><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics (AISTATS)</title>
		<editor>
			<persName><forename type="first">Chen</forename><surname>Zheng</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Bryon</forename><surname>Dan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Pradeep</forename><surname>Aragam</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Ravikumar</surname></persName>
		</editor>
		<editor>
			<persName><surname>Xing</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2018">2018. 2018. 2020</date>
		</imprint>
	</monogr>
	<note>Advances in Neural Information Processing Systems (NeurIPS)</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Causal discovery with reinforcement learning</title>
		<author>
			<persName><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Neural architecture search with reinforcement learning</title>
		<author>
			<persName><forename type="first">Le</forename><forename type="middle">;</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
