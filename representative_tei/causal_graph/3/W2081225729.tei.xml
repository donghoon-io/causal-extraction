<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Sparse Causal Gaussian Networks With Experimental Intervention: Regularization and Coordinate Descent</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Fei</forename><surname>Fu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Qing</forename><surname>Zhou</surname></persName>
						</author>
						<title level="a" type="main">Learning Sparse Causal Gaussian Networks With Experimental Intervention: Regularization and Coordinate Descent</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1080/01621459.2012.754359</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:20+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Adaptive lasso</term>
					<term>Experimental data</term>
					<term>L 1 regularization</term>
					<term>Penalized likelihood</term>
					<term>Structure learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Causal networks are graphically represented by directed acyclic graphs (DAGs). Learning causal networks from data is a challenging problem due to the size of the space of DAGs, the acyclicity constraint placed on the graphical structures, and the presence of equivalence classes. In this article, we develop an L 1 -penalized likelihood approach to estimate the structure of causal Gaussian networks. A blockwise coordinate descent algorithm, which takes advantage of the acyclicity constraint, is proposed for seeking a local maximizer of the penalized likelihood. We establish that model selection consistency for causal Gaussian networks can be achieved with the adaptive lasso penalty and sufficient experimental interventions. Simulation and real data examples are used to demonstrate the effectiveness of our method. In particular, our method shows satisfactory performance for DAGs with 200 nodes, which have about 20,000 free parameters. Supplementary materials for this article are available online.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Conditional independence structures among random variables are often visualized as graphical models, where the nodes represent the variables and the edges encode the relationships among them. Depending on whether the edges are directional or not, graphical models can be classified as either directed or undirected. The Bayesian network (BN) is a special type of graphical models, whose structure is represented by a directed acyclic graph <ref type="bibr">(DAG)</ref>. It has become a popular probabilistic model in many research areas, including computational biology, medical sciences, image processing, speech recognition, etc.</p><p>Learning the structure of a BN from data is an important and challenging problem in statistics. The major difficulty lies in the fact that the number of DAGs grows superexponentially in the number of nodes <ref type="bibr" target="#b23">(Robinson 1973)</ref>. A substantial amount of research has been devoted to the structure learning problem of BNs and many methods have been proposed. These methods can be roughly classified into two primary approaches. The constraintbased approach relies on a set of conditional independence tests. A well-known example in this category is the Peter-Clark (PC) algorithm proposed by <ref type="bibr" target="#b26">Spirtes, Glymour, and Scheines (1993)</ref>. Recently, <ref type="bibr" target="#b15">Kalisch and Bühlmann (2007)</ref> considered the problem of estimating BNs with the PC algorithm and proposed an efficient implementation suitable for sparse high-dimensional DAGs. The second approach to learning BNs is score based, which attempts to find a DAG that maximizes some scoring function through a certain search strategy <ref type="bibr" target="#b1">(Cooper and Herskovits 1992;</ref><ref type="bibr" target="#b18">Lam and Bacchus 1994;</ref><ref type="bibr" target="#b14">Heckerman, Geiger, and Chickering 1995)</ref> or sample DAGs from a Bayesian posterior distribution <ref type="bibr" target="#b20">(Madigan and York 1995;</ref><ref type="bibr" target="#b11">Friedman and Koller 2003;</ref><ref type="bibr" target="#b5">Ellis and Wong 2008;</ref><ref type="bibr" target="#b32">Zhou 2011)</ref>. Many algorithms in this category work well for graphs that do not have a large num-Fei Fu (E-mail: fufei05@hotmail.com) is Ph.D. Student, and Qing Zhou (E-mail: zhou@stat.ucla.edu) is Associate Professor, Department of Statistics, University of California, Los Angeles, CA 90095. This work was supported by the National Science Foundation grant DMS-1055286 to Q.Z. The authors thank the editor, the associate editor, and two referees for helpful comments and suggestions, which significantly improved the article. ber of nodes. However, due to the size of the space of DAGs, they become computationally impractical for large networks.</p><p>In recent years, a number of researchers proposed to estimate the structure of graphical models through L 1 -regularized likelihood approaches (lasso-type penalties). The L 1 penalty becomes popular because of the parsimonious solution it leads to as well as its computational tractability. Much of the research has focused on estimating undirected graphs with the L 1 penalty. <ref type="bibr" target="#b31">Yuan and Lin (2007)</ref> proposed to maximize an L 1 -penalized log-likelihood based on the "max-det" problem considered by <ref type="bibr" target="#b29">Vandenberghe, Boyd, and Wu (1998)</ref>, while <ref type="bibr" target="#b0">Banerjee, El Ghaoui, and d'Aspremont (2008)</ref> employed a blockwise coordinate descent (CD) algorithm to solve the optimization problem. <ref type="bibr" target="#b9">Friedman, Hastie, and Tibshirani (2008)</ref> built on the method of <ref type="bibr" target="#b0">Banerjee, El Ghaoui, and d'Aspremont (2008)</ref> a remarkably efficient algorithm called the graphical lasso. Another computationally attractive method was developed by <ref type="bibr" target="#b21">Meinshausen and Bühlmann (2006)</ref>, where an undirected graph is constructed by fitting a lasso regression for each node separately.</p><p>Compared to undirected graphs, BNs have an attractive property: they can be used to represent causal relationships among random variables. Although some authors discussed the possibility of causal inference from observational data <ref type="bibr" target="#b26">(Spirtes, Glymour, and Scheines 1993;</ref><ref type="bibr" target="#b22">Pearl 2000)</ref>, most researchers agree that causal relations can only be reliably inferred using experimental data. Experimental interventions reveal causality among a set of variables by breaking down various connections in the underlying causal network. As for undirected graphs, sparsity in the structure of a causal BN is desired, which often gives more interpretable results. A natural generalization is to use the L 1 penalty in structure learning of causal BNs with experimental data. However, there are a number of difficulties for this seemingly natural generalization. First, existing theories on L 1 -regularized estimation and penalized likelihood may not be directly applicable to structure learning of DAGs with interventional data. Different interventions effectively change the structure of a DAG as shown in Section 2. Second, it is expected that the computation for estimating the structure of DAGs is much more challenging than that for undirected graphs because of the acyclicity constraint. Indeed, the recent work of <ref type="bibr" target="#b25">Shojaie and Michailidis (2010)</ref> assumed a known ordering of the variables to simplify the computation for the structure learning problem of DAGs, which eliminates the need for estimating the directions of causality among random variables.</p><p>In this article, we develop an L 1 -penalized likelihood approach to structure learning of causal Gaussian Bayesian networks (GBNs) using experimental data. We consider this problem in the general setting where the ordering of the variables is unknown. To the best of our knowledge, this is the first method that estimates the structure of DAGs based on L 1 -penalized likelihood without assuming a known ordering. In Section 2, we formulate the problem of learning causal DAGs with experimental data. We develop a CD algorithm in Section 3 to search for a locally optimal solution to this optimization problem and establish in Section 4 theoretical properties of the corresponding estimator. In Section 5 we present results of a simulation study, and in Section 6, we apply our method to a real dataset. The article is concluded with discussion in Section 7. All proofs are provided in the Appendix or the online supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">PROBLEM FORMULATION</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Causal Bayesian Networks</head><p>The joint probability distribution of a set of random variables X 1 , . . . , X p in a BN can be factorized as</p><formula xml:id="formula_0">P (X 1 , . . . , X p ) = p i=1 P X i | G i ,<label>(1)</label></formula><p>where G i ⊆ {X 1 , . . . , X p }\{X i } is called the set of parents of X i . If X i does not have any parents, then G i = ∅. We can construct a DAG G = (V , E) to represent the structure of a BN. Here, V = {1, . . . , p} denotes the set of nodes in the graph, where the ith node in V corresponds to X i . For simplicity, we use X i and i interchangeably throughout the article to represent the ith node. The set of edges E = {(i, j ) : X i ∈ G j } and an edge (i, j ) ∈ E is written as i → j . The structure of G must be acyclic so that (1) is a well-defined joint distribution. For any DAG G, there exists at least one ordering of the nodes, known as a topological sort of G, such that i ≺ j if i ∈ G j . A more convenient representation of the structure of a DAG is the adjacency matrix, a p × p matrix A whose (i, j )th entry is 1 if i → j and 0 otherwise. Estimating the structure of DAGs from data is equivalent to estimating their adjacency matrices.</p><p>For some joint distributions, there exist multiple factorizations of the form in (1). Those DAGs that encode the same set of joint distributions form an equivalence class. We cannot distinguish equivalent DAGs from observational data. However, equivalent DAGs do not have the same causal interpretation. In this article, we only consider using DAGs for causal inference, following Pearl's formulation of causal BNs <ref type="bibr" target="#b22">(Pearl 2000)</ref>. In this setting, experimental interventions can help us distinguish equivalent DAGs. For instance, consider the causal interpretations of two equivalent DAGs G 1 :</p><formula xml:id="formula_1">X 1 → X 2 and G 2 : X 1 ← X 2 .</formula><p>Suppose that X 2 is fixed experimentally at x 2 (the fixed value itself might be drawn from some distribution independent of the DAG). If G 1 is the true causal model, fixing X 2 eliminates any dependency of X 2 on X 1 , in effect removing the directed edge from X 1 to X 2 . Thus, data generated in this manner follow the joint distribution P (X 1 , X 2 ) = P (X 1 |∅)P (X 2 |•), where P (X 1 |∅) is the marginal distribution of X 1 and P (X 2 |•) is the distribution from which experimental data on X 2 are drawn. On the other hand, if the true causal model is G 2 , interventions on X 2 leave the dependency between X 1 and X 2 intact. Hence, experimental data can be used to infer causal relationships among random variables. As this example demonstrates, if X i (i ∈ M) are under intervention, then the joint distribution in (1) becomes</p><formula xml:id="formula_2">P (X 1 , . . . , X p ) = i / ∈M P X i | G i i∈M P (X i |•),<label>(2)</label></formula><p>where P (X i |•) denotes the distribution of X i under intervention.</p><p>In other words, we can view experimental data from G as being generated from the DAG G obtained by removing all directed edges pointing to the nodes under intervention in G. When we make causal inference using the likelihood function (2), the term i∈M P (X i |•) can be ignored, since they depend only on external parameters that are not relevant to the estimation of DAGs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">L 1 -Regularized Log-Likelihood</head><p>In a causal GBN G, causal relationships among random variables are modeled as</p><formula xml:id="formula_3">X j = i∈ G j β ij X i + ε j , j = 1, . . . , p,<label>(3)</label></formula><p>where β ij is the coefficient representing the influence of X i on X j , and ε j 's are independent Gaussian noise variables with mean 0 and variance σ 2 j . We assume, throughout this article, that all X j have mean 0. Then the joint distribution of (X 1 , . . . , X p ) defined by (3) is N p (0, ), where the covariance matrix depends on β ij (i, j = 1, . . . , p and i = j ) and σ 2 j (j = 1, . . . , p). The set of equations in (3) can be regarded as the mechanism for generating these random variables.</p><p>Consider an n × p data matrix X generated from G. The data matrix X consists of p blocks with the jth block X j having size n j × p, where n = p j =1 n j . Each row in X j is generated by imposing an intervention on the node X j , while the values for all other nodes X k (k = j ) are observational. The experimental data on X j generated by intervention are assumed to follow N (0, σ 2 j ) for j = 1, . . . , p. Let B = (β ij ) p×p be the coefficient matrix, where</p><formula xml:id="formula_4">β ij = 0 if i / ∈ G j .</formula><p>Let σ 2 = (σ 2 j ) 1×p and σ 2 = ( σ 2 j ) 1×p be vectors of variances. Apparently, we can learn the structure of G by estimating the coefficient matrix B. In the rest of the article, we will call G the graph induced by B.</p><p>Let I j denote the collection of the row indices of X j , and then O j = {1, . . . , n} \I j gives the collection of data points in which X j is not fixed experimentally, j = 1, . . . , p. According to the factorization (2), the likelihood of the data matrix X can be written as</p><formula xml:id="formula_5">f (X) ∝ p k=1 h∈I k j =k f (x hj |π hj ) = p j =1 h∈O j f (x hj |π hj ), (4)</formula><p>where x hj is the value of X j in the hth data point [the (h, j )th element of the data matrix X], π hj is the value of its parents, and f (x hj |π hj ) is the conditional density of x hj given π hj . Note that, as mentioned in Section 2.1, the likelihood term f (x hj |•) is ignored if the value x hj is fixed experimentally. Let n -j = |O j | = nn j . Using the relationship in (3), we can easily derive that the negative log-likelihood of B and σ 2 is</p><formula xml:id="formula_6">p j =1 n -j log σ 2 j 2 + X [O j ,j ] -X [O j ,-j ] B [-j,j ] 2 2σ 2 j ,<label>(5)</label></formula><p>where M [I r ,I c ] denotes the submatrix of M with rows in I r and columns in I c . For many applications, it is often the case that the underlying network structure is sparse. It is therefore important to find a sparse structure for the coefficient matrix B. We propose here a penalized likelihood approach with the adaptive lasso penalty to learn the structure of B. Specifically, given a weight matrix W = (w ij ) p×p , we seek the minimizer ( B, σ 2 ) of</p><formula xml:id="formula_7">p j =1 ⎡ ⎣ n -j log σ 2 j 2 + X [O j ,j ] -X [O j ,-j ] B [-j,j ] 2 2σ 2 j + λ i =j w ij |β ij | ⎤ ⎦ , subject to G B is acyclic, (6)</formula><p>where G B denotes the graph induced by B and λ &gt; 0 is the penalty parameter.</p><p>Remark 1. Due to the acyclicity constraint, one cannot transform (6) into an equivalent penalized least squares problem. Moreover, σ 2 j cannot be ignored in our formulation, which makes the minimization problem considerably harder than a penalized least squares problem.</p><p>The adaptive lasso was proposed by <ref type="bibr" target="#b33">Zou (2006)</ref> as an alternative to the lasso technique <ref type="bibr" target="#b27">(Tibshirani 1996)</ref> for regression problems. The adaptive lasso enjoys the oracle properties considered by <ref type="bibr" target="#b7">Fan and Li (2001)</ref>. In particular, it is consistent for variable selection. In our setting, the weights are defined as <ref type="bibr" target="#b33">Zou (2006)</ref> suggested using the ordinary least squares (OLS) estimates to define the weights in the regression setting. However, because of the existence of equivalent DAGs, the OLS estimates may not be consistent in our case. To obtain the initial consistent estimates β( †) ij 's, we let</p><formula xml:id="formula_8">w ij = | β( †) ij | -γ for some γ &gt; 0, where β( †) ij is a consistent estimate of β ij .</formula><formula xml:id="formula_9">w ij = min(| β(OLS) ij | -γ , M γ )</formula><p>, where M is a large positive constant (e.g., M = 10 4 ) and β(OLS) ij 's are the OLS estimates obtained by regressing X j on other nodes using data in O j . As will be shown in Section 4, there exists a √ n-consistent local minimizer B of (6) with the weights w ij , which can be used as β( †) ij 's. Then, a consistent estimate of the graph structure can be obtained with weights</p><formula xml:id="formula_10">w ij = | β( †) ij | -γ .</formula><p>After minimizing with respect to σ 2 , problem (6) becomes</p><formula xml:id="formula_11">min B V (B; W) = p j =1 ⎡ ⎣ n -j 2 log X [O j ,j ] -X [O j ,-j ] B [-j,j ] 2 + λ i =j w ij |β ij | ⎤ ⎦ , subject to G B is acyclic, (7)</formula><p>which is the problem we aim to solve.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">COORDINATE DESCENT ALGORITHM</head><p>Both the objective function V in (7) and the constraint set are nonconvex. Searching for the global minimizer of (7) may be impractical. Moreover, the theoretical results in Section 4 only establish consistency for a local minimizer (see Theorems 2 and 3). Therefore, we develop in this section a CD algorithm to find a local minimum to the constrained optimization problem (7). A local minimizer B is defined as follows: (i) any local change in the structure of G B , that is, addition, removal, or reversal of a single edge, increases the value of V and (ii) given the structure of G B , B is a local minimizer of V. CD methods have been successfully used to solve lasso-type problems <ref type="bibr" target="#b12">(Fu 1998;</ref><ref type="bibr" target="#b8">Friedman et al. 2007;</ref><ref type="bibr" target="#b30">Wu and Lange 2008)</ref>. They are attractive since minimizing the objective function one coordinate at a time is computationally simple and gradient free. As a result, these methods are easy to implement and are usually scalable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Single-Parameter Update</head><p>Before detailing the CD algorithm, let us first consider minimizing V in (7) with respect to a single parameter β kj (k = j ) without the acyclicity constraint. In particular, we seek the minimizer βkj of</p><formula xml:id="formula_12">V j = n -j 2 log X [O j ,j ] -X [O j ,-j ] B [-j,j ] 2 + λ i =j w ij |β ij | = n -j 2 log ⎡ ⎢ ⎣ h∈O j ⎛ ⎝ x hj - i =j,k x hi β ij -x hk β kj ⎞ ⎠ 2 ⎤ ⎥ ⎦ + λ i =j,k w ij |β ij | + λw kj |β kj |,<label>(8)</label></formula><p>assuming all β ij 's (i = j, k) are fixed. We can transform the weighted lasso penalty in (8) into an ordinary lasso penalty:</p><formula xml:id="formula_13">min βkj V j = n -j 2 log ⎡ ⎢ ⎣ h∈O j ⎛ ⎝ x hj - i =j,k xhi βij -xhk βkj ⎞ ⎠ 2 ⎤ ⎥ ⎦ + λ i =j,k | βij | + λ| βkj |,<label>(9)</label></formula><p>by letting βij = w ij β ij and xhi = x hi /w ij for i = j . We further define</p><formula xml:id="formula_14">y (k) hj = x hj -i =j,k xhi βij , ξ kj = h∈O j xhk y (k) hj / h∈O j x2 hk , c kj = h∈O j (y (k)</formula><p>hj ) 2 / h∈O j x2 hk , and η = λ/n -j . Note that according to Cauchy-Schwarz inequality, c kjξ 2 kj ≥ 0. Then equivalently, ( <ref type="formula" target="#formula_13">9</ref>) can be simplified to the problem</p><formula xml:id="formula_15">min βkj g( βkj ) = 1 2 log ( βkj -ξ kj ) 2 + c kj -ξ 2 kj + η| βkj |.<label>(10)</label></formula><p>The form of g is reminiscent of the lasso problem with a single predictor. However, minimizing g with respect to βkj is not as easy as the corresponding lasso problem, since g is not a convex function and might have two local minima for some values of ξ kj , c kj , and η. It is therefore necessary to compare the two local minima under certain conditions. We summarize the solution to (10) in the following proposition and provide its proof in the online supplementary materials.</p><formula xml:id="formula_16">Proposition 1. Let = 1 -4(c kj -ξ 2 kj )η 2 and β * 1 = sgn (ξ kj )(|ξ kj | -1- √ 2η</formula><p>). The solution to the optimization problem ( <ref type="formula" target="#formula_15">10</ref>) is given by</p><formula xml:id="formula_17">arg min βkj g = ⎧ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎨ ⎪ ⎪ ⎪ ⎪ ⎪ ⎪ ⎩ β * 1 , if 0 &lt; η &lt; |ξ kj |/c kj , β * 1 , if |ξ kj |/c kj ≤ η &lt; 2 c kj -ξ 2 kj -1 , η &gt; (2|ξ kj |) -1 and g(β * 1 ) &lt; g(0), 0,</formula><p>otherwise.</p><p>Remark 2. The form of β * 1 suggests that arg min βkj g is similar to a soft thresholded version <ref type="bibr" target="#b2">(Donoho and Johnstone 1995)</ref> of ξ kj in nature. One difference, however, is that arg min βkj g can be zero even when</p><formula xml:id="formula_18">|ξ kj | -(1 - √ )(2η) -1 &gt; 0 (see the proof of Proposition 1 in the online supplementary materials). Note that if 4(c kj -ξ 2 kj )η 2 = o(1), by Taylor expansion √ ≈ 1 -2(c kj -ξ 2 kj )η 2 . Then β * 1 ≈ sgn(ξ kj )(|ξ kj | -(c kj -ξ 2 kj )η) = sgn(ξ kj )(|ξ kj | -c kj (1 -ζ 2 )η)</formula><p>, where ζ is the correlation coefficient between xhk and y (k)  hj for h ∈ O j . Remark 3. In Proposition 1, we could find a more explicit condition on η to determine when g(β * 1 ) &lt; g( <ref type="formula">0</ref>), but the condition does not have a closed-form expression. Thus, it seems more effective to compare g(β * 1 ) and g(0) directly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Description of the CD Algorithm</head><p>The difficulty in minimizing V in ( <ref type="formula">7</ref>) is due to the constraint that the graphical representation of BNs is acyclic. One immediate consequence of this constraint is that a pair of coefficients β ij and β ji cannot both be nonzero. We thus take advantage of this implication when designing the CD algorithm. Instead of minimizing V over a single parameter β ij at each step, we perform minimization over β ij and β ji simultaneously. Hence, our method can be naturally described as a blockwise CD method. For a p-node problem, the p(p -1) coefficients are partitioned into p(p -1)/2 blocks. Each block consists of a pair of coefficients β ij and β ji . The algorithm starts with an initial estimate of the coefficient matrix B (for instance, the zero matrix) and assumes a predefined order to cycle through the p(p -1)/2 blocks. At each step, V is minimized over a certain block of β ij and β ji while all other blocks are held constant. Given the current estimates of other blocks, β ij (or β ji ) is constrained to zero if a nonzero value introduces cycles in the resulting graph. In this case, V is only minimized over β ji (or β ij ). Otherwise, the algorithm compares min β ij ,β ji =0 V with min β ij =0 ,β ji V to update β ij and β ji . We repeat cycling through the p(p -1)/2 blocks until some stopping criterion is satisfied.</p><p>The major steps in the CD algorithm are summarized as follows, where we use βij ⇐ 0 to mean that βij must be set to zero due to the acyclicity constraint. In the following, different X [O j ,•] 's are treated as different entities so that operations on</p><formula xml:id="formula_19">X [O j ,•] will not affect X [O k ,•] for k = j .</formula><p>Algorithm 1. CD Algorithm for Estimating DAGs 1. Center and standardize the columns of X [O j ,•] (j = 1, . . . , p) to have mean zero and unit L 2 norm. Transform the weighted lasso problem ( <ref type="formula">7</ref>) to an ordinary lasso problem by defining</p><formula xml:id="formula_20">X [O j ,i] = X [O j ,i] /w ij , i = j , for j = 1, . . . , p. Choose B 0 such that G B 0 is acyclic. 2. Cycle through the p(p -1)/2 blocks of coefficients.</formula><p>Specifically, do one of the following for the pair of coefficients βij and βji (i &lt; j), given the current estimates of other coefficients.</p><p>(a) If βji ⇐ 0, minimize V j in ( <ref type="formula" target="#formula_13">9</ref>) with respect to βij according to Proposition 1 and find β *</p><formula xml:id="formula_21">ij = arg min βij V j . Then set ( βij , βji ) = ( β * ij , 0). (b) If βij ⇐ 0, minimize V i with respect to βji according to Proposition 1 and find β * ji = arg min βji V i . Then set ( βij , βji ) = (0, β * ji ). (c) If 2(a)</formula><p>and 2(b) do not apply, then compare the following two sums:</p><formula xml:id="formula_22">S 1 = V i | βji =0 + V j | βij = β * ij and S 2 = V i | βji = β * ji + V j | βij =0 . Set ( βij , βji ) = ( β * ij , 0) if S 1 ≤ S 2 . Otherwise, set ( βij , βji ) = (0, β * ji ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Repeat</head><p>Step 2 until the maximum absolute difference among all coefficients between successive cycles is below some threshold or until the maximum number of iterations is reached. 4. Output the estimates βij = βij /w ij for i, j = 1, . . . , p and i = j .</p><p>To ensure the acyclicity constraint by checking whether βij ⇐ 0, we employ a breadth-first search algorithm based on algorithm 4 in <ref type="bibr" target="#b4">Ellis (2006)</ref>. A detailed description of this algorithm is given in the online supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Practical Considerations</head><p>Since it is difficult in practice to predetermine the optimal value of λ, we compute solutions using a decreasing sequence of values for λ, following the practice in <ref type="bibr">Friedman, Hastie, and Tibshirani (2010)</ref>. The solution for the current λ is used as the initial estimate for the next value of λ in the sequence. Since large values of λ force many β ij to be zero and make the optimization much easier, the solution for large λ is likely to agree well with some sub-graph of the true model. Therefore, employing warm starts may boost the performance of the CD algorithm.</p><p>To speed up the CD algorithm, we use an active set method that is better suited for warm starts, as was done by <ref type="bibr">Friedman, Hastie, and Tibshirani (2010)</ref>. The algorithm first performs a complete cycling through all p(p -1)/2 blocks of coefficients to identify the active set-the set of blocks with a nonzero coefficient. We then only iterate over the active set until the maximum coefficient difference falls below the threshold or the maximum number of iterations has been reached. The algorithm stops if another full cycle through all the blocks does not change the active set; otherwise the above process is repeated. Note that when the active set changes, the skeleton of the estimated DAG is updated. Furthermore, when the algorithm iterates over a given active set of blocks, the edges may still be reversed.</p><p>It should be noted that convergence of CD methods often requires the objective function to be strictly convex and differentiable. For nondifferentiable functions, CD may get stuck at nonoptimal points, although <ref type="bibr" target="#b28">Tseng (2001)</ref> considered generalizations to nondifferentiable functions with certain separability and regularity properties. Because of the nonconvex nature of the objective function V in (7) and the constraint set, convergence of the CD algorithm deserves a rigorous investigation, which is beyond the scope of this study. We conjecture that the CD algorithm converges under certain conditions. In practice, we have never encountered any examples so far where the algorithm does not converge. For a demonstration of convergence of our algorithm, see Figure <ref type="figure" target="#fig_2">S2</ref> in the online supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Choice of the Tuning Parameter</head><p>The graphical model learned by the CD algorithm depends on the choice of the penalty λ. Model selection is usually based on an estimated prediction error, and commonly used model selection methods include the Bayesian information criterion (BIC) and cross-validation (CV) among others. As established by <ref type="bibr" target="#b21">Meinshausen and Bühlmann (2006)</ref> for L 1 -penalized linear regression, a model selected based on minimizing the prediction error is often too complex compared to the true model. Figure <ref type="figure" target="#fig_0">1</ref>(a) plots the five-fold CV error for a sequence of graphs learned given a decreasing sequence of λ from a simulated dataset with p = 100, n = 500, and β ij = 1.0. The CV error is minimized at the 67th λ. The corresponding graph Ĝ67 (obtained using λ 67 as the tuning parameter on the whole dataset) has a total of 993 predicted edges with an 82.6% false discovery rate, while the true graph only has 200 directed edges. Similar results are obtained if we use BIC or other scoring metrics such as the Bayesian score of a graph.</p><p>In this article, we employ an empirical model selection criterion that works well in practice. Note that as we decrease λ and thus increase model complexity, the log-likelihood of the estimated graph will increase. Denote by B λ i the solution to (7) with the ith penalty parameter λ i . Given the estimated graph Ĝλ i induced by B λ i , we estimate the unpenalized coefficient matrix, denoted by B i , by regressing X k on Ĝλ i k , k = 1, . . . , p. Given two estimated graphs Ĝλ i and Ĝλ j (λ i &gt; λ j ), let</p><formula xml:id="formula_23">L ij = L( B j ) -L( B i</formula><p>) and e ij = e λ je λ i , where L( B) = -V ( B; 0) denotes the log-likelihood function and e denotes the total number of edges in an estimated graph. We then define the difference ratio between the two estimated graphs as dr (ij ) = L ij / e ij . We reason that an increase in model complexity, which is represented by an increase in the total number of predicted edges, is desirable only if there is a substantial increase in the log-likelihood. Therefore, we compute successively the difference ratios between two adjacent graphs in the solution path, {dr (12) , . . . , dr (m-1,m) }, where m is the number of λ in the sequence. The graph with the following index is selected:</p><formula xml:id="formula_24">K = sup{k : dr (k-1,k) ≥ α × max(dr (12) , . . . , dr (m-1,m) ), k = 2, . . . , m}, (<label>11</label></formula><formula xml:id="formula_25">)</formula><p>where α is a thresholding parameter. Essentially, this is the graph from which further increase in model complexity will not lead to substantial increase in the likelihood. We find that α ∈ [0.05, 0.1] works well in our simulation. Figure <ref type="figure" target="#fig_0">1</ref>(b) plots the difference ratio as well as the log-likelihood for different graphs learned from the same dataset. The graph selected according to (11) with α = 0.05 is Ĝ36 , which has 168 edges with a 77% true positive rate and an 8.3% false discovery rate, much less than 82.6%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">ASYMPTOTIC PROPERTIES</head><p>In this section, we develop asymptotic theories on the penalized likelihood estimator of DAGs. To simplify notations, we write B in a vector format as φ = (φ j ) 1×d = ((B [-1,1] ) T , . . . , (B [-p,p] ) T ), where d = p(p -1) is the length of φ. Similarly, we write the weight matrix W in a vector format as T = (τ j ) 1×d . We say that φ is acyclic if the graph G φ induced by φ (or the corresponding B) is acyclic. Let θ = (φ, σ 2 , σ 2 ) be the vector of parameters and = {θ : φ is acyclic, σ 2 &gt; 0, σ 2 &gt; 0} be the parameter space. Recall that σ 2 = (σ 2 j ) 1×p and σ 2 = ( σ 2 j ) 1×p are vectors of variances defined in Section 2.2. Denote the true parameter value by</p><formula xml:id="formula_26">θ * = (φ * , (σ 2 ) * , ( σ 2 ) * ) ∈ . Let G φ * denote the DAG induced by φ * , that is, the true DAG. Let θ k = (φ k , σ 2 [-k] , σ 2 k )</formula><p>, where φ k is obtained from φ by replacing B <ref type="bibr">[-k,k]</ref> with 0, that is, by suppressing all edges pointing to the kth node from its parents. Here ν <ref type="bibr">[I ]</ref> denotes the subvector of a vector ν with components in I. As mentioned in Section 2.1, X k , the kth block of the data matrix, can be regarded as independent and identically distributed (iid) observations from a distribution factorized according to the DAG G φ k , and we denote the corresponding density by f (x|θ k ), where x = (x 1 , . . . , x p ). For Gaussian random variables, f is the density function of N p (0, (θ k )). Here we emphasize the dependence of the variance-covariance matrix on θ k . Recall that I k denotes the collection of the row indices of X k . Then we define the penalized log-likelihood with the adaptive lasso penalty as</p><formula xml:id="formula_27">R(θ ) = L(θ) -λ n d j =1 τ j |φ j | = p k=1 L k (θ k ) -λ n d j =1 τ j |φ j |,<label>(12)</label></formula><p>where</p><formula xml:id="formula_28">L k (θ k ) = h∈I k log f (X [h,•] |θ k ).</formula><p>Our goal is to seek a local maximizer of R(θ ) in the parameter space to obtain an estimator θ . Note that the log-likelihood function L(θ ) is different from the one in ( <ref type="formula">6</ref>) and ( <ref type="formula">7</ref>), since here we also include in L(θ) terms depending on σ 2 . It is easily seen that these two formulations of the likelihood function are equivalent for the purpose of estimating the coefficients and the structure of BNs.</p><p>Even with interventional data, the coefficient matrix of a DAG may not be identifiable because of interventional Markov equivalence among DAGs <ref type="bibr" target="#b13">(Hauser and Bühlmann 2012)</ref>. We introduce below the notion of natural parameters to establish identifiability of DAGs for the case where each variable has interventional data. Suppose that X i is an ancestor of X j in a DAG G, that is, there exists at least one path from X i to X j (see <ref type="bibr">Lauritzen 1996, chap.</ref> 2, for terminology used in graphical models). Let</p><formula xml:id="formula_29">(i, j ) = {(i 0 , . . . , i m ) : i k → i k+1 for 0 ≤ k ≤ m -1, i 0 = i, i m = j, m ≥ 1} (13)</formula><p>be the set of paths from X i to X j , and define the coefficient of influence of X i on X j by β i→j = (i,j ) m-1 k=0 β i k i k+1 . Denote the set of ancestors of X j by an(X j ).</p><p>Definition 1 (Natural parameters). We say that θ is natural if the corresponding coefficient matrix B satisfies</p><formula xml:id="formula_30">β i→j = 0 for all X i ∈ an(X j ), 1 ≤ j ≤ p. (<label>14</label></formula><formula xml:id="formula_31">)</formula><p>Note that if the underlying DAG is a polytree, the corresponding parameter is always natural. For more general DAGs, a natural parameter implies that the causal effects along multiple paths connecting the same pair of nodes do not cancel, which is a reasonable assumption for many real-world problems. If the true parameter is natural, then with sufficient experimental data, the parameter θ is identifiable as indicated by the following theorem. The proof of Theorem 1 is given in the Appendix.</p><p>Theorem 1. Suppose that X k is an iid sample from the normal distribution N p (0, (θ * k )) with density f (x|θ * k ) for k = 1, . . . , p. Assume that the true parameter θ * is natural. Then</p><formula xml:id="formula_32">f (x|θ k ) = f (x|θ * k ) almost everywhere (a.e.) for all k = 1, . . . , p =⇒ θ = θ * . (<label>15</label></formula><formula xml:id="formula_33">)</formula><p>If we further assume that n k /n → α k &gt; 0 as n → ∞, then</p><formula xml:id="formula_34">P θ * (L(θ * ) &gt; L(θ )) → 1<label>(16)</label></formula><p>for any θ = θ * . Now we state two theorems to establish the asymptotic properties of θ . We follow arguments similar to those given by <ref type="bibr" target="#b6">Fan, Feng, and Wu (2009)</ref> to prove Theorems 2 and 3. However, one cannot directly apply Fan et al.'s results here, because the parameters must satisfy the acyclicity constraint, the data we have are not iid observations due to interventions, and the identifiability of a DAG is not always guaranteed.</p><p>Let φ(OLS) k</p><p>(1 ≤ k ≤ d) be the estimate of φ k when the corresponding β ij (i = j ) is estimated by β(OLS) ij . Let A = {j : φ * j = 0} and φ A = (φ j ) j ∈A . It is assumed that θ * is natural in the following two theorems. We relegate the proof of Theorem 2 to the Appendix. The proof of Theorem 3 is given in the online supplementary materials.</p><p>Theorem 2. Assume the adaptive lasso penalty with weights</p><formula xml:id="formula_35">τ j = min(| φ(OLS) j | -γ , M γ ) for all j, where γ, M &gt; 0. As n → ∞, if λ n / √ n → 0 and n k /n → α k &gt; 0 for k = 1, . . . , p, then there exists a local maximizer θ of R(θ ) such that θ -θ * = O p (n -1/2 ).</formula><p>Theorem 3. Assume the adaptive lasso penalty with weights τ j = | φj | -γ for some γ &gt; 0 and all j, where φj is</p><formula xml:id="formula_36">√ n-consistent for φ * j . As n → ∞, if λ n /</formula><p>√ n → 0, λ n n (γ -1)/2 → ∞, and n k /n → α k &gt; 0 for k = 1, . . . , p, then there exists a local maximizer θ of R(θ ) such that θ -θ * = O p (n -1/2 ). Furthermore, with probability tending to one, the √ n-consistent local maximizer θ must satisfy φA = 0. Remark 4. To achieve consistency in model selection with the adaptive lasso penalty, we need some consistent estimate of the vector φ to construct the weights. Theorem 2 suggests that we first use τ j = min(| φ(OLS) j | -γ , M γ ) as weights to obtain an initial consistent estimate φ. Then with weights constructed from φ, Theorem 3 guarantees model selection consistency. In a similar spirit, Shojaie and Michailidis (2010) proposed a twostage lasso penalty where the initial estimate is constructed by a regular lasso. As seen from Theorem 2, our initial estimate is in fact obtained by a weighted lasso with weights bounded from above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">SIMULATION STUDY</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Performance of the CD Algorithm</head><p>To test the performance of the CD algorithm, we conducted a simulation study. We randomly generated graphs with p nodes (p = 20, 50, 100, 200) and 2p edges. To further control the sparsity of the graphs, we set the maximum number of parents for any given node to be 4. For each value of p, we simulated 10 different random graphs, and for each graph, three datasets were generated according to Equation (3) with β ij = 0.2, 0.5, and 1.0, respectively. The variance σ 2 j of the Gaussian noise variable ε j (j = 1, . . . , p) was set to 1 in all our simulation. The sample size of each dataset is n = 5p. As described in Section 2, a data matrix is divided into p blocks such that the sample size of each block is n j = 5, j = 1, . . . , p. The jth block X j contains experimental data on the node X j , which were drawn from the standard normal distribution N (0, 1). For each dataset, we applied the CD algorithm to compute the solution path using a geometric sequence of λ's, starting from the largest value λ max for which B λ max = 0 and decreasing to the smallest value λ min . The sequence typically contained 50 or 100 different values of λ's with the ratio λ min /λ max set to some small value such as 0.001. Graphical models were then selected according to (11) with α = 0.1. We used γ = 0.15 for all datasets, except for the two cases with p ≥ 100 and β ij = 1.0, where γ was set to 0.5.</p><p>Table <ref type="table" target="#tab_0">1</ref> summarizes the average performance of the CD algorithm over 10 datasets for each combination of p and β ij . For instance, when p = 100 and β ij = 0.5, the estimated graphical model on average contains 220.9 directed edges, of which 156.5 edges are present in the true graph, 28.3 edges have directions reversed, and the rest 36.1 edges are not included in the true graph. On average, there are also 15.2 true edges missing in the estimated model. Results in Table <ref type="table" target="#tab_0">1</ref> suggest that our method can estimate the structure of a DAG with reasonable accuracy even when the sample size is limited. All TPRs (defined in Table <ref type="table" target="#tab_0">1</ref>) are above 0.70 except for cases with β ij = 0.2, where signalto-noise ratios are too small. The accuracy of estimation can be greatly improved if a large sample is available (see Table <ref type="table" target="#tab_0">S1</ref> in the online supplementary materials). Note that when p = 200, the number of parameters to be estimated is around 20,000, which is much larger than the sample size n = 5p = 1000. Even in this high-dimensional setting, our CD algorithm was still able to estimate DAGs quite accurately.</p><p>Since the CD algorithm computes a set of solutions along the solution path of problem ( <ref type="formula">7</ref>), another way to evaluate the performance is to investigate the relationship between TPR and false positive rate [FPR = (R + FP)/(p(p -1) -T)] as the penalty parameter λ varies, which is known as the receiver operating characteristic (ROC) analysis. However, since the sequence of λ's we used was data dependent, we examined the TPR-FPR relationships as the number of predicted edges increases. Figure <ref type="figure" target="#fig_2">2</ref> presents the results of the ROC analysis. Again, these ROC curves suggest satisfactory performance of the CD algorithm except when the signal-to-noise ratio is small (β ij = 0.2). In particular, we note that for large networks (p = 100, 200), as we increase the number of predicted edges and the complexity of estimated graphs by adjusting the penalty λ, we will increase the TPR without affecting the FPR that much until the TPR reaches a plateau, a level at which the estimated DAGs are structurally similar to the true DAG. This is consistent with our sensitivity analysis on the tuning parameter α in (11). The analysis shows that when α is greater than 0.1, the FDR falls into an acceptable and stable level (see Figure <ref type="figure" target="#fig_3">S3</ref>). Further decrease in α to below 0.05 would result in a drastic increase in false positive edges.   As expected, the performance of the CD algorithm decreases when the graph is less sparse. We varied the number of edges from p to 4p in our simulation and found a decrease in the TPR with an increase in the FDR when the underlying DAG became denser. However, even for the most dense cases, the result is still reasonably good, as reported in Table <ref type="table" target="#tab_1">S2</ref> in the online supplementary materials. Note that the parameter α was fixed to 0.1 for all the simulation results. It seems that this choice may control the FDR at an acceptable level unless the sample size is too small or the signal-to-noise ratio is too low (small β ij ). To obtain a rough measure of the amount of information that interventional data can provide to resolve directionality of DAGs, we also applied the CD algorithm to simulated observational data with the same sample sizes as their interventional counterparts. The results are summarized in Tables S3 in the online supplementary materials. We found that interventional data helped to increase the TPR and simultaneously reduce the FDR, and the boost in the TPR ranges from 2% up to about 50%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Comparison With Other Methods</head><p>To benchmark the performance of the CD algorithm, we compared our method to a PC-algorithm-based approach. The PC algorithm is a classical constraint-based method that can estimate DAGs with hundreds of nodes. We did not compare with Monte Carlo approaches, as even the most recent developments, such as the order-graph sampler <ref type="bibr" target="#b5">(Ellis and Wong 2008)</ref>, have not shown convincing performance on graphs with more than 50 nodes.</p><p>The PC algorithm is designed to estimate from observational data a completed partially directed acyclic graph (CPDAG), which contains both directed and undirected edges. We therefore took a two-step approach to produce results favorable for the PC algorithm. We first used the PC algorithm to estimate a CPDAG from data. Then one may try to estimate the direction of an undirected edge using interventions and produce a DAG. In this comparison, however, we simply counted an undirected edge between two nodes in a CPDAG as an expected edge, provided that there is a corresponding directed edge between the two nodes in the true DAG. Thus, the reported result is the best (or an upper bound) one can obtain by a two-step PC-algorithm-based method (PC-based method). The performance of this PC-based method applied to our simulated datasets is shown in Table <ref type="table" target="#tab_1">2</ref>. Unlike graphs selected by criterion (11), a graph learned by the PC-based method generally has fewer edges than the true model. So to make a fair comparison, we selected from the solution path constructed by the CD algorithm the graph that matches the total number of edges of the graph learned by the PC-based method. The corresponding results are also presented in Table <ref type="table" target="#tab_1">2</ref>. It can be easily seen that the CD algorithm outperforms the PC-based method in all the cases of our simulation. Graphs estimated using our method have both higher TPRs and lower FDRs. This result shows the advantage of using experimental data in an integrated penalized likelihood method. In addition, we compared the performance of the CD algorithm and the PC-based method on observational data (see Table <ref type="table" target="#tab_3">S4</ref> in the online supplementary materials). We found that our method still outperforms the PC-based method for most cases. We also compared the running time for both methods. Table <ref type="table" target="#tab_2">3</ref> summarizes the CPU time for one run of each algorithm averaged over 10 datasets. Each run of the CD algorithm uses a sequence of 50 λ's with λ min /λ max = 0.001. The CD algorithm is implemented in R with the majority of its core computation executed in C programs. The PC algorithm we used was implemented by <ref type="bibr" target="#b16">Kalisch et al. (2012)</ref> in the R package pcalg. The running time for the PC algorithm depends on the argument u2pd, which we assume to be rand (see online manuals for further details). According to Table <ref type="table" target="#tab_2">3</ref>, the average CPU time for the PC algorithm is shorter than the CD algorithm. However, considering that the CD algorithm estimates 50 (or more generally a sequence of) graphical models in each run, it is on average at least as fast as the PC algorithm for estimating a single graph.</p><p>Recently, Shojaie and Michailidis ( <ref type="formula">2010</ref>) developed an approach to estimate DAGs assuming a known ordering of the variables, which we will refer to as the KO method. Knowing the ordering greatly simplifies the structure learning problem. Following their formulation, we can simply estimate the coefficient matrix B (and thus the structure of directed graphs) by regressing each variable on all preceding variables in a given ordering. Hence, the problem of estimating directed graphs becomes p -1 separate lasso problems, which can be solved efficiently using either the least angle regression (LARS) algorithm <ref type="bibr" target="#b3">(Efron et al. 2004)</ref> or the pathwise CD algorithm <ref type="bibr" target="#b8">(Friedman et al. 2007)</ref>. To obtain an estimate of a directed graph, <ref type="bibr" target="#b25">Shojaie and Michailidis (2010)</ref> </p><formula xml:id="formula_37">proposed to use λ i (δ) = 2 ñ-1/2 Z * δ/[2p(i-1)]</formula><p>as the penalty for the ith individual lasso problem, where ñ is the sample size, Z * q is the (1q)th quantile of the standard normal distribution, and δ is a parameter controlling the probability of falsely joining two ancestral sets in a graph (see <ref type="bibr" target="#b25">Shojaie and Michailidis 2010)</ref>. We applied their method to our simulated datasets. Though this criterion worked well with a large sample size (see Table <ref type="table" target="#tab_0">S1</ref>), it led to over-sparse solutions when applied to our datasets with a limited sample size (n = 5p). We thus scaled down the tuning parameters λ i (δ) proportionately and the results are summarized in Table <ref type="table" target="#tab_0">1</ref> (KO method). The δ level was chosen to be 0.1 as suggested by <ref type="bibr" target="#b25">Shojaie and Michailidis (2010)</ref>. As anticipated, most of the results obtained by assuming a known ordering are clearly better than the results of the CD algorithm. However, almost all the TPRs from the CD algorithm are above 75% of those from the KO method. Furthermore, the CD algorithm seemed to outperform the KO method when p = 200 and β ij = 1.0. This comparison demonstrates the gain in prediction accuracy when the assumed ordering of the variables is correct. The gain mostly comes from a lower FDR as no reversed edges will be produced. However, such an assumption is often risky in practical applications. Fortunately, the promising result of the CD algorithm for large networks with a reasonably strong signal (p ≥ 100 and β ij ≥ 0.5) suggests that estimating a DAG without knowing the ordering is reliable with sufficient data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">REAL DATA EXAMPLE</head><p>In this section, we analyze a flow cytometry dataset generated by <ref type="bibr" target="#b24">Sachs et al. (2005)</ref>. This dataset contains simultaneous measurement on p = 11 protein and phospholipid components of the signaling network in human immune system cells. The original dataset contains continuous data collected from n = 7466 cells and consists of a mixture of observational and experimental samples on the 11 components. The dataset analyzed by <ref type="bibr" target="#b24">Sachs et al. (2005)</ref> is a discretized version of the continuous dataset. A number of researchers studied the flow cytometry dataset, among whom <ref type="bibr" target="#b9">Friedman, Hastie, and Tibshirani (2008)</ref> and <ref type="bibr" target="#b25">Shojaie and Michailidis (2010)</ref> analyzed the continuous version.</p><p>Figure <ref type="figure" target="#fig_3">3</ref>(a) shows the known causal interactions among the 11 components of the signaling network. These causal relationships are well established, and no consensus has been reached on interactions beyond those present in the network. Thus, this network structure is often used as the benchmark to assess the accuracy of an estimated network structure, and we therefore call it the consensus model. <ref type="bibr" target="#b9">Friedman, Hastie, and Tibshirani (2008)</ref> applied the graphical lasso to this dataset and estimated a number of graphical models using different values of the L 1 penalty. Their models are all undirected and they observed moderate agreement between one of their estimates and the consensus model. <ref type="bibr" target="#b25">Shojaie and Michailidis (2010)</ref> also analyzed the same dataset using their penalized likelihood method by assuming the ordering of the variables is known a priori. Their estimated DAG using the adaptive lasso penalty is shown in Figure <ref type="figure" target="#fig_3">3(b)</ref>. This graph has 27 directed edges in total, among which 14 are expected and 13 are false positives. We obtained a sequence of estimated DAGs by applying the CD algorithm to the continuous flow cytometry data. One of them is shown in Figure <ref type="figure" target="#fig_3">3</ref>(c). Our model also has a total of 27 directed edges, of which 8 are expected, 6 are reversed, and 13 are false positives. It seems that the performance of the CD algorithm, if ignoring the directionality, is very comparable to the method assuming a known ordering.</p><p>To test the robustness of our method, we applied the CD algorithm to the discrete version of the flow cytometry dataset,  which has n = 5400 cells. The discretization transformed the data into three levels, high, medium, and low, which are coded as 2, 1, and 0, respectively. As a result, the magnitude of the original measurement is partially preserved in the discrete data.</p><p>An estimated DAG with 26 edges is shown in Figure <ref type="figure" target="#fig_3">3</ref>(d). To our surprise, this graph is qualitatively better than the one estimated using the continuous dataset. In this graph, there are 11 expected edges and 15 false predictions (R+FP; see Table <ref type="table" target="#tab_3">4</ref>). We also applied the CD algorithm to 100 bootstrap samples generated from the discrete dataset to assess the sensitivity of our method to data perturbation. For each bootstrap sample, we selected a model with 26 edges and found that on average it shared 23.3 edges with the model shown in Figure <ref type="figure" target="#fig_3">3</ref>(d), which confirms that our method is quite robust to data perturbation. Moreover, though our method was designed for Gaussian data, we were still able to obtain a reasonable network structure from the discretized dataset, which does not satisfy the Gaussian assumption.</p><p>Compared to the estimate obtained by Ellis and Wong (2008) using their order-graph sampler, our result with 20 predicted edges is slightly better in terms of the number of expected edges (E) and false predictions (R+FP; see Table <ref type="table" target="#tab_3">4</ref>). The multidomain sampler, recently developed by <ref type="bibr" target="#b32">Zhou (2011)</ref> for Bayesian inference, yields better result than the CD algorithm. However, the CD algorithm is much faster than these Monte Carlo sampling approaches. For large networks with hundreds of nodes, the CD algorithm can still be used to obtain reasonably good estimates of DAGs, while Monte Carlo methods may not be applicable due to their long running time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">DISCUSSION</head><p>We have developed a method to estimate the structure of causal Gaussian networks using a penalized likelihood approach with the adaptive lasso penalty. Without knowing the ordering of the variables, we rely on experimental data to retrieve information about the directionality of the edges in a graph. The acyclicity constraint on the structure of BNs presents a challenge to the maximization of the penalized log-likelihood function. A blockwise CD algorithm has been developed for this optimization problem. The algorithm runs reasonably fast and can be applied to large networks. A simulation study has been conducted to demonstrate the performance of our method for BNs of various sizes, and a real data example is shown as well. Throughout this article, variables are assumed to be Gaussian, although our approach may be applied to datasets from other distributions as demonstrated by the result on the discrete flow cytometry data. However, a more principled generalization to other data types is expected to have a better performance.</p><p>We have established asymptotic properties for the penalized maximum likelihood estimator of the coefficient matrix of a GBN, assuming that the number of variables p is fixed. Asymptotic theory for the estimator if p is allowed to grow as a function of the sample size remains to be established in the future. This type of asymptotic problems has been studied in various settings of undirected graph and precision matrix estimation (e.g., <ref type="bibr" target="#b21">Meinshausen and Bühlmann 2006;</ref><ref type="bibr" target="#b17">Lam and Fan 2009)</ref>, where p(n) = O(n c ) for some c &gt; 0 or is of an even higher order. Following our current setup, however, we may need to restrict our attention to the case where 0 &lt; c &lt; 1 so that every variable will have sufficient interventional data as n → ∞. The satisfactory results in our simulation for p ≥ 100 and n = 5p seem to suggest that our CD algorithm is effective even for p &gt; √ n. It will be interesting to study the theoretical properties of this penalized likelihood approach when not all variables have experimental data, for which the concept of interventional Markov equivalence (Hauser and Bühlmann 2012) will be relevant.  k . This is apparently impossible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX: PROOFS</head><formula xml:id="formula_38">* if θ = θ * . Case 1: S(G φ ) ∩ S(G φ * ) = ∅. Let ∈ S(G φ ) ∩ S(G φ * ),</formula><p>Case 2: S(G φ ) ∩ S(G φ * ) = ∅, that is, none of the orderings of G φ * is compatible with G φ . In this case, there must exist a pair of indices (i, j ) such that in G φ * , X i ∈ an(X j ), but in G φ , X j is a nondescendant of X i . Then X j is independent of X i in f (x|θ i ), since in G φ i , X i has no parents and X j is a nondescendant of X i . So cov(X i , X j ) = 0 in f (x|θ i ). However, in G φ * i , we still have X i ∈ an(X j ). It is easy to show that cov(X i , X j ) = β * i→j var(X i ) = 0 in f (x|θ * i ) since θ * is natural. Therefore, there exists 1 ≤ i ≤ p such that f (x|θ i ) = f (x|θ * i ), which contradicts our assumption.</p><p>So in both Case 1 and Case 2, we have a contradiction. Thus, the first claim holds.</p><p>For the second claim ( <ref type="formula" target="#formula_34">16</ref>), first note that by the law of large numbers,</p><formula xml:id="formula_39">1 n (L(θ) -L(θ * )) = p k=1 n k n 1 n k h∈I k log f (X [h,•] |θ k ) f (X [h,•] |θ * k ) -→ p p k=1 α k E θ * k log f (Y|θ k ) f (Y|θ * k ) , (A.1)</formula><p>where Y is a random vector with probability density f (x|θ * k ). Then the desired result follows immediately using Jensen's inequality and (15). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SUPPLEMENTARY MATERIALS</head><p>Some technical proofs, the algorithm for checking the acyclicity constraint, and additional results can be found in the supplementary materials. <ref type="bibr">[Received October 2011</ref><ref type="bibr">. Revised November 2012. ]</ref> </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Plots of (a) CV error, (b) difference ratio ("histogram-like" vertical lines), and log-likelihood (solid line) for graphs estimated using a decreasing sequence of λ.</figDesc><graphic coords="5,57.18,54.11,500.64,242.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>a TPR = E/T, where T = 2p is the total number of true edges; b FDR = (R + FP)/P.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. ROC curves for β ij = 0.2 (solid lines), β ij = 0.5 (dot-dashed lines), and β ij = 1.0 (long-dashed lines).</figDesc><graphic coords="8,124.89,54.10,360.00,368.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. (a) The classical signaling network of human immune system cells, (b) Shojaie's network estimated from the continuous flow cytometry dataset. The CD networks estimated from (c) the continuous flow cytometry dataset and (d) the discrete flow cytometry dataset.</figDesc><graphic coords="10,124.89,336.44,360.00,389.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Proof of Theorem 1. We prove the first claim (15) by contradiction. Suppose θ = θ * and f (x|θ k ) = f (x|θ * k ) a.e. for k = 1, . . . , p. Let S(G) denote the set of topological sorts of a DAG G. Recall that we denote by G φ and G φ * the DAGs induced by φ and φ * , respectively. There are two possibilities between the topological sorts of G φ and G φ</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>that is, an ordering compatible with both G φ and G φ * . Assume without loss of generality that in this ordering i ≺ j if i &lt; j. Apparently, is also compatible with G φ k and G φ * k for k = 1, . . . , p. Then we can write f(x|θ k ) = p i=1 f (x i |x 1 , . . . , x i-1 , θ k ) = p i=1 f (x i | G φ k i , θ k ) and similarly f (x|θ * k ) = p i=1 f (x i | k ). Since f (x|θ k ) = f (x|θ * k )i and thus G φ k = G φ * kfor all k. However, since θ = θ * , there exists some k such that θ k = θ * k . Therefore, there exists a k such that the common multivariate normal densityf (x|θ k ) = f (x|θ * k ), factorized according to a common structure G φ k = G φ *k , can be parameterized by two different parameters θ k and θ *</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Proof of Theorem 2. Define a n = 1/ √ n and B = {j : φ * j = 0}. LetI(θ k ) = E θ k ∂ ∂θ k log f (x|θ k ) ∂ ∂θ k log f (x|θ k )T be the Fisher information matrix. Consider θ = (φ, σ 2 , σ 2 ) ∈ nb(θ * ), where nb(θ * ) is an arbitrarily small neighborhood of θ * . The components of φ must satisfyφ i φ * i &gt; 0 if φ * i = 0 (i = 1, . . . , d), since otherwise θ -θ * ≥ min j :φ * j =0 |φ * j |.In particular, this implies that if θ ∈ nb(θ * ), i → j in G φ for all i → j in G φ * and thus G φ and G φ * have a compatible ordering. If we restrict to the lower-dimensional space k = {θ k : θ ∈ }, the same arguments apply to an arbitrarily small neighborhood of θ * k in this space, that is, G φ k and G φ * k have a compatible ordering as well. Then it follows from the arguments used in Case 1 in the proof of Theorem 1 thatf (x|θ k ) = f (x|θ * k ) for θ k ∈ nb(θ * k )\{θ * k }.Since f is a Gaussian density, it follows that I(θ * k ) is positive definite for all k. Let u ∈ {u : θ * + a n u ∈ } and denote its components by u j . Let u k be the vector defined in the same way as θ k , k = 1, . . . , p. Note that p k=1 u k 2 ≥ u 2 . Let δ k min &gt; 0 be the minimal eigenvalue of I(θ * k ) and ρ = min k (α k δ k min /2). Then the behavior of R(θ ) in a small neighborhood of the true value θ * by expanding L(θ ) around θ * . We have, as n → ∞,R(θ * + a n u) -R(θ * ) ≤ L(θ * + a n u) -L(θ * )λ n j ∈B τ j (|φ * j + a n u j | -|φ * j (θ * k + a n u k ) -L k (θ * k )]λ n k ) = O p (1) for all k.By assumption, τ j = O p (1) for j = 1, . . . , d and λ n / √ n = o p (1). Therefore, for a sufficiently large C, the second term in the last line of (A.3) dominates the first and the third terms uniformly in {u : u = C, θ * + a n u ∈ }. Hence, for any given ε &gt; 0, there exists a sufficiently large C such thatP sup u =C R(θ * + a n u) &lt; R(θ * ) ≥ 1ε, (A.4)which implies that with probability at least 1ε, there exists a local maximizer θ of R(θ) in the ball {θ * + a n u ∈ : u &lt; C}. Thus, there exists a local maximizer θ of R(θ ) such that θ -θ * = O p (n -1/2 ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>The average number of predicted (P), expected (E), reversed (R), missed (M), and false positive (FP) edges and the average true positive rate (TPR a ) and false discovery rate (FDR b ) for DAGs learned by the CD algorithm</figDesc><table><row><cell>CD algorithm</cell><cell>KO method</cell></row></table><note><p><p>NOTE:</p>1. The numbers in parentheses are the standard deviations across 10 datasets. 2. As a comparison, the last two columns list the average TPR and FDR for DAGs estimated by the approach of Shojaie and Michailidis (2010) assuming that the ordering of the variables is known. (KO: known ordering).</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Performance comparison between the two-step PC-based method and the CD algorithm : The numbers in parentheses are the standard deviations across 10 datasets.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>PC-based method</cell><cell></cell><cell></cell><cell>CD algorithm</cell><cell></cell></row><row><cell>p</cell><cell>β ij</cell><cell>P</cell><cell>TPR</cell><cell>FDR</cell><cell>P</cell><cell>TPR</cell><cell>FDR</cell></row><row><cell>20</cell><cell>0.2</cell><cell>7 .6</cell><cell>0 .103(0.042)</cell><cell>0.443(0.262)</cell><cell>8.3</cell><cell>0 .115(0.044)</cell><cell>0.442(0.184)</cell></row><row><cell></cell><cell>0.5</cell><cell>1 8 .4</cell><cell>0 .313(0.049)</cell><cell>0.311(0.134)</cell><cell>19.8</cell><cell>0 .383(0.095)</cell><cell>0.227(0.148)</cell></row><row><cell></cell><cell>1.0</cell><cell>1 5 .7</cell><cell>0 .290(0.061)</cell><cell>0.254(0.172)</cell><cell>15.7</cell><cell>0 .318(0.103)</cell><cell>0.183(0.088)</cell></row><row><cell>50</cell><cell>0.2</cell><cell>5 3 .3</cell><cell>0 .221(0.037)</cell><cell>0.585(0.071)</cell><cell>52.8</cell><cell>0 .299(0.032)</cell><cell>0.430(0.072)</cell></row><row><cell></cell><cell>0.5</cell><cell>7 0 .3</cell><cell>0 .409(0.081)</cell><cell>0.422(0.082)</cell><cell>72.5</cell><cell>0 .557(0.117)</cell><cell>0.233(0.109)</cell></row><row><cell></cell><cell>1.0</cell><cell>5 4 .7</cell><cell>0 .313(0.042)</cell><cell>0.427(0.054)</cell><cell>50.5</cell><cell>0 .355(0.094)</cell><cell>0.296(0.080)</cell></row><row><cell>100</cell><cell>0.2</cell><cell>173.8</cell><cell>0 .399(0.050)</cell><cell>0.542(0.051)</cell><cell>173.5</cell><cell>0 .610(0.034)</cell><cell>0.297(0.026)</cell></row><row><cell></cell><cell>0.5</cell><cell>153.1</cell><cell>0 .456(0.053)</cell><cell>0.405(0.048)</cell><cell>154.6</cell><cell>0 .596(0.077)</cell><cell>0.231(0.066)</cell></row><row><cell></cell><cell>1.0</cell><cell>107.8</cell><cell>0 .328(0.069)</cell><cell>0.396(0.096)</cell><cell>107.2</cell><cell>0 .513(0.042)</cell><cell>0.041(0.051)</cell></row><row><cell>200</cell><cell>0.2</cell><cell>429.1</cell><cell>0 .506(0.030)</cell><cell>0.528(0.028)</cell><cell>431.8</cell><cell>0 .815(0.053)</cell><cell>0.245(0.051)</cell></row><row><cell></cell><cell>0.5</cell><cell>351.4</cell><cell>0 .493(0.075)</cell><cell>0.438(0.085)</cell><cell>357.4</cell><cell>0 .734(0.093)</cell><cell>0.181(0.068)</cell></row><row><cell></cell><cell>1.0</cell><cell>235.3</cell><cell>0 .335(0.056)</cell><cell>0.433(0.069)</cell><cell>234.8</cell><cell>0 .561(0.059)</cell><cell>0.043(0.046)</cell></row></table><note><p>NOTE</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Comparison of average CPU time (in seconds) between the PC algorithm and the CD algorithm</figDesc><table><row><cell></cell><cell></cell><cell cols="2">PC algorithm</cell><cell></cell><cell></cell><cell cols="2">CD algorithm</cell><cell></cell></row><row><cell></cell><cell>p = 20</cell><cell>p = 50</cell><cell>p = 100</cell><cell>p = 200</cell><cell>p = 20</cell><cell>p = 50</cell><cell>p = 100</cell><cell>p = 200</cell></row><row><cell>β ij = 0.2</cell><cell>0.04</cell><cell>0.28</cell><cell>4.18</cell><cell>28.74</cell><cell>0.09</cell><cell>1.32</cell><cell>17.54</cell><cell>255.09</cell></row><row><cell>β ij = 0.5</cell><cell>0.09</cell><cell>1.23</cell><cell>9.67</cell><cell>76.94</cell><cell>0.15</cell><cell>4.54</cell><cell>112.69</cell><cell>1938.23</cell></row><row><cell>β ij = 1.0</cell><cell>0.09</cell><cell>0.97</cell><cell>5.10</cell><cell>33.73</cell><cell>0.32</cell><cell>10.05</cell><cell>193.17</cell><cell>4595.95</cell></row><row><cell>Mean</cell><cell>0.07</cell><cell>0.83</cell><cell>6.32</cell><cell>46.47</cell><cell>0.19</cell><cell>5.30</cell><cell>107.80</cell><cell>2263.09</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 .</head><label>4</label><figDesc>Comparison among the CD algorithm, the order-graph sampler, and the multidomain sampler applied to the discrete flow cytometry dataset</figDesc><table><row><cell>Method</cell><cell>P</cell><cell>E</cell><cell>M</cell><cell>R+FP</cell></row><row><cell>CD algorithm (26 edges)</cell><cell>26</cell><cell>11</cell><cell>5</cell><cell>15</cell></row><row><cell>CD algorithm (20 edges)</cell><cell>20</cell><cell>9</cell><cell>9</cell><cell>11</cell></row><row><cell>Order-graph sampler</cell><cell>20</cell><cell>8</cell><cell>8</cell><cell>12</cell></row><row><cell>Multidomain sampler</cell><cell>25.9</cell><cell>15.55</cell><cell>2.4</cell><cell>10.35</cell></row><row><cell cols="5">NOTE: The order-graph sampler result comes from the mean graph (figure 11) in Ellis and</cell></row><row><cell cols="5">Wong (2008), while the multidomain sampler result is the average over 20 independent</cell></row><row><cell>runs (see Zhou 2011, table 3).</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Model Selection Through Sparse Maximum Likelihood Estimation for Multivariate Gaussian or Binary Data</title>
		<author>
			<persName><forename type="first">O</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>El Ghaoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aspremont</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="485" to="516" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A Bayesian Method for the Induction of Probabilistic Networks From Data</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">F</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Herskovits</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1992">1992</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="309" to="347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Adapting to Unknown Smoothness via Wavelet Shrinkage</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Donoho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">M</forename><surname>Johnstone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page" from="1200" to="1224" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Least Angle Regression</title>
		<author>
			<persName><forename type="first">B</forename><surname>Efron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Johnstone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="407" to="499" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Inference on Bayesian Network Structures</title>
		<author>
			<persName><forename type="first">B</forename><surname>Ellis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
		<respStmt>
			<orgName>Harvard University</orgName>
		</respStmt>
	</monogr>
	<note>unpublished Ph.D. dissertation</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning Causal Bayesian Network Structures From Experimental Data</title>
		<author>
			<persName><forename type="first">B</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">H</forename><surname>Wong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="page" from="778" to="789" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note>288,295,298</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Network Exploration via the Adaptive Lasso and SCAD Penalties</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Applied Statistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="521" to="541" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Variable Selection via Nonconcave Penalized Likelihood and Its Oracle Properties</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="page" from="1348" to="1360" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Pathwise Coordinate Optimization</title>
		<author>
			<persName><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Höfling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Applied Statistics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="302" to="332" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
	<note>290,296</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Sparse Inverse Covariance Estimation With the Graphical Lasso</title>
		<author>
			<persName><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biostatistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="432" to="441" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note>288,297</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Regularization Paths for Generalized Linear Models via Coordinate Descent</title>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Software</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Being Bayesian About Network Structure: A Bayesian Approach to Structure Discovery in Bayesian Networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="95" to="125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Penalized Regressions: The Bridge versus the Lasso</title>
		<author>
			<persName><forename type="first">W</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational and Graphical Statistics</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="397" to="416" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Characterization and Greedy Learning of Interventional Markov Equivalence Classes of Directed Acyclic Graphs</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bühlmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="2409" to="2464" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note>293,298</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning Bayesian Networks: The Combination of Knowledge and Statistical Data</title>
		<author>
			<persName><forename type="first">D</forename><surname>Heckerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Chickering</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1995">1995</date>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="197" to="243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Estimating High-Dimensional Directed Acyclic Graphs With the PC-Algorithm</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kalisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bühlmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="613" to="636" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Causal Inference Using Graphical Models With the R Package pcalg</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kalisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mächler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Colombo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Maathuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bühlmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Software</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="1" to="26" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Sparsistency and Rates of Convergence in Large Covariance Matrix Estimation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="4254" to="4278" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning Bayesian Belief Networks: An Approach Based on the MDL Principle</title>
		<author>
			<persName><forename type="first">W</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bacchus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Intelligence</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="269" to="293" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">L</forename><surname>Lauritzen</surname></persName>
		</author>
		<title level="m">Graphical Models</title>
		<meeting><address><addrLine>Oxford</addrLine></address></meeting>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Bayesian Graphical Models for Discrete Data</title>
		<author>
			<persName><forename type="first">D</forename><surname>Madigan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>York</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Statistical Review</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="215" to="232" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">High-Dimensional Graphs and Variable Selection With the Lasso</title>
		<author>
			<persName><forename type="first">N</forename><surname>Meinshausen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bühlmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1436" to="1462" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
	<note>288,292,298</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<title level="m">Causality: Models, Reasoning, and Inference</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
	<note>288,289</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Counting Labeled Acyclic Digraphs</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Robinson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1973">1973</date>
			<publisher>Academic Press</publisher>
			<biblScope unit="page" from="239" to="273" />
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
	<note>in New Directions in the Theory of Graphs, ed. Haray F.</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Causal Protein-Signaling Networks Derived From Multiparameter Single-Cell Data</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sachs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pe'er</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Lauffenburger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">P</forename><surname>Nolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">308</biblScope>
			<biblScope unit="page" from="523" to="529" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Penalized Likelihood Methods for Estimation of Sparse High-Dimensional Directed Acyclic Graphs</title>
		<author>
			<persName><forename type="first">A</forename><surname>Shojaie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Michailidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="519" to="538" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note>289,294,296,297</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Scheines</surname></persName>
		</author>
		<title level="m">Causation, Prediction, and Search</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Regression Shrinkage and Selection via the Lasso</title>
		<author>
			<persName><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society, Series B</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="267" to="288" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Convergence of a Block Coordinate Descent Method for Nondifferentiable Minimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Tseng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Optimization Theory and Applications</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="page" from="475" to="494" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Determinant Maximization With Linear Matrix Inequality Constraints</title>
		<author>
			<persName><forename type="first">L</forename><surname>Vandenberghe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-P</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Matrix Analysis and Applications</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="499" to="533" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Coordinate Descent Procedures for Lasso Penalized Regression</title>
		<author>
			<persName><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lange</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Applied Statistics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="224" to="244" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Model Selection and Estimation in the Gaussian Graphical Model</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="page" from="19" to="35" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Multi-Domain Sampling With Applications to Structural Inference of Bayesian Networks</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">106</biblScope>
			<biblScope unit="page" from="1317" to="1330" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
	<note>288,298</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The Adaptive Lasso and Its Oracle Properties</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page" from="1418" to="1429" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
