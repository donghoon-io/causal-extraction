<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Guided Generation of Cause and Effect</title>
				<funder ref="#_bVyMg9d">
					<orgName type="full">National Key Research and Development Program of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhongyang</forename><surname>Li</surname></persName>
							<email>zyli@ir.hit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xiao</forename><surname>Ding</surname></persName>
							<email>xding@ir.hit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ting</forename><surname>Liu</surname></persName>
							<email>tliu@ir.hit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">J</forename><forename type="middle">Edward</forename><surname>Hu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Benjamin</forename><forename type="middle">Van</forename><surname>Durme</surname></persName>
							<email>vandurme@jhu.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Guided Generation of Cause and Effect</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We present a conditional text generation framework that posits sentential expressions of possible causes and effects. This framework depends on two novel resources we develop in the course of this work: a very large-scale collection of English sentences expressing causal patterns (CausalBank); and a refinement over previous work on constructing large lexical causal knowledge graphs (Cause Effect Graph). Further, we extend prior work in lexically-constrained decoding to support disjunctive positive constraints. Human assessment confirms that our approach gives high-quality and diverse outputs. Finally, we use CausalBank to perform continued training of an encoder supporting a recent state-of-the-art model for causal reasoning, leading to a 3-point improvement on the COPA challenge set, with no change in model architecture.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Causal knowledge acquisition is crucial for various Artificial Intelligence tasks, such as causal event graph construction, reading comprehension and future event prediction. We propose an approach for acquiring causal knowledge through generating multiple plausible causes (reasons, explanations) and effects (results, consequences) for a provided input sentence. As exemplified in Figure <ref type="figure" target="#fig_0">1</ref>, we develop two conditional decoders, one per causal direction. To train such models we mine a large-scale corpus of causal expressions from open domain web text, at a scale greatly surpassing prior work. Our goal is to generate multiple distinct possible causes and effects, where each generated sentence is not intended to be a paraphrase of other candidates. To support this output diversity when conditioned on a single shared input sentence, we turn to lexically-constrained decoding <ref type="bibr" target="#b27">[Post and Vilar, 2018;</ref><ref type="bibr">Hu et al., 2019a]</ref>, which allows for efficiently forcing a model to produce output containing one or more provided phrases. Our constraints are derived from a resource we construct for this work, replicating a prior effort in lexicalized causal knowledge graph construction <ref type="bibr" target="#b23">[Luo et al., 2016]</ref>. This graph cap- tures causal relations as a mapping across lexical types, lemmato-lemma, but our goal is to generate naturalistic sentences with appropriately inflected morphology: we therefore develop an approach for disjunctive positive lexical constraints, where a decoder's output must contain one of a set of provided words or phrases. In our case, these are morphological variants of the same base lemma, but our approach should benefit other applications of lexically-constrained decoding.</p><p>While there is recent work in generating story endings conditioned on a context <ref type="bibr" target="#b15">[Guan et al., 2019;</ref><ref type="bibr" target="#b33">Wang and Wan, 2019;</ref><ref type="bibr" target="#b23">Luo et al., 2019]</ref>, such work does not require generated sentences to be strictly causes or effects. The ability to propose explanations for an input sentence by generating multiple causes and effects complements this emerging line of research. To our knowledge, this is the first work to consider open-ended generation of causal sentences at a large scale.</p><p>We evaluate through carefully designed human evaluation by comparing outputs from various baselines and our proposed model, finding that our model's outputs are preferred. We further demonstrate the usefulness of our new resource by taking a recent state-of-the-art causal reasoning system and boosting its results on the COPA test set by 3 points, relying only on continued training of the model's encoder. Our models and resources are made publicly available. <ref type="foot" target="#foot_0">1</ref>In this paper, we make the following contributions: • proposing the task of open causal generation: producing possible causes and effects for any free-form textual event;</p><p>• construction of a causal corpus (CausalBank) containing 314 million CE (cause-effect) pairs;</p><p>• an extension to lexically-constrained decoding that supports disjunctive positive constraints (DPC);</p><p>• human and automatic evaluations illustrating our method can generate high-quality and diverse causes and effects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Approach</head><p>As shown in Figure <ref type="figure">2</ref>, our proposed approach for open-ended causal generation includes a data collection module (Section 2.1), a Cause Effect Graph (Section 2.2), and two DPC (disjunctive positive constraint) decoding based Transformer encoder-decoder models (Section 2.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">CausalBank: A Sentential Causal Corpus</head><p>Existing causal corpora were not built to support our goal for open-ended causal generation given any free-form textual input: as in neural machine translation (NMT), we need a large training set with millions of examples. Thus we harvest a large causal dataset from the preprocessed large-scale English Common Crawl corpus (5.14 TB) <ref type="bibr" target="#b4">[Buck et al., 2014]</ref>. The key guidelines of our dataset are as follows: 1) The causal relation is explicitly expressed in text with a causal pattern e.g. 'because'; 2) The 'cause' and 'effect' arguments must both appear in the same sentence; 3) The 'cause' and 'effect' arguments can be of any length of contiguous text without overlaps between them; 4) Negative causal relations are filtered. We do not rely on a supervised text extractor to pick out specific sub-spans of a sentence that represent a cause-effect pairing between propositions. <ref type="foot" target="#foot_1">2</ref> We instead curate a series of patterns from previous studies <ref type="bibr" target="#b23">[Mirza et al., 2014;</ref><ref type="bibr" target="#b23">Luo et al., 2016;</ref><ref type="bibr" target="#b13">Girju, 2003]</ref>. These patterns can be classified into two categories, according to how they are mostly used in language to convey a causal relation: 1. EPC (effect-pattern-cause) category: I am very sad BECAUSE I lost my phone; 2. CPE (cause-pattern-effect) category: The earthquake RESULTED IN many deaths. For EPC patterns, we simply take the text on the left of the pattern as effect, and take the text on the right of the pattern as cause. The case is reversed for CPE category patterns. These patterns (shown in Table <ref type="table">1</ref>) were applied to the Common Crawl corpus, followed by post-filtering: duplicate removal; filtering explicitly negated relations and verbs in passive voice; and restricting the cause and effect to each contain at least two tokens. This results in our CausalBank corpus, denoted here as B, with 133 M EPC + 181 M CPE = 314 M (c, e) (c refers to cause and e refers to effect) pairs in total. We manually evaluated 1,000 randomly sampled sentences from the corpus and found that 95% conveyed a meaningful causal relation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Cause Effect Graph: A Lexical Causal KB</head><p>Following the method described in <ref type="bibr" target="#b23">Luo et al. [2016]</ref> for creating a causal lexical knowledge base, we reproduce a variant of their CausalNet using the Common Crawl corpus <ref type="bibr" target="#b4">[Buck et al., 2014]</ref>. Given a sentence such as "The storm caused a tremendous amount of damage on the landing beaches.", this approach will harvest the lexical pairs (storm, tremendous), (storm, amount), (storm, damage), (storm, landing), and (storm, beach) as causal evidence. Stop words are removed and only pairs involving nouns, verbs, adjectives and adverbs are retained. The extracted lexical pairs form a directed network of posited causal relations, where nodes in the network are lemmatized terms, and a directed edge between two terms indicates a causal relation, weighted by cooccurrence frequency. For comparison, Figure <ref type="figure" target="#fig_1">3</ref> gives a similar illustration as Figure <ref type="figure" target="#fig_0">1</ref> in <ref type="bibr" target="#b23">Luo et al. [2016]</ref>. We refer to our artifact as a Cause Effect Graph (CEG); Table <ref type="table" target="#tab_5">5</ref> illustrates CEG contains more causal relations than CausalNet,<ref type="foot" target="#foot_2">foot_2</ref> owing to the larger (5.14TB) and cleaner corpus used for extraction <ref type="bibr" target="#b4">[Buck et al., 2014]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Guided Generation</head><p>We use Sockeye <ref type="bibr" target="#b17">[Hieber et al., 2017]</ref>   <ref type="bibr">et al., 2016]</ref> to reduce vocabulary size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Disjunctive Positive Constraints Decoding</head><p>Unlike in NMT, our intended outputs for a given input are diverse in meaning: we wish to generate multiple semantically distinct possible causes or effects. We induce diversity through hard lexical requirements during decoding, using causal keywords from our CEG as positive constraints on the output. A positive constraint forces the decoder to produce a sequence of tokens that contain the constrained sequence, which is achieved through a constrained beam search proposed by <ref type="bibr" target="#b27">Post and Vilar [2018]</ref> and made efficient by <ref type="bibr">Hu et al. [2019a]</ref>.</p><p>Unfortunately, those prior works are restricted to conjunctive positive constraints: all items provided to the decoder must be present in the output. This is problematic in our case: our CEG maps lemmas to lemmas, and thus lemmas will form our constraints, but at generation time we do not require specific morphological inflections of our constrained terms. We wish not to constrain the decoder to a particular lemma, but to allow it to choose the best morphological form as appropriate in its context. For example, when generating a cause for "I brought an umbrella" with rain as the cause keyword, some valid cause sentences, e.g., "It rained" or "It was a rainy day.", would not be permitted based on prior work. One may circumvent this limitation by enumerating all morphological variants of a term, then apply each in turn as a positive constraint in distinct decoding passes. However, this approach does not scale, as its run-time grows exponentially in the number of initial constraints, each with multiple morphological variants.</p><p>Here we propose a solution of disjunctive positive constraint decoding, where each constraint is represented by a set of token sequences, and the decoder needs to include only one sequence from each set of constraints in the final output. We modify the algorithm from <ref type="bibr">Hu et al. [2019a]</ref> to allow the decoder to explore the disjunctively constrained space in a single forward sequence, without significant computational overhead.</p><p>Algorithm 1 Decoding with Disjunctive Positive Constraints. We consider the generation of one sentence with a beam size of 1 for simplicity. Note that while a beam size of 1 reduces the constrained beam search, the handling of DPC is not affected.</p><p>input: a set of disjunctive constraint sets t, for each set s in t, s i = {s 0 i , s 1 i , ..., s n i } and  In that work, constraints are represented in a trie, where each constraint is represented by a path from the root to a leaf. One or more state pointers are used to track how many tokens have been generated for each constraint, and tokens that induce more progress are prioritized in a modified beam search proposed by <ref type="bibr" target="#b27">Post and Vilar [2018]</ref>. When a constraint is satisfied, the algorithm prunes the path representing that constraint. The distinguishing property of a disjunctive constraint is that once a sequence in a disjunctive set is satisfied, others in the set are also removed and no longer constraints.</p><formula xml:id="formula_0">s n i = (w (n)(0) i , w (n)(1) i , ..., w (n)(m) i ) where w (n)(m) i is the m th token in s n i ,</formula><p>For decoding with disjunctive constraints, we represent all constrained sequences, whether they are from the same disjunctive set or not, on a single trie. When a sequence is generated, we prune all sequences in the set as opposed to just the generated sequence. This modification gives us an efficient algorithm for applying disjunctive constraints, as illustrated in Algorithm 1 and Figure <ref type="figure" target="#fig_2">4</ref>. While here we use morphological variants in our disjunctive set, our algorithm is broadly applicable for constraining on a set of synonyms or different subword segmentations of the same sequence.</p><p>Outputs Reranking While DPC decoding supports arbitrary number of disjunctive constraints in one beam search process, in practice only a few preferred constraints under the model will dominate any N-best output. To encourage diver- sity we first select a set of candidate constraint tokens from CEG, generate outputs per constraint, then merge and rerank the results. For example, if generating causes for the input sentence i = "babies cry", we lemmatize each word in the sentence (baby and cry). These terms map to a set of lemmas via CEG, each associated with an observed frequency; we take the N-most frequent (highest weighted) such candidates:</p><formula xml:id="formula_1">t = {w 1 , w 2 ...w N }.</formula><p>For each token w i in t, such as 'love', we get a set of its morphological variants s i ={'love', 'loves', 'loved', 'loving'} via the python package patterns.en, and pass s i as a DPC, keeping the top M outputs. In total we derive N * M (N=300 and M=5) sentences via N beam search decodings. These sentences are ranked by their associated negative log-likelihood scores, and we return the top K.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">CausalBERT</head><p>Previous studies <ref type="bibr" target="#b26">[Phang et al., 2018;</ref><ref type="bibr">Li et al., 2019b]</ref> have shown that applying intermediate auxiliary task training to an encoder such as BERT can improve performance on a target task. We designed an intermediate task for BERT using Causal-Bank B, employing margin loss <ref type="bibr">[Li et al., 2019a;</ref><ref type="bibr">Li et al., 2018a]</ref>  By training BERT with this intermediate supervised task, we expect the model to acquire enhanced knowledge about the meaning of a causal relation, and can have better performance on downstream causal inference tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head><p>We evaluate our proposed causal generation approach by both human and automatic metrics, and evaluate CausalBank by applying CausalBERT to COPA, which requires the model to choose the correct cause or effect from two candidates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Selection</head><p>We first experiment on a small subset of our CausalBank corpus (CB 10M) -10 million CE pairs from the causal pattern 'because' -considering different NMT encoder and decoder architectures (LSTM, CNN, Conv-Transformer <ref type="bibr">[Gehring et</ref> Method Cause Effect P@1 P@3 H Div P@1 P@3 H Div al., 2017], and Transformer). <ref type="foot" target="#foot_4">4</ref> For the cause generation model, e is used as the source and c is used as the target, which is reversed in training the effect model. Perplexity (Per) and word accuracy (Acc) are used to evaluate the model's performance. We find that Transformer constantly achieves the best performance (Table <ref type="table" target="#tab_3">2</ref>).</p><p>Then we train two versions of Transformer on the whole CausalBank corpus (CB all). The small model's encoder and decoder both have 6 layers, with a hidden size and embedding size of 512. The big model's encoder and decoder have 12 layers and 4 layers, with a hidden size and embedding size of 768, leading to 134M parameters in total. The vocabulary size is 15,000. The training is stopped when the validation loss stagnates for 20,000 batches. For the cause generation model, e and c from only the EPC category (c, e) pairs are used as the source and target. For the effect generation model, c and e from only the CPE category (c, e) pair is used as the source and target. This setting always generates the right part of the sentence conditioned on the left part, which we find to give more reasonable outputs than the above architecture exploration experiments. The bottom of Table <ref type="table" target="#tab_3">2</ref> shows the large Transformer model constantly achieves the best performance on development set, which contains 5,000 CE pairs. The compared methods include a simplified KNN method (when the input is "babies cry", we match sentences exactly containing the input as the retrieved neighbors, e.g. "those babies cry loudly", and get the corresponding causes and effects), the GPT-2 124M language model <ref type="bibr">[Radford et al., Method Acc (%)</ref> PMI <ref type="bibr" target="#b19">[Jabeen et al., 2014]</ref> 58.8 PMI EX <ref type="bibr" target="#b14">[Gordon et al., 2011]</ref> 65.4 CS <ref type="bibr" target="#b23">[Luo et al., 2016]</ref> 70.2 CS <ref type="bibr">MWP [Sasaki et al., 2017]</ref> 71.2 Google T5-base <ref type="bibr" target="#b30">[Raffel et al., 2019]</ref> 71.2 BERT-base <ref type="bibr">[Li et al., 2019a]</ref> 75.4 CausalBERT-base (ours)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluating Generation</head><p>78.6</p><p>Google T5-11B <ref type="bibr" target="#b30">[Raffel et al., 2019]</ref> 94.8 Four graduate students from the NLP field were used in annotation. Each was asked to give a score from {0, 1, 2} for the generated {input, cause/effect} pair, where the guidelines are (take cause generation for example): if the generated answer does not make sense or can never be a reasonable cause, reason or explanation for the input event, give a score of 0; if the generated answer has grammatical errors but can be a reasonable cause, reason or explanation for the input event under some rare conditions (or beyond commonsense), give a score of 1; if the generated answer is a fluent sentence and can be a reasonable cause, reason or explanation with high probability, give a score of 2. Each pair was labeled by two annotators, and we average the judgments over two annotators per pair. The cohen's kappa score is 0.53.</p><p>Table <ref type="table">3</ref> shows the human evaluation results. Three metrics are adopted: Precision at 1 P@1 (an average score of 1.5 or above is seen as a valid causal answer); P@3; and average human score for each evaluated (H). For the TrainSub test set, the KNN method shows strong performance, especially for P@1 and the human scores. However, KNN performs worse for P@3, due to the absence of many possible answers for the same input. Meanwhile, our two versions of DPC decoding strategies (CN-cons, Gold-Cons) also show relatively better performance compared to other generation methods (GPT-2, Random and N-best decoding). KNN performs poorly on the COPA dev set, because most of the inputs never appear in the training data. However, CN-Cons and Gold-Cons can still achieve good performance.</p><p>Lexical Diversity We used a modified BLEU score to evaluate lexical diversity (Div in Table <ref type="table">3</ref>) where a lower score means a greater lexical diversity. Specifically, we calculate the associated BLEU-1 score between the gold answers and the generated top 3 outputs without brevity penalty. This modification ensures that we don't reward shorter outputs. In most cases, CN-Cons gets the lowest Div scores, showing that our DPC decoding and constraint tokens from CEG together, allows us to explore more in the causes and effects space, and generate more diverse outputs. Also we find that all of these BLEU scores are very low, compared with the BLEU scores in previous text generation studies <ref type="bibr">[Hu et al., 2019b;</ref><ref type="bibr" target="#b33">Vaswani et al., 2017]</ref>. This is because our generation task is open-ended (as illustrated in Figure <ref type="figure" target="#fig_0">1</ref>).</p><p>Evaluating CausalBank Table <ref type="table" target="#tab_4">4</ref> shows our CausalBERT results on COPA test. Compared with prior strong knowledgedriven baseline methods, a BERT-base model trained with a margin-based loss <ref type="bibr">[Li et al., 2019a]</ref> achieved good performance. Following the experimental settings of <ref type="bibr">Li et al. [2019a]</ref>, when training the BERT-base model with additional CE pairs from CausalBank, we get an improvement of 3.2%, from 75.4% to 78.6%, showing that our corpus successfully augments BERT base to make it better for causal inference, which is a sign the corpus contains useful causal knowledge. We find that the number of CE pairs in the intermediate task matters: performance first improves and then decreases, with more training data added. <ref type="foot" target="#foot_6">5</ref> We get the best performance of 78.6% with 40 K training CE pairs. Though our result still has a big gap from the current SOTA performance on COPA (94.8% from the largest google T5-11B model), the intent of our experiment is just to illustrate how the only difference was in altering the pre-training with CausalBank. One could possibly get a SOTA model based on our corpus and the google T5 model, if publicly available.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Conditional Text Generation Such efforts cover a large body of work, including machine translation, response generation and paraphrase generation. Most related is conditional story generation <ref type="bibr" target="#b15">[Guan et al., 2019;</ref><ref type="bibr" target="#b33">Wang and Wan, 2019;</ref><ref type="bibr" target="#b23">Luo et al., 2019;</ref><ref type="bibr">Li et al., 2018b]</ref>, which aims to generate story continuations based on a given context. These works do not require generated sentences to be strictly causes or effects.</p><p>For causal generation, <ref type="bibr" target="#b30">Rashkin et al. [2018]</ref> aimed to generate the likely intents and reactions of the event's participants, given a short free-form textual event. <ref type="bibr" target="#b31">Sap et al. [2019]</ref> trained a multi-task model for fine-grained kinds of If-Then commonsense reasoning. However, the causal semantics considered in their work are restricted to a narrow space, and their models are trained on no more than one million examples. Further, their resource was based-on crowdsourcing, which carries risks of human bias <ref type="bibr">[Rudinger et al., 2017;</ref><ref type="bibr" target="#b26">Poliak et al., 2018]</ref>. We harvest a significantly larger, open coverage causal corpus,<ref type="foot" target="#foot_7">foot_7</ref> related in approach to DisSent <ref type="bibr">[Nie et al., 2019]</ref> but larger, focused on causality, and aimed primarily at generation rather than sentence representation learning.</p><p>Of various efforts in guided generation <ref type="bibr" target="#b0">[Ammanabrolu et al., 2019;</ref><ref type="bibr" target="#b33">Tang et al., 2019;</ref><ref type="bibr" target="#b8">Clark et al., 2018;</ref><ref type="bibr">Hu et al., 2019b]</ref>,</p><p>Sentential Causal Resource # CE Pairs TCR <ref type="bibr" target="#b24">[Ning et al., 2018]</ref> 172 <ref type="bibr">SemEval-2007</ref><ref type="bibr">Task4 [Girju et al., 2007]</ref> 220 Causal-TimeBank <ref type="bibr" target="#b23">[Mirza et al., 2014]</ref> 318 CaTeRS <ref type="bibr" target="#b23">[Mostafazadeh et al., 2016]</ref> 488 EventCausalityData <ref type="bibr" target="#b9">[Do et al., 2011]</ref> 580 RED <ref type="bibr" target="#b25">[O'Gorman et al., 2016]</ref> 1,147 SemEval2010 Task8 <ref type="bibr" target="#b17">[Hendrickx et al., 2009]</ref> 1,331 BECauSE 2.0 <ref type="bibr">[Dunietz et al., 2017b]</ref> 1,803 EventStoryLine <ref type="bibr" target="#b5">[Caselli and Vossen, 2017]</ref> 5,519 PDTB 2.0 <ref type="bibr" target="#b28">[Prasad et al., 2008]</ref> 8,042 Altlex <ref type="bibr" target="#b17">[Hidey and</ref><ref type="bibr">McKeown, 2016] 9,190 PDTB 3.0 [Webber et al., 2019]</ref> 13 K DisSent <ref type="bibr">[Nie et al., 2019]</ref> 167 K CausalBank (Ours) 314 M</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Causal Knowledge Graph # CE Edges</head><p>Event2mind <ref type="bibr" target="#b30">[Rashkin et al., 2018]</ref> 25 K ConceptNet 5.7 <ref type="bibr" target="#b32">[Speer et al., 2017]</ref> 473 K ASER Core <ref type="bibr" target="#b33">[Zhang et al., 2019]</ref> 494 K Atomic <ref type="bibr" target="#b31">[Sap et al., 2019]</ref> 877 K CausalNet <ref type="bibr" target="#b23">[Luo et al., 2016]</ref> 13.3 M Cause Effect Graph (Ours)</p><p>89.1 M lexically-constrained decoding [Hokamp and Liu, 2017] is a modification of beam search originating in neural machine translation which allows the user to specify tokens that must (or must not) appear in the decoder's output. <ref type="bibr" target="#b27">Post and Vilar [2018]</ref> proposed a variant of lexicallyconstrained decoding that reduced complexity from linear to constant-time, which was made more efficient by <ref type="bibr">Hu et al. [2019a]</ref>. We introduce an extension to lexically-constrained decoding that supports disjunctive positive constraints for multiple optional constraint keywords.</p><p>Sentential Causal Resources Existing causal corpora differ in their annotation guidelines and how they are constructed: (1) whether they consider only explicit or also implicit causal relations; (2) whether they consider only intra-sentence relations or if relations can cross sentences; (3) whether the annotation unit is word level or sentence level; and (4) whether the corpus is constructed automatically or by human effort. Ours is concerned with explicit only relations, within a single sentence, relating one part of a sentence to another, and employs constructed patterns but not sentence-level human annotation.</p><p>Already mentioned are recent crowdsourcing efforts <ref type="bibr" target="#b30">[Rashkin et al., 2018;</ref><ref type="bibr" target="#b31">Sap et al., 2019]</ref>. More related are PDTB <ref type="bibr" target="#b28">[Prasad et al., 2008]</ref> and BECauSE <ref type="bibr">[Dunietz et al., 2017b]</ref>, but where our resource goal is a much larger corpus, for the purpose of training a neural text generation model. Most related would be the extractive approach of DisSent <ref type="bibr">[Nie et al., 2019]</ref>, but where we focus specifically on causality, and derive a much larger corpus. <ref type="bibr" target="#b2">[Bethard and Martin, 2008]</ref> tagged a small corpus of event pairs conjoined with "and" as causal or not causal. <ref type="bibr">CaTeRS [Mostafazadeh et al., 2016]</ref> included causal relations from a commonsense reasoning standpoint. Richer Event Description <ref type="bibr" target="#b25">[O'Gorman et al., 2016]</ref> integrates real-world temporal and causal relations between events into a unified framework. Table <ref type="table" target="#tab_5">5</ref> contrasts the size of causal portion of prior resources with our own. Lexical Causal Resources Lexical semantic resources may encode causal properties on verbs (e.g., <ref type="bibr">[Schuler, 2005;</ref><ref type="bibr" target="#b3">Bonial et al., 2014]</ref>) and prepositions (e.g., <ref type="bibr" target="#b31">[Schneider et al., 2015]</ref>). Force dynamics theory <ref type="bibr" target="#b33">[Talmy, 1988]</ref> from cognitive psychology posits three primary kinds of causal semantics <ref type="bibr" target="#b33">[Wolff, 2007]</ref> -CAUSE, ENABLE and PREVENT -which were lexicalized as causal verbs <ref type="bibr" target="#b33">[Wolff and Song, 2003]</ref>. The annotation scheme of <ref type="bibr">Dunietz et al. [2017b]</ref> distinguishes three types of causal semantics: CONSEQUENCE, MOTI-VATION, and PURPOSE. In PDTB 2.0 <ref type="bibr" target="#b28">[Prasad et al., 2008]</ref>, "CONTINGENCY" has two subtypes ("Cause" and "Condition"). FrameNet <ref type="bibr" target="#b2">[Baker, 2014]</ref> represents causal relations through a variety of unrelated frames (e.g., CAUSATION and THWARTING) and frame roles (e.g., PURPOSE and EXPLANATION). These efforts motivate our own causal patterns, categorized into: CAUSE (e.g. cause, result in, lead to), EXPLANATION (e.g. because, due to), CONDITION (e.g. if-then, as long as), PURPOSE (e.g. in order to, for the purpose of), and PREVENTION (e.g. stop/prevent-from). Causal Knowledge Acquisition Causal knowledge acquisition <ref type="bibr" target="#b29">[Radinsky et al., 2012;</ref><ref type="bibr" target="#b29">Radinsky and Horvitz, 2013]</ref> is crucial for many AI systems, and it is often acquired via text. <ref type="bibr" target="#b16">Hashimoto et al. [2014]</ref> and <ref type="bibr" target="#b19">Kruengkrai et al. [2017]</ref> applied supervised learning techniques using a benchmark training data with over 100K human-annotated CE pairs. <ref type="bibr" target="#b9">Dasgupta et al. [2018]</ref> explored general causal extraction using 5,000 labelled sentences. <ref type="bibr" target="#b9">Do et al. [2011]</ref> is an example of a minimally supervised approach. Recent studies <ref type="bibr">[Dunietz et al., 2017a;</ref><ref type="bibr" target="#b11">Dunietz et al., 2018]</ref> explored new supervised approaches on the BECauSE 2.0 <ref type="bibr">[Dunietz et al., 2017b]</ref> corpus. <ref type="bibr" target="#b7">Church and Hanks [1990]</ref> proposed the use of pointwise mutual information (PMI) for mining patterns via text cooccurrence. Many works have followed this strategy, e.g. <ref type="bibr" target="#b6">[Chambers and Jurafsky, 2008;</ref><ref type="bibr" target="#b30">Riaz and Girju, 2010;</ref><ref type="bibr" target="#b14">Gordon et al., 2011;</ref><ref type="bibr" target="#b9">Do et al., 2011;</ref><ref type="bibr" target="#b23">Luo et al., 2016]</ref>. Others have mined patterns via discourse patterns in the form of 'A led to B', <ref type="bibr">'if A then B', etc., e.g., [Khoo et al., 2000;</ref><ref type="bibr" target="#b13">Girju, 2003;</ref><ref type="bibr" target="#b33">Zhao et al., 2017]</ref>). See <ref type="bibr" target="#b1">Asghar [2016]</ref> for review. Such efforts relate most closely to our CEGraph component, rather than our overall framework. Our concern is the generation of diverse potential causes and effects as natural language statemnts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We investigate open causal generation for free-form textual input, and build a large sentential causal corpus which we used to train a generative model. We introduced a novel extension to lexically-constrained decoding that supports disjunctive positive constraints, where generated output is forced to contain one of a set of candidates. Automatic and human evaluations show that our method can generate high-quality and diverse causes and effects for new inputs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: Possible causes and effects generated by our model, conditioned on the input sentence "babies cry". Tokens in blue are constraint keywords derived from our Cause Effect Graph, which are forced to be included in the outputs by constrained decoding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Cause Effect Graph: A lexical causal knowledge base.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Trie states in positive constraint and disjunctive positive constraint, after generating the token 'rained' in beam search.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>in the objective function: L(Θ) = (c,e) ∈B (max(0, mf (c, e) + f (c , e ))) + λ 2 ||Θ|| 2 , where f (c, e) is the score of true CE pair given by BERT model, f (c , e ) is the score of corrupted CE pair by replacing c or e with randomly sampled negative cause c or effect e from other examples in B. m &gt; 0 is the margin loss function parameter, which is set to 0.3. Θ is the set of BERT model parameters. λ is the parameter for L2 regularization, which is set to 0.00001.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>74.0 70.0 0.81 0.02 72.0 72.0 0.87 0.02 Gold-Cons 73.0 73.0 0.87 0.09 72.0 71.3 0.87 0.09Table 3: Human evaluation results of cause and effect generation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>We evaluate the large Transformer model via human assessment, on two kinds of test sets. The first kind of test sets (TrainSub) contains 100 randomly sampled input examples from the model's training data. The second kind of test sets (COPA Dev) contains 100 randomly sampled examples from the development set of COPA [Roemmele et al., 2011] dataset, which are manually created gold sentences and never seen during the model's training stage.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>to trainTransformerbased [Vaswani et al., 2017]  conditional generation models, one for causes, one for effects. Sockeye supports decoding via N-best (each step greedily chooses the top best N words in beam search based on the generated tokens) and random sampling (each step randomly sampling N words from the softmax distribution based on the generated tokens). The training data (CausalBank) is processed through Byte Pair Encoding [Sennrich</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>one of the sequences of the disjunctive constraint set s i output: a token sequence o = (o 0 , o 1 , ..., o k ) , o 1 , ..., o k )</figDesc><table><row><cell cols="6">trie while o k-1 ! =EOS and k &lt; k max do BuildTrie({s 0 0 , ..., s n i })</cell><cell></cell></row><row><cell cols="7">o k if o k finishes the sequence s ConstrainedBeamSearch((o 0 , ..., o k-1 ), t) q p then</cell></row><row><cell></cell><cell cols="3">for s i p in s p do</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="4">trie trie.prune(s i p )</cell><cell></cell><cell></cell></row><row><cell></cell><cell>end for</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="4">Remove s p from t</cell><cell></cell><cell></cell></row><row><cell cols="2">end if</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>k</cell><cell>k + 1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">end while</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="5">return (o 0 Positive rain</cell><cell></cell><cell>rain</cell></row><row><cell></cell><cell>root</cell><cell>wet</cell><cell></cell><cell>Constraint</cell><cell>root</cell><cell>wet</cell></row><row><cell>root</cell><cell cols="2">rain rain@@</cell><cell>s ed</cell><cell>Disjunctive Positive Constraint</cell><cell>root</cell><cell>rain rain@@</cell><cell>s ed</cell></row><row><cell></cell><cell>wet</cell><cell></cell><cell>ing</cell><cell></cell><cell></cell><cell>wet</cell><cell>ing</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Dev-set results: perplexity (Per), word accuracy (Acc (%)).</figDesc><table><row><cell>Method</cell><cell>Dataset</cell><cell>Cause Per Acc</cell><cell>Effect Per Acc</cell></row><row><cell>RNN-LSTM</cell><cell cols="3">CB 10M 66.0 29.6 55.2 32.2</cell></row><row><cell>RNN-GRU</cell><cell cols="3">CB 10M 67.6 29.5 48.0 33.7</cell></row><row><cell>CNN</cell><cell cols="3">CB 10M 37.6 36.1 39.5 35.4</cell></row><row><cell cols="4">Conv-Transformer CB 10M 29.5 38.9 31.1 38.2</cell></row><row><cell>Transformer</cell><cell cols="3">CB 10M 28.3 39.1 29.9 38.4</cell></row><row><cell>Transformer</cell><cell>CB all</cell><cell cols="2">31.4 38.0 27.6 39.7</cell></row><row><cell>Transformer BIG</cell><cell>CB all</cell><cell cols="2">29.9 38.5 26.4 39.8</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Results on COPA-Test, contrasting prior results to a model by Li et al. built atop BERT-base. This model is improved by 3 points through adoption of CausalBERT.</figDesc><table><row><cell>2019] which can generate continuations conditioned on a start</cell></row><row><cell>sentence (e.g. "babies cry because"), random sampling based</cell></row><row><cell>decoding, N-best decoding, DPC decoding with constraint</cell></row><row><cell>tokens from CEG (CN-cons), and DPC decoding with gold</cell></row><row><cell>answer as constraint tokens (Gold-cons).</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Contrasting size with example prior works: only the causal portion of these corpora are listed. The top are sentential causal corpora, while the bottom are graph-structure causal knowledge bases.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>http://nlp.jhu.edu/causalbank Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence (IJCAI-20)</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>We found poor annotator agreement on span boundaries in an initial investigation on crowdsourcing data for such a system; we intend to return to this in future work, investigating improvements to our results via trained extraction models for corpus pre-processing.Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>89.1M in contrast to 13.3M, with relations with a frequency of 5 or lower removed.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_3"><p>Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_4"><p>Each of these models' encoder and decoder use the same architecture, e.g. both are 6-layer LSTMs, with a hidden size and embedding size of</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="512" xml:id="foot_5"><p>All models are trained for 10 epochs. The vocabulary size is 10,000.Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_6"><p>This was not observed in related studies<ref type="bibr" target="#b26">[Phang et al., 2018;</ref> Li et  al., 2019b], where all training examples from the Multi-NLI dataset were used as an intermediate task. Similar behavior was observed in NMT in continued training for domain adaptation<ref type="bibr" target="#b33">[Thompson et al., 2019]</ref>. We believe ours to be a similar setting, where the "indomain" causal data overwhelms the benefits of pretraining; adapting strategies from Thompson et al. is an avenue for future work.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_7"><p>While we avoid pitfalls of elicitation, we acknowledge that like any corpus-extracted resource ours may suffer from reporting bias [Gordon andVan Durme, 2013]: some types of causes or effects that are known to humans but rarely or ever explicitly stated.Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence </p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We acknowledge the support of the <rs type="funder">National Key Research and Development Program of China</rs> (<rs type="grantNumber">SQ2018AAA0101901</rs>),</p></div>
			</div>
			<div type="funding">
<div><p>* Corresponding author. Performed while the first author was visiting <rs type="institution">Johns Hopkins University</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_bVyMg9d">
					<idno type="grant-number">SQ2018AAA0101901</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>the National Natural Science Foundation of China (NSFC) via Grant 61976073 and 61702137; the China Scholarship Council; and DARPA KAIROS <ref type="bibr">(Hu and Van Durme)</ref>.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Guided neural language generation for automated storytelling</title>
		<author>
			<persName><surname>Ammanabrolu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Storytelling Workshop</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Automatic extraction of causal relations from natural language texts: a comprehensive survey</title>
		<author>
			<persName><forename type="first">;</forename><forename type="middle">N</forename><surname>Asghar</surname></persName>
		</author>
		<author>
			<persName><surname>Asghar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07895</idno>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning semantic links from a corpus of parallel temporal and causal relations</title>
		<author>
			<persName><forename type="first">;</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">H</forename><surname>Martin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies: Short Papers</title>
		<meeting>the 46th Annual Meeting of the Association for Computational Linguistics on Human Language Technologies: Short Papers</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2008">2014. 2014. 2008. 2008</date>
			<biblScope unit="page" from="177" to="180" />
		</imprint>
	</monogr>
	<note>Frame Semantics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Propbank: Semantics of new predicate types</title>
		<author>
			<persName><surname>Bonial</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">N-gram counts and language models from the common crawl</title>
		<author>
			<persName><surname>Buck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">The event storyline corpus: A new benchmark for causal and temporal relation extraction</title>
		<author>
			<persName><forename type="first">Vossen</forename><surname>Caselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Caselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vossen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unsupervised learning of narrative event chains</title>
		<author>
			<persName><forename type="first">Jurafsky</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2008">2008. 2008</date>
			<biblScope unit="page" from="789" to="797" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Word association norms, mutual information, and lexicography</title>
		<author>
			<persName><surname>Church</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><forename type="middle">K Ward</forename><surname>Hanks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Church</surname></persName>
		</author>
		<author>
			<persName><surname>Hanks</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<date type="published" when="1990">1990. 1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Neural text generation in stories using entity representations as context</title>
		<author>
			<persName><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="2250" to="2260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Automatic extraction of causal relations from text using linguistically informed deep neural networks</title>
		<author>
			<persName><surname>Dasgupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual SIGdial Meeting on Discourse and Dialogue</title>
		<imprint>
			<date type="published" when="2011">2018. 2018. 2011</date>
			<biblScope unit="page" from="294" to="303" />
		</imprint>
	</monogr>
	<note>EMNLP</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Automatically tagging constructions of causation and their slot-fillers</title>
		<author>
			<persName><surname>Dunietz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TACL</title>
		<imprint>
			<biblScope unit="page" from="117" to="133" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deepcx: A transition-based approach for shallow semantic parsing with complex constructional triggers</title>
		<author>
			<persName><surname>Dunietz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2017">2017. 2017. 2018. 2018</date>
		</imprint>
	</monogr>
	<note>The because corpus 2.0: Annotating causality and overlapping relations</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Convolutional sequence to sequence learning</title>
		<author>
			<persName><surname>Gehring</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Semeval-2007 task 04: Classification of semantic relations between nominals</title>
		<author>
			<persName><surname>Girju</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AKBC</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Gordon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Van Durme</surname></persName>
		</editor>
		<imprint>
			<publisher>Gordon and Van Durme</publisher>
			<date type="published" when="2003">2007. 2007. 2003. 2013. 2013</date>
		</imprint>
	</monogr>
	<note>Reporting bias and knowledge acquisition</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Commonsense causal reasoning using millions of personal stories</title>
		<author>
			<persName><forename type="first">Gordon</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Story ending generation with incremental encoding and commonsense knowledge</title>
		<author>
			<persName><surname>Guan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Toward future scenario generation: Extracting event causality exploiting semantic relation, context, and association features</title>
		<author>
			<persName><surname>Hashimoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Semeval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals</title>
		<author>
			<persName><surname>Hendrickx</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">A toolkit for neural machine translation</title>
		<imprint>
			<publisher>Chris Hokamp and Qun Liu</publisher>
			<date type="published" when="2009">2009. 2009. 2016. 2016. 2017. 2017</date>
			<biblScope unit="page" from="1424" to="1433" />
		</imprint>
	</monogr>
	<note>ACL</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Improved lexically constrained decoding for translation and monolingual rewriting</title>
		<author>
			<persName><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Improving event causality recognition with multiple background knowledge sources using multi-column convolutional neural networks</title>
		<author>
			<persName><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PRICAI</title>
		<imprint>
			<date type="published" when="2000">2019. 2019. 2014. 2014. 2000. 2017</date>
		</imprint>
	</monogr>
	<note>AAAI</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Constructing narrative event evolutionary graph for script event prediction</title>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="4201" to="4207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Generating reasonable and diversified story ending using sequence to sequence model with adversarial training</title>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Coling</title>
		<imprint>
			<date type="published" when="2018">2018. 2018. 2019. 2019</date>
		</imprint>
	</monogr>
	<note>ACL</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Story ending prediction by transferable bert</title>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Caters: Causal and temporal relation scheme for semantic annotation of event structures</title>
		<author>
			<persName><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Knowledge Representation and Reasoning</title>
		<imprint>
			<date type="published" when="2014">2016. 2016. 2019. 2019. 2014. 2014. 2016. 2019</date>
		</imprint>
	</monogr>
	<note>ACL</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Joint reasoning for temporal and causal relations</title>
		<author>
			<persName><surname>Ning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Richer event description: Integrating event coreference with temporal, causal and bridging annotation</title>
		<author>
			<persName><surname>O'gorman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CNS</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Sentence encoders on stilts: Supplementary training on intermediate labeled-data tasks</title>
		<author>
			<persName><surname>Phang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.01088</idno>
	</analytic>
	<monogr>
		<title level="m">LCS</title>
		<imprint>
			<date type="published" when="2018">2018. 2018. 2018</date>
		</imprint>
	</monogr>
	<note>Hypothesis only baselines in natural language inference</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fast lexically constrained decoding with dynamic beam allocation for neural machine translation</title>
		<author>
			<persName><forename type="first">Vilar</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Post</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vilar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The penn discourse treebank 2.0</title>
		<author>
			<persName><surname>Prasad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><surname>Radford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
		<imprint>
			<publisher>WWW</publisher>
			<date type="published" when="2012">2019. 2019. 2013. 2013. 2012</date>
		</imprint>
	</monogr>
	<note>Learning causality for news events prediction</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Another look at causality: Discovering scenario-specific contingency relationships with no supervision</title>
		<author>
			<persName><surname>Raffel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.10683</idno>
	</analytic>
	<monogr>
		<title level="m">Semantic Comp</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Roemmele</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Bejan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Gordon</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2010">2019. 2019. 2018. 2010. 2011. 2017</date>
		</imprint>
	</monogr>
	<note>AAAI. Rudinger et al., 2017. Social bias in elicited natural language inferences. In Ethics in NLP</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Atomic: An atlas of machine commonsense for if-then reasoning</title>
		<author>
			<persName><surname>Sap</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Linguistic Annotation</title>
		<imprint>
			<date type="published" when="2005">2019. 2019. 2017. 2017. 2015. 2015. 2005. 2016. 2016</date>
		</imprint>
	</monogr>
	<note>ACL</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Conceptnet 5.5: An open multilingual graph of general knowledge</title>
		<author>
			<persName><surname>Speer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Overcoming catastrophic forgetting during domain adaptation of neural machine translation</title>
		<author>
			<persName><forename type="first">;</forename><forename type="middle">L</forename><surname>Talmy</surname></persName>
		</author>
		<author>
			<persName><surname>Talmy</surname></persName>
		</author>
		<author>
			<persName><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">A largescale eventuality knowledge graph</title>
		<title level="s">Force dynamics in language and cognition. Cognitive science</title>
		<imprint>
			<publisher>Wolff</publisher>
			<date type="published" when="1988">1988. 1988. 2019. 2019. 2019. 2019. 2017. 2017. 2019. 2019. 2019. 2019. 2003. 2003. 2007. 2007. 2019. 2019. 2017. 2017</date>
		</imprint>
	</monogr>
	<note>WSDM</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
