<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards Robust and Adaptive Motion Forecasting: A Causal Representation Perspective</title>
				<funder ref="#_sjyBBCs">
					<orgName type="full">Swiss National Science Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yuejiang</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">École Polytechnique Fédérale de Lausanne (EPFL)</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Riccardo</forename><surname>Cadei</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">École Polytechnique Fédérale de Lausanne (EPFL)</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jonas</forename><surname>Schweizer</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">École Polytechnique Fédérale de Lausanne (EPFL)</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sherwin</forename><surname>Bahmani</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">École Polytechnique Fédérale de Lausanne (EPFL)</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">École Polytechnique Fédérale de Lausanne (EPFL)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Towards Robust and Adaptive Motion Forecasting: A Causal Representation Perspective</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Learning behavioral patterns from observational data has been a de-facto approach to motion forecasting. Yet, the current paradigm suffers from two shortcomings: brittle under distribution shifts and inefficient for knowledge transfer. In this work, we propose to address these challenges from a causal representation perspective. We first introduce a causal formalism of motion forecasting, which casts the problem as a dynamic process with three groups of latent variables, namely invariant variables, style confounders, and spurious features. We then introduce a learning framework that treats each group separately: (i) unlike the common practice mixing datasets collected from different locations, we exploit their subtle distinctions by means of an invariance loss encouraging the model to suppress spurious correlations; (ii) we devise a modular architecture that factorizes the representations of invariant mechanisms and style confounders to approximate a sparse causal graph; (iii) we introduce a style contrastive loss that not only enforces the structure of style representations but also serves as a self-supervisory signal for test-time refinement on the fly. Experiments on synthetic and real datasets show that our proposed method improves the robustness and reusability of learned motion representations, significantly outperforming prior state-of-the-art motion forecasting models for out-of-distribution generalization and low-shot transfer.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Motion forecasting is essential for autonomous systems running in dynamic environments. Yet, it is a challenging task due to strong spatial-temporal interactions, which arise from two major sources: (i) physical laws (e.g., inertia, goal-directed behaviors) that govern general dynamics; (ii) social norms (e.g., separation distance, left or righthand traffic) that influence motion styles. Classical models attempt to describe these interactions based on domain knowledge but often fall short of social awareness in complex scenes <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b75">76]</ref>. As an alternative, learning motion representations from observational data has become a de- facto approach <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b64">65]</ref>. In light of rapid progress over the past few years, solving motion forecasting is seemingly just around the corner by pursuing this fashion at larger scales. However, the promise of the current learning paradigm for motion forecasting is shadowed by two shortcomings:</p><p>• struggle to discover physical laws from data, e.g., output inadmissible solutions under spurious shifts <ref type="bibr" target="#b62">[63]</ref>; • inefficient for knowledge transfer, e.g., require a large number of observations to adapt from one environment to another even if the underlying change is sparse <ref type="bibr" target="#b17">[18]</ref>. These issues do not become any less severe with larger models <ref type="bibr" target="#b63">[64]</ref>. Instead, they are profoundly rooted in the principle of statistical learning that only seeks correlations for the prediction task at hand, regardless of their robustness and reusability under distribution shifts that may occur in practice (illustrated in Figure <ref type="figure" target="#fig_0">1</ref>)</p><p>In this work, we aim to tackle these challenges from a causal representation perspective. Incorporating causal relations into statistical modeling has garnered growing interest lately, as it not only offers a mathematical language to articulate distribution changes <ref type="bibr" target="#b55">[56,</ref><ref type="bibr" target="#b58">59]</ref> but also brings critical insights to representation learning <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b67">68,</ref><ref type="bibr" target="#b80">81]</ref>. Studies in cognitive science have also revealed its paramount importance in the motion context: few-month-old infants are already able to reason sensibly about physical and social causalities <ref type="bibr" target="#b66">[67]</ref>; they can even learn that by sorely observing adult behaviors, without any hands-on experience of their own <ref type="bibr" target="#b78">[79]</ref>. How can we build learning algorithms capable of acquiring such causal knowledge in the same way?</p><p>To this end, we introduce a new formalism of motion forecasting that describes human motion behaviors as a dynamical process with three groups of latent variables: (i) domain-invariant causal variables that account for the physical laws universal to everyone at any place, (ii) domainspecific confounders associated with motion styles, which may vary from site to site, (iii) non-causal spurious features, whose correlations with future motions may change drastically under different conditions. This causal formalism motivates us to treat each group distinctively with the following three components.</p><p>First, we propose to promote causal invariance of the learned motion representations by seeking the commonalities across multiple domains. Oftentimes, the training dataset is not collected from a single place but comprises multiple subsets from different locations. Previous work typically merges them into a larger one, e.g., the notable ETH-UCY datasets <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b56">57]</ref>. However, each subset is often inherently different <ref type="bibr" target="#b12">[13]</ref>. Directly combining them not only entails a risk of biases but also destroys the critical information about the stability of correlations. To address this issue, we train motion forecasting models with a penalty on the variation of empirical risks across environments. This regularizer encourages the model to suppress spurious features and only exploit causally invariant ones. As a consequence, the resulting model is close to equally optimal in all environments -both the ones seen during training and those unseen encountered at test -for robust generalization.</p><p>Second, we design a modular architecture that factorizes the representations of invariant mechanisms and style confounders in a structural way. One unique property of motion problems is that the style confounder may also vary across environments, but constitute an indispensable part of causal variables for human motions. To explicitly model their impact, we devise an architecture that contains two encoders responsible for the invariant mechanisms and style confounders separately. This modular design approximates the sparse causal graph <ref type="bibr" target="#b53">[54]</ref> in our motion formalism, enabling the model to precisely localize and adapt a small subset of parameters to account for the underlying style shift.</p><p>Third, we introduce a style contrastive loss to further strengthen the modular structure of motion styles. Specifically, we introduce an auxiliary contrastive task that encourages the style encoder to produce an embedding space capturing style relations between different scenes through a simple distance measure. This peculiar form of discriminative task does not impose prior assumptions on the number of style classes, and is hence particularly suitable for incremental knowledge transfer to new motion styles. Moreover, when the predicted output is sub-optimal, the style contrastive loss can naturally serve as a self-supervisory signal for test-time refinement on the fly <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b40">[41]</ref><ref type="bibr" target="#b41">[42]</ref><ref type="bibr" target="#b42">[43]</ref><ref type="bibr" target="#b70">71]</ref>. By tightly coupling the modular architecture design with the style con-trastive loss, our method makes effective use of the knowledge stored in the style encoder during both training and deployment.</p><p>We evaluate the proposed method in two settings: synthetic simulation datasets and controlled real-world experiments. In the presence of spurious correlations, motion forecasting models trained by our invariant loss demonstrate superior out-of-distribution (OOD) generalization ability over previous methods. Under variations of motion styles, our proposed modular architecture and style loss greatly improve the transferability of forecasting models in the lowshot setting. We hope our findings will pave the way for a tight integration of causal modeling and representation learning in the motion context, a largely under-explored yet highly promising direction towards reliable and adaptive autonomy. Our code is available at <ref type="url" target="https://github.com/vita-epfl/causalmotion">https://github. com/vita-epfl/causalmotion</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Motion forecasting. Modern motion forecasting models <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b64">65]</ref> are largely built with neural networks and trained with the maximum likelihood principle. Despite strong performance for short-range predictions within the training domain, they often struggle to generalize under covariate shift. Recently, a couple of works proposed to promote their robustness using negative data augmentation <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b89">90]</ref>. However, designing negative examples of highdimension, e.g., long sequences, can be difficult in practice. Our work explores a causality-inspired alternative that does not require hand-engineered interventions over training data and is hence more theoretically grounded and algorithmically generic.</p><p>Closely related to ours, another recent work <ref type="bibr" target="#b12">[13]</ref> attempts to mitigate biases in motion datasets through counterfactual analysis. Our method differs from theirs in three aspects: (i) their approach learns to estimate dataset biases before subtracting them in the feature space, whereas our method aims to directly suppress biased features; (ii) their approach inherits the merge-and-shuffle convention, which destroys some critical information for bias estimation; in contrast, we keep each subset separately and exploit unstable correlations across environments; (iii) the counterfactual problem formulated in their approach is generally difficult to solve (cf. Pearl's ladder of causation <ref type="bibr" target="#b55">[56]</ref>); conversely, we consider spurious correlations from the interventional perspective, which is easier to tackle in practice.</p><p>Causal learning. The intersection of causal inference and machine learning has been a vibrant area of research in the past few years <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b80">81]</ref>. Some earlier works attempted to identify causal structures from observational or interventional data <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b77">78]</ref>. Examples include scorebased <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b65">66]</ref>, constraint-based <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b72">73,</ref><ref type="bibr" target="#b81">82]</ref>, condi- tional independence test <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b68">69,</ref><ref type="bibr" target="#b86">87]</ref>, continuous optimization <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b82">83,</ref><ref type="bibr" target="#b88">89]</ref> and many others <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b73">74]</ref>. While these methods are theoretically appealing, they are often practically restricted to classical problems that assume direct access to high-level causal variables rather than the low-level observations present in modern problems <ref type="bibr" target="#b67">[68]</ref>.</p><p>More recently, several different approaches have been proposed to automatically discover causal variables of interest from low-level data. One notable line of work lies in disentangled representation learning <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b31">32]</ref>, which is closely tied to independent causal mechanisms <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b71">72]</ref>. However, separating independent factors of variation in an unsupervised manner is often exceedingly challenging without strong assumptions <ref type="bibr" target="#b44">[45]</ref>. As an alternative, a few other recent works seek casually invariant representations by exploiting observational data collected under different setups <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b61">62]</ref>. Our work also falls into this category: we reveal the strengths and weaknesses of the invariant learning principle in the motion context and propose to tightly integrate invariant representation with structural architectural design based on domain knowledge. Distribution shifts. Previous methods tackle the challenge of distribution shifts from three main paradigms: domain generalization, domain adaptation, and transfer learning. Domain generalization is the most ambitious one, which aims to learn models that can directly function well in related but unseen test distributions <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b27">28]</ref>. Recent literature has proposed a variety of solutions, such as distributionally robust optimization <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b59">60]</ref>, adversarial data augmentation <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b76">77]</ref>. Yet, these techniques often rely on strong assumptions on the test distribution, which may not hold in practice.</p><p>Domain adaptation is another popular approach that relaxes these assumptions by allowing a learning algorithm to observe a set of unlabelled test samples. Modern methods of this kind typically attempt to learn an embedding space where the training and test samples are subject to similar feature distributions through divergence minimization <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b69">70,</ref><ref type="bibr" target="#b85">86]</ref> or adversarial training <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b74">75]</ref>. While this approach has been shown effective in a variety of super-vised tasks <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b79">80,</ref><ref type="bibr" target="#b87">88]</ref>, it is not well suited for motion forecasting where labels in the form of future trajectories are fairly easy to acquire without human annotation but sample efficiency matters crucially.</p><p>Previous work in the third category -transfer learning given limited data -often leverages special architecture designs, e.g., external memory <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b52">53]</ref>, or transfer-oriented objectives, e.g., meta-learning <ref type="bibr" target="#b21">[22]</ref>. Some of these techniques have also been applied to motion forecasting <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b84">85]</ref>. Our work differs from them in that we adopt a causal approach and propose a unified learning framework that facilitates both robust generation and fast adaptation to common types of distribution shifts in motion forecasting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>The conventional learning paradigm for motion forecasting only seeks statistical patterns in the collected observational data at hand, regardless of their robustness and reusability under distribution shifts. As a result, existing models often struggle to effectively generalize or adapt to new environments.</p><p>In this section, we address these challenges by (i) formalizing the motion forecasting problem from a causal representation perspective and (ii) explicitly promoting the causal invariance and structure of the learned motion representations through three algorithmic components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Formalism of Motion Forecasting</head><p>Preliminary. Consider a motion forecasting problem in multi-agent environments. For a scene of M agents, let vironments E = {e 1 , e 2 , . . . , e K }. Previous work typically merges them into a large dataset and assumes the mixture as a representative of the unseen test environment ẽ. Under this assumption, the model is trained to minimize the empirical risk:</p><formula xml:id="formula_0">s t = {s 1 t , • • • , s M t } denote their joint state and s i t = (x i t , y i t )</formula><formula xml:id="formula_1">R(ϕ, g) := 1 |D| (x,y)∈D L task (g(ϕ(x)), y).<label>(1)</label></formula><p>where L task is the loss function of the motion forecasting task, such as mean square error (MSE) or negative loglikelihood (NLL). However, the i.i.d. assumption does not always hold in practice. In fact, recent work <ref type="bibr" target="#b12">[13]</ref> has shown that the test environment can be significantly different from the training ones in the widely used ETH-UCY benchmark.</p><p>We will next introduce a causal formalism of motion forecasting that allows us to formulate this challenge and design solutions to address it.</p><p>Causal formalism. Motion behaviors are essentially dynamic processes governed by latent variables, such as physical laws, traffic rules and social norms. To build accurate predictive models, the conventional learning paradigm typically aims to discover these latent variables and model their correlations with the observed future states. However, the learned correlations may vary across environments and thus fail to generalize at test time.</p><p>To tackle this fundamental challenge, we introduce a new formalism of motion forecasting through the lens of causality. As shown in Figure <ref type="figure" target="#fig_1">2</ref>, we categorize the latent variables into three groups:</p><p>• invariant variables: physical laws that are universal to everyone at any place; • hidden confounders: motion styles that may vary from site to site in a local and sparse manner; • spurious features: other variables, e.g., level of noises, that are not direct causes of future motion.</p><p>Neither the second nor the third group has stable correlations with the target future motion across environments. Yet, they may lead to distinctive effects on forecasting models. Spurious correlations can become drastically different in new settings, resulting in catastrophic errors, as illustrated in Figure <ref type="figure" target="#fig_2">3</ref>. In comparison, variations of motion styles are often more restricted. Models that fail to capture the correct motion style may suffer from inaccurate predictions but should still output plausible solutions subject to physical laws. We will next describe three algorithmic components that treat spurious features and hidden confounders differently in order to promote the robustness and reusability of learned motion representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Causal Invariant Forecasting</head><p>Invariant principle. By definition, invariant features should have identical joint distributions with the target variable (future motion) across different environments, whereas the non-invariant ones are the opposite. This distinction can be formulated as a necessary condition for the domain invariant predictor, i.e., g • ϕ is equally optimal in every environment <ref type="bibr" target="#b57">[58]</ref>. More formally, our goal is to solve the following problem:</p><formula xml:id="formula_2">min ϕ,g 1 |E| e∈E R e (ϕ, g) s.t. g ∈ arg min g * R e (ϕ, g * ) ∀e ∈ E,<label>(2)</label></formula><p>where g * is an optimal predictor built on top of the extracted features in an individual environment e.   <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b61">62]</ref> proposed to relax it to a gradient norm penalty over the empirical risk R e in each training environment:</p><formula xml:id="formula_3">min ϕ,g 1 |E| e∈E R e (ϕ, g) + λ∥∇ g R e (ϕ, g)∥ 2 2 .<label>(3)</label></formula><p>This objective prevents the forecasting model from learning an average effect of spurious features on future trajectories and enforces the model to solve it the hard way by seeking universal mechanisms behind motion behaviors. We will show in Sec. 4.1 that this technique can greatly improve the robustness of the forecasting model against distribution shifts of spurious features. However, the strength of suppressing spurious features comes with a clear drawback, i.e., the learned representation tends to erroneously drop the motion styles that change across environments. This may cause inaccurate predictions in both training and test environments. To cope with this issue, we next introduce a modular architecture that allows the model to properly structure the knowledge and strategically adapt from one style to another.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Modular Forecasting Model</head><p>Most recent forecasting models are built with dense connections at their core, albeit with some detailed differences. On the one hand, this design principle is very powerful when the training data is sufficient; on the other hand, it often lands in a highly inter-twined architecture that lacks semantic structure. As such, one may have to update the whole model even if distribution shifts only arise from variations of motion styles. This fine-tuning convention inevitably leads to low sample efficiency for transfer learning. Ideally, a forecasting model would preserve a clear structure of the learned knowledge, separate the impacts of physical laws and motion styles on motion behaviors, and approximate the high-level sparse causal graph in Figure <ref type="figure" target="#fig_1">2</ref>.</p><p>To achieve this goal, we devise a modular network that consists of two encoders and one decoder. The first encoder ϕ is trained to compute domain-invariant features, as described in §3.2. Subsequently, we introduce a second encoder ψ which aims to capture features of motion styles varying across domains. Given some style observations o from a particular environment e, the role of ψ is to produce a latent representation of the style confounders c. One key difference between the input to the style encoder and that to the invariant encoder lies in that the former is one (or multiple) long sequence where the motion style is fully observable, whereas the latter is the past trajectory x, which may not contain sufficient information about the underlying motion style, e.g., before interactions. More formally, our modular network predicts the future trajectory as follows:</p><formula xml:id="formula_4">z = ϕ(x), c = ψ(o), z = f (z, c) + z, ŷ = g(z),<label>(4)</label></formula><p>where z is the latent feature that incorporates both z and c, and the style modulator f can be modeled by a small multilayer perceptron (MLP). Here, we can also compute c based on multiple scene observations from the same environment, e.g., averaging several style feature vectors, to obtain a more robust estimate of the motion style. As shown in Figure <ref type="figure" target="#fig_3">4</ref>, our modular design allows us to precisely localize and fine-tune a small subset of parameters to account for the underlying style shift.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Style contrastive loss</head><p>Our modular forecasting model composed of multiple sub-networks can be practically difficult to train, especially in the few-shot transfer setting where data collected from the new environment is limited. To overcome this challenge, we introduce a style contrastive loss, which aims to not only strengthen the modular structure of motion representation during training but also allows for reusing the encoded style knowledge at test time.</p><p>Style contrastive learning. Ideally, the feature vector produced by the style encoder should not only provide the basic style information for predicting the future motion accurately but also properly capture the style relation between different scenes. We formulate this intuition into an auxiliary task in the form of supervised contrastive learning. Specifically, we consider two scene observations from the same environment as a pair of positive samples, whereas those from different environment as negative pairs. We map the style feature c to a projected embedding p by a small head h(•). The style contrastive loss for a positive pair of samples (i, j) is as follows,</p><formula xml:id="formula_5">L style = -log exp(sim(p i , p j )/τ ) k 1 [k=j∨e k ̸ =ei] exp(sim(p i , p k )/τ ) ,<label>(5)</label></formula><p>where 1 [e k ̸ =ei] is an indicator function equal to 1 if and only if the two samples i and k are drawn from the same environment, τ is a temperature parameter and sim(u, v) = u ⊤ v/∥u∥∥v∥ denotes the dot product between normalized u and v (cosine similarity).</p><p>One key advantage of the proposed style contrastive loss over the conventional classification loss is that it does not impose any assumptions about the number of domain classes in the design of the projection head h. This property allows the model to incrementally bootstrap from the knowledge already learned about the existing styles to some additional ones without changing the shape of h or learning any parameters from scratch. This is particularly beneficial as in the transfer setting where the number of additional styles is not known a priori.</p><p>Overall, we train our entire modular forecasting model in three steps: 1. train the predictor backbone ϕ and g based on the invariant loss (Eq. 3); 2. train the style embedding ψ and h based on the style contrastive loss (Eq. 5); 3. train ψ, f , g and h on the task loss (Eq. 1) and style loss (Eq. 5) jointly while freezing the invariant encoder ϕ. In the presence of style shifts, we fine-tune a subset of parameters, e.g., the style modulator f , in order to efficiently adapt the model from the learned domains to a new one.</p><p>Test-time style refinement. One common phenomenon in transfer learning is that the model fine-tuned on only a few samples remains sub-optimal in the new environment. To alleviate this performance gap, we reuse the style contrastive loss as a self-supervisory signal for test-time refinement on the fly. Concretely, we feed the predicted output back as an input to the style encoder, examine its style consistency with other observed samples, and iteratively adjust the internal feature z. Here, the variables to optimize are no longer the model weights but rather the feature activations for each test instance. The refinement process gradually reduces the distances between the predicted output and the reference examples of the same style in the learned embedding space. By tightly coupling the modular architecture design with the style contrastive loss, our method enables the effective use of the auxiliary contrastive task during both training and deployment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluate our proposed method on two types of forecasting models (recurrent STGAT <ref type="bibr" target="#b34">[35]</ref> and feedforward PECNet variant <ref type="bibr" target="#b50">[51]</ref>) under distribution shifts of spurious features or style confounders. In the considered forecasting task, a model processes the past 8 time steps (3.2 seconds) of human trajectories in the scene to then predict their future movements in the following 12 (4.8 seconds) time steps. Identical to many prior works <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b64">65]</ref>, we evaluate forecasting models on two metrics:</p><p>• Average Displacement Error (ADE): the average Euclidean distance between the predicted output and the ground truth over all predicted time steps. • Final Displacement Error (FDE): the Euclidean distance between the predicted final destination and the true final destination at the end of the prediction horizon. We evaluate each method over five experiments with different random seeds. More implementation details are summarized in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Spurious Shifts</head><p>We first evaluate the robustness of the forecasting model trained by our invariant loss under different ranges of spurious shifts. In particular, we compare our method against the two following baselines:</p><p>• Vanilla ERM: the conventional learning method that minimizes the average prediction error on all training samples (Eq. 1); • Counterfactual Analysis <ref type="bibr" target="#b12">[13]</ref>: a causality-inspired trajectory forecasting method that estimates and subtracts biased features through counterfactual interventions. For a fair comparison with the recent counterfactual approach <ref type="bibr" target="#b12">[13]</ref>, we implement our method based on the same open-sourced code. Specifically, we follow their choice of the base model, i.e., STGAT <ref type="bibr" target="#b34">[35]</ref>, in our experiments. The encoder of the STGAT contains two LSTMs and one graph attention network (GAT) to account for the historical trajectory and social interaction clues, while the decoder is modeled by an LSTM to rollout the future trajectory.</p><p>Setup. The original ETH-UCY dataset contains five subsets collected at different locations <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b56">57]</ref>. While recent work <ref type="bibr" target="#b12">[13]</ref> has highlighted the intrinsic differences between Figure <ref type="figure">5</ref>. Comparison of different methods on the ETH-UCY dataset with controlled spurious correlations. Our invariant learning approach substantially outperforms the conventional ERM and the counterfactual approach <ref type="bibr" target="#b12">[13]</ref> in the out-of-distribution regime α ∈ <ref type="bibr" target="#b7">(8,</ref><ref type="bibr" target="#b63">64]</ref>, while being on par within the training domains.</p><p>these subsets, it is still not trivial to pinpoint the detailed biases in each environment. To clearly examine the robustness of a motion forecasting model against non-causal biases, we modified the ETH-UCY dataset by introducing a third input variable measuring the level of observation noises, given that variations of spurious noise often occur in real-world problems <ref type="bibr" target="#b3">[4]</ref>. Specifically, at each time step t, we simulate the observation uncertainty σ t as a linear function of the local trajectory curvature (more details in Appendix B):</p><formula xml:id="formula_6">γ t := ( ẋt+δt -ẋt ) 2 + ( ẏt+δt -ẏt ) 2 , σ t := α • (γ t + 1),<label>(6)</label></formula><p>where ẋt = x t+1 -x t and ẏt = y t+1 -y t reflect the velocity of the agent within the temporal window of length δt = 8, and α is a domain specific parameter to control the strength of spurious features. We train the model in four environments ('hotel','univ','zara1' and 'zara2') with α ∈ {1, 2, 4, 8} and test it on the remaining one ('eth') with α ∈ {1, 2, 4, 8, 16, 32, 64}.</p><p>Results. In Figure <ref type="figure">5</ref> we show the prediction accuracy on the test sets resulting from different learning methods. All the methods perform strongly in the training domains, i.e., α ∈ <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b7">8]</ref>. However, in the out-of-distribution regime, the accuracies of the two baseline methods significantly drop with an increased value of the domain parameter. Notably, when the strength of the spurious feature is 8 times of the maximum strength seen during training, the ADE of the vanilla ERM rises to 3.0, approximately three times worse than its performance in the training domains. While the counterfactual approach <ref type="bibr" target="#b12">[13]</ref> is slightly better than the vanilla ERM, it also suffer suffers from a large ADE at ∼2.5. In comparison, the forecasting models trained by our invariant method are clearly less sensitive to the changes of the domain parameter. It is also visually distinct that a large emphasis (λ in Eq. 3) on the invariant penalty term during training leads to a more robust model under spurious shifts.</p><p>In Figure <ref type="figure" target="#fig_11">10</ref> we visualize the qualitative results on a particular test example of the augmented ETH-UCY dataset. While the input trajectories remain the same across all domains, a growing strength of the spurious feature causes a dramatic shrinkage of the predicted trajectories from the baseline methods. In contrast, the outputs of our method stay almost constant under spurious shifts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Style Shifts</head><p>We further evaluate the forecasting models trained by our method in the presence of style shifts. As elaborated in §3.4, it is often impractical for the model to directly generalize to new styles. We therefore consider two different scenarios: robustness in the zero-shot and transfer learning results in the low-shot setting.</p><p>Setup. The motion styles of existing real-world data are often largely unknown. We thus create some synthetic trajectories using ORCA <ref type="bibr" target="#b75">[76]</ref>, a popular multi-agent simulator, in circle-crossing scenarios <ref type="bibr" target="#b11">[12]</ref> with varied style parameters. Specifically, we consider three training styles where the simulated agents keep different minimum separation distances from each other, i.e., {0.1, 0.3, 0.5} meters. For each training domain, we generate 10,000 trajectories for training, 3,000 trajectories for validation and 5,000 trajectories for test. We evaluate each model on the training environments (IID) as well as the two new test environments with the minimum separation distance of 0.4 (OOD-Inter) and 0.6 (OOD-Extra). We use a variant of PECNet <ref type="bibr" target="#b50">[51]</ref> as our base model which employs a MLP as the basic building block for the encoders and decoder in our modular design. More implementation details are reported in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Out-of-distribution Generalization</head><p>Results. In Table <ref type="table">1</ref> we report the results of different forecasting models in both the training domain and the out-ofdistribution regimes. Similar to §4.1, the vanilla baseline suffers from much larger prediction errors in the OOD test sets than in the training ones. Given style changes, our invariant method alone does not yield clear advantages neither, as it tends to ignore the domain-specific style confounder. In contrast, our modular architecture design allows the model to effectively incorporate the domain-specific style features and thus achieves superior performance in all environments. In particular, training the first encoder ϕ in our modular network with the invariant loss results in the best robustness in the OOD regime while being competitive in the training domains. It is also evident that there remains a clear performance gap between the IID and OOD-Extra domain, which suggests the importance of building adaptive models investigated next. 0.063 ± 0.005 0.070 ± 0.006 0.112 ± 0.004 Inv + Mod (ours) 0.065 ± 0.007 0.069 ± 0.007 0.107 ± 0.007 Table <ref type="table">1</ref>. Quantitative comparison of different methods under style shifts. Models are evaluated by ADE (lower is better) over 5 seeds. Both the vanilla baseline and our invariant approach alone suffer from large errors, since they either average the domain-varying styles or ignore them. Our modular network incorporates distinctive style features into prediction and hence yields much better results. In particular, enforcing the causal invariance of the first encoder ψ leads to the best OOD robustness, while being highly competitive in the training environments.  Our modular adaptation strategy (updating the style modulator f ) yields higher sample efficiency than the conventional counterpart in the low-data regime. Moreover, refining the predicted output for 3 iterations further reduces the prediction error on the fly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Low-shot Transfer</head><p>As shown above ( §4.2.1), it is practically unrealistic for a forecasting model to directly generalize to all kinds of distribution shifts. We next evaluate the effectiveness of our proposed adaptation method in the context of low-shot transfer. We again consider the challenging OOD-Extra style shift scenario and compare the following options:</p><p>(a) conventional approach fine-tuning all parameters; (b) our modular adaptation strategy fine-tuning f only; (c) our test-time refinement on top of our method (b). We evaluate all methods given a limited number of samples, i.e., {1, 2, . . . , 6} × BS, where BS = 64 is the batch size.</p><p>Effect of modular adaptation. Figure <ref type="figure" target="#fig_7">6</ref> shows the results of different adaptation methods in the low-shot setting. In the case of only one batch of observations, fine-tuning all style-related parameters (ψ, f and g) leads to noisy outcomes and worsens the results on average. In comparison, updating f while keeping the remaining majority of the parameters fixed yields clearly better performance in the lowdata regime. For instance, fine-tuning f on two batches of the new style achieves the same level of prediction accuracy as fine-tuning the whole model on five batches. Conversely, when conditioned on a scene of a different style (small separation distance) as the reference, our method manages to steer the output towards the corresponding false style as well.</p><p>Effect of test-time refinement. Finally, we evaluate the effectiveness of test-time refinement based on the style contrastive loss. As shown in Figure <ref type="figure" target="#fig_7">6</ref>, our refinement techniques leads to substantial error reductions on top of the fine-tuned models. Figure <ref type="figure" target="#fig_8">7</ref> shows the qualitative effect on a two-agent scenario, where the predicted trajectories gradually get closer to the ground truth based on a scene observation of the target style as a reference. This result suggests a strong promise of reusing the structural knowledge learned in our modular forecasting model at test-time.</p><p>Additional results and discussions. Please refer to Appendix for additional experiments, implementation details as well as discussions about limitations and future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>We present a causality-inspired learning method for motion forecasting. Given data collected from multiple locations, our invariant loss yields stronger generalization than the previous statistical and counterfactual methods in the presence of spurious distribution shifts. In addition, our modular architecture design coupled with the proposed style contrastive loss enhances the robustness and transferability of learned motion representations under style shifts. Our results suggest that incorporating causal invariance and structure into representation learning is a promising direction towards robust and adaptive motion forecasting.  <ref type="bibr" target="#b34">[35]</ref> trained by our invariant approach substantially outperforms the vanilla ERM and the counterfactual counterparts <ref type="bibr" target="#b12">[13]</ref>. The number of sampled trajectories is set to 1 for computational efficiency. Results are averaged over 5 seeds. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Additional Experiments</head><p>Robustness on the original ETH-UCY dataset. In addition to the robustness experiments under controlled distribution shifts of spurious features ( §4.1) or style features ( §4.2), we also evaluate our method on the original ETH-UCY dataset, where each subset is subject to some unknown selection biases. We train the STGAT <ref type="bibr" target="#b34">[35]</ref> on four subsets ('eth','univ','zara1' and 'zara2') and test it on the rest ('hotel'). The results in Table <ref type="table" target="#tab_1">2</ref> confirms the strength of our method on real-world data.</p><p>Low-shot transfer on the SDD dataset. Apart from simulated style shifts in §4.2, we further evaluate our method on the SDD dataset under substantial style shifts. We create four different domains according to the agent type and average speed. We use three domains for training and the last one for evaluation. We apply our method on top of the Y-Net <ref type="bibr" target="#b49">[50]</ref>, and compare our modular adaptation strategy against the standard fine-tuning of the entire model for lowshot transfer. The results in Fig. <ref type="figure" target="#fig_9">8</ref> demonstrate the scalability of our method to real-world style shifts.</p><p>Larger style shifts. As a supplement to Table <ref type="table">1</ref>, we summarize the detailed results under larger style shifts in Table 3. Among these OOD test domains (d &gt; 0.5), the farther the style parameter is from the training ones, the larger improvement we obtain from using the full version of our method. This result confirms the advantage of our modular design with an enforced structure of the invariant and style knowledge for robust generalization. The full version (invariant + modular) of our method yields more performance gains with an increasing degree of style shifts. Style contrastive pre-training. As described in §3.4, one advantage of incorporating the proposed style contrastive loss is to ease the training of our modular model that consists of multiple sub-networks. In Figure <ref type="figure">9</ref> we compare the performance of the models during training with and without the style contrastive pre-training. The model pre-trained on the style contrastive task learns significantly faster than the counterpart without it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experimental Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1. Spurious Shift Experiments</head><p>Experimental design. To clearly examine the robustness of motion forecasting models against spurious shifts ( §4.1), we introduce an additional input variable σ t , concatenated to the 2D coordinates. Its value is defined as a linear function of the trajectory curvature γ t . By changing the scaling coefficient α, we artificially control a varied degree of spurious shifts. This controlled setting allows us to simulate the spurious correlation arising from two co-occurring phenomena in crowded spaces: observations become noisy due to occlusions; trajectories become non-linear because of interactions. Given this coincidence, statistical models may exploit the level of noise σ t to ease predictions. Yet, such non-causal models are brittle. Any changes of the noise pattern (e.g., due to perception algorithm updates illustrated in Figure <ref type="figure" target="#fig_2">3</ref>) may degrade forecasting accuracy.</p><p>Architecture. For the experiments reported in §4.1, we use the standard STGAT <ref type="bibr" target="#b34">[35]</ref> architecture for a fair comparison with the counterfactual analysis approach <ref type="bibr" target="#b12">[13]</ref>. In order to take the x and y coordinate as well as the observa- tional uncertainty σ t as inputs, we adjust the input dimension of the first LSTM module to three. All the remaining configurations align with the original STGAT. Following the previous work <ref type="bibr" target="#b12">[13]</ref>, we train the model in three steps: (i) pre-train the first LSTM, (ii) pre-train the GAT together with the second LSTM, (iii) train the whole model.</p><p>Hyper-parameters. We use the same hyper-parameters as in the original STGAT <ref type="bibr" target="#b34">[35]</ref>. For the invariant penalty coefficient λ, we run grid search in a range from 0.001 to 100. Since the focus of our experiments is on the robustness and adaptability under distribution shifts rather than the performance on training domains, we only predict one trajectory output per instance instead of multiple ones <ref type="bibr" target="#b28">[29]</ref> during training, which reduces computational expenses for the comparison of different methods. Other detailed hyperparameters are summarized in Table <ref type="table" target="#tab_4">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2. Style Shift Experiments</head><p>Architecture. For the experiments in §4.2, we use a PECNet-like <ref type="bibr" target="#b50">[51]</ref> feedforward network as our base model. Specifically, we model all components using MLPs. We train the modular network in four detailed steps: (i) train the invariant encoder together with the decoder, (ii) subsequently pre-train the style encoder and the projection head, (iii) followed by the style modulator, and (iv) finally train the entire model end-to-end.</p><p>Hyper-parameters. We keep most hyper-parameters identical to the setup in Appendix B.1. We further tune the learning rates for each module separately due to their distinct properties. Detailed settings for training, adaptation and refinement are summarized in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3. Other Details</head><p>We train all of our models on a single NVIDIA Tesla V100 GPU. Each run takes around one hour. The source code of our method as well as baselines can be found at https : / / github . com / vitaepfl/causalmotion and <ref type="url" target="https://github.com/sherwinbahmani/ynet_adaptive">https://github.com/ sherwinbahmani/ynet_adaptive</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Additional Discussions</head><p>To the best of our knowledge, our work provides the first attempt to incorporate causal invariance and structure into the design and learning of motion forecasting models. Despite encouraging results, our work is still subject to a couple of limitations.</p><p>Limitations &amp; future work. One major technical limitation lies in the granularity of the considered causal rep-resentations. While our method places great emphasis on three prominent groups of high-level latent features, we have largely overlooked the structure of fine-grained features. One interesting direction for future work is to further exploit detailed causal structure for motion forecasting, for instance, (i) disentangling the left or right-hand traffic rules from social distance conventions within the group of style confounders, (ii) encouraging sparse interplay between submodules, e.g., pruning the connections between inertia features and left or right-hand traffic rules, given their presumably minute significance.</p><p>Another limitation of our work is tied to the scale and diversity of experiments. Thus far, we have demonstrated the strengths of our method on two human motion datasets and two base models as proofs of concept. Nevertheless, our method is highly generic and we hypothesize that it can also bring similar benefits to other types of motion problems and datasets, e.g., vehicles <ref type="bibr" target="#b10">[11]</ref>, sports <ref type="bibr" target="#b83">[84]</ref> and driving simulations <ref type="bibr" target="#b51">[52]</ref>. Extending the current empirical findings to more contexts can be another valuable avenue for future work.</p><p>Societal impact. Out-of-distribution robustness remains a salient weakness of motion forecasting models while having a crucial impact on the safety of autonomous systems, especially in the context of autonomous driving. Even though these machines operate accurately in their training environments, deploying them in unseen test conditions can result in undesired behavior, which may ultimately lead to fatal consequences in specific scenarios. With our work, we contribute to reducing this performance gap. However, we are aware of the remaining deficits of our motion forecasting approach in significantly changing conditions that should not be neglected when utilizing such systems in real-world applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>*Figure 1 .</head><label>1</label><figDesc>Figure 1. Illustration of motion forecasting under environment changes. We introduce a framework that enables deep motion representations to robustly generalize to non-causal shifts of spurious features, e.g., agent density, and efficiently adapt to new motion styles, e.g., from right to left-hand traffic.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure2. Our causal formalism of motion forecasting. We cast the human motion problem as a dynamic process with three groups of latent variables: domain-invariant physical laws (z), domain-specific style confounders (c), and non-causal spurious features (s). The spurious features are not parents of future movements (y) in the causal graph, e.g., no edges or anti-causal (dotted line), and their statistical correlations may vary drastically under different conditions. This formalism motivates our design and training of forecasting models to promote the robustness and reusability of the learned motion representation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Illustration of spurious correlations in motion forecasting. The curvature of the target trajectory y is often correlated with spurious features s, such as observation noises and agent densities. Yet, such correlations are not robust. For instance, the noise level may correlate with y in different ways between training and test, due to the change of exogenous variables u, e.g., sensing devices. Likewise, the agent density and y are not causally related, but confounded by invariant features z, e.g., neighboring environment. In the two illustrated cases, the test example is much closer to the left training example than to the right one in essence. However, models built upon the spurious correlations in the training examples may output erroneous predictions ŷs even on simple test examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Our modular forecasting model contains two separate encoders for universal laws and style confounders, respectively. The model is built in three steps: (i) learn an invariant predictor based on the first encoder ϕ, with the goal to be equally optimal in all training environments ( §3.2), (ii) learn an embedding space based on the second encoder ψ to capture the style relation between different scenes ( §3.4), (iii) incorporate domain-specific style features into the forecasting model by training ψ, f , g and h on the main task and the auxiliary style contrastive task jointly ( §3.3- §3.4).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>± 0.004 0.112 ± 0.003 0.192 ± 0.013 Invariant (ours) 0.115 ± 0.005 0.114 ± 0.004 0.191 ± 0.007 Modular (ours)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Quantitative results of different methods for transfer learning to a new motion style, given limited batch of samples. Our modular adaptation strategy (updating the style modulator f ) yields higher sample efficiency than the conventional counterpart in the low-data regime. Moreover, refining the predicted output for 3 iterations further reduces the prediction error on the fly.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Qualitative effects of test-time refinement in a two-agent scenario. The initial predicted output suffers from a clear prediction error. Given a scene observation of the true style (large separation distance) as a reference, our method gradually closes the discrepancy between the predicted trajectory and the ground truth. Conversely, when conditioned on a scene of a different style (small separation distance) as the reference, our method manages to steer the output towards the corresponding false style as well.</figDesc><graphic coords="8,489.66,139.35,56.07,56.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Quantitative results of low-shot transfer on the SDD<ref type="bibr" target="#b60">[61]</ref> dataset. Our modular adaptation strategy yields higher sample efficiency than the conventional fine-tuning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>7 Figure 9 .</head><label>79</label><figDesc>Figure 9. Comparison of the models with and without style contrastive pre-training. The model pre-trained on the style contrastive task converges faster than the counterpart during the endto-end training in both domains.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 10 .</head><label>10</label><figDesc>Figure10. Visualization of the predicted trajectories from different methods in a particular test case of the ETH-UCY dataset with controlled spurious features. Despite the same past trajectory observation and ground truth future, the predicted trajectories from the two baselines abruptly slow down, when the strength of the spurious feature at test time is larger than that in the training domains. In comparison, our invariant learning approach results in much more robust solutions, even under substantial spurious shifts (e.g., α = 64.0).</figDesc><graphic coords="10,129.73,190.37,98.24,51.34" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Comparison of different methods on the original ETH-UCY dataset. The STGAT</figDesc><table><row><cell>Method</cell><cell cols="2">Vanilla (ERM) Counterfactual Invariant (ours)</cell></row><row><cell cols="2">ADE (↓) 0.536 ± 0.015 0.512 ± 0.057</cell><cell>0.457 ± 0.054</cell></row><row><cell>FDE (↓)</cell><cell>1.088 ± 0.039 1.029 ± 0.136</cell><cell>0.918 ± 0.098</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 .</head><label>3</label><figDesc>Mod (ours) 0.107 ± 0.007 0.156 ± 0.013 0.221 ± 0.020 ADE scores of different methods on OOD-Extra domains.</figDesc><table><row><cell>Method</cell><cell>d = 0.6</cell><cell>d = 0.7</cell><cell>d = 0.8</cell></row><row><cell>Vanilla (ERM)</cell><cell cols="3">0.192 ± 0.013 0.246 ± 0.020 0.309 ± 0.025</cell></row><row><cell>Invariant (ours)</cell><cell cols="3">0.191 ± 0.007 0.245 ± 0.009 0.309 ± 0.011</cell></row><row><cell>Modular (ours)</cell><cell cols="3">0.112 ± 0.004 0.169 ± 0.011 0.242 ± 0.020</cell></row><row><cell>Inv +</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 5 .</head><label>5</label><figDesc></figDesc><table><row><cell>config</cell><cell>value</cell></row><row><cell>batch size</cell><cell>64</cell></row><row><cell>epochs per stage</cell><cell>150, 100, 150</cell></row><row><cell>learning rate</cell><cell>0.001</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Hyper-parameters in spurious shift experiments.</figDesc><table><row><cell>config</cell><cell>value</cell></row><row><cell>batch size</cell><cell>64</cell></row><row><cell>epochs per stage</cell><cell>100, 50, 20, 300</cell></row><row><cell>contrastive loss coefficient</cell><cell>1.0</cell></row><row><cell>learning rate baseline</cell><cell>0.001</cell></row><row><cell>learning rate style encoder</cell><cell>0.0005</cell></row><row><cell>learning rate projection head</cell><cell>0.01</cell></row><row><cell>learning rate style modulator (train)</cell><cell>0.01</cell></row><row><cell>learning rate style modulator (adapt)</cell><cell>0.001</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Hyper-parameters in style shift experiments.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgments. This work is supported by the <rs type="funder">Swiss National Science Foundation</rs> under the Grant <rs type="grantNumber">2OOO21-L92326</rs>. We thank <rs type="person">Bastien Van Delft</rs>, <rs type="person">Brian Alan Sifringer</rs> and <rs type="person">Yifan Sun</rs> for thoughtful feedback on early drafts, <rs type="person">Parth Kothari</rs> and <rs type="person">Hossein Bahari</rs> for valuable suggestions on experiments, as well as reviewers for insightful comments.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_sjyBBCs">
					<idno type="grant-number">2OOO21-L92326</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Invariant Risk Minimization Games</title>
		<author>
			<persName><forename type="first">Kartik</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthikeyan</forename><surname>Shanmugam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kush</forename><surname>Varshney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amit</forename><surname>Dhurandhar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Social LSTM: Human Trajectory Prediction in Crowded Spaces</title>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kratarth</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vignesh</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Robicquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06">June 2016</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Concave penalized estimation of sparse gaussian bayesian networks</title>
		<author>
			<persName><forename type="first">Bryon</forename><surname>Aragam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2273" to="2328" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.02893</idno>
		<idno>arXiv: 1907.02893. 3</idno>
		<imprint>
			<date type="published" when="2020-03">Mar. 2020</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">Invariant Risk Minimization</note>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Seman-tic photo manipulation with a generative image prior</title>
		<author>
			<persName><forename type="first">David</forename><surname>Bau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hendrik</forename><surname>Strobelt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Peebles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2002">July 2019. 2</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Conditional independence testing using generative adversarial networks</title>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Bellot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihaela</forename><surname>Van Der Schaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Representation Learning: A Review and New Perspectives</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2013-08">Aug. 2013</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Generalizing from Several Related Classification Tasks to a New Unlabeled Sample</title>
		<author>
			<persName><forename type="first">Gilles</forename><surname>Blanchard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gyemin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clayton</forename><surname>Scott</surname></persName>
		</author>
		<idno>2011. 00168. 3</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<biblScope unit="volume">24</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">P</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Irina</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arka</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loic</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Watters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Lerchner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.03599</idno>
		<idno>arXiv: 1804.03599. 3</idno>
		<title level="m">Understanding disentangling in $\beta$-VAE</title>
		<imprint>
			<date type="published" when="2018-04">Apr. 2018</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Peter</forename><surname>Bühlmann</surname></persName>
		</author>
		<author>
			<persName><surname>Invariance</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.08233</idno>
		<idno>Dec. 2018. 00057 arXiv: 1812.08233. 3</idno>
		<title level="m">Causality and Robustness</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">nuScenes: A Multimodal Dataset for Autonomous Driving</title>
		<author>
			<persName><forename type="first">Holger</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varun</forename><surname>Bankiti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><forename type="middle">H</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sourabh</forename><surname>Vora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Venice</forename><forename type="middle">Erin</forename><surname>Liong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anush</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giancarlo</forename><surname>Baldan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oscar</forename><surname>Beijbom</surname></persName>
		</author>
		<idno>01006. 11</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="11621" to="11631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Crowd-Robot Interaction: Crowd-Aware Robot Navigation With Attention-Based Deep Reinforcement Learning</title>
		<author>
			<persName><forename type="first">Changan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuejiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sven</forename><surname>Kreiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2019-05">May 2019</date>
			<biblScope unit="page" from="6015" to="6022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Human Trajectory Prediction via Counterfactual Analysis</title>
		<author>
			<persName><forename type="first">Guangyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junlong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiwen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2009">2021. 2, 4, 6, 7, 9</date>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets</title>
		<author>
			<persName><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rein</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Bayesian causal structural learning with zero-inflated poisson bayesian networks</title>
		<author>
			<persName><forename type="first">Junsouk</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Chapkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Ni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Long-term path prediction in urban scenarios using circular distributions</title>
		<author>
			<persName><forename type="first">Pasquale</forename><surname>Coscia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Castaldo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Francesco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Palmieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvio</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lamberto</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName><surname>Ballan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="81" to="91" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">Gabriela</forename><surname>Csurka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.14176</idno>
		<idno>arXiv: 2012.14176. 3</idno>
		<title level="m">Deep Visual Domain Adaptation</title>
		<imprint>
			<date type="published" when="2020-12">Dec. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning Structured Representations of Spatial and Interactive Dynamics for Trajectory Prediction in Crowded Scenes</title>
		<author>
			<persName><forename type="first">Todor</forename><surname>Davchev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Burke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Subramanian</forename><surname>Ramamoorthy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference Name: IEEE Robotics and Automation Letters</title>
		<imprint>
			<date type="published" when="2021-04">Apr. 2021</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="707" to="714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Distributionally Robust Optimization Under Moment Uncertainty with Application to Data-Driven Problems</title>
		<author>
			<persName><forename type="first">Erick</forename><surname>Delage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinyu</forename><surname>Ye</surname></persName>
		</author>
		<idno>01272 Publisher: INFORMS. 3</idno>
	</analytic>
	<monogr>
		<title level="j">Operations Research</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="595" to="612" />
			<date type="published" when="2010-06">June 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning models with uniform performance via distributionally robust optimization</title>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">C</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongseok</forename><surname>Namkoong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1378" to="1406" />
			<date type="published" when="2021-06">June 2021</date>
			<publisher>Institute of Mathematical Statistics</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<author>
			<persName><forename type="first">Amir</forename><surname>Feder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><forename type="middle">A</forename><surname>Keith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emaad</forename><surname>Manzoor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reid</forename><surname>Pryzant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhanya</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zach</forename><surname>Wood-Doughty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Eisenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Grimmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roi</forename><surname>Reichart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename><forename type="middle">E</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brandon</forename><forename type="middle">M</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Veitch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diyi</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.00725</idno>
		<idno>arXiv: 2109.00725</idno>
		<title level="m">Causal Inference in Natural Language Processing: Estimation, Prediction, Interpretation and Beyond</title>
		<imprint>
			<date type="published" when="2021-09">Sept. 2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks</title>
		<author>
			<persName><forename type="first">Chelsea</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2003">July 2017. 3</date>
			<biblScope unit="page" from="1126" to="1135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Kernel measures of conditional dependence</title>
		<author>
			<persName><forename type="first">Kenji</forename><surname>Fukumizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohai</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="489" to="496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Domain-adversarial training of neural networks</title>
		<author>
			<persName><forename type="first">Yaroslav</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evgeniya</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hana</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Franc ¸ois Laviolette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2096" to="2030" />
			<date type="published" when="2003">Jan. 2016. 3</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.5401</idno>
		<idno>arXiv: 1410.5401. 3</idno>
		<title level="m">Neural Turing Machines</title>
		<imprint>
			<date type="published" when="2014-12">Dec. 2014. 02045</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A Kernel Two-Sample Test</title>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karsten</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Malte</forename><forename type="middle">J</forename><surname>Rasch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">25</biblScope>
			<biblScope unit="page" from="723" to="773" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Few-Shot Human Motion Prediction via Meta-Learning</title>
		<author>
			<persName><surname>Liang-Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Xiong</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deva</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename><forename type="middle">M F</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName><surname>Moura</surname></persName>
		</author>
		<idno>00073. 3</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="432" to="450" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno>Sept. 2020. 00109. 3</idno>
		<title level="m">International Conference on Learning Representations</title>
		<imprint/>
	</monogr>
	<note>Search of Lost Domain Generalization</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Social GAN: Socially Acceptable Trajectories with Generative Adversarial Networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018-06">June 2018</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Christina</forename><surname>Heinze-Deml</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marloes</forename><forename type="middle">H</forename><surname>Maathuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolai</forename><surname>Meinshausen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Causal Structure Learning. Annual Review of Statistics and Its Application</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="371" to="391" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Social Force Model for Pedestrian Dynamics</title>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Helbing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Molnar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physics Review E</title>
		<imprint>
			<date type="published" when="1998-05">May 1998. 1</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">Irina</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Amos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Pfau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastien</forename><surname>Racaniere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loic</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Lerchner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.02230</idno>
		<idno>arXiv: 1812.02230. 3</idno>
		<title level="m">Towards a Definition of Disentangled Representations</title>
		<imprint>
			<date type="published" when="2018-12">Dec. 2018</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Generalized score functions for causal discovery</title>
		<author>
			<persName><forename type="first">Biwei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clark</forename><surname>Glymour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1551" to="1560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Causal discovery from heterogeneous/nonstationary data with independent changes</title>
		<author>
			<persName><forename type="first">Biwei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiji</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">D</forename><surname>Ramsey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruben</forename><surname>Sanchez-Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clark</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">89</biblScope>
			<biblScope unit="page" from="1" to="53" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">STGAT: Modeling Spatial-Temporal Interactions for Human Trajectory Prediction</title>
		<author>
			<persName><forename type="first">Yingfan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huikun</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaoxin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlu</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaoqi</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2006">2019. 2, 6</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Causal discovery from soft interventions with unknown targets: Characterization and learning</title>
		<author>
			<persName><forename type="first">Amin</forename><surname>Jaber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Murat</forename><surname>Kocaoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthikeyan</forename><surname>Shanmugam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elias</forename><surname>Bareinboim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Information-geometric approach to inferring causal directions</title>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joris</forename><surname>Mooij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Lemeire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Zscheischler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Povilas</forename><surname>Daniušis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bastian</forename><surname>Steudel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">182</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="31" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Human Trajectory Forecasting in Crowds: A Deep Learning Perspective</title>
		<author>
			<persName><forename type="first">Parth</forename><surname>Kothari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sven</forename><surname>Kreiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference Name: IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Out-of-Distribution Generalization via Risk Extrapolation (REx)</title>
		<author>
			<persName><forename type="first">David</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joern-Henrik</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amy</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Binas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinghuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Remi</forename><surname>Le Priol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning</title>
		<meeting>the 38th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2021-07">July 2021</date>
			<biblScope unit="page" from="5815" to="5826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Crowds by Example</title>
		<author>
			<persName><forename type="first">Alon</forename><surname>Lerner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiorgos</forename><surname>Chrysanthou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dani</forename><surname>Lischinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Test-Time Personalization with a Transformer for Human Pose Estimation</title>
		<author>
			<persName><forename type="first">Yizhuo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miao</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zonglin</forename><surname>Di</surname></persName>
		</author>
		<idno>00000. 2</idno>
	</analytic>
	<monogr>
		<title level="m">Thirty-Fifth Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021-05">May 2021</date>
		</imprint>
	</monogr>
	<note>Nitesh Bharadwaj Gundavarapu, and Xiaolong Wang</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Collaborative Sampling in Generative Adversarial Networks</title>
		<author>
			<persName><forename type="first">Yuejiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parth</forename><surname>Kothari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<idno>Number: 04. 2</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020-04">Apr. 2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="4948" to="4956" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">TTT++: When Does Self-Supervised Test-Time Training Fail or Thrive?</title>
		<author>
			<persName><forename type="first">Yuejiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parth</forename><surname>Kothari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bastien</forename><surname>Germain Van Delft</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baptiste</forename><surname>Bellot-Gurlet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Mordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<idno>00000. 2</idno>
	</analytic>
	<monogr>
		<title level="m">Thirty-Fifth Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021-05">May 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Social NCE: Contrastive Learning of Socially-Aware Motion Representations</title>
		<author>
			<persName><forename type="first">Yuejiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="15118" to="15129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations</title>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Locatello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gunnar</forename><surname>Raetsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Bachem</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<meeting>the 36th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2003">May 2019. 3</date>
			<biblScope unit="page" from="4114" to="4124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Deep Transfer Learning with Joint Adaptation Networks</title>
		<author>
			<persName><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017-07">July 2017</date>
			<biblScope unit="page" from="2208" to="2217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Detecting causality from nonlinear dynamics with short-term time series</title>
		<author>
			<persName><forename type="first">Huanfei</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kazuyuki</forename><surname>Aihara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luonan</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific reports</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Towards Deep Learning Models Resistant to Adversarial Attacks</title>
		<author>
			<persName><forename type="first">Aleksander</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Makelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ludwig</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Tsipras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Vladu</surname></persName>
		</author>
		<idno>04340. 3</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018-02">Feb. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">You Mostly Walk Alone: Analyzing Feature Attribution in Trajectory Prediction</title>
		<author>
			<persName><forename type="first">Osama</forename><surname>Makansi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julius</forename><surname>Von Kügelgen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Locatello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Vincent Gehler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2001">Sept. 2021. 1</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">From Goals, Waypoints &amp; Paths to Long Term Human Trajectory Forecasting</title>
		<author>
			<persName><forename type="first">Karttikeya</forename><surname>Mangalam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harshayu</forename><surname>Girase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="15233" to="15242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">It Is Not the Journey But the Destination: Endpoint Conditioned Trajectory Prediction</title>
		<author>
			<persName><forename type="first">Karttikeya</forename><surname>Mangalam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harshayu</forename><surname>Girase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shreyas</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuan-Hui</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ehsan</forename><surname>Adeli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrien</forename><surname>Gaidon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Horst</forename><surname>Bischof</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jan-Michael</forename><surname>Frahm</surname></persName>
		</editor>
		<editor>
			<persName><surname>Editors</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Mcduff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yale</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiyoung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vibhav</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sai</forename><surname>Vemprala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Gyde</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuang</forename><surname>Hadi Salman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kwanghoon</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName><surname>Kapoor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.13364</idno>
		<idno>arXiv: 2106.13364. 11</idno>
		<title level="m">CausalCity: Complex Simulations with Agency for Causal Discovery and Reasoning</title>
		<imprint>
			<date type="published" when="2021-06">June 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Key-Value Memory Networks for Directly Reading Documents</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Fisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Dodge</surname></persName>
		</author>
		<author>
			<persName><surname>Amir-Hossein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Karimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><surname>Weston</surname></persName>
		</author>
		<idno>00631. 3</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Austin, Texas</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-11">Nov. 2016</date>
			<biblScope unit="page" from="1400" to="1409" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Neural cognitive maps (NCMs)</title>
		<author>
			<persName><forename type="first">T</forename><surname>Obata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hagiwara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Cybernetics and Simulation 1997 IEEE International Conference on Systems, Man, and Cybernetics</title>
		<imprint>
			<date type="published" when="1997-10">Oct. 1997</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="3337" to="3342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Learning Independent Causal Mechanisms</title>
		<author>
			<persName><forename type="first">Giambattista</forename><surname>Parascandolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Kilbertus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateo</forename><surname>Rojas-Carulla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018-07">July 2018</date>
			<biblScope unit="page" from="4036" to="4044" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">The Book of Why: The New Science of Cause and Effect</title>
		<author>
			<persName><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dana</forename><surname>Mackenzie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>Basic Books, Inc</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
	<note>USA, 1st edition</note>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Improving Data Association by Joint Modeling of Pedestrian Trajectories and Groupings</title>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Pellegrini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Ess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2010</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Causal inference by using invariant prediction: identification and confidence intervals</title>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Bühlmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolai</forename><surname>Meinshausen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Statistical Methodology)</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="947" to="1012" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<title level="m" type="main">Elements of Causal Inference: Foundations and Learning Algorithms. Adaptive Computation and Machine Learning series</title>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-11-01">Nov. 2017. 1, 2</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Distributionally Robust Optimization: A Review</title>
		<author>
			<persName><forename type="first">Hamed</forename><surname>Rahimian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Mehrotra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.05659</idno>
		<idno>arXiv: 1908.05659. 3</idno>
		<imprint>
			<date type="published" when="0150">Aug. 2019. 00150</date>
		</imprint>
	</monogr>
	<note>cs, math, stat</note>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Learning Social Etiquette: Human Trajectory Understanding In Crowded Scenes</title>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Robicquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<idno>ing. 9</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">Bastian</forename><surname>Leibe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Nicu</forename><surname>Sebe</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Max</forename><surname>Welling</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publish</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="549" to="565" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">The Risks of Invariant Risk Minimization</title>
		<author>
			<persName><forename type="first">Elan</forename><surname>Rosenfeld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pradeep</forename><surname>Kumar Ravikumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrej</forename><surname>Risteski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint/>
	</monogr>
	<note>Sept. 2020. 00046. 3, 5</note>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning</title>
		<author>
			<persName><forename type="first">Stephane</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Drew</forename><surname>Bagnell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Fourteenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2011-06">June 2011</date>
			<biblScope unit="page" from="627" to="635" />
		</imprint>
		<respStmt>
			<orgName>JMLR Workshop and Conference Proceedings</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">An Investigation of Why Overparameterization Exacerbates Spurious Correlations</title>
		<author>
			<persName><forename type="first">Shiori</forename><surname>Sagawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditi</forename><surname>Raghunathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pang</forename><surname>Wei Koh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020-11">Nov. 2020</date>
			<biblScope unit="page" from="8346" to="8356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Trajectron++: Dynamically-Feasible Trajectory Forecasting with Heterogeneous Data</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boris</forename><surname>Ivanovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Punarjay</forename><surname>Chakravarty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Pavone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2020</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Horst</forename><surname>Bischof</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Jan-Michael</forename><surname>Frahm</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Learning treewidth-bounded bayesian networks with thousands of variables</title>
		<author>
			<persName><forename type="first">Mauro</forename><surname>Scanagatta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giorgio</forename><surname>Corani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Cassio P De Campos</surname></persName>
		</author>
		<author>
			<persName><surname>Zaffalon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1462" to="1470" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Perceptual Causality in Children</title>
		<author>
			<persName><forename type="first">Anne</forename><surname>Schlottmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deborah</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carina</forename><surname>Linderoth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarah</forename><surname>Hesketh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Child Development</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1656" to="1677" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Toward Causal Representation Learning</title>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francesco</forename><surname>Locatello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><forename type="middle">Rosemary</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2021-05">May 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Double generative adversarial networks for conditional independence testing</title>
		<author>
			<persName><forename type="first">Chengchun</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wicher</forename><surname>Bergsma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lexin</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.1442</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Correlation Alignment for Unsupervised Domain Adaptation</title>
		<author>
			<persName><forename type="first">Baochen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Computer Vision and Pattern Recognition</title>
		<editor>
			<persName><forename type="first">Gabriela</forename><surname>Csurka</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="153" to="171" />
		</imprint>
	</monogr>
	<note>Domain Adaptation in Computer Vision Applications</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Test-Time Training with Self-Supervision for Generalization under Distribution Shifts</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020-11">Nov. 2020</date>
			<biblScope unit="page" from="9229" to="9248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Robustly Disentangled Causal Mechanisms: Validating Deep Representations for Interventional Robustness</title>
		<author>
			<persName><forename type="first">Raphael</forename><surname>Suter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Djordje</forename><surname>Miladinovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Bauer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019-05">May 2019</date>
			<biblScope unit="page" from="6056" to="6065" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Constraintbased causal discovery from multiple interventions over overlapping variable sets</title>
		<author>
			<persName><forename type="first">Sofia</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Tsamardinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2147" to="2205" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">The max-min hill-climbing bayesian network structure learning algorithm</title>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Tsamardinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><forename type="middle">E</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Constantin</forename><forename type="middle">F</forename><surname>Aliferis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="31" to="78" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Adversarial Discriminative Domain Adaptation</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Judy</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kate</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="7167" to="7176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Reciprocal n-Body Collision Avoidance</title>
		<author>
			<persName><forename type="first">Jur</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">J</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Guy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinesh</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><surname>Manocha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics Research, Springer Tracts in Advanced Robotics</title>
		<editor>
			<persName><forename type="first">Cédric</forename><surname>Pradalier</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Roland</forename><surname>Siegwart</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Gerhard</forename><surname>Hirzinger</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Generalizing to Unseen Domains via Adversarial Data Augmentation</title>
		<author>
			<persName><forename type="first">Riccardo</forename><surname>Volpi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongseok</forename><surname>Namkoong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ozan</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">C</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vittorio</forename><surname>Murino</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<idno>00251. 3</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018-01">Jan. 2018</date>
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">J</forename><surname>Vowels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Necati</forename><surname>Cihan Camgoz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Bowden</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.02582</idno>
		<idno>arXiv: 2103.02582. 2</idno>
		<title level="m">D&apos;ya like DAGs? A Survey on Structure Learning and Causal Discovery</title>
		<imprint>
			<date type="published" when="2021-03">Mar. 2021</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Learning to make things happen: Infants&apos; observational learning of social and physical causal events</title>
		<author>
			<persName><forename type="first">Anna</forename><surname>Waismeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">N</forename><surname>Meltzoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Child Psychology</title>
		<imprint>
			<biblScope unit="volume">162</biblScope>
			<biblScope unit="page" from="58" to="71" />
			<date type="published" when="2001">Oct. 2017. 1</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<author>
			<persName><forename type="first">Mei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihong</forename><surname>Deng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.03601</idno>
		<idno>arXiv: 1802.03601. 3</idno>
		<title level="m">Deep Visual Domain Adaptation: A Survey</title>
		<imprint>
			<date type="published" when="2018-05">May 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.03795</idno>
		<idno>arXiv: 2109.03795</idno>
		<title level="m">Desiderata for Representation Learning: A Causal Perspective</title>
		<imprint>
			<date type="published" when="2021-09">Sept. 2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Characterizing and learning equivalence classes of causal dags under interventions</title>
		<author>
			<persName><forename type="first">Karren</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abigail</forename><surname>Katcoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caroline</forename><surname>Uhler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Machine Learning</title>
		<meeting>the International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5541" to="5550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Causalvae: Disentangled representation learning via neural structural causal models</title>
		<author>
			<persName><forename type="first">Mengyue</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinwei</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianye</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/CVF Conferenceon Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="9593" to="9602" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Learning Fine-Grained Spatial Models for Dynamic Sports Play Prediction</title>
		<author>
			<persName><forename type="first">Yisong</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Carr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alina</forename><surname>Bialkowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iain</forename><surname>Matthews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE International Conference on Data Mining</title>
		<imprint>
			<date type="published" when="2014-12">Dec. 2014</date>
			<biblScope unit="page" from="670" to="679" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Few-shot Human Motion Prediction via Learning Novel Motion Dynamics</title>
		<author>
			<persName><forename type="first">Chuanqi</forename><surname>Zang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingtao</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Kong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-Ninth International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2020-07">July 2020. 00005</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="846" to="852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Central Moment Discrepancy (CMD) for Domain-Invariant Representation Learning</title>
		<author>
			<persName><forename type="first">Werner</forename><surname>Zellinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Grubinger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edwin</forename><surname>Lughofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Natschläger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susanne</forename><surname>Saminger-Platz</surname></persName>
		</author>
		<idno>00283. 3</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations, International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016-11">Nov. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Kernel-based conditional independence test and application in causal discovery</title>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Seventh Conference on Uncertainty in Artificial Intelligence, UAI&apos;11</title>
		<meeting>the Twenty-Seventh Conference on Uncertainty in Artificial Intelligence, UAI&apos;11<address><addrLine>Arlington, Virginia, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2011-07">July 2011</date>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title level="m" type="main">Multi-source Domain Adaptation in the Deep Learning Era: A Systematic Survey</title>
		<author>
			<persName><forename type="first">Sicheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Colorado</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kurt</forename><surname>Keutzer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.12169</idno>
		<idno>arXiv: 2002.12169. 3</idno>
		<imprint>
			<date type="published" when="2020-02">Feb. 2020</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">DAGs with NO TEARS: Continuous Optimization for Structure Learning</title>
		<author>
			<persName><forename type="first">Xun</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryon</forename><surname>Aragam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Pradeep K Ravikumar</surname></persName>
		</author>
		<author>
			<persName><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Motion Forecasting with Unlikelihood Training in Continuous Space</title>
		<author>
			<persName><forename type="first">Deyao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Zahran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><forename type="middle">Erran</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Elhoseiny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th Annual Conference on Robot Learning</title>
		<imprint>
			<date type="published" when="2002">June 2021. 2</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
