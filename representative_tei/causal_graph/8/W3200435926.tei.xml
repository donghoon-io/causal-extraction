<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Extracting Fine-Grained Knowledge Graphs of Scientific Claims: Dataset and Transformer-Based Results</title>
				<funder ref="#_gVhaXFt">
					<orgName type="full">Army Research Office</orgName>
					<orgName type="abbreviated">ARO</orgName>
				</funder>
				<funder>
					<orgName type="full">Defense Advanced Research Projects Agency</orgName>
					<orgName type="abbreviated">DARPA</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ian</forename><forename type="middle">H</forename><surname>Magnusson</surname></persName>
							<email>magnusson.i@northeastern.edu</email>
							<affiliation key="aff0">
								<orgName type="department">SIFT</orgName>
								<address>
									<settlement>Minneapolis</settlement>
									<region>MN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Northeastern University</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>MA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Scott</forename><forename type="middle">E</forename><surname>Friedman</surname></persName>
							<email>friedman@sift.net</email>
							<affiliation key="aff0">
								<orgName type="department">SIFT</orgName>
								<address>
									<settlement>Minneapolis</settlement>
									<region>MN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Extracting Fine-Grained Knowledge Graphs of Scientific Claims: Dataset and Transformer-Based Results</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:27+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent transformer-based approaches demonstrate promising results on relational scientific information extraction. Existing datasets focus on high-level description of how research is carried out. Instead we focus on the subtleties of how experimental associations are presented by building SciClaim, a dataset of scientific claims drawn from Social and Behavior Science (SBS), PubMed, and CORD-19 papers. Our novel graph annotation schema incorporates not only coarse-grained entity spans as nodes and relations as edges between them, but also fine-grained attributes that modify entities and their relations, for a total of 12,738 labels in the corpus. By including more label types and more than twice the label density of previous datasets, SciClaim captures causal, comparative, predictive, statistical, and proportional associations over experimental variables along with their qualifications, subtypes, and evidence. We extend work in transformer-based joint entity and relation extraction to effectively infer our schema, showing the promise of fine-grained knowledge graphs in scientific claims and beyond.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Using relations as edges to connect nodes consisting of extracted entity mention spans produces expressive and unambiguous knowledge graphs from unstructured text. This approach has been applied to diverse domains from moral reasoning in social media <ref type="bibr">(Friedman et al., 2021b)</ref> to qualitative structure in ethnographic texts <ref type="bibr">(Friedman et al., 2021a)</ref>, and is particularly useful for reasoning about scientific claims, where several experimental variables in a sentence may have differing relations. Scientific information extraction datasets such as SciERC <ref type="bibr" target="#b11">(Luan et al., 2018)</ref> use relations for labeling general scientific language. Utilizing the advances of SciBERT <ref type="bibr" target="#b2">(Beltagy et al., 2019)</ref> in scientific language modeling, SpERT <ref type="bibr">(Eberts and</ref> Text: Levels of social support for medical staff were significantly associated with self -efficacy and sleep quality and negatively associated with the degree of anxiety and stres</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Levels of social support</head><p>Factor Input: "Levels of social support for medical staff were significantly associated with self-efficacy and sleep quality and negatively associated with the degree of anxiety and stress."</p><p>Figure <ref type="figure">1</ref>: SciClaim knowledge graph with entities (nodes), relations (edges), and attributes (parentheticals) connecting an independent variable via arg0 to distinct correlations with dependent variables via arg1.</p><p>Ulges, 2020)-a transformer-based joint entity and relation extraction model-advanced the state of the art on SciERC.</p><p>To extend relational scientific information extraction to specifically target scientific claims, we annotate SciClaim,<ref type="foot" target="#foot_0">foot_0</ref> a dataset of 12,738 annotations on 901 sentences from expert identified claims in Social and Behavior Science (SBS) papers <ref type="bibr" target="#b0">(Alipourfard et al., 2021)</ref>, detected causal language in PubMed papers <ref type="bibr" target="#b14">(Yu et al., 2019)</ref>, and claims and causal language heuristically identified from <ref type="bibr">CORD-19 abstracts (Wang et al., 2020)</ref>.</p><p>For annotation, we developed a novel graph schema that reifies claimed associations as entity spans with fine-grained attributes and extracts factors as additional entities connected with relations to one or more associations in which they are involved. In Figure <ref type="figure">1</ref>, two association entities relate two pairs of dependent factors to an independent factor, while attributes and additional relations delimit the scope and qualitative proportionalities of the claim. Inspired by semantic role labeling, attributes modify associations and the roles of their arguments, allowing us to represent claims of causal, comparative, predictive, statistical, and proportional associations along with their qualifi-Text: We predicted that the subliminal prime would , under specifiable conditions , increase the accessibility of the pertinent negative outcome and thereby increase its perceived likelihood of occurrence . Input: "We predicted that the subliminal prime would, under specifiable conditions, increase the accessibility of the pertinent negative outcome and thereby increase its perceived likelihood of occurrence."</p><p>Figure <ref type="figure">2</ref>: This SciClaim graph captures the chaining together of associations and uncovers a mediating factor in the qualitative proportionality (q+) between the "subliminal prime" and "perceived likelihood of occurrence."</p><p>cations, subtypes, and evidence.</p><p>We adapt SpERT to model this additional multilabel attribute task and demonstrate that extraction of our highly expressive knowledge graphs is within reach of present methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Many previous datasets for relational scientific information extraction-such as SemEval 2017 task 10 and 2018 task 7, SciERC, and SciREX <ref type="bibr" target="#b1">(Augenstein et al., 2017;</ref><ref type="bibr" target="#b7">Gábor et al., 2018;</ref><ref type="bibr" target="#b11">Luan et al., 2018;</ref><ref type="bibr" target="#b9">Jain et al., 2020)</ref>-have annotated corpora from NLP, computer science, or similar engineering-oriented fields. As such their annotation schemas have emphasized the description of how research was carried out, by extracting categories of entities such as methods, tasks, metrics, and datasets as well as relations mostly describing their intrinsic properties such as uses, composition, and hyponymy. Two of these datasets <ref type="bibr" target="#b11">(Luan et al., 2018;</ref><ref type="bibr" target="#b7">Gábor et al., 2018)</ref> contain associative relations that directly link entities being compared or producing a result. Our work extends further in this direction by examining not only which entities are associated, but also how the presentation of the associations is nuanced by the assertion of fine-grained attributes such as causality or proportionality.</p><p>SciClaim provides the largest number of finegrained label types among comparable datasets. Table <ref type="table" target="#tab_0">1</ref> shows SciClaim's remarkable label densities per word. SciClaim also contains 81.88% as many total labels as SciERC and more total labels than SemEval 2017 task 10 and 2018 task 7. On the other hand, SciREX utilizes distant supervision from an existing knowledge base and noisy automatic labeling trained on SciERC to pro-vide an order of magnitude more labels and annotate complete documents. This is one example of how smaller yet more densely and directly labeled datasets like SciERC and SciClaim can enable and compliment larger, higher-level corpora.</p><p>Meanwhile, our dataset also focuses on scientific claims. Some previous work identifies claims within scientific texts <ref type="bibr" target="#b12">(Wadden et al., 2020;</ref><ref type="bibr" target="#b8">Gelman et al., 2021)</ref>, but does not extract the relations and factors within the claims themselves. Other recent symbolic semantic NLP systems do model relational representations of scientific claims (e.g., <ref type="bibr" target="#b4">Friedman et al., 2017)</ref>, but these approaches rely on rule-based engines with hand tuning, which require NLP experts to maintain and adapt to new domains. Instead we modify SpERT (Eberts and Ulges, 2020), a transformer-based method that has been shown to effectively extract relational scientific information on SciERC <ref type="bibr" target="#b11">(Luan et al., 2018)</ref>. We extend this model to accommodate our additional multi-label attributes and apply it to our claim graph extraction task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Definitions</head><p>SciClaim defines the multi-attribute knowledge graph extraction task as follows: for a sentence S of n tokens s 1 , ..., s n , and sets of entity types T e , attribute types T a , and relation types T r , predict the set of entities s j , s k , t ∈ T e ∈ E ranging from tokens s j to s k , where 1 ≤ j ≤ k ≤ n, the set of relations over entities e head ∈ E, e tail ∈ E, t ∈ T r ∈ R where e head = e tail , and the set of attributes over entities e ∈ E, t ∈ T a ∈ A. This defines a directed multi-graph without self-cycles, where each unique span can be represented by at To aid in the labeling of these densely annotated sentences, we iteratively trained on already collected data and utilized the predictions of the partially trained model on new training examples as suggestions in our labeling interface. We disabled these model suggestions on our 100 example test set to ensure that this did not bias our evaluation.</p><p>Due to the dense and potentially overlapping span annotations, small decisions about what tokens to include in a span frequently influence the span boundaries of several other entities in a sentence. However, most of these decisions have negligible impact on the meaningfulness of the annotation (e.g. the decision to include a determiner in span), rendering exact match agreement ineffective. Instead to promote consistency and domain relevance we employed iterative schema design sessions in consultation with a subject matter expert in reproducibility of SBS experiments and a process of consensus, schema re-development, and re-annotation on 250 examples where annotators overlapped.</p><p>Table <ref type="table" target="#tab_0">1</ref> contrasts SciClaim's label counts and density with the other relational scientific information extraction datasets discussed in Section 2, and precise counts for each label type are provided in Table <ref type="table" target="#tab_2">3</ref>. Further details are in Appendix A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Graph Schema</head><p>The SciClaim graph schema is designed to capture associations between factors (e.g., causation, comparison, prediction, statistics, proportionality), monotonicity constraints across factors, epistemic status, subtypes, and high-level qualifiers.</p><p>Text: Compared to control group , the isolated species from T2DM group had higher proteinase activity . Entities are labeled text spans. The same exact span cannot correspond to more than one entity type, but two entity spans can overlap. Entities comprise the nodes of SciClaim graphs upon which attributes and relations are asserted. Our schema includes six entity types: Factors are variables that are tested or asserted within a claim (e.g., "sleep quality" in Figure <ref type="figure">1</ref>). Associations are explicit phrases associating one or more factors (e.g., "higher" Figure <ref type="figure" target="#fig_2">3</ref>). Magnitudes are modifiers of an association indicating its likelihood, strength, or direction (e.g., "significantly" and "negatively" in Figure <ref type="figure">1</ref>). Evidence is an explicit mention of a study, theory, or methodology supporting an association (e.g., "our SEIR model"). Epistemics express the belief status of an association, often indicating whether something is hypothesized, assumed, or observed (e.g., "predicted" in Figure <ref type="figure">2</ref>). Qualifiers constrain the applicability or scope of an assertion (e.g., "for medical staff " in Figure <ref type="figure">1</ref>).</p><p>Attributes are multi-label fine-grained annotations (visualized in parentheses), where zero or more may apply to any given entity. Our schema includes the following attributes, all of which apply solely to Association entities: Causation ex-presses cause-and-effect over its constituent factors (e.g., both "increase" spans in Figure <ref type="figure">2</ref>). Correlation expresses interdependence over its constituent factors (e.g., both "associated with" spans in Figure <ref type="figure">1</ref>). Comparison expresses an association with a frame of reference (as in the "higher" statement of Figure <ref type="figure" target="#fig_2">3</ref>). Sign+ and Signexpresses high/low or increased/decreased factor value (e.g., "correlates more closely with" or "shortened" respectively). Test expresses statistical measurements (e.g., "ANOVA"). Indicates expresses a predictive relationship (e.g., "prognostic factors for").</p><p>Relations are directed edges between labeled entities in SciClaim graphs. They are critical for expressing what-goes-with-what over the set of entities. Note that in Figures <ref type="figure">1</ref> and<ref type="figure">2</ref> the unlabeled arrows are all modifier relations, left blank to avoid clutter. We encode six relations: arg0 relates an association to its cause, antecedent, subject, or independent variable (e.g., "levels of social support" in Figure <ref type="figure">1</ref>). arg1 relates an association to its result or dependent variable (e.g., "self-efficacy" and "stress" in Figure <ref type="figure">1</ref>). comp_to is an explicit frame of reference in a comparative association (e.g., "control group" in Figure <ref type="figure" target="#fig_2">3</ref>). subtype relates a head entity to a subtype tail (e.g., "stillbirth" as a subtype of "pregnancy outcome"). modifier relates associations to qualifiers, magnitudes, epistemics, and evidence (e.g., all unlabeled arrows in Figure <ref type="figure">1</ref> and Figure <ref type="figure">2</ref>). q+ and qindicate positive and negative qualitative proportionality, respectively, where increasing the head factor increases or decreases the tail factor, respectively (e.g., "levels of social support" is q+ to "sleep quality" and qto "stress" in Figure <ref type="figure">1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Model Architecture</head><p>In order to model the additional multi-label task in SciClaim, we extend SpERT (Eberts and Ulges, 2020) with an attribute classifier. SpERT provides components (Figure <ref type="figure" target="#fig_3">4 a-c</ref>) for joint entity and relation extraction and permits the overlapping spans in our data. These classifiers utilize a span representation that combines the SciBERT <ref type="bibr" target="#b2">(Beltagy et al., 2019)</ref> contextual embeddings of all tokens in the span through maxpooling, along with a context representation and learned width embedding. SpERT classifies entities first and only infers relations on pairs of identified entities.</p><p>Instead of maxpool we adopt an attention-based span representation (Figure <ref type="figure" target="#fig_3">4</ref>  et al. ( <ref type="formula">2017</ref>). This produces scalars α i,t for each SciBERT token vector h t in a span i using learned parameters w and b:</p><formula xml:id="formula_0">α i,t = exp(w • h t + b) EN D(i) k=ST ART (1) exp(w • h k + b)<label>(1)</label></formula><p>These attention weights are used to make a span representation ĥi with the following weighted sum:</p><formula xml:id="formula_1">ĥi = EN D(i) t=ST ART (1) α i,t h t (2)</formula><p>We use the same cascaded inference strategy and input the span representations of identified entities x a to an attribute classifier (Figure <ref type="figure" target="#fig_3">4 d</ref>) with weights W a and bias b a . A pointwise sigmoid σ yields seperate confidence scores ŷa for each attribute:</p><formula xml:id="formula_2">ŷa = σ(W a x a + b a ) (3)</formula><p>We train the attribute classifier with a binary cross entropy loss L a summed with the SpERT entity and relation losses, L e and L r , for a joint loss:</p><formula xml:id="formula_3">L = L e + L r + L a (4)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Evaluation</head><p>In Table <ref type="table" target="#tab_1">2</ref> we report micro performance metrics on the SciClaim test set averaged over 5 runs.</p><p>In addition to the modified SpERT (detailed in Section 3.4), we also test a variant attrs-as-ents where all attribute labels on an entity span are collapsed into a single combined annotation, allowing unmodified SpERT to process attributes. Precisely, we collapse T e entity types with all combinations of T a attribute types into {T e × Ta k : 0 ≤ k ≤ |T a |} multi-class entity labels. We hypothesized that the combinatorially larger number of labels required by attrs-as-ents would lower performance on rarely occurring combinations. Surprisingly the variants get almost identical results, suggesting that-at least for our data-a single layer classifier can infer the attributes of a span simultaneously just as well as doing so independently. We tested other model variants that also produced changes ∼1% F1 and thus are relegated to Appendix B.</p><p>To our knowledge no previous models exists that can run directly on all three tasks in our dataset due to the presence of both overlapped entity spans and multi-label attributes. For comparison we include SpERT's state-of-the-art performance on SciERC, the dataset closest to ours in terms of label density. The high performance of our adapted SpERT on SciClaim demonstrates the practicality of extracting our novel graph schema with present methods despite its fine-grained approach.</p><p>The per-class evaluations for our main model are reported in Table <ref type="table" target="#tab_2">3</ref>. With few exceptions performance is good, and generally follows support for the label in the dataset. The Causation attribute metrics may be influenced by noise from anomalously low representation in the test set (only 5 instances compared to 59 instances of Correlation). Likewise the Test attribute unfortunately does not appear in the test set at all, but receives validation F1 of 95.95% despite only appearing 25 times in the corpus. Another outlier, the subtype relation, is particularly challenging, especially with its low rate of occurrence, due to it being one of the few relation types occurring directly between factors rather than mediated through a reified association span. The q+/qrelations are likewise expressed as direct links between factors. Although these require complex reasoning about the qualitative proportionalities of factors (e.g., Figure <ref type="figure">2</ref>), they nonetheless receive promising results. The attributes Sign+/Signserve a similar role and provide partial redundancy for q+/qlabels, allowing downstream reasoning to back off to these less precise, more robust attributes when the qualitative proportionalities are not extracted.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>Previous scientific information extraction crafts useful high-level representation of papers, going as far as document level relations spanning thousands of words in <ref type="bibr" target="#b9">Jain et al. (2020)</ref>. Complimentary to these efforts, we propose fine-grained and densely annotated scientific information extraction that captures not just what is said but how it is presented and argued. SciClaim applies this approach to associative claims and demonstrates that existing models such as SpERT <ref type="bibr" target="#b3">(Eberts and Ulges, 2020)</ref> can be modified to accurately extract fine-grained knowledge graphs ripe for downstream reasoning. ) is sampled with the following keywords as a heuristic identification of claims and causal language similar to our expert identified data from PubMed and Social and Behavioral Science (SBS) papers: associated with, reduce, increase, leads to, led to, our result, greater, less, more, cause, demonstrate, show, improve. 200 of our sentences (100 from PubMed and 100 from SBS) were selected to intentionally minimize the likelihood of claims and causal language. This includes sentences that discuss factors and other entities present in our schema but either do not contain associations or frame associations in unusual ways such as rhetorical questions. We intend for this data to encourage robustness that maintains correct labels for partial graph extractions rather than simply hallucinating associations in all sentences. We employ the following heuristics to identify this data: We sample 100 PubMed sentences from <ref type="bibr" target="#b14">Yu et al. (2019)</ref> that are identified as having low causal content. We sample 50 titles from SBS paper present in <ref type="bibr" target="#b0">Alipourfard et al. (2021)</ref>, as titles contain factors but rarely contain explicit associations and may be present in input data from automatically extracted text from PDFs. Finally we sample 50 first lines of SBS papers from <ref type="bibr" target="#b0">Alipourfard et al. (2021)</ref>, as these lines frequently introduce topics or rhetorical questions which either lack associations or present highly hypothetical associations unlike those in our main corpus.</p><p>Each filtered data source was sampled chrono-logically.</p><p>We utilized the following procedure for labeling: The annotators undertook extensive, iterative schema design sessions in consultation with a subject matter expert in reproducibility of SBS experiments. After the schema was settled on pilot examples, a lead annotator established the annotation standards on several hundred examples through a process of relabeling and retraining the suggestion model. Once the suggestion model became effective, the lead annotator and model suggestions guided the other annotator in adopting the annotation standards. The lead annotator reviewed and corrected the 250 overlapping examples in a consensus process with the other annotator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Variants and Hyperparameters</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Variants</head><p>We experiment with several variants none of which substantially outperformed the others. SpERTmodified-maxpool contains our modifications but simply uses SpERT's original maxpooling span representation instead of the attention-based representations inspired by <ref type="bibr" target="#b10">Lee et al. (2017)</ref>. SpERTmodified-unfiltered forgoes cascading inferences and instead classifies all possible spans for attributes. Full test and averaged validation results are presented in Table <ref type="table" target="#tab_3">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Hyperparameters</head><p>The following hyperparameters and settings were selected using manual tuning of 10-fold cross validation on the training set and optimizing for average micro-f1 performance over all 3 tasks: language model SciBERT uncased, mini batch size 8, epochs 40, optimizer AdamW, linear scheduling, warm up 0.05, learning rate 5e-5, learning rate warm up 0.1, weight decay 0.01, max grad norm 1.0, size embedding dimension 25, dropout probability 0.1, maximum span size 20, attribute filter threshold 0.55, relation filter threshold 0.4.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Comparison attributes modify arguments to account for a (sometimes implicit) frame of reference.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Our extension of SpERT components (a, b, and c) with multi-label attributes (d) and attentionbased entity span representations (e).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Our SciClaim dataset contains the highest label densities per word and comparable label counts to other scientific information extraction datasets except SciREX, which uses distant supervision and noisy automatic labeling. Our dataset contains fine-grained attributes as additional labels, while SciERC contains coreference links.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Entities</cell><cell cols="2">Relations</cell><cell cols="2">Attributes/Corefs</cell><cell cols="2">Total Labels</cell></row><row><cell>Dataset</cell><cell>Words</cell><cell>Count</cell><cell cols="5">Per Word Count Per Word Count Per Word</cell><cell>Count</cell><cell>Per Word</cell></row><row><cell cols="3">SciREX 2512806 157680</cell><cell>6.27%</cell><cell>9198</cell><cell>0.37%</cell><cell>-</cell><cell>-</cell><cell>166878</cell><cell>06.64%</cell></row><row><cell>SemEval2017</cell><cell>84010</cell><cell>9946</cell><cell>11.84%</cell><cell>672</cell><cell>0.79%</cell><cell>-</cell><cell>-</cell><cell>10618</cell><cell>12.64%</cell></row><row><cell>SemEval2018</cell><cell>58144</cell><cell>7483</cell><cell>12.87%</cell><cell>1595</cell><cell>2.74%</cell><cell>-</cell><cell>-</cell><cell>9078</cell><cell>15.61%</cell></row><row><cell>SciERC</cell><cell>65334</cell><cell>8089</cell><cell>12.38%</cell><cell>4716</cell><cell>7.21%</cell><cell>2752</cell><cell>4.21%</cell><cell>15557</cell><cell>23.81%</cell></row><row><cell>SciClaim</cell><cell>20070</cell><cell>5548</cell><cell>27.64%</cell><cell>5346</cell><cell>26.64%</cell><cell>1844</cell><cell>9.19%</cell><cell>12738</cell><cell>63.47%</cell></row></table><note><p><p><p><p><p><p><p>most one entity node with zero to |T a | attributes.</p>3.2 Dataset Construction</p>To create SciClaim, two NLP researchers annotated 901 sentences from several sources: 336 from papers in Social and Behavior Science (SBS) with expert identified claims</p><ref type="bibr" target="#b0">(Alipourfard et al., 2021)</ref></p>, 411 filtered for causal language in PubMed papers</p><ref type="bibr" target="#b14">(Yu et al., 2019)</ref></p>, 135 containing claims and causal language identified from CORD-19 abstracts (Wang et al., 2020) with heuristic keywords, and 19 manual perturbations included only in training data.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>SciClaim 75.92 SciClaim SpERT-modified 89.81 87.87 88.83 91.89 82.62 87.01 76.43 73.72 75.05 Micro Precision, Recall, and F1 averaged over 5 runs on SciClaim with SciERC for comparison.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Entities</cell><cell></cell><cell></cell><cell>Attributes</cell><cell></cell><cell>Relations</cell></row><row><cell></cell><cell>Data Model</cell><cell></cell><cell></cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell></row><row><cell></cell><cell>SciERC SpERT</cell><cell></cell><cell></cell><cell cols="3">70.87 69.79 70.33</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>53.40 48.54 50.84</cell></row><row><cell></cell><cell>Label</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>S</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="5">factor 91.28 89.97 90.62 2756</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Entities</cell><cell cols="5">evidence 88.80 93.33 90.96 epistemic 91.21 72.17 80.52 association 92.45 88.20 90.27 1290 230 299 magnitude 87.71 88.38 88.02 613</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="4">qualifier 75.86 78.33 77.02</cell><cell>360</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Attributes</cell><cell cols="4">causation 38.15 60.00 46.20 comparison 86.69 80.00 83.19 indicates 84.79 76.25 80.20 sign+ 97.27 88.31 92.58 sign-91.97 72.86 81.28</cell><cell>342 329 84 542 202</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="4">correlation 98.42 84.41 90.88</cell><cell>320</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="5">arg0 79.53 75.03 77.19 1325</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Relations</cell><cell cols="5">arg1 79.92 77.57 78.71 1384 comp_to 65.86 60.00 62.78 187 modifier 77.71 76.21 76.95 1582 subtype 40.00 33.33 36.00 156</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="4">q+ 65.53 67.61 66.50</cell><cell>504</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="4">q-70.70 53.00 60.09</cell><cell>208</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>High Precision, Recall, and F1 across most types relative to total Support in SciClaim, using SpERT-modified averaged over 5 runs.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Micro Precision, Recall, and F1 averaged over 5 runs on the SciClaim test set as well as F1 averaged over the 3 tasks on 5 runs of SciClaim validation data (Avg Val F1).</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Entities</cell><cell></cell><cell></cell><cell>Attributes</cell><cell></cell><cell></cell><cell>Relations</cell></row><row><cell cols="2">Model Avg Val F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell><cell>P</cell><cell>R</cell><cell>F1</cell></row><row><cell>SpERT-attrs-as-ents</cell><cell>80.45</cell><cell cols="9">90.13 88.63 89.37 92.35 82.13 86.94 77.59 74.34 75.92</cell></row><row><cell>SpERT-modified</cell><cell>80.66</cell><cell cols="9">89.81 87.87 88.83 91.89 82.62 87.01 76.43 73.72 75.05</cell></row><row><cell>SpERT-modified-maxpool</cell><cell>80.22</cell><cell cols="9">90.32 88.54 89.42 92.00 80.90 86.09 76.11 75.92 75.99</cell></row><row><cell>SpERT-modified-unfiltered</cell><cell>79.99</cell><cell cols="9">89.28 88.03 88.64 91.65 80.74 85.84 75.62 73.98 74.78</cell></row><row><cell>A Claims Dataset</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">Our English language dataset SciClaim consists of</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">901 examples sentences divided into a training set</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">of 721 sentences, a validation set of 80 sentences,</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">and a test set of 100 sentences. The training and</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">validation data contain examples that were labeled</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">from corrected suggestions from a partially trained</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">model, while the test set was labeled from scratch</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">without any model suggestions as a starting point.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="3">Our data from CORD-19 (Wang et al., 2020</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Dataset available at https://github.com/siftech/SciClaim.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This material is based upon work supported by the <rs type="funder">Defense Advanced Research Projects Agency (DARPA)</rs> and <rs type="funder">Army Research Office (ARO)</rs> under Contract No. <rs type="grantNumber">W911NF-20-C-0002</rs>. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the <rs type="institution">Defense Advanced Research Projects Agency (DARPA)</rs> and <rs type="funder">Army Research Office (ARO)</rs>. We thank the reviewers for their helpful feedback.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_gVhaXFt">
					<idno type="grant-number">W911NF-20-C-0002</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We ran 32 trials on 5 RTX 2080 ti GPUs, where each trial takes roughly 20 minutes. Our model contains 110 million parameters.</p><p>We explored the following hyperparameter bounds: language model ∈ {BERT, SciBERT, SpanBERT, SciBERT tuned on SciERC }, epochs ∈ {20, 40, 80}, batch size ∈ {4, 8, 16}, learning rate ∈ {1e-5, 5e-5, 1e-4}, scheduling ∈ {linear, cyclic }, warm up ∈ {0.0, 0.05, 0.1, 0.15. 0.2, 0.25, 0.3}, attribute filter threshold ∈ {0.4, 0.5, 0.55, 0.6}, relation filter threshold ∈ {0.35, 0.4, 0.5, 0.6}. The remaining settings we inherit from SpERT as initial experimentation on early datasets revealed little impact.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Nazanin</forename><surname>Alipourfard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beatrix</forename><surname>Arendt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Daniel M Benjamin</surname></persName>
		</author>
		<author>
			<persName><surname>Benkler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Burstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bush</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiling</forename><surname>Caverlee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chae</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Clark</surname></persName>
		</author>
		<idno type="DOI">10.31235/osf.io/46mnb</idno>
		<title level="m">Systematizing confidence in open research and evidence</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>score</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">ScienceIE -extracting keyphrases and relations from scientific publications</title>
		<author>
			<persName><forename type="first">Isabelle</forename><surname>Augenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mrinal</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lakshmi</forename><surname>Vikraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/S17-2091</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017)</title>
		<meeting>the 11th International Workshop on Semantic Evaluation (SemEval-2017)<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="546" to="555" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">SciB-ERT: A pretrained language model for scientific text</title>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1371</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3615" to="3620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Span-based joint entity and relation extraction with transformer pre-training</title>
		<author>
			<persName><forename type="first">Markus</forename><surname>Eberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Ulges</surname></persName>
		</author>
		<idno type="DOI">10.3233/FAIA200321</idno>
	</analytic>
	<monogr>
		<title level="m">24th European Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning by reading: Extending and localizing against a model</title>
		<author>
			<persName><forename type="first">Scott</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Burstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Mcdonald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Plotnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurel</forename><surname>Bobrow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rusty</forename><surname>Bobrow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brent</forename><surname>Cochran</surname></persName>
		</author>
		<author>
			<persName><surname>Pustejovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Cognitive Systems</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="77" to="96" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Extracting qualitative causal structure with transformer-based nlp</title>
		<author>
			<persName><forename type="first">Scott</forename><forename type="middle">E</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">H</forename><surname>Magnusson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sonja</forename><forename type="middle">M</forename><surname>Schmer-Galunder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">QR2021 @ IJCAI</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">2021b. Toward Transformer-Based NLP for Extracting Psychosocial Indicators of Moral Disengagement</title>
		<author>
			<persName><forename type="first">Scott</forename><forename type="middle">E</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">H</forename><surname>Magnusson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sonja</forename><forename type="middle">M</forename><surname>Schmer-Galunder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruta</forename><surname>Wheelock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Gottlieb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pooja</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annual Meeting of the Cognitive Science Community (CogSci)</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">SemEval-2018 task 7: Semantic relation extraction and classification in scientific papers</title>
		<author>
			<persName><forename type="first">Kata</forename><surname>Gábor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Buscaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anne-Kathrin</forename><surname>Schumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Behrang</forename><surname>Qasemizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haïfa</forename><surname>Zargayouna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thierry</forename><surname>Charnois</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/S18-1111</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 12th International Workshop on Semantic Evaluation</title>
		<meeting>The 12th International Workshop on Semantic Evaluation<address><addrLine>New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="679" to="688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Toward a robust method for understanding the reproducibility and replicability of research</title>
		<author>
			<persName><forename type="first">Ben</forename><surname>Gelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chae</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ugur</forename><surname>Kuter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Gentile</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
			<publisher>AAAI Workshop on Scientific Document Understanding</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">SciREX: A challenge dataset for document-level information extraction</title>
		<author>
			<persName><forename type="first">Sarthak</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madeleine</forename><surname>Van Zuylen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.670</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7506" to="7516" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">End-to-end neural coreference resolution</title>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D17-1018</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Copenhagen, Denmark</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="188" to="197" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multi-task identification of entities, relations, and coreference for scientific knowledge graph construction</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mari</forename><surname>Ostendorf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1360</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3219" to="3232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fact or fiction: Verifying scientific claims</title>
		<author>
			<persName><forename type="first">David</forename><surname>Wadden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shanchuan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucy</forename><forename type="middle">Lu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madeleine</forename><surname>Van Zuylen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.609</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7534" to="7550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Lucy</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoganand</forename><surname>Chandrasekhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russell</forename><surname>Reas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiangjiang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doug</forename><surname>Burdick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Darrin</forename><surname>Eide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathryn</forename><surname>Funk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yannis</forename><surname>Katsis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rodney</forename><surname>Kinney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunyao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Merrill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dewey</forename><surname>Murdick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devvret</forename><surname>Rishi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerry</forename><surname>Sheehan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhihong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brandon</forename><surname>Stilson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Wade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nancy</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ru</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Wilhelm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boya</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douglas</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oren</forename><surname>Etzioni</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>and Sebastian Kohlmeier. 2020. Cord-19: The covid-19 open research dataset</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Detecting causal language use in science findings</title>
		<author>
			<persName><forename type="first">Bei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingya</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1473</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4664" to="4674" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
