<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DAG-WGAN: CAUSAL STRUCTURE LEARNING WITH WASSERSTEIN GENERATIVE ADVERSARIAL NETWORKS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Hristo</forename><surname>Petkov</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer and Information Sciences</orgName>
								<orgName type="institution">University of Strathclyde</orgName>
								<address>
									<settlement>Glasgow</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Colin</forename><surname>Hanley</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Management Science</orgName>
								<orgName type="institution">University of Strathclyde</orgName>
								<address>
									<settlement>Glasgow</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Feng</forename><surname>Dong</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer and Information Sciences</orgName>
								<orgName type="institution">University of Strathclyde</orgName>
								<address>
									<settlement>Glasgow</settlement>
									<country key="GB">United Kingdom</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">the University of Strathclyde. He was awarded a PhD in Zhejiang University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">DAG-WGAN: CAUSAL STRUCTURE LEARNING WITH WASSERSTEIN GENERATIVE ADVERSARIAL NETWORKS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.5121/csit.2022.120611</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Generative Adversarial Networks</term>
					<term>Wasserstein Distance</term>
					<term>Bayesian Networks</term>
					<term>Causal Structure Learning</term>
					<term>Directed Acyclic Graphs</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The combinatorial search space presents a significant challenge to learning causality from data. Recently, the problem has been formulated into a continuous optimization framework with an acyclicity constraint, allowing for the exploration of deep generative models to better capture data sample distributions and support the discovery of Directed Acyclic Graphs (DAGs) that faithfully represent the underlying data distribution. However, so far no study has investigated the use of Wasserstein distance for causal structure learning via generative models. This paper proposes a new model named DAG-WGAN, which combines the Wasserstein-based adversarial loss, an auto-encoder architecture together with an acyclicity constraint. DAG-WGAN simultaneously learns causal structures and improves its data generation capability by leveraging the strength from the Wasserstein distance metric. Compared with other models, it scales well and handles both continuous and discrete data. Our experiments have evaluated DAG-WGAN against the state-of-the-art and demonstrated its good performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>Discovering causal relationships yields new scientific knowledge. Causal discovery involves the process of learning structures of Bayesian Networks (BN) from data. BNs are considered to be one of the most powerful models for causal inference <ref type="bibr" target="#b0">[1]</ref>. They are graphical models representing variables and their conditional dependencies in the form of directed acyclic graphs (DAG).</p><p>However, one of the major challenges associated with causal structure learning arises from its combinatorial nature. Since increasing the number of variables, resulting in a super-exponential increase of possible DAGs, makes the problem computationally intractable, it is often not possible to perform a complete combinatorial search. Nevertheless, over the last few years, several approaches have been proposed to overcome the NP-hardness of finding DAGs <ref type="bibr" target="#b1">[2]</ref>, including the score-based methods, constraint-based methods and continuous optimization. These have achieved a varying degree of success.</p><p>Score-based methods (SBM) reformulate the combinatorial structure learning approach into the optimization of a score function accompanied by a combinatorial constraint for acyclicity. They rely on the utilization of general optimization techniques across the combinatorial search space to discover the DAG structure by optimizing the score function. However, as the complexity of the search space remains super-exponential, additional structure assumptions and approximate searches are often needed. Constraint-based methods (CBM) utilize conditional independence tests to find the DAG structure by identifying the connections between dependent variables. Also, there have been attempts to combine the score-based and constraint-based approaches. One notable product of this idea is the MMHC <ref type="bibr" target="#b2">[3]</ref> algorithm which uses Hill Climbing as the score function and an algorithm called Min-Max Parents and Children (MMPC) to check for relationships between the variables.</p><p>A recent breakthrough by Zheng et al. <ref type="bibr" target="#b3">[4]</ref> utilizes a new acyclicity constraint to transform the problem from combinatorial to continuous optimization, which can be efficiently solved by conventional optimization methods. However, their original algorithm works only with linear data and does not support discrete data. Yu et al. <ref type="bibr" target="#b4">[5]</ref> further expand on Zheng's work by developing a model named DAG-GNN based on the variational auto-encoder architecture. It also reformulates the acyclicity constraint from Zheng et al. <ref type="bibr" target="#b3">[4]</ref> to allow for more efficient computation. In addition, their method handles both linear, non-linear continuous and categorical data. Similarly, another model named GraN-DAG <ref type="bibr" target="#b5">[6]</ref> has also been proposed to use neural networks together with the acyclicity constraint to handle both linear and non-linear continuous and discrete/categorical data for causality learning.</p><p>Our work is focused on the use of generative adversarial networks (GANs) for causal structure learning. GANs have been successfully applied to generating synthetic images by minimizing the difference between synthetic and real data with distance metrics. They have also been experimented with to synthesize tabular data <ref type="bibr" target="#b6">[7]</ref>. Recently, Gao et al. <ref type="bibr" target="#b7">[8]</ref> developed a GAN-based model (DAG-GAN) that learns causal structures from data by using a Maximum Mean Discrepancy (MMD) based score function.</p><p>To leverage GANs for causal structure learning, a fundamental question is whether the data distribution metrics involved in GANs can facilitate causal structure learning. Correspondingly, this work has investigated Wasserstein GAN (WGAN) in the context of learning causal structures from tabular data. The Wasserstein distance metric from optimal transport distance <ref type="bibr" target="#b8">[9]</ref> is an established metric that preserves basic metric properties <ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref><ref type="bibr" target="#b11">[12]</ref><ref type="bibr" target="#b12">[13]</ref>, which has led to the Wasserstein GAN (WGAN) to achieve significant improvement in training stability and convergence by addressing the vanishing gradient problem and partially removing mode collapse <ref type="bibr" target="#b13">[14]</ref>. However, to the best of our knowledge, so far no study has been conducted to experiment with WGAN for causality learning.</p><p>The proposed DAG-WGAN is based upon the combination of an auto-encoder and WGAN-GP by incorporating a critic (discriminator) that is designed to measure the Wasserstein distance between the real and synthetic data, together with the acyclicity constraint from Yu et al. <ref type="bibr" target="#b4">[5]</ref>. This combination allows us to compare the performance of DAG-WGAN with other relevant models that do not involve WGAN in order to test the hypothesis about the Wasserstein metric, namely whether the involvement of the Wasserstein metric can help causal structure learning in a generative process that learns how to realistically generate synthetic data. With the explicit modelling of learnable causal relations of DAGs in the model architecture, the model learns how to generate synthetic data by simultaneously optimizing the causal structure and the model parameters via end-to-end training.</p><p>Our experiments show that the new model performs better than other models when presented with a large data variable size. In particular, the causal graphs learned by using DAG-WGAN are more accurate in higher dimensions compared to those produced by other models and the quality of the generated data is higher than that produced from other data generating models. We demonstrate the capabilities of our model on multiple data types (linear, non-linear, continuous and discrete). The model works well with both continuous and discrete data while being capable of producing less noisy and more realistic data samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">RELATED WORK</head><p>DAGs lie at the centre of causal structure learning. They consist of nodes (variables) and directed edges (connections) between the nodes, which are interpreted as direct causal relationships between the variables. If a DAG entails conditional independencies of the variables in a joint distribution, the faithfulness condition allows us to recover the DAG from the joint distribution <ref type="bibr" target="#b0">[1]</ref>. In causal structure learning, we learn DAGs from data distributions that are exhibited with data samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Constraint and Score based DAG Learning Approaches</head><p>There are three main approaches for learning DAGs from data, including the constraint-based, score-based and hybrid approaches.</p><p>Constraint-based search methods create graph structures by running local independence tests to manually constrain the search space <ref type="bibr" target="#b14">[15]</ref>. Examples of constraint-based algorithms include Causal Inference (CI) <ref type="bibr" target="#b15">[16]</ref> and Fast Causal Inference (FCI) <ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref>. However, typically these methods only lead to equivalence classes, namely, a set of candidate causal structures that satisfy the same conditional independencies. Hence, the causal information in the output is not complete. Score-based methods use a score function to measure how well different graphs fit the data in order to identify the right causal structure based on the scores. Typical score functions include Bayesian Gaussian equivalent (BGe) <ref type="bibr" target="#b18">[19]</ref>, Bayesian Discrete equivalent (BDe) <ref type="bibr" target="#b19">[20]</ref>, Bayesian Information Criterion (BIC) <ref type="bibr" target="#b20">[21]</ref>, Minimum Description Length (MDL) <ref type="bibr" target="#b21">[22]</ref>. As the search space is often intractable, additional assumptions about the DAGs must be made -the most commonly used ones are bounded tree-width <ref type="bibr" target="#b22">[23]</ref>, tree-structure <ref type="bibr" target="#b23">[24]</ref> and sampling-based structure learning <ref type="bibr" target="#b24">[25]</ref><ref type="bibr" target="#b25">[26]</ref><ref type="bibr" target="#b26">[27]</ref>.</p><p>Hybrid methods use a mix of score-based and constraint-based methods to learn DAGs. One such model named Max-Min-Hill-Climbing combines constraint-based modelling and search-based learning for more accurate DAG results <ref type="bibr" target="#b2">[3]</ref>. Another example is RELAX <ref type="bibr" target="#b27">[28]</ref>, which introduces "constraint relaxation" of possibly inaccurate independence constraints of the search space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">DAG Learning with Continuous Optimization</head><p>Recently, a new approach named DAG-NOTEARS was formulated by Zheng et al. <ref type="bibr" target="#b3">[4]</ref>, which transforms the causal graph structure learning problem from its combinatorial nature into a continuous optimization framework. The success of this method facilitates the usage of conventional optimization solvers for causal structure learning. However, the DAG-NOTEARS model has limitations in handling non-linear data. Also, it only supports continuous data.</p><p>New solutions for causal structure learning have been developed recently based on DAG-NOTEARS. Yu et al. <ref type="bibr" target="#b4">[5]</ref> developed DAG-GNN, which performs causal structure learning by using a Variational Auto-Encoder architecture. Their model extends the capabilities of DAG-NOTEARS as it works with linear and non-linear continuous and discrete data. GraN-DAG proposed by <ref type="bibr" target="#b5">[6]</ref> is another extension from DAG-NOTEARS <ref type="bibr" target="#b3">[4]</ref> to handle non-linear data by learning causal relations between the variables using neural networks. The calculation of the neural network weights is constrained by the acyclicity constraint between the variables. The model makes very few assumptions about the data and variables and can generalize well. In addition, the model can work with both continuous and discrete data. Meanwhile, according to the latest work DAG-NoCurl from <ref type="bibr" target="#b28">[29]</ref>, it is also possible to learn DAGs without explicit DAG constraints.</p><p>Notably, DAG-GAN proposed by <ref type="bibr" target="#b7">[8]</ref> is one of the latest works that uses GAN for causal structure learning. The work involves using Maximum Mean Discrepancy (MMD) in its loss function. The resulting model handles multiple data types (continuous and discrete) However, their experiments have only covered up to 40 nodes in the graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">CAUSAL STRUCTURE LEARNING WITH DAG-WGAN</head><p>This section provides an overview of the DAG-WGAN model architecture, together with the details of its loss functions and model training. We cover both continuous and discrete data types. The DAG-WGAN model involves causal structure in the model architecture by incorporating an adjacency matrix under an acyclicity constraint -see Figure <ref type="figure" target="#fig_0">1</ref>. The model has two main components: (1) an auto-encoder which computes the latent representations of the input data; and (2) a WGAN which consists of a critic to synthesize the data with adversarial loss. The decoder of the auto-encoder is also used as the generator in the WGAN to generate synthetic data. The encoder is trained with the reconstruction loss while the decoder is trained according to both the reconstruction and adversarial loss. The joint WGANs and auto-encoders are motivated by the success of the combination of a variational auto-encoder (VAE) with GAN to better capture data and feature representation <ref type="bibr" target="#b29">[30]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Auto-encoder (AE) and Reconstruction</head><p>The encoder produces latent representations of the data and the decoder reconstructs the data. The representations are regularized to prevent over-fitting. The use of the auto-encoder makes sure that the latent space contains meaningful representations as the noise input to the generator in the adversarial loss training.</p><p>Similar to Yu et al. <ref type="bibr" target="#b4">[5]</ref>, we explicitly model the causal structure in both the encoder and decoder by using the structural causal model (SCM). The encoder Enc is as follows: <ref type="bibr" target="#b0">(1)</ref> where f1 is a parameterized function to transform X, X ∈ ℝ m×d is a data sample from a joint distribution of m variables in d dimensions. Z ∈ ℝ m×d is the latent representation. A ∈ ℝ m×m is the weighted adjacency matrix. The corresponding decoder Dec is as follows: <ref type="bibr" target="#b1">(2)</ref> where f2 is also a parameterized function that conceptually inverses f1. The functions f1 and f2 can perform both linear and non-linear transformations on Z and X. Each variable corresponds to a node in the weighted adjacency matrix A.</p><p>The AE computes the latent representations through the reconstruction loss. The reconstruction loss term is defined as:</p><p>(3</p><formula xml:id="formula_0">)</formula><p>where MX is a product of the decoder.</p><p>To avoid over-fitting, a regularizer is added to the reconstruction loss. The regularizer loss term takes the following form: <ref type="bibr" target="#b3">(4)</ref> where MZ is the output of the encoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Wasserstein Generative Adversarial Network</head><p>The decoder of the auto-encoder is also used as the generator of the WGAN model. Alongside the auto-encoder, we use a critic to provide the adversarial loss with gradient penalty. It is based upon the popular PacGAN <ref type="bibr" target="#b30">[31]</ref> framework with the aim of successfully handling mode collapse and is implemented as follows: <ref type="bibr" target="#b4">(5)</ref> where x ̃ is the data produced from the generator and X is the input data used for the model. Leaky-ReLU is the activation function and Dropout is used for stability and over-fitting prevention. GP stands for Gradient Penalty <ref type="bibr" target="#b12">[13]</ref> and is used in the loss term of the critic. Pac is a notion coming from PacGAN <ref type="bibr" target="#b30">[31]</ref> and is used to prevent mode collapse in categorical data, which we found practically useful in terms of improving the outcomes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training</head><p>The final loss function takes the form of: <ref type="bibr" target="#b5">(6)</ref> where L (x, x') and regularizer are defined by Equation ( <ref type="formula">3</ref>) and (4), respectively. In Equation ( <ref type="formula">6</ref>) D is the discriminator and λ is the penalty coefficient used to calculate the gradient penalty associated with the Wasserstein metric. Distribution-wise, ℙg and ℙr are the generated and real distributions, while ℙX̂ is produced by sampling uniformly along a straight line between the aforementioned distributions.</p><p>We utilize the critic loss LD to train the critic, the generator loss LG to train the generator (namely the decoder in the AE), and reconstruction loss LR for both the encoder and decoder. To enforce acyclicity, we use the acyclicity constraint proposed by Yu et al. <ref type="bibr" target="#b4">[5]</ref>, where A is the weighted adjacency matrix of the causal graph, m is the number of the variables, tr is a matrix trace and • is the Hadamard product <ref type="bibr" target="#b31">[32]</ref> of A.</p><p>The acyclicity requirement associated with DAG structure learning reformulates the nature of the structure learning approach into a constrained continuous optimization. As such, we treat our approach as a constrained optimization problem and use the popular augmented Lagrangian method <ref type="bibr" target="#b32">[33]</ref> to solve it.</p><p>In addition, our model naturally handles discrete variables by reformulating the reconstruction loss term using the Cross-Entropy Loss (CEL) as follows: <ref type="bibr" target="#b6">(7)</ref> where PX is the output of the decoder and X is the input data for the auto-encoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENTS</head><p>This section provides experimental results of the model performance by comparing against other related approaches. In particular, our experiments try to identify the contribution from the Wasserstein loss to causal structure learning by making a direct comparison with DAG-GNN <ref type="bibr" target="#b4">[5]</ref> where a similar auto-encoder architecture was used without involving the Wasserstein loss. Furthermore, we have also compared the results against DAG-NOTEARS [4] and DAG-NoCurl <ref type="bibr" target="#b28">[29]</ref>. All the comparisons are measured using the Structural Hamming Distance (SHD) <ref type="bibr" target="#b33">[34]</ref>. More specifically, we measure the SHD between the learned causal graph and the ground truth graph. Moreover, we also test the integrity of the generated data against CorGAN <ref type="bibr" target="#b6">[7]</ref>.</p><p>The implementation was based on PyTorch <ref type="bibr" target="#b34">[35]</ref>. In addition, we used learning rate schedulers and Adam optimizers for both discriminator and auto-encoder with a learning rate of 3-e3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Continuous data</head><p>To evaluate the model with continuous data, our experiments tried to learn causal graphs from synthetic data that were created with known causal graph structures and equations. To allow comparisons, we employed the same underlying graphs and equations like those in the related work, namely DAG-GNN <ref type="bibr" target="#b4">[5]</ref>, DAG-NoCurl <ref type="bibr" target="#b28">[29]</ref> and DAG-NOTEARS <ref type="bibr" target="#b3">[4]</ref>.</p><p>More specifically, the data synthesis was performed in two steps: 1) generating the ground truth causal graph and 2) generating samples from the graph based on the linear SEM of the ground truth graph. In Step (1), we generated an Erdos-Renyi directed acyclic graph with an expected node degree of 3. The DAG was represented in a weighted adjacency matrix A. In Step (2), a sample X was generated based on the following equations. We used the linear SEM X = A T x+z for the linear case, and two different equations, namely X = A T h(x)+z (non-linear-1) and X = 2sin(A T (x+0.5)) + A T cos(x+0,5) + z (non-linear-2) for the nonlinear case. In particular, the two non-linear equations were selected because they were used synthetic data experiments with similar models (DAG-GNN and all other models involved in its evaluation), which allows for more reliable model comparison and more comprehensive experiments.</p><p>The experiments were conducted with 5000 samples per graph. The graph sizes used in the experiments were 10, 20, 50 and 100. We measured the SHD (averaged over five different iterations of each model) between the output of a model and the ground truth, and the outcome was compared against those from the related work models (i.e. those mentioned at the beginning of Section 4). In addition to the mean SHD, confidence intervals were also measured based on the variance in the estimated means. These provide insight into the consistency of the model. Tables <ref type="table" target="#tab_0">1</ref><ref type="table" target="#tab_1">2</ref><ref type="table" target="#tab_3">3</ref>show the results on continuous data samples:  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Data Generation</head><p>DAG-WGAN was also evaluated by comparing its data generation capabilities against other models. More specifically, we compare the data generation capabilities of the models on a 'dimension-wise probability' basis by measuring how well these models learn the real data distributions per dimension. We used the MIMIC-III dataset <ref type="bibr" target="#b35">[36]</ref> in the experiments as the same dataset was also used in other comparable works. The data is presented in the form of a patient record, where each record has a fixed size of 1071 entries.</p><p>Figure <ref type="figure" target="#fig_1">2</ref> depicts the results of the experiment. We have only compared with CorGAN <ref type="bibr" target="#b6">[7]</ref> as it out-performs the other similar models such as medGAN <ref type="bibr" target="#b36">[37]</ref> and DBM <ref type="bibr" target="#b37">[38]</ref> -see <ref type="bibr" target="#b6">[7]</ref> where results of the other models are available. We present the results in a scatter plot, where each point represents one of the 1071 entries and the x and y axes represent the success rate for real and synthetic data respectively. In addition, we use a diagonal line to mark the ideal scenario. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">DISCUSSION</head><p>The results on the continuous datasets are competitive across all three cases (linear, non-linear-1 and non-linear-2). According to Tables 1, 2 and 3, our model dominates DAG-NOTEARS and DAG-NoCurl, producing better results throughout all experiments and outperforming DAG-GNN in most of the cases.</p><p>For small-scale datasets (e.g. d=10 and d=20), our model performs better than DAG-GNN in most cases and is superior to DAG-NOTEARS and DAG-NoCurl in all the experiments.</p><p>For large-scale datasets (e.g. d=50 and d=100), our model outperforms all the other models used in the study by a significantly large margin, which implies that the model can scale better. This is a significant advantage.</p><p>Notably, our experiments have mainly focused on the comparisons with DAG-GNN, as we aim to identify contributions from the Wasserstein loss to causal structure learning. In DAG-GNN, a very similar auto-encoder architecture was employed and DAG-WGAN has added WGAN as an additional component. Hence the comparison is meaningful in order to identify contributions from the Wasserstein metric.</p><p>For the discrete case, the results from the comparison between the two models are competitive. Out of the five experiments conducted during the study, four results were clearly in favor of our model (i.e.DAG-WGAN) and in one case DAG-GNN was slightly better.</p><p>Also, according to the results illustrated in Figure <ref type="figure" target="#fig_1">2</ref>, dimension-wise, the data generated using DAG-WGAN is more accurate and of higher quality than the ones generated using CorGAN, medGAN or DBM.</p><p>These results show that DAG-WGAN can handle both continuous and discrete data effectively. They have also demonstrated the quality of the generated data. As the improvement was achieved by introducing the Wasserstein loss in addition to the auto-encoder architecture, the comparisons between them show that the hypothesis on the contribution from the Wasserstein metric to causal structure learning stands.</p><p>However, the discrepancy which occurred in the synthetic continuous data results provides us with an insight into the limitations of the model. Some of our early analysis shows that further improvement can be achieved by generalizing the current auto-encoder architecture. Furthermore, as it stands currently, our model does not handle vector or mixed-typed data. These aspects will be further experimented with and reported in our future work.</p><p>On the topic of potential improvements, the capability of recovering latent representation places the generative models in a good position to address the hidden confounder challenges in causality learning -some earlier work from <ref type="bibr" target="#b38">[39]</ref><ref type="bibr" target="#b39">[40]</ref> have moved towards this direction. We will further investigate whether DAG-WGAN can contribute. Last but not least, the latest work in DAG-NoCurl <ref type="bibr" target="#b28">[29]</ref> shows that the speed performance can be improved by avoiding the DAG constraints. We will investigate how this new development can be adapted to DAG-WGAN to improve its overall performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSION</head><p>This work studies the use of the Wasserstein distance metric for causal structure learning from tabular data. We investigate if the inclusion of the Wasserstein metric as an adversarial loss can simultaneously improve structure learning while generating more realistic data samples. This leads to a novel approach for learning causal structures, which we coined DAG-WGAN. The effectiveness of DAG-WGAN has been demonstrated through a series of experiments, which show that the new model using the Wasserstein metric can indeed improve the outcomes of causal structure learning. The improved quality of the synthesized data in turn leads to an improvement in causal structure learning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. DAG-WGAN Model Architecture</figDesc><graphic coords="4,120.00,409.70,355.20,130.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Data generation test results</figDesc><graphic coords="9,121.70,127.90,351.60,124.10" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 .</head><label>1</label><figDesc>Comparisons of DAG Structure Learning Outcomes between DAG-NOTEARS, DAG-NoCurl, DAG-GNN and DAG-WGAN with Linear Data Samples</figDesc><table><row><cell></cell><cell></cell><cell cols="2">SHD (5000 linear samples)</cell><cell></cell></row><row><cell>Model</cell><cell>d = 10</cell><cell>d = 20</cell><cell>d = 50</cell><cell>d = 100</cell></row><row><cell>DAG-</cell><cell>8.4 ± 7.94</cell><cell>2.6 ± 1.84</cell><cell>25.2 ± 19.82</cell><cell>106.56 ±</cell></row><row><cell>NOTEARS</cell><cell></cell><cell></cell><cell></cell><cell>56.51</cell></row><row><cell>DAG-NoCurl</cell><cell>7.9 ± 7.26</cell><cell>2.5 ± 1.93</cell><cell cols="2">24.6 ± 19.43 99.18 ± 55.27</cell></row><row><cell>DAG-GNN</cell><cell>6 ± 7.77</cell><cell>3.2 ± 1.6</cell><cell>21.4 ± 14.15</cell><cell>88.8 ± 47.63</cell></row><row><cell>DAG-WGAN</cell><cell>2.2 ± 4.4</cell><cell>2 ± 1.1</cell><cell>4.8 ± 4.26</cell><cell>28.20 ± 12.02</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 .</head><label>2</label><figDesc>Comparison of DAG Structure Learning Outcomes between DAG-NOTEARS, DAG-NoCurl, DAG-GNN and DAG-WGAN with Non-Linear Data Samples 1</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Model SHD (5000 non-linear-1 samples)</head><label></label><figDesc></figDesc><table><row><cell></cell><cell>d = 10</cell><cell>d = 20</cell><cell>d = 50</cell><cell>d = 100</cell></row><row><cell>DAG-</cell><cell>11.2 ± 4.79</cell><cell>19.3 ± 3.14</cell><cell>53.7 ± 11.39</cell><cell>105.47 ±</cell></row><row><cell>NOTEARS</cell><cell></cell><cell></cell><cell></cell><cell>13.51</cell></row><row><cell>DAG-NoCurl</cell><cell>10.4 ± 4.42</cell><cell>17.4 ± 3.27</cell><cell cols="2">51.6 ± 11.43 105.7 ± 13.65</cell></row><row><cell>DAG-GNN</cell><cell>9.40 ± 0.8</cell><cell>15 ± 3.58</cell><cell>49.8 ± 7.03</cell><cell>104.8 ± 12.84</cell></row><row><cell>DAG-WGAN</cell><cell>9.8 ± 2.4</cell><cell>16 ± 5.4</cell><cell cols="2">40.40 ± 10.97 80.40 ± 9.09</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 .</head><label>3</label><figDesc>Comparison of DAG Structure Learning Outcomes between DAG-NOTEARS, DAG-NoCurl, DAG-GNN and DAG-WGAN with Non-Linear Data Samples 2To evaluate the model with discrete data, we used the benchmark datasets available at the Bayesian Network Repository https://www.bnlearn.com/bnrepository/ . The repository provides a variety of datasets together with their ground truth graphs (Discrete Bayesian Networks, Gaussian Bayesian Networks and Conditional Linear Gaussian Bayesian Networks) in different sizes (Small Networks, Medium Networks, Large Networks, Very Large Networks and Massive Networks). To test the scalability of our model, we used datasets of multiple sizes. The datasets utilized in the experiment were Sachs, Alarm, Child, Hailfinder and Pathfinder. The SHD metric was used to measure the performance. Table4contains the results from the experiment.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">SHD (5000 non-linear-2 samples)</cell><cell></cell></row><row><cell>Model</cell><cell>d = 10</cell><cell>d = 20</cell><cell>d = 50</cell><cell>d = 100</cell></row><row><cell>DAG-</cell><cell>9.8 ± 2.61</cell><cell>22.9 ± 2.14</cell><cell>38.3 ± 13.19</cell><cell>125.21 ±</cell></row><row><cell>NOTEARS</cell><cell></cell><cell></cell><cell></cell><cell>61.19</cell></row><row><cell>DAG-NoCurl</cell><cell>7.4 ± 2.78</cell><cell>17.6 ± 2.25</cell><cell>33.6 ± 12.53</cell><cell>116.8 ± 62.3</cell></row><row><cell>DAG-GNN</cell><cell>2.6 ± 2.06</cell><cell>3.80 ± 1.94</cell><cell>13.8 ± 6.88</cell><cell>112.2 ± 59.05</cell></row><row><cell>DAG-WGAN</cell><cell>1 ± 1.1</cell><cell>3.4 ± 2.06</cell><cell cols="2">12.20 ± 7.81 20.20 ± 11.67</cell></row><row><cell cols="2">4.2. Benchmark discrete data</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 .</head><label>4</label><figDesc>Comparison of DAG Structure Learning Outcomes between DAG-WGAN and DAG-GNN with Discrete Data Samples</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>SHD</cell></row><row><cell>Dataset</cell><cell>Nodes</cell><cell>DAG-</cell><cell>DAG-GNN</cell></row><row><cell></cell><cell></cell><cell>WGAN</cell><cell></cell></row><row><cell>Sachs</cell><cell>11</cell><cell>17</cell><cell>25</cell></row><row><cell>Child</cell><cell>20</cell><cell>20</cell><cell>30</cell></row><row><cell>Alarm</cell><cell>37</cell><cell>36</cell><cell>55</cell></row><row><cell>Hailfinder</cell><cell>56</cell><cell>73</cell><cell>71</cell></row><row><cell>Pathfinder</cell><cell>109</cell><cell>196</cell><cell>218</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Computer Science &amp; Information Technology (CS &amp; IT)</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AUTHORS</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988">1988</date>
			<publisher>Morgan Kaufmann Publishers</publisher>
			<pubPlace>Los Angeles, California</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Large sample learning of Bayesian networks is np-hard</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Chickering</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Heckerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Meek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1287" to="1330" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The max-min hill-climbing Bayesian network structure learning algorithm</title>
		<author>
			<persName><forename type="first">I</forename><surname>Tsamardinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Aliferis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="31" to="78" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Aragam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<title level="m">DAGs with NOTEARS: Continuous Optimization for Structure Learning. Paper presented at 32nd Conference on Neural Information Processing Systems, Montr éal</title>
		<meeting><address><addrLine>Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">DAG-GNN: DAG Structure Learning with Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<idno>PMLR 97</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<meeting>the 36th International Conference on Machine Learning<address><addrLine>Long Beach, California</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note>Copyright 2019 by the author(s</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Gradient-Based Neural DAG Learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lachapelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Brouillard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Deleu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lacoste-Julien</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Paper presented at ICLR 2020</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">CORGAN: Correlation-Capturing Convolutional Neural Networks for Generating Synthetic Healthcare Records</title>
		<author>
			<persName><forename type="first">A</forename><surname>Torfi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Fox</surname></persName>
		</author>
		<idno>FLAIRS-33</idno>
	</analytic>
	<monogr>
		<title level="m">The Thirty-Third International FLAIRS Conference</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">DAG-GAN: Causal Structure Learning with Generative Adversarial Nets</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-T</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Mathematical methods of organizing and planning production</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">V</forename><surname>Kantorovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Management Science</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="366" to="422" />
			<date type="published" when="1960">1960</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Sinkhorn Distances: Lightspeed Computation of Optimal Transport</title>
		<author>
			<persName><forename type="first">M</forename><surname>Cuturi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">26</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">W2GAN: RECOVERING AN OPTIMAL TRANSPORT MAP WITH A GAN</title>
		<author>
			<persName><forename type="first">L</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>She</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Almahairi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Rajeswar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>Paper presented at ICLR 2018</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning Generative Models with Sinkhorn Divergences</title>
		<author>
			<persName><forename type="first">A</forename><surname>Genevay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Peyr É</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cuturi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">21st International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Wasserstein GAN</title>
		<author>
			<persName><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Stabilizing Generative Adversarial Networks: A Survey</title>
		<author>
			<persName><forename type="first">M</forename><surname>Wiatrak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">V</forename><surname>Albrecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nystrom</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1910.00927" />
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint at</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An algorithm for fast recovery of sparse causal graphs</title>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Glymour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Social Science Computer Review</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="62" to="72" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Causality: models, reasoning, and inference</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econometric Theory</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">46</biblScope>
			<biblScope unit="page" from="675" to="685" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Causal Inference in the Presence of Latent Variables and Selection Bias</title>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Meek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Paper presented at UAI&apos;95: Proceedings of the Eleventh conference on Uncertainty in artificial intelligence</title>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">On the completeness of orientation rules for causal discovery in the presence of latent confounders and selection bias</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">172</biblScope>
			<biblScope unit="page" from="1873" to="1896" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Addendum on the scoring of Gaussian directed acyclic graphical models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kuipers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Moffa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Heckerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1689" to="1691" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning Bayesian networks: The combination of knowledge and statistical data</title>
		<author>
			<persName><forename type="first">D</forename><surname>Heckerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Chickering</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="197" to="243" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Efficient approximations for the marginal likelihood of Bayesian networks with hidden variables</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Chickering</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Heckerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="181" to="212" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Probabilistic network construction using the minimum description length principle</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bouckaert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on symbolic and quantitative approaches to reasoning and uncertainty</title>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Advances in Learning Bayesian Networks of Bounded Treewidth</title>
		<author>
			<persName><forename type="first">S</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Maua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>De Campos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Approximating discrete probability distributions with dependence trees</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="462" to="467" />
			<date type="published" when="1968">1968</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Structure learning in Bayesian networks of a moderate size by efficient sampling</title>
		<author>
			<persName><forename type="first">R</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3483" to="3536" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Bayesian graphical models for discrete data</title>
		<author>
			<persName><forename type="first">D</forename><surname>Madigan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>York</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Allard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Statistical Review / Revue Internationale de Statistique</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="215" to="232" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Being Bayesian about network structure. a Bayesian approach to structure discovery in Bayesian networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="95" to="125" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Constraint Relaxation for Learning the Structure of Bayesian Networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fast</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jensen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts Amherst</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">DAGs with No Curl: An Efficient DAG Structure Learning Approach</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">Y</forename><surname>Yue Yu</surname></persName>
		</author>
		<author>
			<persName><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning</title>
		<meeting>the 38th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Auto-encoding beyond Pixels Using a Learned Similarity Metric</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B L</forename><surname>Larsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Winther</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on International Conference on Machine Learning</title>
		<meeting>the 33rd International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">PAC-GAN: Packet Generation of Network Traffic using Generative Adversarial Networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 10th Annual Information Technology, Electronics and Mobile Communication Conference</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Johnson</surname></persName>
		</author>
		<title level="m">Matrix Analysis</title>
		<meeting><address><addrLine>Cambridge, New York</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Nonlinear Programming</title>
		<author>
			<persName><forename type="first">D</forename><surname>Bertsekas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Athena Scientific</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
	<note>nd edition</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">A comparison of structural distance measures for causal Bayesian network models. Recent Advances in Intelligent Information Systems</title>
		<author>
			<persName><forename type="first">M</forename><surname>De Jongh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Druzdzel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="443" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">PyTorch: An Imperative Style, High-Performance Deep Learning Library</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">rd Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E W</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Pollard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-W</forename><forename type="middle">H</forename><surname>Lehman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Ghassemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Moody</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Szolovits</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Celi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">G</forename><surname>Mark</surname></persName>
		</author>
		<idno type="DOI">10.1038/sdata.2016.35</idno>
		<ptr target="https://doi.org/10.1038/sdata.2016.35" />
		<title level="m">MIMIC-III, a freely accessible critical care database</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Generating Multi-Label Discrete Patient Records using Generative Adversarial Networks</title>
		<author>
			<persName><forename type="first">E</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Biswal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">A</forename><surname>Malin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Duke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">F</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning for Healthcare</title>
		<meeting>Machine Learning for Healthcare</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<title level="m">Replicated Softmax: An Undirected Topic Model. Paper presented at Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">22</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Causal Effect Inference with Deep Latent-Variable Models</title>
		<author>
			<persName><forename type="first">C</forename><surname>Louizos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Shalit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Mooij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1705.08821" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">Preprint at</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The blessings of multiple causes</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="issue">528</biblScope>
			<biblScope unit="page" from="1574" to="1596" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
