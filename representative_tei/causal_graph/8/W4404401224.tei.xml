<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CausalStock: Deep End-to-end Causal Discovery for News-driven Stock Movement Prediction</title>
				<funder>
					<orgName type="full">Fundamental Research Funds for the Central Universities</orgName>
				</funder>
				<funder ref="#_YNHhFZV">
					<orgName type="full">National Natural Science Foundation of China (NSFC</orgName>
				</funder>
				<funder ref="#_TCuzAgx">
					<orgName type="full">Research Funds of Renmin University of China</orgName>
				</funder>
				<funder ref="#_eHfmnZN">
					<orgName type="full">Intelligent Social Governance Platform, Major Innovation &amp; Planning Interdisciplinary Platform for the &quot;Double-First Class&quot; Initiative, Renmin University of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-11-10">10 Nov 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Shuqi</forename><surname>Li</surname></persName>
							<email>shuqili@ruc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="institution">Renmin University of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuebo</forename><surname>Sun</surname></persName>
							<email>sunyuebo0418@ruc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="institution">Renmin University of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yuxin</forename><surname>Lin</surname></persName>
							<email>linyuxin@stu.pku.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="institution">Peking University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xin</forename><surname>Gao</surname></persName>
							<email>xin.gao@kaust.edu.sa</email>
							<affiliation key="aff2">
								<orgName type="department">King Abdullah</orgName>
								<orgName type="institution">University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shuo</forename><surname>Shang</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">University of Electronic Science and Technology of China</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rui</forename><surname>Yan</surname></persName>
							<email>ruiyan@ruc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="institution">Renmin University of China</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">CausalStock: Deep End-to-end Causal Discovery for News-driven Stock Movement Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-11-10">10 Nov 2024</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2411.06391v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>There are two issues in news-driven multi-stock movement prediction tasks that are not well solved in the existing works. On the one hand, "relation discovery" is a pivotal part when leveraging the price information of other stocks to achieve accurate stock movement prediction. Given that stock relations are often unidirectional, such as the "supplier-consumer" relationship, causal relations are more appropriate to capture the impact between stocks. On the other hand, there is substantial noise existing in the news data leading to extracting effective information with difficulty. With these two issues in mind, we propose a novel framework called CausalStock for news-driven multi-stock movement prediction, which discovers the temporal causal relations between stocks. We design a lag-dependent temporal causal discovery mechanism to model the temporal causal graph distribution. Then a Functional Causal Model is employed to encapsulate the discovered causal relations and predict the stock movements. Additionally, we propose a Denoised News Encoder by taking advantage of the excellent text evaluation ability of large language models (LLMs) to extract useful information from massive news data. The experiment results show that CausalStock outperforms the strong baselines for both news-driven multi-stock movement prediction and multi-stock movement prediction tasks on six real-world datasets collected from the US, China, Japan, and UK markets. Moreover, getting benefit from the causal relations, CausalStock could offer a clear prediction mechanism with good explainability.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The financial services industry has maintained a leading position in embracing data science methodologies to inform investment determinations. Within this domain, quantitative trading has garnered substantial attention from both academia and industry. Researchers have consistently worked on exploring different approaches to predict the stock movement (rise or fall of stock price) for many years, such as uni-stock movement prediction <ref type="bibr" target="#b20">[21]</ref>, multi-stock movement prediction <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b22">23]</ref>, news-driven stock movement prediction <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b18">19]</ref> and so on, which have shown significant success. These methods usually model the stock movement prediction task as a time series classification problem.</p><p>In this paper, we focus on the news-driven multi-stock movement prediction task. A prevalent model paradigm for this task often takes the historical price features and the stock-related news of multiple stocks as inputs and then leverages the well-designed neural networks to make stock movement predictions. There are two key modeling points for tackling this task: modeling the stock relations to enhance the prediction accuracy, and building the text mining module to extract effective information from news data that benefits stock movement prediction. Although previous work has made significant progress, there are still some issues that require further attention. We will elaborate on them in the following.</p><p>For stock relation modeling, many existing works are commonly attention-based <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b22">23]</ref> or graph-based <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b22">23]</ref>. These methods aim to model the correlation relation between stocks. However, the company relations are often unidirectional, such as the "investing" and "member of," leading to the unidirectional relations of their stocks. Thus, causal relations are more appropriate for depicting the impact between stocks, as they identify the direction of information flow and are more informative than correlations. With the development of causal science, many researchers have started to use deep end-to-end networks for causal relations discovery of panel data or temporal data <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b13">14]</ref>, in which the causal relations are defined as directed acyclic graphs, i.e., causal graphs, and the Functional Causal Models (FCMs) are often utilized to optimize the causal graph by simulating the data generation mechanism. This provides a solid theoretical foundation for causal discovery for stocks.</p><p>In recent years, an extrinsic text mining module has emerged as a plausible avenue through the alignment of financial news and social media posts, thereby elucidating intricate market insights that extend well beyond mere considerations of price dynamics, trading volumes, or financial indicators <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b32">33]</ref>. Conventional text representations obtained by using GRU <ref type="bibr" target="#b41">[42]</ref> or LSTM <ref type="bibr" target="#b14">[15]</ref> exhibit many limitations. Specifically, news text data are often characterized by substantial noise because of the presence of irrelevant or ambiguous information <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b36">37]</ref>. The effective information for stock movement prediction gets intertwined with this noise, presenting a considerable challenge for these modules to discern meaningful signals accurately. In contrast, Large Language Models (LLMs) have unique advantages in this situation due to their advanced knowledge and reasoning abilities. Besides, LLMs can identify meaningful information within noisy environments <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b3">4]</ref>.</p><p>Motivated by these requirements, we propose an innovative news-driven multi-stock movement prediction model named CausalStock. In CausalStock, we design a Denoised News Encoder, which leverages LLMs to score every news text from multiple perspectives. Then the evaluation scores are taken as denoised text representations. To discover the causal relations between stocks, we propose a Lag-dependent temporal causal discovery module, from which we obtain the causal graph distribution. Based on the input market information and learned causal graph distribution, CausalStock employs an FCM <ref type="bibr" target="#b13">[14]</ref> to make predictions. We summarize the contributions of our paper as follows:</p><p>• We propose a novel news-driven multi-stock movement prediction method named Causal-Stock, which could discover the causal relations among stocks and make accurate movement predictions simultaneously.</p><p>• Different from the past lag-independent causal discovery method <ref type="bibr" target="#b8">[9]</ref>, CausalStock involves a lag-dependent temporal causal discovery module, which intuitively links the temporal causal relations according to the time lag, making it more suitable for temporal stock data.</p><p>• To extract useful information from the massive noisy news text data, an LLM-based Denoised News Encoder is proposed by taking advantage of the evaluation ability of LLM, which outputs the denoised news representation for better information utilization.</p><p>Experiments on 6 public benchmarks show the performance of CausalStock as a news-driven multistock movement prediction method. Moreover, we conduct extensive analytical experiments to show the explainability of our key modules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Stock prices prediction In traditional trading practices, there are two analysis paradigms commonly used to make stock movement predictions: technical analysis and fundamental analysis. With technical analysis, investors and traders tend to forecast stock prices relying on historical price patterns. Fundamental analysis aims to assess the intrinsic value of a stock by considering other factors besides historical prices, such as financial statements, industry trends, and economic conditions.</p><p>Since stock movement prediction involves sequential data, RNN-based networks are applied in many works. ALSTM <ref type="bibr" target="#b27">[28]</ref> integrated a dual-stage attention mechanism with LSTM. Adv-ALSTM <ref type="bibr" target="#b7">[8]</ref> further employed adversarial training by adding perturbations to simulate the stochastic and unstable nature of the price variable. In recent years, researchers also exploited attention-based mechanisms to model complex interactions. DTML <ref type="bibr" target="#b43">[44]</ref> is proposed to predict by using a transformer and LSTM to capture the asymmetric and dynamic correlations between stocks. With the development and prosperity of NLP technology, text from social media and online news has become a new popular source of fundamental analysis. HAN <ref type="bibr" target="#b14">[15]</ref> designed two attention networks to recognize both the influential time periods of a sequence and the important news at a given time. Stocknet <ref type="bibr" target="#b41">[42]</ref> proposed a deep generative model with recurrent, continuous latent variables. MSHAN <ref type="bibr" target="#b12">[13]</ref> exploited a multi-stage TCN-LSTM hybrid model. PEN <ref type="bibr" target="#b20">[21]</ref> proposed a Shared Representation Learning module to capture interactions between price data and text data. Additionally, many works modeled the correlation between stocks to enhance stock price prediction. MAN-SF <ref type="bibr" target="#b31">[32]</ref> constructed a graph attention network with price features, social media, and inter-stock relationships based on the interrelationship between price and tweets. CMIN <ref type="bibr" target="#b22">[23]</ref> was proposed to model the asymmetric correlations between stocks by computing transfer entropy. In addition, Co-CPC <ref type="bibr" target="#b39">[40]</ref> modeled the dependence between a certain stock industry and relevant macroeconomic variables. All the aforementioned methods aim to discover the correlation relations among stocks, as elaborated before, the causal relations are more appropriate to depict the information flow of stocks. In this work, we aim to model the causal relations for better stock movement prediction performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Causal discovery</head><p>The conventional approach to discovering causal relations typically involves conducting randomized experiments <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b11">12]</ref>. However, conducting randomized experiments can often be excessively expensive, overly time-consuming, or impossible to execute. Consequently, causal discovery, which aims to infer causal relationships from purely observational data, has attracted considerable attention within the machine learning community over the last decade <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b11">12]</ref>. Causal discovery can be classified into three groups: constraint-based <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b30">31]</ref>, score-based <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b45">46]</ref>, and functional causal models (FCMs) <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b25">26]</ref>. FCMs define the causal relations by directed acyclic graphs (DAGs) and identify causal links through nonlinear functions, such as neural networks <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b17">18]</ref>. Specifically, DECI <ref type="bibr" target="#b8">[9]</ref> is a deep end-to-end framework to discover causal relations based on additive noise FCM. After that, Rhino <ref type="bibr" target="#b13">[14]</ref> was proposed to tackle the temporal causal discovery problem, which incorporates non-linear relations, instantaneous effects, and flexible history-dependent noise. In this work, we focus on utilizing the FCM to discover stock relations.</p><p>3 Preliminary &amp; problem formulation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Preliminary</head><p>In CausalStock, we integrate the model inputs with causal relations into FCM for prediction. In this section, we introduce the fundamental concepts of FCM and the temporal causal graph.</p><p>Temporal causal graph Consider a multivariate time series {X i t } D i=1 with D variables, the temporal causal graph G <ref type="bibr" target="#b45">[46]</ref> is commonly defined as a series of directed acyclic graph G = [G 1 , G 2 , . . . , G L ] = {G l } L l=1 ∈ R L×D×D with maximum time lag L. Each G l ∈ R D×D specifies the lagged causal relationships between X t-l and X t , the element G l,ji = 1 if there exists a causal link X j t-l -→ X i t and G l,ji = 0 otherwise.</p><p>Functional causal model (FCM) FCM represents a set of generative functions that incorporate the input features based on causal knowledge (structured as a causal graph) to produce a final prediction.</p><p>Optimizing the prediction accuracy concurrently refines the underlying causal graph. The theoretical demonstration presented in <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b8">9]</ref> indicates that if the prediction is accurate, the causal graph can be considered a reliable approximation of real causal relations. Given the temporal causal graph G defined as before, a temporal FCM is defined as follows:</p><formula xml:id="formula_0">X i t = Fi Pa i G (&lt; t) , z i t ,<label>(1)</label></formula><p>where Pa i G (&lt; t) indicates the time-lagged parent nodes of variable X i t following the temporal causal graph G and z i t represents mutually and serially independent exogenous noise. Here F i is a function which implies how variable X i t depends on its parents and the noise z i t . Given the distribution of noises for different variables {z i t } D i=1 and causal graph, this FCM induces a joint distribution of the multivariate time series process {X i t } D i=1 . The forecasting process is denoted by solid lines with parameters θ and the causal discovery process is denoted by dashed lines with variational approximation parameters ϕ, q ϕ is the posterior distribution of the causal graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Problem formulation</head><p>In this paper, we focus on tackling the news-driven multi-stock movement prediction task. For the target trading day T , we denote the model inputs as the past L time lag information of D stocks as</p><formula xml:id="formula_1">X &lt;T = {X i t } i=1:D t=T -L:T -1 = [C &lt;T , P &lt;T ] = {[C i t , P i t ]} i=1:D t=T -L:T -1</formula><p>, where C i t and P i t represent the news corpora representation and the historical price features representation of i-th stock at time step t respectively. The objective is to predict the movement of adjusted close prices y T = {y i T } D i=1 ∈ R D×1 on T -th trading day of all stocks simultaneously, where y i T ∈ {0, 1} representing the i-th stock price will fall or rise at trading day T , i.e., stock movement. In a theoretical way, this task could be trained by maximizing the log-likelihood of conditional probability distribution p (y T | X &lt;T ), so that the most likely y T are generated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">CausalStock</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Model overview</head><p>The conditional probability distribution could be further factorized as follows:</p><formula xml:id="formula_2">p (yT | X&lt;T ) = G p (yT , G | X&lt;T ) dG = G p (yT | X&lt;T , G) p (G | X&lt;T ) dG.<label>(2)</label></formula><p>The overall process is taken as two joint training parts: temporal causal graph discovery p (G | X &lt;T ) and the prediction process given the causal relations p (y T | X &lt;T , G). The probabilistic graphic representation of this modeling process is shown in Figure <ref type="figure" target="#fig_0">1</ref>. In CausalStock, we develop a lagdependent causal discovery module, according to which we could take another step by modeling p (G | X &lt;T ) as a lag-dependent format:</p><formula xml:id="formula_3">p (G | X&lt;T ) = p (G1 | XT -1) L l=2 p (G l | G l-1 , X T -l ) .<label>(3)</label></formula><p>For the prediction part p (y T | X &lt;T , G), we design an FCM as shown in Equation 9 to predict the future movement based on the past information X &lt;T and the discovered temporal causal graph G.</p><p>In a nutshell, CausalStock comprises three primary components as shown in Figure <ref type="figure" target="#fig_1">2:</ref> 1. Market Information Encoder (MIE) encodes the news text and price features. In this part, an LLM-based Denoised News Encoder is proposed; 2. Lag-dependent Temporal Causal Discovery (Lag-dependent TCD) module leverages variational inference to mine the causal relationship based on the given market information of stocks, i.e., modeling p (G | X &lt;T ); 3. Functional Causal Model (FCM) generates the prediction of future price movements according to the discovered causal graph, i.e., modeling p (y T | X &lt;T , G).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Market information encoder (MIE)</head><p>Market Information Encoder (MIE) takes news corpora and numerical stock price features as inputs, and outputs the historical market information representations  LLM-based denoised news encoder (DNE) News Encoder aims to embed stock-related news text, which evolves from the small sequential module, e.g., GRU <ref type="bibr" target="#b22">[23]</ref>, to pre-trained models, e.g., Bert and Roberta <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b21">22]</ref>, offering greater performance and scalability. However, news text data often contains massive noise due to the following factors. Firstly, news comes from a wide range of sources with varying degrees of reliability and editorial standards. This variability contributes to inconsistencies and inaccuracies in the information presented. Secondly, the sheer volume of news content generated daily can lead to information overload, where significant information is buried under less relevant or redundant information. Thirdly, the use of complex or ambiguous language can also add noise, making it difficult to extract precise information relevant to specific needs, such as stock movement prediction. Addressing these challenges requires sophisticated text mining and natural language processing techniques to filter out noise and extract useful, accurate information from news text data. With the development of large language models, current large language models can accurately capture the meaning of text and have a strong capability to evaluate text. Therefore, here we propose an LLM-based Denoised News Encoder to tackle these standing challenges.</p><formula xml:id="formula_4">X &lt;T = [C &lt;T , P &lt;T ] = {[C i t , P i t ]} i=1:D t=T -L:T -1 = {X i t } i=1:D t=T -L:T -</formula><p>LLM-based Denoised News Encoder is an innovative textual representation approach that not only proficiently captures salient information from extensive news texts but also assimilates external knowledge derived from LLMs to enrich the representations. Specifically, we employ an LLM and devise a series of prompts (see Appendix A for the whole designed prompts) to analyze the relationship between a news text and a specific stock from five dimensions: correlation between the news and the stock, sentiment polarity of the news, significance of the news event, potential impact of the news on stock prices, and duration of the news impact. Each dimension is scored, with Correlation, Importance, Impact, and Duration ranging from 0 to 10, while Sentiment varies from -1 to 1. Thus the i-th text at day t is represented as a five-dimensional representation Ĉi t ∈ R l×5 . After the embedding layer, we obtain the final denoised news embedding C i t ∈ R l×dm . This novel encoding method amalgamates information derived from the primary text, the external knowledge embedded and the evaluation ability within the LLM. Besides, this method effectively reduces the significant noise present in the original text data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Lag-dependent temporal causal discovery (Lag-dependent TCD)</head><p>In this section, we propose Lag-dependent Temporal Causal Discovery module. Inspired by <ref type="bibr" target="#b13">[14]</ref>, our model takes a Bayesian view for modeling the distribution of temporal causal graph, which aims to learn the posterior distribution p (G | X &lt;T ). Unfortunately, the exact graph posterior is intractable because of the large combination space of G. Here we adopt the variational inference <ref type="bibr" target="#b2">[3]</ref> to get the approximator q ϕ (G), where ϕ indicates the parameter set of variational inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph prior</head><p>The prior p (G) consists of two parts: the graph sparseness prior and the domainspecific knowledge prior. The unnormalised graph prior is as follows,</p><formula xml:id="formula_5">p(G) ∝ exp -λs ∥G1:L∥ 2 F -λ d ∥G1:L -G p 1:L ∥ 2 F ,<label>(4)</label></formula><p>where λ s and λ d are scalar weights of graph sparseness and domain-specific knowledge constraint; G p is an optional domain-specific knowledge graph, which allows users to incorporate pre-defined knowledge for guiding CausalStock, turning it into a knowledge and data-driven framework. Suppose a sudden event affects the causal relationships between stocks, such as a company ending a partnership. By incorporating this new pre-defined knowledge into G p , the causal graph is dynamically updated to reflect the latest market structure and relationship changes. ∥•∥ F denotes Frobenius norm. It should be noted that there is no need to give a DAG constraint for the temporal causal graph defined in our paper, it is DAG naturally for the irreversibility of time.</p><p>Variational approximating graph posterior According to Equation <ref type="formula" target="#formula_3">3</ref>, we factorize the approximator q ϕ (G) in the same way. For each underlying causal link G l,ji in G, we let the posterior q ϕ (G l,ji | G l-1,ji ) subject to a Bernoulli distribution B. So that the probability distribution of q ϕ (G) could be a product of Bernoulli distributions as follows,</p><formula xml:id="formula_6">q ϕ (G) = q ϕ (G1) L l=2 q ϕ (G l | G l-1 ) = D i=1 D j=1 q ϕ (G1,ji) L l=2 D i=1 D j=1 q ϕ (G l,ji | G l-1,ji ) .<label>(5)</label></formula><p>The existence and non-existence likelihood tensors of causal links are parameterized as</p><formula xml:id="formula_7">U = {U l } L l=1 = {u l,ji } j,i=1:D l=1:L ∈ R L×D×D and V = {V l } L l=1 = {v l,ji } j,i=1:D l=1:L</formula><p>∈ R L×D×D separately, where u l,ji indicates the likelihood for edge existence from X j T -l to y i T and v l,ji is the likelihood for no-edge, which are all learnable parameters. To model the dependency between G l,ij with G l-1,ij , we propose the following transformation:</p><formula xml:id="formula_8">u ′ l,ji = hu (u l,ji , u l-1,ji ) , v ′ l,ji = hv (v l,ji , v l-1,ji ) ,<label>(6)</label></formula><p>where h u and h v are trainable 3-layer MLPs. After normalization, the link existence probability tensor is denoted as</p><formula xml:id="formula_9">Σ = {Σ l } L l=1 = {σ l,ji } j,i=1:D l=1:L , σ l,ji = exp u ′ l,ji / exp u ′ l,ji + exp v ′ l,ji ,<label>(7)</label></formula><p>where σ l,ji represents the link probability from X j T -l to y i T . Thus, we could derive the variational posterior:</p><formula xml:id="formula_10">q ϕ (G1,ji) ∼ B (1, σ1,ji) , q ϕ (G l,ji |G l-1,ji ) ∼ B (1, σ l,ji ) , q ϕ (G) ∼ L l=1 D i=1 D i=1 B (1, σ l,ji ) .<label>(8)</label></formula><p>In the training stage, we employ the Gumbel-softmax reparameterization <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b15">16]</ref> to stochastically estimate the gradients with respect to ϕ. Besides, we design another parameterized learnable causal weight graph Ĝ = { Ĝl } L l=1 ∈ R L×D×D to measure the causal degree. The separate design of the causal existence graph and the causal weight graph allows for more comprehensive modeling of causality. Once our model is fitted, the time series causal graph G can be sampled by G ∼ q ϕ (G) to represent the relation network and information flow of the stock market.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Functional causal model (FCM)</head><p>In this section, we design an FCM to model p θ (y T | X &lt;T , G), where θ denotes the parameter set of FCM. We focus on additive noise FCM <ref type="bibr" target="#b17">[18]</ref> to generate y T = {y i T } D i=1 ∈ R D×1 :</p><formula xml:id="formula_11">y i T = Fi Pa i G (&lt; T ) , z i T = fi Pa i G (&lt; T ) + z i T ,<label>(9)</label></formula><p>where z i t represents mutually and serially independent dynamical noise, and f i : R D×L -→ R 1 are general differentiable non-linear function that satisfies the relations specified by the temporal causal graph G strictly, namely, if</p><formula xml:id="formula_12">X j t / ∈ Pa i G (&lt; T ), then ∂f i /∂X j t = 0.</formula><p>We design a novel FCM to aggregate market information including news and prices based on the discovered causal graph G and causal weight graph Ĝ:</p><formula xml:id="formula_13">fi Pa i G (&lt; T ) = Sigmoid ζi L l=1 D j=1 G l,ji Ĝl,ji ℓ P j T -l , ψ C j T -l ,<label>(10)</label></formula><p>where ζ i , ℓ and ψ are all neural networks. ℓ and ψ are shared weights across nodes and lags for efficient computation. [•, •] denotes the concatenate operation. We apply the logistic Sigmoid function to output the movement probability of y T and use it directly as the output of CausalStock.</p><p>For the exogenous noise z i T modeling, we adopt Gaussian distribution, i.e., z i T ∼ N 0, σ i 2 , where per-variable variances σ i 2 , i ∈ [1, D] are trainable parameters to represent the uncertainty part. According to Change of variables formula <ref type="bibr" target="#b17">[18]</ref>, the conditional distribution p θ y i T | Pa i G (&lt; t) could be represented as:</p><formula xml:id="formula_14">p θ y i T | Pa i G (&lt; t) = pz i z i T ∂Fi ∂z i T -1 = pz i z i T ,<label>(11)</label></formula><p>where p zi is the aforementioned Gaussian distribution for stock i. ∂Fi ∂z i T indicates the absolute value of the Jacobian-determinant for F i , ∂Fi</p><formula xml:id="formula_15">∂z i T -1</formula><p>= 1 is derived according to Equation <ref type="formula" target="#formula_11">9</ref>. Now the log likelihood log p θ (y T | X &lt;T , G) could be further represented as:</p><formula xml:id="formula_16">log p θ (yT | X&lt;T , G) = D i=1 log p θ y i T | Pa i G (&lt; T ) = D i=1 log pz i z i T .<label>(12)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Training objective</head><p>We train our model by maximizing the conditional log-likelihood log p θ (y T | X &lt;T ). The variational evidence lower bound (ELBO) of the model objective is derived as follows:</p><formula xml:id="formula_17">log p θ (yT | X&lt;T ) = log G q ϕ (G) q ϕ (G) p θ (yT | X&lt;T , G) p (G) dG ≥ G q ϕ (G) log p θ (yT | X&lt;T , G) p (G) dG + H (q ϕ (G)) ≥ E q ϕ (G) [log p θ (yT | X&lt;T , G) + log p (G)] + H (q ϕ (G)) ≥ E q ϕ (G) D i=1 log pz i z i T + log p (G) + H (q ϕ (G)) .<label>(13)</label></formula><p>Here, p (G) represents the prior of causal graph, and H (q ϕ (G)) is the entropy of the posterior approximator. log p θ (y T | X &lt;T , G) = log p z (z T ) is the log-likelihood of the target distribution, in which z T is calculated by Equation <ref type="formula" target="#formula_11">9</ref>at training stage.</p><p>Besides, we further adopt the binary cross entropy loss as another objective BCE (g T , y T ) to improve the learning performance, where g T is the ground truth movement at target trading day T . Overall, the final training loss L is as follows,</p><formula xml:id="formula_18">BCE (gT , yT ) = - D i=1 g i T log y i T + 1-g i T log 1-y i T L = 1 D (-ELBO + λBCE(gT , yT )) (<label>14</label></formula><formula xml:id="formula_19">)</formula><p>where λ is the scalar weight to balance loss terms. We note that the required assumptions and the theoretical guarantees are summarized in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental setup</head><p>Except for the news-driven multi-stock movement prediction task, our model could also handle the multi-stock movement prediction task without news by removing the Denoised News Encoder. Thus, we do the experiments for both two tasks. Dataset (Appendix C.1): We train and evaluate our model and baselines on six datasets: ACL18 <ref type="bibr" target="#b41">[42]</ref>, CMIN-US <ref type="bibr" target="#b22">[23]</ref>, CMIN-CN <ref type="bibr" target="#b22">[23]</ref>, KDD17 <ref type="bibr" target="#b44">[45]</ref>, NI225 <ref type="bibr" target="#b43">[44]</ref>, and FTSE100 <ref type="bibr" target="#b43">[44]</ref>. The first three of them including both historical prices and text data are used for news-driven multi-stock movement prediction task evaluation, while the last three are for multi-stock movement prediction task evaluation without news data. Evaluation metrics (Appendix C.2): We evaluate the prediction performance of models by Accuracy (ACC) and Matthews Correlation Coefficients (MCC). Baselines (Appendix C.3): HAN <ref type="bibr" target="#b14">[15]</ref>, Stocknet <ref type="bibr" target="#b41">[42]</ref>, PEN <ref type="bibr" target="#b20">[21]</ref>, CMIN <ref type="bibr" target="#b22">[23]</ref> for news-driven multi-stock movement prediction task. LSTM <ref type="bibr" target="#b24">[25]</ref>, ALSTM <ref type="bibr" target="#b27">[28]</ref>, Adv-ALSTM <ref type="bibr" target="#b7">[8]</ref>, DTML <ref type="bibr" target="#b43">[44]</ref> for multi-stock movement prediction task. Parameter setup (Appendix C.4): Our model is implemented with Pytorch on 4 NVIDIA Tesla V100 and optimized by Adam <ref type="bibr" target="#b19">[20]</ref>. The parameter sensitivity study can be found in Appendix C.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results of prediction accuracy</head><p>As shown in the top half of Table <ref type="table" target="#tab_1">1</ref>, CausalStock outperforms all baselines on ACC as well as MCC across three datasets demonstrating robustness performance for news-driven multi-stock movement prediction task. For the multi-stock movement prediction task, the results are reported in the bottom half of Table <ref type="table" target="#tab_1">1</ref>. As can be seen, CausalStock exceeds all baselines across three datasets with stable performance. Overall, the results demonstrate that the proposed CausalStock can indeed improve the performance for two stock movement prediction tasks, showing the strong capabilities in handling financial texts and discovering causal relations among stocks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Ablation study</head><p>For the ablation study, we conduct several model variants on ACL18, CMIN-CN and CMIN-US to explore the contributions of different settings in CausalStock. For the main framework, we have the following five variants. CausalStock w/o TCD: removing the causal discovery module from CausalStock; CausalStock w/o News: removing the news encoder from CausalStock and just taking prices data as input; CausalStock w/o link non-existence modeling: only model the causal link existence likelihood and leverage Sigmoid function to obtain the link existence probability; CausalStock w/o Lag-dependent TCD: replacing the Lag-dependent Temporal Causal Discovery module with the Lag-independent Temporal Causal Discovery module; CausalStock with Variable-dependent TCD: we add a variable-dependent causal mechanism that explicitly captures the dependencies among different stock edges. Specifically, each edge's probability is conditioned on the states of all other edges at the same time step, and the conditional function is the same as the function in the lag-dependent mechanism (Equation <ref type="formula" target="#formula_8">6</ref>). Furthermore, we explore the performance of six different Traditional News Encoders by replacing the denoised news encoder, which outputs the news embeddings as representations. CausalStock with Glove + Bi-GRU: leveraging the Glove word embedding and the Bi-GRU as news encoder <ref type="bibr" target="#b22">[23]</ref>; CausalStock with Bert: leveraging the pre-trained Bert (Bert-base-multilingual-cased <ref type="bibr" target="#b5">[6]</ref>) as news encoder; CausalStock with Roberta: leveraging the pre-trained Roberta (Roberta-base <ref type="bibr" target="#b21">[22]</ref>) as news encoder; CausalStock with FinBert: leveraging the pre-trained FinBert <ref type="bibr" target="#b0">[1]</ref> as news encoder; CausalStock with FinGPT: leveraging the pre-trained FinGPT (FinGPT-v3.3 <ref type="bibr" target="#b42">[43]</ref>) as news encoder; CausalStock with Llama: leveraging the pre-trained Llama ( Llama-7b-chat-hf <ref type="bibr" target="#b38">[39]</ref>) as news encoder to output news embeddings. Moreover, we explore the performance of three different LLMs for the denoised news encoder. CausalStock with FinGPT: leveraging a financial LLM FinGPT (FinGPT-v3.3 <ref type="bibr" target="#b42">[43]</ref>) as denoised news encoder; CausalStock with Llama: leveraging Llama (Llama-7b-chat-hf <ref type="bibr" target="#b38">[39]</ref>) as denoised news encoder. The ablation study results are summarized in Table <ref type="table" target="#tab_2">2</ref>.</p><p>We have the following observations: (1) CausalStock with news encoders all perform better than CausalStock without news, suggesting news data is particularly helpful for stock movement prediction.</p><p>(2) Compared to CausalStock w/o Lag-independent TCD, CausalStock with Lag-dependent TCD has a better performance, demonstrating the value of the lag-dependent mechanism. (3) By comparing the CausalStock and CausalStock with Variable-dependent TCD, the results show that incorporating a variable-dependent causal mechanism has the potential to enhance model performance. However, the improvements are not uniform and vary depending on the dataset, which emphasizes that further validation is needed. While the above results show a promising performance of the variable-dependent causal mechanism, it significantly increases the computational complexity (from</p><formula xml:id="formula_20">O(L × D 2 ) to O(L × D 4</formula><p>)), making it challenging to apply the model to markets with large numbers of stocks.</p><p>(3) By using FinGPT and Llama as the news encoder and denoised news encoder respectively, we can observe that denoised news encoders have a relatively higher ACC and MCC than their as the traditional news encoders, suggesting the value of denoised news encoders. Overall, the ablation studies show that every component contributes to CausalStock.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Results of explainability</head><p>Here, we present many cases detailing the interpretability of CausalStock from two perspectives: the news representation from the Denoised News Encoder, and the causal graph discovered by the Lag-dependent TCD module.</p><p>Firstly, regarding the Denoised News Encoder module, three cases are selected as shown in Figure <ref type="figure" target="#fig_2">3</ref>(c). A piece of news about APPL suggests a potential delay in its 5G iPhone launch, with Denoised News Encoder giving it a negative sentiment score of -0.7 and an impact score of 9. Similarly, a news about TSLA hints at surpassing a significant delivery milestone, receiving a positive sentiment score of 0.7. In contrast, a news piece showing no discernible connection to GOOG is scored with negligible impact. These cases indicate the Denoised News Encoder's efficacy in discerning and quantifying the potential influence of news on respective stock prices.</p><p>Secondly, concerning the causal graph discovered by Lag-dependent TCD, we denote the causal strength graph as the dot product of the causal graph G and the causal weight graph Ĝ. Every item of causal strength graph indicates not only the causality of two stocks but also the degree of causality. The visualized causal strength matrix for ACL18 is shown in Figure <ref type="figure" target="#fig_2">3</ref>(b) with a heatmap. From various industries, we select companies with the highest and lowest market value. The top half of Figure <ref type="figure" target="#fig_2">3</ref>(b) represents stocks corresponding to the nine companies with the largest market value, while the bottom half illustrates stocks from companies with the smallest. The causal strength of stocks is determined based on the average overall lags. In this heatmap, we could observe that distinct patterns emerge according to different market values. Stocks of low-market-value companies appear to have less pronounced causal relationships. We could also observe causal connections between certain high and low-market-value stocks. This is attributable to the dominant roles of large-value companies with their significant impact on those small-value firms and the stock prices.</p><p>Based on these observations, we compute the Spearman's rank correlation coefficient <ref type="bibr" target="#b35">[36]</ref> between the aforementioned company's market value and their stock's causal strength on ACL18, CMIN-CN, NI225, and FSTE100 datasets, representing the US, Chinese, Japanese, and UK stock markets respectively. The correlation results are shown in Appendix D and we also visualize some results in Figure <ref type="figure" target="#fig_2">3</ref>(a). These results show a strong positive correlation between the market value and causal influence. This aligns with the intuition that not only do large-value companies hold pivotal economic positions, but also play crucial roles in influencing other companies. Our findings demonstrate that CausalStock does well in uncovering the causal relations within the stock market.  Following prior works <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b22">23]</ref>, we evaluate Causal-Stock's applicability to the real world trading scenario. We conduct a portfolio strategy by choosing the top three stocks (based on predicted probabilities) with equal weight on each day of the test set and calculate the Accumulated Portfolio Value (APV) and Sharpe Ratio (SR) for evaluation. See Appendix C.2 for a metrics details. The results on three datasets are shown in Table <ref type="table" target="#tab_5">4</ref>, which indicates that CausalStock achieves higher profits, and the excellent capabilities of CausalStock to balance risk with returns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Investment simulation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we propose a novel news-driven multi-stock movement prediction framework called CausalStock. We design a lag-dependent temporal causal discovery mechanism to uncover the causal relations among the stocks. Then the functional causal model is employed to encapsulate causal relations and predict future movements. The effectiveness of CausalStock is demonstrated by experiments on multiple real-world datasets. Moreover, CausalStock could offer a clear prediction process with explainability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Prompt design</head><p>This structured prompt encompasses three fundamental components: System: This section defines the role of the AI. It acts as a preliminary introduction to set the tone and context for the AI. It informs the AI that its primary role is to analyze stock-related news in various dimensions such as correlation, sentiment, importance, impact on prices, and duration of impact.</p><p>Default Prompt: This segment provides detailed instructions to the AI on how to carry out its analysis. It outlines the specific criteria and the scales on which the news should be evaluated. It also provides guidance on how to handle ambiguous or non-analyzable content and finally, it prescribes the desired output format.</p><p>Input: The final section is where the user provides the specific details about the stock, the news content, and the time of publication. It acts as the data point based on which the AI will perform its analysis as instructed in the Default Prompt.</p><p>[System] {As a stock trading news analyst, you are a helpful and precise assistant. Your task is to analyze the correlation between news and the given stock, sentiment polarity of the news, importance of the news, the impact of the news on stock prices, and the duration of the news impact.} [Default Prompt] I need you to analyze the provided stock-related news from five dimensions:</p><p>1. Correlation between the news and the given stock: Rate the correlation on a scale of 0 to 10, where a higher score indicates a stronger correlation between the news and the given stock.</p><p>2. Sentiment polarity of the news: Rate the sentiment polarity on a scale of -1 to 1, where a value closer to -1 indicates stronger negative sentiment and a value closer to 1 indicates stronger positive sentiment.</p><p>3. Importance of the news event: Rate the importance on a scale of 0 to 10, where a higher score indicates higher importance of the news event.</p><p>4. Impact of the news on stock prices: Rate the impact on a scale of 0 to 10, where a higher score indicates a greater impact of the news on stock prices. 5. Duration of the news impact: Rate the duration on a scale of 0 to 10, where a higher score indicates a longer potential duration of the news impact.</p><p>(When you encounter a situation where analysis is not possible, please try to avoid assigning all-zero scores and instead make an effort to analyze the text content and derive scores accordingly. Only when analysis is truly impossible should you assign a score of 0 to all factors.) (Please refrain from providing analysis and simply provide the answer according to the following format.) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Assumptions and theoretical guarantees</head><p>There are some common assumptions in causal discovery. In this paper, we assume our model satisfies the Causal Markov Property, Minimality and Structural Identifiability, Correct Specification, Causal Sufficiency and Regularity of log likelihood. A detailed explanation can be found in <ref type="bibr" target="#b8">[9]</ref>, which explains how our model satisfies these assumptions. These assumptions guarantee the validity of the causal relations discovered by CausalStock. Considering the instability of news data, we only leverage price data P &lt;T to discover causal graph G to meet the Causal Stationary assumption. Then we use the discovered causal graph G for aggregating both news information and price information. Technically, this could be realized by detaching the gradient from C &lt;T to G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Datasets &amp; metrics &amp; baselines &amp; parameter setting C.1 Dataset</head><p>Six datasets from various countries' stock markets are employed for conducting the experiments. The first three are used for models of fundamental analysis, which include both historical prices and text data. ACL18 <ref type="bibr" target="#b41">[42]</ref> is a collection of data from 88 stocks in 9 industries in the US market over two years. Specifically, the price vectors after preprocessing are made up of 7 entries: date, movement percent, open price, high price, low price, close price, and volume, and the text data from Twitter are treated with tokenization and cleaning. Two CMIN datasets <ref type="bibr" target="#b22">[23]</ref> are published subsequently following a similar format as ACL18. CMIN-US is collected from the US market, whereas CMIN-CN comes from 300 CSI300 stocks in the Chinese market.</p><p>The other three datasets contain historical prices only and are applied to methods of technical analysis. KDD17 <ref type="bibr" target="#b44">[45]</ref> collects prices of 50 US stocks. <ref type="bibr" target="#b7">[8]</ref> proposed to transfer the raw price vectors of KDD17 into 11 temporal features for normalizing prices and capturing the interaction between different raw price entries. With this transfer calculation, NI225, and FTSE100 <ref type="bibr" target="#b43">[44]</ref> record 11-feature stock prices from the US, China, Japan, and UK market respectively over the different time periods. For all datasets, the train-test set split is chronological. More detailed statistics about the datasets are presented in Table <ref type="table" target="#tab_4">3</ref> below. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Metrics</head><p>Given the confusion matrix ( tp f n f p tn ), where tp, f p, tn, f n represent the true positives, false positives, true negatives and false negatives, we calculate ACC and MCC as follows:</p><formula xml:id="formula_21">ACC = tp + tn tp + tn + f p + gn ,<label>(15)</label></formula><formula xml:id="formula_22">M CC = tp × tn -f p × f n (tp + f p)(f n + tp)(f n + tn)(f p + tn) .<label>(16)</label></formula><p>Accumulated investment portfolio value (APV) shows the accumulation of wealth over time in an intuitive form and the Sharpe Ratio (SR) is probably the most widely used metric to measure a trading strategy's return compared to its risk. The Sharpe ratio is calculated as follows,</p><formula xml:id="formula_23">APV t = t i=1 (1 + r i ),<label>(17)</label></formula><formula xml:id="formula_24">SR = E APV t -R f S APV t -R f ,<label>(18)</label></formula><p>where r i is the daily return ratio on i-th trading day and R f is the risk-free return.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Baselines</head><p>For multi-stock movement prediction</p><p>• LSTM <ref type="bibr" target="#b24">[25]</ref> is an LSTM-based network that is trained with a rolling window of the last 10 months. 175 technical indicators on the characteristic of stocks and 5 features on normalized historical prices jointly form the input layer and are fed into the model. • ALSTM <ref type="bibr" target="#b27">[28]</ref> uses attentive LSTM in both encoder and decoder. The input attention mechanism in the encoder could extract the relevant features of stock price, whereas the temporal attention mechanism in the decoder could help learn the long-term dependencies. • Adv-LSTM <ref type="bibr" target="#b7">[8]</ref> tries to improve ALSTM through adversarial training to capture the stochastic nature of stock price and ameliorate the over-fitting. During the training process, adversarial examples are generated from latent representation and integrated with clean samples to serve as input. • DTML <ref type="bibr" target="#b43">[44]</ref> exploits the correlations between stocks in three parts: compressing the multivariate historical prices of a stock into a context vector with attentive LSTM, generating multi-level context vectors by aggregating local and global context, and capturing the correlations between stocks via transformer encoder and self-attention.</p><p>For news-driven multi-stock movement prediction</p><p>• HAN <ref type="bibr" target="#b14">[15]</ref>: uses attention mechanism to select useful news for stock movement prediction from chaotic online resources. The framework first applies news-level attention to find out more significant news in a date and encodes the output corpus vectors with Bi-GRU. Then, another temporal attention is applied to focus on more impactful time periods. • Stocknet <ref type="bibr" target="#b41">[42]</ref>: predicts stock trend based on text and price with recurrent, continuous latent variables. The model has 3 modules, which are Market Information Encoder (MIE), Variational Movement Decoder (VMD), and Attentive Temporal Auxiliary (ATA) in sequence. • PEN <ref type="bibr" target="#b20">[21]</ref>: a model that fuses the Bi-GRU text embedding and price inputs into Shared Representation Learning (SRL) to study their interaction. SRL also yields a Vector of Salient (VOS) that could display the importance of a piece of news and display the explainability of the model. • CMIN <ref type="bibr" target="#b22">[23]</ref>: integrates causality-enhanced stock correlations and text for stock movement prediction. The approach aims to cover not only the asymmetric correlations between stocks via a newly proposed causal attention mechanism but also the multi-directional interactions between text and stock correlations. In addition, two memory networks are used for selecting the relevant information in text and stock correlation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4 Parameter setting</head><p>Our model is implemented with Pytorch on 4 NVIDIA Tesla V100 and optimized by Adam <ref type="bibr" target="#b19">[20]</ref>. All parameters of our model are initialized with Xavier Initialization <ref type="bibr" target="#b10">[11]</ref>.To better explore the model's performance, we use grid search to decide on many key hyper-parameters. The learning rate is set as 1e -5 selected from [1e -3, 1e -4, 1e -5, 1e -6]. The time lag L is set as 5 selected from <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b8">9]</ref>. We select the price encoder hidden size from <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b15">16]</ref> and get the best performance with size 4. The batch size is set as 32. The scalar weight λ is set to 0.01. For the traditional news encoder, the maximum word number in one piece of news and news number in one day are set to w = 20, l = 10, respectively. The embedding size of word and news are set to d w = 50, d m = 64, respectively. For the Lag-dependent temporal causal discovery module, λ s = 1, h v and h u are all 1-layer MLPs. For the FCM part, the neural modules ζ i , ℓ and ψ are all 3-layer MLPs with hidden size 332.</p><p>Hyper-parameter sensitivity study We take a further step to analyze the main parameter sensitivity of CausalStock. We tune the key hyper-parameters learning rate lr, maximum time lag L and loss weight λ by grid search from this combination lr = 1e -5, L = 5, λ = 0.01 while controlling other parameters. Table <ref type="table" target="#tab_5">4</ref> presents the results of metric ACC with different parameter settings on two tasks.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Limitations and future works</head><p>This paper explores a method that discovers causal relations based on theoretical considerations. In the future, we could try to adopt meta-learning or incremental learning training methods to update the causal graph iteratively, i.e. explore the time-varied causal graph. While the Bernoulli distribution is suitable for determining whether a causal link exists, if we want to further explore the multi-level nature of causal relationships, more complex distributions might be needed. In the future, we could improve the model in this way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Broader impacts and safety issues</head><p>In this paper, we designed an LLM-based Denoised News Encoder to evaluate the news from multiple perspectives by LLMs. There exists a risk that the evaluation results of LLMs may violate human values. This safety issue needs careful consideration.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of the process of stock movement y T forecasting.The forecasting process is denoted by solid lines with parameters θ and the causal discovery process is denoted by dashed lines with variational approximation parameters ϕ, q ϕ is the posterior distribution of the causal graph.</figDesc><graphic coords="4,370.92,62.26,127.56,81.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The structure of CausalStock. For illustration, we use market information during 07/02 -07/05 of three stocks (AAPL, GOOG, META) to predict the movements of 07/05.</figDesc><graphic coords="5,175.60,180.92,90.10,86.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: (a) Correlation visualization between market value and causal strength for the top 20 companies of UK and Chinese markets. (b) Partial causal strength matrix visualization for ACL18, encompassing the companies with the highest and lowest market values across various industries. Each matrix entry indicates causal strength between stocks, with darker shades signifying stronger causality. (c) Examples of denoised news encoder module output.</figDesc><graphic coords="10,203.86,73.31,165.69,132.55" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Investment simulation results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Output format: Correlation: &lt;Correlation score between the news and the stock&gt; Sentiment: &lt;Sentiment polarity score of the news&gt; Importance: &lt;Importance score of the news event&gt; Impact: &lt;Impact score of the news on stock prices&gt; Duration: &lt;Duration score of the news impact&gt; [Input] [Stock Name]: {stock name} [News Content]:{news content} [Publish Time]:{publish time}</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>of Multi-stock G 1 Causal Graph</head><label></label><figDesc><ref type="bibr" target="#b0">1</ref> for D stocks with time lag L. For i-th stock, each time step representation X i t is the combination of the text representation C i t generated by the news encoder and the historical price features representation P i t generated by the price encoder.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>07/06</cell><cell>META</cell><cell>GOOG</cell><cell cols="2">APPLE</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Feed Forward Layer</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">Aggregation Function</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Feed Forward Layer</cell><cell></cell><cell cols="3">Feed Forward Layer</cell></row><row><cell>News Representation</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Prices Encoder</cell><cell>Prices Representation</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>G 4</cell><cell>G3</cell><cell>G 2</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>07/05</cell></row><row><cell>Impact Duration Sentiment Importance Correlation LLM Evaluation Aspects</cell><cell cols="3">07/05 s E n c o d e r N e w</cell><cell>META</cell><cell cols="2">GOOG</cell><cell>APPLE</cell><cell>07/04 Prices Encoder Prices Encoder</cell><cell>time Historical Prices of Stocks price</cell></row><row><cell>News Corpora of Stocks 1. Apple heads for largest</cell><cell>D e n o i s e d</cell><cell>07/04</cell><cell>META</cell><cell cols="2">META</cell><cell></cell><cell>META</cell><cell>META</cell><cell>07/03 Prices Encoder</cell></row><row><cell>2016 as iPhone sales slow. Q3 revenue drop since</cell><cell>07/03</cell><cell></cell><cell>GOOG</cell><cell>GOOG</cell><cell cols="2">GOOG</cell><cell>GOOG</cell><cell>07/02</cell></row><row><cell>2. Q2 Results Show Fears About Alphabet Are</cell><cell></cell><cell>APPLE</cell><cell>APPLE</cell><cell cols="2">APPLE</cell><cell cols="2">APPLE</cell><cell>Distribution of Causal Link Existence</cell></row><row><cell>Overblown …</cell><cell>07/02</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">07/02</cell><cell>07/03</cell><cell>07/04</cell><cell></cell><cell cols="2">07/05</cell></row></table><note><p>Price encoder For i-th stock, we denote the raw adjusted closing, highest, lowest, open, closing prices and trading volume on trading day t as P i t = P i,a t , P i,h t , P i,l t , P i,o t , P i,c t , V t . By feeding P i t into the embedding layer, the historical prices could be represented as P i t ∈ R dp×1 , where d p is the price embedding size. Movement Predictions Causal Link Lag-dependent Temporal Causal Link</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Main results of CausalStock and baselines for two stock movement prediction tasks on multiple datasets. Following the setting of baselines, the standard deviations are calculated across 10 runs for the news-driven task and 5 runs for the task without news.</figDesc><table><row><cell></cell><cell></cell><cell cols="3">News-driven Multi-stock movement prediction task</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>ACL18 (US)</cell><cell cols="2">CMIN-US (US)</cell><cell cols="2">CMIN-CN (CN)</cell></row><row><cell>Models</cell><cell>ACC</cell><cell>MCC</cell><cell>ACC</cell><cell>MCC</cell><cell>ACC</cell><cell>MCC</cell></row><row><cell>HAN</cell><cell cols="6">57.64 ± 0.0040 0.0518 ± 0.0050 53.72 ± 0.0020 0.0103 ± 0.0015 53.59 ± 0.0037 0.0159 ± 0.0026</cell></row><row><cell>StockNet</cell><cell cols="6">58.23 ± 0.0030 0.0808 ± 0.0071 52.46 ± 0.0041 0.0220 ± 0.0025 54.53 ± 0.0062 0.0450 ± 0.0043</cell></row><row><cell>PEN</cell><cell cols="6">59.89 ± 0.0090 0.1556 ± 0.0018 53.20 ± 0.0051 0.0267 ± 0.0023 54.83 ± 0.0086 0.0857 ± 0.0065</cell></row><row><cell>CMIN</cell><cell cols="6">62.69 ± 0.0029 0.2090 ± 0.0016 53.43 ± 0.0085 0.0460 ± 0.0055 55.28 ± 0.0094 0.1110 ± 0.0990</cell></row><row><cell cols="7">CausalStock 63.42 ± 0.0039 0.2172 ± 0.0017 54.64 ± 0.0083 0.0481 ± 0.0057 56.19 ± 0.0084 0.1417 ± 0.0813</cell></row><row><cell></cell><cell></cell><cell cols="3">Multi-stock movement prediction task</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>KDD17 (US)</cell><cell>NI225 (JP)</cell><cell></cell><cell cols="2">FTSE100 (UK)</cell></row><row><cell>Models</cell><cell>ACC</cell><cell>MCC</cell><cell>ACC</cell><cell>MCC</cell><cell>ACC</cell><cell>MCC</cell></row><row><cell>LSTM</cell><cell cols="6">51.18 ± 0.0066 0.0187 ± 0.0110 50.79 ± 0.0079 0.0148 ± 0.0162 50.96 ± 0.0065 0.0187 ± 0.0129</cell></row><row><cell>ALSTM</cell><cell cols="6">51.66 ± 0.0041 0.0316 ± 0.0119 50.60 ± 0.0066 0.0125 ± 0.0139 51.06 ± 0.0038 0.0231 ± 0.0077</cell></row><row><cell>StockNet</cell><cell cols="6">51.93 ± 0.0001 0.0335 ± 0.0050 50.15 ± 0.0054 0.0050 ± 0.0118 50.36 ± 0.0095 0.0134 ± 0.0135</cell></row><row><cell cols="7">Adv-ALSTM 51.69 ± 0.0058 0.0333 ± 0.0137 51.60 ± 0.0103 0.0340 ± 0.0201 50.66 ± 0.0067 0.0155 ± 0.0140</cell></row><row><cell>DTML</cell><cell cols="6">53.53 ± 0.0075 0.0733 ± 0.0195 52.76 ± 0.0103 0.0626 ± 0.0230 52.08 ± 0.0121 0.0502 ± 0.0214</cell></row><row><cell cols="7">CausalStock 56.09 ± 0.0069 0.1235 ± 0.0189 53.01 ± 0.0150 0.0640 ± 0.0310 52.88 ± 0.0009 0.0534 ± 0.0210</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Ablation study results on different datasets.</figDesc><table><row><cell></cell><cell></cell><cell>ACL18</cell><cell>CMIN-US</cell><cell>CMIN-CN</cell></row><row><cell>Ablation Type</cell><cell>Ablation Variants</cell><cell cols="2">ACC MCC ACC MCC ACC MCC</cell></row><row><cell></cell><cell>CausalStock w/o TCD</cell><cell cols="2">51.08 0.0102 51.48 0.0106 51.37 0.0102</cell></row><row><cell></cell><cell>CausalStock w/o news</cell><cell cols="2">58.10 0.1421 53.16 0.0375 54.16 0.1264</cell></row><row><cell>Main Framework</cell><cell>CausalStock w/o link non-existence</cell><cell cols="2">58.21 0.1652 52.32 0.0241 53.96 0.0670</cell></row><row><cell></cell><cell>CausalStock w/o Lag-dependent TCD</cell><cell cols="2">59.19 0.1757 52.93 0.0312 54.97 0.1298</cell></row><row><cell></cell><cell cols="3">CausalStock with Variable-dependent TCD 63.50 0.2175 54.60 0.0479 56.25 0.1419</cell></row><row><cell></cell><cell>CausalStock with Glove+Bi-GRU</cell><cell cols="2">60.78 0.1952 53.87 0.0467 55.13 0.1326</cell></row><row><cell></cell><cell>CausalStock with Bert</cell><cell cols="2">61.74 0.2067 53.92 0.0472 55.43 0.1352</cell></row><row><cell>Traditional News Encoder</cell><cell>CausalStock with Roberta CausalStock with FinBert</cell><cell cols="2">61.81 0.2071 54.06 0.0477 55.58 0.1364 61.72 0.2062 54.01 0.0471 55.61 0.1362</cell></row><row><cell></cell><cell>CausalStock with FinGPT</cell><cell cols="2">61.69 0.2060 54.00 0.0470 55.60 0.1360</cell></row><row><cell></cell><cell>CausalStock with Llama</cell><cell cols="2">62.20 0.2130 54.40 0.0480 55.85 0.1390</cell></row><row><cell></cell><cell>CausalStock with FinGPT</cell><cell cols="2">61.92 0.2105 54.30 0.0475 55.67 0.1386</cell></row><row><cell>Denoised News Encoder</cell><cell>CausalStock with Llama</cell><cell cols="2">62.82 0.2164 54.52 0.0483 55.97 0.1406</cell></row><row><cell></cell><cell>CausalStock (with GPT-3.5)</cell><cell cols="2">63.42 0.2172 54.64 0.0481 56.19 0.1417</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Dataset Description</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Data Range</cell><cell></cell><cell cols="2">Data Resource</cell></row><row><cell>Dataset</cell><cell cols="2">Country Stock</cell><cell>Train</cell><cell>Valid</cell><cell>Test</cell><cell>Price</cell><cell>Text</cell><cell>Price Dim</cell></row><row><cell>ACL18 1</cell><cell>US</cell><cell>88</cell><cell cols="5">2014/01/02-2015/08/02 2015/08/03-2015/09/30 2015/10/01-2016/01/01 Yahoo Finance Twitter</cell><cell>7</cell></row><row><cell>CMIN-US 2</cell><cell>US</cell><cell>110</cell><cell cols="5">2018/01/01-2021/04/30 2021/05/01-2021/08/31 2021/09/01-2021/12/31 Yahoo Finance Yahoo</cell><cell>7</cell></row><row><cell>CMIN-CN 2</cell><cell>CN</cell><cell>300</cell><cell cols="4">2018/01/01-2021/04/30 2021/05/01-2021/08/31 2021/09/01-2021/12/31 Yahoo Finance</cell><cell>Wind</cell><cell>7</cell></row><row><cell>KDD17 3</cell><cell>US</cell><cell>50</cell><cell cols="4">2007/01/03-2015/01/01 2015/01/02-2016/01/03 2016/01/04-2017/01/01 Yahoo Finance</cell><cell>-</cell><cell>11</cell></row><row><cell>NI225 4</cell><cell>JP</cell><cell>51</cell><cell cols="4">2016/07/01-2018/03/01 2018/03/02-2019/01/06 2019/01/07-2019/12/31 Yahoo Finance</cell><cell>-</cell><cell>11</cell></row><row><cell>FTSE100 4</cell><cell>UK</cell><cell>24</cell><cell cols="3">2014/01/06-2017/01/03 2017/01/04-2017/07/03 2017/07/04-2018/06/30</cell><cell>-</cell><cell>-</cell><cell>11</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Hyper-parameter sensitivity study results. ACL18 (with news) 62.56 62.34 63.42 61.58 61.04 63.42 63.29 63.15 58.26 62.35 63.42 63.45 KDD17 (w/o news) 55.45 55.69 56.09 55.13 54.94 56.09 55.95 55.94 53.19 55.57 56.09 55.45</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Learning rate lr</cell><cell></cell><cell></cell><cell cols="2">Time lag L</cell><cell></cell><cell></cell><cell cols="2">Loss weight λ</cell></row><row><cell>Parameters</cell><cell>1e-3</cell><cell>1e-4</cell><cell>1e-5</cell><cell>1e-6</cell><cell>3</cell><cell>5</cell><cell>7</cell><cell>9</cell><cell>0</cell><cell>0.1</cell><cell>0.01 0.001</cell></row></table><note><p>D The correlation results</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>The correlation of the causal strength and the market value of companies on four datasets.</figDesc><table><row><cell>Statistics</cell><cell cols="4">ACL 18 NI225 CMIN-CN FTSE100</cell></row><row><cell cols="3">Spearman Corr. 0.7939 0.7212</cell><cell>0.6491</cell><cell>0.8909</cell></row><row><cell>P-Value</cell><cell>0.006</cell><cell>0.0185</cell><cell>0.0036</cell><cell>0.0005</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://github.com/yumoxu/stocknet-dataset</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://github.com/BigRoddy/CMIN-Dataset</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>https://github.com/fulifeng/Adv-ALSTM</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>https://datalab.snu.ac.kr/dtml</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments and Disclosure of Funding</head><p>This work was supported by the <rs type="funder">National Natural Science Foundation of China (NSFC</rs> Grant No. <rs type="grantNumber">62122089</rs>), <rs type="programName">Beijing Outstanding Young Scientist Program</rs> NO. <rs type="grantNumber">BJJWZYJH012019100020098</rs>, and <rs type="funder">Intelligent Social Governance Platform, Major Innovation &amp; Planning Interdisciplinary Platform for the "Double-First Class" Initiative, Renmin University of China</rs>. <rs type="person">Shuqi Li</rs> is supported by the <rs type="funder">Fundamental Research Funds for the Central Universities</rs>, and the <rs type="funder">Research Funds of Renmin University of China</rs> (<rs type="grantNumber">NO.297522615221</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_YNHhFZV">
					<idno type="grant-number">62122089</idno>
					<orgName type="program" subtype="full">Beijing Outstanding Young Scientist Program</orgName>
				</org>
				<org type="funding" xml:id="_eHfmnZN">
					<idno type="grant-number">BJJWZYJH012019100020098</idno>
				</org>
				<org type="funding" xml:id="_TCuzAgx">
					<idno type="grant-number">NO.297522615221</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Finbert: Financial sentiment analysis with pre-trained language models</title>
		<author>
			<persName><surname>Araci</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.10063</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Bellot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kim</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihaela</forename><surname>Van Der Schaar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.02522</idno>
		<title level="m">Neural graphical modelling in continuous-time: consistency guarantees and algorithms</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Variational inference: A review for statisticians</title>
		<author>
			<persName><forename type="first">Alp</forename><surname>David M Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><forename type="middle">D</forename><surname>Kucukelbir</surname></persName>
		</author>
		<author>
			<persName><surname>Mcauliffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American statistical Association</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">518</biblScope>
			<biblScope unit="page" from="859" to="877" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Sébastien</forename><surname>Bubeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varun</forename><surname>Chandrasekaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronen</forename><surname>Eldan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Gehrke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Horvitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ece</forename><surname>Kamar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yin</forename><surname>Tat Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Lundberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.12712</idno>
		<title level="m">Sparks of artificial general intelligence: Early experiments with gpt-4</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Causaltime: Realistically generated time-series for benchmarking of causal discovery</title>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziqian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qin</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinli</forename><surname>Suo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kunlun</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.01753</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><surname>Bert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<title level="m">Pre-training of deep bidirectional transformers for language understanding</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">News or noise?</title>
		<author>
			<persName><forename type="first">Ilia</forename><surname>Dichev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Research in Higher Education</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="237" to="266" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Enhancing stock movement prediction with adversarial training</title>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huimin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19</title>
		<meeting>the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="5843" to="5849" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Deep end-to-end causal inference</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Geffner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Javier</forename><surname>Antoran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbo</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emre</forename><surname>Kiciman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amit</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angus</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Kukla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Pawlowski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.02195</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">High-recall causal discovery for autocorrelated time series with latent confounders</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Gerhardus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Runge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="12615" to="12625" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirteenth international conference on artificial intelligence and statistics</title>
		<meeting>the thirteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
	<note>JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Review of causal discovery methods based on graphical models</title>
		<author>
			<persName><forename type="first">Clark</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Spirtes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in genetics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">524</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multi-stage hybrid attentive networks for knowledgedriven stock movement prediction</title>
		<author>
			<persName><forename type="first">Jiaying</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hoda</forename><surname>Eldardiry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing -28th International Conference</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<editor>
			<persName><forename type="first">Kok</forename><surname>Wai</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Wong</forename></persName>
		</editor>
		<editor>
			<persName><forename type="first">Achmad</forename><surname>Nizar</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Hidayanto</forename></persName>
		</editor>
		<meeting><address><addrLine>Sanur, Bali, Indonesia</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">December 8-12, 2021. 2021</date>
			<biblScope unit="volume">2021</biblScope>
			<biblScope unit="page" from="501" to="513" />
		</imprint>
	</monogr>
	<note>Proceedings, Part IV</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Wenbo</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Jennings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Pawlowski</surname></persName>
		</author>
		<author>
			<persName><surname>Rhino</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2210.14706</idno>
		<title level="m">Deep causal temporal relationship learning with history-dependent noise</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Listening to chaotic whispers: A deep learning framework for news-oriented stock trend prediction</title>
		<author>
			<persName><forename type="first">Ziniu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiqing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiang</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanzhe</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eleventh ACM international conference on web search and data mining</title>
		<meeting>the eleventh ACM international conference on web search and data mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="261" to="269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Eric</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shixiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01144</idno>
		<title level="m">Categorical reparameterization with gumbel-softmax</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Applications of deep learning in stock market prediction: recent progress</title>
		<author>
			<persName><forename type="first">Weiwei</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Systems with Applications</title>
		<imprint>
			<biblScope unit="volume">184</biblScope>
			<biblScope unit="page">115537</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Causal autoregressive flows</title>
		<author>
			<persName><forename type="first">Ilyes</forename><surname>Khemakhem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Leech</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aapo</forename><surname>Hyvarinen</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on artificial intelligence and statistics</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3520" to="3528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Hats: A hierarchical graph attention network for stock movement prediction</title>
		<author>
			<persName><forename type="first">Raehyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chan</forename><surname>Ho So</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minbyul</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanghoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinkyu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.07999</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd International Conference on Learning Representations, ICLR 2015</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">May 7-9, 2015. 2015</date>
		</imprint>
	</monogr>
	<note>Conference Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Pen: Prediction-explanation network to forecast stock price movement with better explainability</title>
		<author>
			<persName><forename type="first">Shuqi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiheng</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="5187" to="5194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><surname>Roberta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Causality-guided multi-memory interaction network for multivariate stock price movement prediction</title>
		<author>
			<persName><forename type="first">Di</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiheng</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuqi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 61st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="12164" to="12176" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Andriy</forename><surname>Chris J Maddison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yee</forename><forename type="middle">Whye</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><surname>Teh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.00712</idno>
		<title level="m">The concrete distribution: A continuous relaxation of discrete random variables</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Stock market&apos;s price movement prediction with lstm neural networks</title>
		<author>
			<persName><forename type="first">Adriano Cm</forename><surname>David Mq Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renato A De</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><surname>Oliveira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 International joint conference on neural networks (IJCNN)</title>
		<imprint>
			<publisher>Ieee</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1419" to="1426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Dynotears: Structure learning from time-series data</title>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<editor>
			<persName><forename type="first">Roxana</forename><surname>Pamfil</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Nisara</forename><surname>Sriwattanaworachai</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Shaan</forename><surname>Desai</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Philip</forename><surname>Pilgerstorfer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Konstantinos</forename><surname>Georgatzis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Paul</forename><surname>Beaumont</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Bryon</forename><surname>Aragam</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1595" to="1605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Causal inference in statistics: An overview</title>
		<author>
			<persName><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">A dual-stage attention-based recurrent neural network for time series prediction</title>
		<author>
			<persName><forename type="first">Dongjin</forename><surname>Yao Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guofei</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Garrison</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><surname>Cottrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.02971</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Solving general arithmetic word problems</title>
		<author>
			<persName><forename type="first">Subhro</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.01413</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Discovering contemporaneous and lagged causal relations in autocorrelated nonlinear time series datasets</title>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Runge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Uncertainty in Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1388" to="1397" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Detecting and quantifying causal associations in large nonlinear time series datasets</title>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Runge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peer</forename><surname>Nowack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marlene</forename><surname>Kretschmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seth</forename><surname>Flaxman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dino</forename><surname>Sejdinovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science advances</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">4996</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep attentive learning for stock movement prediction from social media text and company correlations</title>
		<author>
			<persName><forename type="first">Ramit</forename><surname>Sawhney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shivam</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arnav</forename><surname>Wadhwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajiv Ratn</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-11">November 2020</date>
			<biblScope unit="page" from="8415" to="8426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Textual analysis of stock market prediction using breaking financial news: The azfin text system</title>
		<author>
			<persName><forename type="first">P</forename><surname>Robert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hsinchun</forename><surname>Schumaker</surname></persName>
		</author>
		<author>
			<persName><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Information Systems (TOIS)</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="19" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Temporal pattern attention for multivariate time series forecasting</title>
		<author>
			<persName><forename type="first">Shun-Yao</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan-Keng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hung-Yi</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">108</biblScope>
			<biblScope unit="page" from="1421" to="1441" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Exploiting topic based twitter sentiment for stock prediction</title>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arjun</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huayi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaotie</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="24" to="29" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">The proof and measurement of association between two things</title>
		<author>
			<persName><forename type="first">Charles</forename><surname>Spearman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1961">1961</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">News or noise? using twitter to identify and understand company-specific news flow</title>
		<author>
			<persName><forename type="first">O</forename><surname>Timm</surname></persName>
		</author>
		<author>
			<persName><surname>Sprenger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Philipp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andranik</forename><surname>Sandner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isabell</forename><forename type="middle">M</forename><surname>Tumasjan</surname></persName>
		</author>
		<author>
			<persName><surname>Welpe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Social Science Electronic Publishing</publisher>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="791" to="830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">News or noise? the stock market reaction to different types of company-specific news events</title>
		<author>
			<persName><forename type="first">O</forename><surname>Timm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isabell</forename><forename type="middle">M</forename><surname>Sprenger</surname></persName>
		</author>
		<author>
			<persName><surname>Welpe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SSRN Electronic Journal</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Touvron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibaut</forename><surname>Lavril</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gautier</forename><surname>Izacard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Martinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Anne</forename><surname>Lachaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothée</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baptiste</forename><surname>Rozière</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Hambro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Faisal</forename><surname>Azhar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.13971</idno>
		<title level="m">Open and efficient foundation language models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Coupling macro-sectormicro financial indicators for learning stock representations with less uncertainty</title>
		<author>
			<persName><forename type="first">Guifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Longbing</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongke</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enhong</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Natural language based financial forecasting: a survey</title>
		<author>
			<persName><forename type="first">Erik</forename><surname>Frank Z Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename><forename type="middle">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><surname>Welsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Review</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="49" to="73" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Stock movement prediction from tweets and historical prices</title>
		<author>
			<persName><forename type="first">Yumo</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shay</forename><forename type="middle">B</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-07">July 2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1970" to="1979" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">Hongyang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao-Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christina</forename><forename type="middle">Dan</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.06031</idno>
		<title level="m">Fingpt: Open-source financial large language models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Accurate multivariate stock movement prediction via data-axis transformer with multi-level contexts</title>
		<author>
			<persName><forename type="first">Jaemin</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejun</forename><surname>Soun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong-Chan</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2037" to="2045" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Stock price prediction via discovering multifrequency trading patterns</title>
		<author>
			<persName><forename type="first">Liheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charu</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guo-Jun</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining</title>
		<meeting>the 23rd ACM SIGKDD international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2141" to="2149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Dags with no tears: Continuous optimization for structure learning</title>
		<author>
			<persName><forename type="first">Xun</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryon</forename><surname>Aragam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Pradeep K Ravikumar</surname></persName>
		</author>
		<author>
			<persName><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning sparse nonparametric dags</title>
		<author>
			<persName><forename type="first">Xun</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Dan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryon</forename><surname>Aragam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pradeep</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Xing</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3414" to="3425" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
