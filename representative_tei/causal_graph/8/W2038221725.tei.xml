<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Bayesian Network Structure Learning Using Causality</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhen</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University at Buffalo</orgName>
								<orgName type="institution" key="instit2">The State University of New York</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Sargur</forename><forename type="middle">N</forename><surname>Srihari</surname></persName>
							<email>srihari@cedar.buffalo.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">University at Buffalo</orgName>
								<orgName type="institution" key="instit2">The State University of New York</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Bayesian Network Structure Learning Using Causality</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Bayesian Networks are probabilistic models of data that are useful to answer probabilistic queries. Since they are usually built by hand, algorithms to automatically learn their structure are lacking, particularly for large data sets encountered today. Existing algorithms use either local measures of deviation from independence or global likelihood measures. We tackle this problem from a new perspective using causality, which is a stronger measure than correlation. Integrating both the global and local views, the proposed algorithm learns a high quality Bayesian network without using any score-based searching. Given a partial directed acyclic graph, causal pairs with the highest accuracy are inferred with the fewest number of pairwise causal inferences. Specifically, with discrete data, the χ 2 statistical test is used to identify the most dependent and possible causal pairs. Furthermore, the learned causality is forward-propagated. Experiments on handwriting data show that, besides the ability of causal inference, our algorithm performs better than two previous algorithms, one based on branch-and-bound search, and the other a greedy algorithm using χ 2 tests and a log-loss function. The learned structure not only has lowest loss in representing the data, but also reveals underlying causal relationships which are useful for scientific discovery.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Bayesian Networks (BNs) are probabilistic graphical models that represent joint distributions of a set of variables efficiently in a factorized way <ref type="bibr" target="#b0">[1]</ref>. A BN is a directed acyclic graph where nodes represent variables and directed edges represent dependency relationships. BNs are widely used in diagnosis, troubleshooting, data mining, etc; some examples are pathologist diagnose lymph node pathologies <ref type="bibr" target="#b0">[1]</ref>, printer troubleshooting and automobile troubleshooting <ref type="bibr" target="#b1">[2]</ref>. Their advantages are in providing a relationship between variables that is easy to understand and in providing a compact form where the amount of data needed to represent the full joint distribution is reasonable.</p><p>BNs are largely constructed manually based on expert knowledge. Their use is to infer answers to probabilistic queries. However, with dynamically changing data sets it is useful to automatically learn their structures from data. BNs structure learning is a very active research area, which has three main approaches:</p><p>• The constraint-based approach <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b3">[4]</ref> finds a set of conditional independencies and learn the structure of BNs that admit these independencies. Since independencies are only partial properties of the data set, the constraint-based approach usually cannot learn the full structure.</p><p>• The score-based approach <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b4">[5]</ref>- <ref type="bibr" target="#b7">[8]</ref> searches the graph space and find the BN structure with the highest score. It is a model selection problem. Because the search space contains a super-exponential number of structures, the search problem is NP-hard <ref type="bibr" target="#b8">[9]</ref> and many heuristic algorithms have been proposed.</p><p>• The Bayesian model averaging approach generates and averages a set of possible BN structures. Since the number of possible structures can be exponential, some approximation algorithms are devised.</p><p>We tackle this problem from a more interesting and fundamental perspective. Roughly speaking, the relationships between two variables are either independent or correlated. The correlation comes either from direct cause and effect relationship between these two variables or from the common cause of these two variables. Instead of using correlation to learn BNs structures the same way as traditional algorithms, we want to use causation to learn the BNs structures. Because causality is more fundamental than correlation, in common sense, the learned structure should be more natural and stable. In addition, it can be served as a good source for scientific discovery. Researchers can search and validate the underlying causality revealed by the learned structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. BN STRUCTURE LEARNING</head><p>A BN structure captures independencies that exist in the data to reduce the complexity of the probabilistic model. Methods to construct BNs use a deviation measure from independence and a score function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Deviation Measure</head><p>An independence test is used to test independency between two variables given observations. For categorical variables we can use Pearson's Chi-squared test to test the null hypothesis of independence. It is a goodness-of-fit test which measures the difference between observed frequency distribution and a theoretical distribution. For two multinomial variables A and B with distributions P (A = a i ) and P (B = b i ), and a data set D with M samples, we define O[a i , b j ] is the observed number of samples that A = a i and B = b i , and E[a i , b j ] = M P (a i )P (b j ) is the expected number of samples. The χ 2 test can be defined as:</p><formula xml:id="formula_0">χ 2 (D) = i,j (O[ai,bj ]-E[ai,bj ]) 2 E[ai,bj ] .<label>(1)</label></formula><p>The bigger test value indicates more dependency between two variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Score Function</head><p>One popular score function for BN learning is the negative log-likelihood, which is known as log-loss <ref type="bibr" target="#b0">[1]</ref>. Given a data set D with N i.i.d samples, the log-loss is defined as:</p><formula xml:id="formula_1">l(D|G) = - n i=1 N j=1 log P (x i [j]|x pa (i) [j]),<label>(2)</label></formula><p>where x i [j] is the value of the i th variable (feature) in the j th sample and x pa (i) <ref type="bibr">[j]</ref> are values of parents of x i in the j th sample. It is the loss in terms of using the model to represent the true probability distribution. Since log-probabilities are negative the measure has the convenience of being a positive number.</p><p>This score is used by the B &amp; B algorithm and the greedy algorithm described next, but not by the proposed causal algorithm. It is only used to compare the the quality of the resulting structure, as described in section VI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Baseline Algorithms</head><p>Here we give brief descriptions of two BN structure learning algorithms used to benchmark the performance of the proposed algorithm (Section VI).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Branch and bound (B &amp; B) algorithm:</head><p>This algorithm is a score-based approach, which leverages some useful properties of the scoring function to reduce the time and memory costs <ref type="bibr" target="#b9">[10]</ref>. Specifically, it uses the minimum description length (MDL) as the score function, which is defined based on logloss and has the desirable decomposable property. Using this property, it builds cache in O(n • 2 n ) time, where n is the number of variables, to avoid redundant computation of logloss scores. The branch and bound approach makes it an any-time algorithm that can stop at the current best solution while showing an upper bound to global optimum. The time complexity of this algorithm is mainly bound to the cache constructing time. The experiment results are very good for both randomly generated data sets and certain public data sets.</p><p>2) Greedy algorithm: This uses both the local deviance measure of independence and global log-loss score <ref type="bibr" target="#b10">[11]</ref>. It adds and orients one edge at a time while using χ 2 statistics to set the order and log-loss score to decide the orientation. The time complexity of this algorithm is O(c • 2 q ), where n is the number of variables and q is the maximum number of parents. Experiments on handwritten data show its good performance. Compared with these two algorithms, our algorithm does not use any global score in the process of structure learning. The log-loss score is only computed with learned BN structure in order to compare the performance among all three algorithms. Because causation is the most important and fundamental factor in correlation, the causal inference is sure to learn a good BN structure, that indirectly implies a good log-loss score. Our experiments in section VI show that the score for learned structure can even be better.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. CAUSALITY</head><p>Broadly speaking, two variables are either independent or correlated. Two variables are independent means the occurrence of one does not influence the probability of the other. While two variables are correlated means there is a dependency relationships between these two. Particularly, the causation describe the causal and effect relationship between two variables. There are four possible relationships between two variables A and B <ref type="bibr" target="#b11">[12]</ref>: A is independent with B; A causes B; B causes A; A and B are effects of a hidden common cause C, but they do not cause each other <ref type="bibr" target="#b12">[13]</ref>, as showed in Fig. <ref type="figure" target="#fig_0">1</ref>. The independence is illustrated in Fig. <ref type="figure" target="#fig_0">1</ref>  <ref type="figure">(c</ref>). from these figures, we can see the causation is more fundamental than correlation and correlation is a broader concept than causation. In most cases, correlation is a good hint to find the causation. While there are many standard statistical independence test to differentiate independency and correlation, it is hard to determine the causation. There are two directions of causal inference from global and local perspectives.</p><p>Observe that both causation and correlation are dependent relationships which can be excluded if two variables are independent. PC algorithm <ref type="bibr" target="#b13">[14]</ref>, which is named by inventors' initials, adopts the constraint-based approach to learn a partial directed acyclic graph (PDAG) which contains directed and undirected edges. This algorithm mainly contains two phases including edge-deleting and edges-orienting. It begins from an undirected complete graph and utilizes the independence test to delete edges. For example, if X and Y are independent, then it deletes edge X -Y . If X|Z is conditional independent with Y |Z, then it deletes X -Y . The edge orienting phase mainly leverages the v-structure and DAG constraints. For example, if there are two edges X -Y and Y -Z, X is independent with Z while X|Y is not conditional independent with Z|Y , then it orients X → Y ← Z. Apply the same procedure until there are no more edges can be oriented by v-structure and DAG constraints, and output the corresponding PDAG, which will still leave lots of edges undirected. The output of this algorithm contains lots of uncertainty and they confined the usefulness of the results for answering probabilistic queries and making predictions. On the other hand, the model-based approaches <ref type="bibr" target="#b14">[15]</ref>- <ref type="bibr" target="#b17">[18]</ref> infer the causality between two variables by modeling the data generating process. These approaches model the effect variable as a function of the cause variable and the additive noise. In order to break the symmetry and infer the causal relationship between two variables, they made some assumptions such as non-Gaussian noise, non-invertible functional relationship, etc. If the modeled functional relationship is the same as the real data generating process, then the additive noise should be independent with the cause variable. The limitations of model-base approaches come from high computation complexity. It is only feasible to a very small graph and suffers from multiple hypothesis testing problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. PROPOSED METHOD</head><p>The global and local views of causal inference can be combined to design a computationally efficient BN structure learning algorithm. At the beginning, the constraint-based approach is used to identify the PDAG efficiently, which contains a set of directed and undirected edges. The modelbased approach can be used to orient the undirected edges. This is a non-trivial task, because different undirected edges have different causal inference difficulties and accuracies, and they carry different amount causal information which can be used to orient other undirected edges. We need to decide the right order to orient undirected edges and the appropriate way of , With the intuition that the causation is a stronger dependent relationship, we use the χ 2 statistical tests <ref type="bibr" target="#b18">[19]</ref> to sort the undirected edges by their dependencies. After identifying the undirected edge with highest dependency, we use the model-based approach to causally orient this edge. with this extra piece of causal information, we apply the causal forward propagation algorithm to other undirected edges which can be oriented further. We repeat this procedure until all edges are causally oriented. Our algorithm uses both the efficiency and stability of the constraint-based method and the causal inference power of the model-based approach to find a BN with small uncertainty. The causal forward propagation algorithm can significant decrease the number of pairwise causal inference and thus save a lot of computations. Moreover, always choosing variable pair with highest dependency improves the accuracy because in common sense the strongest dependency signifies the causality which can be easily identified.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Nonlinear Additive Noise Model</head><p>The nonlinear additive noise model <ref type="bibr" target="#b15">[16]</ref> assumes the nonlinear causal relationship between cause and effect variables. It assumes non-invertible functional relationships between variables to break the symmetry and identify the causality. It models the data generating process as:</p><formula xml:id="formula_2">x i = f (x Pa (i) ) + e i ,<label>(3)</label></formula><p>where f is an arbitrary non-invertible function, x Pa (i) is a vector containing all the parents of x i , and independent noise variable e i may have arbitrary probability densities p(e i ). If the modeled function is the same as real data generating process, then the noise should be independent with the cause variables.</p><p>Given two observed variables x and y. The algorithm first tests whether x and y are statistically independent. If they are not, the algorithm tests weather a model y = f (x) + e is consistent with data. It contains several steps. Firstly, it does a nonlinear regression of y on x to get an estimate f of f . Then it calculates the corresponding residuals ê = y -f (x) and test whether ê is independent of x. If so, it accepts y = f (x)+e; if not, it rejects it. The similar test decides whether the reverse model x = g(y) + e fits the data. There are four possible results. Firstly, if x and y are mutually independent, there is no causal relationship between them. Secondly, if they are dependent and both directional models fit the data, then either model may be correct but we do not know which. Thirdly, if </p><formula xml:id="formula_3">Algorithm 1: CausalBN(V ,D) Input : Vertex Set V = {v 1 , v 2 , ..., v n }, data set D Output : A causal Bayesian network G * = {V, E * } 1 Compute PDAG G p = {V, E p } using</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. STRUCTURE LEARNING ALGORITHM</head><p>The BN structure learning algorithm based on causality has four main steps.</p><p>1) Use the PC algorithm <ref type="bibr" target="#b13">[14]</ref> in Algorithm 2 to identify the PDAG which contains some undirected edges. PC algorithm starts with a complete undirected graph and uses conditional independence to eliminate or orient edges. The edge type for directed pair and undirected pair are expressed as → and -. Here we define two nodes are adjacent if there is a directed and undirected edge between them. 2) Use the χ 2 test to sort the undirected edges based on their dependencies. 3) Orient the most dependent undirected pair using nonlinear additive noise model. 4) Apply causal forward propagation in Algorithm 3 to this oriented edge to orient other undirected edges recursively. 5) Repeat the similar procedure until al edges are oriented.</p><p>The complete algorithm is given in Algorithm 1.</p><p>The causal forward propagation uses four rules to propagate the causal information when we have a new directed edge. The first rule utilizes directed acyclic property to orient the edges. Note that the circle contains undirected edge does not violate directed acyclic property. The other three rules orient the undirected edge by preventing introducing a new v-structure. Because in PC algorithm, we already use the conditional independency to identify all the possible v-structures completely, so we can use this property to oriented edges. The second rule is straightforward. For the third rule, if we orient {v j -v k } as {v j → v k }, then by DAG constraints, we must orient v k and v i as {v i → v k }, and {v l → v k } for v k and v l . It will produce new v-structure. For the fourth rule, if we orient {v j -v k } as {v j → v k }, then there must be two directed edges The time complexity of our algorithm mainly comes from the PC algorithm. In Algorithm 1, PC algorithm in step 1 takes</p><formula xml:id="formula_4">n 2 (n-1) k-1 (k-1)!</formula><p>time in the worst case <ref type="bibr" target="#b13">[14]</ref>, where n is the number of variables, and k is the maximal degree of any vertex. Step 2 takes O(n 2 ), step 3 takes time O(n 2 log(n)). In addition, the while loop runs at most O(n 2 ) times, within which steps 5 and 6 only take O(1) time. The Algorithm 3 in step 7 needs further consideration. Assume there is no recursion, the first for loop in Algorithm 3 runs O(n 2 ) times, the codes within run O(1) time. The second for loop runs O(n 2 ) times, codes within run O(n 2 ) time. The third and fourth for loops run O(n 2 ) times, the codes within run O(n 4 ) time. So the whole algorithm without recursion runs in O(n 6 ) time. Note that there are at most O(n 2 ) undirected edges, so there are at most O(n 2 ) recursions. Each recursion orients one undirected edge, at the same time, it reduces one while loop. So the whole while loop takes O(n 8 ) time. The whole Algorithm 1 takes</p><formula xml:id="formula_5">n 2 (n-1) k-1 (k-1)!</formula><p>time in the worst case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. PERFORMANCE EVALUATION</head><p>We describe here the performance of the proposed causal structure learning algorithm in terms of log-loss, which measures how well the proposed model fits the data. We use two handwriting data sets, both involving writing of the common word "and": 1) Samples from a representative adult population of the United States <ref type="bibr" target="#b19">[20]</ref>. There are writing samples of about 1, 500 writers containing both hand-printed and cursive writing styles. The data set contains 3, 075 cursive "and" samples and 1, 135 hand-print "and" samples, both of which have 9 features. The meaning of these features are in <ref type="bibr" target="#b10">[11]</ref>. 2) Handwriting samples of children from the first-to fifth-grades. There are 524 cursive "and" samples and 918 hand-print "and" samples. Each sample has 13 </p><formula xml:id="formula_6">= E -{v i -v j } + {v i → v j };</formula><p>2 Remove e from list E u ;</p><p>for Each edge e</p><formula xml:id="formula_7">1 = {v m , v n } in list E u do G 1 = G; Orient e 1 = {v m → v n } in G 1 ; if ¬isDAG(G 1 ) then 3 Orient e 1 = {v n → v m };<label>4</label></formula><p>Apply causal forward propagation, G = CausalForwardProp(G,e 1 ); features. The meaning of these features can refer to <ref type="bibr" target="#b20">[21]</ref>.</p><formula xml:id="formula_8">G 1 = G; Orient e 1 = {v n → v m } in G 1 ; if ¬isDAG(G 1 ) then 5 Orient e 1 = {v m → v n };</formula><p>We compare performance on these two data sets using (i) a state-of-art branch and bound algorithm (B &amp; B) <ref type="bibr" target="#b9">[10]</ref>, (ii) the greedy algorithm <ref type="bibr" target="#b10">[11]</ref> introduced in section II-C, and (iii) the proposed Causal BN learning algorithm.</p><p>Table <ref type="table" target="#tab_1">I</ref> shows the relative performance of different BN structure learning algorithms using the log-loss measure. A higher score signifies higher probability of the data set given the proposed model. The first two rows give the results with the first data set and the last two with the second data set. The first three columns show the data sets, writing styles, and the number of variables (features). The fourth column corresponds  to log-loss when all variables are independent (no edges in graph). The fifth column contains results from the BN structure learning algorithm in <ref type="bibr" target="#b10">[11]</ref>. The sixth column contains results from the B &amp; B algorithm. BNs learned from "and" data set are shown in Fig. <ref type="figure" target="#fig_5">2</ref>. From Table <ref type="table" target="#tab_1">I</ref>, we can conclude that beside the ability of causal inference, the proposed algorithm performs better in BN structure learning and fits the data sets very well.</p><formula xml:id="formula_9">X 3 X 4 X 5 X 6 X 7 X 8 X 1 X 2 X 9 (a) X 3 X 4 X 5 X 6 X 7 X 8 X 1 X 2 X 9 (b) X 3 X 4 X 2 X 6 X 7 X 8 X 1 X 5 X 9<label>(c)</label></formula><p>Some examples of causality learnt from the data are as follows:</p><p>• For cursive 'and' in set 1, the formation of staff of 'd' (X 6 ), including tented, retracted, looped, etc, decides the formation of terminal stroke of 'd' (X 8 ), which includes curved up, straight across, curved down, etc.  • For hand-print 'and' in set 2, the 'a'-'n' relationship (x 10 ) decides the 'n'-'d' relationship (x 12 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION</head><p>A new Bayesian network structure learning algorithm has been proposed. It is deterministic, computationally efficient and uses the principle of causality Given intuitions and limitations of both global and local causal inference algorithms, we solve a set of challenges to combine them to result in a better algorithm. Specifically, given the causal skeleton, we use the χ 2 statistical test to identify the most dependent pair and perform the causal inference with strong accurate. After that, our causal forward propagation algorithm propagates the newly learned causal information in order to reduce the number of local causal inference. Experiments on handwritten data show that the algorithm provides better statistical models than two previous algorithms, the B &amp; B algorithm and a greedy algorithm. Moreover the resulting structure has the advantage of discovery of causality.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: The possible relationships between events A and B. (a) A is independent with B; (b) A causes B; (c) B causes A; (d) A and B has a common cause, yet they do not cause each other.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(a). The correlation includes Fig. 1(b)-(d), while the causation includes Fig. 1(b),</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Algorithm 2 : 2 5 If</head><label>225</label><figDesc>Outline of PC-algorithm Input : Vertex Set V = {v 1 , v 2 , ..., v n }, significance level α Output : A PDAG G P 1 Form the complete undirected graph G on the vertex set V ; Test conditional and marginal independencies at a given significance level α. Remove one edge between the adjacent pair if there is certain independent relationship between them; 3 Orient v-structures by conditional independence information; while There are still edges can be oriented by the following rules do 4 If X → Y, Y -Z, X and Z are not adjacent, then oriented Y -Z as Y → Z; X -Y , and there is a directed path from X to Y , then orient X -Y as X → Y ; 6 return G P ; {v i → v k } and {v l → v k } to obey the DAG property, it will produce new v-structure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Algorithm 3 :</head><label>3</label><figDesc>CausalForwardProp(G,e) Input : A PDAG G = {V, E} and a newly oriented edge e = {v i → v j } Output : A PDAG G with less uncertainty 1 Orient e in E, E</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>6 7 Orient e 2 = 9 Orient e 3 = 11 Orient e 3 =</head><label>7293113</label><figDesc>Apply causal forward propagation, G = CausalForwardProp(G,e 1 );for Each edge e 2 in list E p do if There is an undirect edge e 2 = {v j -v k } and there is no edge between v i and v k then {v j → v k }; 8 Apply causal forward propagation, G = CausalForwardProp(G,e 2 ); for Each edge e 3 in list E p do if There is a undirected edge e 3 = {v j -v k }, there are one directed edge {v l → v j }, and there are edges among v k and v i , v l then {v k → v j }; 10 Apply causal forward propagation, G = CausalForwardProp(G,e 3 );for Each edge e 4 in list E p do if There is a undirected edge e 4 = {v j -v k }, there are one directed edge {v l → v i }, there are one edge between v l and v k , and there are no edge between v l and v j then {v k → v j };12 Apply causal forward propagation, G = CausalForwardProp(G,e 3 );13 return G;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 2 :</head><label>2</label><figDesc>Fig.2: Bayesian network structures learned from data set 1 (nine variables corresponding to features of "and" that are cursively written): (a) greedy algorithm<ref type="bibr" target="#b10">[11]</ref>, (b) B &amp; B algorithm, (c) causal algorithm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Two variables (X 6 and X 8 ) and their values in Data Set 1: Cursive 'and'. According to the learnt BN, formation of the staff of d influences the terminal curve.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Two variables and their values in Data Set 2: Hand-print 'and'. The a -n relationship influences the n -d relationship</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Algorithm 2, within E p there is a list of undirect edges E u ; 2 Compute pairwise χ 2 statistics for edge pairs E u ; 3 Sort E u in descending order of χ 2 statistics; 4 G * = G p ; while E u is not empty do ; one of the directions is rejected and the other is accepted, a causal relationship is identified. Fourthly, neither model fits the data, which means the real data generating mechanism is more complex and we cannot use the proposed model to describe it.</figDesc><table /><note><p><p>5</p>Choose edge e t ∈ E u with the highest χ 2 statistics; 6 Infer the causal direct using additive noise model and orient e t = {v i → v j }; 7 Apply causal forward propagation, G * = CausalForwardProp(G * ,e t ); 8 return G *</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I :</head><label>I</label><figDesc>Comparison of four BN structure learning methods on four handwriting data sets. Each algorithm is evaluated using the log-loss (Eq. 2) on the data set. Causal algorithm performs best (lowest loss in representing the data).</figDesc><table><row><cell></cell><cell>Data</cell><cell></cell><cell></cell><cell cols="2">Algorithm</cell><cell></cell></row><row><cell>Data</cell><cell>Type</cell><cell>No.</cell><cell>Ind.Vars.</cell><cell>Greedy</cell><cell>B &amp; B</cell><cell>Causal</cell></row><row><cell>Set</cell><cell>of Data</cell><cell>of</cell><cell>(No</cell><cell>Algo.</cell><cell>Algo.</cell><cell>Algo.</cell></row><row><cell></cell><cell></cell><cell>Vars.</cell><cell>Edges)</cell><cell>[11]</cell><cell>[10]</cell><cell></cell></row><row><cell cols="2">Set 1 Cursive</cell><cell>9</cell><cell>25994</cell><cell>25329</cell><cell>25642</cell><cell>24988</cell></row><row><cell cols="3">Set 1 Handprint 9</cell><cell>8059</cell><cell>7898</cell><cell>7301</cell><cell>7094</cell></row><row><cell cols="2">Set 2 Cursive</cell><cell>12</cell><cell>5316</cell><cell>5142</cell><cell>5139</cell><cell>5008</cell></row><row><cell cols="3">Set 2 Handprint 12</cell><cell>7825</cell><cell>7004</cell><cell>6976</cell><cell>6956</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Proc. Int. Conf. Pattern Recognition (ICPR), Stockholm, Sweden, IEEE-CS Press, pp. 3456-3551.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Probabilistic graphical models: principles and techniques</title>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Friedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Troubleshooting under uncertainty</title>
		<author>
			<persName><forename type="first">D</forename><surname>Heckerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Breese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Rommelse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="page" from="121" to="130" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<title level="m">Causality: models, reasoning and inference</title>
		<imprint>
			<publisher>Cambridge Univ Press</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Causal inference and causal explanation with background knowledge</title>
		<author>
			<persName><forename type="first">C</forename><surname>Meek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh conference on Uncertainty in artificial intelligence</title>
		<meeting>the Eleventh conference on Uncertainty in artificial intelligence</meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="403" to="410" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Ordering-based search: A simple and effective algorithm for learning bayesian networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Teyssier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.1429</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Context-specific bayesian clustering for gene expression data</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Barash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the fifth annual international conference on Computational biology</title>
		<meeting>the fifth annual international conference on Computational biology</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="12" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning bayesian network structure from massive datasets: the sparse candidate algorithm</title>
		<author>
			<persName><forename type="first">N</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Nachman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Peér</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth conference on Uncertainty in artificial intelligence</title>
		<meeting>the Fifteenth conference on Uncertainty in artificial intelligence</meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="206" to="215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Data perturbation for escaping local maxima in learning</title>
		<author>
			<persName><forename type="first">G</forename><surname>Elidan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ninio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shuurmans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Conference on Artificial Intelligence</title>
		<meeting>the National Conference on Artificial Intelligence<address><addrLine>Menlo Park, CA; Cambridge, MA; London</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1999">1999. 2002</date>
			<biblScope unit="page" from="132" to="139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Large-sample learning of bayesian networks is np-hard</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Chickering</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Heckerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Meek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1287" to="1330" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Structure learning of bayesian networks using constraints</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">P</forename><surname>De Campos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International Conference on Machine Learning</title>
		<meeting>the 26th Annual International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="113" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bayesian network structure learning and inference methods for handwriting</title>
		<author>
			<persName><forename type="first">M</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">N</forename><surname>Srihari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Document Analysis and Recognition (ICDAR), 2013 12th International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1320" to="1324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">The direction of time</title>
		<author>
			<persName><forename type="first">H</forename><surname>Reichenbach</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991">1991</date>
			<publisher>Univ of California Press</publisher>
			<biblScope unit="volume">65</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Explanation and understanding</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">C</forename><surname>Keil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annual review of psychology</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page">227</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">N</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Scheines</surname></persName>
		</author>
		<title level="m">Causation Prediction &amp; Search 2e</title>
		<imprint>
			<publisher>MIT press</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">81</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A linear nongaussian acyclic model for causal discovery</title>
		<author>
			<persName><forename type="first">S</forename><surname>Shimizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">O</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kerminen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2003" to="2030" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Nonlinear causal discovery with additive noise models</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">O</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mooij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Distinguishing causes from effects using nonlinear acyclic causal models</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
		<ptr target="http://www.cs.helsinki.fi/u/ahyvarin/papers/Zhang09NIPSworkshop.pdf.Citeseer" />
	</analytic>
	<monogr>
		<title level="m">NIPS 2008 Workshop on Causality</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">On the identifiability of the post-nonlinear causal model</title>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="647" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Principles of data mining (adaptive computation and machine learning)</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Hand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mannila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Smyth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Individuality of handwriting</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">N</forename><surname>Srihari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-H</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Forensic Sciences</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="856" to="872" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Probabilistic modeling of children&apos;s handwriting</title>
		<author>
			<persName><forename type="first">M</forename><surname>Puri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">N</forename><surname>Srihari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hanson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Document Recognition and Retireval</title>
		<meeting>Document Recognition and Retireval</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">XXI</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
