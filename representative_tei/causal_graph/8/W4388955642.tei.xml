<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unbiased Top-ğ‘˜ Learning to Rank with Causal Likelihood Decomposition</title>
				<funder ref="#_QmfD2dP">
					<orgName type="full">Fundamental Research Funds for the Central Universities</orgName>
				</funder>
				<funder ref="#_tKSwBqd">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_WRw2px7">
					<orgName type="full">Research Funds of Renmin University of China</orgName>
				</funder>
				<funder>
					<orgName type="full">Intelligent Social Governance Interdisciplinary Platform, Major Innovation &amp; Planning Interdisciplinary Platform for the &quot;Double-First Class&quot; Initiative, Renmin University of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-06-13">13 Jun 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Haiyuan</forename><surname>Zhao</surname></persName>
							<email>haiyuanzhao@ruc.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Jun</forename><surname>Xu</surname></persName>
							<email>junxu@ruc.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Xiao</forename><surname>Zhang</surname></persName>
							<email>zhangx89@ruc.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Guohao</forename><surname>Cai</surname></persName>
							<email>caiguohao1@huawei.com</email>
						</author>
						<author>
							<persName><forename type="first">Zhenhua</forename><surname>Dong</surname></persName>
							<email>dongzhenhua@huawei.com</email>
						</author>
						<author>
							<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
							<email>jrwen@ruc.edu.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Information Renmin</orgName>
								<orgName type="institution">University of China Beijing</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="institution">Renmin University of China Beijing</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="laboratory">Noah&apos;s Ark Lab</orgName>
								<orgName type="institution">Renmin University of China Beijing</orgName>
								<address>
									<settlement>Huawei Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Gaoling School of Artificial Intelligence</orgName>
								<orgName type="laboratory">Noah&apos;s Ark Lab</orgName>
								<orgName type="institution">Renmin University of China Beijing</orgName>
								<address>
									<settlement>Huawei Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department" key="dep1">Engineering Research Cen- ter of Next-Generation Intelligent Search and Recommendation</orgName>
								<orgName type="department" key="dep2">Ministry of Education</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Unbiased Top-ğ‘˜ Learning to Rank with Causal Likelihood Decomposition</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-06-13">13 Jun 2024</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3624918.3625340</idno>
					<idno type="arXiv">arXiv:2204.00815v2[cs.IR]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>unbiased learning to rank</term>
					<term>position bias</term>
					<term>sample selection bias</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Unbiased learning to rank methods have been proposed to address biases in search ranking. These biases, known as position bias and sample selection bias, often occur simultaneously in real applications. Existing approaches either tackle these biases separately or treat them as identical, leading to incomplete elimination of both biases. This paper employs a causal graph approach to investigate the mechanisms and interplay between position bias and sample selection bias. The analysis reveals that position bias is a common confounder bias, while sample selection bias falls under the category of collider bias. These biases collectively introduce a cascading process that leads to biased clicks. Based on our analysis, we propose Causal Likelihood Decomposition (CLD), a unified method that effectively mitigates both biases in top-ğ‘˜ learning to rank. CLD removes position bias by leveraging propensity scores and then decomposes the likelihood of selection biased data into sample selection bias term and relevance term. By maximizing the overall log-likelihood function, we obtain an unbiased ranking model from the relevance term. We also extend CLD to pairwise neural ranking. Extensive experiments demonstrate that CLD and its pairwise neural extension outperform baseline methods by effectively mitigating both position bias and sample selection bias. The robustness of CLD is further validated through empirical studies considering variations in bias severity and click noise.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>In order to efficiently make use of user interaction data in learning of ranking models, studies on alleviating biases in user interaction data have been conducted, called Unbiased Learning to Rank (ULTR) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b17">18]</ref> or Counterfactual Learning to Rank (CLTR) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b26">27]</ref>. Previously, studies focused on position bias and usually assumed that users can examine the whole ranking list so that every relevant document is guaranteed to be examined <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b47">47]</ref>. Due to the limitation of the device sizes, however, search engines usually only display at most ğ‘˜ relevant documents to the user-issued query, on the basis of existing ranking models. It leads to the problem of unbiased top-ğ‘˜ learning to rank <ref type="bibr" target="#b23">[24]</ref>.If the ranking models are trained on the user interactions with these top-ğ‘˜ displayed documents, the sample selection bias will occur <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>. Moreover, the user interaction data gathered from the top-ğ‘˜ ranking positions still suffers from the effect of position bias, making unbiased top-ğ‘˜ learning to rank more challenging.</p><p>Recently, Oosterhuis and de Rijke <ref type="bibr" target="#b24">[25]</ref> developed policy-aware propensity scoring to eliminate sample selection bias. They proved that the policy-aware estimator is unbiased if every relevant item has a non-zero probability to appear in the top-ğ‘˜ ranking. However, they treated position bias and sample selection bias as identical biases and applied a uniform policy-aware propensity score to reweight them. Moreover, their approach relies on knowledge of multiple logging policies and requires these policies to produce sufficiently different ranking orders, which is an expensive external intervention. In a similar vein, inspired by Heckman's two-stage method <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>, Ovaisi et al. <ref type="bibr" target="#b27">[28]</ref> proposed Heckman rank for top-ğ‘˜ learning to rank. To jointly correct both sample selection bias and position bias, they introduced RankAgg, which combines the results from Heckman rank and IPW (Inverse Propensity Weighting). However, as position bias and sample selection bias were mitigated separately without considering their associations, the approach may still produce biased ranking outcomes. Therefore, effectively mitigating both position bias and sample selection bias simultaneously remains a challenging problem.</p><p>In this study, we analyze the biases present in user interactions using a causal graph framework. By decomposing the likelihood of top-ğ‘˜ ranking, we demonstrate that position bias is generated by the influence of examining items in different positions and represents a typical confounder bias. Conversely, sample selection bias can be understood as a collider bias. Furthermore, we identify an association between position bias and sample selection bias in the context of top-ğ‘˜ ranking. The probability of a click on a document is influenced by both its displayed position and the search engine's selection process. These findings highlight that addressing each bias separately or treating them as identical cannot effectively produce unbiased ranking outcomes. Consequently, we are motivated to develop a novel approach capable of simultaneously mitigating both biases.</p><p>Based on our analysis, we propose a unified approach, called Causal Likelihood Decomposition (CLD), to simultaneously mitigate position bias and sample selection bias. CLD follows a cascade process to address both biases effectively. First, CLD directly applies examination propensity scores to reweight observed user interactions, reducing the impact of position bias. However, these reweighted interactions still suffer from sample selection bias. To further tackle sample selection bias, CLD decomposes the loglikelihood function into distinct components. One component represents an unbiased term solely based on user-perceived relevance, while the other components capture sample selection bias. This decomposition allows for detachment of relevance signals from the observed user interaction data. Theoretical analysis demonstrates that by maximizing the entire log-likelihood function, an unbiased relevance ranking model can be obtained from the unbiased term. Furthermore, we present an extension of CLD that incorporates neural networks as ranking and selection models. The parameters of these models are learned using pairwise losses, enhancing their effectiveness in dealing with bias mitigation.</p><p>CLD offers several advantages: theoretical soundness, elegant extension to pairwise neural ranking, and high accuracy in unbiased top-ğ‘˜ learning to rank. The major contributions of this work are:</p><p>(1) A theoretical analysis towards position bias and sample selection bias from the viewpoint of statistical causal inference; (2) A unified and theoretical sound approach to mitigating both position bias and sample selection bias in top-ğ‘˜ ranking. The method is derived under likelihood maximization and can be applied to both pointwise and pairwise training;</p><p>(3) Extensive experiments on two publicly available datasets demonstrated the effectiveness of the proposed approaches over baselines for the task of unbiased top-ğ‘˜ learning to rank. The empirical analysis also showed the robustness of the approaches in terms of the variation of bias severity and the click noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK 2.1 Unbiased learning to rank</head><p>Recently, there has been a trend towards utilizing the user interaction data (e.g., the click log) as the substitute for the expert annotated relevance labels to train the ranking models in web search <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b43">44]</ref>. In contrast to the expert annotated labels, the user interaction data is massive, cheap, and most importantly, usercentric <ref type="bibr" target="#b0">[1]</ref>. However, the behaviors of the users could probably be affected by some unexpected factors <ref type="bibr" target="#b20">[21]</ref>, including the display ranking position <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b22">23]</ref>, the search engine's selection <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29]</ref>, and others <ref type="bibr">[2, 13, 35-37, 41, 43]</ref>. These factors, along with users' true perceived relevance, impact the observational user interaction data gathered from search engines. Although relevance signal is contained inside, the interaction data cannot be directly used to train the ranking models unless those aforementioned factors are eliminated. Otherwise, a biased ranking model would be learned and hurt the user experience <ref type="bibr" target="#b9">[10]</ref>.</p><p>To mitigate biases, unbiased learning to rank has attracted a lot of research efforts recently. Most approaches focus on addressing a single bias. For example, inverse propensity weighting (IPW) has been widely discussed in many studies <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b22">23]</ref> for addressing the position bias. It estimates the causal effect of examination and extracts them from the click signal directly. The top-ğ‘˜ cut-off of search engines leads to sample selection bias. Inspired by the famous Heckman's two-stage method <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>, Ovaisi et al. <ref type="bibr" target="#b27">[28]</ref> proposed ğ»ğ‘’ğ‘ğ‘˜ğ‘šğ‘ğ‘› ğ‘Ÿğ‘ğ‘›ğ‘˜ for top-ğ‘˜ learning to rank. Furthermore, Oosterhuis and de Rijke <ref type="bibr" target="#b24">[25]</ref> developed policy-aware propensity scoring to eliminate sample selection bias, by assuming that policy-aware estimator knows multiple differ enough logging policies.</p><p>In the real world, various biases could occur simultaneously <ref type="bibr" target="#b9">[10]</ref>. To address the trust bias and position bias simultaneously, Agarwal et al. <ref type="bibr" target="#b1">[2]</ref> proposed a Bayes-IPS estimator. Affine-IPS <ref type="bibr" target="#b36">[37]</ref> improved Bayes-IPS and achieved better performance. Ovaisi et al. <ref type="bibr" target="#b27">[28]</ref> proposed RankAgg who ensembles the ranking results by the model for correcting position bias and the model for correcting sample selection bias. Ovaisi et al. <ref type="bibr" target="#b28">[29]</ref> proposed PIJD which does not require the exact propensity scores and can mitigate both position bias and sample selection bias. More recently, Oosterhuis and de Rijke <ref type="bibr" target="#b25">[26]</ref> introduced an intervention-aware estimator for integrating counterfactual and online learning to rank, which can mitigate position bias, sample selection bias, and trust bias simultaneously. Chen et al. <ref type="bibr" target="#b8">[9]</ref> proposed AutoDebias that leverages the uniform data to learn the optimal debiasing strategy for various biases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Causal inference in information retrieval</head><p>The generation of users' implicit feedback in real search engines is affected by many biased factors. To make this feedback usable, causal inference has been introduced to analyze the generation procedure of users' implicit feedback and mitigate bias inside. For instance, Zheng et al. <ref type="bibr" target="#b45">[46]</ref> analyzed the casual structure of popularity bias and proposed DICE to disentangle the user interest from click. Zhang et al. <ref type="bibr" target="#b44">[45]</ref> further analyzed the causal structure of item popularity and leveraged them to enhance the performance of recommendation. Wang et al. <ref type="bibr" target="#b41">[42]</ref> utilized the causal inference to handle the unobserved confounders in the recommendation. Wang et al. <ref type="bibr" target="#b37">[38]</ref> proposed DecRs to dynamically regulate backdoor adjustment according to user status, thus eliminating the effect of confounders. However, few works conduct a systematical analysis for the bias in ranking from the viewpoint of causal inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">ANALYZING THE BIASES IN TOP-K RANKING 3.1 Problem Formulation</head><p>The problem of unbiased top-ğ‘˜ learning to rank can be described as follows. Given a user query ğ‘ and ğ¾ retrieved documents, each query-document pair (ğ‘, ğ‘‘) is described by a feature vector x = ğœ™ (ğ‘, ğ‘‘) âˆˆ R ğ‘› . The relevance of (ğ‘, ğ‘‘) can be represented by an unobserved variable ğ‘…, which could be binary, ordinal, or real. The retrieved documents are ranked by a logging policy (an existing ranking model) ğœ‹ 0 : R ğ‘› â†¦ â†’ {1, 2, â€¢ â€¢ â€¢ , ğ¾ } according to their features, where each document will be ranked at some position ğ‘ƒ âˆˆ {1, 2, â€¢ â€¢ â€¢ , ğ¾ } by ğœ‹ 0 . In the real world, only the top ğ‘˜ â‰¤ ğ¾ documents can be presented to users due to some limitations (e.g., screen sizes). Let's use ğ‘† âˆˆ {0, 1} to denote whether a document is selected and presented to the user, i.e., ğ‘† = 1 if selected otherwise not. Further, let's use ğ¸ âˆˆ {0, 1} to denote that the user has examined the presented document, and ğ¶ âˆˆ {0, 1} to denote whether a user clicks the document, which is a random variable obeying Bernoulli distribution <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b36">37]</ref>.</p><formula xml:id="formula_0">L biased = ğ‘ ğ‘  âˆ‘ï¸ ğ‘–=1 log (Pr(ğ‘ ğ‘– |x ğ‘– )) ,<label>(1)</label></formula><formula xml:id="formula_1">L de. both biases = ğ‘ ğ‘  âˆ‘ï¸ ğ‘–=1âˆ§ğ‘  ğ‘– =1 log Pr E ğ‘ ğ‘– ğœŒ ğ‘– x ğ‘– + ğ‘ ğ‘  âˆ‘ï¸ ğ‘–=1âˆ§ğ‘  ğ‘– =1 log Pr ğ‘  ğ‘– E ğ‘ ğ‘– ğœŒ ğ‘– , x ğ‘– + ğ‘ ğ‘¢ âˆ‘ï¸ ğ‘–=1âˆ§ğ‘  ğ‘– =0 log (Pr(ğ‘  ğ‘– |x ğ‘– )) ,<label>(2)</label></formula><p>The user interactions with a search engine can be recorded as click log D = {(x ğ‘– , ğ‘ ğ‘– , ğ‘˜ ğ‘– , ğ‘  ğ‘– )} ğ‘ ğ‘–=1 , where x ğ‘– , ğ‘ ğ‘– , ğ‘˜ ğ‘– , ğ‘  ğ‘– respectively denote the ğ‘–-th query-document pair's feature vector, whether the document being clicked, the rank position, and whether being selected. Ideally, we hope an unbiased ranking model could be estimated by maximizing the log-likelihood shown below:</p><formula xml:id="formula_2">L unbiased = ğ‘ âˆ‘ï¸ ğ‘–=1 log (Pr(ğ‘Ÿ ğ‘– |x ğ‘– )) ,<label>(3)</label></formula><p>where the ğ‘Ÿ ğ‘– is the unobserved relevance for query-document pairs encoded by x ğ‘– . Equation (3) cannot be maximized because ğ‘Ÿ ğ‘– cannot be observed directly.  On the other hand, we observed that the click log consists of two parts: D = D ğ‘  D ğ‘¢ , where D ğ‘  = {(x ğ‘– , ğ‘ ğ‘– , ğ‘˜ ğ‘– , ğ‘  ğ‘– = 1)} ğ‘ ğ‘  ğ‘–=1 are the interactions for the ğ‘ ğ‘  selected (ğ‘, ğ‘‘) pairs, and</p><formula xml:id="formula_3">D ğ‘¢ = {(x ğ‘– , ğ‘ ğ‘– = 0, ğ‘˜ ğ‘– , ğ‘  ğ‘– = 0)} ğ‘ ğ‘¢</formula><p>ğ‘–=1 are those for the ğ‘ ğ‘¢ not selected pairs. A naive log-likelihood can be written as:</p><formula xml:id="formula_4">L naive = ğ‘ ğ‘  âˆ‘ï¸ ğ‘–=1 log (Pr(ğ‘ ğ‘– |ğ‘  ğ‘– = 1, x ğ‘– )Pr(ğ‘  ğ‘– = 1|x ğ‘– )) + ğ‘ ğ‘¢ âˆ‘ï¸ ğ‘–=1 log (Pr(ğ‘  ğ‘– = 0|x ğ‘– )) .<label>(4)</label></formula><p>Though it can be optimized, the naive log-likelihood suffers from both position bias from ğ‘ ğ‘– and the sample selection bias from ğ‘  ğ‘– . There exists a large gap between the naive Equation ( <ref type="formula" target="#formula_4">4</ref>) and the ideal unbiased objective Equation (3). Table <ref type="table" target="#tab_0">1</ref> lists the major notations in the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Causal view of the biases in top-ğ‘˜ ranking</head><p>Next, we will illustrate why ranking models are biased if they are trained with user clicks directly, and reveal the distinctions and association between the position bias and sample selection bias. In order to accomplish this, we introduce a causal graph representing the observed log data, depicted in Figure <ref type="figure" target="#fig_0">1</ref>. This causal graph comprises six causal variables denoted as {x, ğ‘ƒ, ğ‘…, ğ‘†, ğ¸, ğ¶} which have been defined in Table <ref type="table" target="#tab_0">1</ref>. The graph's edges describe causal relations between variables:</p><p>â€¢ x â†’ ğ‘…: this edges represents the causal relations of the query-document pair's feature x and their corresponding relevance score ğ‘…, which is the effect that an unbiased ranking model needs to estimate. â€¢ x â†’ ğ‘ƒ: the displayed position ğ‘ƒ is determined by a logging policy ğœ‹ 0 (x) which takes the feature x as the input. â€¢ ğ‘ƒ â†’ ğ¸: the chance of an item examined by users is determined by its displayed position ğ‘ƒ, note that the variable ğ¸ is unobserved. â€¢ (x, ğ‘…) â†’ ğ‘†: the rank position used to select is determined by the predicted score of logging policy ğœ‹ 0 , and ğœ‹ 0 is trained by x and ğ‘… 1 , thus the selection ğ‘† is indirectly determined by x and ğ‘…. The intermediate factors are omitted for simplifying the illustration. â€¢ (ğ¸, ğ‘…) â†’ ğ¶: the click ğ¶ on a query-document pair will be impacted by both the examination ğ¸ and the perceived relevance ğ‘… of a user, which is followed by the examination hypothesis <ref type="bibr" target="#b33">[34]</ref> Drawing upon causal inference principles <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31]</ref>, we identify two types of biases present in our causal graph, aligning with the previously mentioned position bias and sample selection bias:</p><p>â€¢ ğ¸ â† x â†’ ğ¶ (confounder bias): The feature vector x of querydocument pair is the confounder of examination ğ¸ and click ğ¶, which leads to a spurious correlation between ğ¸ and ğ¶.</p><p>Since ğ¸ is determined by rank position, the confounder bias is presented as position bias in top-ğ‘˜ ranking. â€¢ x â†’ ğ‘† â† ğ‘… (collider bias): ğ‘† is the collider <ref type="bibr" target="#b29">[30]</ref> between x and ğ‘…. The backdoor path between x and ğ‘… will be opened when conditioned on ğ‘†. The collider bias manifests as sample selection bias in top-ğ‘˜ ranking <ref type="bibr" target="#b10">[11]</ref>.</p><p>It is worth noting that some studies also discuss the trust bias <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b36">37]</ref>, which introduces an additional causal relation ğ‘ƒ â†’ ğ¶. However, trust bias can also be considered a form of confounder bias, sharing a similar mechanism with position bias. Since our primary focus is on addressing both confounder bias and collider bias, we have chosen to omit the trust bias in this study. From the analysis presented above, it becomes evident that position bias and sample selection bias in top-ğ‘˜ ranking are distinct biases that require separate approaches for mitigation. Meanwhile, the position bias and sample selection bias also have association according to Figure <ref type="figure" target="#fig_0">1</ref>. Note that the existance of causal relations ğ‘… â†’ ğ¶ bridge the association of the path of position bias (ğ¸ â† x â†’ ğ¶) and sample selection bias (x â†’ ğ‘† â† ğ‘…). This implies that the relevance R initially experiences sample selection bias, which subsequently amplifies the spurious correlation introduced by position bias through ğ‘… â†’ ğ¶. As a result, ğ¶ becomes influenced by both biases simultaneously. To provide further clarity, the selected click probability Pr(ğ¶ = 1 | ğ‘† = 1, x) (a component of Equation (4), which is abbreviated as ğ‘ ğ‘†=1 ğ¶ in the following equation) 1 In real search practices, a small number of the human label can be utilized to train ğœ‹ 0 .</p><p>can be decomposed as follows:</p><formula xml:id="formula_5">ğ‘ ğ‘†=1 ğ¶ = âˆ‘ï¸ ğ¸ âˆ‘ï¸ ğ‘… Pr(ğ¶ = 1|ğ¸, ğ‘…)Pr(ğ‘…|ğ‘† = 1, x) ğ¾ âˆ‘ï¸ ğ‘–=1 Pr(ğ¸|ğ‘ƒ = ğ‘–)Pr(ğ‘ƒ = ğ‘– |x) = âˆ‘ï¸ ğ¸ âˆ‘ï¸ ğ‘… Pr(ğ¶ = 1|ğ¸, ğ‘…)Pr(ğ‘…|ğ‘† = 1, x)Pr(ğ¸|ğ‘ƒ = ğ‘˜) = âˆ‘ï¸ ğ‘… Pr(ğ¶ = 1|ğ¸ = 1, ğ‘…)Pr(ğ‘…|ğ‘† = 1, x)Pr(ğ¸ = 1|ğ‘ƒ = ğ‘˜) = Pr(ğ¸ = 1|ğ‘ƒ = ğ‘˜) Position Bias E ğ‘…âˆ¼Pr(ğ‘… |ğ‘†=1,x) [ğœ (ğ‘…)] Selection biased relevance ,<label>(5)</label></formula><p>where ğ‘˜ = ğœ‹ 0 (x). The first line is an expansion based on Figure <ref type="figure" target="#fig_0">1</ref>. The second line is based on the assumption that ğœ‹ 0 is a deterministic ranking policy, meaning that one document in a query only has one corresponding rank position. The third line is according to the examination hypothesis <ref type="bibr" target="#b33">[34]</ref>. In the last line, we treat the click probability Pr(ğ¶ = 1|ğ¸ = 1, ğ‘…) as a function of relevance ğ‘…, denote as ğœ (ğ‘…). Here ğœ (â€¢) is a monotonically increasing function for mapping ğ‘… to the interval 0 to 1 (e.g. Sigmoid function). This mapping serves as an indicator for relevance ğ‘…. The decomposition equation (Equation <ref type="formula" target="#formula_5">5</ref>) demonstrates that user-perceived relevance is initially impacted by sample selection bias and subsequently influenced by position bias, forming a cascade process. To obtain an unbiased relevance signal, it is essential to first eliminate the position bias in the click signal and then mitigate the sample selection bias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">UNIFIED BIAS MITIGATION IN TOP-ğ‘˜ RANKING</head><p>Based on the analysis in the above section, this section presents a model called Causal Likelihood Decomposition (CLD) which simultaneously mitigates the position bias and the sample selection bias in top-ğ‘˜ learning to rank.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Formulation of the log-likelihood</head><p>The analysis of Equation ( <ref type="formula" target="#formula_5">5</ref>) in Section 3.2 indicates that the click signals can be transformed to selection biased relevance with the help of examination propensity Pr(ğ¸ = 1|ğ‘ƒ = ğ‘˜):</p><formula xml:id="formula_6">E ğ‘…âˆ¼Pr(ğ‘… |ğ‘†=1,x) [ğœ (ğ‘…)] = Pr(ğ¶ = 1 | ğ‘† = 1, x) Pr(ğ¸ = 1|ğ‘ƒ = ğ‘˜) = E ğ¶ Pr(ğ¸ = 1|ğ‘ƒ = ğ‘˜)</formula><p>Therefore, the click in Equation ( <ref type="formula" target="#formula_4">4</ref>) can be replaced with the expectation of propensity re-weighted click, achieving a log-likelihood with position bias be detached:</p><formula xml:id="formula_7">L de. posi. = ğ‘ ğ‘  âˆ‘ï¸ ğ‘–=1âˆ§ğ‘  ğ‘– =1 log Pr E ğ‘ ğ‘– ğœŒ ğ‘– , ğ‘  ğ‘– x ğ‘– + ğ‘ ğ‘¢ âˆ‘ï¸ ğ‘–=1âˆ§ğ‘  ğ‘– =0 log (Pr(ğ‘  ğ‘– |x ğ‘– )) ,<label>(6</label></formula><p>) where E ğ‘ ğ‘– ğœŒ ğ‘– = E ğ¶ Pr(ğ¸=1|ğ‘ƒ =ğ‘˜ ) for simplifying the notations. Note that ğœŒ ğ‘– is the examination propensity score for the ğ‘–-th (ğ‘, ğ‘‘) pair. A number of studies have been proposed to estimate the weights <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b39">40]</ref>. In this paper, we treat them as known values.</p><p>Equation ( <ref type="formula" target="#formula_7">6</ref>) still suffers sample selection bias because the first term still contains ğ‘  ğ‘– . Therefore, the likelihood of selected biased data can be further decomposed to contain the unbiased learning </p><formula xml:id="formula_8">L de. both biases = ğ‘ ğ‘  âˆ‘ï¸ ğ‘–=1âˆ§ğ‘  ğ‘– =1 log Pr E ğ‘ ğ‘– ğœŒ ğ‘– x ğ‘– + ğ‘ ğ‘  âˆ‘ï¸ ğ‘–=1âˆ§ğ‘  ğ‘– =1 log Pr ğ‘  ğ‘– E ğ‘ ğ‘– ğœŒ ğ‘– , x ğ‘– + ğ‘ ğ‘¢ âˆ‘ï¸ ğ‘–=1âˆ§ğ‘  ğ‘– =0 log (Pr(ğ‘  ğ‘– |x ğ‘– )) ,<label>(7)</label></formula><p>Same as the naive likelihood in Equation (4), Equation ( <ref type="formula" target="#formula_9">8</ref>) is still the likelihood among all observed data. The difference is both position bias and sample selection bias have been detached, and the unbiased learning target has been decomposed as the first term of Equation <ref type="bibr" target="#b7">(8)</ref>.</p><formula xml:id="formula_9">L de. both biases = ğ‘ ğ‘  âˆ‘ï¸ ğ‘–=1âˆ§ğ‘  ğ‘– =1 log Pr E ğ‘ ğ‘– ğœŒ ğ‘– x ğ‘– + ğ‘ ğ‘¢ âˆ‘ï¸ ğ‘–=1âˆ§ğ‘  ğ‘– =0 log (Pr(ğ‘  ğ‘– |x ğ‘– )) ,<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Optimization</head><p>To optimize Equation (8), we followed the Type II Tobit model <ref type="bibr" target="#b5">[6]</ref>, which parameterized the likelihood in Equation ( <ref type="formula" target="#formula_9">8</ref>) under the linear and Gaussian assumptions. Specifically, assuming that both the selection model and the ranking model are linear:</p><formula xml:id="formula_10">ğ‘  ğ‘– = 0 if x ğ‘‡ ğ‘– ğ + ğœ– ğ‘– â‰¤ 0 1 if x ğ‘‡ ğ‘– ğ + ğœ– ğ‘– &gt; 0 , ğ‘Ÿ ğ‘– = x ğ‘‡ ğ‘– ğœ· + ğœ‡ ğ‘– if ğ‘  ğ‘– = 1 unobserved if ğ‘  ğ‘– = 0 ,</formula><p>where ğ‘  ğ‘– and ğ‘Ÿ ğ‘– are the selection indicator and the selection biased relevance of the ğ‘–-th (ğ‘, ğ‘‘), respectively, and both of them are calculated based on the feature vector x ğ‘– . ğ and ğœ· are the parameters of these two linear models. ğœ– ğ‘– and ğœ‡ ğ‘– are the I.I.D. noises that obey Gaussian distributions and their variances are assumed to be 1.</p><p>According to the derivations and conclusions in <ref type="bibr" target="#b5">[6]</ref>, the parameterized log-likelihood of Equation ( <ref type="formula" target="#formula_9">8</ref>) becomes:</p><formula xml:id="formula_11">L CLD (ğœ·, ğ) = - ğ‘ ğ‘  âˆ‘ï¸ ğ‘–=1âˆ§ğ‘  ğ‘– =1 E ğ‘ ğ‘– ğœŒ ğ‘– -x ğ‘‡ ğ‘– ğœ· 2 + ğ‘ ğ‘  âˆ‘ï¸ ğ‘–=1âˆ§ğ‘  ğ‘– =1 log Î¦ x ğ‘‡ ğ‘– ğ + ğ›¾ E ğ‘ ğ‘– ğœŒ ğ‘– -x ğ‘‡ ğ‘– ğœ· (1 -ğ›¾ 2 ) 1 2 + ğ‘ ğ‘¢ âˆ‘ï¸ ğ‘–=1âˆ§ğ‘  ğ‘– =0 log 1 -Î¦(x ğ‘‡ ğ‘– ğ) ,<label>(9)</label></formula><p>where Î¦ is the cumulative distribution function of a standard normal distribution, ğ›¾ is the correlation coefficient of the error terms of ğœ– ğ‘– and ğœ‡ ğ‘– , which indicates how the selection of a (ğ‘, ğ‘‘) pair related to its relevance. In our implementation, ğ›¾ is treated as a hyper parameter. Maximizing Equation ( <ref type="formula" target="#formula_11">9</ref>) achieves an unbiased estimation of ğœ· and ğ:</p><formula xml:id="formula_12">(ğœ· * , ğ * ) â† arg max ğœ·,ğ L CLD (ğœ·, ğ).</formula><p>Algorithm 1 shows the procedure of the CLD learning algorithm for learning unbiased relevance ranking model ğœ· (and the selection model ğ). The inputs to the algorithm are click log with feature, selection indicator, and propensity score re-weighted click signals.</p><p>After sampling a batch of data, the algorithm updates both models if this data record was selected, and only updates the selection model if it was not selected.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Online ranking</head><p>The outputs of the learning algorithm are the parameters of the ranking model ğœ· * and parameters of the selection model ğ * . Intuitively, the selection model is used to absorb the sample selection bias while the relevance model is used to obtain the unbiased estimation of relevance. Therefore, in online ranking, a (ğ‘, ğ‘‘) pair's ranking score is calculated as an unbiased estimation of relevance: r = âŸ¨ğœ™ (ğ‘, ğ‘‘), ğœ· * âŸ©.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXTENSION TO PAIRWISE NEURAL RANKING</head><p>The models learned by Algorithm 1 are limited to be linear and learned with a pointwise objective function. Previous studies have shown that the neural ranking models learned with a pairwise objective such as BPR <ref type="bibr" target="#b32">[33]</ref> usually achieve better results. In this section, we extend the proposed pointwise and linear CLD model to pairwise neural ranking, denoted as CLD pair .</p><p>To derive the pairwise format of CLD, we first give the unbiased log-likelihood in pairwise format:</p><formula xml:id="formula_13">L pair unbiased = âˆ‘ï¸ ğ‘Ÿ ğ‘– &gt;ğ‘Ÿ ğ‘— log Pr(ğ‘Ÿ ğ‘– &gt; ğ‘Ÿ ğ‘— |x ğ‘– , x ğ‘— ) . (<label>10</label></formula><formula xml:id="formula_14">)</formula><p>For a document ğ‘– and document ğ‘— in the ranking list of query ğ’’, the pairwise unbiased likelihood is consists of the relative order of their relevance comparisons. </p><p>where rğ‘– = E [ğ‘ ğ‘– /ğœŒ ğ‘– ] and r ğ‘— = E ğ‘ ğ‘— /ğœŒ ğ‘— . Since E [ğ‘ ğ‘– /ğœŒ ğ‘– ] = ğ‘Ÿ ğ‘– , the first term of Equation ( <ref type="formula" target="#formula_15">11</ref>) implies an unbiased log-likelihood.</p><p>Maximizing Equation ( <ref type="formula" target="#formula_15">11</ref>) can obtain an unbiased ranking model.</p><p>To conduct the optimization, we first parameterize the models with neural networks, as shown in Figure <ref type="figure" target="#fig_2">2</ref>. Given a (ğ‘, ğ‘‘) pair, its representation is denoted as x. Based on the representation, the relevance ranking model and selection model are defined as feedforward neural networks, denoted as ğ‘“ ğœ· (â€¢) and ğ‘“ ğ (â€¢), respectively. Furthermore, we assume that in the second and third terms of Equation <ref type="bibr" target="#b10">(11)</ref>, the selection of document pairs (x ğ‘– , x ğ‘— ) are independent:</p><formula xml:id="formula_16">Pr(ğ‘  ğ‘– , ğ‘  ğ‘— |r ğ‘– &gt; r ğ‘— , x ğ‘– , x ğ‘— ) = Pr(ğ‘  ğ‘– |r ğ‘– &gt; r ğ‘— , x ğ‘– )Pr(ğ‘  ğ‘— |r ğ‘– &gt; r ğ‘— , x ğ‘— ); Pr(ğ‘  ğ‘– , ğ‘  ğ‘— |x ğ‘– , x ğ‘— ) = Pr(ğ‘  ğ‘– |x ğ‘– )Pr(ğ‘  ğ‘— |x ğ‘— ).</formula><p>As shown in Figure <ref type="figure" target="#fig_2">2</ref>(a), if both of the documents in a pair are selected into the top-ğ‘˜ positions, the likelihood of relevance part and conditional selection part can be formulated with BPR loss and Binary Cross Entropy loss, respectively. if only one document in a pair is selected, the conditional selection likelihood can be approximated as that shown in Figure <ref type="figure" target="#fig_2">2(b)</ref>. If neither of the two documents in a pair is selected, the selection likelihood can be formulated with Binary Cross Entropy loss directly (Figure <ref type="figure" target="#fig_2">2(c)</ref>). Therefore, the overall pairwise objective function becomes:</p><formula xml:id="formula_17">O pair CLD (ğœ·, ğ) = ğ‘  ğ‘– ğ‘  ğ‘— log ğœ ğ‘“ ğœ· (x ğ‘– ) -ğ‘“ ğœ· (x ğ‘— ) + ğ‘  ğ‘– log ğœ ğ‘“ ğ (x ğ‘– ) + ğ‘“ ğœ· (x ğ‘– ) -ğ‘“ ğœ· (x ğ‘— ) + (1 -ğ‘  ğ‘– ) log ğœ (1 -ğ‘“ ğ (x ğ‘– )) + ğ‘  ğ‘— log ğœ ğ‘“ ğ (x ğ‘— ) + ğ‘“ ğœ· (x ğ‘– ) -ğ‘“ ğœ· (x ğ‘— ) + (1 -ğ‘  ğ‘— ) log ğœ 1 -ğ‘“ ğ (x ğ‘— ) ,<label>(12)</label></formula><p>where ğœ denotes the sigmoid function. To maximize Equation ( <ref type="formula" target="#formula_17">12</ref>), we can get the approximately unbiased estimation of ğœ· and ğ:</p><formula xml:id="formula_18">(ğœ· * , ğ * ) â† arg max ğœ·,ğ O pair CLD (ğœ·, ğ).</formula><p>Algorithm 2 illustrates the optimization procedure for Equation <ref type="bibr" target="#b11">(12)</ref>.</p><p>As for online ranking, given a (ğ‘, ğ‘‘) pair, its ranking score is calculated by the ranking model ğ‘“ ğœ· * : r = ğ‘“ ğœ· * (ğœ™ (ğ‘, ğ‘‘)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXPERIMENT SETUP</head><p>We conducted experiments to evaluate the proposed CLD and its extension CLD pair , by following the settings presented in the existing unbiased learning to rank studies <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b24">25]</ref>.</p><p>Datasets: Two widely used public datasets, YaHooC14B <ref type="bibr" target="#b7">[8]</ref> and WEB10K <ref type="bibr" target="#b31">[32]</ref>, were used in our experiments. YaHooC14B contains around 30,000 queries, each associated with averaged of 24 documents. Each query-document pair is depicted with a 700-dimension feature vector and five-grade relevance labels. WEB10K has 10,000 queries and each associated with about 125 documents. Each querydocument pair is depicted with a 136-dimension feature vector and a five-grade relevance label. Following the practices in <ref type="bibr" target="#b22">[23]</ref>, we converted the relevance label in both two datasets with ğ‘Ÿ = 1 for grades 3 and 4 and ğ‘Ÿ = 0 for the others. Only the set 1 of YaHooC14B and the first fold of WEB10K was used for training. Expert annotated labels in the test sets were used to evaluate the ranking accuracy.</p><p>Click simulation: Following the practices in <ref type="bibr" target="#b22">[23]</ref>, the users' interactions with search engines were simulated and got the clicks. First, 1% labeled data were randomly sampled from the dataset and used to train an SVM ğ‘Ÿğ‘ğ‘›ğ‘˜ <ref type="bibr" target="#b19">[20]</ref> as the production ranker. Then for each click session, a query was uniformly sampled and the ranking result was generated by the production ranker. To simulate users' click, the position based model (PBM) <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b33">34]</ref> was adopted in which (ğ¸ = 1 âˆ§ ğ‘… = 1) â‡’ ğ¶ = 1, a click occurs only when the document is examined and is relevant. For every (ğ‘, ğ‘‘) pair, the examination probability is based on the displayed position:</p><formula xml:id="formula_19">Pr(ğ¸ = 1|ğ‘ƒ = ğ‘˜) = 1 ğ‘˜ ğœ‚ , if ğ‘˜ â‰¤ ğ¾ 0, else<label>(13)</label></formula><p>where ğœ‚ is the parameter to control the severity of position bias, and ğ¾ is the cut-off position. The examination probability is also the propensity score in the proposed approach and we assume it is known in advance. During the process, the irrelevant documents were allowed to be clicked with a small probability to simulate the click noise.</p><p>Baselines: State-of-the-art unbiased learning to rank approaches were adopted as the baselines:</p><p>Naive : Directly regarding the clicks as relevance labels. IPS <ref type="bibr" target="#b22">[23]</ref> : Correcting the position bias with propensity score. Heckman rank <ref type="bibr" target="#b27">[28]</ref> : Correcting the sample selection bias with Heckman two-stage method. RankAgg <ref type="bibr" target="#b27">[28]</ref> : Mitigating both the position bias and sample selection bias by combining the results of IPS and Heckman rank . Oracle : Using the non-discarded expert annotated labels to learn the ranking model. It showed the (theoretical) performance upper bound on the dataset.</p><p>Policy-aware IPS <ref type="bibr" target="#b24">[25]</ref> was not chosen as a baseline because it assumes the previous ranking models should be stochastic, which violets the Assumption (1). Evaluation metric: NDCG@1, NDCG@3, and MAP were used to evaluate the accuracy of the baselines and the proposed method.</p><p>Implementation details: Similar to existing studies <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b36">37]</ref>, we used a three layers neural networks with ğ‘’ğ‘™ğ‘¢ activation function as the ranking model for Naive, IPS, Oracle and CLD pair , with the hidden sizes [256, 128, 64], and dropout probability of 0.5. For Heckman rank and CLD (pointwise and linear), the ranking model was set to linear. The selection models for CLD and CLD pair were also set to linear. The learning rate were tuned among {2ğ‘’-4, 5ğ‘’-4, 1ğ‘’-3, 2ğ‘’-3, 5ğ‘’-3}. The ğ¿2 regularization was used and the trade-off factor was tuned between [1ğ‘’-3, 1ğ‘’-2]. The correlation ğ›¾ in Equation ( <ref type="formula" target="#formula_11">9</ref>) was tuned between [0.05, 0.30]. In all of the experiments, the reported numbers were the averaged results after training 12 epochs with 5 different random seeds.</p><p>The source code, data, and experiments will be available at <ref type="url" target="https://github.com/hyz20/CLD.git">https: //github.com/hyz20/CLD.git</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">RESULTS AND DISCUSSIONS</head><p>Table <ref type="table">2</ref> shows the ranking accuracy of our approaches and the baselines, on YaHooC14B and WEB10K. The results showed that the proposed CLD and CLD pair outperformed the baselines in terms of NDCG and MAP. "Oracle" is the upper bound of the performance, since it uses expert annotated labels. The results verified the effectiveness of the unified bias mitigation in top-k ranking.</p><p>To further reveal how CLD and CLD pair outperformed the baselines, we conducted a group of exploratory experiments to answer the following research questions:</p><p>Table <ref type="table">2</ref>: Ranking accuracy on YaHooC14B and WEB10K. Boldface means the best performed approaches (excluding Oracle). Experimental settings: top-5 cut-off, ğœ‚ = 0.1, 10 5 click sessions, and 10% click noise. We also present the 90% confidence interval of ğ‘¡-distribution for our methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>YaHooC14B WEB10K NDCG@1 NDCG@3 MAP NDCG@1 NDCG@ </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">The effect of biases severity (RQ1)</head><p>To varying the severity levels of sample selection bias, we changed the ranking cut-off position ğ‘˜ from 2 to 20. Smaller ğ‘˜ leads to more severe sample selection bias. The left two sub-figures of Figure <ref type="bibr" target="#b2">(3)</ref> show the performance curves of different approaches w.r.t. different ğ‘˜ values. From the results, we can see that in general CLD and CLD pair outperformed the baselines at all of the ğ‘˜ values (except CLD when ğ‘˜ &gt; 10 on WEB10K). On both datasets, when ğ‘˜ was small , CLD and CLD pair outperformed the baselines with a large margin and achieved the performance closing to the upper bound. With the increasing of ğ‘˜, the improvements of CLD and CLD pair over IPS gradually become limited. This is because sample selection bias gets milder for larger ğ‘˜, making position bias dominates the negative effects of bias. Similar performance curves also came to RankAgg, another model which can mitigate both position bias and sample selection bias.</p><p>On the contrary, increasing ğ‘˜ will lead to the performance drop of naive methods. This is because the naive method can handle neither of these two biases. Increasing data will not further improve its performance but decrease its performance instead. Also note that CLD pair outperformed CLD on WEB10K since it learns a neural ranking model based on the pairwise loss. However, all methods can achieve relatively high performances on YaHooC14B. The spaces for further improvement are limited, leading to similar performances for CLD pair and CLD.</p><p>To change the severity levels of position bias, we tuned the the parameter ğœ‚ in Equation ( <ref type="formula" target="#formula_19">13</ref>) from 0.0 to 2.0. Larger ğœ‚ leads to more severe position bias. The right two sub-figures in Figure <ref type="figure">3</ref> illustrate the performances curves w.r.t. different ğœ‚ values. From the results, we can see that CLD and CLD pair still outperformed all the baselines on both datasets. With the increasing of ğœ‚, the methods that can correct position bias (except Heckman rank and Naive) have slight performance drops. Among the baselines, Heckman rank achieved the higher performance when ğœ‚ = 0 (no position bias), but dropped rapidly when ğœ‚ increases. RankAgg also suffered from the </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">The effects of click scales (RQ2)</head><p>We tested the performances of different methods by varying the scale of the click data. Figure <ref type="figure">4</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">The effects of click noises (RQ3)</head><p>We also conducted experiments with variant noise levels in the clicks, by changing the probability of clicking irrelevant documents from 0.0 to 0.5 when generating the training data. According to the results shown in Figure <ref type="figure">5</ref>, both CLD and CLD pair outperformed the baselines at different noise levels, indicating the robustness of the unified bias mitigation approach. Particularly, among all of the methods, CLD has minimal performance drops. We analyzed the reasons and found that each noise click will produce more mistake pairs in pairwise methods than that of in pointwise methods. Therefore, when training with these mistake pairs, pairwise method will be suffered more. However, for pointwise method, each noise click is only presented once in the training set, making it more robust than the pairwise models. The result we reported before assumes that the model knows the true propensity score, which is often difficult in the real world. In this experiment, we conducted experiments to test the performance of each method under various degrees on misspecified propensity scores and different top-ğ‘˜ cut-offs, characterized by parameters ğœ‚ and ğ‘˜, respectively. The true value ğœ‚ = 1 and we varied it in [0.0, 2.0]. We tested the cases when ğ‘˜ = 3 and ğ‘˜ = 5. Note that Heckman rank are not considered as a baseline in this experiment. This is because Heckman rank does not use propensity scores. Figure <ref type="figure">6</ref> illustrates the performance curves of CLD, CLD ğ‘ğ‘ğ‘–ğ‘Ÿ , IPS, and RankAgg on YaHooC14B, under various degrees of misspecified propensity scores. The left and right figures respectively illustrate the results when ğ‘˜ = 3 and ğ‘˜ = 5. From the results, we can see that in general CLD outperformed the best in all degrees of misspecified propensity scores, which indicates the robustness of CLD. When the propensity was overestimated (i.e., ğœ‚ &lt; 1), all methods related to propensity score only have a slight performance drop. However, all methods have violent performance drops if the propensity was underestimated (i.e., ğœ‚ &gt; 1). This is because when the propensity is underestimated, the estimated propensity becomes smaller than its true value, and thus increasing the variance of propensity re-weighting. Even when the propensity was underestimated, the proposed CLD still outperformed other methods with large margins.This is attributed to CLD avoids dividing by propensity score in the whole loss function and therefore can reduce the variance caused by the underestimation of the propensity. Moreover, we found that the effects of misspecified propensity scores were more severe on large ğ‘˜. This is because the larger the ranking positions, the more suffers come from the position bias.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5">The effects of base model (RQ5)</head><p>In previous experiments, CLD was designed as a linear ranking model because of its theoretic grantees, while CLD pair was designed to use nonlinear neural networks as its ranker. In this experiment, we modified these models so that CLD was based on a neural network with three hidden layers and CLD pair used a linear model as the ranker, denoted as CLD-N and CLD pair -L, respectively.</p><p>Table <ref type="table" target="#tab_5">3</ref> reports the ranking accuracy of CLD, CLD pair , and their variations, on YaHooC14B and WEB10K. From the results, we found that (1) CLD-N performed worst among these methods, especially on WEB10K. Compared to CLD, CLD-N used a nonlinear neural network as its ranker, which makes it lose the theoretical guarantees;</p><p>(2) CLD pair outperformed CLD pair -L on WEB10K and performed comparably on YaHooC14B. Please note that WEB10K is larger than YaHooC14B and all methods can achieve relatively scores on YaHooC14B. We concluded that using a linear model in CLD and using nonlinear neural networks in CLD pair are reasonable settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSIONS</head><p>In this paper, we have proposed a novel and theoretical sound model for learning unbiased ranking models in top-ğ‘˜ learning to rank, referred to as CLD. In contrast to existing methods, CLD simultaneously tackles the position bias and sampling selection biases from the viewpoint of a causal graph. It decomposes the log-likelihood function of user interactions as an unbiased relevance term plus other terms that model the biases. An unbiased ranking model can be obtained by maximizing the whole log-likelihood. Extension to the pairwise neural ranking is also developed. Experimental results verified the superiority of the proposed methods over the baselines in terms of ranking accuracy and robustness.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: The casual graphs of observed log data. Each node corresponds a casual variable and the gray node means that the variable is unobserved. The red arrow (x â†’ ğ‘…) denote the effect that an unbiased ranking model needs to estimate.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Algorithm 1 : 6 ğœŒ 7 if ğ‘  ğ‘– = 1 then 8 Update 9 else 10 Update</head><label>1678910</label><figDesc>The training procedure of CLD Input: iteration number ğ‘‡ , click log D = D ğ‘  D ğ‘¢ Output: Model parameters ğœ· and ğ 1 ğ† â† estimate ğ¾ propensity scores; 2 ğœ·, ğ â† Xavier initialization[15]; 3 for 1 â‰¤ ğ‘¡ â‰¤ ğ‘‡ do 4 Randomly sample a batch of sessions D â€² from D ğ‘  âˆª D ğ‘¢ ; 5 for (x ğ‘– , ğ‘ ğ‘– , ğ‘˜ ğ‘– , ğ‘  ğ‘– ) âˆˆ D â€² do ğ‘– â† ğ† [ğ‘˜ ğ‘– ] ğœ·, ğ with the gradient of Eq. (9) ; ğ with the gradient of Eq. (9); return ğœ·, ğ target, thus detaching sample selection bias:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The unified debiasing model structure for optimizing pairwise neural format of CLD.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Notations and explanations.</figDesc><table><row><cell>Notation</cell><cell>Description</cell></row></table><note><p>(ğ‘, ğ‘‘) a query-document pair x = ğœ™ (ğ‘, ğ‘‘) feature vector in R ğ‘› corresponding to a (ğ‘, ğ‘‘) pair ğ‘…, ğ‘Ÿ ğ‘– true relevance of a (ğ‘, ğ‘‘) pair (unobserved) ğ¸ user's examination on a document (unobserved) ğ¶, ğ‘ ğ‘– click on a document (can be observed) ğ‘ƒ, ğ‘˜ ğ‘– position of a document being displayed (observed) ğ‘†, ğ‘  ğ‘– whether a document being selected (observed) D = D ğ‘  D ğ‘¢ click log for selected and not selected (ğ‘, ğ‘‘) pairs ğœŒ ğ‘– the propensity score of the ğ‘–-th (ğ‘, ğ‘‘) pair ğœ‹ 0 logging policy (an existing ranking model)</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Pairwise training for CLD Input: iteration number ğ‘‡ , click log D = D ğ‘  D ğ‘¢ Output: Model parameters ğœ·, ğ // Create preference pairs based on D; ğ‘– , ğ‘  ğ‘– ), (x ğ‘— , ğ‘  ğ‘— ) E ğ‘ ğ‘– ğ† [ğ‘˜ ğ‘– ] &gt; E ğ‘– , ğ‘  ğ‘– ), (x ğ‘— , ğ‘  ğ‘— ) ğ‘  ğ‘– = 0 âˆ¨ ğ‘  ğ‘— = 0 ; 4 ğœ·, ğ â† Xavier initialization[15]; 5 for 1 â‰¤ ğ‘¡ â‰¤ ğ‘‡ do ğ‘– , ğ‘  ğ‘– ), (x ğ‘— , ğ‘  ğ‘— ) âˆˆ D âˆ‘ï¸ rğ‘– &gt;r ğ‘— , ğ‘  ğ‘– =1âˆ§ğ‘  ğ‘— =1 log Pr(r ğ‘– &gt; r ğ‘— |x ğ‘– , x ğ‘— )</figDesc><table><row><cell>6</cell><cell cols="3">Randomly sample a batch D</cell><cell>â€²</cell><cell>from D ğ‘  ğ‘ğ‘ğ‘–ğ‘Ÿ</cell><cell>âˆª D ğ‘¢ ğ‘ğ‘ğ‘–ğ‘Ÿ</cell><cell>;</cell></row><row><cell>7</cell><cell cols="2">for (x â€²</cell><cell cols="2">do</cell><cell></cell></row><row><cell>8</cell><cell cols="2">if ğ‘  ğ‘– = 1 âˆ¨ ğ‘  ğ‘— = 1 then</cell><cell></cell><cell></cell><cell></cell></row><row><cell>9</cell><cell cols="6">Update ğœ·, ğ with the gradient of Eq. (12) ;</cell></row><row><cell>10</cell><cell>else</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>11</cell><cell cols="6">Update ğ with the gradient of Eq. (12);</cell></row><row><cell>12</cell><cell>end</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>13</cell><cell>end</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">14 end</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">15 return ğœ·, ğ</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">of each document, Based on Equation (8) and Equation (10), the</cell></row><row><cell cols="7">decomposed log-likelihood in pairwise can be written as</cell></row><row><cell>L</cell><cell>pair de. both biases =</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>+</cell><cell>âˆ‘ï¸</cell><cell></cell><cell></cell><cell></cell></row></table><note><p>Unfortunately, the relevance ğ‘Ÿ ğ‘– and ğ‘Ÿ ğ‘— is unknown for us. What we can observe is the click signal Algorithm 2: 1 ğ† â† estimate ğ¾ propensity scores; 2 D ğ‘ğ‘ğ‘–ğ‘Ÿ ğ‘  â† (x ğ‘ ğ‘— ğ† [ğ‘˜ ğ‘— ] , ğ‘  ğ‘– = 1 âˆ§ ğ‘  ğ‘— = 1 ; 3 D ğ‘ğ‘ğ‘–ğ‘Ÿ ğ‘¢ â† (x rğ‘– &gt;r ğ‘— , ğ‘  ğ‘– =1âˆ§ğ‘  ğ‘— =1 log Pr(ğ‘  ğ‘– , ğ‘  ğ‘— |r ğ‘– &gt; r ğ‘— , x ğ‘– , x ğ‘— ) + âˆ‘ï¸ ğ‘  ğ‘– =0âˆ¨ğ‘  ğ‘— =0 log Pr(ğ‘  ğ‘– , ğ‘  ğ‘— |x ğ‘– , x ğ‘— ) ,</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>.002 0.631 Â± .001 0.616 Â± .001 0.434 Â± .005 0.391 Â± .003 0.312 Â± .001 CLD pair 0.662 Â± .001 0.630 Â± .001 0.615 Â± .001 0.439 Â± .001 0.397 Â± .001 0.312 Â± .000 RQ2 How does CLD perform under different scales of click data? RQ3 Is CLD robust to click noise? RQ4 Is CLD robust to misspecified propensity score? RQ5 How does CLD perform with different base model?</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>3</cell><cell>MAP</cell></row><row><cell>Naive</cell><cell>0.606</cell><cell>0.593</cell><cell>0.592</cell><cell>0.383</cell><cell>0.350</cell><cell>0.294</cell></row><row><cell>IPS</cell><cell>0.650</cell><cell>0.619</cell><cell>0.609</cell><cell>0.413</cell><cell>0.368</cell><cell>0.282</cell></row><row><cell>Heckman rank</cell><cell>0.608</cell><cell>0.590</cell><cell>0.587</cell><cell>0.350</cell><cell>0.331</cell><cell>0.287</cell></row><row><cell>RankAgg</cell><cell>0.649</cell><cell>0.623</cell><cell>0.608</cell><cell>0.413</cell><cell>0.375</cell><cell>0.299</cell></row><row><cell cols="2">CLD 0.661 Â± Oracle 0.666</cell><cell>0.636</cell><cell>0.622</cell><cell>0.455</cell><cell>0.416</cell><cell>0.332</cell></row><row><cell cols="7">RQ1 How does CLD perform under different severity levels of</cell></row><row><cell cols="5">sample selection bias and position bias?</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>Performance curves of different methods w.r.t. bias severity levels. Experimental settings: 10 5 click sessions, and 10% click noise. Shaded area indicates the 90% confidence intervals of ğ‘¡-distribution. Left two figures: performance curves w.r.t. different severity of sampling selection bias. Right two figures: performance curves w.r.t. different severity of position bias. ğœ‚ = 1.0 and trained with 10 5 click sessions.performance drop with the increasing ğœ‚ because it is an ensemble of Heckman rank . The phenomenon confirmed the conclusion in Section 3.2: only mitigating one bias separately still leads to a biased result in top-ğ‘˜ ranking. Simply aggregating the results outputted by the methods that only correct one bias still leads to biased results.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">YaHooC14B</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>0.42</cell><cell></cell><cell cols="2">WEB10K</cell><cell></cell><cell></cell><cell></cell><cell>0.64</cell><cell></cell><cell></cell><cell>YaHooC14B</cell><cell></cell><cell></cell><cell></cell><cell>0.42</cell><cell></cell><cell></cell><cell>WEB10K</cell><cell></cell><cell>Oracle</cell></row><row><cell cols="2">NDCG@3</cell><cell>0.57 0.58 0.59 0.60 0.61 0.62 0.63</cell><cell>5</cell><cell>10</cell><cell>K</cell><cell>15</cell><cell>20</cell><cell cols="2">NDCG@3</cell><cell>0.32 0.34 0.36 0.40 0.38</cell><cell>5</cell><cell>10</cell><cell>K</cell><cell>15</cell><cell>20</cell><cell>NDCG@3</cell><cell>0.62 0.63 0.57 0.60 0.61 0.59 0.58</cell><cell>0.0</cell><cell>0.5</cell><cell>1.0 eta</cell><cell>1.5</cell><cell>2.0</cell><cell>NDCG@3</cell><cell>0.40 0.36 0.38 0.34 0.32</cell><cell>0.0</cell><cell>0.5</cell><cell>1.0 eta</cell><cell>1.5</cell><cell>2.0</cell><cell>Production Naive IPS Heckman rank RankAgg CLD pair CLD</cell></row><row><cell cols="9">0.62 0.63 Figure 3: 2.0e+04 4.0e+04 6.0e+04 8.0e+04 1.0e+05 YaHooC14B Sessions 0.59 0.60 0.61 NDCG@3 NDCG@3 0.58 0.57</cell><cell cols="2">0.38 0.40 0.42 0.30 0.34 0.36 0.32 0.28</cell><cell cols="4">WEB10K 5.0e+04 1.0e+05 1.5e+05 2.0e+05 Sessions</cell><cell></cell><cell cols="3">Oracle Production Naive IPS Heckman rank RankAgg CLD pair CLD</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="19">Figure 4: Performance curves of different methods w.r.t. the</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="19">number of click sessions. Experimental settings: top-5 cut-</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="11">off, ğœ‚ = 1.0 and 10% click noise.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">YaHooC14B</cell><cell></cell><cell></cell><cell></cell><cell cols="2">0.42</cell><cell></cell><cell cols="2">WEB10K</cell><cell></cell><cell></cell><cell cols="2">Oracle</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">0.63</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">0.40</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">Production</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">0.62</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">0.38</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Naive</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>NDCG@3</cell><cell cols="2">0.57 0.58 0.59 0.60 0.61</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">NDCG@3</cell><cell cols="2">0.28 0.34 0.36 0.32 0.30</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">IPS Heckman rank RankAgg CLD pair</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="6">0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 noise</cell><cell></cell><cell></cell><cell cols="5">0.00 0.05 0.10 0.15 0.20 0.25 0.30 0.35 0.40 noise</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p>CLD Figure 5: Performance curves of different methods w.r.t. click noise severity levels. Experimental settings: top-5 cut-off,</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>illustrates the performance curves of different methods w.r.t. the number of click sessions used for training the models. The results indicate that both CLD and CLD pair consistently outperformed the baseline methods over different click session scales. According to Equation (9) and (12), CLD and CLD pair have the ability of utilizing the unobserved data in training. The</figDesc><table><row><cell>top-k=3</cell><cell>top-k=5</cell><cell></cell></row><row><cell>0.64</cell><cell>0.64</cell><cell></cell></row><row><cell>0.62</cell><cell>0.62</cell><cell>Naive</cell></row><row><cell cols="3">0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 Assumed eta NDCG@3 CLD ability makes them perform well even being trained with a limited 0.50 0.52 0.54 0.56 0.58 0.60 NDCG@3 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 Assumed eta 0.50 0.52 0.56 0.58 0.60 IPS RankAgg CLD 0.54</cell></row><row><cell cols="3">number of click sessions. With more click sessions being involved in</cell></row><row><cell cols="3">training, the performances of CLD and CLD pair steadily improved.</cell></row><row><cell cols="3">In contrast, IPS and Heckman rank can only correct one bias in top-ğ‘˜</cell></row><row><cell cols="3">ranking. Therefore, with the increasing number of click sessions</cell></row><row><cell cols="3">used for training, they underperformed those methods that can</cell></row><row><cell cols="3">mitigate both position bias and sample selection bias (e.g., RankAgg,</cell></row><row><cell cols="3">CLD and CLD pair ). All these results clearly verified the advantages</cell></row><row><cell cols="2">of the proposed unified bias mitigation.</cell><cell></cell></row></table><note><p><p>pair</p>Figure 6: Performance curves of different methods on Ya-HooC14B w.r.t. degrees of misspecified propensity scores in different top-k cut-offs. The true ğœ‚ = 1 and click noise is 10%.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Ranking accuracy comparison among different variants of CLD on YaHooC14B and WEB10K. Boldface means the best performed approaches (excluding Oracle). Experimental settings: top-5 cut-off, ğœ‚ = 0.1, 10 5 click sessions, and 10% click noise.</figDesc><table><row><cell>Method</cell><cell cols="4">YaHooC14B NDCG@1 NDCG@3 MAP NDCG@1 NDCG@3 MAP WEB10K</cell></row><row><cell>CLD</cell><cell>0.661</cell><cell>0.631 0.616</cell><cell>0.434</cell><cell>0.391 0.312</cell></row><row><cell>CLD-N</cell><cell>0.652</cell><cell>0.619 0.610</cell><cell>0.340</cell><cell>0.321 0.284</cell></row><row><cell>CLD pair</cell><cell>0.662</cell><cell>0.630 0.615</cell><cell>0.439</cell><cell>0.397 0.312</cell></row><row><cell>CLD pair -L</cell><cell>0.660</cell><cell>0.634 0.618</cell><cell>0.431</cell><cell>0.389 0.309</cell></row><row><cell>Oracle</cell><cell>0.666</cell><cell>0.636 0.622</cell><cell>0.455</cell><cell>0.416 0.332</cell></row><row><cell cols="5">7.4 Effects of misspecified propensity score (RQ4)</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>This work was funded by the <rs type="funder">National Natural Science Foundation of China</rs> (No. <rs type="grantNumber">62376275</rs>), <rs type="programName">Beijing Outstanding Young Scientist Program</rs> NO. <rs type="grantNumber">BJJWZYJH012019100020098</rs>, the <rs type="funder">Fundamental Research Funds for the Central Universities</rs>, and the <rs type="funder">Research Funds of Renmin University of China</rs> (<rs type="grantNumber">23XNKJ13</rs>). This work was also supported by the <rs type="funder">Intelligent Social Governance Interdisciplinary Platform, Major Innovation &amp; Planning Interdisciplinary Platform for the "Double-First Class" Initiative, Renmin University of China</rs>. The work was partially done at <rs type="institution">Beijing Key Laboratory of Big Data Management and Analysis Methods</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_tKSwBqd">
					<idno type="grant-number">62376275</idno>
					<orgName type="program" subtype="full">Beijing Outstanding Young Scientist Program</orgName>
				</org>
				<org type="funding" xml:id="_QmfD2dP">
					<idno type="grant-number">BJJWZYJH012019100020098</idno>
				</org>
				<org type="funding" xml:id="_WRw2px7">
					<idno type="grant-number">23XNKJ13</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A General Framework for Counterfactual Learning-to-Rank</title>
		<author>
			<persName><forename type="first">Aman</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenta</forename><surname>Takatsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Zaitsev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
		<idno type="DOI">10.1145/3331184.3331202</idno>
		<ptr target="https://doi.org/10.1145/3331184.3331202" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Paris, France; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5" to="14" />
		</imprint>
	</monogr>
	<note>SIGIR&apos;19)</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Addressing Trust Bias for Unbiased Learning-to-Rank</title>
		<author>
			<persName><forename type="first">Aman</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bendersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Najork</surname></persName>
		</author>
		<idno type="DOI">10.1145/3308558.3313697</idno>
		<ptr target="https://doi.org/10.1145/3308558.3313697" />
	</analytic>
	<monogr>
		<title level="m">The World Wide Web Conference</title>
		<meeting><address><addrLine>San Francisco, CA, USA; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4" to="14" />
		</imprint>
	</monogr>
	<note>) (WWW &apos;19)</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Estimating Position Bias without Intrusive Interventions</title>
		<author>
			<persName><forename type="first">Aman</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Zaitsev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Najork</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
		<idno type="DOI">10.1145/3289600.3291017</idno>
		<ptr target="https://doi.org/10.1145/3289600.3291017" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth ACM International Conference on Web Search and Data Mining</title>
		<title level="s">WSDM &apos;19</title>
		<meeting>the Twelfth ACM International Conference on Web Search and Data Mining<address><addrLine>Melbourne VIC, Australia; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="474" to="482" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Unbiased Learning to Rank with Unbiased Propensity Estimation</title>
		<author>
			<persName><forename type="first">Qingyao</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keping</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Bruce</forename><surname>Croft</surname></persName>
		</author>
		<idno type="DOI">10.1145/3209978.3209986</idno>
		<ptr target="https://doi.org/10.1145/3209978.3209986" />
	</analytic>
	<monogr>
		<title level="m">The 41st International ACM SIGIR Conference on Research Development in Information Retrieval</title>
		<meeting><address><addrLine>Ann Arbor, MI, USA; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="385" to="394" />
		</imprint>
	</monogr>
	<note>SIGIR &apos;18)</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Unbiased Learning to Rank: Online or Offline?</title>
		<author>
			<persName><forename type="first">Qingyao</forename><surname>Ai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huazheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxin</forename><surname>Mao</surname></persName>
		</author>
		<idno type="DOI">10.1145/3439861</idno>
		<ptr target="https://doi.org/10.1145/3439861" />
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">29</biblScope>
			<date type="published" when="2021-02">2021. Feb. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Tobit models: A survey</title>
		<author>
			<persName><forename type="first">T</forename><surname>Amemiya</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Econometrics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="1" to="2" />
			<date type="published" when="1984">1984. 1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Offline Comparative Evaluation with Incremental, Minimally-Invasive Online Feedback</title>
		<author>
			<persName><forename type="first">Ben</forename><surname>Carterette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Praveen</forename><surname>Chandar</surname></persName>
		</author>
		<idno type="DOI">10.1145/3209978.3210050</idno>
		<ptr target="https://doi.org/10.1145/3209978.3210050" />
	</analytic>
	<monogr>
		<title level="m">The 41st International ACM SIGIR Conference on Research Development in Information Retrieval</title>
		<meeting><address><addrLine>Ann Arbor, MI, USA; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="705" to="714" />
		</imprint>
	</monogr>
	<note>SIGIR &apos;18)</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Yahoo! learning to rank challenge overview</title>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the learning to rank challenge</title>
		<meeting>the learning to rank challenge</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">AutoDebias: Learning to Debias for Recommendation</title>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hande</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guli</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keping</forename><surname>Yang</surname></persName>
		</author>
		<idno type="DOI">10.1145/3404835.3462919</idno>
		<ptr target="https://doi.org/10.1145/3404835.3462919" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval (Virtual Event, Canada) (SIGIR &apos;21)</title>
		<meeting>the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval (Virtual Event, Canada) (SIGIR &apos;21)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="21" to="30" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hande</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.03240</idno>
		<title level="m">Fuli Feng, Meng Wang, and Xiangnan He. 2020. Bias and debias in recommender system: A survey and future directions</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Illustrating bias due to conditioning on a collider</title>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">W</forename><surname>Stephen R Cole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enrique</forename><forename type="middle">F</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haitao</forename><surname>Schisterman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Westreich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of epidemiology</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="417" to="420" />
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An Experimental Comparison of Click Position-Bias Models</title>
		<author>
			<persName><forename type="first">Nick</forename><surname>Craswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Onno</forename><surname>Zoeter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Ramsey</surname></persName>
		</author>
		<idno type="DOI">10.1145/1341531.1341545</idno>
		<ptr target="https://doi.org/10.1145/1341531.1341545" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2008 International Conference on Web Search and Data Mining</title>
		<title level="s">WSDM &apos;08</title>
		<meeting>the 2008 International Conference on Web Search and Data Mining<address><addrLine>Palo Alto, California, USA; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="87" to="94" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Counterfactual learning for recommender system</title>
		<author>
			<persName><forename type="first">Zhenhua</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengxiang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinhua</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guohao</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiuqiang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jirong</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourteenth ACM Conference on Recommender Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="568" to="569" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Intervention Harvesting for Context-Dependent Examination-Bias Estimation</title>
		<author>
			<persName><forename type="first">Zhichong</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aman</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
		<idno type="DOI">10.1145/3331184.3331238</idno>
		<ptr target="https://doi.org/10.1145/3331184.3331238" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Paris, France; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="825" to="834" />
		</imprint>
	</monogr>
	<note>SIGIR&apos;19)</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the thirteenth international conference on artificial intelligence and statistics</title>
		<meeting>the thirteenth international conference on artificial intelligence and statistics</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
	<note>JMLR Workshop and Conference Proceedings</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">The common structure of statistical models of truncation, sample selection and limited dependent variables and a simple estimator for such models</title>
		<author>
			<persName><forename type="first">J</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName><surname>Heckman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Annals of economic and social measurement</title>
		<imprint>
			<date type="published" when="1976">1976</date>
			<biblScope unit="page" from="475" to="492" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Sample selection bias as a specification error. Econometrica</title>
		<author>
			<persName><forename type="first">J</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName><surname>Heckman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the econometric society</title>
		<imprint>
			<biblScope unit="page" from="153" to="161" />
			<date type="published" when="1979">1979. 1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unbiased LambdaMART: An Unbiased Pairwise Learning-to-Rank Algorithm</title>
		<author>
			<persName><forename type="first">Ziniu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qu</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1145/3308558.3313447</idno>
		<ptr target="https://doi.org/10.1145/3308558.3313447" />
	</analytic>
	<monogr>
		<title level="m">The World Wide Web Conference</title>
		<meeting><address><addrLine>San Francisco, CA, USA; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2830" to="2836" />
		</imprint>
	</monogr>
	<note>) (WWW &apos;19)</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">To Model or to Intervene: A Comparison of Counterfactual and Online Learning to Rank from User Interactions</title>
		<author>
			<persName><forename type="first">Rolf</forename><surname>Jagerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harrie</forename><surname>Oosterhuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
		</author>
		<idno type="DOI">10.1145/3331184.3331269</idno>
		<ptr target="https://doi.org/10.1145/3331184.3331269" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Paris, France; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="15" to="24" />
		</imprint>
	</monogr>
	<note>SIGIR&apos;19)</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Optimizing Search Engines Using Clickthrough Data</title>
		<author>
			<persName><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
		<idno type="DOI">10.1145/775047.775067</idno>
		<ptr target="https://doi.org/10.1145/775047.775067" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining<address><addrLine>Edmonton, Alberta, Canada; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="133" to="142" />
		</imprint>
	</monogr>
	<note>KDD &apos;02)</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Accurately Interpreting Clickthrough Data as Implicit Feedback</title>
		<author>
			<persName><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><surname>Granka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Helene</forename><surname>Hembrooke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geri</forename><surname>Gay</surname></persName>
		</author>
		<idno type="DOI">10.1145/1076034.1076063</idno>
		<ptr target="https://doi.org/10.1145/1076034.1076063" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 28th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Salvador, Brazil; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="154" to="161" />
		</imprint>
	</monogr>
	<note>SIGIR &apos;05)</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Evaluating the Accuracy of Implicit Feedback from Clicks and Query Reformulations in Web Search</title>
		<author>
			<persName><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><surname>Granka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Helene</forename><surname>Hembrooke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Filip</forename><surname>Radlinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geri</forename><surname>Gay</surname></persName>
		</author>
		<idno type="DOI">10.1145/1229179.1229181</idno>
		<ptr target="https://doi.org/10.1145/1229179.1229181" />
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Unbiased Learning-to-Rank with Biased Feedback</title>
		<author>
			<persName><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adith</forename><surname>Swaminathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Schnabel</surname></persName>
		</author>
		<idno type="DOI">10.1145/3018661.3018699</idno>
		<ptr target="https://doi.org/10.1145/3018661.3018699" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth ACM International Conference on Web Search and Data Mining</title>
		<title level="s">WSDM &apos;17</title>
		<meeting>the Tenth ACM International Conference on Web Search and Data Mining<address><addrLine>Cambridge, United Kingdom; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="781" to="789" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Top-k Learning to Rank: Labeling, Ranking and Evaluation</title>
		<author>
			<persName><forename type="first">Shuzi</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiafeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueqi</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="DOI">10.1145/2348283.2348384</idno>
		<ptr target="https://doi.org/10.1145/2348283.2348384" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 35th International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Portland, Oregon, USA; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="751" to="760" />
		</imprint>
	</monogr>
	<note>SIGIR &apos;12)</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Policy-Aware Unbiased Learning to Rank for Top-k Rankings</title>
		<author>
			<persName><forename type="first">Harrie</forename><surname>Oosterhuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
		</author>
		<idno type="DOI">10.1145/3397271.3401102</idno>
		<ptr target="https://doi.org/10.1145/3397271.3401102" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (Virtual Event, China) (SIGIR &apos;20)</title>
		<meeting>the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (Virtual Event, China) (SIGIR &apos;20)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="489" to="498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Unifying Online and Counterfactual Learning to Rank: A Novel Counterfactual Estimator That Effectively Utilizes Online Interventions</title>
		<author>
			<persName><forename type="first">Harrie</forename><surname>Oosterhuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
		</author>
		<idno type="DOI">10.1145/3437963.3441794</idno>
		<ptr target="https://doi.org/10.1145/3437963.3441794" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM International Conference on Web Search and Data Mining (Virtual Event, Israel) (WSDM &apos;21)</title>
		<meeting>the 14th ACM International Conference on Web Search and Data Mining (Virtual Event, Israel) (WSDM &apos;21)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="463" to="471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Robust Generalization and Safe Query-Specializationin Counterfactual Learning to Rank</title>
		<author>
			<persName><forename type="first">Harrie</forename><surname>Oosterhuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>De De Rijke</surname></persName>
		</author>
		<idno type="DOI">10.1145/3442381.3450018</idno>
		<ptr target="https://doi.org/10.1145/3442381.3450018" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Web Conference 2021</title>
		<meeting>the Web Conference 2021<address><addrLine>Ljubljana, Slovenia; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="158" to="170" />
		</imprint>
	</monogr>
	<note>) (WWW &apos;21)</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Correcting for Selection Bias in Learning-to-Rank Systems</title>
		<author>
			<persName><forename type="first">Zohreh</forename><surname>Ovaisi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ragib</forename><surname>Ahsan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathryn</forename><surname>Vasilaky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Zheleva</surname></persName>
		</author>
		<idno type="DOI">10.1145/3366423.3380255</idno>
		<ptr target="https://doi.org/10.1145/3366423.3380255" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Web Conference</title>
		<meeting>The Web Conference<address><addrLine>Taipei, Taiwan; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="1863" to="1873" />
		</imprint>
	</monogr>
	<note>) (WWW &apos;20)</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Propensity-Independent Bias Recovery in Offline Learning-to-Rank Systems</title>
		<author>
			<persName><forename type="first">Zohreh</forename><surname>Ovaisi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathryn</forename><surname>Vasilaky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Zheleva</surname></persName>
		</author>
		<idno type="DOI">10.1145/3404835.3463097</idno>
		<ptr target="https://doi.org/10.1145/3404835.3463097" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval (Virtual Event, Canada) (SIGIR &apos;21)</title>
		<meeting>the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval (Virtual Event, Canada) (SIGIR &apos;21)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1763" to="1767" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Causality</title>
		<author>
			<persName><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Causal inference in statistics : a primer</title>
		<author>
			<persName><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016 -2016</date>
			<publisher>Wiley</publisher>
			<pubPlace>Chichester, West Sussex</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1306.2597</idno>
		<imprint>
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
	<note type="report_type">Introducing LETOR 4.0 datasets. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Rendle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christoph</forename><surname>Freudenthaler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeno</forename><surname>Gantner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Schmidt-Thieme</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1205.2618</idno>
		<title level="m">Bayesian personalized ranking from implicit feedback</title>
		<imprint>
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ewa</forename><surname>Dominowska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Ragno</surname></persName>
		</author>
		<idno type="DOI">10.1145/1242572.1242643</idno>
		<ptr target="https://doi.org/10.1145/1242572.1242643" />
		<title level="m">Predicting Clicks: Estimating the Click-through Rate for New Ads (WWW &apos;07)</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="521" to="530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Asymmetric Tri-Training for Debiasing Missing-Not-At-Random Explicit Feedback</title>
		<author>
			<persName><forename type="first">Yuta</forename><surname>Saito</surname></persName>
		</author>
		<idno type="DOI">10.1145/3397271.3401114</idno>
		<ptr target="https://doi.org/10.1145/3397271.3401114" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (Virtual Event, China) (SIGIR &apos;20)</title>
		<meeting>the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval (Virtual Event, China) (SIGIR &apos;20)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="309" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Recommendations as treatments: Debiasing learning and evaluation</title>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Schnabel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adith</forename><surname>Swaminathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashudeep</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navin</forename><surname>Chandak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
		<idno>PMLR</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1670" to="1679" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">When Inverse Propensity Scoring Does Not Work: Affine Corrections for Unbiased Learning to Rank</title>
		<author>
			<persName><forename type="first">Ali</forename><surname>Vardasbi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harrie</forename><surname>Oosterhuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>De Rijke</surname></persName>
		</author>
		<idno type="DOI">10.1145/3340531.3412031</idno>
		<ptr target="https://doi.org/10.1145/3340531.3412031" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Information Knowledge Management</title>
		<meeting>the 29th ACM International Conference on Information Knowledge Management<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1475" to="1484" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deconfounded Recommendation for Alleviating Bias Amplification</title>
		<author>
			<persName><forename type="first">Wenjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
		<idno type="DOI">10.1145/3447548.3467249</idno>
		<ptr target="https://doi.org/10.1145/3447548.3467249" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery Data Mining (Virtual Event, Singapore) (KDD &apos;21)</title>
		<meeting>the 27th ACM SIGKDD Conference on Knowledge Discovery Data Mining (Virtual Event, Singapore) (KDD &apos;21)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1717" to="1725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning to Rank with Selection Bias in Personal Search</title>
		<author>
			<persName><forename type="first">Xuanhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bendersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Najork</surname></persName>
		</author>
		<idno type="DOI">10.1145/2911451.2911537</idno>
		<ptr target="https://doi.org/10.1145/2911451.2911537" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 39th International ACM SIGIR Conference on Research and Development in Information Retrieval<address><addrLine>Pisa, Italy; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="115" to="124" />
		</imprint>
	</monogr>
	<note>SIGIR &apos;16)</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Position Bias Estimation for Unbiased Learning to Rank in Personal Search</title>
		<author>
			<persName><forename type="first">Xuanhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nadav</forename><surname>Golbandi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bendersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Najork</surname></persName>
		</author>
		<idno type="DOI">10.1145/3159652.3159732</idno>
		<ptr target="https://doi.org/10.1145/3159652.3159732" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining</title>
		<title level="s">WSDM &apos;18</title>
		<meeting>the Eleventh ACM International Conference on Web Search and Data Mining<address><addrLine>Marina Del Rey, CA, USA; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="610" to="618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Doubly robust joint learning for recommendation on data missing not at random</title>
		<author>
			<persName><forename type="first">Xiaojie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianzhong</forename><surname>Qi</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6638" to="6647" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Causal Inference for Recommender Systems</title>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<idno type="DOI">10.1145/3383313.3412225</idno>
		<ptr target="https://doi.org/10.1145/3383313.3412225" />
	</analytic>
	<monogr>
		<title level="m">Fourteenth ACM Conference on Recommender Systems (Virtual Event, Brazil) (RecSys &apos;20)</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="426" to="431" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Model-Agnostic Counterfactual Reasoning for Eliminating Popularity Bias in Recommender System</title>
		<author>
			<persName><forename type="first">Tianxin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinfeng</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<idno type="DOI">10.1145/3447548.3467289</idno>
		<ptr target="https://doi.org/10.1145/3447548.3467289" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery Data Mining (Virtual Event, Singapore) (KDD &apos;21)</title>
		<meeting>the 27th ACM SIGKDD Conference on Knowledge Discovery Data Mining (Virtual Event, Singapore) (KDD &apos;21)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1791" to="1800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Unbiased Ad Click Prediction for Position-Aware Advertising Systems</title>
		<author>
			<persName><forename type="first">Yaxu</forename><surname>Bowen Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jui-Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenhua</forename><surname>Hsia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chih-Jen</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><surname>Lin</surname></persName>
		</author>
		<idno type="DOI">10.1145/3383313.3412241</idno>
		<ptr target="https://doi.org/10.1145/3383313.3412241" />
	</analytic>
	<monogr>
		<title level="m">Fourteenth ACM Conference on Recommender Systems (Virtual Event, Brazil) (RecSys &apos;20)</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="368" to="377" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Causal Intervention for Leveraging Popularity Bias in Recommendation</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianxin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chonggang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guohui</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1145/3404835.3462875</idno>
		<ptr target="https://doi.org/10.1145/3404835.3462875" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval (Virtual Event, Canada) (SIGIR &apos;21)</title>
		<meeting>the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval (Virtual Event, Canada) (SIGIR &apos;21)<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="11" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Disentangling User Interest and Conformity for Recommendation with Causal Embedding</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Depeng</forename><surname>Jin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Web Conference 2021</title>
		<meeting>the Web Conference 2021<address><addrLine>Ljubljana, Slovenia</addrLine></address></meeting>
		<imprint>
			<publisher>WWW</publisher>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title/>
		<idno type="DOI">10.1145/3442381.3449788</idno>
		<ptr target="https://doi.org/10.1145/3442381.3449788" />
		<imprint>
			<publisher>ACM</publisher>
			<biblScope unit="page" from="2980" to="2991" />
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Cross-Positional Attention for Debiasing Clicks</title>
		<author>
			<persName><forename type="first">Honglei</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Bendersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Po</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Chary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename></persName>
		</author>
		<idno type="DOI">10.1145/3442381.3450098</idno>
		<ptr target="https://doi.org/10.1145/3442381.3450098" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Web Conference 2021</title>
		<meeting>the Web Conference 2021<address><addrLine>Ljubljana, Slovenia; New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="788" to="797" />
		</imprint>
	</monogr>
	<note>) (WWW &apos;21)</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
