<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A hierarchical decomposition for explaining ML performance discrepancies</title>
				<funder ref="#_JZ3cRXt">
					<orgName type="full">Food and Drug Administration</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-02-22">22 Feb 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Jean</forename><surname>Feng</surname></persName>
							<email>&lt;jean.feng@ucsf.edu&gt;</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>San Francisco</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Harvineet</forename><surname>Singh</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>San Francisco</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fan</forename><surname>Xia</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>San Francisco</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Adarsh</forename><surname>Subbaswamy</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">U.S. Food and Drug Administration</orgName>
								<orgName type="department" key="dep2">Center for Devices</orgName>
								<orgName type="institution">Radiological Health</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alexej</forename><surname>Gossmann</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">U.S. Food and Drug Administration</orgName>
								<orgName type="department" key="dep2">Center for Devices</orgName>
								<orgName type="institution">Radiological Health</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A hierarchical decomposition for explaining ML performance discrepancies</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-02-22">22 Feb 2024</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2402.14254v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Machine learning (ML) algorithms can often differ in performance across domains. Understanding why their performance differs is crucial for determining what types of interventions (e.g., algorithmic or operational) are most effective at closing the performance gaps. Existing methods focus on aggregate decompositions of the total performance gap into the impact of a shift in the distribution of features p(X) versus the impact of a shift in the conditional distribution of the outcome p(Y |X); however, such coarse explanations offer only a few options for how one can close the performance gap. Detailed variable-level decompositions that quantify the importance of each variable to each term in the aggregate decomposition can provide a much deeper understanding and suggest much more targeted interventions. However, existing methods assume knowledge of the full causal graph or make strong parametric assumptions. We introduce a nonparametric hierarchical framework that provides both aggregate and detailed decompositions for explaining why the performance of an ML algorithm differs across domains, without requiring causal knowledge. We derive debiased, computationally-efficient estimators, and statistical inference procedures for asymptotically valid confidence intervals.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The performance of an ML algorithm can differ across domains due to shifts in the data distribution. To understand what contributed to this performance gap, prior works have suggested decomposing the gap into that due to a shift in the marginal distribution of the input features p(X) versus that due to a shift in the conditional distribution of the outcome p(Y |X) <ref type="bibr" target="#b4">(Cai et al., 2023;</ref><ref type="bibr" target="#b41">Zhang et al., 2023;</ref><ref type="bibr" target="#b25">Liu et al., 2023;</ref><ref type="bibr" target="#b30">Qiu et al., 2023;</ref><ref type="bibr" target="#b12">Firpo et al., 2018)</ref>. Although such aggregate decompositions can be helpful, more detailed ex-As a motivating example, suppose an ML deployment team has an algorithm that predicts the risk of patients being readmitted to the hospital given data from the Electronic Health Records (EHR), e.g. demographic variables and diagnosis codes. The algorithm was trained for a general patient population. The team intends to deploy it for heart failure (HF) patients and observes a large performance drop (e.g. in accuracy) in this subgroup. An aggregate decomposition of the performance drop into the marginal versus conditional components (p(X) and p(Y |X)) provides only a high-level understanding of the underlying cause and limited suggestions on how the model can be fixed. A small conditional term in the aggregate decomposition but a large marginal term is typically addressed by retraining the model on a reweighted version of the original data that matches the target population <ref type="bibr" target="#b31">(Quionero-Candela et al., 2009)</ref>; otherwise, the standard suggestion is to collect more data from the target population to recalibrate/fine-tune the algorithm <ref type="bibr" target="#b33">(Steyerberg, 2009)</ref>. A detailed decomposition would help the deployment team conduct a root cause analysis and assess the utility of targeted variable-specific interventions. For instance, if the performance gap is primarily driven by a marginal shift in a few diagnoses, the team can investigate why the rates of these diagnoses differ between general and HF patients. The team may find that certain diagnoses differ due to variations in patient case mix, which may be better addressed through model retraining, whereas others are due to variations in documentation practices, which may be better addressed through operational interventions.</p><p>Existing approaches for providing a detailed understanding of why the performance of an ML algorithm differs across domains fall into two general categories. One category, which is also the most common in the applied literature, is to quantify how much the data distribution shifted <ref type="bibr" target="#b25">(Liu et al., 2023;</ref><ref type="bibr" target="#b3">Budhathoki et al., 2021;</ref><ref type="bibr" target="#b24">Kulinski et al., 2020;</ref><ref type="bibr" target="#b32">Rabanser et al., 2019)</ref>, e.g. one can compare how the distribution of each input variable differs across domains <ref type="bibr" target="#b7">(Cummings et al., 2023)</ref>. However, given the complex in- teractions and non-linearities in (black-box) ML algorithms, it is difficult to quantify how exactly such shifts contribute to changes in performance. For instance, a large shift with respect to a given variable does not necessarily translate to a large shift in model performance, as that variable may have low importance in the algorithm or the performance metric may be insensitive to that shift. Thus the other category of approaches-which is also the focus of this manuscript-is to directly quantify how a shift with respect to a variable (subset) influences model performance. Existing proposals only provide detailed variable-level breakdowns for either the marginal or conditional terms in the aggregate decomposition, but no unified framework exists. Moreover, existing methods either require knowledge of the causal graph relating all variables <ref type="bibr" target="#b41">(Zhang et al., 2023)</ref> or strong parametric assumptions <ref type="bibr" target="#b39">(Wu et al., 2021;</ref><ref type="bibr" target="#b9">Dodd &amp; Pepe, 2003)</ref>.</p><p>This work introduces a unified nonparametric framework for explaining the performance gap of an ML algorithm that first decomposes the gap into terms at the aggregate level and then provides detailed variable importance (VI) values for each aggregate term <ref type="bibr">(Fig 1)</ref>. Whereas prior works only provide point estimates for the decomposition, we derive debiased, asymptotically linear estimators for the terms in the decomposition which allow for the construction of confidence intervals (CIs) with asymptotically valid coverage rates. Uncertainty quantification is crucial in this setting, as one often has limited labeled data from the target domain. The estimation and statistical inference procedure is computationally efficient, despite the exponential number of terms in the Shapley value definition. We demonstrate the utility of our framework in real-world examples of prediction models for hospital readmission and insurance coverage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Prior work</head><p>Describing distribution shifts. This line of work focuses on detecting and localizing which distributions shift between datasets <ref type="bibr" target="#b24">(Kulinski et al., 2020;</ref><ref type="bibr" target="#b32">Rabanser et al., 2019)</ref>. <ref type="bibr" target="#b3">Budhathoki et al. (2021)</ref> identify the main variables contributing to a distribution shift via a Shapley framework, <ref type="bibr" target="#b23">Kulinski &amp; Inouye (2023)</ref> fits interpretable optimal transport maps, and <ref type="bibr" target="#b25">Liu et al. (2023)</ref> finds the region with the largest shift in the conditional outcome distribution. However, these works do not quantify how these shifts contribute to changes in performance, the metric of practical importance.</p><p>Explaining loss differences across subpopulations. Understanding differences in model performance across subpopulations in a single dataset is similar to understanding differences in model performance across datasets, but the focus is typically to find subpopulations with poor performance rather than to explain how distribution shifts contributed to the performance change. Existing approaches include slice discovery methods <ref type="bibr" target="#b29">(Plumb et al., 2023;</ref><ref type="bibr" target="#b17">Jain et al., 2023;</ref><ref type="bibr" target="#b8">d'Eon et al., 2022;</ref><ref type="bibr" target="#b10">Eyuboglu et al., 2022)</ref> and structured representations of the subpopulation using e.g. Euclidean balls <ref type="bibr" target="#b0">(Ali et al., 2022)</ref>.</p><p>Attributing performance changes. Prior works have described similar aggregate decompositions of the performance change into covariate and conditional outcome shift components <ref type="bibr" target="#b4">(Cai et al., 2023;</ref><ref type="bibr" target="#b30">Qiu et al., 2023)</ref>. To provide more granular explanations of performance shifts, existing works quantify the importance of shifts in each variable assuming the true causal graph is known <ref type="bibr" target="#b41">(Zhang et al., 2023)</ref>; covariate shifts restricted to variable subsets assuming partial shifts follow a particular structure <ref type="bibr" target="#b39">(Wu et al., 2021)</ref>; and conditional shifts in each variable assuming a parametric model <ref type="bibr" target="#b9">(Dodd &amp; Pepe, 2003)</ref>. However, the strong assumptions made by these methods make them difficult to apply in practice, and model misspecification can lead to unintuitive interpretations. In addition, there is no unifying framework for decomposing both covariate and outcome shifts, and many methods do not output CIs, which is important when the amount of labeled data from a given domain is limited. A summary of how the proposed framework compares against prior works is shown in Table <ref type="table" target="#tab_3">1</ref> of the Appendix.</p><p>Decomposition methods in econometrics. Explaining performance differences is similar to the long-studied problem of explaining income and health disparities between groups in econometrics. There, researchers regularly use frameworks such as the Oaxaca-Blinder decomposition <ref type="bibr" target="#b26">(Oaxaca, 1973;</ref><ref type="bibr" target="#b2">Blinder, 1973;</ref><ref type="bibr" target="#b13">Fortin et al., 2011)</ref>, which decomposes disparities into components quantifying the impact of covariate shifts and outcome shifts. These frameworks commonly decompose the components further to describe the importance of each variable <ref type="bibr" target="#b26">(Oaxaca, 1973;</ref><ref type="bibr" target="#b21">Kirby et al., 2006)</ref>. Existing methods typically rely on strong parametric assumptions <ref type="bibr" target="#b11">(Fairlie, 2005;</ref><ref type="bibr" target="#b40">Yun, 2004;</ref><ref type="bibr" target="#b12">Firpo et al., 2018)</ref>, which is inappropriate for the complex data settings in ML.</p><p>In summary, the distinguishing contribution of this work is that it unifies aggregate and detailed decompositions under a nonparametric framework with uncertainty quantification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">A hierarchical explanation framework</head><p>Here we introduce how the aggregate and detailed decompositions are defined for explaining the performance gap of a risk prediction algorithm f : X ⊆ R m → Y for binary outcomes across source and target domains, denoted by D = 0 and D = 1, respectively. Let the performance of f be quantified in terms of a loss function ℓ : X × Y → R, such as the 0-1 misclassification loss 1{f(X) ̸ = Y }. Suppose variables X can be partitioned into W ∈ R m1 and Z = X \ W ∈ R m2 , where m = m 1 + m 2 . This partitioning allows for a factorization of the data distribution p D in the source domain D = 0 and target domain</p><formula xml:id="formula_0">D = 1 into p D (W )p D (Z|W )p D (Y |W, Z) (1) (see Fig 2 top left).</formula><p>As such, we refer to W as baseline variables and Z as conditional covariates. For the readmission example, W may refer to demographics and Z to diagnosis codes. The total change in performance of f can thus be written as</p><formula xml:id="formula_1">Λ = E 111 [ℓ(W, Z, Y )] -E 000 [ℓ(W, Z, Y )] ,</formula><p>where E DWDZDY denotes the expectation over the distribution p DW (W )p DZ (Z|W )p DY (Y |W, Z). A summary of notation used is provided in Table <ref type="table" target="#tab_4">2</ref> of the Appendix.</p><p>Aggregate. At the aggregate level, the framework quantifies how replacing each factor in (1) from source to target contributes to the performance gap. We refer to such replacements as "aggregate shifts," as the shift is not restricted to a particular subset of variables. This leads to the aggregate decomposition Λ = Λ W + Λ Z + Λ Y , where Λ W quantifies the impact of a shift in the baseline distribution p(W ) (also known as marginal or covariate shifts <ref type="bibr" target="#b31">(Quionero-Candela et al., 2009)</ref>), Λ Z quantifies the impact of a shift in the conditional covariate distribution p(Z|W ), and Λ Y quantifies the impact of a shift in the outcome distribution p(Y |W, Z) (also known as concept shift). More concretely,</p><formula xml:id="formula_2">Λ W = E 100 [ℓ] -E 000 [ℓ] Λ Z = E 1•• E •10 [ℓ | W ] -E •00 [ℓ | W ] ∆•10(W ) Λ Y = E 11• E ••1 [ℓ | W, Z] -E ••0 [ℓ | W, Z] ∆••0(W,Z)</formula><p>.</p><p>Prior works have proposed similar aggregate decompositions <ref type="bibr" target="#b4">(Cai et al., 2023;</ref><ref type="bibr" target="#b25">Liu et al., 2023;</ref><ref type="bibr" target="#b12">Firpo et al., 2018)</ref>.</p><p>Detailed.  closing the performance gap. For instance, a variable with high importance to the conditional covariate shift term Λ Z can be due to differences in missingness rates, prevalence, or selection bias; and a variable with high importance to the conditional outcome shift term Λ Y may indicate inherent differences in the conditional distribution (also known as effect modification), differences in measurement error or outcome definitions, or omission of variables predictive of the outcome. Note that the framework does not output a detailed decomposition of the baseline shift term Λ W , for reasons we discuss later.</p><formula xml:id="formula_3">Z Y W Z s Y W Z -s Z s Y W Z -s Q D D Z Y W D Z Y W D D Aggregate Detailed</formula><p>We leverage the Shapley attribution framework for its axiomatic properties, which result in VI values with intuitive interpretations <ref type="bibr">(Shapley, 1953;</ref><ref type="bibr">Charnes et al., 1988)</ref>. Recall that for a real-valued value function v defined for all subsets s ⊆ {1, • • • , m}, the attribution to element j is defined as the average gain in value due to the inclusion of j to every possible subset, i.e.</p><formula xml:id="formula_4">ϕ j := 1 m s⊆{1,••• ,m}\j m -1 |s| -1 {v(s ∪ j) -v(s)}.</formula><p>So to define a detailed decomposition, the key question is how to define the value of a partial distribution shift only with respect to a variable subset s; henceforth we refer to such shifts as s-partial shifts. It turns out that the answer is far from straightforward.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Value of partial distribution shifts</head><p>If the true causal graph is known, it would be straightforward to determine how the data distribution shifted with respect to a given variable subset s: we would assign the value of subset s as the change in overall performance due to shifts only in s with respect to this graph. However, in the absence of more detailed causal knowledge, one can only hypothesize the form of partial distribution shifts. Some proposals define importance as the change in average performance due to hypothesized partial shifts (see e.g. <ref type="bibr" target="#b39">Wu et al. (2021)</ref>), but it can lead to unintuitive behavior where a hypothesized distribution shift inconsistent with the true causal graph is attributed high importance. We illustrate such an example in a simulation in Section 5.</p><p>Instead, our proposal defines the importance of variable subset s as the proportion of variation in performance changes across strata explained by its corresponding hypothesized s-partial shift, denoted by p s . The definition generalizes the traditional R 2 measure in statistics, which quantifies how well do covariates explain variation in outcome rather than in performance changes. Prior works on variable importance have leveraged similar definitions <ref type="bibr" target="#b38">(Williamson et al., 2021;</ref><ref type="bibr" target="#b16">Hines et al., 2023)</ref>. For the conditional covariate decomposition, we define the value of s as</p><formula xml:id="formula_5">v Z (s) := 1 - E 1•• (∆ •s0 (W ) -∆ •10 (W )) 2 E 1•• [∆ 2 •10 (W )]</formula><p>.</p><p>where </p><formula xml:id="formula_7">∆ •DZ0 (W ) = E •DZ0 [ℓ|W ] -E •00 [ℓ|W ]</formula><formula xml:id="formula_8">E 11• (∆ ••s (W, Z) -∆ ••1 (W, Z)) 2 E 11• [∆ 2 ••1 (W, Z)]</formula><p>where ∆(W, Z)</p><formula xml:id="formula_9">••s = E ••s [ℓ|W, Z] -E ••0 [ℓ|W, Z] . Setting ϕ Y,0 = v Y (∅), we denote the corresponding Shapley values by {ϕ Y,j : j = 0, • • • , m 2 }.</formula><p>Because this framework defines importance in terms of variance explained, it does not provide a detailed decomposition of the baseline shift.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Candidate partial shifts</head><p>The previous section defines a general scoring scheme for quantifying the importance of any hypothesized s-partial shift. In this work, we consider partial shifts of the following forms. We leave other forms of partial shifts to future work. </p><formula xml:id="formula_10">(Y |W, Z) ̸ ≡ p 0 (Y |W, Z) even when p 1 (Y |W, Z) ≡ p 0 (Y |W, Z).</formula><p>Instead, we define an s-partial outcome shift based on models commonly used in model recalibration/revision <ref type="bibr" target="#b33">(Steyerberg, 2009;</ref><ref type="bibr" target="#b28">Platt, 1999)</ref>, where the modified risk is a function of the risk in the source domain Q := q(W, Z) := p 0 (Y = 1|W, Z), W , and Z s . In particular, we define the shift as</p><formula xml:id="formula_11">p s (y|z, r, w) := p 1 (y|z s , r, w) (3) = p 1 (y|z -s , z s , w)p 1 (z -s |z s , w, q(w, z s , z-s ) = r)dz -s .</formula><p>By defining the shifted outcome distribution solely as a function of Q, W , and Z s , it encompasses the special scenario where there is no conditional outcome shift and eliminates any direct effect from Z -s to Y . Note that v Y (∅) may be non-zero when the risk in the target domain is a recalibration (i.e. temperature-scaling) of the risk in the source domain <ref type="bibr" target="#b28">(Platt, 1999;</ref><ref type="bibr" target="#b14">Guo et al., 2017)</ref>. For instance, there may be general environmental factors such that readmission risks in the target domain are uniformly lower.</p><p>In the readmission example, a partial outcome distribution shift may be due to, for instance, a hospital protocol where HF patients with given diagnoses z s (e.g. kidney failure) are scheduled for more frequent follow-ups than just having those diagnoses alone. Thus readmission risks in the HF population can be viewed as a fine-tuning of readmission risks in the general patient population with respect to z s .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Estimation and Inference</head><p>The key estimands in this hierarchical attribution framework are the aggregate terms Λ W , Λ Z , and Λ Y and the Shapleybased detailed terms ϕ Z,j and ϕ Y,j for j = 0, • • • , m 2 . We now provide nonparametric estimators for these quantities as well as statistical inference procedures for constructing CIs. Nearly all prior works for decomposing performance gaps have relied on plug-in estimators, which substitute in estimates of the conditional means (also known as outcome models) or density ratios <ref type="bibr" target="#b34">(Sugiyama et al., 2007)</ref>, which we collectively refer to as nuisance parameters. For instance, given ML estimators μ•10 and μ•00 for the conditional means µ</p><formula xml:id="formula_12">•10 (w) = E •10 [ℓ|W ] and µ •00 (w) = E •00 [ℓ|W ],</formula><p>respectively, the empirical mean of μ•10 -μ•00 with respect to the target domain is a plug-in estimator for</p><formula xml:id="formula_13">Λ Z = E 1•• [µ •10 -µ •00</formula><p>]. However, because ML estimators typically converge to the true nuisance parameters at a rate slower than n -1/2 , plug-in estimators generally fail to be consistent at a rate of n -1/2 and cannot be used to construct CIs <ref type="bibr" target="#b19">(Kennedy, 2022)</ref>. Nevertheless, using tools from semiparametric inference theory (e.g. one-step correction and Neyman orthogonality), one can derive debiased ML estimators that facilitate statistical inference <ref type="bibr" target="#b6">(Chernozhukov et al., 2018;</ref><ref type="bibr" target="#b35">Tsiatis, 2006)</ref>. Here we derive debiased ML estimators for terms in the aggregate and detailed decompositions.</p><p>The detailed conditional outcome decomposition is particularly interesting, as its unique structure is not amenable to standard techniques for debiasing ML estimators.</p><p>For ease of exposition, theoretical results are presented for split-sample estimators; nonetheless, the results readily extend to cross-fitted estimators under standard convergence criteria <ref type="bibr" target="#b6">(Chernozhukov et al., 2018;</ref><ref type="bibr" target="#b19">Kennedy, 2022)</ref>. Let {(W</p><formula xml:id="formula_14">(d) i , Z<label>(d)</label></formula><p>i , Y</p><p>i </p><formula xml:id="formula_16">) : i = 1, • • • , n d } denote n d indepen</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Aggregate decomposition</head><p>The aggregate decomposition terms can be formulated as an average treatment effect, a well-studied estimand in causal inference, where domain D corresponds to treatment. As such, one can use augmented inverse probability weighting (AIPW) to define debiased ML estimators of the aggregate decomposition terms (e.g. <ref type="bibr" target="#b18">(Kang &amp; Schafer, 2007)</ref>). We review estimation and inference for these terms below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Estimation. Using the training data, estimate outcome models</head><formula xml:id="formula_17">µ •00 (W ) = E •00 [ℓ|W = w] and µ ••0 (W, Z) = E[ℓ|W, Z] and density ratio models π 100 (W ) = p 1 (W )/p 0 (W ) and π 110 (W, Z) = p 1 (W, Z)/p 0 (W, Z). The debiased ML estimators for Λ W , Λ Z , Λ Y are ΛW =P Ev 0,n (ℓ -μ•00 (W )) π100 (W ) + P Ev 1,n μ•00 (W ) -P Ev 0,n ℓ ΛZ =P Ev 0,n (ℓ -μ••0 (W, Z)) π110 (W, Z) + P Ev 1,n μ••0 (W, Z) -P Ev 0,n (ℓ -μ•00 (W )) π100 (W ) -P Ev 1,n μ•00 (W ) ΛY =P Ev 1,n ℓ -P Ev 0,n (ℓ -μ••0 (W, Z)) π(W, Z) -P Ev 1,n μ0 (W, Z)</formula><p>Inference. Assuming the estimators for the outcome and density ratio models converge at a fast enough rate, the AIPW estimators for the aggregate decomposition terms converge at the desired o p (n -1/2 ) rate. Theorem 4.1. Suppose π 100 and π 110 are bounded; estimators μ•00 , π••0 , π100 , and π110 are consistent; and</p><formula xml:id="formula_18">P 0 (μ •00 -µ •00 ) (π 100 -π 100 ) = o p (n -1/2 ) P 0 (μ ••0 -µ ••0 ) (π 110 -π 110 ) = o p (n -1/2 ).</formula><p>Then ΛW , ΛZ , and ΛY are asymptotically linear estimators of their respective estimands.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Detailed decomposition</head><p>The Shapley-based detailed decomposition terms ϕ Z,j and ϕ Y,j for j = 0, Standard recipes for deriving asymptotically linear, nonparametric efficient estimators rely on pathwise differentiability of the estimand and analyzing its efficient influence function (EIF) <ref type="bibr" target="#b19">(Kennedy, 2022)</ref>. However, v Y (s) is not pathwise differentiable because it is a function of (3), which conditions on the source risk q(w, z) equalling some value r. Taking the pathwise derivative of v Y (s) requires taking a derivative of the indicator function 1{q(w, z) = r}, which generally does not exist. Given the difficulties in deriving an asymptotically normal estimator for v Y (s), we propose estimating a close alternative that is pathwise differentiable.</p><p>The idea is to replace q in (3) with its binned variant q bin (w, z) = 1 B ⌊q(w, z)B + 1 2 ⌋ for some B ∈ Z + , which discretizes outputs from q into B disjoint bins. As long as B is sufficiently high, the binned version of the estimand, denoted v Y,bin (s), is a close approximation to v Y (s). (We use B = 20 in the empirical analyses, which we believe to be sufficient in practice.) The benefit of this binned variant is that the derivative of the indicator function 1{q bin (w, z) = r} is zero almost everywhere as long as observations with source risks exactly equal to a bin edge is measure zero. More formally, we require the following: Condition 4.2. Let Ξ be the set of (W, Z) such that q(W, Z) falls precisely on some bin edge but q(W, Z) is not equal to zero or one. The set Ξ is measure zero.</p><p>Under this condition, v Y,bin (s) is pathwise differentiable and, using one-step correction, we derive a debiased ML estimator that has the unique form of a V-statistic (this follows from the integration over "phantom" z-s in (3)). We represent V-statistics using the operator P Ev </p><formula xml:id="formula_19">(W, Z s , Z -s , Q bin ) = p 1 (Z -s |W, Z s , q bin (W, Z) = Q bin )/p 1 (Z -s ) and the s- shifted outcome model µ ••s (W, Z) = E ••s [ℓ|W, Z]. The estimator for v Y,bin (s) is vY,bin (s) = vnum Y,n (s)/v den Y,n where vnum Y,n (s) is P Ev 1,n ξs(W, Z) 2 (4) + 2P Ev 1,n ξs(W, Z)(ℓ -μ••1(W, Z)) -2P Ev 1,n PEv 1,n ξs(W, Zs, Z-s)ℓ(W, Zs, Z-s, Y )π(W, Zs, Z-s, Qbin) + 2P Ev 1,n PEv 1,n ξs(W, Zs, Z-s)μ••s(W, Zs, Z-s)π(W, Zs, Z-s, Qbin)</formula><p>where ξs (W,</p><formula xml:id="formula_20">Z) = μ••1 (W, Z) -μ••s (W, Z) and vden Y,n is P Ev 1,n (μ••1(W, Z) -μ••0(W, Z)) 2 (5) + 2P Ev 1,n (μ••1(W, Z) -μ••0(W, Z)) (ℓ -μ••1(W, Z)) -2P Ev 0,n (μ••1(W, Z) -μ••0(W, Z)) (ℓ -μ••0(W, Z))π110(W, Z).</formula><p>Inference. This estimator is asymptotically linear assuming the nuisance estimators converge at a fast enough rate. Theorem 4.3. Suppose Condition 4.2 holds. For variable subset s, suppose π(W, Z s , Z -s , Q bin ), π 110 are bounded;</p><formula xml:id="formula_21">v den Y (∅) &gt; 0; estimator π is consistent; estimators μ••0 , μ••1 and μ••s converge at an o p (n -1/4</formula><p>) rate, and</p><formula xml:id="formula_22">P 1 (q bin -q bin ) 2 = o p (n -1 ) (6) P 1 (µ ••s -μ••s )(π -π) = o p (n -1/2 ) P 0 (µ ••0 -μ••0 )(π 110 -π110 ) = o p (n -1/2 )</formula><p>Then the estimator vY,bin (s) is asymptotically normal centered at the estimand v Y,bin (s).</p><p>Convergence rate of o p (n -1/2 ) can be achieved by ML estimators in a wide variety of conditions, and such assumptions are commonly used to construct debiased ML estimators. The additional requirement in (6) that qbin converges at a o p (n -1 ) rate is new, but fast or even super-fast convergence rates of binned risks is achievable under suitable margin conditions <ref type="bibr" target="#b1">(Audibert &amp; Tsybakov, 2007)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2.">SHAPLEY VALUES</head><p>Calculating the exact Shapley value is computationally intractable as it involves an exponential number of terms. Nevertheless, <ref type="bibr" target="#b37">Williamson &amp; Feng (2020)</ref> showed that calculating the exact Shapley value for variable importance measures is not necessary when the importance measures are estimated with uncertainty. One can instead estimate the Shapley values by sampling variable subsets and inflate the corresponding CIs to reflect the additional uncertainty due to subset sampling. As long as the number of subsets scales linearly or super-linearly in n, one can show that this additional uncertainty will not be larger than that due to estimation of subset values themselves. Here we follow the same procedure to estimate and construct CIs for the detailed decomposition Shapley values (see Algorithm 4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Simulation</head><p>We now present simulations that verify the theoretical results by showing that the proposed procedure achieves the desired coverage rates (Sec 5.1) and illustrate how the proposed method provides more intuitive explanations of performance gaps (Sec 5.2). In all empirical analyses, performance of the ML algorithm is quantified in terms of 0-1 accuracy. Below, we briefly describe the simulation settings; full details are provided in Sec E in the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Verifying theoretical properties</head><p>We first verify that the inference procedures for the decomposition terms have CIs with coverage close to their nominal rate. We check the coverage of the aggregate decomposition as well as the value of s-partial conditional covariate and partial conditional outcome shifts for s = {Z 1 }, {Z 2 }, {Z 3 }.</p><p>(W, Z 1 , Z 2 , Z 3 ) are sampled from independent normal distributions with different means in source and target, while Y is simulated from logistic regression models with different coefficients. CIs for the debiased ML estimator converge to the nominal 90% coverage rate with increasing sample size, whereas those for the naïve plug-in estimator do not (Fig <ref type="figure" target="#fig_5">3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Comparing explanations</head><p>We now compare the proposed definitions for the detailed decomposition with existing methods. For the detailed con- ditional covariate decomposition, the comparators are:</p><p>• MeanChange Tests for a difference in means for each variable. Defines importance as 1p-value. • Oaxaca-Blinder: Fits a linear model of the logittransformed expected loss with respect to Z in the source domain. Defines importance of Z i as its coefficient multiplied by the difference in the mean of Z i <ref type="bibr" target="#b2">(Blinder, 1973</ref>). • WuShift <ref type="bibr" target="#b39">(Wu et al., 2021)</ref>: Defines importance of subset s as change in overall performance due to s-partial conditional covariate shifts. Applies Shapley framework to obtain VIs.</p><p>For the detailed decomposition of the performance gap due to shifts in the outcome, we compare against:</p><p>• ParametricChange: Fits a logistic model for Y with interaction terms between domain and Z. Defines importance of Z i as the coefficient of its interaction term. • ParametricAcc: Same as ParametricChange but models the 0-1 loss rather than Y . • RandomForestAcc: Compares VI of RFs trained on data from both domains with input features D, Z, and W to predict the 0-1 loss. • Oaxaca-Blinder: Fits linear models for the logittransformed expected loss in each domain. Defines importance of Z i as its mean in the target domain multiplied by the difference in its coefficients across domains.</p><p>Although the proposed method agrees on important features with these other methods in certain cases, there are important situations where the methods differ as highlighted below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conditional covariate. (Fig 4a)</head><p>We simulate (W, Z 1 ) from a standard normal distribution, Z 2 from a mixture of two Gaussians whose means depend on the value of Z 1 (i.e.</p><p>Z 1 → Z 2 ), and Y from a logistic regression model depending on (W, Z 1 , Z 2 ). We induce a shift from the source domain to the target domain by shifting only the distribution of Z 1 , so that p 1 (Z|W ) = p 0 (Z 2 |Z 1 , W )p 1 (Z 1 |W ).</p><p>Only the proposed estimator correctly recovers that Z 1 is more important than Z 2 , because this shift explains all the variation in performance gaps across strata W . The other methods incorrectly assign higher importance to Z 2 because they simply assess the performance change due to hypothesized s-partial shifts but do not check if the hypothesized shifts are good explanations in the first place. Perhaps a more objective evaluation is to compare the utility of the different explanations. To this end, we define a targeted algorithmic modification as one where the source risk is revised with respect to a subset of features by fitting an ML algorithm with input features as Q, W , and Z s on the target domain. Comparing the performance of the targeted algorithmic modifications that take in the top k features from each explanation method, we find that model revisions based on the proposed method achieve the highest performance for k = 1 to 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Real-world data case studies</head><p>We now analyze two datasets with naturally-occurring shifts.</p><p>Hospital readmission. Using electronic health record data from a large safety-net hospital, we analyzed performance of a Gradient Boosted Tree (GBT) trained on the general patient population (source) to predict 30-day readmission risk but applied to patients diagnosed with HF (target). Features include 4 demographic variables (W ) and 16 diagnosis codes (Z). Each domain supplied n = 3750 observations.</p><p>Model accuracy drops from 70% to 53%. From the aggre- gate decompositions (Fig <ref type="figure" target="#fig_8">5a</ref>), we observe that the drop is mainly due to covariate shift. If one performed the standard check to see which variables significantly changed in their mean value (MeanChange), then one would find a significant shift in nearly every variable. Little support is offered to identify main drivers of the performance drop. In contrast, the detailed decomposition from the proposed framework estimates diagnoses "Drug-induced or toxic-related condition" and "mental and substance use disorder in remission" as having the highest estimated contributions to the conditional covariate shift, and most other variables having little to no contribution. Upon discussion with clinicians from this hospital, differences in the top two diagnoses may be explained by (i) substance use being a major cause of HF at this hospital, with over eighty percent of its HF patients reporting current or prior substance use, and (ii) substance use and mental health disorders often occurring simultaneously in this HF patient population. Based on these findings, closing the performance gap may require a mixture of both operational and algorithmic interventions. Finally, CIs from the debiased ML procedure provide valuable information on the uncertainty of the estimates and highlight, for instance, that more data is necessary to determine the true ordering between the top two features. In contrast, existing methods do not provide (asymptotically valid) CIs.</p><p>ACS Public Coverage. We analyze a neural network trained to predict whether a person has public health insurance using data from Nebraska in the American Community Survey (source, n = 3000), applied to data from Louisiana (target, n = 6000). Baseline variables include 3 demographics (sex, age, race), and covariates Z include 31 variables related to health conditions, employment, marital status, citizenship status, and education.</p><p>Model accuracy drops from 84% to 66% across the two states. The main driver is the shift in the outcome distribution per the aggregate decomposition (Fig 5b ) and the most important contributor to the outcome shift is annual income, perhaps due to differences in cost of living across the two states. Income is significantly more important than all the other variables; the ranking between the remaining variables is unclear. In comparing the performance of targeted model revisions with respect to the top variables from each explanation method, we find that model revisions based on top variables identified by the proposed procedure lead to AUCs that are better or as good as those based on RandomForestAcc (Table <ref type="table" target="#tab_14">3</ref> in the Appendix).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Discussion</head><p>ML algorithms regularly encounter distribution shifts in practice, leading to drops in performance. We present a novel framework that helps ML developers and deployment teams build a more nuanced understanding of the shifts. Compared to past work, the approach is nonparametric, does not require fine-grained knowledge of the causal relationship between variables, and quantifies the uncertainty of the estimates by constructing confidence intervals. We present case studies on real-world datasets to demonstrate the use of the framework for understanding and guiding interventions to reduce performance drops.</p><p>Extensions of this work include relaxing the assumption of overlapping support of covariates such as by restricting to the common support <ref type="bibr" target="#b4">(Cai et al., 2023)</ref>, allowing for decompositions of more complex measures of model performance such as AUC, and analyzing other factorizations of the data distribution (e.g. label or prior shifts <ref type="bibr" target="#b22">(Kouw &amp; Loog, 2019)</ref>). For unstructured data (e.g. image and text), the current framework can be applied to low-dimensional embeddings or by extracting interpretable concepts <ref type="bibr" target="#b20">(Kim et al., 2018)</ref>; more work is needed to extend this framework to directly analyze unstructured data. Finally, the focus of this work is to interpret performance gaps. Future work may extend ideas in this work to design optimal interventions for closing the performance gap.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Impact statement</head><p>This work presents a method for understanding failures of ML algorithms when they are deployed in settings or populations different from the ones in development datasets. Therefore, the work can be used to suggest ways of improving the algorithms or mitigating their harms. The method is generally applicable to tabular data settings for any classification algorithm, hence, it can potentially be applied across multiple domains where ML is used including medicine, finance, and online commerce.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Contents of the Appendix</head><p>• Table <ref type="table" target="#tab_3">1</ref> summarizes the comparison with prior work.</p><p>• Table <ref type="table" target="#tab_4">2</ref> collects all the notation used for reference.</p><p>• Algorithms 1 and 4 provides the steps required for computing the aggregate and detailed decomposition respectively. Detailed decompositions require computing the value of s-partial conditional outcome and conditional covariate shifts which is described in Algorithms 2 and 3. • Section B describes the estimation and inference for detailed decomposition of conditional covariate shift.</p><p>• Section C provides the derivations of the results.</p><p>• Sections D and E describe the implementation and simulation details.</p><p>• Section F provides additional details on the two real world datasets and results.  Influence function defined in the linear approximation of an estimand, see e.g. ( <ref type="formula" target="#formula_23">11</ref>) 2 Fit nuisance parameters η W , η Z , η Y , defined in Section C.1, on the Tr partition as outlined in Section D.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Papers</head><p>3 Estimate Λ W , Λ Z , Λ Y using fitted nuisance parameters on the Ev partitions following the equations in Section 4.1.</p><p><ref type="foot" target="#foot_2">foot_2</ref> Estimate variance of influence functions ψ W (d, w, z, y; ηN ), ψ Z (d, w, z, y; ηN ), and ψ Y (d, w, z, y; ηN ) as defined in ( <ref type="formula">12</ref>), ( <ref type="formula">13</ref>), and ( <ref type="formula" target="#formula_25">14</ref>), respectively. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Proofs</head><p>Notation. For all proofs, we will write P to mean P Ev (and likewise for the empirical version) for notational simplicity.</p><p>Overview of derivation strategy. We first present the general strategy for proving asymptotic normality of the estimators for the decompositions. Details on nonparametric debiased estimation can be found in texts such as <ref type="bibr" target="#b35">Tsiatis (2006)</ref> and <ref type="bibr" target="#b19">Kennedy (2022)</ref>.</p><p>Let v(P) be a pathwise differentiable quantity that is a function of the true regular (differentiable in quadratic mean) probability distribution P over random variable O. For instance, v in the case of mean is defined as v(P) := E o∼P(O) <ref type="bibr">[o]</ref>. Let P denote an arbitrary regular estimator of P, such as the maximum likelihood estimator. The plug-in estimator is then defined as v( P).</p><p>The von-Mises expansion of the functional v (which linearizes v in analogy to the first-order Taylor expansion), given it is pathwise differentiable, gives v( P) -v(P) = -P ψ(o; P) + R( P, P).</p><p>Here, the function ψ is called an influence function (or a functional gradient of v at P). R( P, P) is a second-order remainder term. The one-step corrected estimators we consider have the form of v( P) + P n ψ(o; P) where P n denotes a sample average. Following the expansion above, the one-step corrected estimator can be analyzed as follows, v( P) + P n ψ(o; P) -v(P) = (P n -P)ψ(o; P) + R( P, P) = (P n -P)ψ(o; P) + (P n -P)(ψ(o; P) -ψ(o; P)) + R( P, P)</p><p>Our goal will be to analyze each of the three terms and to show that they are asymptotically negligible at √ n-rate, such that the one-step corrected estimator satisfies v( P) + P n ψ(o; P) -v(P) = P n ψ(o; P) + o p (n -1/2 ), where we used the property of influence functions that they have zero mean. Thus the one-step corrected estimator is asymptotically normal with mean v(P) and variance var(ψ(o; P))/n, which allows for the construction of CIs. In the following proofs, we present the influence functions without derivations; see <ref type="bibr" target="#b19">Kennedy (2022)</ref> and <ref type="bibr" target="#b15">Hines et al. (2022)</ref> for strategies for deriving influence functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1. Aggregate decompositions</head><p>Let the nuisance parameters in the one-step estimators Λ W , Λ Z , Λ Y be denoted by η</p><formula xml:id="formula_24">W = (µ •00 , π 100 ), η Z = (µ ••0 , µ •00 , π 110 ), η Y = (µ ••0 , π 110</formula><p>) respectively. Denote the estimated nuisances by ηW , ηZ , ηY . The canonical gradients for the three estimands are</p><formula xml:id="formula_25">ψ W (d, w, z, y; η W ) = [(ℓ(w, z, y) -µ •00 (w)) π 100 (w) -ℓ(w, z, y)] 1{d = 0} p(d = 0) + µ •00 (w) 1{d = 1} p(d = 1) -Λ W (12) ψ Z (d, w, z, y; η Z ) = [(ℓ(w, z, y) -µ ••0 (w, z)) π 110 (w, z)] 1{d = 0} p(d = 0) + µ ••0 (w, z) 1{d = 1} p(d = 1) -[(ℓ(w, z, y) -µ •00 (w)) π 100 (w)] 1{d = 0} p(d = 0) + µ •00 (w) 1{d = 1} p(d = 1) -Λ Z (13) ψ Y (d, w, z, y; η Y ) = (ℓ(w, z, y) -µ ••0 (w, z)) 1{d = 1} p(d = 1) -[(ℓ(w, z, y) -µ ••0 (w, z)) π 110 (w, z)] 1{d = 0} p(d = 0) -Λ Y . (<label>14</label></formula><formula xml:id="formula_26">)</formula><p>Theorem C.1 (Theorem 4.1). Under conditions outlined in Theorem 4.1, the one-step corrected estimators for the aggregate decomposition terms, baseline, conditional covariate, and conditional outcome ΛW , ΛZ , and ΛY , are asymptotically linear, i.e. ΛN -</p><formula xml:id="formula_27">Λ N = P n ψ N + o p (n -1/2 ) ∀N ∈ {W, Z, Y}.<label>(15)</label></formula><p>From convergence conditions in Condition B.1, this simplifies to</p><formula xml:id="formula_28">(22) =P 2(μ •s0 (W ) -μ•10 (W ))× (µ •0-s0 (Z s , W ) -μ•0-s0 (Z s , W ))π 1s0 (W, Z s ) 1{D = 0} p(D = 0) + (μ •0-s0 (Z s , W ) -μ•s0 (W )) 1{D = 1} p(D = 1) -(µ ••0 (W, Z) -μ••0 (W, Z))π 110 (W, Z) 1{D = 0} p(D = 0) -(μ ••0 (W, Z) -μ•10 (W )) 1{D = 1} p(D = 1)<label>(27)</label></formula><formula xml:id="formula_29">+ P((μ •s0 (W ) -μ•10 (W )) 2 -(µ •s0 (W ) -µ •10 (W )) 2 ) 1{D = 1} p(D = 1)<label>(28)</label></formula><formula xml:id="formula_30">+ o p (n -1/2 ),<label>(29)</label></formula><p>Given the true density ratios, we can further simplify the expectations over D = 0 weighted by the density ratios in the expression above to expectations over D = 1. By definition of µ •0-s0 (Z s , W ) in ( <ref type="formula">7</ref>) and µ •s0 (W ) in ( <ref type="formula">9</ref>) and the definition of µ ••0 (W, Z) and µ •10 (W ) in Section 4.1, (22) simplifies to</p><formula xml:id="formula_31">(22) =P 1 2(μ •s0 (W ) -μ•10 (W ))(µ •s0 (W ) -μ•s0 (W )) -P 1 2(μ •s0 (W ) -μ•10 (W ))(µ •10 (W ) -μ•10 (W )) + P 1 (μ •s0 (W ) -μ•10 (W )) 2 -(µ •s0 (W ) -µ •10 (W )) 2 ) + o p (n -1/2 ),</formula><p>which is o p (n -1/2 ) as long as the convergence conditions in Condition B.1 hold.</p><p>As the denominator v den Z is equal to the numerator v num Z (∅), it follows that the one-step estimator for the denominator vden Z is asymptotically linear with influence function ψ num Z,∅ .</p><p>Proof for Theorem B.2. Combining Lemma C.2 and the Delta method (van der Vaart, 1998, Theorem 3.1), the estimator</p><formula xml:id="formula_32">vZ (s) = vnum Z (s)/v den Z is asymptotically linear vnum Z (s) vden Z - v num Z (s) v den Z = P n ψ Z,s (D, W, Z, Y ; η num Z,s , η den Z ) + o p (n -1/2 ),</formula><p>with influence function</p><formula xml:id="formula_33">ψ Z,s (D, W, Z, Y ; η num Z,s , η den Z,s ) = 1 v den Z ψ num Z,s (D, W, Z, Y ; η num Z,s ) - v num Z (s) (v den Z ) 2 ψ den Z (D, W, Z, Y ; η den Z ),<label>(30)</label></formula><p>where</p><formula xml:id="formula_34">ψ num Z,s (D, W, Z, Y ; η num Z,s ) is defined in (17) and ψ den Z (D, W, Z, Y ; η den Z ) = ψ num Z,∅ (D, W, Z, Y ; η num Z,∅</formula><p>). Accordingly, the estimator asymptotically follows the normal distribution,</p><formula xml:id="formula_35">√ n (v Z (s) -v Z (s)) → d N (0, var(ψ Z,s (D, W, Z, Y ; η num Z,s , η den Z,s ))<label>(31)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3. Value of s-partial conditional outcome shifts</head><p>Let the nuisance parameters in v num Y,bin be denoted η num Y,s = (Q bin , µ  We represent the one-step corrected estimator for v num Y,bin (s) as the V-statistic</p><formula xml:id="formula_36">vnum Y,bin (s) =P 1,n P1,n (μ ••1 (W, Z) -μ••s (W, Z)) 2 (32) + 2P 1,n P1,n (μ ••1 (W, Z) -μ••s (W, Z)) (ℓ -µ ••1 (W, Z)) (33) -2P 1,n P1,n μ••1 (W, Z s , Z-s ) -μ••s (W, Z s , Z-s ) (ℓ(W, Z s , Z-s , Y ) -µ ••s (W, Z s , Z-s ))π( Z-s , Z s , W, q bin (W, Z)) (34) =P 1,n P1,n h W, Z, Y, W , Z, Ỹ ; ηnum Y,s .<label>(35)</label></formula><p>In more detail, the conditions in Theorem 4.3 are as follows.</p><p>Condition C.3. For variable subset s, suppose the following hold </p><formula xml:id="formula_37">• π(W, Z s , Z -s , Q bin ) is bounded • π is consistent • P 1 (μ ••0 -µ ••0 ) 2 = o p (n -1/2 ) • P 1 (μ ••1 -µ ••1 ) 2 = o p (n -1/2 ) • P 1 (μ ••s -µ ••s ) 2 = o p (n -1/2 ) • P 1 (q bin -q bin ) 2 = o p (n -1 ) • P 1 (μ ••s -µ ••s ) (π -π) = o p (n -1/2</formula><formula xml:id="formula_38">(s) -v num Y,bin (s) = P 1,n ψ num Y,s (D, W, Z, Y ; η num Y,s ) + o p (n -1/2 ),<label>(36)</label></formula><p>with influence function</p><formula xml:id="formula_39">ψ num Y,s d, w, z, y; η num Y,s =(µ ••1 (w, z) -µ ••s (w, z)) 2 + 2(µ ••1 (w, z) -µ ••s (w, z)) [ℓ(w, z, y) -µ ••1 (w, z)] -2P 1 (µ ••1 (w, z s , Z -s ) -µ ••s (w, z s , Z -s )) [ℓ (w, z s , Z -s , y)) -µ ••s (w, z s , Z -s )] π (Z -s , z s , w, q bin (w, z)) -v num Y,bin (s). (<label>37</label></formula><formula xml:id="formula_40">)</formula><p>Proof. Defining the symmetrized version of h in (35) as </p><formula xml:id="formula_41">h sym (W, Z, Y, W , Z, Ỹ ) = h(W,Z,Y, W , Z, Ỹ )+h( W , Z, Ỹ ,W,Z,</formula><formula xml:id="formula_42">P 1 P 1,n P1,n h sym W, Z, Y, W , Z, Ỹ ; ηnum Y,s -vnum Y (s) | X i , Y i = n i=1 P 1 h sym X i , Y i , X (2) , Y (2) ; ηnum Y,s -vnum Y (s) | X i , Y i = n i=1 h sym,1 X i , Y i ; ηnum Y,s</formula><p>where vnum </p><formula xml:id="formula_43">Y (s) = P 1 P1 h sym W, Z</formula><formula xml:id="formula_44">+ (P 1,n -P 1 ) h sym,1 X, Y ; ηnum Y,s + vnum Y (s) -h sym,1 (X, Y ) -v num Y (s)<label>(39)</label></formula><formula xml:id="formula_45">+ (P 1,n -P 1 ) (h sym,1 (X, Y ) + v num Y (s))<label>(40)</label></formula><formula xml:id="formula_46">+ P 1 h sym,1 X, Y ; ηnum Y,s + vnum Y (s) -h sym,1 X, Y ; η num Y,s -v num Y (s) . (<label>41</label></formula><formula xml:id="formula_47">)</formula><p>We analyze each term in turn. Then by Theorem 11.2 in van der Vaart (1998) and Slutsky's lemma, we have</p><formula xml:id="formula_48">P 1,n P1,n h sym W, Z, Y, W , Z, Ỹ ; ηnum Y,s -P 1,n h sym,1 W, Z, Y ; ηnum Y,s + vnum Y (s) = o p n -1/2 .</formula><p>Term (39): We perform sample splitting to estimate the nuisance parameters and calculate the estimator for vnum Y (s). Then by Lemma 1 in <ref type="bibr" target="#b19">Kennedy (2022)</ref>, we have that</p><formula xml:id="formula_49">(P 1,n -P 1 ) h sym,1 W, Z, Y ; ηnum Y,s + vnum Y (s) -h sym,1 W, Z, Y ; η num Y,s -v num Y (s) = o p (n -1/2</formula><p>) as long as the estimators for the nuisance parameters are consistent.</p><p>Term (40): This term (P 1,n -P 1 ) h sym,1 W, Z, Y ; η num Y,s + v num Y (s) = (P 1,n -P 1 ) h sym,1 W, Z, Y ; η num Y,s follows an asymptotic normal distribution per CLT.</p><p>Term (41): We will show that this bias term is asymptotically negligible. For notational simplicity, let ξ(W, Z s , Z -s ) = μ••1 (W, Z) -μ••s (W, Z).</p><formula xml:id="formula_50">P 1 P1 h sym W, Z, Y, W , Z, Ỹ ; ηnum Y,s + vnum Y (s) -h sym W, Z, Y, W , Z, Ỹ ; η num Y,s -v num Y (s) =P 1 P1 h W, Z, Y, W , Z, Ỹ ; ηnum Y,s -h W, Z, Y, W , Z, Ỹ ; η num Y,s =P 1 (μ ••1 (W, Z) -μ••s (W, Z)) 2 -P 1 (µ ••1 (W, Z) -µ ••s (W, Z)) 2 + 2P 1 (μ ••1 (W, Z) -μ••s (W, Z)) [µ ••1 (W, Z) -μ••1 (W, Z)] -2P 1 P1 μ••1 (W, Z s , Z-s ) -μ••s (W, Z s , Z-s ) ℓ(W, Z s , Z-s , Y ) -μ••s (W, Z s , Z-s ) π Z-s , Z s , W, qbin (W, Z) =P 1 (μ ••1 (W, Z) -μ••s (W, Z)) 2 -P 1 (µ ••1 (W, Z) -µ ••s (W, Z)) 2 + 2P 1 (μ ••1 (W, Z) -μ••s (W, Z)) [µ ••1 (W, Z) -μ••1 (W, Z)] -2P 1 P1 ξ(W, Z s , Z-s ) µ ••s (W, Z s , Z-s ) -μ••s (W, Z s , Z-s ) π Z-s , Z s , W, q bin (W, Z) -2P 1 P1 ξ(W, Z s , Z-s ) ℓ(W, Z s , Z-s , Y ) -μ••s (W, Z s , Z-s ) π Z-s , Z s , W, qbin (W, Z) -π Z-s , Z s , W, q bin (W, Z) =P 1 (µ ••s (W, Z) -μ••s (W, Z)) (μ ••1 (W, Z) -μ••s (W, Z) + µ ••1 (W, Z) -µ ••s (W, Z)) (42) + P 1 (μ ••1 (W, Z) -µ ••1 (W, Z)) (µ ••1 (W, Z) -µ ••s (W, Z) -μ••1 (W, Z) + μ••s (W, Z)) (43) -2P 1 P1 ξ(W, Z s , Z-s ) µ ••s (W, Z s , Z-s ) -μ••s (W, Z s , Z-s ) π Z-s , Z s , W, q bin (W, Z)<label>(44)</label></formula><formula xml:id="formula_51">-2P 1 P1 ξ(W, Z s , Z-s ) ℓ(W, Z s , Z-s , Y ) -μ••s (W, Z s , Z-s ) π Z-s , Z s , W, qbin (W, Z) -π Z-s , Z s , W, q bin (W, Z) .<label>(45)</label></formula><p>Note that ( <ref type="formula" target="#formula_51">45</ref>) is o p (n -1/2 ) under the assumed convergence rates for qbin . In addition, ( <ref type="formula">43</ref>) is o p (n -1/2 ), under the assumed convergence rates for μ••1 and μ••s .</p><p>Analyzing the remaining summands ( <ref type="formula">42</ref>) + (44), we note that it simplifies as follows:</p><formula xml:id="formula_52">P 1 (µ ••s (X) -μ••s (X)) (μ ••1 (X) -μ••s (X) + µ ••1 (X) -µ ••s (X)) -2P 1 P1 ξ(W, Z s , Z-s ) µ ••s (W, Z s , Z-s ) -μ••s (W, Z s , Z-s ) π Z-s , Z s , W, q bin (W, Z)) -π Z-s , Z s , W, q bin (W, Z)) -2P 1 P1 ξ(W, Z s , Z-s ) µ ••s (W, Z s , Z-s ) -μ••s (W, Z s , Z-s ) π Z-s , Z s , W, q bin (W, Z)) =P 1 (µ ••s (X) -μ••s (X)) (µ ••1 (X) -μ••1 (X) -µ ••s (X) + μ••s (X)) -2P 1 P1 ξ(W, Z s , Z-s ) µ ••s (W, Z s , Z-s ) -μ••s (W, Z s , Z-s ) π Z-s , Z s , W, q bin (W, Z)) -π Z-s , Z s , W, q bin (W, Z)) ,</formula><p>which is o p (n -1/2 ), under the assumed convergence rates for μ••s , μ••1 , and π.</p><p>Condition C.5 (Convergence conditions for vden Y ). Suppose the following holds </p><formula xml:id="formula_53">• P 1 (µ ••1 -µ ••0 -(μ ••1 -μ••0 )) 2 = o p (n -1/2 ) • P 0 (µ ••0 -μ••0 )(π 110 -π110 ) = o p (n -1/2 ) • P 0 (µ ••1 -µ ••0 ) 2 is bounded •<label>(</label></formula><formula xml:id="formula_54">= P n ψ den Y (D, W, Z, Y ; η den Y ) + o p (n -1/2 )</formula><p>with influence function</p><formula xml:id="formula_55">ψ den Y (D, W, Z, Y ; η den Y ) = (µ ••1 (W, Z) -µ ••0 (W, Z)) 2 1{D = 1} p(D = 1) (46) + 2 (µ ••1 (W, Z) -µ ••0 (W, Z)) (ℓ -µ ••1 (W, Z)) 1{D = 1} p(D = 1) (47) -2 (µ ••1 (W, Z) -µ ••0 (W, Z)) (ℓ -µ ••0 (W, Z))π 110 (W, Z) 1{D = 0} p(D = 0) (48) -v den Y .<label>(49)</label></formula><p>Proof. Consider the following decomposition of bias in the one-step corrected estimate</p><formula xml:id="formula_56">vden Y -v den Y =(P n -P)ψ den Y (D, W, Z, Y ; η den Y )<label>(50)</label></formula><formula xml:id="formula_57">+ (P n -P)(ψ den Y (D, W, Z, Y ; ηden Y ) -ψ den Y (D, W, Z, Y ; η den Y ))<label>(51)</label></formula><formula xml:id="formula_58">+ P(ψ den Y (D, W, Z, Y ; ηden Y ) -ψ den Y (D, W, Z, Y ; η den Y ))<label>(52)</label></formula><p>We observe that (50) converges to a normal distribution per CLT assuming that the variance of ψ den Y is finite. The empirical process term (51) is asymptotically negligible since the nuisance parameters η den Y are evaluated on an separate evaluation data split from the training data used for estimation. In addition assuming that the estimators for the nuisance parameters are consistent, Kennedy (2022, Lemma 1) states that</p><formula xml:id="formula_59">(P n -P)(ψ den Y (D, W, Z, Y ; ηden Y ) -ψ den Y (D, W, Z, Y ; η den Y )) = o p (n -1/2 ). p(D = 1) (61) -2P (μ ••1 (W, Z) -μ••0 (W, Z)) (µ ••0 (W, Z) -μ••0 (W, Z))(π 110 (W, Z) -π 110 (W, Z)) 1{D = 0} p(D = 0)<label>(62)</label></formula><p>Thus the remainder term is o p (n </p><formula xml:id="formula_60">Y ) = 1 v den Y ψ num Y,s (D, W, Z, Y ; η num Y,s ) - v num Y,bin (s) (v den Y ) 2 ψ den Y (D, W, Z, Y ; η den Y ),<label>(63)</label></formula><p>where ψ num Y,s and ψ den Y are defined in ( <ref type="formula" target="#formula_39">37</ref>) and ( <ref type="formula" target="#formula_55">49</ref>).</p><p>Accordingly, the estimator follows a normal distribution asymptotically,</p><formula xml:id="formula_61">√ n (v Y (s) -v Y (s)) → d N (0, var(ψ Y,bin,s (D, W, Z, Y ; η num Y , η den Y ))<label>(64)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Implementation details</head><p>Here we describe how the nuisance parameters can be estimated in each of the decompositions. In general, density ratio models can be estimated via a standard reduction to a classification problem where a probabilistic classifier is trained to discriminate between source and target domains <ref type="bibr" target="#b34">(Sugiyama et al., 2007)</ref>.</p><p>Note on computation time. Shapley value computation can be parallelized over the subsets. For high-dimensional tabular data, grouping together variables can further reduce computation time (and increase interpretability).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1. Aggregate decompositions</head><p>Density ratio models. Using direct importance estimation <ref type="bibr" target="#b34">(Sugiyama et al., 2007)</ref>, density ratio models π 100 (W ) and π 110 (W, Z) can be estimated by fitting classifiers on the combined source and target data to predict D = 0 or 1 from features W and (W, Z), respectively.</p><p>Outcome models. The outcome models µ •00 (W ) and µ  <ref type="formula">4</ref>) can be estimated as follows. Create a second ("phantom") dataset of the target domain in which Z -s is independent of Z s by permuting the original Z -s in the target domain. Compute q bin for all observations in the original dataset and the permuted dataset. Concatenate the original dataset from the target domain with the permuted dataset. Train a classifier to predict if an observation is from the original versus the permuted dataset.</p><formula xml:id="formula_62">, Z -s , Q) = p 1 (Z -s |W, Z s , q(W, Z) = Q)/p 1 (Z -s ) in (</formula><p>Outcome models. The outcome models µ ••1 and µ ••s (W, Z) can be similarly fit by estimating the conditional distribution p 0 (Y |W, Z) and p s (Y |W, Z, q bin (W, Z)) on the target domain, and then taking expectation of the loss.</p><p>Computing U-statistics. Calculating the double average P Ev</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1,n</head><p>PEv 1,n in the estimator requires evaluating all n 2 pairs of data points in target domain. This can be computationally expensive, so a good approximation is to subsample the inner average. We take 2000 subsamples. We did not see large changes in the bias of the estimates compared to calculating the exact U-statistics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3. Detailed decomposition for s-partial conditional outcome shift</head><p>Density ratio models. The ratio π 1s0 (W, Z s ) = p 1 (W, Z s )/p 0 (W, Z s ) can be similarly fit using direct importance estimation.</p><p>Outcome models. We require the following models.</p><p>• µ •0-s0 (z s , w) = E •00 [ℓ|z s , w], defined in (7), can be estimated by regressing loss against w, z s on the source domain.</p><p>• µ •10 (w) = E •10 [ℓ|w], defined in (8), can be estimated by regressing µ ••0 (w, z) against w in the target domain.</p><p>• µ •s0 (w) = E •s0 [ℓ|z s , w], defined in (9), can be estimated by regressing µ •0-s0 (z s , w) against w in the target domain.</p><p>For all models, we use cross-validation to select among model types and hyperparameters. Model selection is important so that the convergence rate conditions for the asymptotic normality results are met.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Simulation details</head><p>Data generation: We generate synthetic data under two settings. For the coverage checks in Section 5.1, all features are sampled independently from a multivariate normal distribution. The mean of the (W, Z) in the source and target domains are (0, 2, 0.7, 3) and (0, 0, 0, 0), respectively. The outcome in the source and target domains are simulated from a logistic regression model with coefficients (0.3, 1, 0.5, 1) and (0.3, 0.1, 0.5, 1.4).</p><p>In the second setting for baseline comparisons in Figure <ref type="figure" target="#fig_6">4b</ref>, each feature in W ∈ R and Z ∈ R 5 is sampled independently from the rest from a uniform distribution over <ref type="bibr">[-1, 1)</ref>. The binary outcome Y is sampled from a logistic regression model with coefficients (0.2, 0.4, 2, 0.25, 0.1, 0.1) in source and (0.2, -0.4, 0.8, 0.1, 0.1, 0.1) in target.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Sample-splitting:</head><p>We fit all models on 80% of the data points from both source and target datasets which is the Tr partition, and keep the remaining 20% for computing the estimators which is the Ev partition.</p><p>Model types: We use scikit-learn implementations for all models <ref type="bibr" target="#b27">(Pedregosa et al., 2011)</ref>. We use 3-fold cross validation to select models. For density models, we fit random forest classifiers and logistic regression models with polynomial features of degree 3. Depending on whether the target outcome in outcome models is binary or real-valued, we fit random forest classifiers or regressors, and logistic regression or linear regression models with ridge penalty. Specific hyperparameter ranges for the grid search will be provided in the code, to be made publicly available.</p><p>Computing time and resources: Computation for the VI estimates can be quite fast, as Shapley value computation can be parallelized over the subsets and the number of unique variable subsets sampled in the Shapley value approximation is often quite small. For instance, for the ACS Public Coverage case study with 34 features, the unique subsets is 131 even when the number of sampled subsets is 3000, and it takes around 160 seconds to estimate the value of a single variable subset. All experiments are run on a 2.60 GHz processor with 8 CPU cores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Data analysis details</head><p>Synthetic. We describe accuracy of the ML algorithm after it is retrained with the top k features and predictions from the original model.</p><p>Hospital readmission. Using data from the electronic health records of a large safety-net hospital in the US, we analyzed the transferability of performance measures of a Gradient Boosted Tree (GBT) trained to predict 30-day readmission risk for the general patient population (source) but applied to patients diagnosed with heart failure (target). Each of the source and target datasets have 3750 observations for analyzing the performance gap. The GBT is trained on a held-out sample of 18,873 points from the general population. Features include 4 demographic variables (W ) and 16 diagnosis codes (Z).</p><p>While training, we reweigh samples by class weights to address class imbalance. ACS Public Coverage. We extract data from the American Community Survey (ACS) to predict whether a person has public health insurance. The data only contains persons of age less than 65 and having an income of less than $30,000. We analyze a neural network (MLP) trained on data from Nebraska (source) to data from Louisiana (target) given 3000 and 6000 observations from the source and target domains, respectively. Another 3300 from source for training the model. Figure <ref type="figure" target="#fig_12">7</ref> shows the detailed decomposition of conditional outcome shift for the dataset.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Hierarchical Decomposition of Performance Differences (HDPD): Aggregate decomposes a drop in model performance between two domains into that due to shifts in the covariate versus outcome distribution. Detailed quantifies the proportion of variation in accuracy changes explained by partial covariate/outcome shifts with respect to a variable or variable subset.</figDesc><graphic coords="2,93.43,61.39,155.52,85.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure2. Decomposition framework for explaining the performance gap from source domain (D = 0) to target domain (D = 1), visualized through directed acyclic graphs. Aggregate decompositions describe the incremental impact of replacing each aggregate variable's distribution at the source with that in the target, indicated by arrows from D. Detailed decompositions quantify how well hypothesized partial distribution shifts with respect to variable subsets Zs explain performance gaps. This work considers partial outcome shifts that fine-tune the risk in the source domain with respect to Zs (as indicated by the additional node Q = p0(Y = 1|W, Z)) and partial conditional covariate shifts when Zs → Z-s.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>is the performance gap observed at in strata W when p 0 (Z|W ) is replaced with p DZ (Z|W ). (Note that we overload the notation where D Z = 0 means source domain, D Z = 1 means target domain, and D Z = s means an s-partial shift.) Setting ϕ Z,0 = v Z (∅), we denote the corresponding Shapley values by {ϕ Z,j : j = 0, • • • , m 2 }. Similarly, for the conditional outcome decomposition, we define the importance of s as how well the hypothesized s-partial outcome shift explains the variation in performance gaps across strata (W, Z), i.e. v Y (s) := 1 -</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>dent and identically distributed (IID) observations from the source and target domains d = 0 and 1, respectively. Let a fixed fraction of the data be partitioned towards "training" (Tr) and the remaining to "evaluation" (Ev); let n Ev be the number of observations in the evaluation partition. Let P d denote the expectation with respect to domain d and P Ev d,n denote the empirical average over observations in partition Ev from domain d. All estimators are denoted using hat notation. Proofs, detailed theoretical results, and psuedocode are provided in the Appendix.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>PEv 1,n , which takes the average over all pairs of observations O i from the evaluation partition with replacement, i.e. 1 n 2 Ev nEv i=1 nEv j=1 g(O i , O j ) for some function g. Calculation of this estimator and its theoretical properties are as follows. Estimation. Using the training partition, estimate a density ratio model defined as π</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Coverage rates of 90% CIs for aggregate decomposition terms (left) and value of s-partial shifts for the conditional covariate (middle) and outcome shifts (right) across dataset sizes n.</figDesc><graphic coords="6,311.36,61.39,72.90,156.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Comparison of variable importance reported by proposed method HDPD versus existing methods for conditional covariate (a) and conditional outcome (b) terms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Conditional outcome. (Fig 4b) W and Z ∈ R 5 are simulated from the same distribution in both domains. Y is generated from a logistic regression model with coefficients for (W, Z 1 , • • • , Z 5 ) as (0.2, 0.4, 2, 0.25, 0.1, 0.1) in the source and (0.2, -0.4, 0.8, 0.1, 0.1, 0.1) in the target. Interestingly, none of the methods have the same ranking of the features. ParametricChange identifies Z 2 as having the largest shift on the logit scale, but this does not mean that it is the most important explanation for changes in the loss. According to our decomposition framework, Z 1 is actually the most important for explaining changes in model performance due to outcome shifts. Oaxaca-Blinder and ParametricAcc have odd behavior-Oaxaca-Blinder assigns Z 1 the lowest importance and ParametricAcc assigns Z 3 the highest importance), because the models are misspecified. The VI from RandomForestAcc is also difficult to interpret because it measures which variables are good predictors of performance, not performance shift.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Aggregate and detailed decompositions for the performance gaps of (a) a model predicting readmission risk across two patient populations (General→Heart Failure) and (b) a model predicting insurance coverage across US states (NE→LA). A subset of VI estimates is shown; see full list in Sec F in the Appendix.</figDesc><graphic coords="8,61.62,299.46,221.64,83.87" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>, Conditional covariates, Outcome f Prediction model being analyzed ℓ(W, Z, Y ) or ℓ Loss function e.g. 0-1 loss D = 0 and D = 1 Indicators for source and target domain p 0 , p 1 Probability density (or mass) function for the two domains D = 0, 1 E DWDZDY Expectation over the distribution p DW (W )p DZ (Z|W )p DY (Y |W, Z) Q := q(W, Z) Source domain risk at W, Z, i.e. p 0 (Y = 1|W, Z) Tr and Ev Training dataset used to fit models and evaluation dataset used to compute decompositions ϕ Z,j and ϕ Y,j Shapley values for variable j in the detailed decomposition of conditional covariate and outcome shifts v Z (s) and v Y (s) Value of a subset s for s-partial conditional covariate shift and s-partial outcome shift v num • (s) and v den • (s)Numerator and denominator of the ratio defined in the value of a subset Models µ • Outcome models for the conditional expectation of the loss across different settings Models π • Density ratio models for feature densities across datasets P Notation for expectation P Ev 0,n and P Ev 1,n sample average over source and target data in the evaluation dataset ψ(d, w, z, y)   </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>Positivity) p(w, z|d = 0) &gt; 0 almost everywhere, such that the density ratios π 110 (w, z) are well-defined and bounded. Let the nuisance parameters in the one-step estimator v den Y be denoted by η den Y = (µ ••0 , µ ••1 , π 110 ) and the set of estimated nuisances by ηden Y . Lemma C.6. Assuming Condition C.5 holds, then vden Y is an asymptotically linear estimator for v den Y , i.e.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Sample estimates and CIs for simulation from Section 5.1. Proposed is debiased ML estimator for HDPD.</figDesc><graphic coords="24,175.70,199.98,243.00,73.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Detailed decompositions for the performance gap of a model predicting insurance coverage prediction across two US states (NE → LA). Plot shows values for the full set of 31 covariates.</figDesc><graphic coords="25,158.67,126.49,277.06,220.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>At the detailed level, the framework outputs Shapley-based variable attributions that describe how shifts with respect to each variable contribute to each term in the aggregate decomposition. Based on the VI values, an</figDesc><table /><note><p>ML development team can better understand the underlying cause for a performance gap and brainstorm which mixture of operational interventions (e.g. changing data collection) and algorithmic interventions (e.g. retraining the model with respect to the variable(s)) would be most effective at</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>s-partial conditional covariate shift: We hypothesize that Z -s is downstream of Z s and define p s (z|w) as p 1 (z s |w)p 0 (z -s |z s , w), as illustrated in Fig 2 right top. A similar proposal was considered in<ref type="bibr" target="#b39">Wu et al. (2021)</ref>. For s = ∅, note that v Z (∅) = 0. Shifting the conditional distribution of Y only with respect to a variable subset Z s but not Z -s requires care. We cannot simply split Z into Z s and Z -s , and define the distribution of Y solely as a function of Z s and W , because defining p s (Y |W, Z) := p(Y |W, Z s ) for some p generally implies that p s</figDesc><table><row><cell>In the readmission example, such a shift would hypothesize</cell></row><row><cell>that certain diagnosis codes in Z s are upstream of the oth-</cell></row><row><cell>ers. For instance, one could reasonably hypothesize that</cell></row><row><cell>diagnosis codes that describe family history of disease are</cell></row><row><cell>upstream of the other diagnosis codes. Similarly, medical</cell></row><row><cell>treatments/procedures are likely downstream of a patient's</cell></row><row><cell>baseline characteristics and comorbidities.</cell></row></table><note><p>s-partial conditional outcome shift:</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>• • • , m 2 are an additive combination of the value functions that quantify the proportion of variability explained by s-partial shifts. Below, we present novel estimators for values {v</figDesc><table /><note><p><p>Y (s) : s ⊆ {1, • • • , m 2 }} for s-partial conditional outcome shifts, their theoretical properties, and computationally efficient estimation for their corresponding Shapley values. Due to space constraints, results pertaining to partial conditional covariate shifts are provided in Section B.1 of the Appendix.</p>4.2.1. VALUE OF s-PARTIAL OUTCOME SHIFTS</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 .</head><label>1</label><figDesc>Comparison to prior works that decompose performance change.</figDesc><table><row><cell></cell><cell></cell><cell>Aggregate</cell><cell></cell><cell>Detailed decomp.</cell><cell>Does not re-</cell><cell>Confidence</cell><cell>Nonparametric</cell></row><row><cell></cell><cell></cell><cell>decomp.</cell><cell></cell><cell></cell><cell>quire knowing</cell><cell>intervals</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>causal DAG</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Cond. covariate Cond. outcome</cell><cell></cell><cell></cell></row><row><cell cols="2">Zhang et al. (2023)</cell><cell>✓</cell><cell>✓</cell><cell></cell><cell></cell><cell></cell><cell>✓</cell></row><row><cell cols="2">Cai et al. (2023)</cell><cell>✓</cell><cell></cell><cell></cell><cell>✓</cell><cell>✓</cell><cell>✓</cell></row><row><cell cols="2">Wu et al. (2021)</cell><cell></cell><cell>✓</cell><cell></cell><cell>✓</cell><cell></cell><cell>✓</cell></row><row><cell cols="2">Liu et al. (2023)</cell><cell>✓</cell><cell></cell><cell>✓</cell><cell>✓</cell><cell></cell><cell>✓</cell></row><row><cell cols="2">Dodd &amp; Pepe</cell><cell></cell><cell></cell><cell>✓</cell><cell>✓</cell><cell>✓</cell></row><row><cell>(2003)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Oaxaca</cell><cell>(1973),</cell><cell>✓</cell><cell>✓</cell><cell>✓</cell><cell>✓</cell><cell>✓</cell></row><row><cell cols="2">Blinder (1973)</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">HDPD (this paper)</cell><cell>✓</cell><cell>✓</cell><cell>✓</cell><cell>✓</cell><cell>✓</cell><cell>✓</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 .</head><label>2</label><figDesc>Notation Algorithm 1 Aggregate decompositions into baseline, conditional covariate, and conditional outcome shifts Input: Source and target data {(W )} n d i=1 for d ∈ {0, 1}, loss function ℓ(W, Z, Y ; f ). Output: Performance change due to baseline, conditional covariate, and conditional outcome shifts Λ W , Λ Z , Λ Y . 1 Split source and target data into training Tr and evaluation Ev partitions. Let n Ev be the total number of data points in the Ev partition.</figDesc><table><row><cell>(d) i , Z i , Y (d)</cell><cell>(d)</cell></row></table><note><p>i</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>5</head><label></label><figDesc>Compute α-level confidence intervals as ΛN ± z 1-α/2 var(ψ N (d, w, z, y; ηN ))/n Ev for N ∈ {W, Z, Y}, where z is the inverse CDF of the standard normal distribution. 6 return ΛW , ΛZ , ΛY and confidence intervals Algorithm 2 VALUECONDITIONALOUTCOME(S): Value for s-partial conditional outcome shift for a subset s Input: Training Tr and evaluation Ev partitions of source and target data, subset of variables s. Output: Value for s-partial conditional outcome shift for subset s. Algorithm 3 VALUECONDITIONALCOVARIATE(S): Value for s-partial conditional covariate shift for a subset s Input: Training Tr and evaluation Ev partitions of source and target data, subset of variables s. Output: Value for s-partial conditional covariate shift for subset s. 1 Fit nuisance parameters η num Z,s , defined in Sections C.2, on the Tr partition, as outlined in D.3. Theorem B.2. For variable subset s, suppose v den Z (s) &gt; 0 and Condition B.1 hold. Then the estimator vZ (s) is asymptotically normal.</figDesc><table /><note><p>1 Fit nuisance parameters η num Y,s , η den Y , defined in Sections C.3, on the Tr partitions as outlined in D.2. 2 Estimate v Y (s) by vnum Y (s)/v den Y where vnum Y (s) is estimated using (4) and vden Y is estimated using (5) on the Ev partition. 3 Estimate variance of influence function ψ Y,bin,s (d, w, z, y; ηnum Y,s , ηden Y ) as defined in (63). 4 Compute α-level confidence interval as vY (s) ± z 1-α/2 var(ψ Y,bin,s (d, w, z, y; ηnum Y,s , ηden Y ))/n Ev . 5 return vY (s) and confidence interval</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>).</figDesc><table><row><cell>Lemma C.4. Assuming Condition C.3 holds, vnum Y,bin is an asymptotically linear estimator for v num Y,bin , i.e.</cell></row><row><cell>vnum Y,bin</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head></head><label></label><figDesc>, Y, W , Z, Ỹ ; ηnum Y,s . =P 1,n P1,n h sym W, Z, Y, W , Z, Ỹ ; ηnum</figDesc><table><row><cell>vnum Y,bin (s) -v num Y,bin (s) =P 1,n P1,n h sym W, Z, Y, W , Z, Ỹ ; ηnum Y,s</cell><cell>-P 1 P1 h sym W, Z, Y, W , Z, Ỹ ; η num Y,s</cell><cell></cell></row><row><cell>Y,s</cell><cell>-P 1,n h sym,1 X, Y ; ηnum Y,s + vnum Y (s)</cell><cell>(38)</cell></row><row><cell>Consider the decomposition</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head></head><label></label><figDesc>Term (38): SupposeP 1 h 2 sym (W, Z, Y, W , Z, Ỹ ; ηnum Y,s ) &lt; ∞.Via a straightforward extension of the proof in Theorem 12.3 in van der Vaart (1998), one can show that var P 1,n P1,n h sym W, Z, Y, W , Z, Ỹ ; ηnum</figDesc><table><row><cell>Y,s</cell><cell></cell></row><row><cell>Y,s var P 1,n h sym,1 W, Z, Y ; ηnum</cell><cell>→ p 1.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head></head><label></label><figDesc>••0 (W, Z) can be fit in a number of ways. One option is to estimate the conditional distribution of the outcome (i.e. p 0 (Y |W ) or p 0 (Y |W, Z)) using binary classifiers, from which one can obtain an estimate of the conditional expectation of the loss. Alternatively, one can estimate the conditional expectations of the loss directly by fitting regression models.</figDesc><table><row><cell>D.2. Detailed decomposition for s-partial outcome shift</cell></row><row><cell>Density ratio models. The density ratio π(W, Z s</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 3 .</head><label>3</label><figDesc>Difference in AUCs between the revised insurance prediction model with respect to the top k variables identified by the proposed versus RandomForestAcc procedures (Diff AUC-k = Proposed -RandomForestAcc). 95% CIs are shown.</figDesc><table><row><cell cols="4">k Diff AUC-k Lower CI Upper CI</cell></row><row><cell>1</cell><cell>0.000</cell><cell>0.000</cell><cell>0.000</cell></row><row><cell>2</cell><cell>0.006</cell><cell>0.001</cell><cell>0.010</cell></row><row><cell>3</cell><cell>-0.002</cell><cell>-0.007</cell><cell>0.002</cell></row><row><cell>4</cell><cell>0.004</cell><cell>-0.002</cell><cell>0.008</cell></row><row><cell>5</cell><cell>-0.001</cell><cell>-0.006</cell><cell>0.003</cell></row><row><cell>6</cell><cell>-0.002</cell><cell>-0.008</cell><cell>0.003</cell></row><row><cell>7</cell><cell>0.007</cell><cell>0.002</cell><cell>0.011</cell></row><row><cell>8</cell><cell>0.006</cell><cell>0.001</cell><cell>0.010</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>Estimate v Z (s) by vnum Z (s)/v num Z (∅) using (10) on the Ev partition.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>Estimate variance of influence function ψ Z,s (d, w, z, y; ηnum Z,s ) as defined in (30).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>Compute α-level confidence interval as vZ (s) ± z 1-α/2 var(ψ Z,s (d, w, z, y; ηnum Z,s ))/n Ev .</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_3"><p>return vZ (s) and confidence interval</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_4"><p>1(D = 1)</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We would like to thank <rs type="person">Lucas Zier</rs>, <rs type="person">Avni Kothari</rs>, <rs type="person">Berkman Sahiner</rs>, <rs type="person">Nicholas Petrick</rs>, <rs type="person">Gene Pennello</rs>, <rs type="person">Mi-Ok Kim</rs>, and <rs type="person">Romain Pirracchio</rs> for their invaluable feedback on the work. This work was funded through a <rs type="grantName">Patient-Centered Outcomes Research Institute® (PCORI®) Award</rs> (<rs type="grantNumber">ME-2022C1-25619</rs>). The views presented in this work are solely the responsibility of the author(s) and do not necessarily represent the views of the PCORI®, its Board of Governors or Methodology Committee, and the <rs type="funder">Food and Drug Administration</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_JZ3cRXt">
					<idno type="grant-number">ME-2022C1-25619</idno>
					<orgName type="grant-name">Patient-Centered Outcomes Research Institute® (PCORI®) Award</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>3 Estimate v Y (s) ← VALUECONDITIONALOUTCOME(S) and v Z (s) ← VALUECONDITIONALCOVARIATE(S) for</p><p>4 Get estimated Shapley values {ϕ Y,j } and {ϕ Z,j } by solving constrained linear regression problems in (7) in <ref type="bibr" target="#b37">Williamson &amp; Feng (2020)</ref> with value functions v Y (s) and v Z (s), respectively.</p><p>5 Compute confidence intervals based on the influence functions defined in Theorem 1 in <ref type="bibr" target="#b37">Williamson &amp; Feng (2020)</ref>.</p><p>6 return Shapley values {ϕ Y,j : j = 0, . . . , m 2 } and {ϕ Z,j : j = 1, . . . , m 2 } and confidence intervals</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Estimation and Inference</head><p>B.1. Value of s-partial conditional covariate shifts Estimation. Using the training partition, estimate the density ratio π 1s0 (z s , w) = p 1 (z s , w)/p 0 (z s , w) and the outcome models</p><p>in addition to the other nuisance models previously mentioned. We propose the estimator vZ (s) = vnum Z (s)/v den Z , where</p><p>and vden</p><p>Inference. The estimator is asymptotically normal as long as the outcome and density ratio models are estimated at a fast enough rate defined formally as follows.</p><p>Condition B.1. For variable subset s, suppose the following holds</p><p>• (Positivity) p 0 (z s , w) &gt; 0 and p 0 (w, z) &gt; 0 almost everywhere, such that the density ratios π 1s0 (w, z s ) and π 110 <ref type="bibr">(w, z)</ref> are well-defined and between (0, 1).</p><p>Proof. The estimands Λ W , Λ Z , Λ Y have similarities to the standard average treatment effect (ATE) in the causal inference literature (see <ref type="bibr">(Kennedy, 2022, Example 2)</ref>. Hence, the estimators and their asymptotic properties directly follow. For treatment T , outcome O, and confounders C, the mean outcome under T = 1 among the population with T = 0 is identified as</p><p>and its one-step corrected estimator can be derived from the canonical gradient of ϕ, which takes the following form after plugging in the estimates of the nuisance models:</p><p>where µ 1 (c) = E[o|c, t = 1] and π(c) = p(c|t = 0)/p(c|t = 1) as long as the following conditions hold:</p><p>• p(c|t = 1) &gt; 0 almost everywhere such that the density ratios π(c) are well-defined and bounded,</p><p>•</p><p>We establish the estimators and their influence functions by showing that they can all be viewed as mean outcomes of the form ( <ref type="formula">16</ref>).</p><p>Baseline term Λ W . The first term E 100 [ℓ(w, z, y)] is a mean outcome with respect to p(ℓ(w, z, y)|w, d = 0)p(w|d = 1), which is the same as that in ( <ref type="formula">16</ref>) but with ℓ(w, z, y) as the outcome, w as the confounder, and d as the (flipped) treatment.</p><p>The second term E 000 [ℓ(w, z, y)] is a simple average over D = 0 population whose influence function is the ℓ(w, z, y) itself.</p><p>Conditional covariate term Λ Z . First term E 110 [ℓ(w, z, y)] is the mean outcome with respect to p(ℓ(w, z, y)|w, z, d = 0)p(w, z|d = 1), where the chief difference is (w, z) is the confounder. Second term E 100 [ℓ(w, z, y)] is also a mean outcome, as discussed above.</p><p>Conditional outcome term Λ Y . First term E 111 [ℓ(w, z, y)] is a simple average over the D = 1 population.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2. Value of s-partial conditional covariate shifts</head><p>Let nuisance parameters in the one-step estimator v num Z,s be denoted η num Z,s = (µ •s0 , µ •10 , µ •0-s0 , µ 001 , µ ••0 , π 1s0 , π 110 ) and the set of estimated nuisances by ηnum Z,s . The canonical gradient of v num Z (s) is</p><p>Proof. Consider the following decomposition</p><p>We note that (21) converges to a normal distribution per CLT assuming the variance of ψ num Z,s is finite. The empirical process term ( <ref type="formula">21</ref>) is asymptotically negligible, as the nuisance parameters η num Z,s are estimated using a separate training data split from the evaluation data and (Kennedy, 2022, Lemma 1) states that</p><p>as long as estimators for all nuisance parameters are consistent. We now establish that the remainder term ( <ref type="formula">22</ref>) is also asymptotically negligible. Integrating with respect to Y , we have that</p><p>We now show that the remainder term ( <ref type="formula">52</ref>) is also asymptotically negligible. Substituting the influence function and integrating with respect to Y , (52) becomes </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The lifecycle of a statistical model: Model failure detection, identification, and refitting</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cauchois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Duchi</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2202.04166" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fast learning rates for plug-in classifiers</title>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Audibert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">B</forename><surname>Tsybakov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Stat</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="608" to="633" />
			<date type="published" when="2007-04">April 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Wage discrimination: Reduced form and structural estimates</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Blinder</surname></persName>
		</author>
		<ptr target="http://www.jstor.org/stable/144855" />
	</analytic>
	<monogr>
		<title level="j">The Journal of Human Resources</title>
		<idno type="ISSN">0022166</idno>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="436" to="455" />
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Why did the distribution change?</title>
		<author>
			<persName><forename type="first">K</forename><surname>Budhathoki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bloebaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Ng</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="https://proceedings.mlr.press/v130/budhathoki21a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 24th International Conference on Artificial Intelligence and Statistics</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Banerjee</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Fukumizu</surname></persName>
		</editor>
		<meeting>The 24th International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2021-04">Apr 2021</date>
			<biblScope unit="volume">130</biblScope>
			<biblScope unit="page" from="13" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Diagnosing model performance under distribution shift</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">T</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Namkoong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yadlowsky</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/2303.02011" />
		<imprint>
			<date type="published" when="2023-03">March 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Extremal principle solutions of games in characteristic function form: Core, chebychev and shapley value generalizations</title>
		<author>
			<persName><forename type="first">A</forename><surname>Charnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Golany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Keane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rousseau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Econometrics of Planning and Efficiency</title>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Sengupta</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">K</forename><surname>Kadekodi</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="page" from="123" to="133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Double/debiased machine learning for treatment and structural parameters</title>
		<author>
			<persName><forename type="first">V</forename><surname>Chernozhukov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chetverikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Demirer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Duflo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Newey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Robins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econom. J</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="C68" />
			<date type="published" when="2018-02">February 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">External validation and comparison of a general ward deterioration index between diversely different health systems</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Cummings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Blackmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Motyka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Farzaneh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Bisco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Glassbrook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Roebuck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Gillies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Admon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Critical Care Medicine</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">775</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The spotlight: A general method for discovering systematic errors in deep learning models</title>
		<author>
			<persName><forename type="first">G</forename><surname>Eon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>D'eon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Leyton-Brown</surname></persName>
		</author>
		<idno type="DOI">10.1145/3531146.3533240</idno>
		<ptr target="https://doi.org/10.1145/3531146.3533240" />
	</analytic>
	<monogr>
		<title level="m">2022 ACM Conference on Fairness, Accountability, and Transparency, FAccT &apos;22</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1962" to="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Semiparametric regression for the area under the receiver operating characteristic curve</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Dodd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Pepe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Am. Stat. Assoc</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="issue">462</biblScope>
			<biblScope unit="page" from="409" to="417" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Discovering systematic errors with cross-modal embeddings</title>
		<author>
			<persName><forename type="first">S</forename><surname>Eyuboglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Varma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">K</forename><surname>Saab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-B</forename><surname>Delbrouck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lee-Messer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Dunnmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Re</surname></persName>
		</author>
		<author>
			<persName><surname>Domino</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=FPCMqjI0jXN" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">An extension of the blinder-oaxaca decomposition technique to logit and probit models</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">W</forename><surname>Fairlie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of economic and social measurement</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="305" to="316" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Decomposing wage distributions using recentered influence function regressions</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Firpo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Fortin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lemieux</surname></persName>
		</author>
		<idno type="DOI">10.3390/econometrics6020028</idno>
		<ptr target="https://www.mdpi.com/2225-1146/6/2/28" />
	</analytic>
	<monogr>
		<title level="j">Econometrics</title>
		<idno type="ISSN">2225-1146</idno>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Chapter 1decomposition methods in economics</title>
		<author>
			<persName><forename type="first">N</forename><surname>Fortin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lemieux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Firpo</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0169-7218(11)00407-2</idno>
		<ptr target="https://www.sciencedirect.com/science/article/pii/S0169721811004072" />
	</analytic>
	<monogr>
		<title level="m">of Handbook of Labor Economics</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="1" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">On calibration of modern neural networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1321" to="1330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Variable importance measures for heterogeneous causal effects</title>
		<author>
			<persName><forename type="first">O</forename><surname>Hines</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Diaz-Ordaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vansteelandt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022-04">April 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Variable importance measures for heterogeneous causal effects</title>
		<author>
			<persName><forename type="first">O</forename><surname>Hines</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Diaz-Ordaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vansteelandt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Distilling model failures as directions in latent space</title>
		<author>
			<persName><forename type="first">S</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Moitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=99RpBVpLiX" />
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Demystifying Double Robustness: A Comparison of Alternative Strategies for Estimating a Population Mean from Incomplete Data</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D Y</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Schafer</surname></persName>
		</author>
		<idno type="DOI">10.1214/07-STS227</idno>
		<ptr target="https://doi.org/10.1214/07-STS227" />
	</analytic>
	<monogr>
		<title level="j">Statistical Science</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="523" to="539" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Semiparametric doubly robust targeted double machine learning: a review</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Kennedy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022-03">March 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (TCAV)</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wexler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Viegas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="https://proceedings.mlr.press/v80/kim18d.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<title level="s">Proceedings of Machine Learning Research</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Dy</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Krause</surname></persName>
		</editor>
		<meeting>the 35th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018-07">Jul 2018</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="10" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Explaining racial and ethnic disparities in health care</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Kirby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Taliaferro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Zuvekas</surname></persName>
		</author>
		<ptr target="http://www.jstor.org/stable/3768359" />
	</analytic>
	<monogr>
		<title level="j">Medical Care</title>
		<idno type="ISSN">00257079</idno>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
	<note>):I64-I72</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">An introduction to domain adaptation and transfer learning</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">M</forename><surname>Kouw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Loog</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Towards explaining distribution shifts</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kulinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">I</forename><surname>Inouye</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="https://proceedings.mlr.press/v202/kulinski23a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Krause</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Brunskill</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Engelhardt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Sabato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Scarlett</surname></persName>
		</editor>
		<meeting>the 40th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2023-07">Jul 2023</date>
			<biblScope unit="volume">202</biblScope>
			<biblScope unit="page" from="23" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Feature shift detection: Localizing which features have shifted via conditional distribution tests</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kulinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bagchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">I</forename><surname>Inouye</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="19523" to="19533" />
		</imprint>
	</monogr>
	<note>cc/paper/2020/file/ e2d52448d36918c575fa79d88647ba66-Paper. pdf</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">On the need for a language describing distribution shifts: Illustrations on tabular datasets</title>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Namkoong</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023-07">July 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Male-female wage differentials in urban labor markets</title>
		<author>
			<persName><forename type="first">R</forename><surname>Oaxaca</surname></persName>
		</author>
		<ptr target="http://www.jstor.org/stable/2525981" />
	</analytic>
	<monogr>
		<title level="j">International Economic Review</title>
		<idno type="ISSN">00206598, 14682354</idno>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="693" to="709" />
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods</title>
		<author>
			<persName><forename type="first">J</forename><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in large margin classifiers</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="61" to="74" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Towards a more rigorous science of blindspot discovery in image classification models</title>
		<author>
			<persName><forename type="first">G</forename><surname>Plumb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cabrera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Talwalkar</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=MaDvbLaBiF.ExpertCertification" />
	</analytic>
	<monogr>
		<title level="j">Transactions on Machine Learning Research</title>
		<idno type="ISSN">2835- 8856</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Efficient and multiply robust risk estimation under general forms of dataset shift</title>
		<author>
			<persName><forename type="first">H</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">T</forename><surname>Tchetgen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Dobriban</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023-06">June 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Dataset shift in machine learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Quionero-Candela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Schwaighofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Failing loudly: An empirical study of methods for detecting dataset shift</title>
		<author>
			<persName><forename type="first">S</forename><surname>Rabanser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Günnemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lipton</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2019/file/846" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">H</forename><forename type="middle">W</forename><surname>Kuhn</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Tucker</surname></persName>
		</editor>
		<meeting><address><addrLine>Princeton</addrLine></address></meeting>
		<imprint>
			<publisher>Princeton University Press</publisher>
			<date type="published" when="1953">2019. December 1953</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="307" to="318" />
		</imprint>
	</monogr>
	<note>Contributions to the Theory of Games (AM-28)</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Clinical Prediction Models: A Practical Approach to Development, Validation, and Updating</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">W</forename><surname>Steyerberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Springer</publisher>
			<pubPlace>New York, NY</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Direct importance estimation with model selection and its application to covariate shift adaptation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nakajima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kashima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Buenau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kawanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Semiparametric Theory and Missing Data</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Tsiatis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Springer</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">Vaart</forename><surname>Van Der</surname></persName>
		</author>
		<title level="m">Asymptotic Statistics</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="1998-10">October 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Efficient nonparametric statistical inference on population feature importance using shapley values</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<ptr target="https://proceedings.icml.cc/static/paper_files/icml/2020/3042-Paper.pdf" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Nonparametric variable importance assessment using machine learning techniques</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">B</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Carone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Simon</surname></persName>
		</author>
		<idno type="DOI">10.1111/biom.13392</idno>
		<ptr target="https://onlinelibrary.wiley.com/doi/abs/10.1111/biom.13392" />
	</analytic>
	<monogr>
		<title level="j">Biometrics</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="9" to="22" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Explaining medical AI performance disparities across sites with confounder shapley value analysis</title>
		<author>
			<persName><forename type="first">E</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zou</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/2111.08168" />
		<imprint>
			<date type="published" when="2021-11">November 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Decomposing differences in the first moment</title>
		<author>
			<persName><forename type="first">M.-S</forename><surname>Yun</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.econlet.2003.09.008</idno>
		<ptr target="https://www.sciencedirect.com/science/article/pii/S0165176503002866" />
	</analytic>
	<monogr>
		<title level="j">Economics Letters</title>
		<idno type="ISSN">0165- 1765</idno>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="275" to="280" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Why did the model fail?&quot;: Attributing model performance changes to distribution shifts</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ghassemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Joshi</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="https://proceedings.mlr.press/v202/zhang23ai.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Krause</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Brunskill</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Engelhardt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Sabato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Scarlett</surname></persName>
		</editor>
		<meeting>the 40th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2023-07">Jul 2023</date>
			<biblScope unit="volume">202</biblScope>
			<biblScope unit="page" from="23" to="29" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
