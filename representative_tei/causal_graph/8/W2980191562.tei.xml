<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Estimating and Controlling the False Discovery Rate of the PC Algorithm Using Edge-Specific P-Values</title>
				<funder ref="#_J9Jck74">
					<orgName type="full">National Human Genome Research Institute</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2017-05-10">10 May 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Eric</forename><forename type="middle">V</forename><surname>Strobl</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Peter</forename><forename type="middle">L</forename><surname>Spirtes</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Biomedical Informatics</orgName>
								<orgName type="institution">University of Pittsburgh Pittsburgh</orgName>
								<address>
									<postCode>15206</postCode>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Philosophy Carnegie</orgName>
								<orgName type="institution">Mellon University Pittsburgh</orgName>
								<address>
									<postCode>15213</postCode>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Department of Biomedical Informatics</orgName>
								<orgName type="institution">University of Pittsburgh Pittsburgh</orgName>
								<address>
									<postCode>15206</postCode>
									<region>PA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Estimating and Controlling the False Discovery Rate of the PC Algorithm Using Edge-Specific P-Values</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2017-05-10">10 May 2017</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1607.03975v2[stat.ML]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>PC Algorithm</term>
					<term>Causal Inference</term>
					<term>False Discovery Rate</term>
					<term>Bayesian Network</term>
					<term>Directed Acyclic Graph</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The PC algorithm allows investigators to estimate a complete partially directed acyclic graph (CPDAG) from a finite dataset, but few groups have investigated strategies for estimating and controlling the false discovery rate (FDR) of the edges in the CPDAG. In this paper, we introduce PC with p-values (PC-p), a fast algorithm which robustly computes edge-specific p-values and then estimates and controls the FDR across the edges. PC-p specifically uses the p-values returned by many conditional independence (CI) tests to upper bound the p-values of more complex edge-specific hypothesis tests. The algorithm then estimates and controls the FDR using the bounded p-values and the Benjamini-Yekutieli FDR procedure. Modifications to the original PC algorithm also help PC-p accurately compute the upper bounds despite non-zero Type II error rates. Experiments show that PC-p yields more accurate FDR estimation and control across the edges in a variety of CPDAGs compared to alternative methods 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Discovering causal relationships is often much more important than discovering associational relationships in the sciences. As a result, the research community has been conducting extensive investigations into causal inference with the hope of developing practically useful algorithms to speed-up the scientific process. This research has resulted in a wide range of high performing algorithms over the years such as PC <ref type="bibr" target="#b22">(Spirtes et al., 2000)</ref>, FCI <ref type="bibr" target="#b21">(Spirtes et al., 1995</ref><ref type="bibr" target="#b22">(Spirtes et al., , 2000))</ref>, and CCD <ref type="bibr" target="#b19">(Richardson, 1996)</ref> The PC algorithm is currently one of the most popular methods for inferring causation from observational data. Given an observational dataset, the algorithm outputs a complete partially directed acyclic graph (CPDAG) which has helped some investigators elucidate important causal relationships in several domains. For example, the PC algorithm has been used to discover new causal relationships between genes and brain regions in biology <ref type="bibr" target="#b26">(Wu and Ye, 2006;</ref><ref type="bibr" target="#b15">Li et al., 2008;</ref><ref type="bibr" target="#b12">Joshi et al., 2010;</ref><ref type="bibr" target="#b23">Sun et al., 2012;</ref><ref type="bibr" target="#b9">Harris and Drton, 2013;</ref><ref type="bibr" target="#b11">Iyer et al., 2013;</ref><ref type="bibr" target="#b13">Le et al., 2013;</ref><ref type="bibr" target="#b24">Teramoto et al., 2014;</ref><ref type="bibr" target="#b8">Ha et al., 2015)</ref>. The algorithm has also been used to discover causal relations between corporate structures and strategies in economics <ref type="bibr" target="#b3">(Chong et al., 2009)</ref> as well as academic and musical achievements in psychology <ref type="bibr" target="#b18">(Mullensiefen et al., 2015)</ref>.</p><p>The increased use of PC in recent years has nonetheless led to growing concern about algorithm's confidence level in each edge of the CPDAG. For example, PC may have more confidence in the edge A -B but have less confidence in the edge B → C in the subgraph A-B → C of the CPDAG. Currently, PC alone does not output any edge-specific measure of confidence, even though scientists often must report measures of confidence such as p-values or confidence intervals in their scientific articles. This incongruency has resulted in the relatively slow adoption or even avoidance of the PC algorithm in the sciences, despite the algorithm's impressive capabilities in causal inference. Clearly then rectifying the problem by developing an edge-specific measure of confidence will increase the adoption of PC as well as hopefully ease the transition of ever-more complex causal inference algorithms into the scientific community.</p><p>We can of course consider multiple different ways of representing the confidence level in each edge of the CPDAG. However, we choose to pay special attention to the p-value, since it is by far the most popular notion of confidence in the sciences. Indeed, nearly all scientists report p-values in modern scientific reports because they rely on p-values to help justify their hypotheses. We therefore would ideally like to assign a p-value to each edge in the CPDAG as in Figure <ref type="figure">1a</ref> in order to best integrate the algorithm within a well-known framework. In this paper, we propose such a "causal p-value" in detail.</p><p>We however also believe that assigning p-values to each edge is not enough to ease the transition of PC into mainstream science, since the CPDAG actually contains many edges and therefore also represents a complicated multiple hypothesis testing problem. Fortunately, the problem of multiple hypothesis testing has a long history, as scientists have often required the results of multiple hypothesis tests in order to answer complex scientific questions. Currently, a standard approach to tackling the multiple hypothesis testing problem involves controlling the proportion of false positives among the rejected null hypotheses, or the false discovery rate (FDR), by using an FDR controlling procedure that takes a desired FDR level q and a set of p-values as input. The procedure then outputs a corresponding significance level α * for the set of p-values. An investigator subsequently rejects the null hypotheses for those tests with p-values that fall below α * in order to ensure that the expected FDR does not exceed q. For example, consider the set of p-values {0.02, 0.01, 0.03} and suppose that the FDR controlling procedure with q = 0.1 outputs α * = 0.019. Then, rejecting the null hypotheses of the first and third hypothesis tests guarantees that the expected FDR does not exceed 10%. Several other FDR controlling strategies also exist, but the ease of use, speed and accuracy of the above method have made it the most widely adopted strategy in the last two decades.</p><p>We would therefore like to control the FDR in the edges of the CPDAG using an FDR controlling procedure like in Figure <ref type="figure">1b</ref>. As a first idea, one may wonder whether an investigator can control the FDR in the CPDAG by simply feeding in all of the CI test p-values computed by PC into an FDR controlling procedure. Unfortunately, this approach fails for at least two reasons. First, it is unclear how to use the p-values which exceed the α * cut-off to reject edges in the CPDAG, since one would first need to elucidate the correspondence between the p-values and edges. Second, even if one could solve this problem, the strategy may only loosely bound the expected FDR. An accurate FDR controlling procedure should instead take into account the specific computations executed by PC in order to identify a sharp bound. The p-value based approach therefore necessitates a more fine-grained strategy which has thus far remained undiscovered.</p><p>Several groups have nonetheless attempted to control the FDR in the CPDAG by avoiding the complicated nature of the above problem with a different, data re-sampling approach. For example, Friedman and colleagues proposed to estimate the FDR by using the parametric bootstrap <ref type="bibr" target="#b6">(Friedman et al., 1999)</ref>. This procedure involves first learning a causal graph with the PC algorithm. The procedure then generates data from the causal graph and re-applies the PC algorithm multiple times on each generated dataset to estimate the FDR using the learnt causal graphs. An investigator can subsequently control the FDR by repeating the above process with different α values until he or she reaches the desired FDR level q. However, notice that the method requires multiples calls to PC and can therefore require too much time with high dimensional data. The procedure also requires parametric knowledge about the underlying distribution which limits the applicability of the method to simple cases. Fortuitously, two groups later proposed a permutation-based method which drops the parametric assumption <ref type="bibr" target="#b16">(Listgarten and Heckerman, 2007;</ref><ref type="bibr" target="#b1">Armen and Tsamardinos, 2014)</ref>. The permutation method nevertheless also requires multiple calls Figure <ref type="figure">1</ref>: We seek to associate edge-specific p-values to the output of the PC algorithm such as in (a). The PC algorithm currently does not associate such p-values with its output. We would also like to control the FDR of the edges. In (b), we set the FDR to 0.1 and obtained a α * cutoff of 0.031 for the output in (a), so we eliminated the edge between B and D because its p-value exceeds α * .</p><p>to an algorithm and in fact only applies to the parts of PC which can be decomposed into independent searches for the parents of each vertex; this has thus far limited the applicability of the method to adjacency discovery with local to global discovery algorithms (e.g., MMHC) and incomplete edge orientation. We conclude that both the bootstrap and permutation approaches to FDR estimation and control are either incomplete or too time consuming.</p><p>Another class of methods fortunately attempts to control the FDR without resampling procedures by instead using a standard FDR controlling procedure with bounded p-values. For instance, one method proposed in <ref type="bibr" target="#b25">(Tsamardinos and Brown, 2008)</ref> and then refined in <ref type="bibr">(Armen and</ref><ref type="bibr">Tsamardinos, 2011, 2014)</ref> assigns a p-value to each adjacency by taking the maximum over all of the significant p-values from the associated CI tests executed by PC. The method then controls the FDR in the estimated adjacencies by applying an FDR controlling procedure, such as the one proposed by Benjamini and Yekutieli (BY) <ref type="bibr" target="#b2">(Benjamini and Yekutieli, 2001)</ref>, on the edge-specific p-values. Under faithfulness and a zero Type II error rate, the method controls the FDR across the estimated adjacencies, or the estimated skeleton <ref type="bibr" target="#b1">(Armen and Tsamardinos, 2014)</ref>. This two stage method also performs comparably with the one stage method proposed in <ref type="bibr" target="#b15">(Li et al., 2008;</ref><ref type="bibr" target="#b14">Li and Wang, 2009)</ref>, which controls the FDR during, as opposed to after, the execution of the skeleton discovery phase of the PC algorithm. Of course, the Type II error rate never reaches zero in practice but researchers have also investigated a strategy for reducing the realized Type II error rate by introducing a heuristic reliability criterion for CI tests when dealing with discrete data <ref type="bibr" target="#b1">(Armen and Tsamardinos, 2014)</ref>. Experiments have shown that these methods finish in a relatively short amount of time and perform well in practice. However, the methods are also incomplete because they only apply to the skeleton discovery phase of PC.</p><p>In this report, we build on the previous outstanding work for deriving p-values for adjacencies by contributing a sound, complete and fast algorithm called PC with p-values (PC-p) which appropriately combines the p-values of PC's CI tests and then uses the BY FDR controlling procedure to accurately control the FDR in a CPDAG. The method relies on two upper bounds of the p-value that relate to logical conjunctions and disjunctions as described in Section 3. These upper bounds allow us to formulate several hypothesis tests for recovering the skeleton, discovering unshielded v-structures, and orienting additional edges as presented in Section 4. Accurately estimating the p-values of the hypothesis tests nonetheless requires a modified version of PC called PC-p which we propose in Section 5. Finally, we provide experimental results in Section 6 which show that PC-p's p-value estimates yield accurate estimates of the FDR with the BY procedure and improve upon alternative methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Preliminaries</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Causal graphs</head><p>A causal graph consists of vertices representing variables and edges representing causal relationships between any two variables. In this paper, we will use the terms "vertices" and "variables" interchangeably. Directed graphs are graphs where two distinct vertices can be connected by edges "→" and "←." We only consider simple graphs in this paper, or graphs with no edges originating from and connecting to the same vertex. Directed acyclic graphs (DAGs) are directed graphs without directed cycles. We say that X and Y are adjacent if they are connected by an edge independent of the edge's direction. A path p from X to Y is a set of consecutive edges (also independent of their direction) from X to Y such that no vertex is visited more than once. Given a path between two vertices X and Y with a middle vertex Z, the path is a</p><formula xml:id="formula_0">chain if X → Y → Z, a fork if X ← Y → Z, and a v-structure if X → Y ← Z. We refer to Y as a collider, if it is the middle vertex in a v-structure. A v-structure is called an unshielded v-structure if X → Y ← Z, but X and Z are non-adjacent. A directed path from X to Y is a set of consecutive edges with direction.</formula><p>We say that X is an ancestor of Y (and Y is a descendant of X), if there exists a directed path from X to Y .</p><p>If G is a directed graph in which X, Y and Z are disjoint sets of vertices, then X and Y are d-connected by Z in G if and only if there exists an undirected path p between some vertex in X and some vertex in Y such that, for every collider C on p, either C or a descendant of C is in Z, and no non-collider on p is in Z. On the other hand, X and Y are d-separated by Z in G if and only if they are not d-connected by Z in G. Next, the joint probability distribution P over variables X satisfies the global directed Markov property for a directed graph G if and only if, for any three disjoint subsets of variables A, B and C from X, if A and B are d-separated given C in G, then A and B are conditionally independent given C in P. We refer to the converse of the global directed Markov property as d-separation faithfulness; that is, if A and B are conditionally independent given C in P, then A and B are d-separated given C in G.</p><p>A Markov equivalence class of DAGs refers to a set of DAGs which entail the same conditional independencies. A complete partially directed acyclic graph (CPDAG) is a partially directed acyclic graph with the following properties: (1) each directed edge exists in every DAG in the Markov equivalence class, and (2) there exists a DAG with X → Y and a DAG with X ← Y in the Markov equivalence class for every undirected edge X -Y . A CPDAG G C represents a DAG G, if G belongs to the Markov equivalence class described by G C . We will occasionally use the meta-symbol "•" at the endpoint(s) of an edge to denote the presence or absence of an arrowhead. For example, the edge " -• " may denote either "-" or "→".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">The PC Algorithm</head><p>The PC algorithm is comprised of three stages. We have summarized these stages as pseudocode in Algorithms 5, 6 and 7 in Section A.1 of the Appendix. The first stage estimates the adjacencies of G, or the skeleton of G. Starting with a fully connected skeleton, the algorithm attempts to eliminate the adjacency between any two variables, say A and B, by testing if A and B are conditionally independent given some subset of the neighbors of A or the neighbors of B. The search is performed progressively, whereby the algorithm increases the size of the conditioning set starting from zero using a step size of 1. The edge between A and B is removed, if A and B are rendered conditionally independent given some subset of the neighbors of A or the neighbors of B.</p><p>The PC algorithm orients unshielded colliders in its second stage. Specifically, PC finds triples A, B, C such that A -B -C, but A and C are non-adjacent. The algorithm then determines whether B is contained in the set which rendered A and C conditionally independent in the first stage of PC.</p><formula xml:id="formula_1">If not, A -B -C is replaced with A → B ← C.</formula><p>The third and final stage of PC involves the repetitive application of three rules to orient as many of the remaining undirected edges as possible. The three rules include:</p><p>1. If A -B, C → A and C and B are non-adjacent, then replace A -B with (1)</p><formula xml:id="formula_2">A → B. 2. If A -B and A → C → B, then replace A -B with A → B. 3. If A -B, A -C → B, A -D → B,</formula><p>Overall, the PC algorithm has been shown to be complete in the sense that it finds and then orients edges up to G C , a CPDAG that represents G <ref type="bibr" target="#b17">(Meek, 1995)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Hypothesis Testing</head><p>A hypothesis test is a method of statistical inference usually composed of one null (H 0 ) and one alternative (H 1 ) hypothesis which are mutually exclusive; that is, if one occurs, then the other cannot occur. The null hypothesis refers to the default position which asserts that whatever one is trying to statistically infer actually did not happen. Note that the null and alternative do not necessarily need to be logical complements of each other. For example, one may be interested in determining whether the parameter µ is greater than zero. In this case, the null can be defined as µ = 0 while the alternative can be defined as µ &gt; 0 instead of µ = 0.</p><p>A Type I error is the incorrect rejection of a true null hypothesis, or a false positive. On the other hand, a Type II error is the failure to reject a false null hypothesis, or a false negative. The p-value (p) is the probability of the Type I error, or the Type I error rate. More specifically, the p-value is the probability of obtaining a result equal to or more extreme than the observed value under the assumption of the null hypothesis. The null hypothesis is thus rejected when the p-value is at or below a predefined α threshold (typically the α threshold is set to 0.05), because a low p-value demonstrates the improbability of the null hypothesis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">False Discovery Rate</head><p>Multiple comparisons or multiple hypothesis testing refers to the process of considering more than one statistical inference simultaneously. Failure to compensate for multiple comparisons can result in erroneous inferences. For example, if an investigator performs one hypothesis test with an α threshold of 0.05, then he or she has only a 5% chance of making a Type I error. However, if the investigator performs 100 independent tests with the same α threshold, then he or she has a 1 -(1 -0.05) 100 = 99.4% chance of making a Type I error on at least one test.</p><p>In multiple hypothesis testing, the false discovery rate (FDR) at threshold α is the expected proportion of false positives among the rejected null hypotheses. Specifically, we define the FDR at α as follows:</p><formula xml:id="formula_3">F DR(α) E V max{R, 1}</formula><p>, where V is the number of false positives, R is the total number of null hypotheses rejected, and max{R, 1} ensures that F DR(α) is well-defined when R = 0. We define the realized FDR at α as V / max{R, 1}. FDR estimation, or conservative point estimation of the FDR, refers to the process of estimating F DR(α) in a conservative manner such that:</p><formula xml:id="formula_4">E[ F DR(α)] ≥ F DR(α),</formula><p>where F DR(α) represents an estimate of F DR(α). We denote E[ F DR(α)]-F DR(α) as the estimation bias. Note that there are several ways of obtaining F DR(α). In 2001, Benjamini and Yekutieli proposed the following FDR estimator for m hypothesis tests:</p><formula xml:id="formula_5">F DR BY (α) mα m i=1 1 i max{R, 1} .<label>(2)</label></formula><p>FDR estimators such as F DR BY can be used to define FDR controlling procedures. These procedures determine the optimal threshold α * which achieves strong control 2 of the FDR in the following sense:</p><formula xml:id="formula_6">α * arg max α { F DR(α) ≤ q}<label>(3)</label></formula><p>The FDR controlling procedure based on F DR involves the rejection of all null hypotheses with p-values below the α * threshold. We refer to the quantity F DR(α * ) -q as the control bias. Benjamini and Yekutieli proved that the estimate F DR BY in particular achieves strong control of the FDR with any form of dependence among the p-values of m hypothesis tests.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Upper Bounds on the P-Value</head><p>We present two upper bounds of the Type I error rate of hypothesis tests which can be constructed using a set of simpler hypothesis tests. These upper bounds will serve as useful tools in Section 4 for bounding the Type I error rate of the hypothesis tests which will be used to infer the presence or absence of edges in a CPDAG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Union Bound</head><p>Consider the following hypothesis test for two random variables given a conditioning set:</p><formula xml:id="formula_7">H 0 : Conditionally independent,</formula><p>H 1 : Conditionally dependent.</p><p>2. Strong control of the FDR refers the process of controlling the FDR under any configuration of true and false null hypotheses; on the other hand, weak control refers to the process of controlling the FDR when all of the null hypotheses are true. Strong control is therefore preferable to weak control. (4)</p><p>From here on, we write Pr(CI test i outputs dependent | CI oracle i outputs independent) to denote Pr(| s i | ≥ s α i s i = 0), where s i refers to a parameter of some standardized distribution used by CI test i, s i a random variable and the test statistic estimating s i , and s α i a value of s i determined by an α level. We provide an example in Figure <ref type="figure">2</ref>, where s i may correspond to Fisher's z-statistic in the case of Fisher's z-test for the mean parameter s i = 0 of the standard normal distribution.</p><p>We now bound the Type I error rate of the hypothesis test (4) by using the new notation and the union bound: </p><p>where p i denotes the Type I error rate of CI test i. Thus, if the r.h.s. of ( <ref type="formula" target="#formula_8">5</ref>) is less than the α threshold, then we can conclude that the Type I error rate of (4) is also below the threshold. In other words, ( <ref type="formula" target="#formula_8">5</ref>) is a conservative p-value. Note that the third equality in the derivation of (5) uses the simplifying assumption that the probability of the output of CI test i only depends on the output of CI oracle i when given the outputs of all CI oracles. Several papers have used this assumption implicitly in their proofs <ref type="bibr" target="#b25">(Tsamardinos and Brown, 2008;</ref><ref type="bibr" target="#b14">Li and Wang, 2009)</ref>, and we will also use it throughout this paper. We can justify the assumption based on three facts. First, most CI test statistics s i have a limiting distribution which only depends on s i = 0 under the null. For example, Fisher's z-statistic has a limiting standard normal distribution with mean parameter z i = 0 and constant variance. Moreover, the G-statistic for the Gtest has a limiting χ 2 -distribution with non-centrality parameter g i = 0 and degrees of freedom determined by the number of cells in the contingency table. Second, existing methods which utilize bounds based on the assumption have strong empirical performance; loose-enough bounds therefore appear to accommodate the assumption well in most finite sample cases. Third, recall that simplifying assumptions are not new in the causality literature; indeed, many authors have made simplifying assumptions regarding parameter independence for Bayesian methods which similarly increase computational efficiency and achieve strong empirical performance (e.g., <ref type="bibr" target="#b5">(Cooper and Yoo, 1999)</ref>).</p><p>We can now also generalize the bound in (5) to any hypothesis test consisting of a series of logical disjunctions in the alternative and a series of logical conjunctions in the null. Namely:</p><formula xml:id="formula_9">H 0 : m i=1 oracle i outputs ¬P i , H 1 : m i=1 oracle i outputs P i ,<label>(6)</label></formula><p>where P i denotes an arbitrary output of oracle i. We now have:</p><formula xml:id="formula_10">Pr(Type I error) = Pr( m i=1 test i outputs P i |H 0 ) ≤ m i=1 Pr(test i outputs P i |H 0 ) = m i=1 Pr(test i outputs P i |oracle i outputs ¬P i ) = m i=1 h i ,</formula><p>where h i is the Type I error rate of test i, and the second equality uses the assumption that the probability of the output of test i only depends on the output of oracle i when given all oracles. We will use this generalization in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Intersection Bound</head><p>Suppose we want to perform a hypothesis test with the following null and alternative which are different than the null and alternative in (4): H 0 : At least one CI oracle outputs independent, H 1 : All CI oracles output dependent.</p><p>(7)</p><p>Now assume we know that the i th CI oracle outputs independent. We can then bound the Type I error rate of (7) as follows with m queries to the CI oracle:</p><p>Pr(Type I error) = Pr(all m CI tests output dependent|H 0 )</p><formula xml:id="formula_11">≤ Pr(CI test i outputs dependent |H 0 ) = Pr(CI test i outputs dependent | CI oracle i outputs independent ∧ other CI oracles may output independent) = Pr(CI test i outputs dependent | CI oracle i outputs independent) = p i ,<label>(8)</label></formula><p>where the second equality again holds under the assumption that the probability of the output of CI test i only depends on the output of CI oracle i when given the outputs of all CI oracles. We can therefore bound the Type I error rate of (7) using the p-value of a single CI test for which the CI oracle outputs independent. Nevertheless, in practice, we often do not know for which query the oracle outputs independent in the null. We do however know that at least one unknown CI oracle i outputs independent, so we can bound the Type I error rate of (7) using the maximum over all of the m CI test p-values:</p><formula xml:id="formula_12">Pr(Type I error) = Pr(all m CI tests output dependent|H 0 ) ≤ Pr(CI test i outputs dependent | CI oracle i outputs independent) = p i ≤ max j=1,...,m p j .<label>(9)</label></formula><p>Note that we can generalize the above bound to any hypothesis test consisting of a series of logical conjunctions in the alternative and a series of logical disjunctions in the null. Namely:</p><formula xml:id="formula_13">H 0 : m i=1 oracle i outputs ¬P i , H 1 : m i=1 oracle i outputs P i . (10)</formula><p>We therefore have:</p><formula xml:id="formula_14">Pr(Type I error) = Pr( m i=1 test i outputs P i |H 0 ) ≤ Pr(test i outputs P i |H 0 ) = Pr(test i outputs P i |oracle i outputs ¬P i ) = h i ≤ max j=1,...,m h j ,<label>(11)</label></formula><p>where the second equality again uses the assumption that the probability of the output of test i only depends on the output of oracle i when given all oracles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Edge-Specific Hypothesis Tests</head><p>We now show how to apply the two upper bounds of the Type I error rate to derive p-value estimates for both the undirected and directed edges in the CPDAG as estimated by PC.</p><p>Bounding the p-value for each edge therefore amounts to adding up and/or maximizing over the p-values returned from multiple CI tests.</p><p>Note that we will sometimes invoke a zero Type II error rate assumption in this section. This assumption is necessary to correctly upper bound the p-values of the edge-specific hypothesis tests of the CPDAG according to the CI tests executed by the PC algorithm. In fact, we can always correctly bound the p-values, if we perform all of the possible CI tests between the considered variables; however, this approach is impractical, since it ignores the efficiencies of the PC algorithm. A more interesting strategy involves designing the edge-specific hypothesis tests so that the p-value bounds are robust to Type II errors as well as redesigning the PC algorithm to catch many Type II errors. We will discuss these approaches in detail in Sections 4.4 and 5, so we encourage readers to accept the zero Type II error rate assumption for now.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Skeleton Discovery</head><p>We first consider the skeleton discovery phase of the PC algorithm. We wish to test whether each edge is absent in the true skeleton starting from a completely connected undirected graph. This problem has already been investigated in <ref type="bibr" target="#b15">(Li et al., 2008;</ref><ref type="bibr" target="#b25">Tsamardinos and Brown, 2008;</ref><ref type="bibr" target="#b14">Li and Wang, 2009;</ref><ref type="bibr">Armen and</ref><ref type="bibr">Tsamardinos, 2011, 2014)</ref>, but we review it here for completeness. We construct a hypothesis test with the following null and alterna-tive:</p><formula xml:id="formula_15">H 0 : A -B is absent, H 1 : A -B is present. (12)</formula><p>Now consider the following proposition, where P a(A) denotes the true parents of A:</p><p>Proposition 1 <ref type="bibr" target="#b22">(Spirtes et al., 2000)</ref> Consider a DAG G which satisfies the global directed Markov property. Moreover, assume that the probability distribution is d-separation faithful.</p><p>Then, there is an edge between two vertices A and B if and only if A and B are conditionally dependent given any subset of P a(A) \ B and any subset of P a(B) \ A.</p><p>We thus consider the following two scenarios for the undirected edge A -B:</p><p>1. If A and B are conditionally independent given some subset of P a(A) \ B</p><p>or some subset of P a(B) \ A, then A -B is absent.</p><p>2. If A and B are conditionally dependent given any subset of P a(A) \ B and any subset of P a(B) \ A, then A -B is present.</p><p>(</p><formula xml:id="formula_16">)<label>13</label></formula><p>The following null and alternative are therefore equivalent to ( <ref type="formula">12</ref>), where CI oracles are queried about A and B given all possible subsets of P a(A) \ B and all possible subsets of P a(B) \ A: H 0 : At least one CI oracle outputs independent,</p><formula xml:id="formula_17">H 1 : All CI oracles output dependent. (<label>14</label></formula><formula xml:id="formula_18">)</formula><p>Notice that the above hypothesis test is the same as the hypothesis test in (7). We can therefore bound the p-value of (12) using:</p><formula xml:id="formula_19">p A-B max i=1,...,q p A⊥ ⊥B|R i ,<label>(15)</label></formula><p>where R i ⊆ {Pa(A) \ B} or R i ⊆ {Pa(B) \ A} and q denotes the total number of such subsets.</p><p>Note that the skeleton discovery phase of the PC algorithm cannot differentiate between the parents and children of a particular vertex using its neighbors. However, we can further bound (15) using the following quantity:</p><formula xml:id="formula_20">p A-B ≤ max i=1,...,q p A⊥ ⊥B|S i p A-B ,<label>(16)</label></formula><p>where S i ⊆ {N(A) \ B} or S i ⊆ {N(B) \ A} and q denotes the total number of such subsets. Now assume that the Type II error rate of all CI tests is zero. Then, if the alternative holds for the CI tests (conditional dependence), then the alternative is accepted. Hence, the PC algorithm will not remove any of the edges between N (A) and A as well as any of the edges between N (B) and B. PC therefore performs all necessary CI tests for computing (16), so upper bounding the Type I error rate for (12) reduces to taking the maximum of the p-values for all of the CI tests performed by PC regarding A and B. For example, suppose we measure three random variables A, B and C. Then we obtain p-values after the PC algorithm tests whether A ⊥ ⊥ B and A ⊥ ⊥ B|C. Suppose these p-values are (0.03, 0.04) so that the PC algorithm with an α threshold of 0.05 determines that A -B is present. The p-value upper bound of ( <ref type="formula">12</ref>) thus corresponds to max {0.03, 0.04} = 0.04.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Detecting V-Structures</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Deterministic Skeleton</head><p>The hypothesis testing procedure for directed edges is more complicated than the procedure for adjacencies. Edges can be oriented in the PC algorithm according to unshielded vstructures or the orientation rules as described in Section 2.2. Let us first focus on the former and, for further simplicity, let us also assume that 1) we have access to the ground truth skeleton and 2) no edge is involved in more than one unshielded v-structure (we will later drop these assumptions in Section 4.2.2). Our task then is to statistically infer the presence of an unshielded v-structure.</p><p>We now present the following null and alternative for each unshielded v-structure after finding a triple A -C -B such that A and B are non-adjacent in the skeleton:</p><formula xml:id="formula_21">H 0 : Unshielded A → C ← B is absent, H 1 : Unshielded A → C ← B is present. (<label>17</label></formula><formula xml:id="formula_22">)</formula><p>Next, consider the following proposition:</p><p>Proposition 2 <ref type="bibr" target="#b22">(Spirtes et al., 2000)</ref> Consider the same assumptions as Proposition 1. Further assume that A, C are adjacent and C, B are adjacent but A, B are non-adjacent.</p><p>Then, A and B are conditionally independent given some subset of P a(A)\B which does not include C or some subset of P a(B)\A which does not include C if and only if</p><formula xml:id="formula_23">A → C ← B.</formula><p>The following null and alternative is therefore equivalent to ( <ref type="formula" target="#formula_21">17</ref>): </p><p>The above alternative is reminiscent of the way in which PC determines the presence of an unshielded v-structure according to Algorithm 6 in the Appendix; specifically, if C is not in the set which renders A and B conditionally independent, then C in A -C -B must be a collider. We however cannot bound the p-value of (18) using CI tests, because conditional dependence is in the null and conditional independence is in the alternative, as opposed to vice versa. As a result, we also consider the following proposition:</p><p>Proposition 3 Consider the same assumptions as Proposition 2. Then, A and B are conditionally dependent given any subset of P a(A) \ B containing C and any subset of</p><formula xml:id="formula_25">P a(B) \ A containing C if and only if A → C ← B.</formula><p>Proof First notice that P a(A) = {P a(A) \ B} and P a(B) = {P a(B) \ A}, since A and B are non-adjacent. As a result, we can instead prove that the if and only if statement holds for P a(A) and P a(B) without loss of generality.</p><p>For the forward direction, suppose A and B are conditionally dependent given any subset of P a(A) containing C and any subset of P a(B) containing C. Then A and B are d-connected given any subset of P a(A) containing C and any subset of P a(B) containing C by the global directed Markov property. Clearly, C ∈ N (A) and C ∈ N (B), so C must either be a parent of A and a parent of B, a child of A and a parent of B, a parent of A and a child B, or a child of A and a child of B. Note that A and B are non-adjacent, so A and B are d-separated given some subset of P a(A) or some subset of P a(B) by Proposition 1 and d-separation faithfulness. Moreover, the subset must include C if C is a parent of A and a parent of B, a child of A and a parent of B, or a parent of A and a child B; otherwise, A and B would be d-connected. As a result, in those three situations, we arrive at the contradiction that A and B are d-separated given some subset of P a(A) containing C or some subset of P a(B) containing C. We conclude that C must be a child of A and a child of B.</p><p>For the other direction, if A → C ← B holds, then A and B are d-connected given any subset of P a(A) containing C and any subset of P a(B) containing C. D-separation faithfulness then implies that A and B are conditionally dependent given any subset of P a(A) containing C and any subset of P a(B) containing C.</p><p>We can thus equivalently write (18) as:  We can bound the Type I error rate of the above hypothesis test by taking the maximum p-value over certain CI tests:</p><formula xml:id="formula_26">p γ AB|C max i=1,..,m p A⊥ ⊥B|M i ,</formula><p>where M i denotes a subset of P a(A) \ B containing C or a subset of P a(B) \ A containing C, and m is the total number of subsets M i . Of course, in practice, we do not know which vertices are the parents. However, we can also upper bound (19) as follows:</p><formula xml:id="formula_27">p γ AB|C ≤ max i=1,..,m p A⊥ ⊥B|T i p γ AB|C ,<label>(20)</label></formula><p>where T i denotes a subset of N (A) \ B containing C or a subset of N (B) \ A containing C, and m denotes the total number of subsets T i . Note that we do not need the zero Type II error rate assumption for computing (20), since we assume that the skeleton is provided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Inferred Skeleton</head><p>We have considered orienting the colliders, if we have access to the ground truth skeleton. We now consider the more complex problem of orienting the colliders, if we must also statistically infer the skeleton. We again consider the following null and alternative:</p><formula xml:id="formula_28">H 0 : Unshielded A → C ← B is absent, H 1 : Unshielded A → C ← B is present. (21)</formula><p>Now, the PC algorithm determines that the alternative holds, if all of the following conditions are true:</p><p>1. A and C are conditionally dependent given any subset of P a(A) \ C</p><p>and any subset of P a(C) \ A. </p><p>We therefore have the following equivalent form of the null and alternative as in ( <ref type="formula">21</ref>), if we assume A and B are non-adjacent:</p><p>H 0 : At least one condition from ( <ref type="formula" target="#formula_29">22</ref>) does not hold,</p><formula xml:id="formula_30">H 1 : All conditions from (22) hold. (<label>23</label></formula><formula xml:id="formula_31">)</formula><p>Note that the non-adjacency assumption is reasonable because we did not have enough statistical evidence to invalidate the assumption when we executed (12). Indeed, nonadjacencies are always assumed unless the data suggests that the null of ( <ref type="formula">12</ref>) is unlikely. Now, the alternative of ( <ref type="formula" target="#formula_30">23</ref>) is a series of three logical conjunctions, and the null is a series of three logical disjunctions as in ( <ref type="formula">10</ref>), so the Type I error rate of (23) can be bounded using the intersection bound:</p><formula xml:id="formula_32">Pr(Conditions 1, 2, 3 |H 0 ) ≤ Pr(Any one condition |H 0 ) ≤ max{h 1 , h 2 , h 3 }. (<label>24</label></formula><formula xml:id="formula_33">)</formula><p>We will be using shorthand from here on. We write (23) equivalently as:</p><formula xml:id="formula_34">H 0 : ¬(A -C) ∨ ¬(B -C) ∨ ¬γ AB|C , H 1 : (A -C) ∧ (B -C) ∧ γ AB|C ,<label>(25)</label></formula><p>where A -C, B -C, and γ AB|C represent Condition 1, 2 and 3 from ( <ref type="formula" target="#formula_29">22</ref>), respectively. We therefore have a p-value bound of (21) similar to (24):</p><formula xml:id="formula_35">Pr (A -C) ∧ (B -C) ∧ γ AB|C |H 0 ≤ Pr A -C ¬(A -C) ≤ max Pr A -C ¬(A -C) , Pr B -C ¬(B -C) , Pr γ AB|C ¬γ AB|C ≤ max{p A-C , p B-C , p γ AB|C }. (<label>26</label></formula><formula xml:id="formula_36">)</formula><p>Notice that computing p γ AB|C requires N (A) and N (B), not just their respective empirical estimates N (A) and N (B) which PC can discover. However, we can invoke a zero Type II error rate assumption in order to ensure that N (A) ⊆ N (A) and N (B) ⊆ N (B) as explained in detail in Section 4.4, so p γ AB|C can still be upper bounded. The assumption also ensures that we can upper bound p A-C and p B-C according to Section 4.1. We conclude that a zero Type II error rate ensures that (26) can be computed.</p><p>Next, consider the situation where PC can orient any one edge by using more than one unshielded v-structure. For example, consider the DAG in Figure <ref type="figure">3</ref>. In this case, PC can orient A -C by using either B 1 → C or B 2 → C (or both); we may therefore want to take both situations into account. Note that the original PC algorithm always orients an edge according to one v-structure which it picks arbitrarily according to the ordering of its computations. We thus only require the bound (26) in this case. However, we will propose a modified PC algorithm in Section 5 which takes into account all possible ways to orient one edge. Now, we can use the following null and alternative for Figure <ref type="figure">3</ref> when assuming that both A and B 1 and A and B 2 are non-adjacent:</p><formula xml:id="formula_37">H 0 : ¬(A -C) ∨ [¬(B 1 -C) ∨ ¬γ AB 1 |C ] ∧ [¬(B 2 -C) ∨ ¬γ AB 2 |C ] , H 1 : (A -C) ∧ [(B 1 -C) ∧ γ AB 1 |C ] ∨ [(B 2 -C) ∧ γ AB 2 |C ] . (27)</formula><p>We can therefore bound the Type I error rate of (27) as follows, where</p><formula xml:id="formula_38">G = ¬(A -C) and H = [¬(B 1 -C) ∨ ¬γ AB 1 |C ] ∧ [¬(B 2 -C) ∨ ¬γ AB 2 |C ], H 1 = ¬(B 1 -C) ∨ ¬γ AB 1 |C , and H 2 = ¬(B 2 -C) ∨ ¬γ AB 2 |C : Pr (A -C) ∧ [(B 1 -C) ∧ γ AB 1 |C ] ∨ [(B 2 -C) ∧ γ AB 2 |C ] H 0 ≤ max Pr(A -C|G), Pr (B 1 -C) ∧ γ AB 1 |C ∨ (B 2 -C) ∧ γ AB 2 |C H ≤ max Pr(A -C|G), Pr (B 1 -C) ∧ γ AB 1 |C |H + Pr (B 2 -C) ∧ γ AB 2 |C |H = max Pr(A -C|G), Pr (B 1 -C) ∧ γ AB 1 |C |H 1 + Pr (B 2 -C) ∧ γ AB 2 |C |H 2 ≤ max Pr(A -C|G), max Pr B 1 -C ¬(B 1 -C) , Pr(γ AB 1 |C ¬γ AB 1 |C ) + max Pr B 2 -C ¬(B 2 -C) , Pr(γ AB 2 |C ¬γ AB 2 |C ) ≤ max p A-C , max{p B 1 -C , p γ AB 1 |C } + max{p B 2 -C , p γ AB 2 |C } . (28) A C B 1 B 2</formula><p>Figure <ref type="figure">3</ref>: Here, one can orient the edge A -C according to the two unshielded v-structures</p><formula xml:id="formula_39">A → C ← B 1 and A → C ← B 2 .</formula><p>More generally, for an arbitrary number, say j, of multiple possible ways to orient A -C by unshielded v-structures, we have:</p><formula xml:id="formula_40">Pr (A -C) ∧ [(B 1 -C) ∧ γ AB 1 |C ] ∨ ... ∨ [(B j -C) ∧ γ AB j |C ] H 0 ≤ max p A-C , j i=1 max{p B i -C , p γ AB i |C } ,<label>(29)</label></formula><p>assuming that B 1 , . . . , B j are all non-adjacent to A.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Orientation Rules</head><p>We now consider bounding the p-values of edges which are oriented using the orientation rules of the PC algorithm. Recall from (1) that the PC algorithm only requires the repeated application of three orientation rules to be complete. We analyze these three orientation rules in separate subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">First orientation rule</head><p>We can construct the hypothesis test for the first orientation rule as follows according to the sufficient conditions of the first rule in (1):</p><formula xml:id="formula_41">H 0 : ¬(A -B) ∨ ¬(C → A), H 1 : (A -B) ∧ (C → A).<label>(30)</label></formula><p>We again also assume that C and B are non-adjacent. Now the PC algorithm determines that the alternative holds, if all of the following conditions are true:</p><p>1. A -B : A and B are conditionally dependent given any subset of P a(A) \ B and any subset of P a(B) \ A.</p><p>2. C → A : An edge is oriented from C to A under two scenarios. In the first, the edge is oriented because A is the collider in an unshielded v-structure. In the second, the edge is oriented due to the previous application of an orientation rule.</p><p>We thus have a logical conjunction and can bound the Type I error rate using the intersection bound:</p><formula xml:id="formula_42">Pr((A -B) ∧ (C → A)|H 0 ) ≤ max{p A-B , p C→A },</formula><p>where p C→A refers to the p-value bound for the hypothesis test of an unshielded v-structure or a previously applied orientation rule. Of course, p C→A will be the former when the PC algorithm begins to execute the orientation rules. More generally, for C i → A that can orient A -B where i = 1, ..., j, we have:</p><formula xml:id="formula_43">Pr (A -B) ∧ (C 1 → B) ∨ • • • ∨ (C j → A) |H 0 ≤ max p A-B , j i=1 p C i →A ,<label>(31)</label></formula><p>where we require that C 1 , . . . , C j are all non-adjacent to B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Second orientation rule</head><p>We have the following hypothesis test according to the sufficient conditions of the second rule in (1):</p><formula xml:id="formula_44">H 0 : ¬(A -B) ∨ ¬(A → C → B), H 1 : (A -B) ∧ (A → C → B).<label>(32)</label></formula><p>Hence, by conjunction:</p><formula xml:id="formula_45">Pr((A -B) ∧ (A → C → B)|H 0 ) ≤ max{p A-B , p A→C→B },</formula><p>where p A→C→B ≤ max{p A→C , p C→B }. The above Type I error rate can therefore be further upper bounded by max{p A-B , p A→C , p C→B }. More generally, we have:</p><formula xml:id="formula_46">Pr (A -B) ∧ (A → C 1 → B) ∨ ... ∨ (A → C j → B) |H 0 ≤ max p A-B , j i=1 p A→C i →B ≤ max p A-B , j i=1 max{p A→C i , p C i →B } . (<label>33</label></formula><formula xml:id="formula_47">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Third orientation rule</head><p>We have the following null and alternative by the sufficient conditions of the third rule in (1), assuming that C and D are non-adjacent:</p><formula xml:id="formula_48">H 0 : ¬(A -B) ∨ ¬(A -C → B) ∨ ¬(A -D → B), H 1 : (A -B) ∧ (A -C → B) ∧ (A -D → B). (<label>34</label></formula><formula xml:id="formula_49">)</formula><p>We can bound the Type I error rate of the above hypothesis test as follows:</p><formula xml:id="formula_50">Pr (A -B) ∧ (A -C → B) ∧ (A -D → B)|H 0 ≤ max{p A-B , p A-C→B , p A-D→B } ≤ max p A-B , max{p A-C , p C→B }, max{p A-D , p D→B } .</formula><p>The general case is slightly more complicated than the first and second orientation rules.</p><p>In this case, we need to control the Type I error rate of accepting at least two paths as opposed to one. Let the set D include all three-node paths from A to B with the first edge undirected from A to a middle vertex and the second edge directed from the middle vertex to B such that the i th element of D is:</p><formula xml:id="formula_51">D i A -C i → B.</formula><p>Let us suppose D has a total of n elements and assume that no middle vertex C i is adjacent to any other middle vertex. Now, let D be the set containing all of the n choose 2 elements of D. The i th element in D is therefore:</p><formula xml:id="formula_52">D i {A -C k → B, A -C l → B},</formula><p>where k and l are the distinct indices represented the two chosen middle vertices. Let D i,1 and D i,2 be the first and second elements in D i , respectively. Also let r = n 2 . We then have:</p><formula xml:id="formula_53">Pr (A -B) ∧ (D 1,1 ∧ D 1,2 ) ∨ ... ∨ (D r,1 ∧ D r,2 ) H 0 ≤ max p A-B , r i=1 Pr(D i |H 0 ) ,</formula><p>where Pr(D i |H 0 ) p {A-C k →B,A-C l →B} and is bounded as follows:</p><formula xml:id="formula_54">Pr(D i |H 0 ) ≤ max{p A-C k →B , p A-C l →B } ≤ max{p A-C k , p C k →B , p A-C l , p C l →B } Pr(D i |H 0 ),</formula><p>We therefore have:</p><formula xml:id="formula_55">Pr (A -B) ∧ (D 1,1 ∧ D 1,2 ) ∨ ... ∨ (D r,1 ∧ D r,2 ) H 0 ≤ max p A-B , r i=1 Pr(D i |H 0 ) .<label>(35)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Summary and Analysis of the Bounds</head><p>We derived several bounds for edge orientation as summarized in Table <ref type="table" target="#tab_1">1</ref>. We created the bounds by engineering specific hypothesis tests and successively applying the union and intersection bounds accordingly. Note that j and r are usually very small in sparse graphs.</p><p>One may now wonder whether PC can actually control the bounds listed in Table <ref type="table" target="#tab_1">1</ref> (we say that a quantity can be controlled, if the quantity can be upper bounded). Recall that we   <ref type="formula" target="#formula_46">33</ref>) and ( <ref type="formula" target="#formula_55">35</ref>) can be controlled trivially because the pvalue bounds for ( <ref type="formula" target="#formula_19">15</ref>) and ( <ref type="formula" target="#formula_40">29</ref>) can be controlled.</p><p>In other words, PC can control the bounds in Table <ref type="table" target="#tab_1">1</ref> with some additional CI tests and a zero Type II error rate.</p><p>Of course, the Type II error rate is never zero in practice, but this becomes less of an issue as the sample size increases. We may also consider reducing the Type II error rate by simultaneously implementing three strategies:</p><p>1. Use a liberal (higher) α threshold. We for example often use an α threshold of 0.20 in the experiments. This is the simplest strategy which decreases the Type II error rate but also increases the Type I error rate. However, we can then control the Type I error rate post-hoc with an FDR controlling procedure. Of course, setting the α threshold too high will prevent the PC algorithm from terminating within a reasonable amount of time as well as loosen the p-value bounds, since the CI tests will fail to explain away many edges. We therefore cannot rely entirely on this first strategy.</p><p>2. Use hypothesis tests whose p-value bounds are robust to Type II errors. The hypothesis tests in Section 4.4 are in fact robust to such errors due to the intersection bound as explained in detail in Appendix A.2. Briefly, we can also reasonably consider modifying the null hypotheses of ( <ref type="formula" target="#formula_34">25</ref>), ( <ref type="formula" target="#formula_41">30</ref>), ( <ref type="formula" target="#formula_44">32</ref>) and ( <ref type="formula" target="#formula_48">34</ref>) to "no edges between any of the vertices." This corresponds to converting the logical disjunctions in the null of (10) into conjunctions which in turns leads to a less robust p-value bound involving the minimum of a set of p-values instead of the maximum. As a result, under-estimating one p-value in the p-value set due to Type II error(s) can cause PC to also under-estimate the bound of ( <ref type="formula" target="#formula_34">25</ref>), ( <ref type="formula" target="#formula_41">30</ref>), ( <ref type="formula" target="#formula_44">32</ref>) or (34).</p><p>3. Modify the PC algorithm to prevent and catch many Type II errors.</p><p>The last strategy is more complex, so we discuss it in detail in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">The PC Algorithm with P-Values</head><p>We now propose a modified PC algorithm called PC with p-values (PC-p) that reduces the influence of Type II errors by preventing and catching potential Type II errors. At the same time, PC-p is correct -the algorithm operates differently than PC, but it maintains PC's desirable soundness and completeness properties. The PC-p algorithm involves two ideas. First, PC-p performs skeleton discovery with the same skeleton discovery procedure used in the PC-stable algorithm <ref type="bibr" target="#b4">(Colombo and Maathuis, 2014)</ref>. This procedure ensures that the algorithm does not skip some CI tests due to Type II errors and variable ordering. The second idea behind PC-p involves a modification to the procedure for propagating edge orientations. Specifically, if two edge orientations conflict, PC-p admits bidirected edges instead of over-writing previous orientations like PC. PC-p then unorients the bidirected edges as well as the directed edges which were directly used to infer the presence of the bidirected edges. The algorithm subsequently labels the resulting undirected edges as "ambiguous" which ensures that PC-p does not orient additional edges using the ambiguous edges. Indeed, the PC-p algorithm uses conflicts in edge orientation to detect potential Type II errors and prevent the propagation of the errors throughout the graph. In practice, we find that these two modifications to the PC algorithm help PC-p with the BY estimator achieve more accurate strong estimation and control of the FDR than PC, as we will see in Section 6.</p><p>We now describe the PC-p algorithm in detail; however, we will not describe the computation of the p-value upper bounds until Section 5.5 in order to keep the presentation clear. We have divided the PC-p algorithm into Algorithms 1, 2, 3 and 4, where the first three procedures correspond to Algorithms 5, 6 and 7 of the original PC algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Skeleton Discovery</head><p>We first consider skeleton discovery. The original PC algorithm uses Algorithm 5 to discover the skeleton. However, Algorithm 5 can cause the sample version of the PC algorithm to skip some CI tests due to variable ordering and Type II errors. For example, consider the causal graph in Figure <ref type="figure" target="#fig_8">4a</ref> as first presented in <ref type="bibr" target="#b4">(Colombo and Maathuis, 2014)</ref>. In this example, suppose the CI tests correctly determine that A ⊥ ⊥ B and B ⊥ ⊥ D|{A, C} but incorrectly determine that C ⊥ ⊥ D|{A, E}. The incorrect inference is a Type II error, since C and D are adjacent in the true graph. Now consider the following ordering of variables for the PC algorithm: order 1 (X) = (A, D, B, C, E). In this case, the ordered pair (D, B) is considered before (D, C) in Algorithm 5, since (D, B) comes earlier in order 1 (X). The PC algorithm removes D -B because a CI test determines that D ⊥ ⊥ B|{A, C} and {A, C} is a subset of N (D) = {A, B, C, E}. Next, D -C is considered and erroneously removed because a CI test determines that D ⊥ ⊥ C|{A, E} and {A, E} is a subset of N (D) = {A, C, E}. We thus ultimately obtain the skeleton in Figure <ref type="figure" target="#fig_8">4b</ref> with order 1 (X).</p><p>Now consider an alternative ordering of the variables: order 2 (X) = (A, C, D, B, E). In this case, (C, D) is considered before (D, B) in Algorithm 5, and the algorithm erroneously removes C -D. Next, the algorithm considers D -B but {A, C} is not a subset of N (D) = {A, B, E}, so D -B remains. Even when the PC algorithm eventually also considers the same undirected edge as B -D, {A, C} is again not a subset of N (B) = {C, D, E}, so B -D remains. In other words, (C, D) is considered first in order 2 (X) which causes C to be removed from N (D). Algorithm 5 therefore never executes test B⊥ ⊥D|{A,C} . We thus ultimately obtain the skeleton in Figure <ref type="figure" target="#fig_8">4c</ref> with order 2 (X).</p><p>The previous two examples show that the Type II error of incorrectly determining that C ⊥ ⊥ D|{A, E} leads PC to infer two different skeletons due to differences in variable ordering. Clearly, we would like to eliminate the dependency of skeleton discovery on variable ordering and also reduce its dependency on Type II errors at the same time. Fortunately, Colombo and Maathius proposed such a modification of Algorithm 5 as outlined in Algorithm 1. The key difference between Algorithm 5 and 1 involves the for loop in steps 5-7 of Algorithm 1 which computes and stores the adjacency sets after each new conditioning set size. As a result, an incorrect edge deletion due to a Type II error on line 16 of Algorithm 1 no longer effects which CI tests are performed for other pairs of variables with conditioning set size l. Indeed, the algorithm only modifies the adjacency sets when it increases the conditioning set size. Colombo and Maathius proved that Algorithm 1 is order-independent. We review the proof here, since it is informative:</p><p>Proposition 5 <ref type="bibr" target="#b4">(Colombo and Maathuis, 2014)</ref>. The skeleton resulting from Algorithm 1 is order-independent. Proof Consider the removal or retention of some undirected edge A-B at some conditioning set size l. The ordering of the variables determines the order in which the edges (line 9) and subsets S ⊆ a(A) and S ⊆ a(B) (line 11) are considered. However, by construction, the order in which the edges are considered does not affect the sets a(A) and a(B).</p><p>If there is at least one subset S of a(A) or a(B) such that A ⊥ ⊥ B|S, then any ordering of the variables will find a separating set for A and B (but different orderings may lead to different separating sets as illustrated in Example 2 of <ref type="bibr" target="#b4">(Colombo and Maathuis, 2014)</ref>). Conversely, if there is no subset S of a(A) or a(B) such that A ⊥ ⊥ B|S , then no ordering will find a separating set.</p><p>Hence, any ordering of the variables leads to the same edge deletions and therefore to the same skeleton.</p><p>In other words, modifying the adjacency sets only when changing the conditioning set size prevents PC-p from skipping some CI tests during skeleton discovery because of Type II errors and variable ordering. As a result, Algorithm 1 enables PC-p to perform more of the required CI tests than Algorithm 5 in order to correctly upper bound the p-value of ( <ref type="formula">12</ref>). However, notice that Algorithm 1 does not prevent all Type II errors from effecting the skeleton. The edge C -D is for example eliminated in Figure <ref type="figure" target="#fig_8">4</ref> regardless of the ordering because of the erroneous conclusion that C ⊥ ⊥ D|{A, E}. As a result, we have C ∈ N (D) which may lead to under-estimation of the p-value bounds for undirected edges connected to D. We will nonetheless see in Section 6 that Algorithm 1 does help PC-p achieve tighter estimation and control of the FDR than the original skeleton discovery procedure, since Algorithm 1 eliminates the influence of at least some Type II errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Unshielded V-Structures</head><p>We now describe Algorithm 2, where we use the circle edge endpoint "•" as a meta-symbol representing either a tail or an arrowhead. In Algorithm 2, PC-p orients edges according to all unshielded v-structures in line 3, even if two v-structures conflict with each other in the direction of a particular edge. In the case of conflict, PC-p admits a bidirected edge instead of favoring one particular direction over the other. The algorithm then unorients all v-structures involving the bidirected edges and labels the unoriented edges as "ambiguous" in line 23 because bidirected edges may result from a Type II error. For example, consider the ground truth in Figure <ref type="figure" target="#fig_9">5a</ref> and assume that Algorithm 1 correctly discovers all of the undirected edges. Moreover, assume Algorithm 1 correctly finds a separating set of B and D that does not contain C but incorrectly finds a separating set of A and C that does not contain B. The latter is a Type II error, since the alternative should have been accepted rather than rejected when conditioning on a subset not containing B. In this case, PC-p first orients the edges according to Figure <ref type="figure" target="#fig_9">5b</ref>. However, notice that the two unshielded v-structures conflict with each other due to the bidirected edge B ↔ C, and PC-p cannot determine which v-structure admitted the Type II error. As a result, the algorithm unorients all of the edges in both v-structures as in Figure <ref type="figure" target="#fig_9">5c</ref>. PC-p then labels the three unoriented edges as "ambiguous" so that the algorithm does not orient any other undirected edges based on these three edges using the orientation rules. The labeling thus prevents the algorithm from propagating Type II errors by orienting additional edges based on the erroneous directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Orientation Rules</head><p>Notice that Algorithm 7 uses "else if" statements instead of all "if" statements. The "else if" approach is of course faster, but it also causes PC to ignore any interactions between the orientation rules in the sense that, if one rule orients an edge, then no other rule can orient an edge. PC-p performs the orientation rules according Algorithm 3 which uses the "if" approach to attempt to apply all three orientation rules to each non-ambiguous undirected edge. Now, if bidirected edges exist after the rules are applied, then Algorithm 3 unorients the edge as well as all edges involved in the sufficient conditions of the associated oientation rules in lines 16-18. The algorithm then labels the unoriented edges as "ambiguous" in line 19 similar to unshielded v-structure orientation in Section 5.2. For example, in Figure <ref type="figure" target="#fig_10">6</ref>, rule 1 of PC-p induces a bidirected edge between A -B, so PC-p unorients and labels all directed edges which satisfy the sufficient conditions of rule 1 as ambiguous; these include D → A, C → A, E → B, and F → B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Analysis of PC-p</head><p>We now have the following analysis of the PC-p algorithm: </p><formula xml:id="formula_56">Replace A •-• B in G with A •→ B P A,B ← sum P A,B , sum{P 2 C i A , ∀i s.t. C i → A with C i , B non-adjacent} end if A -B non-ambiguous and ∃i s.t. A → C i → B in G then Replace A •-• B in G with A •→ B P AB ← sum P AB , sum max{P 2 AC i , P 2 C i B }, ∀i s.t. A → C i → B end if A -B non-ambiguous and ∃i, j s.t. A -C i → B, A -C j → B with A -C i</formula><p>and A -C j non-ambiguous, and C i and Theorem 6 The PC-p algorithm with a CI oracle is sound and complete.</p><formula xml:id="formula_57">C j non-adjacent in G then Replace A •-• B in G with A •→ B P AB ← sum P AB , sum max{P 1 AC i , P 2 C i B , P 1 AC j , P 2 C j B }, ∀i, j s.t. A -C i → B, A -C j → B with A -C i and A -C j non-</formula><p>Proof PC is sound and complete, so it is enough to prove that PC-p and PC will perform the exact same edge deletion and edge orientation operations with a CI oracle. Note that Algorithm 1 has already been shown to be sound and complete up to skeleton discovery <ref type="bibr">(Colombo &amp; Maathius 2014)</ref>. Algorithm 1 will therefore perform the exact same edge deletions as Algorithm 5 with a CI oracle. Now, Algorithm 2 will also perform the same edge orientations as Algorithm 6 with a CI oracle, since there will never be conflicting edge orientations. Lastly, for Algorithm 3, if there exists an edge that can be oriented by more than rule, then the edge must be oriented in the same direction by the other two rules. Algorithm 3 therefore returns the same edge orientations as Algorithm 7. We have proved equivalence in outputs of Algorithms 1, 2 and 3 of PC-p to Algorithms 5, 6 and 7 of PC, respectively. Algorithm 4 is not involved in graph structure discovery.</p><p>The output of PC-p is therefore equivalent to the output of PC in the large sample limit with a consistent CI test, even though PC-p performs more operations than PC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Computation of the P-Values</head><p>We now address the issue of computing the upper bounds of the p-values. Let us first consider Algorithm 1. Algorithm 1 takes as input the dataset X n and the significance threshold α. The algorithm then stores the p-values of all significant CI tests in cell</p><formula xml:id="formula_58">P 1 A B C D E F Figure 7</formula><p>: An example of a situation where two of PC's orientation rules, specifically rules 2 and 3, can orient one undirected edge (A -B) in the same direction. In this case, PC oriented all of the currently directed edges using unshielded v-structures.</p><p>when it reaches line 14. Notice that the algorithm stores the p-values of all significant tests involving A and B in both P 1 AB and P 1 BA . Algorithm 1 next computes the maximum over the p-values for all surviving edges in line 24 as in (16).</p><p>Algorithm 2 takes P 1 from Algorithm 1 as input. Moreover, unlike Algorithm 6 of PC, Algorithm 2 also takes as input the dataset X n , since PC-p must apply (29) in order to obtain the upper bounds of the p-values for oriented unshielded v-structures. Indeed, Algorithm 2 executes test A⊥ ⊥B|S for all S ⊆ N (A) containing C and all S ⊆ N (B) containing C in steps 4-12 for each A -C -B such that A and B are non-adjacent and C ∈ S AB . Now, Algorithm 2 ultimately stores all of the p-values needed to compute p γ AB|C as in (20) in P via line 10. Algorithm 2 then stores the maximum over p A-C and p γ AB|C in P BC instead of P AC in line 13. A similar set of operations eventually stores the maximum over p B-C and p γ AB|C into P AC in line 14. Note that multiple elements can enter into P BC and P AC when multiple v-structures can orient one edge. Finally, in line 27, Algorithm 2 takes the maximum over P 1 AC as returned from Algorithm 1 and the sum of P AC to obtain p A→C in P 2 according to (29) and similarly takes the maximum over P 1 BC and the sum of P BC to obtain p B→C in P 2 .</p><p>Algorithm 3 takes P 2 from Algorithm 2 as input. Next, in rule 1, Algorithm 3 adds up the p-values associated with C i → A, ∀i and places the result in P AB in line 5 for computing (31). Then, Algorithm 3 sums over the maxima of p A→C i and p C i →B in rule 2 ∀i s.t. A → C i → B in line 9 for ultimately computing (33). Subsequently, in rule 3, Algorithm 3 finds all n edges such that A -C i → B. The algorithm then finds all of the n choose 2 pairs, say r of them. For each pair, say A -C 1 → B and A -C 2 → B, Algorithm 3 computes p A-C 1 →B and p A-C 2 →B as the maximum over p A-C 1 and p C 1 →B and the maximum over p A-C 2 and p C 2 →B , respectively. Algorithm 3 next sums the p-values over all r pairs in line 13 for computing <ref type="bibr">(35)</ref>. Note that Algorithm 3 also takes an outersum involving P AB in lines 5, 9 and 13 of rules 1, 2 and 3, respectively; these summations correspond to logical disjunctions when multiple orientation rules can orient one edge in the same direction. For example, rules 2 and 3 can orient A -B in the same direction in Figure <ref type="figure">7</ref>. Two applications of rule 1 can also orient A -B in the same direction in Figure <ref type="figure" target="#fig_10">6</ref>, if we remove one of the unshielded v-structures from the graph. Now, for all non-ambiguous edges, Algorithm 3 then stores the maximum over the p-values from Algorithm 1 and P into P 2 in line 24. This process is repeated until no more edges can be oriented. Algorithm 3 finally transfers the p-values of all of the remaining undirected edges in G from P 1 to P 2 in lines 28-30. The algorithm therefore eventually outputs all of the final p-values in P 2 as desired.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Controlling the False Discovery Rate</head><p>PC-p controls the FDR per hypothesis test as opposed to per edge, since the algorithm can sometimes orient two edges according to the same hypothesis test during unshielded vstructure discovery. Indeed, controlling the p-values per edge as opposed to per hypothesis test can result in overly conservative FDR estimation or control because an FDR estimator or controlling procedure may count the p-value of one hypothesis test multiple times.</p><p>PC-p keeps track of each distinct hypothesis test in Algorithms 1, 2 and 3 by using indexing cell I as follows. First, Algorithm 1 assigns the same, unique identifier to the p-value bounds in both P AB and P BA in line 25. Next, if we have one v-structure A → C ← B that orients A -C and C -B, then Algorithm 2 associates both A → C and C ← B with the same hypothesis test and therefore the same identifier in line 29. On the other hand, if multiple unshielded v-structures can orient one edge, then Algorithm 2 assigns the edge a unique identifier in line 31, since a unique hypothesis test exists per edge in this case. Algorithm 3 finally assigns a unique identifier to each newly oriented edge in line 23 because each newly oriented edge also corresponds to a distinct hypothesis test.</p><p>We can now use Algorithm 4 to estimate and control the FDR using the identifiers in I and the p-value bounds in P 2 as returned from Algorithm 3. Algorithm 4 estimates the FDR by solving 2 to obtain F DR BY , where m corresponds to the number of unique identifiers in I. The algorithm subsequently controls the FDR by solving 3 to obtain α * . Algorithm 4 then eliminates all edges with p-values below α * in P 2 in order to obtain G * ; this process ensures that the expected FDR does not exceed q in G * .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">Conclusion</head><p>We wrap-up this section with the following theorem:</p><p>Theorem 7 Consider the same assumptions as Theorem 4. Then PC-p achieves conservative point estimation and strong control of the FDR across the edges in G.</p><p>Proof We have already shown that PC-p can control the p-values of all of the edges in G from Theorem 4. Estimation follows because the solution of 2 achieves conservative point estimation of the FDR at threshold α when the p-values are controlled <ref type="bibr" target="#b2">(Benjamini and Yekutieli, 2001)</ref>. Similarly, control follows because eliminating the edges associated with p-values above α * as obtained from 3 achieves strong control of the FDR at level q when the p-values are in turn controlled <ref type="bibr" target="#b2">(Benjamini and Yekutieli, 2001)</ref>.</p><p>The PC-p algorithm thus corresponds to a valid method for estimating and controlling the FDR in the estimated CPDAG.</p><p>Note finally that PC-p takes slightly longer than original PC to complete because it performs extra computations. However, PC-p runs at approximately the same speed as PC-stable, since v-structure detection and orientation rule application take an infinitesimal amount of time compared to skeleton discovery.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Algorithms and Metrics</head><p>We evaluated six algorithms:</p><p>1. PC-p, 2. PC-p without stabilization in the skeleton discovery procedure, 3. PC-p without ambiguous labelings during v-structure orientation and orientation rule application (PC-p without ambiguation), 4. PC-p without both stabilization and ambiguation, 5. PC-p without hypothesis tests with robust p-value bounds -we chose the null hypotheses to be a series of logical conjunctions so that no edges are present between any of the variables. As a result, the p-values take on minimal values as described in Appendix A.2. We call this procedure PC-p without robust p-values.</p><p>6. The original PC algorithm with p-value computation -that is, we do not incorporate stabilization, and the algorithm arbitrarily over-writes edge orientations. We compute p-values according to the v-structure or rule which ultimately orients each edge in the CPDAG. The algorithm also performs some additional CI tests in order to compute 20 as described in Section 4.1.</p><p>We ran these six algorithms because they are the only algorithms that allow us to compute the FDR across the entire CPDAG from the estimated p-values.</p><p>We assessed the FDR of the above six algorithms in detail using control and estimation bias<ref type="foot" target="#foot_0">foot_0</ref> . An algorithm exhibits low control bias at FDR level q when an FDR controlling procedure can accurately eliminate edges in the CPDAG using the p-values so that the FDR is in fact q. On the other hand, an algorithm exhibits low estimation bias when an FDR estimate closely matches the true FDR of the CPDAG. Notice that both control and estimation bias are important and can serve different purposes. As a result, we prefer an algorithm that exhibits both low control and estimation bias.</p><p>We used the mean of the following quantities to assess control bias:</p><p>uc( F DR BY , q) := max{F DR(α * ) -q, 0}, oc( F DR BY , q) := max{q -F DR(α * ), 0},</p><p>where uc( F DR BY , q) denotes under-control at FDR level q with the BY FDR estimate, and oc( F DR BY , q) similarly denotes over-control. In the experiments, we varied q from [0.001, 0.1] using 100 equispaced intervals. Note that we compute both under-control and over-control per CPDAG. A method achieves strong control when the mean under-control taken across the hypothesis tests is zero <ref type="bibr" target="#b1">(Armen and Tsamardinos, 2014)</ref>. Moreover, the less the mean over-control, the tighter the strong control. As a result, achieving a lower mean under-control is more important than achieving a lower mean over-control. We therefore say that one method outperforms another if the method achieves a lower mean under-control while also maintaining a reasonably low mean over-control. We used the mean of the following similar quantities for estimation bias:</p><formula xml:id="formula_60">ue( F DR BY , α) := max{F DR(α) -F DR BY (α), 0}, oe( F DR BY , α) := max{ F DR BY (α) -F DR(α), 0},<label>(37)</label></formula><p>where ue( F DR BY , α) denotes under-estimation at threshold level α with the BY FDR estimate, and oe( F DR BY , α) similar denotes over-estimation. We varied the α threshold from [1E-10, 0.1] with 100 equispaced intervals in the experiments. Now, we say that estimation is conservative in a α threshold region when the underestimation is zero. Moreover, the greater the over-estimation in a p-value threshold region, the more conservative the estimate. A method should conservatively estimate the FDR but not do so over-conservatively. As a result, achieving lower under-estimation is more important than achieving lower overestimation, and one method outperforms another if the method achieves a lower mean under-estimation while maintaining a reasonably low mean over-estimation.</p><p>Below, we report the relative performance differences of the six algorithms in recovering the CPDAG at a liberal α threshold of 0.20, since this threshold consistently provided a nice tradeoff between p-value bound looseness and low Type II error rates. We have reported the results using other α thresholds of 0.01, 0.05, 0.10, and 0.15 or 0.50 in Figures 12-15 of Appendix A.3, with similar relative performance differences between the algorithms. Figures 16-18 in the Appendix also contain results for skeleton discovery, where we compared the original skeleton discovery procedure of PC against the same procedure with stabilization. As expected, the stabilization procedure improved performance. We finally provide results with the more commonly used structural Hamming distance in 19 of the Appendix; here, PC-p achieved superior performance by conservatively estimating the graph.</p><p>Note that for the simulations in Sections 6.2 and 6.3, we generated the DAGs using the TETRAD V package (version 5.2.1) by drawing uniformly over all DAGs with a maximum in-degree of 2 and a maximum out-degree of 2. We then converted each of the DAGs to linear non-recursive SEM-IEs by 1) drawing the linear coefficients from independent standard normal distributions, and 2) setting independent Gaussian distributions over the error terms with standard deviations also drawn from the standard normal. Each linear SEM-IE with the error distributions therefore induced a multivariate Gaussian distribution across the observed variables. We finally ran all of the six algorithms using Fisher's z-test with a liberal α threshold of 0.20 and a maximum conditioning set size of 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Low Dimensional Inference</head><p>We generated 30 DAGs by drawing uniformly over all DAGs with 20 vertices. We converted each of the DAGs to 5 linear non-recursive SEM-IEs. We subsequently created 5 datasets using each linear SEM-IE with sample sizes of 100, 500, 1000, 5000, and 10000. We therefore created a total of 30 × 5 × 5 = 750 datasets.</p><p>We analyzed the ability of the algorithms in correctly estimating the CPDAG in terms of the four metrics proposed in Section 6.1 as well as the FDR values. Results as averaged over DAGs, parameters and sample sizes are summarized in Figure <ref type="figure" target="#fig_12">8</ref>. We assessed the significance of all inter-algorithm differences using paired Wilcoxon signed rank tests. PC-p obtained lower mean FDR values than PC-p without robust p-values (Figures <ref type="figure" target="#fig_12">8a</ref> and<ref type="figure" target="#fig_12">8b</ref>; z = -4.782, p = 1.734E-6), PC-p without stabilization (z = -4.371, p = 1.238E-5), PC-p without ambiguation (z = -4.782, p = 1.734E-6), PC-p without both stabilization and ambiguation (z = -4.782, p = 1.734E-6), and PC original (z = -4.433, p = 9.316E-6). Moreover, PC-p achieved significantly lower mean under-control than the competing methods (Figures <ref type="figure" target="#fig_12">8c</ref> and<ref type="figure" target="#fig_12">8d</ref>; vs. no robust: z = -4.782, p = 1.734E -6; vs. no stable: z = -3.898, p = 9.711E-5; vs. no ambig: z = -4.782, p = 1.734E-6; vs. no stable &amp; no ambig: z = -4.782, p = 1.734E-6; vs. PC original: z = -4.700, p = 2.603E-6); meanwhile, PC-p kept the mean over-control small at 3.865% (SD: 0.795%).  Results for estimation were similar. PC-p achieved significantly lower mean underestimation than the competing methods (Figures <ref type="figure" target="#fig_12">8e</ref> and<ref type="figure" target="#fig_12">8f</ref>; vs. no robust: z = -4.782, p = 1.734E-6; vs. no stable: z = -4.206, p = 2.597E-5; vs. no ambig: z = -4.782, p = 1.734E-6; vs. no stable &amp; no ambig: z = -4.782, p = 1.734E-6; vs. PC original: z = -4.186, p = 2.843E-5). PC-p also achieved a small degree of mean over-estimation (8.222%, SD: 0.400%). We conclude that robust p-values, stabilization, and ambiguation all help PC-p achieve the lowest under-control and under-estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">High Dimensional Inference</head><p>We next tested PC-p and the other five algorithms on high dimensional graph estimation. To do this, we generated thirty 100, thirty 200 and thirty 300 variable DAGs. We subsequently converted each of the DAGs to one linear non-recursive SEM-IE. Finally, we generated 1000 samples from each SEM-IE in order to obtain sample size to variable ratios of 10, 5 and 3.333.</p><p>Results are summarized using the FDR, control bias, and estimation bias metrics as averaged over the DAGs and their parameters in Figure <ref type="figure" target="#fig_13">9</ref>. PC-p achieved similar results in low dimensions as it did for high dimensions. Specifically, PC-p obtained lower mean FDR values across the same α thresholds than PC-p without robust p-values (Figures <ref type="figure" target="#fig_13">9a</ref> and<ref type="figure" target="#fig_13">9b</ref>; z = -4.703, p = 2.563E-6), PC-p without stabilization (z = -2.232, p = 0.026), PC-p without ambiguation (z = -4.782, p = 1.734E -6), PC-p without both stabilization and ambiguation (z = -4.782, p = 1.734E-6), and PC original (z = -4.782, p = 1.734E-6). Moreover, PC-p achieved significantly lower mean under-control than four of the five competing methods (Figures <ref type="figure" target="#fig_13">9c</ref> and<ref type="figure" target="#fig_13">9d</ref>; vs. no robust: z = -4.623, p = 3.790E-6; vs. no ambig: z = -4.782, p = 1.734E -6; vs. no stable &amp; no ambig: z = -4.782, p = 1.734E-6; vs. PC original: z = -4.782, p = 1.734E-6). PC-p did not outperform PC-p without stabilization at four of the five threshold α thresholds tested (0.05 : z = -1.121, p = 0.262; 0.10 : z = 0.504, p = 0.614; 0.20 : z = 0.985, p = 0.324; 0.50 : z = 0.760, p = 0.447); however, PC-p did outperform PC-p without stabilization at an α threshold of 0.01 (z = -3.692, p = 2.225E-4). Meanwhile, PC-p kept the mean over-control small at 4.507% (SD: 0.240%).</p><p>Results for estimation were again similar. PC-p achieved significantly lower mean underestimation than the competing methods except PC-p without stabilization (Figures <ref type="figure" target="#fig_13">9e</ref> and<ref type="figure" target="#fig_13">9f</ref>; vs. all methods except no stable: z = -4.782, p = 1.734E-6). PC-p did not outperform PC-p without stabilization at four of the five threshold α thresholds tested (0.05 : z = -1.820, p = 0.069; 0.10 : z = 0.175, p = 0.861; 0.20 : z = 0.625, p = 0.532; 0.50 : z = 0.608, p = 0.543); however, PC-p did outperform PC-p without stabilization at an α threshold of 0.01 (z = -2.293, p = 0.028). PC-p also achieved a small degree of mean over-control (2.129%, SD: 0.084%).</p><p>We conclude that the results for control and estimation bias for high dimensional graph estimation are similar to the low dimensional case. However, stabilization only increased performance at lower α thresholds in the high dimensional scenario; we may nonetheless view this as a desirable property, since a lower α threshold helps the algorithm complete more quickly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Real Data: CYTO</head><p>We evaluated the six algorithms on the CYTO dataset which contains single cell recordings of the abundance of 11 phosphoproteins and phospholipids in human primary naive CD4+ T cells using flow cytometry <ref type="bibr" target="#b20">(Sachs et al., 2005)</ref>. The variables in the dataset and their causal relationships can be represented as a DAG, where vertices are proteins or lipids and edges are phosphorylation interactions between the proteins and lipids. We used the general perturbation samples (i.e., CD3-CD28 and CD3-CD28-ICAM2) as our observational data; these perturbations are required to activate the phosphorylation pathways. Note that algorithms typically cannot accurately infer the gold standard solution set using the observational data alone, as noted by the original authors<ref type="foot" target="#foot_1">foot_1</ref> . As a result, we created a silver standard DAG by running LiNGAM as implemented in TETRAD V using default parameters on the full dataset of 1,755 samples; recall that LiNGAM is a method within a different class of causal discovery algorithms based on functional causal models. We then ran the six algorithms described in Section 6.1 on 1000 bootstrapped datasets of sample size 100 using Spearman's rho to handle the class of non-paranormal distributions.</p><p>We have summarized the results in Figure <ref type="figure" target="#fig_14">10</ref>. PC-p obtained lower mean FDR across the same α thresholds than PC-p without robust p-values (z = -12.774, p = 2.282E-37), PC-p without stabilization (z = -8.402, p = 4.378E-17), PC-p without ambiguation (z = -24.598, p = 1.343E-133), PC-p without both stabilization and ambiguation (z = -23.714, p = 2.616E-124), and original PC (z = -4.924, p = 8.469E-7). Moreover, PC-p achieved significantly lower mean under-control than four of the five competing methods (vs. no robust: z = -12.601, p = 2.081E-36; vs no stable: z = -4.310, p = 1.631E-5; vs. no ambig: z = -24.339, p = 7.559E-131; vs. no stable &amp; no ambig: z = -22.893, p = 5.515E-116). PC-p did not outperform the original PC algorithm in mean under-control (z = 0.827, p = 0.408); however, PC-p did outperform the original PC algorithm in mean under-estimation (z = -2.662, p = 0.008). Meanwhile, PC-p kept the mean over-control small at 4.232% (SD: 1.635%). PC-p also outperformed the other four methods in mean under-estimation (vs. no robust: z = -12.684, p = 7.289E-37; vs. no stable: z = -4.893, p = 9.917E-7; vs. no ambig: z = -24.411, p = 1.322E-131; vs. no stable &amp; no ambig: z = -23.301, p = 4.333E-120) while maintaining low mean over-estimation at 1.200% (SD: 0.559%). We conclude that PC-p outperforms the other methods similar to the results with synthetic data. PC-p only outperformed PC in 2 of the 3 metrics, however, probably because the LiNGAM solution is only an estimate of the ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Real Data: GDP Dynamics</head><p>One way of approximating the underlying DAG involves learning the graph with a large number of samples. Another way uses time series data, where we know a priori that we must have contemporaneous causal relations or causal relations directed forward in time. In this experiment, we strip the time information from the six algorithms, and then identify the false discoveries when algorithms mistakenly detect a causal relation directed backwards in time. We used a time series dataset downloaded from the Economic Research Service of the United States Department of Agriculture containing ten economic indicators per year related to GDP among 192 countries 5 . We specifically evaluated the algorithms on their ability to discover causal relations among the indicators within and between <ref type="bibr">1987, 1988 and 1989</ref>, where we treated each country as an i.i.d. sample and used 100 bootstrapped datasets.</p><p>We have summarized the results in Figure <ref type="figure" target="#fig_15">11</ref>. PC-p again obtained lower mean FDR values across the α thresholds than PC-p without robust p-values (z = -4.623, p = 3.784E-6), PC-p without ambiguation (z = -8.054, p = 4.128E-16), PC-p without both stabilization and ambiguation (z = -8.135, p = 4.128E-16), and original PC (z = -6.624, p = 3.500E-11). However, PC-p did not obtain significantly lower mean FDR values than PC-p without stabilization (signed-rank = 70, p = 0.600). Next, PC-p achieved significantly lower mean under-control than three of the five competing methods including PC-p without robust pvalues (z = -4.374, p = 1.218E-5), PC-p without ambiguation (z = -7.867, p = 3.647E-15), and PC-p without both stabilization and ambiguation (z = -7.819, p = 5.306E-15). PC-p did not outperform PC-p without stabilization (signed-ranked = 3, p = 1) as well as the original PC algorithm (signed-ranked = 7, p = 0.625) in mean under-control; nevertheless, PC-p did outperform the former in over-control (signed-ranked = 3, p = 0.020) while keeping its own mean over-control low at 4.920% (SD: 0.483%). PC-p also outperformed the same three methods in mean under-estimation (vs. no robust: z = -4.372, p = 1.229E-5; vs. no ambig: z = -7.818, p = 5.363E-15; vs. no stable &amp; no ambig: z = -7.770, p = 7.850E-15) while maintaining low mean over-estimation at 2.032% (SD: 0.281%). PC-p again did not outperform PC-p without stabilization (signed-rank = 2, p = 0.750) and original 5. Web link: <ref type="url" target="http://www.ers.usda.gov/data/macroeconomics/Data/HistoricalRealGDPValues.xls">http://www.ers.usda.gov/data/macroeconomics/Data/HistoricalRealGDPValues.xls</ref> PC (signed-rank = 7, p = 0.625) in under-estimation, but it outperformed both in overestimation (no stable: z = -7.386, p = 1.519E-13; PC original: z = -8.682, p = 3.897E-18). We conclude that PC-p outperforms most methods in either the under or over-metrics. The results however are not as clean as the results with the synthetic data because we only have access to a portion of the ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>We developed a new algorithm called PC-p which outputs a causal DAG with p-value bounds associated with each edge. One can then use the bounds with the BY procedure to achieve almost strong control and estimation of the FDR. The PC-p algorithm specifically integrates the skeleton discovery procedure of PC-stable, edge orientation with ambiguation, and robust hypothesis tests in order to accurately estimate p-values bounds while maintaining computational efficiency.</p><p>The PC-p algorithm represents the first global constraint-based method which can recover p-value estimates for every edge of a CPDAG. In our opinion, the algorithm is a significant advancement over previous methods which can only achieve strong control of the FDR under special conditions. Moreover, PC-p lays a foundation for developing similar methods which can also recover edge-specific p-values and achieve strong control of the FDR for graphs recovered by algorithms such as FCI and CCD. In particular, we suspect that a combination of the max and union bounds will also be sufficient for deriving upper bounds of the edge-specific p-values for more sophisticated constraint-based methods. The proposed approach may therefore represent one the earliest forms of a "causal p-value."</p><p>Now readers may wonder whether PC-p can also use the p-values to control the familywise error rate (FWER). The answer is yes, and we recommend using the Benjamini-Holm step-down procedure as opposed to Hochberg's step-up procedure to control the FWER <ref type="bibr" target="#b10">(Hochberg, 1988)</ref>, since the latter assumes positive dependency among the test statistics. However, application of an FWER controlling procedure to constraint-based causal discovery requires additional justification, since most investigators do not use constraint-based methods to definitively conclude causal relationships but rather to screen for potential causal variables. With the screening goal in mind, the FWER may be too conservative in practice, since it controls the rate of making a single Type I error across all of the hypothesis tests as opposed to controlling the proportion of Type I errors.</p><p>In summary, we introduced an algorithm called PC-p which outputs a causal DAG along with edge-specific p-value bounds. One can then use the BY procedure with the bounds to achieve almost strong control or estimation of the FDR and therefore assess the algorithm's confidence in each edge in a principled manner. We ultimately hope that this work will encourage more applications of constraint-based causal discovery to important problems in science.</p><p>since the null of (41) implies the null in (40). From (39), the Type I error rate of (40) is bounded by min{p A-C , p B-C , p γ AB|C }. This is a less robust bound than (26) in terms of the Type II error rate, since failing to control one p-value can cause an algorithm to underestimate the bound. For example, suppose the underlying truth corresponds to p A-C = 0.01, p B-C = 0.03, p γ AB|C = 0.02. Thus, the Type I error rate of (41) is truly bounded by 0.01. However, suppose a Type II error causes PC-p to skip CI tests and therefore compute p A-C = 0.01, p B-C = 0.03, p γ AB|C = 0.003, where the third term is under-estimated. Then, PC-p will under-estimate the bound at 0.003 instead of the true 0.01.</p><p>Note that generalizing (41) to account for multiple possible ways of orienting a vstructure does not robustify the bound either, since we have: </p><p>The above bound is less robust to Type II errors than (11), since under-estimating one term in each group i composed of n i terms can cause PC-p to also under-estimate (44).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Other Experimental Results</head><p>We have summarized the results for the low dimensional, high dimensional and real datasets across multiple α thresholds in Figures 12, 13, 14 and 15 respectively. Relative differences in performance largely remained consistent across the thresholds, since PC-p usually achieved the lowest mean FDR, under-control and under-estimation values with minimal increases in mean over-control and over-estimation.</p><p>We have also summarized the results for adjacency discovery in Figures 16, 17, and 18, where we tested whether the skeleton discovery procedure of PC with stabilization could improve the estimation of the p-value bounds relative to the procedure without stabilization. Results show that stabilization improves performance across the three metrics particularly with the low α threshold values of 0.01 and 0.05. Note that we cannot compute the same figures for the GDP dataset, since we can only evaluate relative performance levels based on edge direction in this case.</p><p>We have finally summarized the results using the structural Hamming distance in Figure <ref type="figure" target="#fig_13">19</ref>. Notice that ambiguation helps PC-p achieve significantly lower Hamming distances across multiple α thresholds by forcing the algorithm to conservatively orient the edges. Again, we cannot compute the structural Hamming distances for the GDP dataset for the aforementioned reason.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>and C and D are non-adjacent, then replace A -B with A → B.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Pr</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>H 0 :</head><label>0</label><figDesc>A and B are conditionally dependent given any subset of P a(A) \ B which does not include C and any subset of P a(B) \ A which does not include C, H 1 : A and B are conditionally independent given some subset of P a(A) \ B which does not include C or some subset of P a(B) \ A which does not include C.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>H 0 :</head><label>0</label><figDesc>A and B are conditionally independent given some subset of P a(A) \ B containing C or some subset of P a(B) \ A containing C, H 1 : A and B are conditionally dependent given any subset of P a(A) \ B containing C and any subset of P a(B) \ A containing C.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>2. B and C are conditionally dependent given any subset of P a(B) \ C and any subset of P a(C) \ B. 3. A and B are conditionally dependent given any subset of P a(A) \ B containing C and any subset of P a(B) \ A containing C.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Proof</head><label></label><figDesc>Consider any two vertices A and B. Algorithm 1 starts with a fully connected graph, so we have B ∈ N (A) and A ∈ N (B) in the beginning. Note that Algorithm 1 executes test A⊥ ⊥B|S for all S ⊆ N (A) \ B and for all S ⊆ N (B) \ A. The zero Type II error rate ensures the following: if the alternative holds, then the alternative is accepted. As a result, Algorithm 1 will not remove any vertices adjacent to A and any vertices adjacent to B with a zero Type II error rate. Hence, we always have {N (A) \ B} ⊆ { N (A) \ B} and {N (B) \ A} ⊆ { N (B) \ A}. Algorithm 1 therefore must eventually execute test A⊥ ⊥B|S for all S ⊆ {N (A) \ B} and for all S ⊆ {N (B) \ A}, so (16) can be controlled. For (29), the p-value bounds for undirected edges can already be controlled by the previous paragraph. We must now argue that p γ AB|C can be controlled. Let C be a collider between non-adjacent vertices A and B. Now notice that C ∈ N (A) ⊆ N (A) and C ∈ N (B) ⊆ N (B), so Algorithm 2 must execute test A⊥ ⊥B|S for all S ⊆ {N (A) \ B} containing C and for all S ⊆ {N (B) \ A} containing C. Hence p γ AB|C can be controlled. Now the p-value bounds (31), (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: An example of a situation when PC infers different skeletons due to a Type II error and two variable orderings. (a) The true causal graph, (b) the skeleton inferred by PC from order 1 (X), (c) the skeleton inferred by PC from order 2 (X).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Example of how Algorithm 2 deals with conflicting edge orientations. a) The ground truth, b) the inferred graph with two v-structures A → B ← C and D → C ← B that lead to the bi-directed edge B ↔ C, and c) the final graph after unorienting both v-structures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Here, a bidirected edge between A and B results from the application of rule 1.PC-p therefore unorients and labels all edges in the above graph as "ambiguous" according to the sufficient conditions of rule 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>ambiguous, and C i and C j non-adjacent end for each A ↔ B in G do Unorient to A -B in G For each edge in the sufficient conditions of each orientation rule that led to the creation of a directed edge to A in G, unorient the edge in G For each edge in the sufficient conditions of each orientation rule that led to the creation of a directed edge to B in G, unorient the edge in G Label the unoriented edges as "ambiguous" in G end G ← G for each A → B in G do Place a unique identifier into I AB P 2 AB ← max{P 1 AB , P AB } end Empty P until there are no more edges to orient; for each non-empty cell P 1 AB s.t. P 2 AB and P 2 BA are empty do P 2 AB , P 2 BA ← P 1 AB end Algorithm 3: Orientation Rules Data: G, P 2 , I, α, q Result: F DR BY (α), G * // Estimation 1 F DR BY (α) ← Solution of 2 using threshold α and m corresponding to the number of unique identifiers in I // Control 2 α * ← Solution of 3 using FDR level q and m corresponding to the number of unique identifiers in I 3 G * ← G with edges associated with p-values above α * eliminated Algorithm 4: FDR Estimation and Control</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Performances of PC-p, PC-p without robust p-values (no robust), PC-p without ambiguation (no ambig), PC-p without stabilization (no stable), PC-p without ambiguation and stabilization (no stable &amp; no ambig), and the original PC algorithm (PC) as assessed by (a,b) the FDR, (c,d) control bias, and (e,f) estimation bias in units of percent. PC-p achieved significantly lower FDR, under-estimation and under-control than the other five methods suggesting that robust p-values, stabilization and ambiguation are all important components of PC-p.</figDesc><graphic coords="33,90.00,230.08,432.00,212.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Same setup as Figure 8 except with high dimensional data. PC-p significantly outperformed all other methods except PC-p without stabilization in terms of under-control and under-estimation.</figDesc><graphic coords="34,90.00,90.86,432.00,213.36" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Same setup as Figure 8 except with the CYTO dataset. PC-p significantly outperformed all methods across all metrics except the original PC algorithm in under-control.</figDesc><graphic coords="35,90.00,90.86,432.00,212.98" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Similar to Figure 8 except with the GDP dataset as well as over-control and over-estimation bias values instead of under. PC-p did not achieve lower undercontrol and under-estimation than PC, but it did achieve significantly lower mean FDR and over-estimation that PC.</figDesc><graphic coords="37,90.00,90.86,432.00,209.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>H 0 :</head><label>0</label><figDesc>All edges between A, C, B 1 are absent, and all edges between A, C, B 2 are absent,H 1 : (A -C) ∧ (B 1 -C) ∧ γ AB 1 |C ] ∨ [(B 2 -C) ∧ γ AB 2 |C . (42)whose Type I error rate is bounded by minp A-C , min{p B 1 -C , p γ AB 1 |C }+min{p B 2 -C , p γ AB 2 |C } .More broadly, we can consider the following hypothesis test: j outputs P i,j |H 0 )≤ max i=1,...,m min j=1,...,n i Pr(test i, j outputs P i,j |H 0 ) i, j outputs P i,j | oracle i, j outputs ¬P i,j )</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="51,90.00,90.86,432.00,179.41" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>P-value bounds for all of the edge types in a CPDAG. Note that S i ⊆ {N (A) \ B} or S i ⊆ {N (B) \ A}. , affirmative answer to the question in Sections 4.1 and 4.2.2 by assuming a zero Type II error rate. We now spell out a more detailed answer via a theorem whose proof builds on the argument of Theorem 4 in<ref type="bibr" target="#b1">(Armen and Tsamardinos, 2014)</ref>. Suppose that the PC algorithm is applied to a sample from P represented by DAG G. If we have:1. P is d-separation faithful to G, 2. TheType II error rate is zero, 3. The PC algorithm also tests whether any two non-adjacent vertices A, B with common neighbor C are conditionally dependent given any subset of P a(A) \ B containing C and any subset of P a(B) \ A containing C, then all of the p-value bounds in Table 1 can be controlled using the p-values of the CI tests executed by PC.</figDesc><table><row><cell>Edge Type</cell><cell>P-Value Bound</cell><cell>Equation Num.</cell></row><row><cell>Undirected</cell><cell>max i p A⊥ ⊥B|S i</cell><cell>(16)</cell></row><row><cell cols="2">Unshielded v-structure max p A-C , j i=1 max{p B i -C , p γ AB i |C }</cell><cell>(29)</cell></row><row><cell>First orientation rule</cell><cell>max p A-B , j i=1 p C i →A</cell><cell>(31)</cell></row><row><cell cols="2">Second orientation rule max p A-B , j i=1 max{p A→C i , p C i →B }</cell><cell>(33)</cell></row><row><cell>Third orientation rule</cell><cell>max p A-B , r i=1 Pr(D i |H 0 )</cell><cell>(35)</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0"><p>We also measured the false negative rate using the structural Hamming distance as a metric in Figure19of the Appendix.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1"><p>In general, we do not have gold standard causal graphs for real data, so we must approximate the solution in some manner.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>Research reported in this publication was supported by grant <rs type="grantNumber">U54HG008540</rs> awarded by the <rs type="funder">National Human Genome Research Institute</rs> through funds provided by the trans-</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_J9Jck74">
					<idno type="grant-number">U54HG008540</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>NIH Big Data to Knowledge initiative. The research was also supported by the National Library of Medicine of the National Institutes of Health under award numbers T15LM007059 and R01LM012095. The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Institutes of Health.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A. Appendix</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 PC Algorithm Pseudocode</head><p>We provide pseudocode for the original PC algorithm. We summarize skeleton discovery in Algorithm 5, unshielded v-structure discovery in Algorithm 6, and orientation rule application in Algorithm 7. We claimed to propose edge-specific hypothesis tests whose bounds are robust to Type II errors in Section 4.4. We now explain our rationale. Consider the following modification to (10), where we have replaced the null with a series of logical conjunctions:</p><p>We can bound the Type I error rate of the above hypothesis test as follows:</p><p>We can use the above bound with the following variant of (26) for unshielded vstructures:</p><p>The above hypothesis test follows from the following natural hypothesis test:       </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A unified approach to estimation and control of the false discovery rate in bayesian network skeleton identification</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Armen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Tsamardinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning</title>
		<meeting>the European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning</meeting>
		<imprint>
			<publisher>ESANN</publisher>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Estimation and control of the false discovery rate of bayesian network skeleton identification</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Armen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Tsamardinos</surname></persName>
		</author>
		<idno>TR-441</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
		<respStmt>
			<orgName>University of Crete</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Investigating the importance of self-theories of intelligence and musicality for students&apos; academic and musical achievement</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Benjamini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yekutieli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1165" to="1188" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">On corporate structure, strategy, and performance: a study with directed acyclic graphs and pc algorithm. Managerial and Decision Informatics</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Bessler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="47" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Order-independent constraint-based causal structure learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Colombo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Maathuis</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=2627435.2750365" />
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<idno type="ISSN">1532-4435</idno>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3741" to="3782" />
			<date type="published" when="2014-01">January 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Causal discovery from a mixture of experimental and observational data</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">F</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yoo</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=2073796.2073810" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence, UAI&apos;99</title>
		<meeting>the Fifteenth Conference on Uncertainty in Artificial Intelligence, UAI&apos;99<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="116" to="125" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Data analysis with bayesian networks: A bootstrap approach</title>
		<author>
			<persName><forename type="first">N</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Goldszmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wyner</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=2073796.2073819" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence, UAI&apos;99</title>
		<meeting>the Fifteenth Conference on Uncertainty in Artificial Intelligence, UAI&apos;99<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="196" to="205" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Structural Hamming distances for the a) low dimensional, b) high dimensional and c) CYTO datasets. The three sub-figures associate 5 bars with each algorithm</title>
	</analytic>
	<monogr>
		<title level="j">Figure</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
		</imprint>
	</monogr>
	<note>these bars correspond to α thresholds of 0.01, 0.05, 0.1, 0.20 and 0.50 for the low dimensional and CYTO datasets, and α thresholds of 0.01, 0.05, 0.1, 0.15 and 0.20 for the high dimensional datasets. Error bars represent standard errors for a) and standard deviations otherwise</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Prognostic gene signature identification using causal structure learning: applications in kidney cancer</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Baladandayuthapani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">A</forename><surname>Do</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cancer Inform</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">Suppl 1</biblScope>
			<biblScope unit="page" from="23" to="35" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Pc algorithm for nonparanormal graphical models</title>
		<author>
			<persName><forename type="first">N</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Drton</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=2567709.2567770" />
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<idno type="ISSN">1532-4435</idno>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3365" to="3383" />
			<date type="published" when="2013-01">January 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A sharper Bonferroni procedure for multiple tests of significance</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Hochberg</surname></persName>
		</author>
		<idno type="DOI">10.1093/biomet/75.4.800</idno>
		<ptr target="http://dx.doi.org/10.1093/biomet/75.4.800" />
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<idno type="ISSN">1464-3510</idno>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="800" to="802" />
			<date type="published" when="1988-12">December 1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Inferring functional connectivity in MRI using Bayesian network structure learning with a modified PC algorithm</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Iyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Shafran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Grayson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Nigg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Fair</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">75</biblScope>
			<biblScope unit="page" from="165" to="175" />
			<date type="published" when="2013-07">Jul 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Bayesian approach for network modeling of brain structural features</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Leahy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Shattuck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Dinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Toga</surname></persName>
		</author>
		<idno type="DOI">10.1117/12.844548</idno>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">7626</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Inferring microRNA-mRNA causal regulatory relationships from expression data</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tsykin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Goodall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="765" to="771" />
			<date type="published" when="2013-03">Mar 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Controlling the false discovery rate of the association/causality structure learned with the pc algorithm</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">J</forename><surname>Wang</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=1577069.1577086" />
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<idno type="ISSN">1532-4435</idno>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="475" to="514" />
			<date type="published" when="2009-06">June 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning brain connectivity with the false-discoveryrate-controlled PC-algorithm</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Mckeown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Conf Proc IEEE Eng Med Biol Soc</title>
		<imprint>
			<biblScope unit="page" from="4617" to="4620" />
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Determining the number of non-spurious arcs in a learned dag model: Investigation of a bayesian and a frequentist approach</title>
		<author>
			<persName><forename type="first">J</forename><surname>Listgarten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Heckerman</surname></persName>
		</author>
		<ptr target="http://dblp.uni-trier.de/db/conf/uai/uai2007.html#ListgartenH07" />
	</analytic>
	<monogr>
		<title level="m">UAI</title>
		<editor>
			<persName><forename type="first">Ronald</forename><surname>Parr</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Linda</forename><forename type="middle">C</forename><surname>Van Der Gaag</surname></persName>
		</editor>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="251" to="258" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Strong completeness and faithfulness in bayesian networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Meek</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=2074158.2074205" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence, UAI&apos;95</title>
		<meeting>the Eleventh Conference on Uncertainty in Artificial Intelligence, UAI&apos;95<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="411" to="418" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Investigating the importance of self-theories of intelligence and musicality for students&apos; academic and musical achievement</title>
		<author>
			<persName><forename type="first">D</forename><surname>Mullensiefen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Caprini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Fancourt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Front Psychol</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">1702</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A discovery algorithm for directed cyclic graphs</title>
		<author>
			<persName><forename type="first">T</forename><surname>Richardson</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=2074284.2074338" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth International Conference on Uncertainty in Artificial Intelligence, UAI&apos;96</title>
		<meeting>the Twelfth International Conference on Uncertainty in Artificial Intelligence, UAI&apos;96<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="454" to="461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Causal protein-signaling networks derived from multiparameter single-cell data</title>
		<author>
			<persName><forename type="first">K</forename><surname>Sachs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pe'er</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">A</forename><surname>Lauffenburger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">P</forename><surname>Nolan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">308</biblScope>
			<biblScope unit="issue">5721</biblScope>
			<biblScope unit="page" from="523" to="529" />
			<date type="published" when="2005-04">Apr 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Causal inference in the presence of latent variables and selection bias</title>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Meek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Richardson</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=2074158.2074215" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence, UAI&apos;95</title>
		<meeting>the Eleventh Conference on Uncertainty in Artificial Intelligence, UAI&apos;95<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="1995">1995</date>
			<biblScope unit="page" from="499" to="506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Scheines</surname></persName>
		</author>
		<title level="m">Causation, Prediction, and Search</title>
		<imprint>
			<publisher>MIT press</publisher>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
	<note>2nd edition</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Inferring consistent functional interaction patterns from natural stimulus FMRI data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="987" to="999" />
			<date type="published" when="2012-07">Jul 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Estimating causal effects with a non-paranormal method for the design of efficient intervention experiments</title>
		<author>
			<persName><forename type="first">R</forename><surname>Teramoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Saito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Funahashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page">228</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Bounding the false discovery rate in local bayesian network learning</title>
		<author>
			<persName><forename type="first">I</forename><surname>Tsamardinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Brown</surname></persName>
		</author>
		<ptr target="http://www.aaai.org/Library/AAAI/2008/aaai08-174.php" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Third AAAI Conference on Artificial Intelligence, AAAI 2008</title>
		<meeting>the Twenty-Third AAAI Conference on Artificial Intelligence, AAAI 2008<address><addrLine>Chicago, Illinois, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008">July 13-17, 2008. 2008</date>
			<biblScope unit="page" from="1100" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Exploring gene causal interactions using an enhanced constraint-based method</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.patcog.2006.05.003</idno>
		<ptr target="http://dx.doi.org/10.1016/j.patcog.2006.05.003" />
	</analytic>
	<monogr>
		<title level="j">Pattern Recogn</title>
		<idno type="ISSN">0031-3203</idno>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2439" to="2449" />
			<date type="published" when="2006-12">December 2006</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
