<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CAUSAL DISCOVERY FROM TIME-SERIES DATA WITH SHORT-TERM INVARIANCE-BASED CONVOLUTIONAL NEURAL NETWORKS A PREPRINT</title>
				<funder ref="#_NMDmTBC">
					<orgName type="full">National Key Research and Development Program of China</orgName>
				</funder>
				<funder ref="#_XYW25SA">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-08-16">August 16, 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Rujia</forename><surname>Shen</surname></persName>
							<email>shenrujia@stu.hit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Computing</orgName>
								<orgName type="institution">Harbin Institute of Technology Harbin</orgName>
								<address>
									<settlement>Heilongjiang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Boran</forename><surname>Wang</surname></persName>
							<email>wangboran@hit.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">The Artificial Intelligence Institute</orgName>
								<orgName type="institution">Harbin Institute of Technology Shenzhen</orgName>
								<address>
									<settlement>Guangdong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chao</forename><surname>Zhao</surname></persName>
							<email>zhaochao@cs.unc.edu</email>
							<affiliation key="aff2">
								<orgName type="department">The Department of Computer Science</orgName>
								<orgName type="institution">The University of North Carolina at Chapel Hill North Carolina</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yi</forename><surname>Guan</surname></persName>
							<email>guanyi@hit.edu.cn</email>
							<affiliation key="aff3">
								<orgName type="department">Faculty of Computing</orgName>
								<orgName type="institution">Harbin Institute of Technology Harbin</orgName>
								<address>
									<settlement>Heilongjiang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jingchi</forename><surname>Jiang</surname></persName>
							<email>jiangjingchi@hit.edu.cn</email>
							<affiliation key="aff4">
								<orgName type="department">The Artificial Intelligence Institute</orgName>
								<orgName type="institution">Harbin Institute of Technology Harbin</orgName>
								<address>
									<settlement>Heilongjiang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CAUSAL DISCOVERY FROM TIME-SERIES DATA WITH SHORT-TERM INVARIANCE-BASED CONVOLUTIONAL NEURAL NETWORKS A PREPRINT</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-08-16">August 16, 2024</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2408.08023v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Causal discovery</term>
					<term>Time-series data</term>
					<term>Time invariance</term>
					<term>Mechanism invariance</term>
					<term>Convolutional neural networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Causal discovery from time-series data aims to capture both intra-slice (contemporaneous) and inter-slice (time-lagged) causality between variables within the temporal chain, which is crucial for various scientific disciplines. Compared to causal discovery from non-time-series data, causal discovery from time-series data necessitates more serialized samples with a larger amount of observed time steps. To address the challenges, we propose a novel gradient-based causal discovery approach STIC, which focuses on Short-Term Invariance using Convolutional neural networks to uncover the causal relationships from time-series data. Specifically, STIC leverages both the short-term time and mechanism invariance of causality within each window observation, which possesses the property of independence, to enhance sample efficiency. Furthermore, we construct two causal convolution kernels, which correspond to the short-term time and mechanism invariance respectively, to estimate the window causal graph. To demonstrate the necessity of convolutional neural networks for causal discovery from time-series data, we theoretically derive the equivalence between convolution and the underlying generative principle of time-series data under the assumption that the additive noise model is identifiable. Experimental evaluations conducted on both synthetic and FMRI benchmark datasets demonstrate that our STIC outperforms baselines significantly and achieves the state-ofthe-art performance, particularly when the datasets contain a limited number of observed time steps. Code is available at <ref type="url" target="https://github.com/HITshenrj/STIC">https://github.com/HITshenrj/STIC</ref>.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Causality behind time-series data plays a significant role in various aspects of everyday life and scientific inquiry. Questions like "What factors in the past have led to the current rise in blood glucose?" or "How long will my headache be alleviated if I take that pill?" require an understanding of the relationships among observed variables, such as the relation between people's health status and their medical interventions <ref type="bibr" target="#b0">[Cowls and</ref><ref type="bibr">Schroeder, 2015, Pawlowski et al., 2020]</ref>. People usually expect to find cyclical and invariant principles in a changing world, which we call causal relationships <ref type="bibr">[Chan et al., 2024, Entner and</ref><ref type="bibr" target="#b3">Hoyer, 2010]</ref>. These relationships can be represented as a directed acyclic graph (DAG), where nodes represent observed variables and edges represent causal relationships between variables with time lags. This underlying graph structure forms the factual foundation for causal reasoning and is essential for addressing such queries <ref type="bibr">[Pearl, 2009]</ref>.</p><p>Current causal discovery approaches utilize intra-slice and inter-slice information of time-series data, leveraging techniques such as conditional independence, smooth score functions, and auto-regression. These methods can be broadly classified into three categories: Constraint-based methods <ref type="bibr" target="#b3">[Entner and Hoyer, 2010</ref><ref type="bibr" target="#b4">, Runge et al., 2019</ref><ref type="bibr" target="#b5">, Runge, 2020]</ref>, Score-based methods <ref type="bibr">[Pamfil et al., 2020]</ref>, and Granger-based methods <ref type="bibr" target="#b7">[Nauta et al., 2019</ref><ref type="bibr" target="#b8">, Cheng et al., 2022</ref><ref type="bibr">, 2023]</ref>.Constraint-based methods rely on conditional independence tests to infer causal relationships between variables. These methods perform independence tests between pairs of variables under different conditional sets to determine whether a causal relation exists. However, due to the difficulty of sampling, real-world data often suffers from the limited length of observed time steps, making it challenging for statistical conditional independence tests to fully capture causal relationships <ref type="bibr" target="#b10">[Zhang et al., 2011, Zhang and</ref><ref type="bibr">Suzuki, 2023]</ref>. Additionally, these methods often rely on strong yet unrealistic assumptions, such as Gaussian noise, when searching for statistical conditional independence <ref type="bibr">[Spirtes and</ref><ref type="bibr">Zhang, 2016, Wang and</ref><ref type="bibr" target="#b12">Michoel, 2017]</ref>. Score-based methods regard causal discovery as a constrained optimization problem using augmented Lagrangian procedures. They assign a score function that captures properties of the causal graph, such as acyclicity, and minimize the score function to identify potential causal graphs. While these methods offer simplicity in optimization, they relying heavily on acyclicity regularization and often lack guarantees for finding the correct causal graph, potentially leading to suboptimal solutions <ref type="bibr" target="#b13">[Varando, 2020</ref><ref type="bibr" target="#b14">, Lippe et al., 2021</ref><ref type="bibr">, Zhang et al., 2023]</ref>. Granger-based methods, inspired by <ref type="bibr" target="#b16">[Granger, 1969, Granger and</ref><ref type="bibr" target="#b17">Hatanaka, 2015]</ref>, offer an intriguing perspective on causal discovery. These methods utilize auto-regression algorithms under the assumption of additive noise to assess if one time series can predict another, thereby identifying causal relationships. However, they tend to exhibit lower precision when working with limited observed time steps.</p><p>To overcome the limitations of existing approaches, such as low sample efficiency in constraint-based methods, suboptimal solutions from acyclicity regularizers in score-based methods and low precision when limited observed time steps in Granger-based methods, we propose a novel Short-Term Invariance-based Convolutional causal discovery approach (STIC). STIC leverages the properties of short-term invariance to enhance the sample efficiency and accuracy of causal discovery. More concretely, by sliding a window along the entire time-series data, STIC constructs batches of window observations that possess invariant characteristics and improves sample utilization. Unlike existing score-based methods, our model does not rely on predefined acyclicity constraints to avoid local optimization. As the window observations move along the temporal chain, the structure of the window causal graph exhibits periodic patterns, demonstrating short-term time invariance. Simultaneously, the conditional probabilities of causal effects between variables remain unchanged as the window observations slide, indicating short-term mechanism invariance. The contributions of our work can be summarized as follows:</p><p>• We propose STIC, the Short-Term Invariance-based Convolutional causal discovery approach, which leverages the properties of short-term invariance to enhance the sample efficiency and accuracy of causal discovery.</p><p>• STIC uses the time-invariance block to capture the causal relationships among variables, while employing the mechanism-invariance block for the transform function.</p><p>• To dynamically capture the contemporaneous and time-lagged causal structures of the observed variables, we establish the equivalence between the convolution of the space-domain (contemporaneous) and time-domain (time-lagged) components, and the multivariate Fourier transform (the underlying generative mechanism) of time-series data.</p><p>• We conduct experiments to evaluate the performance of STIC on synthetic and benchmark datasets. The experimental results show that STIC achieves the state-of-the-art results on synthetic time-series datasets, even when dealing with relatively limited observed time steps. Experiments demonstrate that our approach outperforms baseline methods in causal discovery from time-series data. In W ∈ R 5×5×3 , for each W τ i,j represents the causal effect of X i on X j with τ time lags. For example, the blue lines in window causal graph indicate the following three causal effects with time lags τ = 2 at any time step t, i.e.</p><formula xml:id="formula_0">W 2 1,3 = 1 ⇒ X 1 2 -→ X 3 ; W 2 1,5 = 1 ⇒ X 1 2 -→ X 5 ; W 2 4,2 = 1 ⇒ X 4 2 -→ X 2 .</formula><p>Moreover, the red lines indicate the causal relationships with time lags τ = 1, i.e.</p><formula xml:id="formula_1">W 1 3,2 = 1 ⇒ X 3 1 -→ X 2 ; W 1 3,4 = 1 ⇒ X 3 1 -→ X 4 ; W 1 3,5 = 1 ⇒ X 3 1 -→ X 5 ; W 1 5,4 = 1 ⇒ X 5 1 -→ X 4 .</formula><p>Finally, the green lines represent contemporaneous causal relationships, i.e.</p><formula xml:id="formula_2">W 0 1,2 = 1 ⇒ X 1 0 -→ X 2 ; W 0 1,4 = 1 ⇒ X 1 0 -→ X 4 ; W 0 5,2 = 1 ⇒ X 5 0 -→ X 2 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>In this section, we introduce the background of causal discovery from time-series data. Firstly, we show all symbols and their definitions in Section 2.1. Secondy, in Section 2.2, we present the problem definition and formal representation of window causal graph. Thirdly, in Section 2.3, we introduce the concepts of short-term time invariance and mechanism invariance. Building upon these concepts, we derive an independence property specific to window causal graph. Fourthly, in Section 2.4, we delve into the theoretical aspects of our approach. Specifically, we establish the equivalence between the convolution operation and the underlying generative mechanism of the observed time-series data. This theoretical grounding provides a solid basis for the proposed STIC approach. Finally, in Section 2.5, we introduce Granger causality, an auto-regressive approach to causal discovery from time-series data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Symbol Summarization</head><p>Firstly, to better represent the symbols used in Section 2, we arrange a table to summarize and show their definitions, as shown in Table <ref type="table" target="#tab_0">1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Problem Definition</head><p>Let an observed dataset denoted as</p><formula xml:id="formula_3">X = {X 1 , • • • , X d } ∈ R d×T , which consists of d observed continuous time-series variables. Each variable X i is represented as a time sequence X i = {X 1 i , • • • , X T i }</formula><p>with the length of T . Here, each X t i corresponds to the observed value of the i-th variable X i at the t-th time step. Unlike graph embedding algorithms <ref type="bibr" target="#b18">[Cheng et al., 2020</ref><ref type="bibr" target="#b19">[Cheng et al., , 2021] ]</ref> which aims to learn time series representations, the objective of causal discovery is to uncover the underlying structure within time-series data, which represents boolean relationships between observed variables. Furthermore, following the Consistency Throughout Time assumption <ref type="bibr" target="#b20">[Spirtes et al., 2000</ref><ref type="bibr" target="#b21">, Zhang and Spirtes, 2002</ref><ref type="bibr" target="#b22">, Robins et al., 2003</ref><ref type="bibr" target="#b23">, Kalisch and Bühlman, 2007</ref><ref type="bibr" target="#b3">, Entner and Hoyer, 2010</ref><ref type="bibr" target="#b24">, Assaad et al., 2022]</ref>, the objective of causal discovery from time-series data is to uncover the underlying window causal graph G as an invariant causal structure. The true window causal graph for X encompasses both intra-slice causality with 0 time lags and inter-slice causality with time lags ranging from 1 to τ . Here, τ denotes the maximum time lag. Mathematically, the window causal graph is defined as a finite Directed Acyclic Graph (DAG) denoted by G = (V, E). The set V = {X 1 , ..., X d } represents the nodes within the graph G, wherein each node corresponds to an observed variable X i . The set E represents the </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>T</head><p>The length of observed time steps</p><formula xml:id="formula_4">X t i</formula><p>The observed value of the i-th variable at the t-th time step</p><formula xml:id="formula_5">X i = {X 1 i , • • • , X T i } ∈ R T</formula><p>The observed value of i-th variable within all T time steps</p><formula xml:id="formula_6">X = {X 1 , • • • , X d } ∈ R d×T</formula><p>The observed dataset τ</p><p>The maximum time lag</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G</head><p>The underlying window causal graph</p><formula xml:id="formula_7">V = {X 1 , ..., X d }</formula><p>The nodes within the graph G</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E</head><p>The contemporaneous and time-lagged relationships among nodes</p><formula xml:id="formula_8">V W ∈ R d×d×( τ +1)</formula><p>The window causal matrix</p><formula xml:id="formula_9">X i τ -→ X j</formula><p>The causal relationship with τ lags between X i and X j P a τ t (•)</p><p>The set of parents of a variable with τ time lags at time step t P a </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>∝</head><p>The directly proportional relationship</p><formula xml:id="formula_10">σ 2 τ (X i |X )</formula><p>The variance of predicting X i using X with τ time lags contemporaneous and time-lagged relationships among these nodes, encompassing all 2 ( τ +1)×d possible combinations.</p><p>The window causal graph is often represented by the window causal matrix, which is defined as follows.</p><p>Definition 1 (Window Causal Matrix) The window causal graph G, which captures both contemporaneous and timelagged causality, can be effectively represented using a three-dimensional boolean matrix W ∈ R d×d×( τ +1) . Each entry W τ i,j in the boolean matrix corresponds to the causal relationship between variables X i and X j with τ time lags. To be more specific, if W τ =0 i,j = 1, it signifies the presence of an intra-slice causal relationship between X i and X j , meaning they influence each other at the same time step. On the other hand, if W τ &gt;0 i,j = 1, it indicates that X i causally affects X j with τ time lags.</p><p>Figure <ref type="figure" target="#fig_0">1</ref> provides a visual example of a window causal graph along with its corresponding matrix defined in Definition 1. As shown in Figure <ref type="figure" target="#fig_0">1</ref>, the time-series causal relationships of the form X i τ -→ X j can be represented as W τ i,j = 1. Conversely, W τ i,j = 1 in the boolean matrix indicates that the value X t i at any time step t influences the value X t+τ j with τ time lags later.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Short-Term Causal Invariance</head><p>There has been an assertion that causal relationships typically exhibit short-term time and mechanism invariance across extensive time scales <ref type="bibr" target="#b3">[Entner and Hoyer, 2010</ref><ref type="bibr">, Liu et al., 2023</ref><ref type="bibr">, Zhang et al., 2017]</ref>. These two aspects of invariance are commonly regarded as fundamental assumptions of causal invariance in causal discovery from time-series data. In the following, we will present the definitions for these two forms of invariance.</p><p>Definition 2 (Short-Term Time Invariance) Given X ∈ R d×T , for any X i , X j , τ ≥ 0, if X i ∈ P a τ t (X j ) at time t, then there exists X i ∈ P a τ t ′ (X j ) at time t ′ ̸ = t in a short period of time, where P a τ t (•) denotes the set of parents of a variable with τ time lags at time step t.</p><p>Short-term time invariance refers to the stability of parent-child relationships over time. In other words, it implies that the dependencies between variables remain consistent regardless of specific time points. For instance, considering Figure <ref type="figure" target="#fig_0">1</ref>: X 5 is a parent of X 4 with time lag τ = 1 at t, then X 5 will also be a parent of X 4 with time lag τ = 1 at t ′ = t + 1; similarly, when τ = 0, if X 5 is a parent of X 2 at t, then X 5 will be a parent of X 2 at no matter t ′ = t + 1 or t ′ = t + 2.</p><p>Definition 3 (Short-Term Mechanism Invariance) For any X i , the conditional probability distribution P (X i |P a • t (X i )) remains constant across the short-term temporal chain. In other words, for any time step t and t ′ , it holds that P (X i |P a</p><formula xml:id="formula_11">• t (X i )) = P (X i |P a • t ′ (X i ))</formula><p>, where P a • t (X i ) means the set of parents of X i with all time lags range from 0 to τ at time step t.</p><p>In particular, based on Definition 3, short-term mechanism invariance implies that conditional probability distributions remain constant over time. For instance, in Figure <ref type="figure" target="#fig_0">1</ref>, we have P a</p><formula xml:id="formula_12">• t (X 2 ) = {X 3 , X 1 , X 5 } = P a • t+1 (X 2 ). Then, we have P (X 2 |P a • t (X 2 )) = P (X 2 |P a • t+1 (X 2 ))</formula><p>Building upon the definitions of short-term time invariance and mechanism invariance, we can derive the following lemma, which characterizes the invariant nature of independence among variables. Inspired by causal invariance <ref type="bibr" target="#b3">[Entner and Hoyer, 2010]</ref>, we further provide a detailed proof procedure as outlined below.</p><p>Lemma 1 (Independence Property) Given X ∈ R d×T be the observed dataset. If we have</p><formula xml:id="formula_13">X i ⊥ ⊥ t τ X j |X k , ..., X l , then we have X i ⊥ ⊥ t ′ τ X j |X k , ..., X l . ⊥ ⊥ t</formula><p>τ means conditional independence with τ time lags at time step t.</p><p>Proof 1 Due to the short-term time invariance of the relationships among variables and the short-term mechanism invariance of conditional probabilities, different value X t i and X t ′ i of X i is mapped to the same variable X i in the window causal graph G. Consequently, P a τ t (X i ) and P a τ t ′ (X i ) correspond to the same variable set. Thus, if the condition</p><formula xml:id="formula_14">X i ⊥ ⊥ t τ X j |X k , ..., X l holds, then X i ⊥ ⊥ G τ X j |X k , ..., X l holds in the window causal graph G, which further implies X i ⊥ ⊥ t ′ τ X j |X k , ..., X l .</formula><p>This lemma establishes that, in an identifiable window causal graph, the independence property remains invariant with time translation. Leveraging this insight, we can transform the observed time series into window observations to perform causal discovery while maintaining the invariance conditions, as outlined in Section 3.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Necessity of Convolution</head><p>Granger demonstrated, through the Cramer representation and the spectral representation of the covariance sequence <ref type="bibr" target="#b16">[Granger, 1969</ref><ref type="bibr" target="#b27">, Mills and Granger, 2013</ref><ref type="bibr" target="#b17">, Granger and Hatanaka, 2015]</ref>, that time-series data can be decomposed into a sum of uncorrelated components. Inspired by these representations and the concept of graph Fourier transform <ref type="bibr" target="#b28">[Shuman et al., 2013</ref><ref type="bibr" target="#b29">, Sandryhaila and Moura, 2013</ref><ref type="bibr" target="#b30">, Sardellitti et al., 2017]</ref>, we propose considering a underlying function X = f (P a G (X ), W) + E, where P a G (X ) denotes relationships among X in the window causal graph G and E is the noise term, to describe the generative process of the observed dataset</p><formula xml:id="formula_15">X = {X 1 , • • • , X d } ∈ R d×T ,</formula><p>with an underlying window causal matrix W ∈ R d×d×( τ +1) . We can then decompose f (P a G (X ), W) into Fourier integral forms:</p><formula xml:id="formula_16">X = f (P a G (X ), W) + E = f (s, t) + E (1)</formula><p>Here, s and t denote the spatial and temporal projections, respectively, of f (P a G (X ), W). Equation 1 is derived from the observation that the contemporaneous part in time-series data corresponds to the spatial domain, while the time-lagged part corresponds to the temporal domain. Therefore, we employ the multivariate Fourier transform,</p><formula xml:id="formula_17">F(X ) = ∞ -∞ f (x, y; s, t)e -iω(sx+ty) dxdy ∝ ∞ -∞ h(ŝ)g( t)e -iω(ŝ+ t) dŝd t (2)</formula><p>where ŝ represents the spatial domain component, t represents the temporal domain component, and ω represents the angular frequency along with transform function f , h and g. The first line corresponds to applying the Fourier transform  -1) , into a window representation W ∈ R d×τ ×c using a sliding window with a predefined window length τ and step length 1, where c = T -τ . Time-Invariance Block (B t ): In order to better discover the causal structure from X , we use convolution kernel K t ∈ R d×τ to act on W , and get the common representation K t ⊙ W ψ of X for each window observations W ψ . Afterwards, we pass the commonality through an FNN network to obtain a predicted window causal matrix Ŵ ∈ R d×d×τ . Mechanism-Invariance Block (B m ): To identify numerical transform in window causal graph, we use another convolution kernel K m ∈ R d×τ in each B m to transform W . Then we output W ∈ R d×τ ×c as the prediction of f (W ). Next, we do hadamard product of each W τ ψ ∈ R d in W and each Ŵτ ∈ R d×d in Ŵ to get the predicted X τ +ψ until we get all X ∈ R d×c . Finally, we calculate the Mean Squared Error (MSE) loss between X and X , and adopt gradient descent to optimize the parameters within the time-invariance and mechanism-invariance blocks.</p><formula xml:id="formula_18">-1 = {X 1:T -1 1 , • • • , X 1:T -1 d } ∈ R d×(T</formula><p>to both sides of Equation 1. In the second line, inspired by the Time-Independent Schrödinger Equation <ref type="bibr">[Zabusky, 1968, Rana and</ref><ref type="bibr" target="#b32">Liao, 2019]</ref>, we assume that f (x, y; s, t) can be decomposed into the spatial and temporal domains, i.e., f (x, y; s, t) = h(ŝ)g( t). Next, by utilizing the convolution theorem <ref type="bibr" target="#b33">[Zayed, 1998]</ref> for tempered distributions, which states that under suitable conditions the Fourier transform of a convolution of two functions (or signals) is the pointwise product of their Fourier transform, i.e., F(h * g) = F(h) • F(g), where F(•) represents the Fourier transform, we convert the convolution formula into the following expression:</p><formula xml:id="formula_19">F[h(ŝ) * g( t)] ∝ F(h(ŝ)) • F(g( t)) ∝ ∞ -∞ h(ŝ)e -iωŝ dŝ ∞ -∞ g( t)e -iω td t ∝ F(X )<label>(3)</label></formula><p>The first line of the Formula 3 is obtained through the convolution theorem, while the second line expands F(h(ŝ)) and F(g( t)) using the Fourier transform. The third line is derived from Equation <ref type="formula">2</ref>. Therefore, it indicates that the observed dataset X can be obtained by convolving the convolution kernel with temporal information and the spatial details, which we will deal with corresponding to the two kinds of invariance. We posit that the convolution operation precisely aligns with the functional causal data generation mechanism, i.e., X ∝ h(ŝ) * g( t). Conversely, the convolution operation can be used to analytically model the generation mechanism of functional time-series data. Therefore, we will employ the convolution operation to extract the functional causal relationships within the window causal graph. In conclusion, the equivalence between the generation mechanism of time-series causal data and convolution operations serves as motivation to incorporate convolution operations into our STIC framework. </p><formula xml:id="formula_20">= τ + 1 W ∈ R d×τ ×c</formula><p>The window representation, where c = T -τ</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B t</head><p>The time-invariance block</p><formula xml:id="formula_21">K t ∈ R d×τ</formula><p>The convolution kernel in the time-invariance block</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>⊙</head><p>The Hadamard product</p><formula xml:id="formula_22">Ŵ ∈ R d×d×τ</formula><p>The predicted window causal matrix</p><formula xml:id="formula_23">B m</formula><p>The mechanism-invariance block</p><formula xml:id="formula_24">K m ∈ R d×τ</formula><p>The convolution kernel in the mechanism-invariance block</p><formula xml:id="formula_25">W ∈ R d×τ ×c The prediction of f (W ) X ∈ R d×c</formula><p>The prediction of the observed dataset</p><formula xml:id="formula_26">f 1 : R c×d×τ → R d×d×τ The feed-forward neural network Ŵτ i,j</formula><p>The estimated binary existence of the causal effect of X i on X j with τ time lags p The threshold used to eliminate edges with low probability of existence</p><formula xml:id="formula_27">f 2 : R d×τ → R d×τ</formula><p>The estimated transformation function</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.5">Granger Causality</head><p>Granger causality <ref type="bibr" target="#b16">[Granger, 1969</ref><ref type="bibr" target="#b34">, Pavasant et al., 2021</ref><ref type="bibr" target="#b24">, Assaad et al., 2022]</ref> is a method that utilizes numerical calculations to assess causality by measuring fitting loss and variance. Formally, we say that a variable X i Grangercauses another variable X j when the past values of X i at time t (i.e.,</p><formula xml:id="formula_28">X 1 i , • • • , X t-1 i</formula><p>) enhance the prediction of X j at time t (i.e., X t j ) compared to considering only the past values of X j . The definition of Granger causality is as follows:</p><formula xml:id="formula_29">Definition 4 (Granger Causality) Let X = {X 1 , • • • , X d } ∈ R d×T be a observed dataset containing d variables. If σ 2 τ (X j |X ) &lt; σ 2 τ (X j |X -X i )</formula><p>, where σ 2 τ (X j |X ) denotes the variance of predicting X j using X with τ time lags, we say that X i causes X j , which is represented by W τ i,j = 1.</p><p>In simpler terms, Granger causality states that X i Granger-causes X j if past values of X i (i.e., X t ′ i ) provide unique and statistically significant information for predicting future values of X j (i.e., X t j ). Therefore, following the definition of Granger causality, we can approach causal discovery as an autoregressive problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Method</head><p>In this section, we introduce STIC, which involves four components: Window Representation, Time-Invariance Block, Mechanism-Invariance Block, and Parallel Blocks for Joint Training. The process is depicted in Figure <ref type="figure" target="#fig_1">2</ref>. Firstly, we transform the observed time series into a window representation format, leveraging Lemma 1. Next, we input the window representation into both the time-invariance block and the mechanism-invariance block (B t and B m in Figure <ref type="figure" target="#fig_1">2</ref>). Finally, we conduct joint training using the extracted features from two kinds of parallel blocks. In particular, the time-invariance block B t generates the estimated window causal matrix Ŵ. To better represent the symbols used in Section 3, we also arrange a table to summarize and show their definitions, as shown in Table <ref type="table" target="#tab_2">2</ref>. The subsequent subsections provide a detailed explanation of the key components of STIC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Window Representation</head><p>The observed dataset X ∈ R d×T contains d observed continuous time series (variables) with T time steps. We also define a predefined maximum time lag as τ . To ensure that the entire causal contemporaneous and time-lagged influence is observed, we calculate the minimum length of the window that can capture this influence as τ = τ + 1. To construct the window observations, we select the observed values from the first T -1 time steps, i.e. -1) . Using a sliding window approach along the temporal chain of observations, we create window observations of length τ and width d, with a step size of 1. This process results in c = T -τ window observations W ψ where ψ = 1, ..., c. These window observations are referred to as the window representation W , as illustrated in Figure <ref type="figure" target="#fig_2">3</ref>.</p><formula xml:id="formula_30">X 1:T -1 = {X 1:T -1 1 , • • • , X 1:T -1 d } ∈ R d×(T</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Time-Invariance Block</head><p>According to Definition 2, the causal relationships among variables remain unchanged as time progresses. Exploiting this property, we can extract shared information from the window representation W and utilize it to finally obtain the estimated window causal matrix Ŵ. Inspired by convolutional neural networks used in causal discovery <ref type="bibr" target="#b7">[Nauta et al., 2019]</ref>, we introduce a invariance-based convolutional network structure denoted as B t to incorporate temporal information within the window representation W . For each window observation W ψ ∈ R d×τ , we employ the following formula to aggregate similar information among the time series within the window observations</p><formula xml:id="formula_31">Ŵ = f 1 (K t ⊙ W 1 , ..., K t ⊙ W c )<label>(4)</label></formula><p>Here, shared K t ∈ R d×τ represents a learnable extraction kernel utilized to extract information from each window observation. The symbol ⊙ denotes the Hadamard product between matrices, and f 1 refers to a neural network structure. By applying the Hadamard product with the shared kernel K t , the resulting output exhibits similar characteristics across the time series. Moreover, K t serves as a time-invariant feature extractor, capturing recurring patterns that appear in the input series and aiding in forecasting short-term future values of the target variable. In Granger causality, these learned patterns reflect causal relationships between time series, which are essential for causal discovery <ref type="bibr" target="#b35">[Nauta, 2018]</ref>. To ensure the generality of STIC, we employ a simple feed-forward neural network (FNN) f 1 : R c×d×τ → R d×d×τ to extract shared information from each K t ⊙ W ψ , ψ = 1, ..., c. Furthermore, we impose a constraint to prohibit self-loops in the estimated window causal matrix Ŵ when the time lag is zero. That is:  where Ŵτ i,j represents the estimated binary existence of the causal effect of X i on X j with a time delay of τ ∈ {0, ..., τ }, and p is a threshold used to eliminate edges with low probability of existence.</p><formula xml:id="formula_32">Ŵτ i,j =    0 if i = j and τ = 0 0 if Ŵτ i,j &lt; p 1 else , (<label>5</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Mechanism-Invariance Block</head><p>As stated in Definition 3, the causal conditional probability relationships among the time series remain unchanged as time varies. Consequently, the causal functions between variables also remain constant over time. With this in mind, our objective in B m is to find a unified transform function f 2 : R d×τ → R d×τ that accommodates all window observations. To achieve this goal, as depicted in Figure <ref type="figure" target="#fig_1">2</ref>, we employ a convolution kernel K m ∈ R d×τ as f 2 . This kernel performs a Hadamard product operation with each window W ψ ∈ R d×τ in W , where ψ = 1, ..., c. Subsequently, we employ the Parametric Rectified Linear Unit (PReLU) activation function <ref type="bibr" target="#b36">[Zhu et al., 2017]</ref> to obtain the output W ψ ∈ R d×τ ,</p><formula xml:id="formula_33">W ψ = P ReLU (K m ⊙ W ψ )<label>(6)</label></formula><p>Each W ψ represents the transformed matrix obtained from the window observation W ψ by a unified transform function f 2 implemented with convolution kernel K m . Each W ψ is finally used to predict X τ +ψ . Note that this transform function f 2 can also be composed of N different but equal dimensional kernels K 1 m , ..., K N m ∈ R d×τ , which are nested to perform complex nonlinear transformations. After f 2 , the value inside the window W ψ ∈ R d×τ is then pressed for Ŵ-selected column summation to predict X τ +ψ ∈ R d .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Parallel Blocks for Joint Training</head><p>So far, we have obtained the estimated window causal matrix Ŵ by using B t . In addition, we also obtained the transformed matrix W with B m . We used convolutional neural networks in both B t and B m . Their structures are similar, but their functions and purposes are different. In B t , we focus on the shared underlying unified structure of all window observations. Following the Definition 2 of short-term time invariance, we choose a convolutional neural network structure with translation invariance <ref type="bibr" target="#b37">[Kayhan and</ref><ref type="bibr">Gemert, 2020, Singh et al., 2023]</ref>. We expect that f 1 with K t as the main component can extract the invariant structure of the window representation W . In B m , we focus on the convolution kernel K m , which is expected to serve as a unified transform function f 2 to satisfy the Definition 3 of short-term mechanism invariance and perform complex nonlinear transformations.</p><p>Based on Definition 4 described in Section 2.5, after obtaining the estimated window causal matrix and the transform functions between variables, we can combine the outputs from the time-invariance and mechanism-invariance blocks and using Ŵ-selected column summation to predict X . We consider that the time-invariance block facilitates the identification of parent-child relationships between variables, formalized as Ŵ, while the mechanism-invariance block helps to explore the generative mechanisms, i.e., transform functions. Consequently, we can naturally combine the outputs Ŵ and W . Specifically, by utilizing Ŵ and the computed W ψ , ψ = 1, ..., c, we can ultimately obtain the estimates X τ +ψ , namely Ŵ-selected column summation,</p><formula xml:id="formula_34">X τ +ψ = τ τ =0 W τ ψ ⊙ Ŵτ (7)</formula><p>Here, we need to consider each τ ∈ {0, ..., τ } and combine the estimated window causal matrix Ŵ with the corresponding transformed window observations W ψ obtained through B m to obtain the values of X τ +ψ . Our ultimate goal is to find a window causal matrix Ŵ that satisfies the conditions by optimizing the squared error loss (MSE) L between the predicted X and the ground truth X at each time point t. The final auto-regressive equation is expressed as follows:</p><formula xml:id="formula_35">L = T t=τ +1 d i=1 ||X t i -X t i || 2 2 (8)</formula><p>We adopt the gradient ▽L to optimize the parameters within the time-invariance and mechanism-invariance blocks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment Results</head><p>In this section, we present a comprehensive series of experiments on both synthetic and benchmark datasets to verify the effectiveness of the proposed STIC. Following the experimental setup of <ref type="bibr" target="#b4">[Runge et al., 2019</ref><ref type="bibr" target="#b5">, Runge, 2020]</ref>, we compare STIC against the constraint-based approaches such as PCMCI <ref type="bibr" target="#b4">[Runge et al., 2019]</ref> and PCMCI+ <ref type="bibr" target="#b5">[Runge, 2020]</ref>, the score-based approaches such as DYNOTEARS <ref type="bibr">[Pamfil et al., 2020]</ref>, and the Granger-based approaches TCDF <ref type="bibr" target="#b7">[Nauta et al., 2019]</ref>, CUTS <ref type="bibr" target="#b8">[Cheng et al., 2022]</ref> and CUTS+ <ref type="bibr" target="#b9">[Cheng et al., 2023]</ref>.</p><p>Our causal discovery algorithm is implemented using PyTorch. The source code for our algorithm is publicly available at the following URL<ref type="foot" target="#foot_0">foot_0</ref> . Both the time-invariance block and mechanism-invariance block are implemented using convolutional neural networks.</p><p>Firstly, we conducted experiments on synthetic datasets, encompassing both linear and non-linear cases. The methods of generating synthetic datasets for both linear and non-linear cases will be introduced separately in Section 4.2. Secondly, we proceeded to perform experiments on benchmark datasets to demonstrate the practical value of our model in Section 4.3. Thirdly, to evaluate the sensitivity of hyper-parameters, such as the learning rate (default 1e -5 ), the predefined τ (default 0.4d) and the threshold p (default 0.3), we conducted ablation experiments as detailed in Section 4.4.</p><p>We employ two kinds of evaluation metrics to assess the quality of the estimated causal matrix: the F1 score and precision. A higher F1 score indicates a more comprehensive estimation of the window causal matrix, while a higher precision indicates the ability to identify a larger number of causal edges. In this paper, we consider causal edges with different time lags for the same pair of variables as distinct causal edges. Specifically, if there exists a causal edge from X i to X j with a time lag of τ 1 , and another causal edge from X i to X j with the time lags of τ 2 , where i ̸ = j and τ 1 ̸ = τ 2 , we regard these as two separate causal edges. Due to the need to predefine the maximum time lag in STIC, we truncate the estimated Ŵ ∈ R d×d×(τ +1) to Ŵ ∈ R d×d×( τ +1) and then compute the evaluation metrics. We handle other baselines (such as PCMCI, PCMCI+, DYNOTEARS, CUTS, CUTS+) requiring a predefined maximum time lag parameter in the same manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Baselines</head><p>We select six state-of-the-art causal discovery methods as baselines for comparison:</p><p>• PCMCI <ref type="bibr" target="#b4">[Runge et al., 2019]</ref> is a notable work that extends the PC algorithm <ref type="bibr" target="#b23">[Kalisch and Bühlman, 2007]</ref> for causal discovery from time-series data. The source code for PCMCI is available at <ref type="url" target="https://github">https://github</ref>. com/jakobrunge/tigramite. PCMCI divides the causal discovery process into two components: the identification of relevant sets through conditional independence tests and the direction determination. It assumes causal stationarity, the absence of contemporaneous causal links, and no hidden variables. Specifically, the PC-stable algorithm <ref type="bibr" target="#b39">[Colombo et al., 2014]</ref> is employed to remove irrelevant conditions through iterative independence tests. Furthermore the Multivariate Conditional Independence test is conducted to address false-positive control in scenarios with highly interdependent time series.</p><p>• PCMCI+ <ref type="bibr" target="#b5">[Runge, 2020]</ref> improves upon PCMCI by reducing the number of independence tests and optimizing the selection of conditional sets, resulting in superior effectiveness and efficiency in the same experimental setting. The source code is also available at <ref type="url" target="https://github.com/jakobrunge/tigramite">https://github.com/jakobrunge/tigramite</ref>. PCMCI+ overcomes the limitation of the "no contemporaneous causal links" assumption in PCMCI. PCMCI+ expedites the selection of conditional sets by testing all time-lagged pairs conditional on only the strongest p adjacencies in each p-iteration, without evaluating all p-dimensional subsets of adjacencies. Moreover, intra-slice sets are introduced to further refine the determination of all structures.</p><p>• DYNOTEARS <ref type="bibr">[Pamfil et al., 2020]</ref> represents a groundbreaking advancement in the field of causal discovery from time-series data by transforming the combinatorial graph search problem into a continuous optimization problem. The details of this work can be found in the repository located at <ref type="url" target="https://github.com/ckassaad/causal_discovery_for_time_series">https://github.com/ckassaad/ causal_discovery_for_time_series</ref>. This approach characterizes the acyclicity constraint as a smooth equality constraint through the minimization of a penalized loss while adhering to the acyclicity constraint.</p><p>• TCDF <ref type="bibr" target="#b7">[Nauta et al., 2019]</ref> is an outstanding work that utilizes attention-based convolutional neural networks (CNNs) to explore causal relationships between time series and the time delay between cause and effect. The code for TCDF can be accessed at <ref type="url" target="https://github.com/M-Nauta/TCDF">https://github.com/M-Nauta/TCDF</ref>. By leveraging Granger causality, TCDF predicts one time series based on other time series and its own historical values, employing CNNs to identify and analyze causal relationships within time-series data.</p><p>• CUTS <ref type="bibr" target="#b8">[Cheng et al., 2022]</ref> is an outstanding neural Granger causal discovery algorithm for jointly imputing unobserved data points and building causal graphs, by incorporating two mutually boosting modules (latent data prediction and causal graph fitting) in an iterative framework. fter hallucinating and registering unstructured data, which might be of high dimension and with complex distribution, CUTS builds a causal adjacency matrix with imputed data under sparse penalty. The code for CUTS is available at <ref type="url" target="https://github.com/jarrycyx/UNN/tree/main/CUTS">https://github.com/ jarrycyx/UNN/tree/main/CUTS</ref>. CUTS is a promising step toward applying causal discovery to real-world applications with non-ideal observations.</p><p>• CUTS+ <ref type="bibr" target="#b9">[Cheng et al., 2023]</ref> is built on the Granger-causality-based causal discovery method CUTS and increases scalability through coarse-to-fine discovery and message-passing-based methods. The code for CUTS+ can be accessed at <ref type="url" target="https://github.com/jarrycyx/UNN/tree/main/CUTS_Plus">https://github.com/jarrycyx/UNN/tree/main/CUTS_Plus</ref>. CUTS+ significantly improves causal discovery performance on high-dimensional data with various types of irregular sampling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experiments on Synthetic Datasets</head><p>We generate synthetic datasets in the following manner. Firstly, we consider several typical challenges <ref type="bibr" target="#b4">[Runge et al., 2019</ref><ref type="bibr" target="#b5">, Runge, 2020]</ref> with contemporaneous and time-lagged causal dependencies, following an additive noise model. We set the ground truth maximum time lag to 0.4d and initialize the existence of each edge in the true window causal matrix W with a probability of 50%. For each variable X i , its relation to with its parents P a G (X i ) is defined as</p><formula xml:id="formula_36">X i = f i (P a G (X i )) + ε i</formula><p>, where f i represents the ground truth transformation function between X i 's parents P a G (X i ) and X i . If X j ∈ P a τ G (X i ), then in the ground truth causal matrix W, W τ ji = 1. Secondly, for linear datasets, each f i is defined by a weighted linear function, while for nonlinear datasets, each f i is defined using a weighted cosine function. We sample the weights from a uniform distribution, such that if a causal edge exists, the corresponding weight in the additive noise model is sampled from the interval U (-2, -0.5] ∪ [0.5, 2) to ensure non-zero values. For non-causal edges, the weight is set to 0. The noise term ε i follows either a standard normal distribution N (0, 1) or is uniformly sampled from the interval U [0, 1]. These data-generating procedures are similar to those used by the PCMCI family <ref type="bibr" target="#b4">[Runge et al., 2019</ref><ref type="bibr" target="#b5">, Runge, 2020]</ref> and CUTS family <ref type="bibr" target="#b8">[Cheng et al., 2022</ref><ref type="bibr" target="#b9">[Cheng et al., , 2023]]</ref>.</p><p>In the following, we present different results on linear Gaussian datasets (Section 4.2.1), nonlinear Gaussian datasets (Section 4.2.2), and linear uniform datasets (Section 4.2.3) to demonstrate the superiority of our model. Specifically, to reduce the impact of random initialization, we conduct 10 experiments for each type of datasets and report the mean and variance of the experimental results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Linear Gaussian Datasets</head><p>The data generation process for linear Gaussian datasets follows the relationship X i = w i P a G (X i ) + ε i , where ε i is sampled from a standard normal distribution N (0, 1). To demonstrate the capability of our model in causal discovery from time-series data on datasets of varying sizes, we compare STIC with baselines under different conditions, including different numbers of variables (d = {5, 10, 15, 20}) and different lengths of time steps (T = {100, 200, 500, 1000}).</p><p>The results are summarized in Figure <ref type="figure" target="#fig_3">4</ref>. Figure <ref type="figure" target="#fig_3">4</ref> left presents the variation of F1 score as the number of variables increases, while Figure <ref type="figure" target="#fig_3">4</ref> right shows the variation of precision with the number of variables. A comprehensive analysis of the experiments requires the joint consideration of both Figure <ref type="figure" target="#fig_3">4</ref> left and right. From a macroscopic perspective, our proposed STIC achieves the highest F1 scores on linear Gaussian datasets, while precision reaches the state-of-the-art levels in most cases. We will compare the performance of STIC and the baselines from two aspects of analysis: Aspect 1: The relationship between the number of variables d and the model when T remains constant. When the observed time steps is fixed at T = 1000, corresponding to the top-left graphs in Figure <ref type="figure" target="#fig_3">4</ref> left and right, we observe that as the number of variables increases, the F1 scores of all causal discovery methods tend to decrease. However, our proposed STIC achieves an average F1 score of 0.86, 0.77, 0.79, 0.77 and an average precision of 0.80, 0.62, 0.74, 0.72 across the four different numbers of variables, surpassing other strong baselines. By comparing the line plots in the corresponding positions of Figure <ref type="figure" target="#fig_3">4</ref> left and right, especially when T = 100, corresponding to the bottom-right graphs in Figure <ref type="figure" target="#fig_3">4</ref> left and right, we find that our proposed STIC achieves an average F1 score of 0.76, 0.76, 0.77, 0.65 and an average precision of 0.66, 0.70, 0.77, 0.65 across the four different numbers of variables, significantly outperforming other strong baselines.</p><p>In the case of fixed observed time steps, as the number of variables increases, constraint-based approaches such as PCMCI and PCMCI+ suffer from severe performance degradation because they require significant prior knowledge involvement in determining the threshold p, which determines the presence of causal edges. For score-based methods, the DYNOTEARS method shows relatively stable performance as the number of variables increases, but it does not achieve the optimal performance among all methods. As for Granger-based methods, CUTS and CUTS+ often suffer from poor performance due to the inability to recognize time lags. Our proposed STIC and the TCDF method achieve competitive results in terms of F1 scores. However, our method exhibits higher precision.</p><p>We attribute this superior performance to the window representation employed in STIC. By repeatedly extracting features from observed time series in different window observations, such representation acts as a form of data augmentation and aggregation. It enables a macroscopic view of common characteristics among multiple window observations, facilitating the learning of more accurate causal structures. Thus, our STIC model achieves optimal performance when the number of variables d changes. Aspect 2: The relationship between the observed time steps T and the model when d remains constant. When examining the impact of observed time steps T on the models while keeping the number of variables constant, we observe that our STIC method consistently maintains an F1 score of approximately 0.7 across different values of T . However, PCMCI+ and DYNOTEARS exhibit a significant decline in their F1 scores as T decreases. For instance, at T = 1000, PCMCI+ and DYNOTEARS perform similarly to our STIC method, but at T = 100, their F1 scores drop to half of that achieved by our STIC method. For PCMCI, it consistently falls behind our STIC method, regardless  of changes in T . While TCDF achieves a relatively consistent level of performance, it exhibits lower performance compared to our model. Furthermore, we find that after treating different time lags as different causal edges, the F1 scores and precisions of CUTS and CUTS+ are maintained at a relatively low level.</p><formula xml:id="formula_37">A PREPRINT 1XPEHURI2EVHUYHG9DULDEOHVG ) 1XPEHURI2EVHUYHG9DULDEOHVG 3UHFLVLRQ 3&amp;0&amp;, 3&amp;0&amp;, '&lt;127($56 7&amp;') &amp;876 &amp;876 67,&amp;RXUV</formula><p>For constraint-based approaches, the PCMCI and PCMCI+ algorithms perform poorly because as the number of samples decreases, the statistical significance of conditional independence cannot fully capture the causal relationships between variables. As for score-based methods, DYNOTEARS does not perform well on linear data. One possible reason is that DYNOTEARS heavily relies on acyclicity in its search, which may not converge to the correct causal graph. Regarding Granger-based methods, we believe that they are overly conservative and fail to accurately predict all correct causal edges.</p><p>In contrast, our STIC model is capable of predicting a greater number of causal edges, which is crucial for discovering new knowledge. This superior performance can be attributed to the design of the convolutional time-invariance block. This design allows for the extraction of more causal structure features from limited observed data, enabling a more accurate exploration of potential causal relationships even with a small number of samples. Consequently, our STIC model effectively addresses the challenge of causal discovery in low-sample scenarios, i.e., improving sample efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Nonlinear Gaussian Datasets</head><p>In this section, we perform experiments on nonlinear Gaussian datasets to evaluate the performance of STIC. We set the number of variables (d = 5) and the observed time steps (T = 1000). For each X i , its relationship with its parents P a G (X i ) is defined using the cosine function, and the noise term ε i follows the standard normal distribution.</p><p>The performance of STIC and the baselines is visualized in Figure <ref type="figure" target="#fig_4">5</ref>. It can be observed that STIC achieves an F1 score of 0.44, which is higher than all baselines (PCMCI: 0.41, PCMCI+: 0.43, DYNOTEARS: 0.22, TCDF: 0.41, CUTS: 0.24, CUTS+: 0.37). It can be seen that STIC achieves a higher F1 score despite having lower precision compared to the other baselines. For constraint-based methods (PCMCI and PCMCI+), one possible reason for achieving similar F1 scores with our proposed STIC is that the length of observed time steps is set to 1000, which is sufficient for statistical independence tests. Thus, the conditional independence tests can directly operate on the data without being affected by noise. Regarding score-based methods, we believe that DYNOTEARS uses a simple network that may not effectively Table <ref type="table">4</ref>: The results of ablation study on the linear Gaussian datasets with the number of variables (d = 5).</p><p>Learning Rate Max Time Lag Threshold lr=1e -4 lr=1e -5 lr=1e -6 τ = 2 τ = 3 τ = 4 p = 0.1 p = 0.3 p = 0.5 F1 0.77±0.005 0.76±0.008 0.78±0.004 0.76±0.008 0.68±0.004 0.60±0.003 0.43±0.001 0.76±0.008 0.80±0.020 precision 0.66±0.006 0.66±0.013 0.68±0.016 0.66±0.013 0.53±0.005 0.45±0.003 0.27±0.001 0.66±0.013 0.89±0.019 capture nonlinear transforms, leading to lower F1 scores. For Granger-based methods, although TCDF achieves a comparable F1 score to STIC (and even higher precision), the variance of STIC is significantly lower. This indicates that TCDF is highly unstable, and there is a considerable amount of uncertainty in causal discovery. One possible reason for this is that TCDF does not incorporate window representation like STIC, which could lead to inefficient training of the convolutional neural network. We find that CUTS and CUTS+ are not very good at causal discovery on nonlinear Gaussian datasets, and both F 1 and precision are lower than our STIC. One possible reason is that both models rely on graph neural networks and treat learnable graph structures as estimated causal graphs. However, the graph structure in the graph neural network is full of correlational relationships rather than causal relationships, so the output graph structure does not contain only causal relationships, resulting in a decrease in both F1 and precision. We believe that the robustness of our proposed STIC lies in the mechanism-invariance block, which repeatedly verifies the functional causal relationships within each single window, effectively reducing model instability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Linear Uniform Datasets</head><p>The linear uniform datasets is generated with observed time steps (T = 1000) by varied numbers of variables (d = {5, 10, 15, 20}). For each X i , f i is set as a linear function, while the noise term ε i follows a uniform distribution</p><formula xml:id="formula_38">U [0, 1].</formula><p>The performance of STIC and baselines are shown in Figure <ref type="figure" target="#fig_5">6</ref>. STIC outperforms baselines in terms of F1 score and precision in most cases, especially when the number of time series d is large. For constraint-based methods, PCMCI and PCMCI+ perform poorly in terms of F1 score and precision when the number of variables is relatively large (d = {10, 15, 20}). We consider that since conditional independence tests serve as strict indicators of causal relationships, they may fail due to the limited number of time steps and the presence of uniform noise. Moreover, PCMCI cannot determine intra-slice causal relationships and performs much worse than our STIC model in terms of F1 score and precision. For score-based methods, DYNOTEARS identifies causal relationships by fitting auto-regressive coefficients between variables, treating them as estimated causal relationships. However, due to the strong influence of noise, DYNOTEARS fails to recognize causal relationships in the linear uniform datasets. Interestingly, TCDF, which shows competitive performance compared to our STIC model in Section 4.2.1, performs particularly poorly on the linear uniform datasets. From the high precision and low F1 score of TCDF, we can deduce that the uniform distribution introduces many incorrectly estimated causal edges during the process of estimating temporal causality based on Granger causality using past value of other variables. The F1 score and precision of CUTS and CUTS+ further support the idea that Granger causality is not well applicable to linear uniform datasets. One possible reason is that for a uniform distribution, its inverse transformation equation still exists, which leads to the performance degradation of finding causality from correlation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experiments on Benchmark Datasets</head><p>In this section, we utilize FMRI benchmark datasets, a common neuroscientific benchmark dataset called Functional Magnetic Resonance Imaging <ref type="bibr" target="#b40">[Smith et al., 2011]</ref>, to explore and discover brain blood flow patterns. The dataset contains 28 different underlying brain networks with the number of observed variables (d = {5, 10, 15}). For each of the 28 brain networks, we observe 200 time steps for causal discovery. The results are reported in Table <ref type="table" target="#tab_4">3</ref>.</p><p>The results demonstrate that STIC achieves the highest average F1 scores on all kinds of observed variables, surpassing the average F1 scores of PCMCI, PCMCI+, DYNOTEARS, TCDF, CUTS and CUTS+. Moreover, in terms of precision, STIC achieves significantly higher precisions than those of the other baselines. For constraint-based methods, such as PCMCI and PCMCI+, their poor performance on the FMRI datasets may be attributed to the short length of observed time steps, which affects their ability to accurately test for conditional independence. Regarding DYNOTEARS, we believe that acyclicity regularizers still limit its performance. In comparison, our STIC model outperforms TCDF, CUTS and CUTS+ by utilizing a window representation, which enhances the representation of observed data within each window. This enables more accurate learning of common causal features and structures across multiple windows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study</head><p>We conduct ablation experiments on the linear Gaussian datasets with the number of variables (d = 5), to investigate the impact of different hyper-parameters on the experimental results, such as the learning rate (default: 1e -5 ), the predefined maximum time lag (default: 0.4d = 2), and the threshold p (default: 0.3). Specifically, we vary the learning rate by increasing it to 1e -4 and decreasing it to 1e -6 . We also increased the predefined maximum lag to τ = 3 and τ = 4, respectively, and change the threshold to p = 0.1 or p = 0.5. The empirical results are summarized in Table <ref type="table">4</ref>.</p><p>• The learning rate lr: The experiments reveal that manipulating the learning rate, either by increasing or decreasing it, has little effect on the F1 score and precision. This finding suggests that our convolutional neural network architecture is not sensitive to changes in the learning rate, simplifying the parameter tuning process.</p><p>• The predefined maximum time lag τ : However, increasing the predefined maximum lag τ gradually deteriorates performance. We speculate that this decline occurs because, with a longer lag, the window for observations expands, potentially causing STIC to learn multi-hop causal edges (X</p><formula xml:id="formula_39">i τ1 -→ X j τ2 -→ X k ) as single-hop causal edges (X i τ1+τ2 -→ X k ).</formula><p>Addressing this issue could be a focus for future research.</p><p>• The threshold p: Furthermore, comparing the default setting to STIC with p = 0.1, we observe a significant decline in both F1 score and precision when the threshold is lower. When comparing the default setting to STIC with p = 0.5, we find that while the F1 score remains relatively stable, precision notably improves when the threshold is increased. These findings indicate that reducing the threshold adversely affects the model's ability to explore causal edges, while setting a higher threshold may cause the model to consider nearly all estimated edges as causal, resulting in increased precision but a similar F1 score. Thus, the threshold plays a pivotal role in discovering more causal edges, and a trade-off needs to be made.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>This study presents two kinds of short-term invariance-based convolutional neural networks for discovery causality from time-series data. Major findings include: (1) our methods, based on gradients, effectively discover causality from time-series data;</p><p>(2) convolutional neural network based on short-term invariance improves the sample efficiency of causal discovery. (3) our proposed STIC demonstrates significantly superior performance compared to baseline causal discovery algorithms. In this section, we discuss these results in detail.  <ref type="bibr">et al., 2020]</ref>, and TCDF <ref type="bibr" target="#b7">[Nauta et al., 2019]</ref>, CUTS <ref type="bibr" target="#b8">[Cheng et al., 2022]</ref> and CUTS+ <ref type="bibr" target="#b9">[Cheng et al., 2023]</ref> within Grangerbased approaches. Including our proposed STIC, these gradient-based methods aim to optimize estimated causal matrix by maximizing or minimizing constrained functions. With the rapid advancement and widespread adoption of deep Neural Networks (NNs), researchers have begun employing NNs to infer nonlinear Granger causality, demonstrating the effectiveness of gradient-based methods in causal discovery <ref type="bibr" target="#b41">[Tank et al., 2021</ref><ref type="bibr" target="#b42">, Wu et al., 2021</ref><ref type="bibr" target="#b43">, Khanna and Tan, 2019]</ref>. In our approach, we maintain the assumption and the constrained functions of Granger causality so that our method remains effective in discovering causal relationships.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Why can STIC find the true causality</head><p>As time progresses, the values of observed variables change due to statistical shifts in distributions. However, the causal relationships between the variables remain the same. For example, carbohydrate intake may lead to an increase in blood glucose, but the specific magnitude of the increase may vary with covariates such as body weight. The "lead" property is used as an indicator of causal relationships, i.e., invariance <ref type="bibr" target="#b44">[Magliacane et al., 2018</ref><ref type="bibr" target="#b45">, Rojas-Carulla et al., 2018</ref><ref type="bibr" target="#b46">, Santos, 2021</ref><ref type="bibr" target="#b47">, Li et al., 2021]</ref>. In this paper, we observe that some causal relationships may also vary over time. Therefore, we make a more reasonable assumption, namely short-term time invariance and mechanism invariance <ref type="bibr" target="#b3">[Entner and Hoyer, 2010</ref><ref type="bibr">, Liu et al., 2023</ref><ref type="bibr">, Zhang et al., 2017]</ref>. Building on these two forms of short-term invariance, we posit that both the window causal matrix W and the transform functions f remain unchanged in the short term. For example, within a few days (short-term), since covariates affecting blood glucose levels, such as body weight, remain nearly constant, the increase in blood glucose levels due to carbohydrate intake is also essentially constant. The short-term mechanism invariance proposed in this paper is also considered an invariant principle <ref type="bibr" target="#b48">[Liu et al., 2022]</ref>. Building on these forms of invariance, a natural extension is the introduction of parallel time-invariance and mechanism-invariance blocks for joint training, as proposed in this paper. Through the theoretical validation of convolution in Section 2.4, we further affirm the applicability of convolution to causal discovery from time-series data. Additionally, the Granger causality is commonly employed to examine short-term causal relationships <ref type="bibr" target="#b49">[Ahmad et al., 2005]</ref>, which further aligns with our assumptions. Under the premise of theoretical soundness and practical applicability, our STIC framework proves highly effective.</p><p>5.2 What contributes to the exceptional performance of STIC?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">High F1 scores and precisions</head><p>The experiments conducted on both synthetic and FMRI benchmark datasets in Section 4 demonstrate that our STIC model achieves the state-of-the-art F1 scores and precisions in most cases. We attribute the performance improvement to the incorporation of the window representation, the time-invariance block, and the mechanism-invariance block. The window representation serves as a form of data augmentation and aggregation, providing a macroscopic understanding of common features across multiple window observations, thereby facilitating the learning of more accurate causal structures. The time-invariance block extracts common features from multiple window observations and achieves effective information aggregation, enhancing sample efficiency and enabling the model to achieve high performance.</p><p>The mechanism-invariance block, with nested convolution kernels, iteratively examines the functional transform within each individual window, enabling complex nonlinear transformations. With improved accuracy in both causal structures and complex nonlinear transformations, STIC demonstrates exceptional performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">High sample efficiency</head><p>The window representation, introduced in Section 3.1, facilitates the segmentation of the entire observed dataset X ∈ R d×T into c = T -τ -1 partitions, leveraging only a predefined hyperparameter τ . Each window observation W ψ , where ψ = 1, ..., c, is ensured to satisfy both short-term time invariance and mechanism invariance simultaneously. This representation method, similar to batch training techniques <ref type="bibr" target="#b50">[Liang et al., 2006</ref><ref type="bibr" target="#b51">, Li et al., 2014</ref><ref type="bibr" target="#b52">, Hong et al., 2020]</ref> optimizes data utilization, thus enhancing sample efficiency. Moreover, another pivotal aspect contributing to sample efficiency is the novel invariance-based convolutional neural network design. This architecture enables the extraction of richer causal structure features from limited observed data, facilitating more accurate exploration of potential causal relationships even with a limited length of observed time steps. Consequently, our STIC model effectively tackles the challenge of causal discovery in low-sample scenarios, thereby improving sample efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion, limitations and Future Works</head><p>This paper introduces STIC, a novel method designed for causal discovery from time-series data by leveraging both short-term time invariance and mechanism invariance. STIC employs sliding windows in conjunction with convolutional neural networks to incorporate these two kinds of invariance, and then transforms the searching for window causal matrix into a continuous auto-regressive problem. The compatibility between causal structures in time series and convolutional neural networks is supported by our theoretical analysis, reinforcing the rationale behind STIC's design.</p><p>Our experimental results on synthetic and benchmark datasets demonstrate the efficiency and stability of STIC, particularly when dealing with datasets that have shorter lengths of observed time steps. It showcases the effectiveness of the short-term invariance-based approach in capturing temporal causal structures.</p><p>However, STIC has certain limitations that require further investigation. Firstly, while STIC demonstrates effectiveness under this assumption, it becomes constrained when faced with non-additive noise. Future research should aim to develop more comprehensive approaches capable of handling various types of non-additive noise. Secondly, the manual predefined maximum lag used in STIC may pose limitations. Ablation experiments indicate that this hyperparameter setting can lead to the model learning multi-hop causal edges as single-hop causal edges. To overcome this, future research should explore more advanced blocks, such as attention mechanisms, to further enhance the performance of STIC.</p><p>In summary, STIC represents a promising research direction for addressing the challenges of causal discovery from time-series data, and we hope that STIC can discover new causal knowledge and provide new research ideas for medical and other fields.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A example showing the correspondence among the given observed variables, the underlying window causal graph, and the window causal matrix. The observed dataset consists of d = 5 observed variables, and the true maximum lag τ is 2.In W ∈ R 5×5×3 , for each W τ i,j represents the causal effect of X i on X j with τ time lags. For example, the blue lines in window causal graph indicate the following three causal effects with time lags τ = 2 at any time step t, i.e. W 2 1,3 = 1 ⇒ X 1</figDesc><graphic coords="3,148.53,177.00,284.05,77.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: An illustration of the STIC framework. Let X = {X 1 , • • • , X d } ∈ R d×T be the observed dataset, representing d observed continuous time series of the same length T . First, we convert the observations of the first T -1 time steps, X 1:T -1 = {X 1:T -1</figDesc><graphic coords="6,72.00,79.33,468.00,203.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Window representation. First, we get c matrices W ψ by sliding window with predefined window length τ and step size 1, where each W ψ ∈ R d×τ , ψ = 1, ..., c represents the data we observe in the window. Then, we concatenate the obtained W ψ together to get the final window representation W ∈ R d×τ ×c .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The results of F1 (detailed in Figure 4 left) and precision (detailed in Figure 4 right) evaluated on linear Gaussian datasets with varying numbers of variables (d) and observed time steps (T The observed data X is generated by sampling d time series with T observed time steps from a linear Gaussian distribution. We consider different values of d ranging from 5 to 20, and varying observed time steps T including 100, 200, 500, and 1000.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The results of F1 and precision evaluated on nonlinear Gaussian datasets. We fix numbers of variables d = 5 and observed time steps T = 1000.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: The results of F1 and precision evaluated on linear uniform datasets with fixed observed time steps (T = 1000) and the number of variables (d) ranging from 5 to 20.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>5. 1</head><label>1</label><figDesc>What contributes to the effectiveness of STIC? 5.1.1 Why can STIC find causal relationships Numerous gradient-based methods have been developed, such as DYNOTEARS within score-based approaches [Pamfil</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Summary of symbol definitions in Section 2.</figDesc><table><row><cell>Symbol</cell><cell>Description</cell></row><row><cell>d</cell><cell>The number of observed variables</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Summary of symbol definitions in Section 3.</figDesc><table /><note><p><p>Symbol Description τ</p>The predefined maximum time lag τ The predefined window length, τ</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>The results of F1 and precision evaluated on FMRI dataset. In terms of average of both F1 and precision, STIC outperforms the other baselines. Moreover, STIC shows a better stablity in view of variance.</figDesc><table><row><cell></cell><cell>PCMCI</cell><cell>PCMCI+</cell><cell>DTNOTEARS</cell><cell>TCDF</cell><cell>CUTS</cell><cell>CUTS+</cell><cell>STIC (ours)</cell></row><row><cell>d = 5</cell><cell cols="2">F1 precision 0.31±0.014 0.33±0.016 0.38±0.004 0.37±0.009</cell><cell>0.43±0.009 0.31±0.017</cell><cell cols="4">0.42±0.007 0.35±0.001 0.36±0.034 0.45±0.003↑ 0.44±0.006 0.22±0.007 0.29±0.035 0.70±0.030↑</cell></row><row><cell>d = 10</cell><cell cols="2">F1 precision 0.34±0.001 0.37±0.00 0.24±0.001 0.31±0.001</cell><cell>0.20±0.002 0.33±0.004</cell><cell cols="4">0.42±0.001 0.11±0.001 0.44±0.001 0.47±0.001↑ 0.44±0.003 0.20±0.000 0.51±0.001 0.60±0.023↑</cell></row><row><cell>d = 15</cell><cell cols="2">F1 precision 0.19±0.002 0.20±0.001 0.27±0.003 0.35±0.007</cell><cell>0.19±0.011 0.11±0.010</cell><cell cols="4">0.35±0.002 0.14±0.001 0.26±0.020 0.53±0.006↑ 0.43±0.032 0.08±0.002 0.26±0.003 0.80±0.005↑</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://github.com/HITshenrj/STIC</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_1"><p>&amp; 0 &amp; , 3 &amp; 0 &amp; , ' &lt; 1 2 7 ( $ 5 6</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This study was supported in part by a grant from the <rs type="funder">National Key Research and Development Program of China</rs> [<rs type="grantNumber">2021ZD0110900</rs>] and the <rs type="funder">National Natural Science Foundation of China</rs> [<rs type="grantNumber">72293584</rs>].</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_NMDmTBC">
					<idno type="grant-number">2021ZD0110900</idno>
				</org>
				<org type="funding" xml:id="_XYW25SA">
					<idno type="grant-number">72293584</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Causation, correlation, and big data in social science research</title>
		<author>
			<persName><forename type="first">Josh</forename><surname>Cowls</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ralph</forename><surname>Schroeder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Policy &amp; Internet</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="447" to="472" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep structural causal models for tractable counterfactual inference</title>
		<author>
			<persName><forename type="first">Nick</forename><surname>Pawlowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel Coelho De</forename><surname>Castro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Glocker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="857" to="869" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fuzzy clustering-based deep learning for short-term load forecasting in power grid systems using time-varying and time-invariant features</title>
		<author>
			<persName><forename type="first">Kit</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<editor>
			<persName><forename type="first">Ka</forename><surname>Fai</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Cedric</forename><surname>Yiu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Dowon</forename><surname>Kim</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ahmed</forename><surname>Abu-Siada</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">1391</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">On causal discovery from time series data using fci. Probabilistic graphical models</title>
		<author>
			<persName><forename type="first">Doris</forename><surname>Entner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrik</forename><forename type="middle">O</forename><surname>Hoyer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="121" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Detecting and quantifying causal associations in large nonlinear time series datasets</title>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Runge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peer</forename><surname>Nowack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marlene</forename><surname>Kretschmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seth</forename><surname>Flaxman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dino</forename><surname>Sejdinovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science advances</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page">4996</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Discovering contemporaneous and lagged causal relations in autocorrelated nonlinear time series datasets</title>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Runge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Uncertainty in Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1388" to="1397" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Dynotears: Structure learning from time-series data</title>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<editor>
			<persName><forename type="first">Roxana</forename><surname>Pamfil</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Nisara</forename><surname>Sriwattanaworachai</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Shaan</forename><surname>Desai</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Philip</forename><surname>Pilgerstorfer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Konstantinos</forename><surname>Georgatzis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Paul</forename><surname>Beaumont</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Bryon</forename><surname>Aragam</surname></persName>
		</editor>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1595" to="1605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Causal discovery with attention-based convolutional neural networks</title>
		<author>
			<persName><forename type="first">Meike</forename><surname>Nauta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Doina</forename><surname>Bucur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christin</forename><surname>Seifert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning and Knowledge Extraction</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="312" to="340" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Cuts: Neural causal discovery from irregular time-series data</title>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Runzhao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongren</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinli</forename><surname>Suo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kunlun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qionghai</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Cuts+: Highdimensional causal discovery from irregular time-series</title>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianglong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongren</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinli</forename><surname>Suo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kunlun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qionghai</forename><surname>Dai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.05890</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Extending hilbert-schmidt independence criterion for testing conditional independence</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">27th Conference on Uncertainty in Artificial Intelligence (UAI 2011)</title>
		<editor>
			<persName><forename type="first">Bingyuan</forename><surname>Zhang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Joe</forename><surname>Suzuki</surname></persName>
		</editor>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2011">2011. 2023</date>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page">425</biblScope>
		</imprint>
	</monogr>
	<note>Kernel-based conditional independence test and application in causal discovery</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Causal discovery and inference: concepts and recent methodological advances</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Applied informatics</title>
		<imprint>
			<publisher>SpringerOpen</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Efficient and accurate causal inference with hidden confounders from genometranscriptome variation data</title>
		<author>
			<persName><forename type="first">Lingfei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Michoel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS computational biology</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">1005703</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Gherardo</forename><surname>Varando</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.03005</idno>
		<title level="m">Learning dags without imposing acyclicity</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Efficient neural causal discovery without acyclicity constraints</title>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Lippe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taco</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">An</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fangfu</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenchang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhibo</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.03187</idno>
		<title level="m">Boosting differentiable causal discovery via adaptive sample reweighting</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Investigating causal relations by econometric models and cross-spectral methods</title>
		<author>
			<persName><forename type="first">Clive Wj</forename><surname>Granger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econometrica: journal of the Econometric Society</title>
		<imprint>
			<biblScope unit="page" from="424" to="438" />
			<date type="published" when="1969">1969</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Clive</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Granger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michio</forename><surname>Hatanaka</surname></persName>
		</author>
		<title level="m">Spectral Analysis of Economic Time Series.(PSME-1)</title>
		<imprint>
			<publisher>Princeton university press</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">2066</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Time2graph: Revisiting time series modeling with dynamic shapelets</title>
		<author>
			<persName><forename type="first">Ziqiang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenjie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yueting</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guojie</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="3617" to="3624" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Time2graph+: Bridging time series and graph representation learning via multiple attentions</title>
		<author>
			<persName><forename type="first">Ziqiang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuo</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenjie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangchi</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunping</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Causation, prediction, and search</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clark</forename><forename type="middle">N</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Scheines</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Strong faithfulness and uniform consistency in causal inference</title>
		<author>
			<persName><forename type="first">Jiji</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Spirtes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Nineteenth conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the Nineteenth conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="632" to="639" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Uniform consistency in causal inference</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>James M Robins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Scheines</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Larry</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><surname>Wasserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="491" to="515" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Estimating high-dimensional directed acyclic graphs with the pc-algorithm</title>
		<author>
			<persName><forename type="first">Markus</forename><surname>Kalisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Bühlman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Survey and evaluation of causal discovery methods for time series</title>
		<author>
			<persName><forename type="first">Emilie</forename><surname>Charles K Assaad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Devijver</surname></persName>
		</author>
		<author>
			<persName><surname>Gaussier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="767" to="819" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Causal discovery from subsampled time series with proxy variables</title>
		<author>
			<persName><forename type="first">Mingzhou</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinwei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingjing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.05276</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Causal discovery from nonstationary/heterogeneous data: Skeleton estimation and orientation determination</title>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Biwei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiji</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clark</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI: Proceedings of the Conference</title>
		<imprint>
			<publisher>NIH Public Access</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2017</biblScope>
			<biblScope unit="page">1347</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Terence</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clive</forename><surname>Mills</surname></persName>
		</author>
		<author>
			<persName><surname>Granger</surname></persName>
		</author>
		<author>
			<persName><surname>Granger</surname></persName>
		</author>
		<title level="m">Spectral analysis, causality, forecasting, model interpretation and non-linearity. A Very British Affair: Six Britons and the Development of Time Series Analysis During the 20th Century</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="288" to="342" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains</title>
		<author>
			<persName><surname>David I Shuman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sunil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Frossard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE signal processing magazine</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="83" to="98" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Discrete signal processing on graphs: Graph fourier transform</title>
		<author>
			<persName><forename type="first">Aliaksei</forename><surname>Sandryhaila</surname></persName>
		</author>
		<author>
			<persName><surname>José</surname></persName>
		</author>
		<author>
			<persName><surname>Moura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="6167" to="6170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">On the graph fourier transform for directed graphs</title>
		<author>
			<persName><forename type="first">Stefania</forename><surname>Sardellitti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergio</forename><surname>Barbarossa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorenzo</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="796" to="811" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Solitons and bound states of the time-independent schrödinger equation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Norman</surname></persName>
		</author>
		<author>
			<persName><surname>Zabusky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical review</title>
		<imprint>
			<biblScope unit="volume">168</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">124</biblScope>
			<date type="published" when="1968">1968</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">On time independent schrödinger equations in quantum mechanics by the homotopy analysis method</title>
		<author>
			<persName><forename type="first">Jyotirmoy</forename><surname>Rana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shijun</forename><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theoretical and Applied Mechanics Letters</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="376" to="381" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A convolution and product theorem for the fractional fourier transform</title>
		<author>
			<persName><surname>Ahmed I Zayed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal processing letters</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="101" to="103" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Spatio-temporal change detection using granger sequence pattern</title>
		<author>
			<persName><forename type="first">Nat</forename><surname>Pavasant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masayuki</forename><surname>Numao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ken-Ichi</forename><surname>Fukui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence</title>
		<meeting>the Twenty-Ninth International Conference on International Joint Conferences on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5202" to="5203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Temporal causal discovery and structure learning with attention-based convolutional neural networks</title>
		<author>
			<persName><forename type="first">Meike</forename><surname>Nauta</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
		<respStmt>
			<orgName>University of Twente</orgName>
		</respStmt>
	</monogr>
	<note>Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Diverse neuron type selection for convolutional neural networks</title>
		<author>
			<persName><forename type="first">Guibo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaoxiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu-Yao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng-Lin</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3560" to="3566" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">On translation invariance in cnns: Convolutional layers can exploit absolute spatial location</title>
		<author>
			<persName><forename type="first">Osman</forename><surname>Semih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kayhan</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><forename type="middle">C</forename><surname>Van Gemert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="14274" to="14285" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Orthogonal transforms for learning invariant representations in equivariant neural networks</title>
		<author>
			<persName><forename type="first">Jaspreet</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandan</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Rana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="1523" to="1530" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Order-independent constraint-based causal structure learning</title>
		<author>
			<persName><forename type="first">Diego</forename><surname>Colombo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Marloes</surname></persName>
		</author>
		<author>
			<persName><surname>Maathuis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3741" to="3782" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Network modelling methods for fmri</title>
		<author>
			<persName><forename type="first">M</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karla</forename><forename type="middle">L</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gholamreza</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Salimi-Khorshidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><forename type="middle">F</forename><surname>Webster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">E</forename><surname>Beckmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">D</forename><surname>Nichols</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">W</forename><surname>Ramsey</surname></persName>
		</author>
		<author>
			<persName><surname>Woolrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuroimage</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="875" to="891" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Neural granger causality</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Tank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Covert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Foti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Shojaie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><forename type="middle">B</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="4267" to="4279" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Granger causal inference on dags identifies genomic loci regulating transcription</title>
		<author>
			<persName><forename type="first">Rohit</forename><surname>Alexander P Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bonnie</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><surname>Berger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Economy statistical recurrent units for inferring nonlinear granger causality</title>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Khanna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">F</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Domain adaptation by using causal inference to predict invariant conditional distributions</title>
		<author>
			<persName><forename type="first">Sara</forename><surname>Magliacane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thijs</forename><surname>Van Ommen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Claassen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Bongers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Versteeg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joris</forename><forename type="middle">M</forename><surname>Mooij</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Invariant models for causal transfer learning</title>
		<author>
			<persName><forename type="first">Mateo</forename><surname>Rojas-Carulla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Peters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1309" to="1342" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Domain generalization, invariance and the Time Robust Forest</title>
		<author>
			<persName><forename type="first">Gustavo Moneda Dos</forename><surname>Luis</surname></persName>
		</author>
		<author>
			<persName><surname>Santos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
		<respStmt>
			<orgName>Universidade de São Paulo</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Revisiting the valuable roles of global financial assets for international stock markets: Quantile coherence and causality-in-quantiles approaches</title>
		<author>
			<persName><forename type="first">Zhenghui</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiming</forename><surname>Ao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Mo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">15</biblScope>
			<biblScope unit="page">1750</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Towards robust and adaptive motion forecasting: A causal representation perspective</title>
		<author>
			<persName><forename type="first">Yuejiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Riccardo</forename><surname>Cadei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Schweizer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherwin</forename><surname>Bahmani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="17081" to="17092" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Is the indian stock market integrated with the us and japanese markets? an empirical analysis</title>
		<author>
			<persName><forename type="first">Ahmad</forename><surname>Khan Masood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shahid</forename><surname>Ashraf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shahid</forename><surname>Ahmed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">South Asia Economic Journal</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="193" to="206" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">A fast and accurate online sequential learning algorithm for feedforward networks</title>
		<author>
			<persName><forename type="first">Nan-Ying</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guang-Bin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paramasivan</forename><surname>Saratchandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Narasimhan</forename><surname>Sundararajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on neural networks</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1411" to="1423" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Efficient mini-batch training for stochastic optimization</title>
		<author>
			<persName><forename type="first">Mu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuqiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 20th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="661" to="670" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Graph convolutional networks for hyperspectral image classification</title>
		<author>
			<persName><forename type="first">Danfeng</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lianru</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Plaza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jocelyn</forename><surname>Chanussot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="5966" to="5978" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
