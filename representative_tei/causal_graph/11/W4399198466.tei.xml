<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SIG: Efficient Self-Interpretable Graph Neural Network for Continuous-time Dynamic Graphs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-05-29">29 May 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Lanting</forename><surname>Fang</surname></persName>
							<email>ltfang@bit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Beijing Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yulian</forename><surname>Yang</surname></persName>
							<email>yulianyang@seu.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Southeast University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kai</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Southeast University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shanshan</forename><surname>Feng</surname></persName>
							<email>fengky@bit.edu.cn</email>
							<affiliation key="aff2">
								<orgName type="institution">Centre for Frontier AI Research, A*STAR</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kaiyu</forename><surname>Feng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Beijing Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jie</forename><surname>Gui</surname></persName>
							<email>guijie@seu.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Southeast University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="laboratory">Purple Mountain Laboratories</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shuliang</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Beijing Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yew-Soon</forename><surname>Ong</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">SIG: Efficient Self-Interpretable Graph Neural Network for Continuous-time Dynamic Graphs</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-05-29">29 May 2024</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2405.19062v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-21T20:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>While dynamic graph neural networks have shown promise in various applications, explaining their predictions on continuous-time dynamic graphs (CTDGs) is difficult. This paper investigates a new research task: self-interpretable GNNs for CTDGs. We aim to predict future links within the dynamic graph while simultaneously providing causal explanations for these predictions. There are two key challenges: (1) capturing the underlying structural and temporal information that remains consistent across both independent and identically distributed (IID) and out-of-distribution (OOD) data, and (2) efficiently generating high-quality link prediction results and explanations. To tackle these challenges, we propose a novel causal inference model, namely the Independent and Confounded Causal Model (ICCM). ICCM is then integrated into a deep learning architecture that considers both effectiveness and efficiency. Extensive experiments demonstrate that our proposed model significantly outperforms existing methods across link prediction accuracy, explanation quality, and robustness to shortcut features. Our code and datasets are anonymously released at <ref type="url" target="https://github.com/2024SIG/SIG">https://github.com/2024SIG/SIG</ref>.</p><p>Preprint. Under review.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Graph neural networks (GNNs) have demonstrated remarkable efficacy in representing graphstructured data. However, the inherent opacity of GNNs poses challenges in comprehending and trusting their predictions, particularly in high-stakes domains such as fraud detection in financial systems <ref type="bibr" target="#b12">[13]</ref> or disease progression prediction in healthcare <ref type="bibr" target="#b13">[14]</ref>, where interpretability is important.</p><p>Recent advancements in explainable Graph Neural Networks (GNNs) have aimed to unravel the underlying rationale guiding GNN predictions <ref type="bibr" target="#b45">[46]</ref>. These models are broadly classified into two categories: post-hoc interpretable models <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21]</ref> and self-interpretable models <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b16">17]</ref>. Post-hoc interpretable models focus on elucidating the behaviors of the primary predictive GNN model after its construction without altering its structural or training aspects. Conversely, selfinterpretable models are inherently transparent in their decision-making processes, obviating the requirement for additional post-hoc techniques. Existing self-interpretable models include decision trees <ref type="bibr" target="#b11">[12]</ref>, subgraph extraction based models <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b4">5]</ref>, attention-based mechanisms <ref type="bibr" target="#b32">[33]</ref>, rulebased models <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b9">10]</ref>, and causal inference models <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b47">48]</ref>. These models offer interpretability naturally, enabling clear and understandable explanations of their predictions without the need for supplementary interpretive tools. This work tackles a novel research task: developing self-interpretable models specifically designed for continuous-time dynamic graphs (CTDGs). Unlike static graphs or discrete-time dynamic graphs (DTDGs), CTDGs continuously evolve with time, enabling more precise modeling of dynamic processes. However, achieving interpretability in CTDGs presents two challenges. The first challenge is susceptibility to shortcut features, which is a prevalent issue in most existing self-interpretable models. Shortcut features are patterns that provide good performance on test data but fail to generalize to out-of-distribution (OOD) data <ref type="bibr" target="#b8">[9]</ref>. Recent causal inference methods <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b16">17]</ref> have been developed to address this challenge. However, they are based on static graphs or DTDGs, and cannot effectively handle the CTDGs. The second challenge is the efficiency of the self-interpretable model. This challenge is amplified in CTDGs due to their constantly evolving structure. Different from static or discrete-time graphs, CTDGs undergo continuous node and edge additions/deletions, resulting in a much larger number of possible topologies. This significantly increases the computational burden of performing interventions in causal inference models for CTDGs. Exhaustive sampling of topologies becomes computationally expensive, while limited sampling might hinder model effectiveness.</p><p>Designing self-interpretable models for CTDGs is intricate, as the model must meet three critical requirements: (1) Handle both independent and identically distributed (IID) and out-of-distribution (OOD) data; (2) Capture invariant subgraphs in both structural and temporal aspects; (3) Perform interventions efficiently. To fulfill these requirements, we propose the self-interpretable GNN (SIG). SIG begins by analyzing the problem from a causal effect perspective and proposes a novel causal inference model, namely the Independent and Confounded Causal Model (ICCM). ICCM incorporates two key components: the Independent Causal Model (ICM) and the Confounded Causal Model (CCM). The ICM is designed for IID data, where the causal subgraph is the unique exogenous variable influencing the predictive label. In contrast, the CCM is tailored for OOD data, where shortcut features act as confounding factors, creating backdoor paths that result in spurious correlations between causal subgraphs and prediction labels. SIG employs interventions to disrupt these "backdoor paths" and mitigate the influence of confounding factors in CCM. To achieve efficient intervention optimization, SIG utilizes the Normalized Weighted Geometric Mean (NWGM) <ref type="bibr" target="#b40">[41]</ref> instead of directly pairing causal subgraphs or their representations with each element in the confounders set. During implementation, SIG leverages a deep learning clustering technique to approximate the actual confounders within CTDGs. SIG makes final predictions based on both temporal and structural representations from the CTDG, along with these confounders.</p><p>The main contributions of this paper are summarized as follows:</p><p>• We investigate a new research task on CTDGs, which outputs not only the prediction label but also a concise causal subgraph for the prediction. To the best of our knowledge, the proposed SIG is the first self-interpretable GNN for CTDGs that is capable of handling both IID and OOD data.</p><p>• We present a thorough causal analysis of SIG, elucidating the causal effects and underlying mechanisms. This theoretical analysis serves as the foundation for our innovative model design and optimization strategies.</p><p>• We develop a novel deep learning framework that implements theoretically established causal models, effectively and efficiently addressing challenges of self-interpretability on CTDGs.</p><p>• Extensive experiments on five real-world datasets demonstrate that SIG significantly outperforms state-of-the-art methods in link prediction, graph explanation, and handling OOD datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Dynamic Graph Neural Networks. Dynamic graph neural networks encompass two primary classifications: Discrete-Time Dynamic Graphs (DTDGs) <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b27">28]</ref> and Continuous-Time Dynamic Graphs (CTDGs) <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b14">15]</ref>. DTDGs comprise a sequence of static graph snapshots captured at regular time intervals <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b22">23]</ref>. CTDGs capture the evolution of graphs by considering modifications on the graph that occur continuously rather than discretely at predefined time steps <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b7">8]</ref>. These GNNs focus on modeling graph dynamics and fail to offer sufficient interpretability for the underlying processes.</p><p>Explainability of Graph Neural Networks. The majority of existing explainable GNNs fall into the category of post-hoc interpretable GNNs. These frameworks are formulated as an optimization task that maximizes the mutual information between a GNN's prediction and distribution of possible subgraph structures <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b28">29]</ref>. However, post-explanation methodologies may encounter inaccuracies or incompleteness in elucidating the genuine reasoning process of the underlying model and require iterative executions of the prediction model to delve into the intricate relationships between inputs and outputs, consequently incurring a notable computational overhead. Few efforts are devoted to self-interpretable GNNs. Prototype-based methods <ref type="bibr" target="#b46">[47]</ref> learn prototype vectors as explanations. These methods either fail to produce an explainable subgraph or depend on computationally expensive subgraph exploration techniques. Neighborhood-based methods <ref type="bibr" target="#b2">[3]</ref> extract nearest neighbors as explanations. Although these methods consider node and local structure similarity, they often fall short of constructing a truly interpretable subgraph. Subgraph extraction-based methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b18">19]</ref> identify the most influential subgraph for decision-making. They may neglect the influence of confounding factors, potentially leading to inaccurate explanations.</p><p>Causal Inference on Graph Neural Networks. Causal inference seeks to unveil and comprehend the causal variables responsible for observed phenomena. On real-world graphs, uncovering these causal variables becomes an act of explanation, revealing the "why" behind intricate relationships. Most existing methods focus on static graphs. These approaches either manipulate non-causal elements within a graph to create counterfactual graph data, as demonstrated in <ref type="bibr" target="#b37">[38]</ref>, or utilize implicit interventions at the representation level, as shown in <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b23">24]</ref>. The method most closely associated with this context is DIDA <ref type="bibr" target="#b47">[48]</ref>, an invariant rational discovery approach tailored specifically for DTDGs. DIDA requires the construction of an intervention set for each node and snapshot. When the graph is divided into too many snapshots, applying DIDA becomes time-consuming. Conversely, dividing the graph into too few snapshots leads to a loss of significant time-related information. The relationships between related studies and this work can be found in App. D.2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Problem Definition</head><p>This paper investigates the problem of developing a self-interpretable graph learning model tailored to the analysis of continuous-time dynamic graphs, with a particular emphasis on its inherent capabilities for link prediction and explainability. Definition 1 (Continuous-Time Dynamic Graph (CTDG)). A continuous-time dynamic graph G = (V, E, T ) comprises a set of vertices V, a set of edges E, and a time domain T . This graph evolves continuously over time t ∈ T , where at each time instance t, edges might undergo additions, removals, or changes in their characteristics. Formally, the graph G can be denoted as a sequence of edges G = ⟨e ij (t k )⟩. Each edge e ij (t k ) signifies an interaction occurring between the source node v i and the target node v j at time t k . Additionally, we introduce x e ij (t k ) to denote the feature vector of edge e ij (t k ), while x n i indicates the feature vector of node v i , Definition 2 (Self-interpretable GNN for CTDG). Given a CTDG G and two distinct nodes, v i ∈ V and v j ∈ V, the primary objectives of self-interpretable GNN are twofold: firstly, to accurately predict whether an edge will form between nodes v i and v j ; and secondly, to discern a causal subgraph that provides insights into the underlying reasons for the prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Casual Effect Look 4.1 Independent Causal Model (ICM)</head><p>The link prediction label can be influenced by both the structural topology and temporal dynamics <ref type="bibr" target="#b1">[2]</ref>. Consequently, this paper proposes to capture causal information emanating from both the structural and the temporal perspectives, as shown in Figure <ref type="figure" target="#fig_0">1 (a)</ref>. In this subsection, we formalize the causal inference <ref type="bibr" target="#b24">[25]</ref> by inspecting the causalities among six variables: the input graph G, the structural causal subgraph G s , the temporal causal subgraph G t , the temporal feature M T , the structural feature M S and the prediction label Y I . The following equations summarizes the core assumptions:</p><formula xml:id="formula_0">Assumption 1 (ICM). G t , G s := f ext (G), M T := f I t (G t ), M S := f I s (G s ), Y I := f I o (M T , M S )<label>(1)</label></formula><p>In this assumption, f ext performs the extraction of the causal subgraphs from the input graph G, f I t (•) and f I s (•) encode the causal subgraph into latent representations M T and M S , f I o calculates the ultimate prediction outcome.   </p><formula xml:id="formula_1">H S := f C s (C s ), H T := f C t (C t ), Y S := f S o (H S , U * ), Y T := f T o (H T , U * )<label>(2)</label></formula><p>Within these formulations, U * denotes the set of confounders, which can be either </p><formula xml:id="formula_2">{H S , G b , U } or {H T , G b , U }, f C s (•)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Combination of ICM and CCM (ICCM)</head><p>This subsection discusses the Independent and Confounded Causal Model (ICCM), which forms the foundation of our proposed SIG framework.</p><p>Recall that in ICM, we use M S and M T to capture the structural and temporal features from the causal subgraph. In CCM, we use H S and H T to denote the structural and temporal representations from the causal subgraph. To ensure consistency between these models, we define:</p><formula xml:id="formula_3">H S := M S , H T := M T .<label>(3)</label></formula><p>Assumption 3 summarizes the core principles guiding ICCM: Assumption 3 (ICCM).</p><formula xml:id="formula_4">Y I := f I o (H S , H T ), Y S := f S o (H S , U * ), Y T := f T o (H T , U * )<label>(4)</label></formula><p>where H S and H T are structural and temporal representations from the causal subgraphs, f I o , f S o and f H o are linear networks followed by a sigmoid activation functions. The following equations present the mathematical formulation of ICCM:</p><formula xml:id="formula_5">P Y I = y I |G = σ W I 1 f I S y (H S ) + W I 2 f I T y (H T ) ,<label>(5)</label></formula><formula xml:id="formula_6">P Y S = y S |do(G s = C s ) = E d∼D [σ W c 1 f s y (H S ) + W c 3 f u y (d) ],<label>(6)</label></formula><formula xml:id="formula_7">P Y T = y T |do(G t = C t ) = E d∼D [σ W c 2 f t y (H T ) + W c 4 f u y (d) ].<label>(7)</label></formula><p>where D denotes the set of confounding factors, W * * denotes the model parameters, f * y (•) denotes linear network, σ denotes the activation function. This becomes computationally expensive for large temporal networks. To address this, we leverage the Normalized Weighted Geometric Mean (NWGM) approximation <ref type="bibr" target="#b40">[41]</ref></p><formula xml:id="formula_8">, i.e., E d∼D [σ W c * f s y (H S ) + W c * f u y (d) ] ≈ σ E d∼D [W c * f s y (H S ) + W c * f u y (d)]</formula><p>. After applying NWGM, Equations 6 and 7 can be reformulated as:</p><formula xml:id="formula_9">P Y S = y S |do(G s = C s ) ≈ σ W c 1 f s y (H S ) + W c 3 E d∼D [f u y (d)] ,<label>(8)</label></formula><formula xml:id="formula_10">P Y T = y T |do(G t = C t ) ≈ σ W c 2 f t y (H T ) + W c 4 E d∼D [f u y (d)] .<label>(9)</label></formula><p>The designed causal model ICCM is exploited as the theoretical underpinning for implementing our deep learning framework, which will be presented in the next section.</p><p>5 Deep Learning Implementation</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Overview</head><p>ICCM relies on structural and temporal representations derived from constant causal subgraphs C s and C t . However, in real-world scenarios, these causal subgraphs are typically unobserved. To address this issue, SIG employs two causal subgraph extractors to extract structural and temporal subgraphs Ĉs and Ĉt from the input data. These extracted subgraphs are then used to approximate C s and C t . Figure <ref type="figure" target="#fig_1">2</ref> illustrates the overall structure of the SIG framework. First, the causal subgraph extraction aims to identify structural and temporal subgraphs Ĉs and Ĉt . These subgraphs are then encoded into hidden representations H S and H T . Subsequently, the confounder generation component produces a confounder dictionary D. Finally, both H S and H T are passed to the classifier f I o to generate y I . Simultaneously, along with the produced confounder dictionary, H S and H T are also fed into classifiers f S o and f T o to output y S and y T , respectively. We will delve deeper into the details of each module in the following sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Causal Subgraph Extracting and Encoding</head><p>Temporal causal subgraph extraction and encoding. Given a dynamic graph G and two nodes (u and v) for prediction, we initially generate two edge sequences S u and S v by selecting the top N most recent temporal edges linked to u and v, respectively. The parameter N functions as a dataset-specific hyper-parameter. If the number of edges linked to a node is fewer than N , all available connections will be retained. For each edge e ui (t k ) ∈ S u , a temporal encoding is performed using cos (t 0 -t k )ω <ref type="bibr" target="#b1">[2]</ref>, where t 0 denotes the timestamp used for predicting the edge's existence, ω = α -(i-1)/β d i=1 , with α and β representing hyperparameters. This encoding is combined with its corresponding edge features as</p><formula xml:id="formula_11">[cos (t 0 -t k ) ω ∥x e ui (t k )].</formula><p>Let F (0) u denote the stack of edge features within the sequence S u . A 1-layer MLP-mixer <ref type="bibr" target="#b34">[35]</ref> is employed to produce the final temporal representations, i.e., F u = MLP-mixer(F (0) u ). Two queries and keys are generated for node u and v using:</p><formula xml:id="formula_12">q u = W m 1 Mean(F u ), K v = W m 2 (F v ), q v = W m 1 Mean(F v ), K u = W m 2 (F u ).</formula><p>The subgraph is generated by:</p><formula xml:id="formula_13">M e v = Softmax q T u K v √ d , M e u = Softmax q T v K u √ d , Ĉt = TOP k (M e u , M e v ).<label>(10)</label></formula><p>Here, d denotes a specific hyperparameter, and M e * [k] represents the importance score assigned to the k-th edge within S * . Consequently, the highest top-k scores in M e v and M e u are selected to construct the temporal causal subgraphs Ĉt . Finally, the temporal representation H T is encoded by:</p><formula xml:id="formula_14">h t u = Mean {F v |v ∈ N T (u)} , h t v = Mean {F u |u ∈ N T (v)} , H T = [h t u ||h t v ],<label>(11)</label></formula><p>where N T (u) denotes the nodes linked to u in Ĉt .</p><p>Structural causal subgraph extraction and encoding. Structural node representation is encoded based on its n-hop neighborhood:</p><formula xml:id="formula_15">z u = x n u + Mean {x n v | v ∈ N n (u; t 0 -T, t 0 )} .</formula><p>Here, N n (u; t 0 -T, t 0 ) denotes the n-hop neighbors of node u with edge timestamps ranging from t 0 -T to t 0 , where T represents a dataset-specific hyperparameter. The node mask matrices are computed through the equations:</p><formula xml:id="formula_16">M n v = Softmax z T u Z v √ d , M n u = Softmax z T v Z u √ d , Ĉs = TOP k (M n u , M n v ).<label>(12)</label></formula><p>Here, Z u and Z v is the stack of the encoded node features of all nodes in N n (u; t 0 -T, t 0 ) and N n (v; t 0 -T, t 0 ), respectively. The nodes with the highest top-k scores in M n u and M n v are chosen to form the structural causal subgraph. The final structural representation H S is computed by:</p><formula xml:id="formula_17">h s u = x n u + Mean {x n i |i ∈ N n S (u)} , h s v = x n v + Mean {x n i |i ∈ N n S (v)} , H S = [h s u ||h s v ],<label>(13)</label></formula><p>where x n v represents the node feature of v, N n S (u) represents the n-hop neighbors of u in Ĉs .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Confounders Generation</head><p>The implementation of SIG necessitates the inclusion of a confounder dictionary, denoted as D.</p><p>However, the unavailability of confounders during the training phase presents a significant challenge.</p><p>To pragmatically tackle this issue, we approximate confounders with a representation matrix, denoted as</p><formula xml:id="formula_18">D = [d 1 , d 2 , . . . , d k ],</formula><p>where each d i denotes a distinct confounder type. Given the innate capability of deep learning models to naturally encapsulate contextual information within their higher-level layers <ref type="bibr" target="#b39">[40]</ref>, each confounder type is generated utilizing a deep learning clustering method.</p><p>Specifically, given the dynamic graph G = (V, E, T ), we adopt a dynamic GNN encoder <ref type="bibr" target="#b1">[2]</ref> to extract the representations for each link in E based on its temporal and structural subgraph, resulting in the matrix X ∈ R |E|×l , where l denotes the embedding dimension. By utilizing the deep learning clustering method VaDE <ref type="bibr" target="#b10">[11]</ref>, we group X into k clusters, i.e., {C 1 , . . . , C k } = VaDE(X ). The centroids within each cluster serve as indicators of the central tendencies, effectively summarizing the overall features or characteristics among subgraph information within the same cluster. Consequently, computing the cluster-wise average yields a representation for each cluster, resulting in a confounder dictionary with the shape D ∈ R k×l , where</p><formula xml:id="formula_19">D[i] = Mean(C i ).</formula><p>Finally, the expectation of confounders is computed by:</p><formula xml:id="formula_20">E d∼ D [f u y (d)] = | D| i=1 α i D[i], [α 1 , α 2 , . . . , α k ] = Softmax (W c 1 D) T W c 2 q |q| (<label>14</label></formula><formula xml:id="formula_21">)</formula><p>where W c 1 and W c 2 are learnable matrices. We set q = H S and q = H T for y S and y T , respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Prediction and Optimization</head><p>Given representations H S and H T , and the expectation of confounders E d∼ D [f u y (d)], we can make the final predictions based on Equations 5, 8, and 9.</p><p>Intuitively, if a subgraph Ĉ * is irrelevant to the final prediction Y , then changing the subgraph should not affect the prediction. In other words, a subgraph that is relevant to the prediction should have high mutual information with the label. Formally, the learning objectives of the proposed model can be formulated as follows:</p><p>max</p><formula xml:id="formula_22">Ω I( Ĉs , Y ) + I( Ĉt , Y ), s.t. Ĉs ⊥U * , Ĉt ⊥U * (<label>15</label></formula><formula xml:id="formula_23">)</formula><p>where Ω is the set of model parameters. I( Ĉ * , Y ) is the mutual information between the causal subgraph Ĉ * and the label Y , Ĉ * ⊥U * means that Ĉ * is independent of the unobserved variables U * .</p><p>Maximizing mutual information is equivalent to minimizing a variational upper bound of the risk functions <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b43">44]</ref>. Hence, we define the total learning objective of SIG as :</p><formula xml:id="formula_24">L = λ i R i (y I , y) + λ t R t (y T , y) + λ s R s (y S , y),<label>(16)</label></formula><p>where R i , R t , R s are risk functions of IID prediction, temporal intervention prediction, and structural intervention prediction, respectively. λ * are hyperparameters, and y is ground-truth label. This paper adopts cross-entropy loss as risk functions R * . Details are in App. C.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>In this section, we conducted extensive experiments on five different datasets. Our experiments aimed to answer the following questions: RQ1: Does SIG improve the performance of methods for link prediction in dynamic graphs? RQ2: What is the effectiveness and efficiency of SIG? RQ3: How well does SIG perform in mitigating OOD issues?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Experimental Settings</head><p>Dataset and evaluation metrics. We conducted experiments on five real-world datasets: Wikipedia, Reddit, MOOC, LastFM and SX. We employ the average precision (AP) and area under the curve (AUC) as the evaluation metrics for link prediction. We adopt fidelity (FID) w.r.t. sparsity (SP) as the evaluation metrics for graph explanation. Details are in App. E.1 and E.2.</p><p>Baselines. Note that the proposed SIG is the first self-interpretable GNN specifically designed for CTDGs. Given the limited studies in self-interpretable GNNs for dynamic graphs, our evaluation spans several comparisons by considering different types of baselines. (1) Initially, SIG undergoes comparison with three existing dynamic GNN models: TGN <ref type="bibr" target="#b29">[30]</ref>, TGAT <ref type="bibr" target="#b39">[40]</ref>, GM_ori and GM_50n <ref type="bibr" target="#b1">[2]</ref>. These models are designed to handle CTDGs. However, as they lack the capacity to produce explainable outcomes, our comparison primarily focuses on link prediction tasks across original datasets and synthetic OOD datasets. (2) Additionally, we compare the proposed model with four post-interpretable models, including an attention-based explainer (ATTN <ref type="bibr" target="#b38">[39]</ref>), a perturbing-based explainer (PBONE <ref type="bibr" target="#b38">[39]</ref>), a static graph explainer (PGExp <ref type="bibr" target="#b19">[20]</ref>), and a dynamic graph explainer (TGExp <ref type="bibr" target="#b38">[39]</ref>). These models were thoughtfully chosen to represent diverse graph explanation approaches. Given their post-interpretable nature, our comparison focuses solely on graph explanation tasks. (3) Further, we compare SIG with DIDA <ref type="bibr" target="#b47">[48]</ref>, a self-interpretable GNN for DTDG. Our comparative analysis with DIDA spans across all tasks. Details are in App. E.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Comparison with SOTA dynamic GNNs (RQ1)</head><p>Effectiveness. Table <ref type="table" target="#tab_1">1</ref> illustrates a comparative analysis between SIG and recent dynamic graph neural networks w.r.t. link prediction tasks. Among all dynamic graph neural networks, GM_ori and  SIG consistently outperformed all baselines across all datasets. Specifically, SIG surpassed the best baseline, GM_ori, in AP performance by an average of 1.25% and in AUC performance by 0.92%. These results highlight the effectiveness of SIG's novel causal inference model and its ability to capture complex temporal relationships within dynamic graphs, effectively removing the shortcut features that hinder performance.</p><p>Efficiency. Table <ref type="table" target="#tab_2">2</ref> shows the efficiency of our method w.r.t all dynamic GNN baselines. TGAT and TGN exhibit slower performance compared to SIG due to their utilization of more complex encoding networks. Additionally, DIDA operates at a slower pace than SIG as it necessitates gathering a confounder dictionary in each snapshot. GM demonstrates slightly better efficiency than SIG as GM does not output an explainable subgraph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Comparison with SOTA Graph Explanation Models (RQ2)</head><p>Effectiveness. Table <ref type="table" target="#tab_3">3</ref> presents a comparative analysis between SIG and state-of-the-art graph explanation methodologies. In this comparison, the category of 'Post-hoc' block denotes the application of post-hoc interpretable models. Building upon prior techniques <ref type="bibr" target="#b38">[39]</ref>, we apply these post-hoc interpretable models to two dynamic GNN models: TGAT and TGN. Meanwhile, the 'Self-int' block refers to the self-interpretable GNNs designed specifically for dynamic graphs.</p><p>Our empirical investigation reveals that all post-hoc interpretable models require over 24 hours to process the Reddit, MOOC, LastFM, and SX datasets. This extensive computational time is primarily attributed to their reliance on complex computation methodologies for extracting explainable subgraphs. For instance, TGExp utilizes Monte Carlo Tree Search for subgraph extraction, rendering it impractical when generating explanations for each prediction. Although DIDA manages to produce results within 24 hours, its explanatory performance significantly lags behind SIG. This occurs because when transitioning from a cntinuous time dynamic graph to a discrete time dynamic graph, a significant amount of dynamic information is lost.</p><p>Given the prevalent occurrence of TLE issues in most models documented in Table <ref type="table" target="#tab_3">3</ref>, we sought to assess the efficacy of SIG against established baselines. To this end, we randomly sampled 500 edges  Conversely, the majority of existing explainable methods attain best fidelity at a sparsity of 1. These outcomes underscore SIG's capability to discern the most distinctive subgraph as the explanation.</p><p>Efficiency. Table <ref type="table" target="#tab_2">2</ref> also illustrates the efficiency comparison of our method against all graph explanation methods. Notably, all post-hoc explainable GNNs exhibit high computational costs, leading to delayed detections. Each of these methods requires over 0.8 seconds to explain an edge. Among the baselines, the self-interpretable GNN model, namely DIDA, emerges as the most efficient baseline. However, despite its efficiency, DIDA's speed remains slower than SIG. This discrepancy arises from DIDA's necessity to gather a confounder dictionary in each snapshot, a process that consumes considerable time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Evaluation on OOD Datasets (RQ3)</head><p>Following <ref type="bibr" target="#b36">[37]</ref>, we generate the OOD datasets by injecting synthetic biases into the original dataset.</p><p>For each node, we introduce two times the number of its existing connections as intervention edges.</p><p>We employ three scales of 0.4, 0.6, and 0.8 to distinguish between positive and negative samples within the added intervention edges. Positive samples are drawn from the edges directly connected to the node, while negative samples are drawn from edges not connected to the node. Empirical results on OOD Datasets (Table <ref type="table" target="#tab_5">5</ref>) reveal the following observations: 1) SIG demonstrates superior performance across all datasets and distribution shift scales compared to existing baselines. While the best baseline, GM_ori, achieves comparable results to SIG on the IID datasets of Wikipedia, Reddit, and MOOC (Table <ref type="table" target="#tab_1">1</ref>), its performance drastically drops on OOD datasets. 2) SIG exhibits remarkable resilience to varying levels of distribution shift, indicating its ability to exploit invariant patterns under distribution shift scenarios. This robustness is particularly evident in the LastFM dataset, where SIG outperforms the best-performing baseline by nearly 8.00% in terms of AP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>This paper introduces the ICCM, a novel causal inference model meticulously designed to address both IID and OOD scenarios for CTDGs. Building upon the theoretical foundations of ICCM, we propose a novel deep learning architecture, which translates theoretically established causal models into a practical solution for dynamic graphs. Our extensive empirical evaluations demonstrate the superior effectiveness and efficiency of the proposed SIG model, exhibiting significant advancements over existing methods in link prediction, explainability, and robustness when handling OOD data. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Notations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Preliminaries</head><p>In this section, we introduce some necessary causal inference model concepts <ref type="bibr" target="#b25">[26]</ref> used in the paper. Causal inference models. Causal models reflect the causal relationships between variables. Figure <ref type="figure" target="#fig_4">3</ref> shows three instances of causal inference models. Chain (Fig. <ref type="figure" target="#fig_4">3 (a)</ref>) represents sequential relationships where one variable influences another, which in turn influences a third, and so on. Fig. <ref type="figure" target="#fig_4">3</ref> (b) illustrates the instance of confound. A variable U is a confounder of the effect of X on Y if U meets 3 conditions: U is associated with X; U is associated with Y conditional on X; U is not on a causal pathway from X to Y . The confounder U and backdoor path X ← U → Y make Y and X spuriously correlated. For instance, low blood pressure is seemingly linked to a higher risk of mortality. However, this association may be misleading, as it would be influenced by the confounding effect of heart disease. In this scenario, blood pressure (X) might appear as a direct cause of mortality (Y ). Yet, the confounder heart disease (U ) is associated with both low blood pressure and mortality.</p><p>Do-operation. The do(X = x) operator is a mathematical tool used to simulate interventions within a model. As shown in Fig. <ref type="figure" target="#fig_4">3</ref> (c), it works by altering specific functions associated with X in the model, replacing them with a constant X = x, while keeping the remaining model unchanged.</p><p>In the case of blood pressure (X) and mortality (Y ), employing the do(X = 'low') or do(X = 'normal') operator entails fixing the blood pressure variable to a low or normal state for individuals. This intentional manipulation facilitates the analysis of mortality, particularly concerning the alteration in blood pressure, while holding other influential factors constant. Since it's impractical to collect data directly using the do-operation, adjustment formulas are proposed to compute the probability P (Y = y | do(X = x)). The adjustment formula is shown as follows: </p><formula xml:id="formula_25">P (Y = y | do(X = x)) = E u [P (Y = y | U = u, do(X = x))P (U = u | do(X = x))] = E u [P (Y = y | U = u, X = x)P (U = u)] .<label>(17)</label></formula><p>Here, the set E refers to the training dataset, which comprises pairs of positive and negative samples.</p><p>Positive samples originate from the original edge sets, while negative samples are generated by substituting the destination nodes with randomly sampled nodes from the vocabulary, maintaining an equal ratio to the positive samples. The variable y e denotes the ground-truth label of edge e, assuming a value of 1 for positive samples and 0 for negative samples. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Time Complexity Analysis</head><formula xml:id="formula_27">It takes O(|P 1 |•N ) time to</formula><formula xml:id="formula_28">| • N + |V n | • |x| + |P 2 |).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Further Analyses D.1 Problem Analysis</head><p>In the domain of explainable dynamic graph link prediction, a causal subgraph is extracted from the dynamic graph and elucidates the rationale behind the predicted label. A straightforward approach involves utilizing subgraph extraction techniques to extract the causal subgraph, from the initial graph G. Subsequently, link prediction is performed based on the information encoded within the extracted causal graph. Though this straightforward method may perform well w.r.t. IID data, its performance would downgrade when handling OOD data, as it is susceptible to the influence of confounding factors, i.e., variables correlated with both the causal subgraph and the target variable. These confounding factors can originate from the remaining subgraph of G that is not encompassed by causal subgraph or can arise from latent and unobserved variables.</p><p>In the example in Figure <ref type="figure" target="#fig_6">4</ref>, where a node x consistently establishes a connection with node u in the triadic closure pattern (red) within the training data. Though the triadic closure pattern is the reason for the link between u and v, this straightforward method may tend to capture the bridging link (blue) rather than recognizing the specific triadic closure pattern. This bridging link could be a shortcut feature. In the test data, if the triadic closure pattern does not appear, the aforementioned models may still predict the link (u, v) as long as it sees the bridging link. The presence of shortcut features makes it difficult to capture essential mechanisms, leading to inaccurate predictions. Therefore, it is crucial to carefully consider the potential for confounding factors when designing the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Relationships with Related Models</head><p>This section delves into the connections between the proposed SIG framework and other relevant models in the field.</p><p>DIR <ref type="bibr" target="#b37">[38]</ref> is an invariant rational discovery method specifically designed for static graphs. Similar to SIG, DIR mitigates spurious correlations between G c and Y through the adoption of a do-operation. However, DIR's approach to performing the do-operation differs from SIG's. DIR modifies noncausal elements in the graph to generate counterfactual graph data, while SIG employs the Normalized Weighted Geometric Mean (NWGM) approximation to efficiently estimate the causal effect without directly modifying the graph structure.</p><p>DIR's learning strategy is formulated as follows:</p><formula xml:id="formula_29">min E c [R( Ŷ , Y )|do(G s = s)] + λV ar s R( Ŷ , Y )|do(G s = s) ,<label>(19)</label></formula><p>where R represents the risk function, Ŷ denotes the predicted label, and λ controls the trade-off between minimizing interventional risks and their variance. DIR aims to minimize both interventional risks and their variance, ensuring that the model is not overly sensitive to specific interventions. However, these interventions can pose computational challenges, especially as graph sizes increase.</p><p>Without classifiers f I o and f T o , and using the same do-operation implementation method as DIR, the proposed SIG methodology becomes equivalent to DIR. This demonstrates that SIG encompasses DIR as a special case, while capturing the temporal information and offering improved computational efficiency through the NWGM approximation.</p><p>DIDA <ref type="bibr" target="#b47">[48]</ref> is an invariant rational discovery method specifically designed for DTDGs. Its learning strategy aligns with that of DIR, aiming to minimize interventional risks and their variance. However, DIDA proposes an approximation to the intervention process by sampling and replacing the variant pattern representation instead of directly modifying the original graph structure. This approach aims to reduce the computational burden of interventions in DTDGs.</p><p>The probability function associated with DIDA's intervention process is expressed as follows:</p><formula xml:id="formula_30">P (Y = y|do(G c = C)) = E d∼D [Softmax g(z c + z d ) ]<label>(20)</label></formula><p>where z c and z d represent the hidden representations for the cause and bias graph, respectively. g(•) makes predictions using both z c and z d . Notably, DIDA requires the construction of an intervention set, denoted as D for each node and time step, which requires expensive sampling.</p><p>By omitting the classifier f I o , integrating structural and temporal interventions, discarding the NWGM approximation, and utilizing the same implementation approach, the proposed SIG methodology becomes equivalent to DIDA. This again highlights the generality of SIG and its ability to incorporate existing methods as special cases.</p><p>GraphMixer (GM) <ref type="bibr" target="#b1">[2]</ref> presents a neural network architecture specifically designed for temporal graphs. Its main goal is to learn effective representations of temporal graphs for predictive tasks. If the causal subgraph extraction and the do-operation are omitted from the SIG framework, SIG reduces to GM.</p><p>In summary, SIG represents the first self-interpretable GNN tailored explicitly for both IID and OOD CTDGs. Temporal graph neural networks designed for CTDGs, such as GM, fail to provide explainable outcomes. Moreover, existing self-interpretable graph neural networks intended for static graphs (e.g., DIR) and DTDGs (e.g., DIDA) encounter limitations in their adaptation to CTDGs due to computational complexities. SIG effectively tackles these challenges by introducing two novel causal models, ICM and CCM. These meticulously designed models capture both temporal and structural information within CTDGs, simultaneously addressing confounding effects. Additionally, SIG specifies the essential components for implementing the causal models, including an extractor for identifying invariant subgraphs, two encoders for transforming subgraphs into latent representations, and classifiers for predictive modeling based on the derived causal graphs. We conducted experiments on five real-world datasets. The details of the datasets are reported in Table <ref type="table" target="#tab_9">7</ref>, where #edge and #node represent the number of node and edges. #dim-E and #dim-N represent the dimensions of node and edge features, respectively.</p><p>Wikipedia<ref type="foot" target="#foot_0">foot_0</ref> captures edits made by Wikipedia editors over a month, with extracted link features derived by converting edit text into LIWC feature vectors <ref type="bibr" target="#b26">[27]</ref>. Reddit<ref type="foot" target="#foot_1">foot_1</ref> compiles posts from various subreddits within a month. The source node represents a user, while the target node denotes a subreddit. Each edge signifies a user's post in a specific subreddit. Similar to the Wikipedia dataset, link features are extracted through the conversion of text into LIWC feature vectors. MOOC<ref type="foot" target="#foot_2">foot_2</ref> constitutes a bipartite network involving online resources. It comprises two kinds of nodes: students and units of course content. The connection between nodes signifies a student's interaction with specific content units. LastFM<ref type="foot" target="#foot_3">foot_3</ref> serves as a commonly used dataset for music recommendation and analysis. It contains user listening histories and music tag information from the LastFM music platform. SX<ref type="foot" target="#foot_4">foot_4</ref> stands as a temporal network of interactions on the Stack Exchange website "super user".</p><p>Note that for the datasets without node features, we utilize one-hot vectors as the node's features.</p><p>Considering the large size of SX, it is impractical to use this manner. Hence, we randomly select 100 nodes for each node, and the corresponding shortest distances between them are used as the node's feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 Evaluation Metrics.</head><p>We partitioned the datasets based on the edge occurrence time: the initial 70% of edges were designated as the training set, the subsequent 15% were allocated to the validation set, and the remaining 15% formed the test set. We employ the average precision (AP) and area under the curve (AUC) as the evaluation metrics for link prediction. AP and AUC are two common metrics used to evaluate the performance of binary classification models. AP is a measure of the average precision across all possible recall thresholds. AUC is a measure of the area under the receiver operating characteristic (ROC) curve.</p><p>We adopt fidelity w.r.t. sparsity as the evaluation metrics for graph explanation. The definitions of Fidelity and sparsity are shown as follows: An early-stopping mechanism was employed, terminating training when the Average Precision (AP) metric showed no improvement for five consecutive epochs. The model underwent training for 300 epochs using the Adam optimizer with a learning rate set at 0.0001 and a weight decay of 1e-6. We set the batch size to 600, and the hidden layer dimension to 100. For the extraction of the causal subgraph, we specified the number of recent edges (N ) as 50 and employed 1-hop neighbors. All MLP layers were configured to 2. Regarding the link prediction task, negative samples were set at a ratio of 1:5 in the training set and adjusted to 1:1 in both the validation and test sets. Hyperparameters λ i , λ t , and λ s were set to 1.0, 0.5, and 0.5, respectively.</p><formula xml:id="formula_31">F idelity ap = 1 N N i=1 ap(G) -ap(G b ) .<label>(21)</label></formula><formula xml:id="formula_32">Sparsity = |G c | e |G| e .<label>(22</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.4 Baselines</head><p>Note that the proposed SIG is the first self-interpretable GNN specifically designed for CTDGs.</p><p>Given the limited studies in self-interpretable GNNs for dynamic graphs, our evaluation spans several comparisons by considering different types of baselines. (1) Initially, SIG undergoes comparison with three existing dynamic GNN models: TGN <ref type="bibr" target="#b29">[30]</ref>, TGAT <ref type="bibr" target="#b39">[40]</ref>, and GraphMixer (GM) <ref type="bibr" target="#b1">[2]</ref>. These models are designed to handle CTDGs. However, as they lack the capacity to produce explainable outcomes, our comparison primarily focuses on link prediction tasks across original datasets and synthetic OOD datasets. ( <ref type="formula" target="#formula_1">2</ref>) Additionally, we compare the proposed model with four post-interpretable models, including an attention-based explainer (ATTN <ref type="bibr" target="#b38">[39]</ref>), a perturbing-based explainer (PBONE <ref type="bibr" target="#b38">[39]</ref>), a static graph explainer (PGExp <ref type="bibr" target="#b19">[20]</ref>), and a dynamic graph explainer (TGExp <ref type="bibr" target="#b38">[39]</ref>). These models were thoughtfully chosen to represent diverse graph explanation approaches. Given their post-interpretable nature, our comparison focuses solely on graph explanation tasks. (3) Further, we compare SIG with DIDA <ref type="bibr" target="#b47">[48]</ref>, a self-interpretable GNN for DTDG. Our comparative analysis with DIDA spans across all tasks.</p><p>• TGN initially captures temporal information using Recurrent Neural Networks, followed by the graph attention convolution to jointly encompass spatial and temporal information.</p><p>• TGAT leverages a self-attention mechanism as its foundational element, incorporating a novel functional time encoding technique. TGAT can discern node embeddings as functions of time and can deduce embeddings for previously unseen nodes in an inductive manner.</p><p>• GM represents a straightforward architecture composed of three core components: a link-encoder utilizing MLPs, a node-encoder relying solely on neighbor mean-pooling, and an MLP-based link classifier. For GM_ori, we adhered to the default parameters provided in the paper's source code. Conversely, for GM_50n, we configured the number of recent edges to 50, aligning it with the setting used in SIG.</p><p>• ATTN extracts the attention weights in TGAT/TGN and averages the values over all layers. The averaged weights are regarded as importance scores.</p><p>• PBONE functions as a direct explainer by perturbing a single candidate edge. We configured the interpretation process for TGAT and TGN.</p><p>• PGExp employs a deep neural network to parameterize the generation process of explanations. In line with <ref type="bibr" target="#b38">[39]</ref>, we tailor it for temporal graph scenarios by computing weights for each event rather than each edge.</p><p>• TGExp comprises an explorer that identifies event subsets using Monte Carlo Tree Search and a navigator that learns event correlations to reduce the search space.  • DIDA represents self-interpretable GNN tailored explicitly for DTDGs. To enable a comparative analysis with the proposed model, we adjust our datasets by converting edges that occur within monthly periods into snapshots, thereby aligning our datasets with the DTDG setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Supplementary Experiments F.1 Case Study</head><p>Figure <ref type="figure" target="#fig_7">5</ref> depicts two examples of the extracted causal subgraph at sparsity values of 0.2 and 0.6, respectively. In this visualization, the dotted line represents the edge to be predicted. The red nodes and edges represent the extracted causal subgraph G c , while the grey nodes and edges signify the non-causal subgraph G b . The time labels alongside each edge denote t 0 -t k , wherein t 0 denotes the timestamp used for predicting the edge's existence and t k denotes the timestamp used for the edges themselves. Overall, we can learn that extracted subgraphs encompass the surrounding neighbors and effectively elucidate the dynamic link predictions. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2 Ablation Study</head><p>We conducted ablation studies by removing ICM, temporal, and structural classifiers. The ablation experiments are summarized in Table <ref type="table" target="#tab_11">8</ref>. The result reveals that the complete solution achieves the highest performance, validating the efficacy of our proposed design. Specifically, we notice that ICM significantly contributes to the performance in both the original and OOD datasets. Moreover, the removal of structural and temporal losses results in marginal performance changes in the original dataset, whereas their absence notably impacts the performance in OOD datasets, indicating their substantial contribution in handling out-of-distribution scenarios.</p><p>Figure <ref type="figure" target="#fig_8">6</ref> shows the throughput of our solution by varying the number of edges N and the number of hops n. We observe that the change of throughput is linear to the hyperparameter N , which is consistent with the complexity analysis. As the number of hops n increases, the number of nodes in the extracted structural causal graph increases greatly, reducing the throughput in a linear trend.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Causal Models.</figDesc><graphic coords="4,111.60,72.00,388.81,90.05" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>4. 2</head><label>2</label><figDesc>Confounded Causal Model (CCM)To handle the confounding variables that may introduce bias in OOD data predictions, we introduce CCM. As shown in Figure1(b), CCM considers confounders consisting of non-causal subgraph G b and unobserved variables U , where G b is the residual part of the graph once the causal subgraphs are excluded. These confounders contain information about possible shortcut features, which could lead to spurious correlations between the causal subgraph and the prediction labels.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>and f C t (•) are structural and temporal encoders, f S o (•) and f T o (•) are structural and temporal predictors, Y S and Y T represent the prediction labels resulting from structural and temporal interventions, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The deep learning implementation of SIG. Equations 6 and 7 require evaluating the model for each confounder d ∈ D with both H S and H T .This becomes computationally expensive for large temporal networks. To address this, we leverage the Normalized Weighted Geometric Mean (NWGM) approximation<ref type="bibr" target="#b40">[41]</ref>, i.e., E d∼D [σ W c * f s y (H S ) + W c * f u y (d) ] ≈ σ E d∼D [W c * f s y (H S ) + W c * f u y (d)]. After applying NWGM, Equations 6 and 7 can be reformulated as:</figDesc><graphic coords="5,111.60,72.00,388.80,139.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Basic causal inference models.</figDesc><graphic coords="13,180.00,324.55,251.99,67.99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>5 C. 1</head><label>51</label><figDesc>Here, P (Y = y | U = u, X = x) represents the probability considering the causal feature X and confounding factors U , and P (U = u) denotes the prior probability of these confounding factors. Note that P (Y = y | X = x) ̸ = P (Y = y | do(X = x)) unless there are no confounders present.C More Details on Section Risk FunctionsThe risk functions are formulated as: R i (y I , y) = 1 |E| e∈E y e log(y I e ) + (1 -y e )log(1 -y I e ) R t (y T , y) = 1 |E| e∈E y e log(y T e ) + (1 -y e )log(1 -y T e ) R s (y S , y) = 1 |E| e∈E y e log(y S e ) + (1 -y e )log(1 -y S e ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Example of shortcut features.</figDesc><graphic coords="15,234.00,72.00,144.00,77.47" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Case study on the Wikipedia dataset.</figDesc><graphic coords="18,124.76,72.00,165.58,124.19" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Throughput w.r.t. hyper-parameters.</figDesc><graphic coords="18,118.80,243.80,374.37,173.59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Let G * denote a causal subgraph which is either G s or G t . To block the backdoor pathsG * ← G b → Y and G * ← U → Y ,we perform interventions on G * . Specifically, we perform interventions as do(G s = C s ) based on structural features and do(G t = C t ) based on temporal features, where C s and C t are constant subgraphs. Through the replacement of G s with C s and G t with C t , these interventions effectively block the backdoor paths, thereby eliminating the previously existing spurious correlation between G * and Y . The foundational assumptions guiding these models are summarized as follows: Assumption 2 (CCM).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Comparison with SOTA graph link prediction models w.r.t. AUC and AP. The best scores are highlighted in bold, and the second highest scores are highlighted in underline.</figDesc><table><row><cell>Model</cell><cell>Wikipedia AP AUC</cell><cell>Reddit AP AUC</cell><cell cols="2">MOOC AP AUC</cell><cell>LastFM AP AUC AP</cell><cell>SX</cell><cell>AUC</cell></row><row><cell>TGN</cell><cell cols="7">95.54 95.06 95.96 96.16 79.56 81.73 79.03 77.90 68.28 73.64</cell></row><row><cell>TGAT</cell><cell cols="7">97.25 96.92 98.20 98.12 86.91 88.44 82.46 80.97 71.44 74.01</cell></row><row><cell>GM_ori</cell><cell cols="7">99.75 99.79 99.90 99.91 99.91 99.93 96.16 97.73 97.60 97.62</cell></row><row><cell cols="8">GM_50n 99.69 99.73 99.92 99.93 99.83 99.86 96.18 97.49 96.94 96.97</cell></row><row><cell>DIDA</cell><cell cols="7">86.46 89.09 83.04 81.72 97.47 98.43 55.56 54.57 92.33 91.42</cell></row><row><cell>SIG</cell><cell cols="3">99.94 99.94 99.99 99.99 99.95</cell><cell cols="4">99.97 99.96 99.98 99.71 99.70</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Average running time per edge (seconds). 2×10 -4 2.8×10 -3 1.4×10 -3 3.7×10 -3 1.5×10 -4 2.1×10 -4</figDesc><table><row><cell></cell><cell></cell><cell>Self-int</cell><cell></cell><cell cols="2">Dynamic GNN</cell><cell>Post-hoc</cell></row><row><cell></cell><cell>SIG</cell><cell>DIDA</cell><cell>TGN</cell><cell>TGAT</cell><cell>GM_ori</cell><cell>GM_50n ATTN PBONE PGExp TGExp</cell></row><row><cell>Reddit</cell><cell cols="6">5.8×10 -4 3.7×10 -3 1.7×10 -3 4.2×10 -3 2.2×10 -4 3.7×10 -4</cell><cell>0.95</cell><cell>1.10</cell><cell>0.82</cell><cell>412.65</cell></row><row><cell cols="7">LastFM 6.3.44</cell><cell>2.89</cell><cell>2.59</cell><cell>716.08</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Comparison with SOTA explanation models on the original datasets. 'TLE' indicates that the time limit of 24 hours was exceeded. 'FID(SP)' denotes the best fidelity value FID along with its corresponding occurred sparsity SP (SP ∈ {0.2, 0.4, 0.6, 0.8, 1.0}). 'AUFSC' stands for the Area Under the Fidelity-Sparse Curve.GM_50n achieve higher AP and AUC values compared to TGAT and TGN. These results suggest that recurrent neural networks and self-attention mechanisms are not always essential for effective temporal graph learning.</figDesc><table><row><cell cols="2">Type</cell><cell>Model</cell><cell cols="2">Wikipedia FID(SP) AUFSC</cell><cell cols="2">Reddit FID(SP) AUFSC</cell><cell cols="2">MOOC FID(SP) AUFSC</cell><cell cols="2">LastFM FID(SP) AUFSC</cell><cell>SX FID(SP)</cell><cell>AUFSC</cell></row><row><cell>Post-hoc</cell><cell>TGAT TGN</cell><cell cols="3">ATTN PBONE 18.92(1.0) 2.57 18.92(1.0) 3.36 PGExp 18.92(1.0) 3.18 TGExp TLE TLE ATTN 23.90(1.0) 9.48 PBONE 23.90(1.0) 7.73 PGExp 23.90(1.0) 7.92</cell><cell>TLE TLE TLE TLE TLE TLE TLE</cell><cell>TLE TLE TLE TLE TLE TLE TLE</cell><cell>TLE TLE TLE TLE TLE TLE TLE</cell><cell>TLE TLE TLE TLE TLE TLE TLE</cell><cell>TLE TLE TLE TLE TLE TLE TLE</cell><cell>TLE TLE TLE TLE TLE TLE TLE</cell><cell>TLE TLE TLE TLE TLE TLE TLE</cell><cell>TLE TLE TLE TLE TLE TLE TLE</cell></row><row><cell></cell><cell></cell><cell>TGExp</cell><cell>TLE</cell><cell>TLE</cell><cell>TLE</cell><cell>TLE</cell><cell>TLE</cell><cell>TLE</cell><cell>TLE</cell><cell>TLE</cell><cell>TLE</cell><cell>TLE</cell></row><row><cell cols="2">Self-int</cell><cell>DIDA SIG</cell><cell cols="2">1.31(1.0) 0.34 53.70(0.6) 42.09</cell><cell>0(0) 58.21(0.4)</cell><cell>-0.75 38.29</cell><cell>0(0) 30.71(1.0)</cell><cell>-0.17 6.47</cell><cell>0(0) 28.29(0.2)</cell><cell>-0.17 17.10</cell><cell>0(0) 53.94(0.2)</cell><cell>-0.47 25.86</cell></row></table><note><p>DIDA, a self-interpretable GNN tailored explicitly for DTDGs, obtained low scores across multiple datasets. This disparity in performance stems from the finer granularity in modeling temporal dynamics offered by CTDGs compared to DTDGs. CTDGs enable a more precise representation of event occurrences and thereby are more challenging.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Comparison with SOTA graph explanation models on the sampled datasets.</figDesc><table><row><cell cols="2">Type</cell><cell>Model</cell><cell cols="2">Wikipedia_sample FID(SP) AUFSC</cell><cell cols="2">Reddit_sample FID(SP) AUFSC</cell><cell cols="2">MOOC_sample FID(SP) AUFSC</cell><cell cols="2">LastFM_sample FID(SP) AUFSC</cell><cell>SX_sample FID(SP) AUFSC</cell></row><row><cell>Post-hoc</cell><cell>TGAT TGN</cell><cell cols="3">ATTN PBONE 40.40(1.0) 6.88 40.40(1.0) 11.45 PGExp 40.40(1.0) 7.63 TGExp TLE TLE ATTN 27.62(0.8) 14.66 PBONE 25.10(1.0) 11.02 PGExp 25.10(1.0) 10.84</cell><cell>36.00(1.0) 36.00(1.0) 36.00(1.0) TLE 20.43(1.0) 20.43(1.0) 20.43(1.0)</cell><cell>5.48 6.51 6.17 TLE 5.49 8.70 3.03</cell><cell>6.29(1.0) 6.29(1.0) 6.29(1.0) TLE 1.12(1.0) 1.12(1.0) 1.23(0.8)</cell><cell>1.35 0.88 1.43 TLE 0.63 0.63 0.69</cell><cell>21.18(1.0) 21.18(1.0) 21.18(1.0) TLE 2.79(1.0) 2.77(1.0) 2.79(1.0)</cell><cell>6.59 6.35 5.49 TLE 1.13 1.27 1.24</cell><cell>22.63(0.4) 22.33(0.2) 18.26(1.0) TLE 1.82(0.8) 4.85(0.2) 1.04(0.4)</cell><cell>20.17 20.97 9.21 TLE 0.95 2.41 -0.44</cell></row><row><cell></cell><cell></cell><cell>TGExp</cell><cell>TLE</cell><cell>TLE</cell><cell>TLE</cell><cell>TLE</cell><cell>TLE</cell><cell>TLE</cell><cell>TLE</cell><cell>TLE</cell><cell>TLE</cell><cell>TLE</cell></row><row><cell cols="2">Self-int</cell><cell>DIDA SIG</cell><cell cols="2">0.90(0.6) 0.34 54.58(0.6) 42.43</cell><cell>0(0) 58.27(0.4)</cell><cell>-0.75 38.19</cell><cell>0(0) 17.82(0.2)</cell><cell>-0.07 5.11</cell><cell>0(0) 28.68(0.2)</cell><cell>-0.17 17.88</cell><cell>0(0) 52.63(0.2)</cell><cell>-0.47 25.26</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 :</head><label>5</label><figDesc>Comparison on OOD datasets.</figDesc><table><row><cell>Model</cell><cell></cell><cell cols="2">Reddit_OOD</cell><cell></cell><cell cols="2">LastFM_OOD</cell><cell></cell><cell cols="2">SX_OOD</cell></row><row><cell>Split</cell><cell></cell><cell>0.4</cell><cell>0.6</cell><cell>0.8</cell><cell>0.4</cell><cell>0.6</cell><cell>0.8</cell><cell>0.4</cell><cell>0.6</cell><cell>0.8</cell></row><row><cell></cell><cell>AP</cell><cell>AUC AP</cell><cell>AUC AP</cell><cell>AUC AP</cell><cell>AUC AP</cell><cell>AUC AP</cell><cell>AUC AP</cell><cell>AUC AP</cell><cell>AUC AP</cell><cell>AUC</cell></row><row><cell>TGN</cell><cell cols="10">63.89 59.97 65.15 61.07 65.58 61.46 54.66 53.12 55.67 53.99 56.57 54.66 66.63 59.41 67.46 60.33 67.72 60.88</cell></row><row><cell>TGAT</cell><cell cols="10">69.00 63.27 70.85 65.14 71.69 65.90 60.83 56.71 62.86 58.56 64.15 59.74 70.83 71.03 70.92 72.55 71.15 73.32</cell></row><row><cell>GM_ori</cell><cell cols="10">99.52 99.66 99.55 99.67 99.56 99.68 92.25 94.89 92.01 94.73 91.92 94.64 96.07 96.43 96.17 96.54 96.20 96.57</cell></row><row><cell cols="11">GM_50n 99.62 99.71 99.63 99.71 99.63 99.72 90.80 94.42 90.87 94.44 90.90 94.47 85.68 91.18 87.03 91.92 87.61 92.22</cell></row><row><cell>DIDA</cell><cell cols="10">64.16 63.16 66.08 64.71 67.35 65.67 53.33 54.86 53.34 54.96 53.24 54.29 64.25 66.50 65.66 68.15 66.59 69.26</cell></row><row><cell>SIG</cell><cell cols="10">99.85 99.90 99.86 99.89 99.90 99.92 99.88 99.93 99.94 99.97 99.92 99.96 99.85 99.86 99.79 99.81 99.81 99.84</cell></row><row><cell cols="11">from the datasets, following the methodology outlined in [39], thereby creating a test set of edges, as</cell></row><row><cell cols="2">depicted in</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 .</head><label>4</label><figDesc>Empirical results reveal that, on average, SIG outperforms the best baselines by 17.10% and 16.77% concerning FID(SP) and AUFSC, respectively. Notably, our observations indicate that SIG achieves best fidelity, particularly at sparsity levels below 0.6 across most datasets.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Notations and descriptions. Dynamic graph G with nodes set V, edges set E , and time domain T eij(t k ) The edge between nodes ui and vj occurred at time t k The feature vector of vi Gs, Gt, G b Structural causal, temporal causal, and non-causal subgraph in causal model Cs, Ct Constant structural and temporal subgraph in causal model M</figDesc><table><row><cell>Notations</cell><cell>Descriptions</cell></row><row><cell>G = (V, E, T )</cell><cell></cell></row><row><cell>x e ij (t k ) x n i</cell><cell>The feature vector of eij(t k )</cell></row><row><cell></cell><cell>Hidden representations in causal model</cell></row><row><cell>Y</cell><cell>The prediction label in causal model</cell></row><row><cell>U</cell><cell>Unobserved variables in causal model</cell></row><row><cell>W  *   *  , W  *   *</cell><cell>The model parameters</cell></row><row><cell>D</cell><cell>Confounder dictionary</cell></row><row><cell>fs, ft</cell><cell>Structural and temporal encoding functions</cell></row><row><cell>f  *  y (•)</cell><cell>Linear networks</cell></row></table><note><p>* , H *</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>extract the temporal causal graph and generate the temporal representation, where |P 1 | is the number of learnable parameters in the causal graph extractor f ext , and N is a hyperparameter denoting the number of recent edges for representation generation. It takes O(|V n | • |x|) time to extract the structural causal graph and generate the structural representation, where |V n | is the number of nodes in the n-hop neighborhoods of two nodes for prediction and |x| is the number of node features. The prediction takes O(|P 2 |) time, where |P 2 | is the number of parameters in the prediction model. Therefore, the total complexity of SIG is O(|P 1</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Summary of dataset statistics.</figDesc><table><row><cell cols="2">Dataset Wikipedia</cell><cell>Reddit</cell><cell>MOOC</cell><cell>LastFM</cell><cell>SX</cell></row><row><cell>#edge</cell><cell>157,474</cell><cell cols="4">672,447 411,749 1,293,103 1,443,339</cell></row><row><cell>#node</cell><cell>8,227</cell><cell>10,000</cell><cell>7,047</cell><cell>1,980</cell><cell>194,085</cell></row><row><cell>#dim-E</cell><cell>172</cell><cell>172</cell><cell>/</cell><cell>/</cell><cell>/</cell></row><row><cell>#dim-N</cell><cell>/</cell><cell>/</cell><cell>/</cell><cell>/</cell><cell>/</cell></row><row><cell cols="2">E Experimental Settings</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="6">All the experiments are conducted on a computer with Intel(R) Core(TM)2 Duo CPU @2.40 GHz</cell></row><row><cell cols="2">processor, 128 GB RAM, and Tesla T4.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>E.1 Datasets.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>)</head><label></label><figDesc>Here, N is the number of test graphs, G represents the input graph, G b represents the residual portion of graph G after excluding the explanatory subgraph G c , ap(G) represents the average precision output by graph G. |G| e (|G c | e ) represents the number of edges in G (G c ). Higher values of F idelity ap signify better explanatory outcomes, indicating the identification of more distinctive features. Lower values for sparsity indicate that the explanations are sparser and can focus primarily on more essential input information. Furthermore, we obtain the fidelity-sparsity curve and calculate the area under the curve (AUFSC) to evaluate interpretability performance, where a higher AUFSC value indicates better performance.</figDesc><table><row><cell>E.3 Training Protocols.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 8 :</head><label>8</label><figDesc>Ablation study</figDesc><table><row><cell>Dataset</cell><cell cols="2">Reddit</cell><cell cols="2">LastFM</cell><cell>SX</cell><cell></cell><cell cols="4">Reddit_OOD LastFM_OOD</cell><cell cols="2">SX_OOD</cell></row><row><cell></cell><cell>AP</cell><cell>AUC</cell><cell>AP</cell><cell>AUC</cell><cell>AP</cell><cell>AUC</cell><cell>AP</cell><cell>AUC</cell><cell>AP</cell><cell>AUC</cell><cell>AP</cell><cell>AUC</cell></row><row><cell cols="10">remove structural classifier 99.88 99.89 98.37 99.03 99.16 99.23 74.22 71.73 50.15</cell><cell>49.97</cell><cell cols="2">93.01 95.07</cell></row><row><cell>remove temporal classifier</cell><cell cols="9">99.87 99.88 97.75 98.51 99.53 99.48 98.37 98.19 78.29</cell><cell>82.90</cell><cell cols="2">99.31 99.17</cell></row><row><cell>remove ICM</cell><cell cols="9">98.76 98.66 53.96 54.95 95.86 97.14 97.75 97.71 50.38</cell><cell>50.48</cell><cell cols="2">95.27 95.07</cell></row><row><cell>SIG</cell><cell cols="3">99.99 99.99 99.96</cell><cell cols="6">99.98 99.71 99.70 99.90 99.92 99.92</cell><cell>99.96</cell><cell cols="2">99.81 99.84</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>http://snap.stanford.edu/jodie/wikipedia.csv</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>http://snap.stanford.edu/jodie/reddit.csv</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>http://snap.stanford.edu/jodie/mooc.csv</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>http://snap.stanford.edu/jodie/lastfm.csv</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>https://snap.stanford.edu/data/sx-superuser.html</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep variational information bottleneck</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">V</forename><surname>Dillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Do we really need complicated model architectures for temporal networks</title>
		<author>
			<persName><forename type="first">W</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mahdavi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Towards self-explainable graph neural network</title>
		<author>
			<persName><forename type="first">E</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management</title>
		<meeting>the 30th ACM International Conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Combining temporal aspects of dynamic networks with node2vec for a more efficient dynamic link prediction</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">De</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Decuypere</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mitrović</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Baesens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">De</forename><surname>Weerdt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Social Networks Analysis and Mining</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Self-interpretable graph learning with sufficient and necessary explanations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="11749" to="11756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Debiasing graph neural networks via learning disentangled causal substructure</title>
		<author>
			<persName><forename type="first">S</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="24934" to="24946" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Kergnns: Interpretable graph neural networks with graph kernels</title>
		<author>
			<persName><forename type="first">A</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tassiulas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Intelligence</title>
		<meeting>the AAAI Conference on Intelligence</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Towards open temporal graph neural networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Shortcut learning in deep neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Geirhos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-H</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Michaelis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">A</forename><surname>Wichmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="665" to="673" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Computing rule-based explanations by leveraging counterfactuals</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schleich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Suciu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment</title>
		<meeting>the VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Variational deep embedding: An unsupervised and generative approach to clustering</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05148</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Decision trees: a recent overview</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Kotsiantis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence Review</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="261" to="283" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Predicting dynamic embedding trajectory in temporal interaction networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery &amp; data mining</title>
		<meeting>the 25th ACM SIGKDD international conference on knowledge discovery &amp; data mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1269" to="1278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Explaining point processes by learning interpretable temporal logic rules</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Essofi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Zebra: When temporal graph neural networks meet temporal personalized pagerank</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment</title>
		<meeting>the VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="1332" to="1345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Diffusion convolutional recurrent neural network: Datadriven traffic forecasting</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shahabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Graph rationalization with environment-based augmentations</title>
		<author>
			<persName><forename type="first">G</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Towards self-interpretable graph-level anomaly detection</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">36</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Tlogic: Temporal logical rules for explainable link forecasting on temporal knowledge graphs</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hildebrandt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Joblin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Tresp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Parameterized explainer for graph neural network</title>
		<author>
			<persName><forename type="first">D</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020">19620-19631, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">On data-aware global explainability of graph neural networks</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment</title>
		<meeting>the VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Embedding models for episodic knowledge graphs</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Tresp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Daxberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Web Semantics</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page">100490</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">dynnode2vec: Scalable dynamic network embedding</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mahdavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khoshraftar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>An</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Big Data</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Interpretable and generalizable graph learning via stochastic attention mechanism</title>
		<author>
			<persName><forename type="first">S</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Models, reasoning and inference</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>CambridgeUniversityPress</publisher>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">3</biblScope>
			<pubPlace>Cambridge, UK</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Causal inference in statistics: A primer</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">P</forename><surname>Jewell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Linguistic inquiry and word count: Liwc</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Pennebaker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Booth</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001. 2001. 2001. 2001</date>
			<publisher>Lawrence Erlbaum Associates</publisher>
			<biblScope unit="volume">71</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Seign: A simple and efficient graph neural network for large dynamic graphs</title>
		<author>
			<persName><forename type="first">X</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Reinwald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Domeniconi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Data Engineering</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Explaining link prediction systems based on knowledge graph embeddings</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Firmani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Merialdo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Teofili</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 international conference on management of data</title>
		<meeting>the 2022 international conference on management of data</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Temporal graph networks for deep learning on dynamic graphs</title>
		<author>
			<persName><forename type="first">E</forename><surname>Rossi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chamberlain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Frasca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Eynard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Dysat: Deep neural representation learning on dynamic graphs via self-attention networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The International Conference on Web Search and Data Mining</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Rule-based modeling: Precision and transparency</title>
		<author>
			<persName><forename type="first">M</forename><surname>Setnes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Babuska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">B</forename><surname>Verbruggen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="165" to="169" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">defend: Explainable fake news detection</title>
		<author>
			<persName><forename type="first">K</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery &amp; data mining</title>
		<meeting>the 25th ACM SIGKDD international conference on knowledge discovery &amp; data mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Causal attention for interpretable and generalizable graph classification</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Mlp-mixer: An all-mlp architecture for vision</title>
		<author>
			<persName><forename type="first">I</forename><surname>Tolstikhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Houlsby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Keysers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lucic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="24261" to="24272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Dyrep: Learning representations over dynamic graphs</title>
		<author>
			<persName><forename type="first">R</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Farajtabar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Biswal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Handling distribution shifts on graphs: An invariance perspective</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wipf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Discovering invariant rationales for graph neural networks</title>
		<author>
			<persName><forename type="first">Y.-X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Explaining temporal graph models through an explorer-navigator framework</title>
		<author>
			<persName><forename type="first">W</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Inductive representation learning on temporal graphs</title>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Korpeoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Achan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Train once and explain everywhere: Pre-training interpretable graph neural networks</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">36</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Gnnexplainer: Generating explanations for graph neural networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bourgeois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Graph information bottleneck for subgraph recognition</title>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Netwalk: A flexible deep embedding approach for anomaly detection in dynamic networks</title>
		<author>
			<persName><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">C</forename><surname>Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery &amp; data mining</title>
		<meeting>the 24th ACM SIGKDD international conference on knowledge discovery &amp; data mining</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2672" to="2681" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Explainability in graph neural networks: A taxonomic survey</title>
		<author>
			<persName><forename type="first">H</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="5782" to="5799" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Protgnn: Towards self-explaining graph neural networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Dynamic graph neural networks under spatio-temporal distribution shift</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="6074" to="6089" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
		<title level="m" type="main">Ci-gnn: A granger causality-inspired graph neural network for interpretable brain network-based psychiatric diagnosis</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.01642</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
