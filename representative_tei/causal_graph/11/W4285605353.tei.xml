<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Linear Combinatorial Semi-Bandit with Causally Related Rewards</title>
				<funder ref="#_TQeRmQ6 #_Rtj4YZY">
					<orgName type="full">Deutsche Forschungsgemeinschaft (DFG, German Research Foundation</orgName>
				</funder>
				<funder>
					<orgName type="full">German Federal Ministry of Education and Research (BMBF)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Behzad</forename><surname>Nourani-Koliji</surname></persName>
							<email>behzad.nourani-koliji@uni-tuebingen.de</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Tübingen</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Saeed</forename><surname>Ghoorchian</surname></persName>
							<email>saeed.ghoorchian@tu-berlin.de</email>
							<affiliation key="aff1">
								<orgName type="institution">Technical University of Berlin</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Setareh</forename><surname>Maghsudi</surname></persName>
							<email>setareh.maghsudi@uni-tuebingen.de</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Tübingen</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Linear Combinatorial Semi-Bandit with Causally Related Rewards</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-21T20:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In a sequential decision-making problem, having a structural dependency amongst the reward distributions associated with the arms makes it challenging to identify a subset of alternatives that guarantees the optimal collective outcome. Thus, besides individual actions' reward, learning the causal relations is essential to improve the decision-making strategy. To solve the two-fold learning problem described above, we develop the 'combinatorial semibandit framework with causally related rewards', where we model the causal relations by a directed graph in a stationary structural equation model. The nodal observation in the graph signal comprises the corresponding base arm's instantaneous reward and an additional term resulting from the causal influences of other base arms' rewards. The objective is to maximize the long-term average payoff, which is a linear function of the base arms' rewards and depends strongly on the network topology. To achieve this objective, we propose a policy that determines the causal relations by learning the network's topology and simultaneously exploits this knowledge to optimize the decision-making process. We establish a sublinear regret bound for the proposed algorithm. Numerical experiments using synthetic and real-world datasets demonstrate the superior performance of our proposed method compared to several benchmarks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In the seminal form of the Multi-Armed Bandit (MAB) problem, an agent selects an arm from a given set of arms at sequential rounds of decision-making. Upon selecting an arm, the agent receives a reward, which is drawn from the unknown reward distribution of that arm. The agent aims at maximizing the average reward over the gambling horizon <ref type="bibr" target="#b6">[Robbins, 1952]</ref>. The MAB problem portrays the exploration-exploitation dilemma, where the agent decides between accumulating immediate reward and obtaining information that might result in larger reward only in the future * Equal Contribution <ref type="bibr">[Maghsudi and Hossain, 2016]</ref>. To measure the performance of a strategy, one uses the notion of regret. It is the difference between the accumulated reward of the applied decisionmaking policy and that of the optimal policy in hindsight.</p><p>In a combinatorial semi-bandit setting <ref type="bibr" target="#b1">[Chen et al., 2013]</ref>, at each round, the agent selects a subset of base arms. This subset is referred to as a super arm. She then observes the individual reward of each base arm that belongs to the selected super arm. Consequently, she accumulates the collective reward associated with the chosen super arm. The combinatorial MAB problem is challenging since the number of super arms is combinatorial in the number of base arms. Thus, conventional MAB algorithms such as <ref type="bibr" target="#b0">[Auer et al., 2002]</ref> are not appropriate for combinatorial problems as they result in suboptimal regret bounds. The aforementioned problem becomes significantly more difficult when there are causal dependencies amongst the reward distributions.</p><p>In some cases, it is possible to model the causal structure that affects the rewards <ref type="bibr" target="#b4">[Lattimore et al., 2016]</ref>. Therefore, exploiting the knowledge of this structure helps to deal with the aforementioned challenges. In our paper, we develop a novel combinatorial semi-bandit framework with causally related rewards, where we rely on Structural Equation Models (SEMs) <ref type="bibr" target="#b3">[Kaplan, 2008]</ref> to model the causal relations. At each time of play, we see the instantaneous rewards of the chosen base arms as controlled stimulus to the causal system. Consequently, in our causal system, the solution to the decision-making problem is the choice over the exogenous input that maximizes the collected reward. We propose a decision-making policy to solve the aforementioned problem and prove that it achieves a sublinear regret bound in time.</p><p>Our developed framework can be used to model various realworld problems, such as network data analysis of biological networks or financial markets. We apply our framework to analyze the development of Covid-19 in Italy. We show that our proposed policy is able to detect the regions that contribute the most to the spread of Covid-19 in the country.</p><p>Compared to previous works, our proposed framework does not require any prior knowledge of the structural dependencies. For example, in <ref type="bibr" target="#b7">[Tang et al., 2017]</ref>, the authors exploit the prior knowledge of statistical structures to learn the best combinatorial strategy. At each decision-making round, the agent receives the reward of the selected super arm and some side rewards from the selected base arms' neighbors. In <ref type="bibr">[Huyuk and Tekin, 2019]</ref> a Combinatorial Thompson Sampling (CTS) algorithm to solve a combinatorial semi-bandit problem with probabilistically triggered arms is proposed. The proposed algorithm has access to an oracle that determines the best decision at each round of play based on the already collected data. Similarly, the authors in <ref type="bibr" target="#b1">[Chen et al., 2016]</ref> study a setting where triggering super arms can probabilistically trigger other unchosen arms. They propose an Upper Confidence Bound (UCB)-based algorithm that uses an oracle to improve the decision-making process. In <ref type="bibr" target="#b9">[Yu et al., 2020]</ref>, the authors formulate a combinatorial bandit problem where the agent has access to an influence diagram that represents the probabilistic dependencies in the system. The authors propose a Thompson sampling algorithm and its approximations to solve the formulated problem. Further, there are some works that study the underlying structure of the problem. For example, in <ref type="bibr" target="#b8">[Toni and Frossard, 2018]</ref>, the authors attempt to learn the structure of a combinatorial bandit problem. However, they do not assume any causal relations between rewards. Moreover, in <ref type="bibr" target="#b6">[Sen et al., 2017]</ref>, the MAB framework is employed to identify the best soft intervention on a causal system while it is assumed that the causal graph is only partially unknown.</p><p>The rest of the paper is organized as follows. In Section 2, we formulate the structured combinatorial semi-bandit problem with causally related rewards. In Section 3, we introduce our proposed algorithm, namely SEM-UCB. Section 4 includes the theoretical analysis of the regret performance of SEM-UCB. Section 5 is dedicated to numerical evaluation. Section 6 concludes the paper. We consider a causally structured combinatorial semibandit problem where an agent sequentially selects a subset of base arms over time. We refer to this subset as the super arm. More precisely, at each time t, the agent selects a decision vector</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Problem Formulation</head><formula xml:id="formula_0">x t = [x t [1], x t [2], . . . , x t [N ]] ∈ {0, 1}</formula><p>N . If the agent selects the base arm i at time t, we have x t</p><formula xml:id="formula_1">[i] = 1, otherwise x t [i] = 0. The agent observes the value of b t [i] at time t only if x t [i] = 1.</formula><p>The agent is allowed to select at most s base arms at each time of play. Hence, we define the set of all feasible super arms as</p><formula xml:id="formula_2">X = x | x ∈ {0, 1} N ∧ ∥x∥ 0 ≤ s ,<label>(1)</label></formula><p>where ∥•∥ 0 determines the number of non-zero elements in a vector. In our problem, the parameter s is pre-determined and is given to the agent. We take advantage of a directed graph structure to model the causal relationships in the system. We consider an unknown stationary sparse Directed Acyclic Graph (DAG) G = (V, E, A), where V denotes the set of N vertices, i.e., |V| = N , E is the edge set, and A denotes the weighted adjacency matrix. By p ≤ N -1, we denote the length of the largest path in the graph G. We assume that the reward generating processes in the bandit setting follow an error-free Structural Equation Model (SEM) <ref type="bibr">([Giannakis et al., 2018]</ref>, <ref type="bibr" target="#b0">[Bazerque et al., 2013]</ref>). The exogenous input vector and the endogenous output vector of the SEM at each time t are denoted by</p><formula xml:id="formula_3">z t = [z t [1], z t [2], . . . , z t [N ]] and y t = [y t [1], y t [2], . . . , y t [N ]],</formula><p>respectively. At each time t, the exogenous input z t represents the semi-bandit feedback in the decision-making problem. Formally,</p><formula xml:id="formula_4">z t = diag(b t )x t ,<label>(2)</label></formula><p>where diag(•) represents the diagonalization of its given input vector. Consequently, we define the elements of the endogenous output vector y t as</p><formula xml:id="formula_5">y t [i] = i̸ =j A[i, j]y t [j] + F[i, i]z t [i], ∀i = 1, . . . , N,<label>(3)</label></formula><p>where F is a diagonal matrix that captures the effects of the exogenous input vector z t . The SEM in Equation ( <ref type="formula" target="#formula_5">3</ref>) implies that the output measurement y t [i] depends on the single-hop neighbor measurements in addition to the exogenous input signal z t [i]. In our formulation, at each time t, the endogenous output y t [i] represents the overall reward of the corresponding base arm i ∈ [N ]. Therefore, at each time t, the overall reward of each base arm comprises two parts; one part directly results from its instantaneous reward, while the other part reflects the effect of causal influences of other base arms' overall rewards.</p><p>In Equation ( <ref type="formula" target="#formula_5">3</ref>), the overall rewards are causally related. Thus, the adjacency matrix A represents the causal relationships between the overall rewards; accordingly, the element A[i, j] of the adjacency matrix A denotes the causal impact of the overall reward of base arm j on the overall reward of base arm i, and we have A[i, i] = 0, ∀i = 1, 2, . . . , N . We assume that the agent is not aware of the causal relationships between the overall rewards. Hence, the adjacency matrix A is unknown a priori. In the following, we work with the matrix form of Equation ( <ref type="formula" target="#formula_5">3</ref>), defined at time t as</p><formula xml:id="formula_6">y t = Ay t + Fz t .<label>(4)</label></formula><p>In Figure <ref type="figure" target="#fig_1">1</ref>, we illustrate an exemplary network consisting of N vertices and the underlying causal relations. Based on our problem formulation, the agent is able to observe both the exogenous input signal vector z t and the endogenous output signal vector y t . As we see, there does not exist necessarily a causal relation between every pair of nodes. By inserting (2) in ( <ref type="formula" target="#formula_6">4</ref>) and solving for y t we obtain</p><formula xml:id="formula_7">y t = (I -A) -1 F diag(b t )x t .<label>(5)</label></formula><p>Finally, we define the payoff received by the agent upon choosing the decision vector x t as</p><formula xml:id="formula_8">r(x t ) = 1 ⊤ y t = 1 ⊤ (I -A) -1 F diag(b t )x t ,<label>(6)</label></formula><p>where 1 is the N -dimensional vector of ones. Since the graph G is a DAG, it implies that with a proper indexing of the vertices, the adjacency matrix A is a strictly upper triangular matrix. This guarantees that the matrix (I -A) is invertible. In our problem, since the agent directly observes the exogenous input, we assume that the effects of F on the exogenous inputs are already integrated in the instantaneous rewards. Therefore, to simplify the notation and without loss of generality, we assume that F = I in the following. Given a decision vector x t ∈ X , the expected payoff at time t is calculated as</p><formula xml:id="formula_9">y t [1] y t [2] y t [3] y t [N ] A[2, 1] A[2, 3] A[3, 1] A[3, N ] ... zt[1] y t [1] F[1, 1] zt[2] y t [2] F[2, 2] zt[3] y t [3] F[3, 3] zt[N ] y t [N ] F[N, N ]</formula><formula xml:id="formula_10">µ(x t ) = E [r(X)|X = x t ] ,<label>(7)</label></formula><p>where the expectation is taken with respect to the randomness in the reward generating processes.</p><p>Ideally, the agent's goal is to maximize her total mean payoff over a time horizon T . Alternatively, the agent aims at minimizing the expected regret, defined as the difference between the expected accumulated payoff of an oracle that follows the optimal policy and that of the agent that follows the applied policy. Formally, the expected regret is defined as</p><formula xml:id="formula_11">R T (X ) = T µ(x * ) - T t=1 µ(x t ),<label>(8)</label></formula><p>where x * = argmax x∈X µ(x) is the optimal decision vector, and</p><p>x t denotes the selected decision vector at time t under the applied policy.</p><p>Remark 1. The definition of payoff in (6) implies that we are dealing with a linear combinatorial semi-bandit problem with causally related rewards. In general, due to the randomness in selection of the decision vector x t , the consecutive overall reward vectors y t become non-identically distributed. In the following section, we propose our algorithm that is able to deal with such variables. This is an improvement over the previous methods, such as <ref type="bibr" target="#b1">[Chen et al., 2016]</ref> and <ref type="bibr">[Huyuk and Tekin, 2019]</ref>, that are not able to cope with our problem formulation, as they are specially designed to work with i.i.d. random variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Decision-Making Strategy</head><p>In this section, we present our decision-making strategy to solve the problem described in Section 2. Our proposed policy consists of two learning components: (i) an online graph learning and (ii) an Upper Confidence Bound (UCB)-based reward learning. In the following, we describe each component separately and propose our algorithm, namely SEM-UCB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Online Graph Learning</head><p>The payoff defined in (6) implies that the knowledge of A is necessary to select decision vectors that result in higher accumulated payoffs. Therefore, the agent aims at learning the matrix A to improve her decision-making process. To this end, we propose an online graph learning framework that uses the collected feedback, i.e., the collected exogenous input and endogenous output vectors, to estimate the ground truth matrix A. In the following, we formalize the online graph learning framework.</p><p>At each time t, we collect the feedback up to the current time in</p><formula xml:id="formula_12">Z t = [z 1 . . . z t ] and Y t = [y 1 . . . y t ]. Therefore, Y t = AY t + Z t .<label>(9)</label></formula><p>We assume that the right indexing of the vertices is known prior to estimating the ground truth adjacency matrix. We use the collected feedback Y t and Z t as the input to a parametric graph learning algorithm <ref type="bibr">([Giannakis et al., 2018]</ref>, <ref type="bibr" target="#b1">[Dong et al., 2019]</ref>). More precisely, we use the following optimization problem to estimate the adjacency matrix at time t.</p><formula xml:id="formula_13">Ât = argmin A ∥Y t -AY t -Z t ∥ 2 2 + g(A) s.t. A[i, j] ≥ 0, ∀i, j ∈ [N ], A[i, j] = 0, ∀i ≥ j,<label>(10)</label></formula><p>where ∥•∥ 2 represents the L 2 -norm of matrices and g(A) is a regularization function that imposes sparsity over A. In our numerical experiments, we work with different regularization functions to demonstrate the effectiveness of our proposed algorithm in different scenarios. As an example, we impose the sparsity property on the estimated matrix Ât in (10) by defining g(A) = λ ∥A∥ 1 , where ∥•∥ 1 is the L 1 -norm of the matrices and λ is the regularization parameter. Our choices of regularization function guarantee that the optimization problem (10) is convex.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">SEM-UCB Algorithm</head><p>We propose our decision-making policy in Algorithm 1. The key idea behind our algorithm is that it works with observations for each base arm, rather than the payoff observations for each super arm. As the same base arm can be observed while selecting different super arms, we can use the obtained information from selection of a super arm to improve our payoff estimation of other relevant super arms. This, combined with the fact that our algorithm simultaneously learns the causal relations, significantly improves the performance of our proposed algorithm and speed up the learning process.</p><p>For each base arm i, we define the empirical average of instantaneous rewards at time t as</p><formula xml:id="formula_14">βt [i] = t τ =1 b τ [i]1 {x τ [i] = 1} m t [i] ,<label>(11)</label></formula><p>where m t [i] denotes the number of times that the base arm i is observed up to time t. Formally, Select column t of the initialization matrix M as the decision vector x t .</p><formula xml:id="formula_15">m t [i] = t τ =1 1 {x τ [i] = 1} . (<label>12</label></formula><p>3:</p><p>Observe z t and y t . 4: end for 5: for t = N + 1, . . . , T do 6: Solve (10) to obtain Ât-1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7:</head><p>Calculate E t-1 [i] using (13), ∀i ∈ [N ].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>8:</head><p>Select decision vector x t that solves (14). 9:</p><p>Observe z t and y t . 10: end for</p><p>The initialization phase of SEM-UCB algorithm follows a specific strategy to create a rich data that helps to learn the ground truth adjacency matrix. At each time t during the first N times of play, SEM-UCB picks the column t of an uppertriangular initialization matrix M ∈ {0, 1} N ×N , where M is created as follows. All diagonal elements of M are equal to 1. As for the column i, if i ≤ s, we set all elements above diagonal to 1. If s + 1 ≤ i ≤ N , we select s -1 elements above diagonal uniformly at random and set them to 1. The remaining elements are set to 0.</p><p>After the initialization period, our proposed algorithm takes two steps at each time t to learn the causal relationships and the expected instantaneous rewards of the base arms. First, it uses the collected feedback Y t and Z t and solves the optimization problem (10) to obtain the estimated adjacency matrix. It then uses the reward observations to calculate the UCB index E t [i] for each base arm i, defined as</p><formula xml:id="formula_16">E t [i] = βt [i] + (s + 1)lnt m t [i] .<label>(13)</label></formula><p>Afterward, the algorithm selects a decision vector x t using the current estimate of the adjacency matrix and the developed UCB indices of the base arms. Let</p><formula xml:id="formula_17">E t = [E t [1], E t [2], . . . , E t [N ]]. At time t, SEM-UCB selects x t as x t = argmax x∈X 1 ⊤ (I -Ât-1 ) -1 diag(E t-1 )x s.t. ∥x∥ 0 ≤ s. (<label>14</label></formula><formula xml:id="formula_18">)</formula><p>Remark 2. The initialization phase of our algorithm guarantees that all the base arms are pulled at least once and the matrix M is full rank. Consequently, the adjacency matrix A is uniquely identifiable from the collected feedback <ref type="bibr" target="#b0">[Bazerque et al., 2013]</ref>.</p><formula xml:id="formula_19">Remark 3. Let c ⊤ = 1 ⊤ (I -Ât-1 ) -1 diag(E t-1</formula><p>). Since all the elements of both matrices E t-1 and Ât-1 are nonnegative, we have c[i] &gt; 0, ∀i ∈ [N ]. Thus, the optimization problem ( <ref type="formula" target="#formula_17">14</ref>) reduces to finding the s-biggest elements of c. Therefore, ( <ref type="formula" target="#formula_17">14</ref>) can be solved efficiently based on the choice of sorting algorithm used to order the elements of c.</p><p>The computational complexity of the SEM-UCB algorithm varies depending on the solver that is used to learn the graph. For example, if we use OSQP solver <ref type="bibr" target="#b6">[Stellato et al., 2020]</ref>, we achieve a computational complexity of order O(N 4 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Theoretical Analysis</head><p>In this section, we prove an upper bound on the expected regret of SEM-UCB algorithm. We use the following definitions in our regret analysis. For any decision vector x ∈ X , let ∆(x) = µ(x * ) -µ(x). We define ∆ max = max</p><p>x:µ(x)&lt;µ(x * ) ∆(x) and ∆ min = min</p><p>x:µ(x)&lt;µ(x * ) ∆(x). Moreover, let w ⊤ t = 1 ⊤ (I-Ât ) -1 diag(x t+1 ). We define w max = max</p><formula xml:id="formula_20">t max i w t [i].</formula><p>The following theorem states an upper bound on the expected regret of SEM-UCB. Theorem 1. The expected regret of SEM-UCB algorithm is upper bounded as</p><formula xml:id="formula_21">R T (X ) ≤ 4w 2 max s 2 (s + 1)N ln T ∆ 2 min + N + π 2 3 s p N ∆ max . (<label>15</label></formula><formula xml:id="formula_22">)</formula><p>Proof. See Section 1 of supplementary material. ■</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Analysis</head><p>In this section, we present experimental results to provide more insight on the usefulness of learning the causal relations for improving the decision-making process. We evaluate the performance of our algorithm on synthetic and realworld datasets by comparing it to standard benchmark algorithms.</p><p>Benchmarks. We compare SEM-UCB with state-of-theart combinatorial semi-bandit algorithms that do not learn the causal structure of the problem. Specifically, we compare our algorithm with the following policies: (i) CUCB <ref type="bibr" target="#b1">[Chen et al., 2016]</ref> calculates a UCB index for each base arm at each time t and feeds them to an approximation oracle that outputs a super arm. (ii) DFL-CSR <ref type="bibr" target="#b7">[Tang et al., 2017]</ref> develops a UCB index for each base arm and selects a super arm at each t based on a prior knowledge of a graph structure that shows the correlations among base arms. (iii) CTS <ref type="bibr">[Huyuk and Tekin, 2019]</ref> employs Thompson sampling and uses an oracle to select a super arm at each time t. (iv) FTRL <ref type="bibr" target="#b10">[Zimmert et al., 2019]</ref> selects a super arm at each time t based on the method of Follow-the-Regularized-Leader. To be comparable, we apply these benchmarks on the vector of overall reward y t at each time t. If a benchmark requires y t to be in [0, 1], we feed the normalized version of y t to the corresponding algorithm. Finally, in our experiments, we choose s = 6, meaning that the algorithms can choose 6 base arms at each time of play.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Synthetic Dataset</head><p>Our simulation setting is as follows. We first create a graph consisting of N = 20 nodes. The elements of the adjacency matrix A are drawn from a uniform distribution over [0.4, 0.7]. The edge density of the ground truth adjacency matrix is 0.15. At each time t, the vector of instantaneous rewards b t is drawn from a multivariate normal distribution with the support in [0, 1] 20 and a spherical covariance matrix.</p><p>As demonstrated in Section 2, we generate the vector of overall rewards according to the SEM in (3). We use g(A) = λ ∥A∥ 1 as the regularization function in (10) when estimating the adjacency matrix A. The regularization parameter λ is tuned by grid search over [0.0001, 1000]. We evaluate the estimated adjacency matrix at each time t by using the mean squared error defined as</p><formula xml:id="formula_23">MSE = 1 N 2 A -Â 2 F</formula><p>, where ∥•∥ F denotes the Frobenius norm.</p><p>Comparison with the benchmarks. We run the algorithms using the aforementioned synthetic data with T = 4000. In Figure <ref type="figure" target="#fig_2">2</ref>, we depict the trend of time-averaged expected regret for each policy. As we see, SEM-UCB surpasses all other policies. This is due to the fact that SEM-UCB learns the network's topology and hence, it has a better knowledge of the causal relationships in the graph structure, unlike other policies that do not estimate the graph structure. As we see, the time-averaged expected regret of SEM-UCB tends to zero. This matches with our theoretical results in Section 4. Note that, the benchmark policies exhibit a suboptimal regret performance as they have to deal with nonidentically distributed random y t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Covid-19 Dataset</head><p>We evaluate our proposed algorithm on the Covid-19 outbreak dataset of daily new infected cases during the pandemic in different regions within Italy. 1 The dataset fits in our framework as the daily new cases in each region results from the causal spread of Covid-19 among the regions in a country <ref type="bibr">[Mastakouri and Schölkopf, 2020]</ref> and the regionspecific characteristics <ref type="bibr" target="#b2">[Guaitoli and Pancrazi, 2021]</ref>. As the regions differ in their regional characteristics, such as socioeconomic and geographical characteristics, each region has a specific exposure risk of Covid-19 infection. To be consistent with our terminology in Section 2, at each time (day) t, we use the overall reward y t [i] to refer to the overall daily new cases in region i and use the instantaneous reward b t [i] to 1 <ref type="url" target="https://github.com/pcm-dpc/COVID-19">https://github.com/pcm-dpc/COVID-19</ref> refer to the region-specific daily new cases in region i. Naturally, the overall daily new cases includes the region-specific daily new cases of Covid-19 infection.</p><p>Governments around the world strive to track the spread of Covid-19 and find the regions that are contributing the most to the total number of daily new cases in the country <ref type="bibr">[Bridgwater and Bóta, 2021]</ref>. By the end of this experiment, we address this critical problem and highlight that our algorithm is capable of finding the optimal candidate regions for political interventions in order to contain the spread of a contagious disease such as Covid-19.</p><p>Data preparation. We focus on the recorded daily new cases from 10 August to 15 October, 2020, for N = 21 regions within Italy. The Covid-19 dataset only provides us with the overall daily new cases of each region. Hence, in order to apply our algorithm, we need to infer the distribution of region-specific daily new cases for each region. In the following, we describe this process and further pre-processing of the Covid-19 dataset.</p><p>According to <ref type="bibr" target="#b1">[Bull, 2021]</ref>, for the time period from 18 May to 3 June, 2020, all places for work and leisure activities were opened and travelling within regions was permitted while travelling between regions was forbidden. Consequently, during this period, there are no causal effects on the overall daily new cases of each region from other regions. In addition, according to google mobility data <ref type="bibr" target="#b5">[Nouvellet et al., 2021]</ref>, during 4 weeks prior to 18 May the mobility was increasing within the regions while travel ban between the regions was still imposed. Hence, we use this expanded period to estimate the underlying distributions of the region-specific daily new cases using a kernel density estimation. Finally, considering that the daily recorded data noticeably fluctuates, a 7-day moving average was applied to the signals.</p><p>We create the region-specific daily new cases for each region by sampling from the estimated distributions. Below, we present the results of applying our algorithm on the preprocessed Covid-19 dataset. Since the data only contains the reported overall daily new cases for a limited time period, care should be exercised in interpreting the results. However, by providing more relevant data, our proposed framework helps towards more accurate detection of the regions that contribute the most to the development of Covid-19.</p><p>Learning the structural dependencies. Our algorithm learns the ground truth adjacency matrix A using (10). As for the choice of regularization function in (10), we employ Directed Total Variation (DTV) which is a novel application of the Graph Directed Variation (GDV) function <ref type="bibr" target="#b6">[Sardellitti et al., 2017]</ref>. DTV regularization function is defined as</p><formula xml:id="formula_24">g(A) = λ i,j=1,...,N A[i, j] k=1,...,t [Y[i, k] -Y[j, k]] + ,<label>(16)</label></formula><p>[y] + = max {y, 0} .</p><p>The regularization function addresses the smoothness of the entire observations Y over the underlying directed graph. To be more realistic, since the causal spread of the disease might create cycles, we additionally include cyclic graphs in the search space of the optimization problem (10). We perform cross-validation technique to tune the regularization parameter λ. As mentioned before, we work on a limited time period with T = 66 days. Thus, we split the data into train and validation sets in 10:1 ratio. More specifically, we split the data into 6 subsets of 11 consecutive days.</p><p>In each subset, one day is chosen uniformly at random to be included in the validation set while the remaining 10 days are added to the train set. We calculate the prediction error at each time t by</p><formula xml:id="formula_26">Error(t) = 1 N K(t) i∈K(t) ∥y i -ŷi ∥ 1 ,<label>(18)</label></formula><p>where K(t) is the validation set at time t with cardinality K(t) = |K(t)| and y i and ŷi are the validation data and the corresponding predicted value using the estimated graph for day i, respectively. Figure <ref type="figure" target="#fig_3">3</ref> compares the ground truth overall daily new cases and the predicted overall daily new cases using the estimated graph on 4 different days of the Covid-19 outbreak in our validation data. Due to space limitation, we use abbreviations for region names. Table <ref type="table">1</ref> in Section 2.1 of supplementary material lists the abbreviations together with the original names of the regions. We observe that our proposed framework is capable to estimate the data for each region efficiently, that helps the agent to improve its decisionmaking process in a real-world scenario.</p><p>Learning regions with the highest contribution. In  time by following the SEM-UCB policy. Dark rectangles represent the 6 selected regions at each day (time). Based on our framework, we represent the selected regions by our algorithm as those with biggest contributions to the development of Covid-19 during the time interval considered in our experiment. More specifically, we find the regions of Lombardia, Emilia-Romagna, Lazio, Veneto, Piemonte, and Liguria as the ones that contribute the most to the spread of Covid-19 during that period in Italy.</p><p>We emphasize that, due to the causal effects among the regions, contribution of each region to the spread of covid-19 differs from its overall daily cases of infection. Thus, the set of regions with the highest contribution does not necessarily equal to the set of regions with the highest total number of daily cases. This is a key aspect of our problem formulation that is addressed by SEM-UCB in Figure <ref type="figure" target="#fig_5">4</ref>. We elaborate more on this fact in Section 2.3 of supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we developed a combinatorial semi-bandit framework with causally related rewards, where we modelled the causal relations by a directed graph in a structural equation model. We developed a decision-making policy, namely SEM-UCB, that learns the structural dependencies to improve the decision-making process. We proved that SEM-UCB achieves a sublinear regret bound in time. Our framework is applicable in a number of contexts such as network data analysis of biological networks or financial markets. We applied our method to analyze the development of Covid-19. The experiments showed that SEM-UCB outperforms several state-of-the-art combinatorial semi-bandit algorithms. Future research directions would be to extend the current framework to deal with piece-wise stationary environments where the causal graph and/or the expected instantaneous rewards of the base arms undergo abrupt changes over time.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Let [N ] = {1, 2, . . . , N } denote the set of base arms. b t = [b t [1], b t [2], . . . , b t [N ]] ∈ [0, 1] N represents the vector of instantaneous rewards of the base arms at time t. The instantaneous rewards of each base arm i ∈ [N ] are independent and identically distributed (i.i.d.) random variables drawn from an unknown probability distribution with mean β[i]. We collect the mean rewards of all the base arms in the mean reward vector of β = [β[1], β[2], . . . , β[N ]].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An exemplary illustration of a graph consisting of N vertices and their causal relations. The black directed edges represent the causal relationships amongst the vertices.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Time-averaged expected regret of different policies.</figDesc><graphic coords="5,54.54,54.01,241.82,152.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Original overall daily new cases and the corresponding predicted values for different days in the validation set.</figDesc><graphic coords="6,54.54,54.00,241.92,266.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>, we show the decision-making process of the agent over</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Selected regions on each day.</figDesc><graphic coords="6,315.54,54.00,241.92,113.59" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>)</figDesc><table><row><cell>Algorithm 1 SEM-UCB: Structural Equation Model-Upper</cell></row><row><cell>Confidence Bound</cell></row><row><cell>Input: Parameter s, initialization matrix M.</cell></row></table><note><p>1: for t = 1, . . . , N do 2:</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence </p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work was partially funded by the <rs type="funder">Deutsche Forschungsgemeinschaft (DFG, German Research Foundation</rs>) under Germany's Excellence Strategy -EXC number 2064/1 -Project number <rs type="grantNumber">390727645</rs>, and by Grant <rs type="grantNumber">01IS20051</rs> from the <rs type="funder">German Federal Ministry of Education and Research (BMBF)</rs>. We are grateful to <rs type="person">Sergio Barbarossa</rs> and <rs type="person">Sofien Dhouib</rs> for fruitful discussions and comments.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_TQeRmQ6">
					<idno type="grant-number">390727645</idno>
				</org>
				<org type="funding" xml:id="_Rtj4YZY">
					<idno type="grant-number">01IS20051</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Bridgwater and Bóta, 2021] Alexander Bridgwater and András Bóta. Identifying regions most likely to contribute to an epidemic outbreak in a human mobility network</title>
		<author>
			<persName><surname>Auer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2013 IEEE Global Conference on Signal and Information Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2002">2002. 2002. 2013. 2013. 2021. 2021</date>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
	<note>Machine learning</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Combinatorial multi-armed bandit and its extension to probabilistically triggered arms</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Bull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Bull</surname></persName>
		</author>
		<author>
			<persName><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Georgios B Giannakis, Yanning Shen, and Georgios Vasileios Karanikolas. Topology identification and learning over graphs: Accounting for nonlinearities and dynamics. Proceedings of the IEEE</title>
		<editor>
			<persName><forename type="first">Dorina</forename><surname>Dong</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Michael</forename><surname>Thanou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Pascal</forename><surname>Rabbat</surname></persName>
		</editor>
		<editor>
			<persName><surname>Frossard</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2013">2021. 2021. 2013. 2013. 2016. 2016. 2019. 2019. 2018</date>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="787" to="807" />
		</imprint>
	</monogr>
	<note>The italian government response to covid-19 and the making of a prime minister</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Huyuk and Tekin, 2019] Alihan Huyuk and Cem Tekin. Analysis of thompson sampling for combinatorial multiarmed bandit with probabilistically triggered arms</title>
		<author>
			<persName><forename type="first">Pancrazi</forename><surname>Guaitoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Guaitoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roberto</forename><surname>Pancrazi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 22nd International Conference on Artificial Intelligence and Statistics</title>
		<title level="s">The Lancet Regional Health-Europe</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2021. 2021. 2019</date>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1322" to="1330" />
		</imprint>
	</monogr>
	<note>Covid-19: Regional policies and local infection risk: Evidence from italy with a modelling study</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Structural equation modeling: Foundations and extensions</title>
		<author>
			<persName><forename type="first">David</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><surname>Kaplan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008. 2008</date>
			<publisher>Sage Publications</publisher>
			<biblScope unit="volume">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Mastakouri and Schölkopf, 2020] Atalanti Mastakouri and Bernhard Schölkopf. Causal analysis of covid-19 spread in germany</title>
		<author>
			<persName><surname>Lattimore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</editor>
		<meeting>the 30th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016">2016. 2016. 2016. 2020</date>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="3153" to="3163" />
		</imprint>
	</monogr>
	<note>Advances in Neural Information Processing Systems</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Reduction in mobility and covid-19 transmission</title>
		<author>
			<persName><surname>Nouvellet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature communications</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Identifying best interventions through online importance sampling</title>
		<author>
			<persName><forename type="first">Herbert</forename><surname>Robbins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Robbins</surname></persName>
		</author>
		<author>
			<persName><surname>Sardellitti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<title level="s">Some aspects of the sequential design of experiments. Bulletin of the American Mathematical Society</title>
		<imprint>
			<date type="published" when="1952">1952. 1952. 2017. 2017. 2017. 2017. 2020</date>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="637" to="672" />
		</imprint>
	</monogr>
	<note>Mathematical Programming Computation</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Networked stochastic multi-armed bandits with combinatorial strategies</title>
		<author>
			<persName><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE 37th International Conference on Distributed Computing Systems (ICDCS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page" from="786" to="793" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Spectral mab for unknown graph processes</title>
		<author>
			<persName><forename type="first">Toni</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Frossard</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Laura</forename><surname>Toni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Frossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 26th European Signal Processing Conference (EUSIPCO)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="116" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Graphical models meet bandits: A variational thompson sampling approach</title>
		<author>
			<persName><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="10902" to="10912" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Beating stochastic and adversarial semibandits optimally and simultaneously</title>
		<author>
			<persName><surname>Zimmert</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="7683" to="7692" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
