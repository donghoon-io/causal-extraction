<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PPAT: Progressive Graph Pairwise Attention Network for Event Causality Identification</title>
				<funder ref="#_JUVEeEH">
					<orgName type="full">Stable Support Program for Higher Education Institutions of Shenzhen</orgName>
				</funder>
				<funder ref="#_XSCzF69">
					<orgName type="full">Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_smj7d3K">
					<orgName type="full">Strategic Emerging Industry Development Special Funds of Shenzhen</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhenyu</forename><surname>Liu</surname></persName>
							<email>liuzhenyuhit@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<settlement>Shenzhen</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Baotian</forename><surname>Hu</surname></persName>
							<email>hubaotian@hit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<settlement>Shenzhen</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhenran</forename><surname>Xu</surname></persName>
							<email>xuzhenran@stu.hit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<settlement>Shenzhen</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Min</forename><surname>Zhang</surname></persName>
							<email>zhangmin2021@hit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Harbin Institute of Technology</orgName>
								<address>
									<settlement>Shenzhen</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">PPAT: Progressive Graph Pairwise Attention Network for Event Causality Identification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-21T20:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Event Causality Identification (ECI) aims to identify the causality between a pair of event mentions in a document, which is composed of sentencelevel ECI (SECI) and document-level ECI (DECI). Previous work applies various reasoning models to identify the implicit event causality. However, they indiscriminately reason all event causality in the same way, ignoring that most inter-sentence event causality depends on intra-sentence event causality to infer. In this paper, we propose a Progressive graph Pairwise Attention network (PPAT) to consider the above dependence. PPAT applies a progressive reasoning strategy, as it first predicts the intra-sentence event causality, and then infers the more implicit inter-sentence event causality based on the SECI result. We construct a sentence boundary event relational graph, and PPAT leverages a simple pairwise attention mechanism, which attends to different reasoning chains on the graph. In addition, we propose a causality-guided training strategy for assisting PPAT in learning causalityrelated representations on every layer. Extensive experiments show that our model achieves stateof-the-art performance on three benchmark datasets (5.5%, 2.2% and 4.5% F1 gains on EventStoryLine, MAVEN-ERE and Causal-TimeBank). Code is available at <ref type="url" target="https://github.com/HITsz-TMG/PPAT">https://github.com/HITsz-TMG/PPAT</ref>.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Event Causality Identification (ECI) seeks to identify the causal relation between two events in text. For example, as shown in Figure <ref type="figure" target="#fig_1">1</ref>, in the sentence "The strong 6.1-magnitude quake left hundreds more injured ...", the ECI model should identify the causality between "quake" and "injured". ECI presents the causal structure of text, which is beneficial to a wide range of applications in natural language processing (NLP), including future event forecasting <ref type="bibr">[Hashimoto, 2019]</ref>, machine reading comprehension <ref type="bibr" target="#b1">[Berant et al., 2014]</ref>, and question answering <ref type="bibr" target="#b9">[Oh et al., 2016]</ref>.</p><p>On July 03, an earthquake killed six children and left 14 others trapped when a mosque collapsed during a Koran reading session in •••••• The strong 6.1-magnitude quake left hundreds more injured as it rocked a region that was devastated by the quake-triggered tsunami of 2004.  ECI consists of two parts: sentence-level ECI (SECI) <ref type="bibr">[Liu et al., 2020]</ref> which aims to identify the intra-sentence event causality, and document-level ECI (DECI) <ref type="bibr" target="#b5">[Gao et al., 2019]</ref> which aims to identify the inter-sentence event causality. Previous studies <ref type="bibr">[Phu and Nguyen, 2021;</ref><ref type="bibr" target="#b3">Chen et al., 2022]</ref> do not explicitly distinguish intra-and inter-sentence event causality and use the same model to learn their representation, yet intra-and inter-sentence causality are expressed differently. Most intra-sentence event causality are explicitly expressed with causal cues in local context. Take Figure <ref type="figure" target="#fig_1">1</ref> as an example, the causality of intra-sentence event pair "(quake, injured)" could be identified easily with the causality indicator "left". However, inter-sentence event causality is more implicitly expressed with multiple sentences, and needs to be inferred from intra-sentence event causality. As shown in Figure <ref type="figure" target="#fig_1">1</ref>, based on the above intra-sentence event causality and coreference relation "(quake, earthquake)", we can propagate the causality via the coreference chain and infer that the event pair "(earthquake, injured)" also has causality.</p><p>In this paper, we aim to address the above issue by presenting a novel Progressive Graph Pairwise Attention Network (PPAT). PPAT applies a progressive reasoning strategy, i.e., it first predicts the intra-sentence causality with local context, and then reasons the inter-sentence causality based on the previous SECI prediction, taking the dependence of inter-sentence causality on intra-sentence causality into consideration. For the implementation of progressive reasoning, we construct a Sentence boundary Event Relational Graph (SERG). Each node of SERG denotes an event pair, and two nodes that share one event have two directed edges connecting with each other. Specially, the intra-sentence nodes only connect with the intra-sentence nodes in SERG. Figure <ref type="figure" target="#fig_1">1</ref> shows an example. The intra-sentence node (in blue) does not have edges directed from inter-sentence nodes (in green), while inter-sentence nodes can aggregate information from the intra-sentence node via directed edges. The edges model the interaction between directly related node pairs that share the same event, and encourage PPAT to reason the intrasentence event causality on the local patterns.</p><p>Moreover, different from previous work that uses graph neural networks to simply aggregate representations of neighborhood nodes (i.e., event pairs) <ref type="bibr">[Phu and Nguyen, 2021;</ref><ref type="bibr" target="#b3">Chen et al., 2022]</ref>, PPAT applies a simple pairwise attention mechanism, which aggregates neighbors at a reasoning chain level instead of node level. Take Figure <ref type="figure" target="#fig_1">1</ref> as an example. When the node of "(earthquake, injured)" is the target node to be reasoned, its two neighbors form a premise node pair if they contain the same event that the target node does not contain, e.g., nodes of "(quake, injured)" and "(quake, earthquake)". Then the causality of the target node could be reasoned via the following reasoning chain: Cause(quake, injured) ∧ Coreference(earthquake, quake) → Cause(earthquake, injured). Therefore, the reasoning model should regard the premise node pair as a whole part and aggregate neighbors at a reasoning chain level. To this end, our proposed pairwise attention mechanism can capture interaction between the target node and its premise node pairs, thus attending to the possible reasoning chains and inferring the target causality.</p><p>In addition, we propose a causality-guided training strategy for PPAT. Since node representations on every layer of PPAT will be served as auxiliary information for reasoning on the next layer, it is important for every layer of PPAT to learn causality-related node representations, so we apply an additional loss to provide causality supervision on every layer and assist PPAT to have better reasoning performance.</p><p>To summarize, our contributions can be listed as:</p><p>• We propose a novel progressive graph pairwise attention network (PPAT), which reasons progressively on the sentence boundary event relational graph. To the best of our knowledge, we are the first to capture the dependence of inter-sentence causal reasoning on intrasentence causality. • We propose a pairwise attention mechanism, a simple yet effective approach to attending to reasoning chains on the graph for causality propagation. • Extensive experiments on three ECI datasets show that PPAT significantly outperforms previous state-of-the-art methods, demonstrating the effectiveness of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Early feature-based methods for SECI mainly focus on designing better causality features or using external resources to improve performance, including the lexicon of causality indicators <ref type="bibr">[Mirza, 2014;</ref><ref type="bibr" target="#b5">Hidey and McKeown, 2016]</ref>, temporal patterns <ref type="bibr">[Mirza, 2014;</ref><ref type="bibr" target="#b9">Ning et al., 2018]</ref>, event semantics <ref type="bibr">[Riaz and Girju, 2014a;</ref><ref type="bibr">Riaz and Girju, 2014b]</ref>,</p><p>event co-occurrence <ref type="bibr">[Do et al., 2011;</ref><ref type="bibr" target="#b6">Hu et al., 2017]</ref>, and weakly supervised data <ref type="bibr">[Hashimoto, 2019]</ref>. As Pre-trained Language Models (PLMs) have achieved great success in a wide range of NLP tasks, many SECI work shows promising performance gains based on PLMs <ref type="bibr" target="#b6">[Kadowaki et al., 2019;</ref><ref type="bibr">Liu et al., 2020;</ref><ref type="bibr" target="#b13">Zuo et al., 2020]</ref>.</p><p>In recent years, more and more studies pay attention to document-level NLP tasks, such as event argument extraction <ref type="bibr" target="#b7">[Li et al., 2021]</ref> and relation extraction <ref type="bibr" target="#b12">[Yao et al., 2019]</ref>. Recent ECI work focuses on global inference: <ref type="bibr" target="#b5">Gao et al. [2019]</ref> use Integer Linear Programming (ILP) to model global causal structures; <ref type="bibr">RichGCN [Phu and Nguyen, 2021]</ref> utilizes several NLP tools (e.g., dependency parser) and external corpus for building event graphs, and uses graph convolutional network <ref type="bibr" target="#b6">[Kipf and Welling, 2017]</ref> for reasoning. <ref type="bibr">ERGO [Chen et al., 2022]</ref> achieves state-of-the-art (SOTA) performance with a graph transformer on an event relational graph for high-order interaction of event relations. Compared with previous work, our model focuses on reasoning progressively and attending to reasoning chains, with no need for sophisticated graph design, external NLP tools or external knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head><p>As illustrated in Figure <ref type="figure" target="#fig_2">2</ref>, the overall architecture consists of two tiers: (1) A document encoder yields event contextual representations, then concatenates the event representations for initial event pair representations. (2) The intra-and inter-sentence pairwise attention layers reason the event pair causality representation progressively, and then a classifier predicts causality based on the learned representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Document Encoder</head><p>Given a document D = {w i } L D i=1 containing L D words with event mention set N (|N | = N ), Document Encoder aims to represent all event pairs. We use BERT <ref type="bibr" target="#b4">[Devlin et al., 2019]</ref> and <ref type="bibr">Longformer [Beltagy et al., 2020]</ref> respectively as a basic encoder to obtain contextualized embeddings. For the document longer than the length limitation of encoder, we use a dynamic window to encode the entire document. Specifically, we divide D into overlapping spans according to a fixed step and input them to the encoder separately.</p><p>We apply the levitated marker [Zhong and Chen, 2021] to represent the event mentions in the document. Specifically, for each event mention, we add two marker tokens (i.e., t 1 and t 2 ) to the end of text. t 1 will share position embedding with the first token of the event mention, and t 2 will share position embedding with the last token of the event mention. By setting the attention matrix, the original document tokens cannot attend to the marker tokens, and each marker pair can only attend to the corresponding event mention tokens. We also insert "[CLS]" at the start of document ("&lt;s&gt;" for Longformer). The input text for BERT encoder could be written as follows: event i , L D is the length of document. We use BERT or Longformer to encode S and then obtain the representation of event i , denoted as e i , as follows:</p><formula xml:id="formula_0">S = [CLS], w 1 , w 2 • • • event i • • • w L D • • • t i 1 , t i 2 • • •</formula><formula xml:id="formula_1">e i = H(t i 1 ) + H(t i 2 ) 2 ⊕ H([CLS])<label>(1)</label></formula><p>where ⊕ denotes concatenation, H( * ) denotes the contextualized word embedding computed by the encoder. Then the raw representation of the event pair (event i , event j ), i.e. r ij , can be obtained by the following equation:</p><formula xml:id="formula_2">r ij = e i ⊕ e j ⊕ (e i * e j )<label>(2)</label></formula><p>where e i and e j are the representation of event i and event j respectively, * means pointwise product.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Progressive Reasoning Strategy</head><p>As stated in Section 1, the intra-and inter-sentence event causality should be reasoned separately. We thus propose a progressive reasoning strategy for learning the high-order relation of event pairs. Furthermore, we build a sentence boundary relational event graph (SERG) G = {V, E} for constraining event pair interaction, where V is the set of nodes and E is the set of edges.</p><p>With the initial node representations from the document encoder as input, PPAT first reasons sentence-level causality with single layer, and then reasons document-level causality with three layers. The output representations in the last layer is used for causality prediction. Note that the three layers for document-level reasoning share parameters. Here we only introduce the input and output of PPAT on each layer, leaving the details of graph pairwise attention to Section 3.3.</p><p>For the node of (event i , event j ), after updating its representation at layer l, we obtain the input node embedding for the next layer, i.e., n l+1 ij , by concatenating causality prediction, a binary intra-sentence marker and the updated node representation in layer l:</p><formula xml:id="formula_3">n l+1 ij = v l ij ⊕ p l ij ⊕ a ij<label>(3)</label></formula><p>where ⊕ denotes concatenated operations. v l ij is the node representation output in the l-th layer. a ij is 1 if (event i , event j ) is an intra-sentence event pair, otherwise a ij is 0. p l ij is the predicted causality possibility of (event i , event j ) in the l-th layer. We use a binary classifier to predict the causality of nodes in each layer.</p><formula xml:id="formula_4">p l ij = softmax(v l ij W c )<label>(4)</label></formula><p>where W c is the parameter weight matrix in the linear classifier. Before the first step of reasoning (i.e., l = 0), the node embedding is initialized by the raw event pair representation r ij from the document encoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Graph Pairwise Attention</head><p>In order to introduce reasoning chain-level information into representation learning, we propose a graph pairwise attention for SERG. As shown in the right part of Figure <ref type="figure" target="#fig_2">2</ref>, when (event i , event j ) is the target node to be reasoned, its premise node pairs are defined as ((event i , event k ), (event j , event k )), where 0 ≤ k &lt; N , k ̸ = i ̸ = j. Then we perform a pairwise self-attention mechanism to measure the importance of each premise node pair for the target node:</p><formula xml:id="formula_5">atten ij,k = (n ij W q )((n ik ⊕ n jk )W k ) T √ d (5)</formula><p>where n ij is the input node embedding of (event i , event j ) described in Section 3.2, W q , W k are parameter weight matrices, √ d is a scaling factor and d is the hidden size. Then we normalize the attention coefficients:</p><formula xml:id="formula_6">α ij,k = softmax ij (atten ij ) = mask ij,k exp(atten ij,k ) z∈N - ij mask ij,z exp(atten ij,z )<label>(6)</label></formula><p>where N - ij is the event mention set without event i and event j . The attention mask mask ij,k is 1 if node of (event i , event j ) have edges directed from the premise node pair, i.e., (event i , event k ) and (event k , event j ), otherwise mask ij,k is 0. After obtaining the normalized attention coefficients α ij,k , we aggregate relational knowledge from each reasoning chain:</p><formula xml:id="formula_7">v l ij = k∈N - ij α ij,k ((n ik ⊕ n jk )W v ) (7)</formula><p>where W v is the parameter weight matrix. Following Vaswani et al.</p><p>[2017], we also perform multihead attention to combine the information from different representation subspaces. The final output embedding of node (event i , event j ) can be represented as:</p><formula xml:id="formula_8">v l ij = C c=1 k∈N - ij α ij,k ((n ik ⊕ n jk )W v ) W o ,<label>(8)</label></formula><p>where ∥ and ⊕ are both concatenation operation, C is the number of heads, W o is the parameter weight matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Training Objective</head><p>Following Chen et al.</p><p>[2022], we adopt the focal loss <ref type="bibr" target="#b8">[Lin et al., 2017]</ref> to address the imbalance of positive and negative examples, as most of the event pairs have no causal relations:</p><formula xml:id="formula_9">FL(p) = -β(1 -p) γ log(p)<label>(9)</label></formula><p>where p is the predicted possibility of right label, β is a weighting factor to balance the huge number of negative examples. γ(γ ≥ 0) is a focusing parameter. We calculate the main loss L m with the predicted causality possibility at the last layer (i.e., p L ij ):</p><formula xml:id="formula_10">L m = (i,j)∈M FL(p L ij ) (<label>10</label></formula><formula xml:id="formula_11">)</formula><p>where M is the event pair set, L is the number of layers.</p><p>We adopt a causality-guided training strategy to assist PPAT to learn causality-related representation on each layer. Specifically, we use the predicted causality possibility on each layer p l ij computed from Equation 4 and calculate the focal loss as follows:</p><formula xml:id="formula_12">L c = 0≤l≤L-1 (λ l (i,j)∈M l FL(p l ij )),<label>(11)</label></formula><p>where λ l is loss weight in the l-th layer. M l is the focused event pair set in the l-th layer (in the first layer M l is intrasentence event pair set, otherwise M l is inter-sentence event pair set). PPAT's final loss is given by:</p><formula xml:id="formula_13">L = L m + L c (12)</formula><p>4 Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Evaluation Metrics</head><p>We evaluate PPAT on three datasets: EventStoryLine (version 0.9) <ref type="bibr">[Caselli and Vossen, 2017]</ref>, MAVEN-ERE <ref type="bibr" target="#b11">[Wang et al., 2022]</ref> and Causal-TimeBank <ref type="bibr">[Mirza, 2014]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>We employ BERT-BASE-UNCASED <ref type="bibr" target="#b4">[Devlin et al., 2019]</ref> or Longformer-base <ref type="bibr" target="#b0">[Beltagy et al., 2020]</ref> as the encoder. The models are optimized with AdamW [Loshchilov and <ref type="bibr" target="#b8">Hutter, 2019]</ref> with the learning rate of 1e-5 and weight decay of 0.01. We use the linear warmup with 0.1 warmup ratio. We apply a dynamic window to encode the entire document. The window length is 512 for BERT and 2048 for Longformer, and the shift step is 120 for BERT and 500 for Longformer. We train the model for 128 epochs on EventStoryLine, 64 on Causal-TimeBank and MAVEN-ERE. We choose the best checkpoint on the development set for testing. As token-level attention cannot be set on Longformer, we use the solid marker, i.e. inserting marker tokens before and after the event mention, and set "&lt;s&gt;" and the marker tokens as global tokens. The loss weight λ l are set as 2, 6, 0.1, 0.3 for l from 0 to 3. We run all the experiments on a single NVIDIA A100 GPU. proposes a self-supervised method to learn context-specific causal patterns from external causal statements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Baselines</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ECI baseline</head><p>We compare PPAT with the following ECI methods, which can handle both SECI and DECI: (1) OP <ref type="bibr">[Caselli and Vossen, 2017]</ref> is a heuristic rule that assigns causal relations to neighboring events. (2) LR+ and LIP <ref type="bibr" target="#b5">[Gao et al., 2019]</ref> are feature-based methods to construct document-level structures with various resources. ( <ref type="formula" target="#formula_4">4</ref>) BERT <ref type="bibr" target="#b4">[Devlin et al., 2019]</ref> is a baseline that consists of the BERT encoder and a linear classifier. ( <ref type="formula">5</ref>) Longformer <ref type="bibr" target="#b0">[Beltagy et al., 2020]</ref> is a baseline that consists of the Longformer encoder and a linear classifier. Due to the lack of reported results, we report the performance of our implementation. ( <ref type="formula" target="#formula_6">6</ref>) <ref type="bibr">RichGCN [Phu and Nguyen, 2021]</ref> proposes a documentlevel event interaction graph built with various NLP tools and heuristic rules, and uses a graph convolutional network (GCN) for transitivity. ( <ref type="formula">7</ref>) <ref type="bibr">ERGO [Chen et al., 2022]</ref> proposes an event relational graph and a graph transformer for high-order event relational interaction. On EventStoryLine and Causal-TimeBank, ERGO achieves the current SOTA performance on both SECI and DECI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Main Result</head><p>We report the main results on EventStoryLine, MAVEN-ERE and Causal-TimeBank in Table <ref type="table" target="#tab_1">1</ref>, 2 and 3 respectively. We break down the results on EventStoryLine and MAVEN-ERE into the SECI setting (i.e., intra-sentence event pairs) and DECI setting (i.e., inter-sentence event pairs). From the results, we have the following observations:</p><p>(1) As shown in Table <ref type="table" target="#tab_1">1</ref>, 2 and 3, our two versions of PPAT both outperform all baselines on three benchmarks in all settings. Compared with ERGO (Longformer-base), the previous SOTA method, PPAT (BERT-base) achieves the best F1 score on EventStoryLine (+1.4 on SECI, +4.9 on DECI and +5.5 on ECI) and MAVEN-ERE (+1.1 on SECI, +2.0 on DECI and +2.2 on ECI); PPAT (Longformer-base) achieves the best F1 score on Causal-TimeBank (+4.5 on SECI). The improvement demonstrates the effectiveness of PPAT.</p><p>(2) From Table <ref type="table" target="#tab_1">1</ref> and 2, on the EventStoryLine and MAVEN-ERE, although PPAT (Longformer-base) has competitive SECI performance with PPAT (BERT-base), it performs worse than PPAT (BERT-base) on DECI. The reason might be: (i) PPAT has introduced document-level interaction via graph pairwise attention network, so the ability of Long- former to encode longer text does not show much advantages.</p><p>(ii) The global attention pattern and simplified local attention in Longformer seem not competent for inter-sentence causality reasoning.</p><p>(3) On all datasets, PPAT (Longformer-base) achieves comparable or better performance than PPAT (BERT-base) on SECI. A possible reason is that Longformer can capture more abundant local context for SECI than BERT by extending token length limitation. Since the sentence-level event interaction can be introduced through the encoder, reasoning might be less important for SECI compared with DECI. Simply changing the encoder to a more expressive PLM could boost SECI performance. This also verifies the intuition that DECI is more complex to solve than SECI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Ablation Study</head><p>We provide an ablation study of PPAT (BERT-base) on the EventStoryLine in Table <ref type="table" target="#tab_4">4</ref> to analyse the effectiveness of components in PPAT.</p><p>(1) PPAT (w/o pairwise attention) reasons node embedding via the original attention method of Transformer <ref type="bibr" target="#b10">[Vaswani et al., 2017]</ref>. Compared with full version of PPAT, PPAT (w/o pairwise attention) has much poorer ability in identifying the inter-sentence event causality (-2.9 on DECI). It demonstrates that pairwise attention can effectively improve inter-sentence causality reasoning. The performance of SECI has a slight drop. A possible reason is that addi-tional reasoning might be less important for SECI, since the sentence-level event interaction has been introduced via the encoder.</p><p>(2) PPAT (w/o progressive reasoning) reasons the intraand inter-sentence event pairs together in the same time on each layer of event relational graph. Compared with removing other components, performance of PPAT (w/o progressive reasoning strategy) decreases the most on DECI and ECI . This shows that it would be better to predict inter-sentence causality based on well-reasoned intra-sentence causality representation than reasoning them together. In addition, the performance decrease of SECI verifies our hypothesis when building sentence boundary event relational graph: inter-sentence event relational information is unnecessary for intra-sentence causality reasoning.</p><p>(3) PPAT (w/o causality-guided training) is trained without causality guided loss on each layer. We see that causalityguided training strategy has significant improvement on both SECI and DECI, which proves that assisting model in learning causality-related representations is universally useful.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Case Study</head><p>In this section, we conduct a case study shown in Figure <ref type="figure" target="#fig_3">3</ref> to compare our PPAT (BERT-base) with current SOTA method, i.e, ERGO (Longformer-base). We also visualize the attention score of a relatively hard case, to explore the reasoning ability of our PPAT.</p><p>From the prediction table in Figure <ref type="figure" target="#fig_3">3</ref>, we can observe that: although ERGO is good at identifying sentence-level causality (e.g., case No.1 and No.2), it has limitations in reasoning implicit inter-sentence causality. ERGO fails at identifying the case No.7's causality, which can be reasoned from No.1 and No.4 or from No.2 and No.5. ERGO also mistakenly takes coreference as causality (No.3).</p><p>In contrast, PPAT correctly identify the case No.7's causality via effective reasoning. As shown in the attention visualization in Figure <ref type="figure" target="#fig_3">3</ref>, the predicted causality possibility increases from 0.41 to 0.76 after reasoning, indicating that: (i) PPAT infers inter-sentence event causality based on intra-sentence event causality as expected. (ii) PPAT infers with several transitivity patterns. Specifically, with the causality of "(Death, shooting)" and "(Riots, Death)", PPAT could reason that "(Riots, shooting)" has causal relation via causality transitivity pattern. Another reasoning pattern is coreference transitivity pattern: Previous work <ref type="bibr" target="#b3">[Chen et al., 2022]</ref> has shown PLMs could recognize coreference through similar word semantics, e.g., "(Riots, protests)". Together with the causality of "(protests, shooting)", PPAT can reason the causality of "(Riots, shooting)". In conclusion, the attention visualization indicates PPAT can perform highly effective reasoning with progressive reasoning and pairwise attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Representation Visualization</head><p>A good performance on ECI needs good causality representations for each event pair before classifying, so in Figure <ref type="figure" target="#fig_4">4</ref>, to further explore the representation learning ability of PPAT, we choose the event pair causality representations from PPAT and then visualize them with t-SNE method [ <ref type="bibr">Van der Maaten and Hinton, 2008]</ref>  . The text above is the original document, where events are in bold. We focus on the five colored events and show the results of ERGO and PPAT in the table (left), where the correct predictions are in green and the wrong ones are in red. In the graph (right), the two event pairs in a circle denote a reasoning chain, and the graph shows the attention scores of various reasoning chains in the 3rd layer of PPAT when reasoning the No.7 case. The predicted causality possibility P of "(Riots, shooting)" increases after passing the 3rd layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Text</head><p>Accused Philadelphia Kraft Foods Killer Held For Trial The woman who allegedly shot and killed two co-workers in September at the Kraft Foods plant in Northeast Philadelphia will face trial on multiple counts of murder and attempted murder. At a hearing on Tuesday, assistant district attorney Gail Fairman described 43-year-old Yvonne Hiller as a "methodical killer" who waved a gun within a foot of a security guard's face to regain access to the plant less than ten minutes after she had been escorted out ... between inter-sentence event pairs (i.e., star-shaped nodes) and intra-sentence event pairs (i.e., circle-shaped nodes), which indicates that PPAT treats intra-and inter-sentence event pairs differently as expected. (ii) Most causal event pairs' representations are gathered together, which brings a lot convenience for later classification, showing the effectiveness of representation learning in the pairwise attention block. (iii) Pairs of semantically similar events (e.g., "killed" and "murder") are close to the causal node cluster, as they might be helpful for reasoning and need to interact with causal event pairs. Meanwhile, the event pairs that cannot help reasoning (e.g. "hearing" and "killed") are far away from causal event pairs. This shows PPAT can utilize available relational information for learning good representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we propose a Progressive Graph Pairwise Attention Network (PPAT), which leverages pairwise attention to capture reasoning chains on the sentence boundary event relational graph. PPAT infers progressively, as it uses SECI results to help reason implicit document-level event causality. Our PPAT achieves SOTA performance on three widely-used ECI datasets with significant improvements. We conduct extensive ablation experiments, case studies and representation visualization to analyse PPAT's effectiveness. Future work may include extending PPAT to identification of other event relations, especially implicit relations in need of reasoning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example of ECI and SERG. The purple lines denote target causal relations. The coreference relation assists reasoning, denoted by the blue line. In SERG, the nodes of intra-and inter-sentence event pairs are in blue and green respectively. The orange edges denote a reasoning chain.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: The overall architecture of our PPAT (left) and the detail of pairwise attention (right). With initial event pair representations from the document encoder, the intra-and inter-sentence pairwise attention layers consecutively update representations. The pairwise attention mechanism finds all possible reasoning chains of the target node in Sentence boundary Event Relational Graph (SERG), uses target node representation as query (Q) and reasoning chain representations as key (K) and value (V) for the attention block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure3: Case study of ERGO (Longformer-base) and our PPAT (BERT-base). The text above is the original document, where events are in bold. We focus on the five colored events and show the results of ERGO and PPAT in the table (left), where the correct predictions are in green and the wrong ones are in red. In the graph (right), the two event pairs in a circle denote a reasoning chain, and the graph shows the attention scores of various reasoning chains in the 3rd layer of PPAT when reasoning the No.7 case. The predicted causality possibility P of "(Riots, shooting)" increases after passing the 3rd layer.</figDesc><graphic coords="7,54.65,314.56,303.77,183.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Visualization (left) of event pair representations and the original text (right). The blue nodes have causal relations and the red ones do not. The star-shaped and circle-shaped nodes denote inter-and intra-sentence event pairs respectively. Event mentions in text are in bold.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>EventStoryLine contains 258 documents across 22 topics, 5334 event mentions, 10347 intra-sentence event pairs and 60232 inter-sentence event pairs (1770 and 3885 of them have causal relations respectively). Following previous work<ref type="bibr" target="#b5">[Gao et al., 2019;</ref><ref type="bibr" target="#b3">Chen et al., 2022]</ref>, we use documents in the last two topics as development set, and employ 5-fold crossvalidation on the remaining documents.</figDesc><table><row><cell>MAVEN-ERE contains 3555 documents, 85912 event</cell></row><row><cell>mentions, 97521 intra-sentence event pairs and 1226168</cell></row><row><cell>inter-sentence event pairs (16044 and 47108 of them have</cell></row><row><cell>causal relations respectively). Since the original test set does</cell></row><row><cell>not contain gold labels, we divide the development set into a</cell></row><row><cell>new development set and a new test set, both of which contain</cell></row><row><cell>348 documents. As MAVEN-ERE is a relatively new dataset,</cell></row><row><cell>we reproduce the SOTA method and several strong baselines.</cell></row><row><cell>Causal-TimeBank contains 183 documents, 6811 event</cell></row><row><cell>mentions, 7608 intra-sentence event pairs (300 of them have</cell></row><row><cell>causal relations). Following previous work [Liu et al., 2020;</cell></row><row><cell>Chen et al., 2022], we employ 10-fold cross-validation eval-</cell></row><row><cell>uation for intra-sentence event pairs. Note that the number</cell></row><row><cell>of inter-sentence causal event pairs is quite small (only 20 of</cell></row><row><cell>252084 inter-sentence event pairs). Following the above pre-</cell></row><row><cell>vious work, we only evaluate the performance of SECI on</cell></row><row><cell>Causal-TimeBank.</cell></row></table><note><p><p><p>Evaluation Metrics We adopt Precision (P), Recall (R) and F1-score (F1) as evaluation metrics, same as previous work</p><ref type="bibr" target="#b5">[Gao et al., 2019;</ref> Phu and Nguyen, 2021;<ref type="bibr" target="#b3">Chen et al., 2022]</ref></p>.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Main result on EventStoryLine. The best results are in bold, * denotes model with Longformer encoders. SECI baselines listed in Section 4.3 cannot handle DECI task, and thus labeled as "-" in DECI and overall results.</figDesc><table><row><cell>SECI baseline We compare PPAT with the following SECI</cell></row><row><cell>methods: (1) KMMG [Liu et al., 2020] leverages external</cell></row><row><cell>knowledge and proposes a mention masking generalization</cell></row><row><cell>method for accurate reasoning. (2) KnowDis [Zuo et al.,</cell></row><row><cell>2020] uses a knowledge-enhanced data augmentation method</cell></row><row><cell>to tackle the data lacking problem. (3) LSIN [Cao et al.</cell></row></table><note><p><p><p><p><p><p>, 2021] uses a descriptive graph induction module for exploiting external structural knowledge. (4) LearnDA</p>[Zuo et al.,  2021b]  </p>proposes a knowledge-guided dual learning method for data augmentation. (</p>5</p>) CauSeRL</p>[Zuo et al., 2021a]   </p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Main result on MAVEN-ERE. The best results are in bold, * denotes models that apply Longformer encoders.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Main result on Causal-TimeBank. The best results are in bold, * denotes models that apply Longformer encoders. Note that Causal-TimeBank only supports SECI task.</figDesc><table><row><cell>Model</cell><cell cols="3">Causal-TimeBank (SECI) P R F1</cell></row><row><cell>OP</cell><cell>3.0</cell><cell>40.7</cell><cell>5.5</cell></row><row><cell>KMMG</cell><cell>36.6</cell><cell>55.6</cell><cell>44.1</cell></row><row><cell>KnowDis</cell><cell>42.3</cell><cell>60.5</cell><cell>49.8</cell></row><row><cell>LSIN</cell><cell>51.5</cell><cell>56.2</cell><cell>52.9</cell></row><row><cell>LearnDA</cell><cell>41.9</cell><cell>68.0</cell><cell>51.9</cell></row><row><cell>CauSeRL</cell><cell>43.6</cell><cell>68.1</cell><cell>53.2</cell></row><row><cell>BERT</cell><cell>47.6</cell><cell>55.1</cell><cell>51.1</cell></row><row><cell cols="2">Longformer* 63.6</cell><cell>55.3</cell><cell>59.2</cell></row><row><cell>RichGCN</cell><cell>39.7</cell><cell>56.5</cell><cell>46.7</cell></row><row><cell>ERGO</cell><cell>58.4</cell><cell>60.5</cell><cell>59.4</cell></row><row><cell>ERGO*</cell><cell>62.1</cell><cell>61.3</cell><cell>61.7</cell></row><row><cell cols="4">PPAT (ours) 62.5±2.2 62.4±2.4 62.4±1.1</cell></row><row><cell cols="4">PPAT (ours)* 67.9±1.7 64.6±0.3 66.2±0.7</cell></row><row><cell>Model</cell><cell></cell><cell cols="3">SECI DECI ECI</cell></row><row><cell>PPAT</cell><cell></cell><cell>65.3</cell><cell>52.0</cell><cell>56.4</cell></row><row><cell cols="2">w/o pairwise attention</cell><cell>64.8</cell><cell>49.1</cell><cell>54.3</cell></row><row><cell cols="2">w/o progressive reasoning</cell><cell>64.3</cell><cell>44.3</cell><cell>49.9</cell></row><row><cell cols="3">w/o causality-guided training 63.1</cell><cell>48.6</cell><cell>53.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>F1-score of ablation study on EventStoryLine.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>, observing that: (i) There is an obvious gap Sunday, March 17, 2013 | 5 : 00 PM Riots Erupt Following Death of Brooklyn Teen Killed By Police In the week following the fatal shooting of 16-year-old Kimani Gray, several protests and riots have erupted in the …</figDesc><table><row><cell>No.</cell><cell cols="2">Event Pair</cell><cell cols="2">Golden ERGO</cell><cell>PPAT</cell><cell>No.1 (Riots Death)</cell><cell></cell></row><row><cell>1</cell><cell>Riots</cell><cell>Death</cell><cell>Yes</cell><cell>Yes</cell><cell>Yes</cell><cell cols="2">No.4 (Death shooting)</cell></row><row><cell>2</cell><cell>Riots</cell><cell>Killed</cell><cell>Yes</cell><cell>Yes</cell><cell>Yes</cell><cell></cell><cell></cell></row><row><cell>3</cell><cell>Riots</cell><cell>protests</cell><cell>No</cell><cell>Yes</cell><cell>No</cell><cell>No.5 (Killed shooting)</cell><cell>No.3 (Riots protests)</cell></row><row><cell>4</cell><cell>Death</cell><cell>shooting</cell><cell>Yes</cell><cell>Yes</cell><cell>Yes</cell><cell>No.2 (Riots Killed)</cell><cell>No.6 (protests shooting)</cell></row><row><cell>5</cell><cell>Killed</cell><cell>shooting</cell><cell>Yes</cell><cell>Yes</cell><cell>Yes</cell><cell></cell><cell></cell></row><row><cell>6</cell><cell>protests</cell><cell>shooting</cell><cell>Yes</cell><cell>Yes</cell><cell>Yes</cell><cell>No.7 (Riots shooting)</cell><cell></cell></row><row><cell>7</cell><cell>Riots</cell><cell>shooting</cell><cell>Yes</cell><cell>No</cell><cell>Yes</cell><cell></cell><cell></cell></row><row><cell>8</cell><cell>protests</cell><cell>Killed</cell><cell>Yes</cell><cell>No</cell><cell>Yes</cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>Proceedings of the International Joint Conference on Artificial Intelligence </p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work is jointly supported by grants: <rs type="funder">Natural Science Foundation of China</rs> (No. <rs type="grantNumber">62006061</rs>), <rs type="funder">Stable Support Program for Higher Education Institutions of Shenzhen</rs> (No.<rs type="grantNumber">GXWD20201230155427003-20200824155011001</rs>) and <rs type="funder">Strategic Emerging Industry Development Special Funds of Shenzhen</rs>(No.<rs type="grantNumber">JCYJ20200109113441941</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_XSCzF69">
					<idno type="grant-number">62006061</idno>
				</org>
				<org type="funding" xml:id="_JUVEeEH">
					<idno type="grant-number">GXWD20201230155427003-20200824155011001</idno>
				</org>
				<org type="funding" xml:id="_smj7d3K">
					<idno type="grant-number">JCYJ20200109113441941</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Longformer: The long-document transformer</title>
		<author>
			<persName><forename type="first">[</forename><surname>References</surname></persName>
		</author>
		<author>
			<persName><surname>Beltagy</surname></persName>
		</author>
		<idno>CoRR, abs/2004.05150</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Modeling biological processes for reading comprehension</title>
		<author>
			<persName><surname>Berant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014-10">2014. October 2014</date>
			<biblScope unit="page" from="1499" to="1510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Caselli and Vossen, 2017] Tommaso Caselli and Piek Vossen. The event StoryLine corpus: A new benchmark for causal and temporal relation extraction</title>
		<author>
			<persName><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2021. August 2021. August 2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="77" to="86" />
		</imprint>
	</monogr>
	<note>Proceedings of the Events and Stories in the News Workshop</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">ERGO: Event relational graph transformer for document-level event causality identification</title>
		<author>
			<persName><forename type="first">Chen</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Conference on Computational Linguistics</title>
		<meeting>the 29th International Conference on Computational Linguistics<address><addrLine>Gyeongju, Republic of Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022-10">2022. October 2022</date>
			<biblScope unit="page" from="2118" to="2128" />
		</imprint>
	</monogr>
	<note>International Committee on Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><surname>Devlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<editor>
			<persName><forename type="first">Yee</forename><surname>Do</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Dan</forename><surname>Seng Chan</surname></persName>
		</editor>
		<editor>
			<persName><surname>Roth</surname></persName>
		</editor>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota; Scotland, UK.</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011">2019. June 2019. July 2011</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="294" to="303" />
		</imprint>
	</monogr>
	<note>Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing. inburgh. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Modeling document-level causal structures for event causal relation identification</title>
		<author>
			<persName><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Minneapolis, Minnesota; Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016">2019. June 2019. 2019. 2019. August 2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1424" to="1433" />
		</imprint>
	</monogr>
	<note>Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Event causality recognition exploiting multiple annotators&apos; judgments and background knowledge</title>
		<author>
			<persName><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Vancouver, Canada; Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017-08">2017. August 2017. 2019. November 2019. 2017. 2017</date>
			<biblScope unit="page" from="5816" to="5822" />
		</imprint>
	</monogr>
	<note>International Conference on Learning Representations</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Documentlevel event argument extraction by conditional generation</title>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2021-06">2021. June 2021</date>
			<biblScope unit="page" from="894" to="908" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Knowledge enhanced event causality identification with mention masking generalizations</title>
		<author>
			<persName><forename type="first">Lin</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence</title>
		<editor>
			<persName><forename type="first">Christian</forename><surname>Bessiere</surname></persName>
		</editor>
		<meeting>the Twenty-Ninth International Joint Conference on Artificial Intelligence<address><addrLine>Maryland, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Paramita Mirza</publisher>
			<date type="published" when="2014-06">2017. 2017. 2020. 2019. 2019. 2014. June 2014</date>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="10" to="17" />
		</imprint>
	</monogr>
	<note>Proceedings of the ACL 2014 Student Research Workshop. Baltimore. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Phu and Nguyen, 2021] Minh Tran Phu and Thien Huu Nguyen. Graph convolutional networks for event causality identification with rich document-level structures</title>
		<author>
			<persName><surname>Ning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL)</title>
		<meeting>the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL)<address><addrLine>Melbourne, Australia; Philadelphia, PA, U.S.A.</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014">2018. July 2018. 2016. Mar. 2016. 2021. June 2014</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="161" to="170" />
		</imprint>
	</monogr>
	<note>Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Recognizing causality in verb-noun pairs via noun and verb semantics</title>
		<author>
			<persName><forename type="first">Girju</forename><surname>Riaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehwish</forename><surname>Riaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roxana</forename><surname>Girju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ł Ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the EACL 2014 Workshop on Computational Approaches to Causality in Language (CAtoCL)</title>
		<editor>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><surname>Von Luxburg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<meeting>the EACL 2014 Workshop on Computational Approaches to Causality in Language (CAtoCL)<address><addrLine>Gothenburg, Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2008">2014. April 2014. 2008. 2008. 2017</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="48" to="57" />
		</imprint>
	</monogr>
	<note>Advances in Neural Information Processing Systems</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">MAVEN-ERE: A unified large-scale dataset for event coreference, temporal, causal, and subevent relation extraction</title>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
		<idno>CoRR, abs/2211.07342</idno>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">DocRED: A large-scale document-level relation extraction dataset</title>
		<author>
			<persName><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy; Online</addrLine></address></meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2019-07">2019. July 2019. 2021. June 2021</date>
			<biblScope unit="page" from="50" to="61" />
		</imprint>
	</monogr>
	<note>Proceedings of the 2021 Conference of the North American Chapter. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Knowdis: Knowledge enhanced data augmentation for event causality detection via distant supervision</title>
		<author>
			<persName><surname>Zuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics</title>
		<meeting>the 28th International Conference on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="1544" to="1550" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Improving event causality identification via selfsupervised representation learning on external causal statement</title>
		<author>
			<persName><surname>Zuo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2021-08">2021. 2021. August 2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3558" to="3571" />
		</imprint>
	</monogr>
	<note>Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
