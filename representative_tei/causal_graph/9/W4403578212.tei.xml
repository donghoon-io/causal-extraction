<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Causality-Aware Spatiotemporal Graph Neural Networks for Spatiotemporal Time Series Imputation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-10-23">23 Oct 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Baoyu</forename><surname>Jing</surname></persName>
							<email>baoyuj2@illinois.edu</email>
						</author>
						<author>
							<persName><forename type="first">Carl</forename><surname>Yang</surname></persName>
							<email>j.carlyang@emory.edu</email>
						</author>
						<author>
							<persName><forename type="first">Dawei</forename><surname>Zhou</surname></persName>
							<email>zhoud@vt.edu</email>
						</author>
						<author>
							<persName><forename type="first">Kan</forename><surname>Ren</surname></persName>
							<email>kan.ren@microsoft.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois Urbana-Champaign</orgName>
								<address>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Virginia Polytechnic Institute and State University Blacksburg</orgName>
								<address>
									<region>VA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Microsoft Research Asia Shanghai</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Emory University Atlanta</orgName>
								<address>
									<addrLine>CIKM&apos;24 October 21-25</addrLine>
									<postCode>2024</postCode>
									<settlement>Boise</settlement>
									<region>GA ID</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Causality-Aware Spatiotemporal Graph Neural Networks for Spatiotemporal Time Series Imputation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-10-23">23 Oct 2024</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3627673.3679642</idno>
					<idno type="arXiv">arXiv:2403.11960v4[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:25+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Spatiotemporal Time Series Imputation</term>
					<term>Spatiotemporal Graph Neural Network</term>
					<term>Causal Attention</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Spatiotemporal time series are usually collected via monitoring sensors placed at different locations, which usually contain missing values due to various failures, such as mechanical damages and Internet outages. Imputing the missing values is crucial for analyzing time series. When recovering a specific data point, most existing methods consider all the information relevant to that point regardless of the cause-and-effect relationship. During data collection, it is inevitable that some unknown confounders are included, e.g., background noise in time series and non-causal shortcut edges in the constructed sensor network. These confounders could open backdoor paths and establish non-causal correlations between the input and output. Over-exploiting these non-causal correlations could cause overfitting. In this paper, we first revisit spatiotemporal time series imputation from a causal perspective and show how to block the confounders via the frontdoor adjustment. Based on the results of frontdoor adjustment, we introduce a novel Causality-Aware Spatiotemporal Graph Neural Network (Casper), which contains a novel Prompt Based Decoder (PBD) and a Spatiotemporal Causal Attention (SCA). PBD could reduce the impact of confounders and SCA could discover the sparse causal relationships among embeddings. Theoretical analysis reveals that SCA discovers causal relationships based on the values of gradients. We evaluate Casper on three real-world datasets, and the experimental results show that Casper could outperform the baselines and could effectively discover the causal relationships.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS Concepts</head><p>â€¢ Information systems â†’ Data mining; Sensor networks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Spatiotemporal data mining <ref type="bibr" target="#b2">[3]</ref> is the cornerstone of analyzing and understanding the patterns of spacetime and human activities, such as environmental monitoring <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b56">57,</ref><ref type="bibr" target="#b93">94]</ref>, e-business <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b76">77,</ref><ref type="bibr" target="#b79">80,</ref><ref type="bibr" target="#b94">95]</ref> and social science <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b75">76,</ref><ref type="bibr" target="#b78">79,</ref><ref type="bibr" target="#b85">86,</ref><ref type="bibr" target="#b87">88,</ref><ref type="bibr" target="#b89">90,</ref><ref type="bibr" target="#b95">96]</ref>. Time series <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b69">70,</ref><ref type="bibr" target="#b90">91]</ref> is one of the most common data types, which is usually collected by monitoring sensors. For example, traffic flow time series <ref type="bibr" target="#b42">[43]</ref>, e.g., speed, is recorded by the radar sensors on roads. Air pollution time series <ref type="bibr" target="#b93">[94]</ref>, e.g., concentrations of PM2.5, is collected from air quality monitoring sites across cities.</p><p>In the real world, it is not uncommon that the collected spatiotemporal time series is incomplete with missing data due to various failures, e.g., sensors have mechanical damage. The missing data usually significantly impacts the process and conclusion of data analysis. Therefore, how to reconstruct the missing data from the observed data, i.e., imputation, is a fundamental problem of spatiotemporal time series analysis. In recent years, deep learning methods become the mainstream for time series imputation. Most existing deep time series imputation methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b45">46]</ref> use Recurrent Neural Network (RNN) to capture the temporal dynamics of time series and autoregressively recover the missing data by the predicted values. Recent deep learning methods <ref type="bibr" target="#b82">[83]</ref> propose to use non-autoregressive structures, e.g., Transformer <ref type="bibr" target="#b67">[68]</ref>, to avoid the progressive error propagation incurred via the autoregression in RNN by concurrently considering the entire input context. However, these methods only consider the temporal patterns yet overlook the spatial relationships among sensors, e.g., geographical distances. To further account for spatial relationships, graph neural networks <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b68">69,</ref><ref type="bibr" target="#b73">74,</ref><ref type="bibr" target="#b74">75,</ref><ref type="bibr" target="#b86">87,</ref><ref type="bibr" target="#b91">92]</ref> are extended to the spatiotemporal setting <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b69">70]</ref>. Although these methods have achieved impressive performance in recovering the missing values, they tend to include all the available information related to the missing point as references without distinguishing whether there is a causal relationship between them.</p><p>When collecting datasets, it is inevitable to include some unknown confounders <ref type="bibr" target="#b53">[54]</ref>. For example, the background noise might be recorded, and non-causal shortcut edges might be built for two sensors. Let's take the air monitoring sensor network as a concrete example to understand the non-causal edges. A common practice to build the network is adding an edge for two sensors if their distance is below a threshold <ref type="bibr" target="#b49">[50]</ref>. Although simple and usually effective, the distance-based network does not necessarily imply the real causality between sensors. In the real world, air flow between two locations could be influenced by other factors, e.g., wind direction and terrain. An example is shown in Figure <ref type="figure" target="#fig_7">6</ref> in Section 4.4. Simply exploiting the shortcut edges without discovering the causality could make the model overfit the training data and be vulnerable to noise during inference.</p><p>To reduce the negative effects brought by confounders, we first review the process of spatiotemporal time series imputation from a causal perspective <ref type="bibr" target="#b53">[54]</ref> to show the causal relationships among the input, output, embeddings, and confounders. The results show that confounders could establish undesired non-causal shortcut backdoor paths between the input and output. Then, we show how to eliminate the backdoor paths via the frontdoor adjustment <ref type="bibr" target="#b53">[54]</ref>. Based on the results of the frontdoor adjustment, we introduce a novel Causality-Aware Spatiotemporal Graph Neural Network (Casper), which is equipped with a novel Prompt Based Decoder (PBD) and a Spatiotemporal Causal Attention (SCA). The proposed PBD effectively reduces the impact of unknown confounders by injecting the global context information of datasets into the embeddings. Inspired by <ref type="bibr" target="#b25">[26]</ref>, which uses learnable prompts to capture the contextual information of downstream tasks when tuning visual models, PBD leverages prompts to learn the contextual information of datasets automatically rather than employing external models to approximate the context. To further enforce sparse causality between embeddings, we introduce SCA, which determines the cause-and-effect relationship via a causal gate. It can be theoretically proven that the proposed causal gate (1) enforces the sparsity since it converges to 0 or 1; (2) is a gradient-based explanation similar to <ref type="bibr" target="#b60">[61]</ref>, which determines the causality based on the values of gradients. We extensively evaluate Casper on three real-world datasets. The experimental results show that Casper could significantly outperform baselines and could effectively discover causality.</p><p>The major contributions of the paper are summarized as follows:</p><p>â€¢ We review the spatiotemporal time series imputation task from a causal perspective, where we show the problems of the undesired confounders. Then, we show how to eliminate confounders via the frontdoor adjustment. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminary</head><p>In this section, we briefly introduce the definitions of spatiotemporal time series and spatiotemporal time series imputation. We also review the definitions of Granger causality and attention function.</p><p>Definition 1 (Incomplete Spatiotemporal Time Series). We denote an incomplete spatiotemporal time series with missing values as G = (X, A, M), where X âˆˆ R ğ‘ Ã—ğ‘‡ is the multivariate time series collected from ğ‘ sensors with totally ğ‘‡ steps, A âˆˆ R ğ‘ Ã—ğ‘ is the adjacency matrix of the sensor network, M âˆˆ {0, 1} ğ‘ Ã—ğ‘‡ is the binary mask and 0/1 denotes the absence/presence of a data point.</p><p>Definition 2 (Spatiotemporal Time Series Imputation). Given an incomplete spatiotemporal time series G = (X, A, M), we denote Y âˆˆ R ğ‘ Ã—ğ‘‡ as the complete time series of X, such that X = M âŠ™ Y, where âŠ™ is the Hadamard product. The task is to build a function Å¶ = ğ‘“ (G) to minimize the reconstruction error, e.g., Mean Absolute Error (MAE), between Å¶ and Y.</p><p>Definition 3 (Granger Causality <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b18">19]</ref>). Let X âˆˆ R ğ‘ Ã—ğ‘‡ be the values of past ğ‘‡ steps of ğ‘ time series, and Å·ğ‘–,ğ‘‡ +1 = ğ‘“ ğ‘– (X) âˆˆ R be the time series forecasting function predicting the future value of the ğ‘–-th time series at the ğ‘‡ +1 step, where ğ‘– âˆˆ {1, . . . , ğ‘ }. The ğ‘– â€² -th time series is said to Granger cause the ğ‘–-th time series if there exists a point</p><formula xml:id="formula_0">ğ‘¥ â€² ğ‘– â€² ,ğ‘¡ â€² â‰  ğ‘¥ ğ‘– â€² ,ğ‘¡ â€² , ğ‘¡ â€² âˆˆ {1, . . . ,ğ‘‡ }, such that ğ‘“ ğ‘– (X â€² ) â‰  ğ‘“ ğ‘– (X), where X â€² is obtained by replacing ğ‘¥ ğ‘– â€² ,ğ‘¡ â€² in X with ğ‘¥ â€² ğ‘– â€² ,ğ‘¡ â€² . Generally, if ğ‘¥ ğ‘– â€² ,ğ‘¡ â€² impacts</formula><p>the prediction of the future value of the ğ‘–-th time series, then the ğ‘– â€² -th time series Granger causes the ğ‘–-th time series. In the case that ğ‘“ ğ‘– is a linear model:</p><formula xml:id="formula_1">Å·ğ‘–,ğ‘‡ +1 = ğ‘“ ğ‘– (X) = ğ‘ ,ğ‘‡ âˆ‘ï¸ ğ‘– â€² =1,ğ‘¡ â€² =1 ğ›¼ ğ‘– â€² ,ğ‘¡ â€² ğ‘¥ ğ‘– â€² ,ğ‘¡ â€² . (<label>1</label></formula><formula xml:id="formula_2">)</formula><p>if the coefficient ğ›¼ ğ‘– â€² ,ğ‘¡ â€² â‰  0, then the ğ‘– â€² -th time series Granger causes the ğ‘–-th time series.</p><p>Definition 4 (Attention Function). Let q and {k ğ‘– } ğ‘ ğ‘–=1 be the ğ‘‘dimensional query and key embeddings, let ğ‘  and ğ‘“ ğ‘£ be the scoring and message functions, then the attention function is defined as:</p><formula xml:id="formula_3">h = ğ‘ âˆ‘ï¸ ğ‘–=1 ğ›¼ ğ‘– v ğ‘– , ğ›¼ ğ‘– = exp(ğ‘  (q, k ğ‘– )) ğ‘ ğ‘– â€² =1 exp(ğ‘  (q, k ğ‘– â€² )) , v ğ‘– = ğ‘“ ğ‘£ (q, k ğ‘– ),<label>(2)</label></formula><p>where h âˆˆ R ğ‘‘ is the output, and v ğ‘– is the message from k ğ‘– to q.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>In this section, we first provide a causal view of the spatiotemporal imputation task, and show how to eliminate the impact of unknown confounds by frontdoor adjustment, based on which, we introduce a novel Causality-Aware Spatiotemporal Graph Neural Network (Casper). Finally, we provide further analysis of Casper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Causal View of Spatiotemporal Imputation</head><p>Given an incomplete spatiotemporal time series G = (X, A, M), a standard deep imputation model ğ‘“ = ğ‘“ ğ·  a reconstruction error e.g., MAE or RMSE. Since minimizing MAE (or RMSE) is equivalent to maximizing the log-likelihood of Laplace (or Gaussian) distribution <ref type="bibr" target="#b21">[22]</ref>, and thus we can view the objective of spatiotemporal imputation as maximizing ğ‘ƒ (Y|G). Most existing studies focus on maximizing ğ‘ƒ (Y|G) yet few discuss the cause-andeffect relationship between G and Y. In this paper, we study their causality based on the Structure Causal Model (SCM) <ref type="bibr" target="#b53">[54]</ref>.</p><p>Structure Causal Model. During data collection, it is inevitable that some unknown confounders C are included in datasets, which influence both G and Y. For example, sensors might record random background noise, and the constructed sensor network might contain shortcut edges. The undesired information might bridge the input G and the output Y with spurious correlations, which could lead to overfitting and make the model error-prone.</p><p>The causal relationship between G, H , Y and C can be modeled by the Structure Causal Model (SCM) <ref type="bibr" target="#b53">[54]</ref> shown in Figure <ref type="figure" target="#fig_0">1</ref>. First, it is evident that C and H are not d-separable <ref type="bibr" target="#b51">[52]</ref> The model ğ‘“ might take advantage of the backdoor paths to make decisions instead of struggling to discover the real cause-and-effect relationships <ref type="bibr" target="#b64">[65,</ref><ref type="bibr" target="#b80">81]</ref>. Our goal is to eliminate the backdoor paths.</p><p>Frontdoor Adjustment. In statistics <ref type="bibr" target="#b51">[52]</ref>, a simple way to exclude the variable C in the SCM in Figure <ref type="figure" target="#fig_0">1</ref> is to marginalize it out. However, marginalization requires C to be observable and measured by the marginal distribution ğ‘ƒ (C), but in spatiotemporal imputation, C is usually unknown and difficult to measure. Rather than directly marginalizing C out, we resort to the frontdoor adjustment <ref type="bibr" target="#b53">[54]</ref>, which uses Pearl's do-calculus <ref type="bibr" target="#b53">[54]</ref> to block the backdoor paths. We follow the three steps of the frontdoor adjustment as follows.</p><p>(1) Remove the backdoor path from G to H . Given G, there is no backdoor path from G to H . Note that G cannot reach H via G â† C â†’ Y â† H according to the d-separation theory <ref type="bibr" target="#b51">[52]</ref>. Therefore, we have:</p><formula xml:id="formula_4">ğ‘ƒ (H |ğ‘‘ğ‘œ (G)) = ğ‘ƒ (H |G).<label>(3)</label></formula><p>(2) Remove the backdoor path from H to Y. There is a backdoor path between H and Y: H â† G â† C â†’ Y. This backdoor path can be blocked by marginalizing out G: (3) Combine the results of the above two steps:</p><formula xml:id="formula_5">ğ‘ƒ (Y|ğ‘‘ğ‘œ (H )) = âˆ‘ï¸ G ğ‘ƒ (Y|H, G ğ‘€ )ğ‘ƒ (G).<label>(4)</label></formula><formula xml:id="formula_6">ğ‘ƒ (Y|ğ‘‘ğ‘œ (G)) = âˆ‘ï¸ ğ‘–,ğ‘¡ ğ‘ƒ (h ğ‘–,ğ‘— |ğ‘‘ğ‘œ (G))ğ‘ƒ (Y|ğ‘‘ğ‘œ (h ğ‘–,ğ‘¡ )) = âˆ‘ï¸ ğ‘–,ğ‘¡ ğ‘ƒ (h ğ‘–,ğ‘¡ |G) âˆ‘ï¸ G ğ‘ƒ (Y|h ğ‘–,ğ‘¡ , G)ğ‘ƒ (G).<label>(5)</label></formula><p>In general, ğ‘ƒ (h ğ‘–,ğ‘¡ |G) can be viewed as the encoder ğ‘“ ğ¸ , and the rest part, including G ğ‘ƒ (Y|h ğ‘–,ğ‘¡ , G)ğ‘ƒ (G) and the sum over all data points ğ‘–,ğ‘¡ , can be viewed as the decoder ğ‘“ ğ· . In the next subsection, we show how to implement Equation (5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Architecture of Casper</head><p>In this subsection, based on Equation ( <ref type="formula" target="#formula_6">5</ref>), we propose a novel Cusality-Aware Spatiotemporal Graph Neural Network (Casper). We first present an overview of Casper, consisting of a Prompt Based Decoder (PBD) and an encoder with Spatiotemporal Causal Attention (SCA). Next, we elaborate PBD and SCA in detail.</p><p>Overview. Figure <ref type="figure" target="#fig_2">2</ref> shows an overview of Casper. The encoder ğ‘“ ğ¸ is comprised of an input project and ğ¿ layers of the combination of skip project, transformer, SCA and add &amp; norm. Let m âˆˆ R ğ‘‘ be the embedding for the missing points. The input project module encodes the raw input G = (X, A, M) into H (0) via:</p><formula xml:id="formula_7">H (0) = MLP(X) âŠ™ M + m âŠ™ (1 -M),<label>(6)</label></formula><p>where MLP stands for Multi-Layer Perceptron and âŠ™ denotes the Hadamard product. The skip project module prevents gradient vanishing and improves the performance by injecting the G into the embeddings from the previous layer H (ğ‘™ -1) :</p><formula xml:id="formula_8">H ğ‘ ğ‘˜ğ‘–ğ‘ = H (ğ‘™ -1) + MLP(X) âŠ™ M + m âŠ™ (1 -M).<label>(7)</label></formula><p>The transformer encoder layer <ref type="bibr" target="#b67">[68]</ref> learns temporal information for each time series within H ğ‘ ğ‘˜ğ‘–ğ‘ :</p><formula xml:id="formula_9">H ğ‘–ğ‘› = Transformer(H ğ‘ ğ‘˜ğ‘–ğ‘ ).<label>(8)</label></formula><p>SCA discovers spatiotemporal causal relationships among embeddings based on A, and encodes causal information into embeddings:</p><formula xml:id="formula_10">H ğ‘œğ‘¢ğ‘¡ = SCA(H ğ‘–ğ‘› , A).<label>(9)</label></formula><p>The final embeddings of the ğ‘™-th layer are given by: Given the embeddings H = H (ğ¿) obtained by the encoder ğ‘“ ğ¸ , the PBD module in ğ‘“ ğ· generates the predictions Å¶ = { Å·ğ‘–,ğ‘¡ } ğ‘ ,ğ‘‡ ğ‘–=1,ğ‘¡ =1 : Å¶ = PBD(H). ( <ref type="formula">11</ref>)</p><formula xml:id="formula_11">H (ğ‘™ ) = LayerNorm(H ğ‘–ğ‘› + H ğ‘œğ‘¢ğ‘¡ ).<label>(10)</label></formula><p>Prompt Based Decoder. Suppose we are given the input G and the target is to recover Y = ğ‘¦ ğ‘–,ğ‘¡ . In Equation ( <ref type="formula" target="#formula_6">5</ref>), the decoder ğ‘“ ğ· is comprised of (1) a sum over all possible G â€² in the dataset</p><formula xml:id="formula_12">G â€² ğ‘ƒ (ğ‘¦ ğ‘–,ğ‘¡ |h ğ‘– â€² ,ğ‘¡ â€² , G â€² )ğ‘ƒ (G â€²</formula><p>) and ( <ref type="formula" target="#formula_3">2</ref>) a sum over all data points ğ‘– â€² ,ğ‘¡ â€² in G. For (2), since the encoders nowadays, e.g., Transformer <ref type="bibr" target="#b67">[68]</ref>, are so powerful that could encode sufficient context information G in h ğ‘–,ğ‘¡ , and thus the decoder ğ‘“ ğ· could only take h ğ‘–,ğ‘¡ as input <ref type="bibr" target="#b49">[50]</ref>, instead of all possible h ğ‘– â€² ,ğ‘¡ â€² . Therefore, we could drop ğ‘– â€² ,ğ‘¡ â€² and only implement</p><formula xml:id="formula_13">G â€² ğ‘ƒ (ğ‘¦ ğ‘–,ğ‘¡ |h ğ‘–,ğ‘¡ , G â€² )ğ‘ƒ (G â€² ).</formula><p>Now, the challenge is how to implement the sum over G â€² . Simple solutions include randomly sampling a set from the training data, or clustering G â€² into ğ¾ clusters and using the cluster centers as an approximation. However, random sampling could be unstable in practice, and clustering requires extra pre-trained models to extract embeddings of G â€² in advance. Inspired by <ref type="bibr" target="#b25">[26]</ref>, which uses prompts to capture the context information of the downstream task, we introduce a Prompt Based Decoder (PBD) to automatically capture the global context information of the dataset during model training.</p><p>An illustration of the proposed PBD is shown in Figure <ref type="figure" target="#fig_3">3</ref>.</p><formula xml:id="formula_14">Q = h ğ‘–,ğ‘¡ is the query, P = {p ğ‘› } ğ‘ ğ‘ƒ</formula><p>ğ‘›=1 is the set of learnable prompts, which are randomly initialized embedding vectors. In Figure <ref type="figure" target="#fig_3">3</ref>, the Project is a linear function followed with a LayerNorm <ref type="bibr" target="#b3">[4]</ref>. Details of the scaled dot-product attention can be found in <ref type="bibr" target="#b67">[68]</ref>, and it can be easily extended into the multi-head version as in <ref type="bibr" target="#b67">[68]</ref>.</p><p>Spatiotemporal Causal Attention. Attention functions (Definition 4) have become indispensable in deep learning models <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b67">68]</ref>, which could effectively capture the context information for the target embedding. Although attention scores could show the correlation between embeddings, correlation does not necessarily imply causality and thus sometimes could induce undesired non-causal information into embeddings <ref type="bibr" target="#b64">[65]</ref>. Based on the frontdoor adjustment, PBD could eliminate the impact of unknown confounders C and ensure the causality between the input G and the output Y by summing over G, ğ‘– and ğ‘¡. However, it guides the attention functions to discover the causal relationships at a high level, and thus the learned causal relationships, i.e., attention scores, might still be dense and a little bit difficult to interpret (see Figuers 5c and 5g). To directly guide the model to discover the sparse causal relationships, ğ‘–,ğ‘¡ , then h ğ‘–ğ‘› ğ‘– â€² ,ğ‘¡ â€² Granger causes h ğ‘œğ‘¢ğ‘¡ ğ‘–,ğ‘¡ . We do not strictly enforce the time ğ‘¡ â€² â‰¤ ğ‘¡ for the imputation task as for the forecasting task (Definition 3), since (1) a missing value could appear at the beginning of the input time series segment, and there are no prior points available; (2) most imputation methods in the literature consider both past ğ‘¡ â€² â‰¤ ğ‘¡ and future ğ‘¡ â€² &gt; ğ‘¡ reference points h ğ‘– â€² ,ğ‘¡ â€² for the data point to be imputed h ğ‘–,ğ‘¡ ; (3) given the learned weight ğ‘¤ ğ‘– â€² ,ğ‘¡ â€² between h ğ‘– â€² ,ğ‘¡ â€² and h ğ‘–,ğ‘¡ , it is easy to distinguish whether it is from the past or future by comparing ğ‘¡ â€² and ğ‘¡, and thus we can easily obtain the time-constrained causal graph if necessary.</p><p>Let ğ‘“ ğ‘–,ğ‘¡ be an attention function as shown in Definition 4:</p><formula xml:id="formula_15">h ğ‘œğ‘¢ğ‘¡ ğ‘–,ğ‘¡ = ğ‘ ,ğ‘‡ âˆ‘ï¸ ğ‘– â€² =1,ğ‘¡ â€² =1 ğ›¼ ğ‘– â€² ,ğ‘¡ â€² v ğ‘– â€² ,ğ‘¡ â€² , v ğ‘– â€² ,ğ‘¡ â€² = ğ‘“ ğ‘£ (h ğ‘–ğ‘› ğ‘–,ğ‘¡ , h ğ‘–ğ‘› ğ‘– â€² ğ‘¡ â€² ),<label>(12)</label></formula><p>where ğ›¼ ğ‘– â€² ,ğ‘¡ â€² is the attention weight, and ğ‘“ ğ‘£ is the message function.</p><p>According to Definition 5, if ğ›¼ ğ‘– â€² ,ğ‘¡ â€² â‰  0, then h ğ‘–ğ‘› ğ‘– â€² ,ğ‘¡ â€² Granger causes h ğ‘œğ‘¢ğ‘¡ ğ‘–,ğ‘¡ ; otherwise h ğ‘–ğ‘› ğ‘– â€² ,ğ‘¡ â€² does not Granger cause h ğ‘œğ‘¢ğ‘¡ ğ‘–,ğ‘¡ . In practice, without directly manipulating, ğ›¼ ğ‘– â€² ,ğ‘¡ â€² &gt; 0 holds for many noisy messages v ğ‘– â€² ,ğ‘¡ â€² , as shown in Figure <ref type="figure" target="#fig_5">5c</ref>-5d in our experiments. To further enforce the weights of the noisy points to be zero and discover Granger causality, we propose a novel SCA. As shown in Figure <ref type="figure" target="#fig_4">4</ref>, SCA is comprised of two components: (1) a spatiotemporal graph attention function (orange), which learns the correlation between embeddings; (2) a causal gate (blue), which discovers the causal and non-causal relationships.</p><p>Let Q = h ğ‘–ğ‘› ğ‘–,ğ‘¡ be the target query embedding, and N (ğ‘–) be the neighbors of the ğ‘–-th sensor, i.e., A[ğ‘–, ğ‘– â€² ] â‰  0, âˆ€ğ‘– â€² âˆˆ N (ğ‘–). Denote the context keys as K = {h ğ‘–ğ‘› ğ‘– â€² ,ğ‘¡ â€² } ğ‘– â€² ,ğ‘¡ â€² and the message values from N (ğ‘–) to the point (ğ‘–, ğ‘¡) as</p><formula xml:id="formula_16">V = {v ğ‘– â€² ,ğ‘¡ â€² } ğ‘– â€² ,ğ‘¡ â€² , where ğ‘– â€² âˆˆ N (ğ‘–), ğ‘¡ â€² âˆˆ {1, â€¢ â€¢ â€¢ ,ğ‘‡ } and v ğ‘– â€² ,ğ‘¡ â€² = MLP([h ğ‘–ğ‘› ğ‘–,ğ‘¡ ; h ğ‘–ğ‘› ğ‘– â€² ,ğ‘¡ â€² ]</formula><p>). We define SCA as:</p><formula xml:id="formula_17">h ğ‘œğ‘¢ğ‘¡ ğ‘–,ğ‘¡ = 1 ğ‘ âˆ‘ï¸ ğ‘– â€² âˆˆ N (ğ‘– ) ğ‘‡ âˆ‘ï¸ ğ‘¡ â€² =1 ğ›½ ğ‘– â€² ,ğ‘¡ â€² ğ›¼ ğ‘– â€² ,ğ‘¡ â€² v ğ‘– â€² ,ğ‘¡ â€² ,<label>(13)</label></formula><p>where </p><formula xml:id="formula_18">ğ‘ âˆˆ R is a normalization factor, ğ›¼ ğ‘– â€² ,ğ‘¡ â€² is the correlation weight, ğ›½ ğ‘– â€² ,ğ‘¡ â€² âˆ¼ Bernoulli(ğœŒ ğ‘– â€² ,ğ‘¡ â€² )</formula><formula xml:id="formula_19">ğ›¼ ğ‘– â€² ğ‘¡ â€² = exp(ğ‘  (h ğ‘–,ğ‘¡ ; h ğ‘– â€² ,ğ‘¡ â€² )) ğ‘— âˆˆ N (ğ‘– ) ğ‘‡ ğ‘Ÿ =1 exp(ğ‘  (h ğ‘–,ğ‘¡ ; h ğ‘—,ğ‘Ÿ )) ,<label>(14)</label></formula><formula xml:id="formula_20">ğ‘  (h ğ‘–ğ‘› ğ‘–,ğ‘¡ ; h ğ‘–ğ‘› ğ‘– â€² ,ğ‘¡ â€² ) = (W ğ‘„ h ğ‘–ğ‘› ğ‘–,ğ‘¡ ) ğ‘‡ (W ğ¾ h ğ‘–ğ‘› ğ‘– â€² ,ğ‘¡ â€² )/ âˆš ğ‘‘,<label>(15)</label></formula><p>where W ğ‘„ , W ğ¾ âˆˆ R ğ‘‘ Ã—ğ‘‘ are learnable weights, and ğ‘‘ is the size of the hidden dimension. We build a neural network to learn the probability ğœŒ ğ‘– â€² ,ğ‘¡ â€² of the causal gate ğ›½ ğ‘– â€² ,ğ‘¡ â€² :</p><formula xml:id="formula_21">ğœŒ ğ‘– â€² ,ğ‘¡ â€² = ğœ (W ğ‘ [W ğ‘„ ğ‘ h ğ‘–ğ‘› ğ‘–,ğ‘¡ ; W ğ¾ ğ‘ h ğ‘–ğ‘› ğ‘– â€² ,ğ‘¡ â€² ]),<label>(16)</label></formula><p>where W ğ‘ âˆˆ R 1Ã—2ğ‘‘ , W ğ‘„ ğ‘ , W ğ¾ ğ‘ âˆˆ R ğ‘‘ Ã—ğ‘‘ are learnable weights, and ğœ is the Sigmoid activation function.</p><p>There are two practical issues of directly using ğœŒ in the above equation. First, the sampling operation ğ›½ ğ‘– â€² ,ğ‘¡ â€² âˆ¼ Bernoulli(ğœŒ ğ‘– â€² ,ğ‘¡ â€² ) is in-differentiable. To address this issue, we use the differentiable reparameterization technique Gumbel-Softmax <ref type="bibr" target="#b24">[25]</ref> to obtain ğ›½ ğ‘– â€² ,ğ‘¡ â€² :</p><formula xml:id="formula_22">ğ›½ ğ‘– â€² ,ğ‘¡ â€² = exp((log ğœŒ ğ‘– â€² ,ğ‘¡ â€² + ğ‘”)/ğœ) exp((log ğœŒ ğ‘– â€² ,ğ‘¡ â€² + ğ‘”)/ğœ) + exp((log(1 -ğœŒ ğ‘– â€² ,ğ‘¡ â€² ) + ğ‘”)/ğœ) ,<label>(17)</label></formula><p>where ğ‘” =log(-log(ğ‘¢)), ğ‘¢ âˆ¼ Uniform(0, 1), and ğœ is the temperature parameter. Second, if ğœŒ ğ‘– â€² ,ğ‘¡ â€² is not close to 0 or 1, the model's decision could be ambiguous during inference. For example, if ğœŒ ğ‘– â€² ,ğ‘¡ â€² = 0.2, then for the same input data, for 20% time, the model shows h ğ‘–ğ‘› ğ‘– â€² ,ğ‘¡ â€² Granger causes h ğ‘œğ‘¢ğ‘¡ ğ‘–,ğ‘¡ , and for the other 80% time, the model shows h ğ‘–ğ‘› ğ‘– â€² ,ğ‘¡ â€² does not Granger cause h ğ‘œğ‘¢ğ‘¡ ğ‘–,ğ‘¡ . To avoid such an ambiguous situation, we enforce ğœŒ ğ‘– â€² ,ğ‘¡ â€² â†’ 0/1 by placing the ğ‘™ 1 regularization over ğœŒ ğ‘– â€² ,ğ‘¡ â€² .</p><p>It can be theoretically proven that ğœŒ ğ‘– â€² ,ğ‘¡ â€² will converge to 0 or 1 (see Section 3.3). Additionally, in practice, the correlation weight ğ›¼ can be easily extended to the multi-head version as in <ref type="bibr" target="#b67">[68]</ref>.</p><p>Loss Function. Casper is trained by the masked MAE. For a given spatiotemporal time series segment with ğ‘ nodes and ğ‘‡ length, the loss is defined as:</p><formula xml:id="formula_23">L = ğ‘ ,ğ‘‡ âˆ‘ï¸ ğ‘–=1,ğ‘¡ =1 ğ‘š ğ‘–,ğ‘¡ â€¢ |ğ‘¦ ğ‘–,ğ‘¡ -Å·ğ‘–,ğ‘¡ | + ğœ†||Î¦|| 1 ,<label>(18)</label></formula><p>where ğ‘š ğ‘–,ğ‘¡ is the mask, ğ‘¦ ğ‘–,ğ‘¡ is the ground-truth value, Å·ğ‘–,ğ‘¡ is the predicted value, Î¦ is the set of all ğœŒ in SCA, ğœ† is a tunable coefficient, and || â€¢ || 1 denotes the ğ‘™ 1 norm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Framework Analysis</head><p>In this subsection, we provide further analysis of the proposed Casper, including theoretical analysis and complexity analysis.</p><p>Theoretical Analysis. We theoretically prove that ğœŒ ğ‘– â€² ,ğ‘¡ â€² in Equation <ref type="bibr" target="#b15">(16)</ref> will converge to 0 or 1 in Theorem 1, and thus ğœŒ ğ‘– â€² ,ğ‘¡ â€² indicates the Granger causality. If ğœŒ ğ‘– â€² ,ğ‘¡ â€² = 0, then ğ›½ ğ‘– â€² ,ğ‘¡ â€² â€¢ ğ›¼ ğ‘– â€² ,ğ‘¡ â€² = 0, showing that h ğ‘–ğ‘› ğ‘– â€² ,ğ‘¡ â€² does not Granger cause h ğ‘œğ‘¢ğ‘¡ ğ‘–,ğ‘¡ . Moreover, from the proof of Theorem 1, it can be observed that ğœŒ ğ‘– â€² ,ğ‘¡ â€² is actually a gradient-based explanation (see Remark 1), which determines causal and non-causal relationships based on the gradients. Compared with the classic gradient-based explanation methods <ref type="bibr" target="#b60">[61]</ref>, which needs extra steps to calculate gradients after the model is trained, the proposed ğœŒ has two advantages: (1) ğœŒ does not require extra steps for calculating derivatives, and the value of ğœŒ could directly provide the explanation; (2) the parameters associated with ğœŒ are jointly trained with the model, and thus it can guide the model to focus on the most important relationships during training.</p><p>Theorem 1 (Convergence of ğœŒ). ğœŒ could converge to 0 or 1 by updating its parameters based on L in Equation <ref type="bibr" target="#b17">(18)</ref>.</p><p>Proof Sketch. For simplicity, let's only consider the loss for a single point (ğ‘–, ğ‘¡), where ğ‘š ğ‘–,ğ‘¡ = 1 and ğ‘¦ ğ‘–,ğ‘¡ -Å·ğ‘–,ğ‘¡ &gt; 0:</p><formula xml:id="formula_24">L ğ‘–,ğ‘¡ = ğ‘¦ ğ‘–,ğ‘¡ -Å·ğ‘–,ğ‘¡ + ğœ†||Î¦|| 1 (<label>19</label></formula><formula xml:id="formula_25">)</formula><p>Since Equation ( <ref type="formula" target="#formula_21">16</ref>) is essentially a linear function with a Sigmoid activation, we can rewrite it as:</p><formula xml:id="formula_26">ğœŒ ğ‘– â€² ,ğ‘¡ â€² = ğœ (w ğ‘‡ h ğ‘–ğ‘› ), h ğ‘–ğ‘› = [h ğ‘–ğ‘› ğ‘–,ğ‘¡ ; h ğ‘–ğ‘› ğ‘– â€² ,ğ‘¡ â€² ], w âˆˆ R 2ğ‘‘<label>(20)</label></formula><p>Let ğ‘¤ ğ‘— = w[ ğ‘—] and â„ ğ‘–ğ‘› ğ‘— = h ğ‘–ğ‘› [ ğ‘—], then the gradient at ğ‘¤ ğ‘— is:</p><formula xml:id="formula_27">ğœ•L ğ‘–,ğ‘¡ ğœ•ğ‘¤ ğ‘— = ğœ•ğ‘¦ ğ‘–,ğ‘¡ -Å·ğ‘–,ğ‘¡ ğœ•h ğ‘œğ‘¢ğ‘¡ ğ‘–,ğ‘¡ â€¢ ğœ•h ğ‘œğ‘¢ğ‘¡ ğ‘–,ğ‘¡ ğœ•ğ›½ ğ‘– â€² ,ğ‘¡ â€² â€¢ ğœ•ğ›½ ğ‘– â€² ,ğ‘¡ â€² ğœ•ğœŒ ğ‘– â€² ,ğ‘¡ â€² â€¢ ğœ•ğœŒ ğ‘– â€² ,ğ‘¡ â€² ğœ•ğ‘¤ ğ‘— + ğœ† ğœ•ğœŒ ğ‘– â€² ,ğ‘¡ â€² ğ‘¤ ğ‘— = (ğœ† -ğ‘”)ğœŒ ğ‘– â€² ,ğ‘¡ â€² (1 -ğœŒ ğ‘– â€² ,ğ‘¡ â€² )â„ ğ‘–ğ‘› ğ‘— (21) ğ‘” = ğœ• Å·ğ‘–,ğ‘¡ ğœ•h ğ‘œğ‘¢ğ‘¡ ğ‘–,ğ‘¡ â€¢ ğœ•h ğ‘œğ‘¢ğ‘¡ ğ‘–,ğ‘¡ ğœ•ğ›½ ğ‘– â€² ,ğ‘¡ â€² â€¢ ğœ•ğ›½ ğ‘– â€² ,ğ‘¡ â€² ğœ•ğœŒ ğ‘– â€² ,ğ‘¡ â€² = ğœ• Å·ğ‘–,ğ‘¡ ğœ•h ğ‘œğ‘¢ğ‘¡ ğ‘–,ğ‘¡ â€¢ ğ›¼ ğ‘– â€² ,ğ‘¡ â€² ğ‘ v ğ‘– â€² ,ğ‘¡ â€² â€¢ 1 (22)</formula><p>where h ğ‘œğ‘¢ğ‘¡ ğ‘–,ğ‘¡ , ğ›½ ğ‘– â€² ,ğ‘¡ â€² are from Equation <ref type="bibr" target="#b16">(17)</ref>. In gradient descent, the updating function of ğ‘¤ ğ‘— is:</p><formula xml:id="formula_28">ğ‘¤ (ğ‘˜+1) ğ‘— = ğ‘¤ (ğ‘˜ ) ğ‘— -ğœ‚ (ğœ† -ğ‘”)ğœŒ (ğ‘˜ ) ğ‘– â€² ,ğ‘¡ â€² (1 -ğœŒ (ğ‘˜ ) ğ‘– â€² ,ğ‘¡ â€² )â„ ğ‘–ğ‘› ğ‘— (<label>23</label></formula><formula xml:id="formula_29">)</formula><p>where ğ‘˜ is the iteration index and ğœ‚ &gt; 0 is the learning rate. As we only consider the parameters w of ğœŒ, for simplicity, let's fix all other parameters in the model. Therefore, ğœ• Å·ğ‘–,ğ‘¡ /ğœ•h ğ‘œğ‘¢ğ‘¡ ğ‘–,ğ‘¡ , v ğ‘– â€² ,ğ‘¡ â€² , ğ›¼ ğ‘– â€² ,ğ‘¡ â€² are fixed. The normalization factor ğ‘ â‰¤ 1, and ğ‘ = 1 if and only if</p><formula xml:id="formula_30">ğ›½ ğ‘– â€² ,ğ‘¡ â€² = 1 for âˆ€ğ‘– â€² , ğ‘¡ â€² . ğœŒ ğ‘– â€² ,ğ‘¡ â€² (1 -ğœŒ ğ‘– â€² ,ğ‘¡ â€² ) &gt; 0 since ğœŒ ğ‘– â€² ,ğ‘¡ â€² âˆˆ (0, 1). Now, suppose ğ‘” &gt; ğœ†, if â„ ğ‘–ğ‘› ğ‘— &gt; 0, then ğ‘¤ (ğ‘˜+1) ğ‘— &gt; ğ‘¤ (ğ‘˜ ) ğ‘— . When ğ‘˜ â†’ âˆ, ğ‘¤ (ğ‘˜ ) ğ‘— â†’ +âˆ. Otherwise, if â„ ğ‘–ğ‘› ğ‘— &lt; 0, then ğ‘¤ (ğ‘˜+1) ğ‘— &lt; ğ‘¤ (ğ‘˜ ) ğ‘— . When ğ‘˜ â†’ âˆ, ğ‘¤ (ğ‘˜ ) ğ‘— â†’ -âˆ. Therefore, when ğ‘˜ â†’ âˆ, w ğ‘‡ h â†’ +âˆ. As a result, ğœŒ (ğ‘˜ ) ğ‘– â€² ,ğ‘¡ â€² = ğœ (w (ğ‘˜ ) ğ‘‡ h ğ‘–ğ‘› ) â†’ 1, as ğ‘˜ â†’ âˆ. Similarly, if ğ‘” &lt; ğœ†, then ğœŒ (ğ‘˜ )</formula><p>ğ‘– â€² ,ğ‘¡ â€² will converge to 0. â–¡ Remark 1 (ğœŒ is a gradient based explanation). From the proof of Theorem 1, it can be noted that, if ğ‘” &gt; ğœ†, then ğœŒ ğ‘– â€² ,ğ‘¡ â€² â†’ 1; if ğ‘” &lt; ğœ†, then ğœŒ ğ‘– â€² ,ğ‘¡ â€² â†’ 0. This phenomenon reflects that ğœŒ ğ‘– â€² ,ğ‘¡ â€² serves as a binary indicator showing whether the gradient at ğœŒ ğ‘– â€² ,ğ‘¡ â€² is greater or less than the threshold ğœ†.</p><p>Complexity Analysis. The complexities of the input project, the skip project, and the transformer layer are ğ‘‚ (ğ‘ğ‘‡ ), ğ‘‚ (ğ‘ğ‘‡ ), and ğ‘‚ (ğ‘ğ‘‡ 2 ). The complexities of SCA and PBD are ğ‘‚ (ğ¸ğ‘‡ ) and ğ‘‚ (ğ‘ ğ‘ ğ‘ƒ ğ‘‡ ), where ğ¸ is the number of edges in the sensor network, and ğ‘ ğ‘ƒ is the number of prompts. The overall complexity is ğ‘‚ (max(ğ¸, ğ‘ğ‘‡ , ğ‘ ğ‘ ğ‘ƒ )ğ‘‡ ).</p><p>In SCA, if we calculate attention weights and causal gates for each pair of the data points without using A, then the complexity will be extremely high: ğ‘‚ (ğ‘ 2 ğ‘‡ 2 ) â‰« ğ‘‚ (ğ¸ğ‘‡ ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments 4.1 Experimental Setup</head><p>In this subsection, we briefly explain the datasets, evaluation metrics, and baselines used for the experiments.</p><p>Datasets. Three public real-world benchmark datasets are used to evaluate Casper. AQI <ref type="bibr" target="#b93">[94]</ref> is an hourly record of air pollutants from 437 air quality monitoring stations in China from May 2014 to April 2015. We also use the popular AQI-36 <ref type="bibr" target="#b6">[7]</ref> which is a reduced version of the full AQI containing records from 36 sensor stations scattered around Beijing. METR-LA <ref type="bibr" target="#b42">[43]</ref> contains traffic speed time series collected from 207 sensors on highways in Los Angeles for 4 months. PEMS-BAY <ref type="bibr" target="#b42">[43]</ref> is a traffic speed time series collected from 325 sensors on highways in San Francisco Bay Area for 6 months. The time series records in METR-LA and PEMS-BAY are collected every 5 minutes. For AQI, METR-LA, and PEMS-BAY, the temporal window is set as ğ‘‡ = 24, and for AQI-36, the temporal window is 36. To be consistent with prior works <ref type="bibr" target="#b49">[50]</ref>, the adjacency matrices of sensor networks are built by applying a thresholded Gaussian kernel <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b63">64]</ref> over the geographical distances between sensor stations.</p><p>For AQI and AQI-36, we use the evaluation masks in <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b81">82]</ref> which simulates the real missing data distribution in the datasets. We refer to this setting as the general missing. For METR-LA and PEMS-BAY, we consider both points missing and block missing settings as in <ref type="bibr" target="#b49">[50]</ref>. In point missing, 25% data points are masked out. In block missing, 5% spatial blocks and 0.15% temporal blocks ranging from 1 hour to 4 hours are masked out.</p><p>Evaluation Metrics. We use the standard Mean Absolute Error (MAE) and Mean Squared Error (MSE) between the ground truth values and the imputed values as the evaluation metrics.</p><p>Baselines. We consider three groups of methods as our baselines. (1) Traditional statistical methods: the mean value of the sequence (MEAN); neighbor mean (KNN); matrix factorization (MF); multiple imputations using chained equations (MICE <ref type="bibr" target="#b71">[72]</ref>); vector auto-regression (VAR); (2) Early deep learning models: rGAIN <ref type="bibr" target="#b9">[10]</ref>: an adversarial method similar to <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b50">51]</ref>; BRITS <ref type="bibr" target="#b6">[7]</ref>: a bidirectional RNN imputation method; (3) Recent deep learning models: ST-Transformer <ref type="bibr" target="#b49">[50]</ref>: a spatiotemporal extension of the original Transformer <ref type="bibr" target="#b67">[68]</ref>; GRIN <ref type="bibr" target="#b9">[10]</ref>: a graph enhanced recurrent neural network; SPIN <ref type="bibr" target="#b49">[50]</ref>: a spatiotemporal graph attention based imputation model; PoGeVon <ref type="bibr" target="#b69">[70]</ref>: a recent spatiotemporal imputation method which is based on the position-aware spatiotemporal graph variational auto-encoder. Whenever possible, the results of baselines are copied from the corresponding paper.</p><p>Implementation Details. Most of the training configurations follow prior works <ref type="bibr" target="#b49">[50]</ref>. We set the size of embeddings as 32. The numbers of layers for the encoder for AQI-36 and other datasets are 2 and 4 respectively. We use the Adam optimizer <ref type="bibr" target="#b37">[38]</ref> with a learning rate of 0.0008 and a cosine scheduler to train the model. The maximum number of epochs is 300 and the patience of early stopping is 40 epochs. Batch size is fixed as 8. During training, ğ‘ âˆˆ [0.2, 0.5, 0.8] data points are randomly masked out for each batch, and the loss is calculated based on these masked points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Overall Performance</head><p>In this subsection, we show the overall performance (MSE and MAE) of Casper for the imputation task to demonstrate the overall competence of Casper for imputation.</p><p>The overall performance of different methods is presented in Table <ref type="table" target="#tab_3">1</ref>. The upper, middle, and lower groups of baselines are the traditional statistical methods, the RNN methods and the recent methods (Transformer based and graph based methods). Generally speaking, the RNN methods perform better than the statistical methods, and the recent methods further outperform the RNN methods. When imputing a data point, these methods exploit all the available information in the context, without identifying the causal relationships between the data point and the context. However, it is inevitable that some confounders are included in the data, such as the non-causal shortcut edges. Over-reliance on the confounders could lead to overfitting and make the model susceptible to noise. The proposed Casper could effectively remove the impact of confounders. As shown in Table <ref type="table" target="#tab_3">1</ref>, Casper achieves the lowest overall MAE and MSE scores and also has lower standard deviations, demonstrating the effectiveness of enforcing the model to discover causality during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head><p>In this subsection, we study the impact of different components in Casper, and the results are shown in Table <ref type="table" target="#tab_4">2</ref>.</p><p>Effectiveness of PBD,SCA and A. In the upper part of Table <ref type="table" target="#tab_4">2</ref>, we investigate the impact of the Prompt-Based Decoder (PBD), the Spatiotemporal Causal Attention (SCA) and the sensor network A. First, a significant performance drop on MAE and MSE can be observed when we remove PBD (replace PBD with an MLP) and/or SCA (remove the causal gate ğ›½) from the full model Casper, indicating the effectiveness of the frontdoor adjustment and the causal gate for improving the overall performance. It is also worth noting that, on MSE, the standard deviations of the ablated versions of Casper (w/o PBD and/or SCA) are significantly higher than the full model Casper, demonstrating that enforcing the model to focus on the causality could improve its robustness. Second, although A contains non-causal relationships among sensors, it still has critical contributions to the overall performance, demonstrating the necessity of considering A when imputing missing values.</p><p>Effectiveness of the Prompts. In the middle part of Table <ref type="table" target="#tab_4">2</ref>, we demonstrate the effectiveness of using the learnable prompts to capture the global contextual information of the datasets by replacing the prompts with other approximations, including K-means cluster centers and randomly sampled data. To obtain the other two approximations, we first pre-train an imputation model, i.e., Casper without SCA and PBD, and then extract the embedding of each training sample G by applying average/max pooling over the embeddings of all the points in G. For the cluster center approximation, we apply K-means over the embeddings to obtain 1,000 cluster centers. For the sampling approximation, we randomly sample 1,000 embeddings for each training sample. Compared with the prompts, the cluster centers and randomly sampled embeddings not only perform worse but also require extra effort to obtain embeddings, which demonstrates the superiority of prompts. Effectiveness of Other Components. In the lower part of Table <ref type="table" target="#tab_4">2</ref>, we study the impact of time constraints and the skip project layer in the model. First, if we enforce the time constraint, i.e., ğ‘¡ â€² â‰¤ ğ‘¡ in Definition 5, for the imputation task, then the performance will significantly drop. These results demonstrate that it is vital to take into consideration both the past and future points for the imputation task. Second, removing the skip project layer will have negative impacts on the overall performance, showing the power of the skip project, which aligns with the observation of the residue connections in the literature <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b77">78]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Visualization</head><p>In this subsection, we provide visualization to further analyze Casper's ability of causality discovery.</p><p>Attention Maps. In Figure <ref type="figure" target="#fig_5">5</ref>, we visualize two input time series from the test set of AQI-36, and their corresponding attention maps of the last encoder layer from different models, i.e., Casper, Casper w/o SCA (no causal gate) and Casper w/o SCA, PBD (no causal gate, PBDâ†’MLP). Figure <ref type="figure" target="#fig_5">5a</ref> and 5e are the inputs and the diamonds are the target query points. Other figures show the attention scores between the query point and all other points in the context. Figure <ref type="figure" target="#fig_5">5b</ref> and 5f are the causal aware attention scores ğ›½ â€¢ ğ›¼/ğ‘ in Equation <ref type="bibr" target="#b12">(13)</ref>. Figure <ref type="figure" target="#fig_5">5c</ref>-5d and Figure <ref type="figure" target="#fig_5">5g</ref>-5h are the attention scores ğ›¼. First, by comparing the last two figures in each row, we can observe that the attention maps learned by Casper w/o SCA are sparser than Casper w/o SCA, PBD. This observation demonstrates that the frontdoor adjustment can effectively remove the noise and confounders by forcing the model to focus on only a small set of important points. Second, by comparing Casper and Casper w/o SCA, we can observe that SCA further improves the sparsity of attention by focusing on only a few points. Since ğ‘™ 1 norm is placed over the causal probabilities ğœŒ during training, therefore, points with non-zero attention weights are critical for the query point, which cannot be removed. According to the Granger causality (Definition 5), these non-zero points are the causes for the query. Discovered Causal Relationships. We draw the most salient causal relationships in Figure <ref type="figure" target="#fig_5">5b</ref> and 5f on the map of Beijing in Figure <ref type="figure" target="#fig_7">6a</ref> and 6b. Each number corresponds to a sensor and the arrow width corresponds to the attention weights. It is evident that for the query sensors 14 and 1 in the two figures, not every nearby sensor has a causal relationship with them. In Figure <ref type="figure" target="#fig_7">6a</ref>, although the sensors 6, 5, 15 are very close to the query 14, Casper discovers that there is no causal relationship among them. We conjecture that this is because the wind blew generally westward and northward in Beijing for Figure <ref type="figure" target="#fig_5">5b</ref> and 5f <ref type="bibr" target="#b88">[89]</ref>. In Figure <ref type="figure" target="#fig_7">6b</ref>, the neighboring sensors 24, 15, 6, 20, 9, 13 are not regarded as the causes for the query sensor 1. According to the spatial relationships, the information of the sensors 15, 6, 20, 9, 13 might be included in the causal sensors such as 2, 5 14, and 8, since the causal sensors are in between with the query and these non-causal sensors. However, there is no other sensor between 1 and 24. By taking a closer look at the upper-left corner of the map, we find that sensors 1 and 24 are separated by the Fragrant Hills. Therefore, the air quality at sensor 1 might not be directly influenced by sensor 24 for the period of the input data.</p><p>The two examples in Figure <ref type="figure" target="#fig_7">6</ref> show that the proposed Casper could effectively discover the causal relationships among sensors, which provides better insights for further data analysis. Additionally, the two examples also show that the simple distance-based sensor network construction is biased, which contains many non-causal correlations since it ignores other factors in the real world, such as wind direction and terrain.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Causal Graph Discovery on the Quasi-Realistic Dataset</head><p>In the real world, the ground truth causal graphs are usually unavailable, and thus it is difficult to quantitatively evaluate the ability of causal graph discovery. Therefore, we follow <ref type="bibr" target="#b8">[9]</ref> and evaluate our Casper on a quasi-realistic data called DREAM-3 <ref type="bibr" target="#b54">[55]</ref>, which is a gene expression data regulation dataset. DREAM-3 contains ğ‘ = 100 gene expression levels and the length of the expressions is ğ‘‡ = 21. The goal is to discover the causal relationship among the 100 gene expression levels.  <ref type="table">3</ref>, where the results of baselines are copied from <ref type="bibr" target="#b8">[9]</ref>. Table <ref type="table">3</ref> shows that Casper achieves the highest AUC score. This experiment quantitatively demonstrates that Casper has a strong ability of discovering causal relationships.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">More Results</head><p>In this subsection, we provide more experimental results of Casper, including convergence of ğœŒ, and sensitivity analysis.</p><p>Convergence of ğœŒ. We set 0.1 and 0.9 as the thresholds to round ğœŒ. Specifically, if ğœŒ â‰¤ 0.1 then we regard ğœŒ has converged to 0; similarly, if ğœŒ â‰¥ 0.9, then we regard ğœŒ has converged to 1. The statistical results of the AQI-36 dataset show that 98% ğœŒ converges to 0 or 1, which corroborates Theorem 1.</p><p>Sensitivity Analysis. We present the results of sensitivity experiments of ğœ† and the number of prompts ğ‘ ğ‘ƒ in Figure <ref type="figure" target="#fig_10">7a</ref>-7b. For ğœ†, the lowest MAE can be obtained when ğœ† is around 0.001. For ğ‘ ğ‘ƒ , the best results can be obtained when ğ‘ ğ‘ƒ âˆˆ [200, 1400].  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>In this section, we briefly review the most relevant works to ours, including spatiotemporal time series methods as well as causal inference methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Spatiotemporal Time Series Imputation</head><p>Spatiotemporal time series imputation is one of the fundamental tasks for time series analysis <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b92">93]</ref>. Traditional machine learning approaches are based on statistical analysis, such as linear autoregression <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b81">82]</ref> and matrix factorization <ref type="bibr" target="#b83">[84]</ref>. At present, deep learning methods have become the mainstream. Most existing deep learning methods are based on Recurrent Neural Networks (RNN). GRU-D <ref type="bibr" target="#b7">[8]</ref> is one of the first RNN-based imputation models. BRITS <ref type="bibr" target="#b6">[7]</ref> leverage bi-directional RNN to impute missing data. GAIN <ref type="bibr" target="#b44">[45]</ref> and E2GAN <ref type="bibr" target="#b45">[46]</ref> further apply Generative Adversarial Network (GAN) <ref type="bibr" target="#b17">[18]</ref> to enhance the performance. mTAND <ref type="bibr" target="#b62">[63]</ref> adds attention mechanism to RNN. These methods suffer from error propagation and accumulation brought by the auto-regression nature of RNN. To address this issue, non-autoregressive methods are proposed such as NAOMI <ref type="bibr" target="#b43">[44]</ref>, NRTSI <ref type="bibr" target="#b61">[62]</ref> and the recent Transformer <ref type="bibr" target="#b67">[68]</ref> based methods <ref type="bibr" target="#b82">[83]</ref>. There are also some other types of methods, such as Ordinary Differential Equations (ODEs) methods <ref type="bibr" target="#b57">[58,</ref><ref type="bibr" target="#b57">58]</ref>, state space models <ref type="bibr" target="#b1">[2]</ref> and diffusion models <ref type="bibr" target="#b66">[67]</ref> The above methods mainly focus on the temporal patterns of time series, yet largely ignore the spatial relationships, e.g., distance, among time series. To capture spatial relationships, graph neural networks <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b68">69]</ref> are extended to the spatiotemporal setting. LG-ODE <ref type="bibr" target="#b23">[24]</ref> combines graph neural networks with ODE methods <ref type="bibr" target="#b57">[58]</ref>. RETIME <ref type="bibr" target="#b33">[34]</ref> introduces a retrieval-based time series model, which leverages retrieved time series as an augmentation for the target time series. NET 3 <ref type="bibr" target="#b28">[29]</ref> introduces a tensor graph neural network to model the high-order relationships among time series. GRIN <ref type="bibr" target="#b9">[10]</ref> introduces a bidirectional message passing RNN with a spatial decoder. SPIN <ref type="bibr" target="#b49">[50]</ref> presents a sparse spatiotemporal graph neural network for spatiotemporal time series imputation. Recently, PoGeVon <ref type="bibr" target="#b69">[70]</ref> proposes a position-aware graph neural network based variational auto-encoder to impute both time series and edges. However, these methods try to exploit all the available related information for the target missing point, without distinguishing the causal and noncausal relationships, which might have the overfitting problem and make the model vulnerable to noise. Our proposed Casper could distinguish causal and non-causal relationships.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Causal Inference</head><p>Causality theory <ref type="bibr" target="#b53">[54]</ref> provides theoretical guidance to design causalityaware models. It has been widely explored in the computer vision domain to discover causal relationships <ref type="bibr" target="#b5">[6]</ref>, generate counterfactual samples <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b84">85]</ref> and reduce bias <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b80">81]</ref>. In the graph mining domain, CGI <ref type="bibr" target="#b14">[15]</ref> studies how to select trustworthy neighbors during inference; CLEAR <ref type="bibr" target="#b47">[48]</ref> explores how to generate counterfactual explanations for graph-level prediction models based on Independent Component Analysis (ICA) <ref type="bibr" target="#b36">[37]</ref>; HyperSCI <ref type="bibr" target="#b48">[49]</ref> explores the Individual Treatment Effect (ITE) on hyper-graphs. NEAT <ref type="bibr" target="#b46">[47]</ref> investigates the impact of MRSA infection via the Neyman-Rubin potential outcome framework <ref type="bibr" target="#b58">[59]</ref>. CAL <ref type="bibr" target="#b64">[65]</ref> introduces a causal attention learning framework for graph neural networks based on the backdoor adjustment <ref type="bibr" target="#b53">[54]</ref>. There are two differences between Casper and the above graph methods: (1) our setting is the dynamic spatiotemporal setting but their setting is static graph; (2) Casper is based on the frontdoor adjustment and Granger causality, which is fundamentally different from their theoretical basis of causality. In the time series domain, the Granger causality <ref type="bibr" target="#b18">[19]</ref> is widely used for analyzing the causality between time series in the forecasting setting. GrID-Net <ref type="bibr" target="#b72">[73]</ref> leverages the Granger causality to infer regulatory locus-gene links. cLSTM <ref type="bibr" target="#b65">[66]</ref> and economy-SRU <ref type="bibr" target="#b35">[36]</ref> integrates the Granger causality with LSTM <ref type="bibr" target="#b20">[21]</ref> and SRU <ref type="bibr" target="#b52">[53]</ref>. However, these methods require the input time series to be fully observed. CUTS <ref type="bibr" target="#b8">[9]</ref> is a recently proposed two-stage model, which first imputes missing data and then discovers causality between time series. There are three major differences between Casper and CUTS. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we review the spatiotemporal time series imputation task via the Structure Causal Model (SCM), which shows the causal relationships among the input, output, embeddings, and confounders. The confounders could open shortcut backdoor paths between the input and output, which could mislead the model to learn the non-causal relationships. We use the frontdoor adjustment to block the backdoor paths. Based on the results of the frontdoor adjustment, we propose a novel Causality-Aware Spatiotemporal Graph Neural Network (Casper), which is comprised of a Prompt Based Decoder (PBD) and an encoder equipped with Spatiotemporal Causal Attention (SCA). The proposed PBD could reduce the impact of the confounders at a high level. For SCA, we first extend the definition of Granger causality for time series to embeddings. Then we introduce the architecture of SCA based on the definition, which could discover the sparse causal relationships among embeddings. Theoretical analysis shows that SCA decides causal and non-casual relationships based on the values of gradients. Experimental results on three real-world benchmark datasets show that Casper could outperform the baseline methods for the imputation task. Further analysis shows that Casper could effectively discover the sparse causal relationships.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The Structure Causal Model (SCM) for spatiotemporal imputation. The frontdoor adjustment removes the edge between confounder C and input G.</figDesc><graphic coords="3,349.81,83.68,176.55,122.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>, since C can reach H via the path C â†’ G â†’ H . This means that H and C are not independent and thus H contains information of C. Second, besides G â†’ H â†’ Y, C introduces backdoor paths between G and Y, as well as H and Y: G â† C â†’ Y, H â† G â† C â†’ Y.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Overview of Casper.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Prompt Based Decoder (PBD).</figDesc><graphic coords="4,110.87,83.69,126.09,107.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Spatiotemporal Causal Attention (SCA). Corr. Weight and Causal Prob. correspond to Equation (14)(16).</figDesc><graphic coords="4,349.81,83.69,176.54,85.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Input matrices and the associated attention maps of Casper and its ablated versions. Diamonds are the query points.</figDesc><graphic coords="8,175.04,348.11,118.53,101.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>(a) Causal relations in Figure 5b. (b) Causal relations in Figure 5f.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Discovered causal relationships.</figDesc><graphic coords="8,54.27,348.11,118.53,101.94" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>For DREAM-3, we train Casper via the imputation task, and use similar training configurations as AQI-36. Following<ref type="bibr" target="#b8">[9]</ref>, we use AUC between the ground-truth graph and the discovered graph as the evaluation metric. Specifically, for Casper we obtain a causal weight matrix C ğ‘–,ğ‘¡ âˆˆ R ğ‘ Ã—ğ‘‡ for each data point ğ‘¥ ğ‘–,ğ‘¡ , where C ğ‘–,ğ‘¡ [ğ‘– â€² , ğ‘¡ â€² ] = ğ›½ ğ‘– â€² ,ğ‘¡ â€² in Equation17. By concatenating all the C ğ‘–,ğ‘¡ together, where ğ‘– âˆˆ {1, â€¢ â€¢ â€¢ , ğ‘ } and ğ‘¡ âˆˆ {1, â€¢ â€¢ â€¢ ,ğ‘‡ }, we have a tensor C âˆˆ R ğ‘ Ã—ğ‘‡ Ã—ğ‘ Ã—ğ‘‡ , where the first two dimensions correspond to the data point ğ‘¥ ğ‘–,ğ‘¡ , Table3: Causal Graph Discovery on DREAM-3.Models PCMCI<ref type="bibr" target="#b59">[60]</ref> NGC<ref type="bibr" target="#b65">[66]</ref> eSRU<ref type="bibr" target="#b35">[36]</ref> LCCM<ref type="bibr" target="#b10">[11]</ref> NGM<ref type="bibr" target="#b4">[5]</ref> CUTS[9and the last two dimensions correspond to the causal weight C ğ‘–,ğ‘¡ for ğ‘¥ ğ‘–,ğ‘¡ . We obtain the final causal weight matrix by max pooling over the two dimensions corresponding to the time, and normalizing by ğ‘‡ 2 : A = max-pool(C, dim=2, 4)/ğ‘‡ 2 âˆˆ R ğ‘ Ã—ğ‘ .The results of Casper and several SOTA time series causal discovery baselines are presented in Table</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>(a) The weight of ğ‘™ 1 norm ğœ†. (b) The number of prompts ğ‘ ğ‘ƒ .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Sensitivity experiments.</figDesc><graphic coords="9,53.80,83.68,119.03,102.03" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>( 1 )</head><label>1</label><figDesc>CUTS does not consider confounders, while Casper removes confounders via the frontdoor adjustment. (2) CUTS has to re-train the causal model for each input segment, but Casper does not have such a requirement. (3) CUTS is a two-stage model, while Casper is a one-stage end-to-end model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>â€¢ ğ‘“ ğ¸ , where ğ‘“ ğ¸ , ğ‘“ ğ· are the encoder and decoder, works as follows: (1) ğ‘“ ğ¸ extracts embeddings H = {h ğ‘–,ğ‘¡ } ğ‘ ,ğ‘‡ ğ‘–=1,ğ‘¡ =1 from G, (2) ğ‘“ ğ· generates predictions { Å·ğ‘–,ğ‘¡ } ğ‘ ,ğ‘‡ ğ‘–=1,ğ‘¡ =1 based on H to recover Y = {ğ‘¦ ğ‘–,ğ‘¡ } ğ‘ ,ğ‘‡ ğ‘–=1,ğ‘¡ =1 . The model ğ‘“ is trained by</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>is the causal gate, and ğœŒ ğ‘– â€² ,ğ‘¡ â€² is the probability that h ğ‘–ğ‘› ğ‘– â€² ,ğ‘¡ â€² Granger causes h ğ‘œğ‘¢ğ‘¡ ğ‘–,ğ‘¡ . According to Definition 5, if ğ›½ ğ‘– â€² ,ğ‘¡ â€² â€¢ ğ›¼ ğ‘– â€² ,ğ‘¡ â€² &gt; 0, then h ğ‘–ğ‘› ğ‘– â€² ,ğ‘¡ â€² Granger causes h ğ‘œğ‘¢ğ‘¡ ğ‘–,ğ‘¡ ; otherwise, there is no Granger causality between h ğ‘–ğ‘› ğ‘– â€² ,ğ‘¡ â€² and h ğ‘œğ‘¢ğ‘¡ ğ‘–,ğ‘¡ . The function of calculating correlation weight ğ›¼ ğ‘– â€² ,ğ‘¡ â€² is given by:</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Performance (MAE, MSE) of different methods.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">General Missing</cell><cell></cell><cell></cell><cell cols="2">Point Missing</cell><cell></cell><cell></cell><cell cols="2">Block Missing</cell></row><row><cell></cell><cell>AQI-36</cell><cell></cell><cell>AQI</cell><cell></cell><cell cols="2">METR-LA</cell><cell cols="2">PEMS-BAY</cell><cell cols="2">METR-LA</cell><cell cols="2">PEMS-BAY</cell></row><row><cell></cell><cell>MAE</cell><cell>MSE</cell><cell>MAE</cell><cell>MSE</cell><cell>MAE</cell><cell>MSE</cell><cell>MAE</cell><cell>MSE</cell><cell>MAE</cell><cell>MSE</cell><cell>MAE</cell><cell>MSE</cell></row><row><cell>Mean</cell><cell cols="12">53.48Â±0.00 4578.08Â±00.00 39.60Â±0.00 3231.04Â±00.00 7.56Â±0.00 142.22Â±0.00 5.42Â±0.00 86.59Â±0.00 7.48Â±0.00 139.54Â±0.00 5.46Â±0.00 87.56Â±0.00</cell></row><row><cell>KNN</cell><cell cols="12">30.21Â±0.00 2892.31Â±00.00 34.10Â±0.00 3471.14Â±00.00 7.88Â±0.00 129.29Â±0.00 4.30Â±0.00 49.80Â±0.00 7.79Â±0.00 124.61Â±0.00 4.30Â±0.00 49.90Â±0.00</cell></row><row><cell>MF</cell><cell cols="12">30.54Â±0.26 2763.06Â±63.35 26.74Â±0.24 2021.44Â±27.98 5.56Â±0.03 113.46Â±1.08 3.29Â±0.01 51.39Â±0.64 5.46Â±0.02 109.61Â±0.78 3.28Â±0.01 50.14Â±0.13</cell></row><row><cell>MICE</cell><cell cols="12">30.37Â±0.09 2594.06Â±07.17 26.98Â±0.10 1930.92Â±10.08 4.42Â±0.07 55.07Â±1.46 3.09Â±0.02 31.43Â±0.41 4.22Â±0.05 51.07Â±1.25 2.94Â±0.02 28.28Â±0.37</cell></row><row><cell>VAR</cell><cell cols="12">15.64Â±0.08 833.46Â±13.85 22.95Â±0.30 1402.84Â±52.63 2.69Â±0.00 21.10Â±0.02 1.30Â±0.00 6.52Â±0.01 3.11Â±0.08 28.00Â±0.76 2.09Â±0.10 16.06Â±0.73</cell></row><row><cell>rGAIN</cell><cell cols="12">15.37Â±0.26 641.92Â±33.89 21.78Â±0.50 1274.93Â±60.28 2.83Â±0.01 20.03Â±0.09 1.88Â±0.02 10.37Â±0.20 2.90Â±0.01 21.67Â±0.15 2.18Â±0.01 13.96Â±0.20</cell></row><row><cell>BRITS</cell><cell cols="12">14.50Â±0.35 662.36Â±65.16 20.21Â±0.22 1157.89Â±25.66 2.34Â±0.00 16.46Â±0.05 1.47Â±0.00 7.94Â±0.03 2.34Â±0.01 17.00Â±0.14 1.70Â±0.01 10.50Â±0.07</cell></row><row><cell cols="13">ST-Transformer 11.98Â±0.53 557.22Â±46.52 18.11Â±0.25 1135.46Â±89.27 2.16Â±0.00 13.66Â±0.03 0.74Â±0.00 1.96Â±0.03 3.54Â±0.00 52.22Â±0.99 1.70Â±0.02 20.37Â±0.43</cell></row><row><cell>GRIN</cell><cell cols="12">12.08Â±0.47 523.14Â±57.17 14.73Â±0.15 775.91Â±28.49 1.91Â±0.00 10.41Â±0.03 0.67Â±0.00 1.55Â±0.01 2.03Â±0.00 13.26Â±0.05 1.14Â±0.01 6.60Â±0.10</cell></row><row><cell>SPIN</cell><cell cols="12">11.77Â±0.54 455.53Â±12.27 13.92Â±0.15 773.60Â±26.64 1.90Â±0.01 18.47Â±0.31 0.70Â±0.01 1.91Â±0.01 1.98Â±0.01 18.47Â±0.31 1.06Â±0.02 7.42Â±0.16</cell></row><row><cell>PoGeVon</cell><cell cols="12">10.92Â±0.24 493.94Â±51.89 14.18Â±0.04 740.57Â±8.01 1.96Â±0.01 11.08Â±0.05 0.67Â±0.01 1.51Â±0.03 1.95Â±0.01 13.08Â±0.08 1.54Â±0.02 17.18Â±0.48</cell></row><row><cell>Casper</cell><cell cols="12">10.09Â±0.13 396.16Â±12.94 13.30Â±0.06 658.07Â±4.88 1.84Â±0.00 9.99Â±0.01 0.65Â±0.00 1.63Â±0.01 1.92Â±0.01 11.98Â±0.23 1.00Â±0.00 5.37Â±0.04</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Ablation study on the AQI-36 dataset.</figDesc><table><row><cell></cell><cell>MAE</cell><cell>MSE</cell></row><row><cell>Casper</cell><cell cols="2">10.09Â±0.13 396.16Â±12.94</cell></row><row><cell>w/o PBD (i.e., PBDâ†’MLP)</cell><cell>10.36Â±0.11</cell><cell>445.03Â±29.03</cell></row><row><cell>w/o SCA (i.e., w/o ğ›½)</cell><cell>10.45Â±0.17</cell><cell>426.64Â±34.41</cell></row><row><cell>w/o PBD, SCA</cell><cell>10.84Â±0.20</cell><cell>472.24Â±42.77</cell></row><row><cell>w/o PBD, SCA, A</cell><cell>14.86Â±0.21</cell><cell>767.96Â±25.32</cell></row><row><cell>Prompts â†’ K-means Centers (max)</cell><cell>10.18Â±0.14</cell><cell>427.20Â±28.84</cell></row><row><cell>Prompts â†’ K-means Centers (avg)</cell><cell>10.23Â±0.15</cell><cell>421.43Â±19.40</cell></row><row><cell>Prompts â†’ Sampling (max)</cell><cell>10.25Â±0.18</cell><cell>421.78Â±33.26</cell></row><row><cell>Prompts â†’ Sampling (avg)</cell><cell>10.56Â±0.19</cell><cell>474.27Â±50.55</cell></row><row><cell>unconstrainedâ†’constrained causality</cell><cell>10.83Â±0.19</cell><cell>477.85Â±45.62</cell></row><row><cell>w/o skip project</cell><cell>10.35Â±0.21</cell><cell>438.02Â±28.38</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Counterfactual vision and language learning</title>
		<author>
			<persName><forename type="first">Ehsan</forename><surname>Abbasnejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Damien</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amin</forename><surname>Parvaneh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Javen</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="10044" to="10054" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Diffusion-based Time Series Imputation and Forecasting with Structured State Space Models</title>
		<author>
			<persName><forename type="first">Juan</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alcaraz</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Nils</forename><surname>Strodthoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Machine Learning Research</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Spatio-temporal data mining: A survey of problems and methods</title>
		<author>
			<persName><forename type="first">Gowtham</forename><surname>Atluri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anuj</forename><surname>Karpatne</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vipin</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="1" to="41" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Lei Ba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Ryan Kiros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<title level="m">Layer normalization</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Neural graphical modelling in continuous-time: consistency guarantees and algorithms</title>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Bellot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kim</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihaela</forename><surname>Van Der Schaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A Meta-Transfer Objective for Learning to Disentangle Causal Mechanisms</title>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tristan</forename><surname>Deleu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nasim</forename><surname>Rahaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><forename type="middle">Rosemary</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastien</forename><surname>Lachapelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olexa</forename><surname>Bilaniuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Pal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Brits: Bidirectional recurrent imputation for time series</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yitan</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Recurrent neural networks for multivariate time series with missing values</title>
		<author>
			<persName><forename type="first">Zhengping</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Purushotham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific reports</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">6085</biblScope>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">CUTS: Neural Causal Discovery from Irregular Time-Series Data</title>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Runzhao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingxiong</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongren</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinli</forename><surname>Suo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kunlun</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qionghai</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Filling the G_ap_s: Multivariate Time Series Imputation by Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Cini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Marisca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cesare</forename><surname>Alippi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Latent convergent cross mapping</title>
		<author>
			<persName><forename type="first">Edward</forename><forename type="middle">De</forename><surname>Brouwer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Arany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaak</forename><surname>Simm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yves</forename><surname>Moreau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">New frontiers of multi-network mining: Recent developments and future trend</title>
		<author>
			<persName><forename type="first">Boxin</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Si</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanghang</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4038" to="4039" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Time series analysis by state space methods</title>
		<author>
			<persName><forename type="first">James</forename><surname>Durbin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siem</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2012-01">Jan Koopman. 2012</date>
			<publisher>OUP</publisher>
			<biblScope unit="volume">38</biblScope>
			<pubPlace>Oxford</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Time-series data mining</title>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Esling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlos</forename><surname>Agon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="1" to="34" />
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Xin Xin, Qifan Wang, and Tat-Seng Chua. 2021. Should graph convolution trust neighbors? a simple causal inference method</title>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiran</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<biblScope unit="page" from="1208" to="1218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adversarial graph contrastive learning with information regularization</title>
		<author>
			<persName><forename type="first">Shengyu</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baoyu</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yada</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanghang</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Web Conference 2022</title>
		<meeting>the ACM Web Conference 2022</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1362" to="1371" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Ariel: Adversarial graph contrastive learning</title>
		<author>
			<persName><forename type="first">Shengyu</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baoyu</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yada</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanghang</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Knowledge Discovery from Data</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Investigating causal relations by econometric models and cross-spectral methods</title>
		<author>
			<persName><forename type="first">Clive Wj</forename><surname>Granger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econometrica: journal of the Econometric Society</title>
		<imprint>
			<biblScope unit="page" from="424" to="438" />
			<date type="published" when="1969">1969. 1969</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">JÃ¼rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Root-mean-square error (RMSE) or mean absolute error (MAE): When to use them or not</title>
		<author>
			<persName><surname>Timothy O Hodson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Geoscientific Model Development</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="5481" to="5487" />
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Distilling causal effect of data in class-incremental learning</title>
		<author>
			<persName><forename type="first">Xinting</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaihua</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyan</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian-Sheng</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3957" to="3966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning continuous system dynamics from irregularly-sampled partial observations</title>
		<author>
			<persName><forename type="first">Zijie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="16177" to="16187" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Categorical Reparameterization with Gumbel-Softmax</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shixiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Visual prompt tuning</title>
		<author>
			<persName><forename type="first">Menglin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luming</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bor-Chun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claire</forename><surname>Cardie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bharath</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ser-Nam</forename><surname>Lim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="709" to="727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">X-GOAL: Multiplex heterogeneous graph prototypical contrastive learning</title>
		<author>
			<persName><forename type="first">Baoyu</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengyu</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuejia</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanghang</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st ACM International Conference on Information &amp; Knowledge Management</title>
		<meeting>the 31st ACM International Conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="894" to="904" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Hdmi: High-order deep multiplex infomax</title>
		<author>
			<persName><forename type="first">Baoyu</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chanyoung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanghang</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Web Conference 2021</title>
		<meeting>the Web Conference 2021</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2414" to="2424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Network of tensor time series</title>
		<author>
			<persName><forename type="first">Baoyu</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanghang</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yada</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Web Conference 2021</title>
		<meeting>the Web Conference 2021</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2425" to="2437" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Baoyu</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yansen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guoxin</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingrui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuqing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongsheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kan</forename><surname>Ren</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.12641</idno>
		<title level="m">Automated Contrastive Learning Strategy Search for Time Series</title>
		<imprint>
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Sterling: Synergistic representation learning on bipartite graphs</title>
		<author>
			<persName><forename type="first">Baoyu</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaize</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chanyoung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yada</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanghang</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="12976" to="12984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Coin: Co-cluster infomax for bipartite graphs</title>
		<author>
			<persName><forename type="first">Baoyu</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yada</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanghang</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS 2022 Workshop: New Frontiers in Graph Learning</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Multiplex Graph Neural Network for Extractive Text Summarization</title>
		<author>
			<persName><forename type="first">Baoyu</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyu</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanghang</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="133" to="139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">Baoyu</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Si</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yada</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiyu</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Margenot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanghang</forename><surname>Tong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.13525</idno>
		<title level="m">Retrieval based time series forecasting</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Self-attentive sequential recommendation</title>
		<author>
			<persName><forename type="first">Wang-Cheng</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE international conference on data mining (ICDM)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="197" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Economy Statistical Recurrent Units For Inferring Nonlinear Granger Causality</title>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Khanna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">F</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Variational autoencoders and nonlinear ica: A unifying framework</title>
		<author>
			<persName><forename type="first">Ilyes</forename><surname>Khemakhem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aapo</forename><surname>Hyvarinen</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2207" to="2217" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">CausalGAN: Learning Causal Implicit Generative Models with Adversarial Training</title>
		<author>
			<persName><forename type="first">Murat</forename><surname>Kocaoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Snyder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>Alexandros G Dimakis, and Sriram Vishwanath</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Graph communal contrastive learning</title>
		<author>
			<persName><forename type="first">Bolian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baoyu</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanghang</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM web conference 2022</title>
		<meeting>the ACM web conference 2022</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1203" to="1213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Outlier impact characterization for time series data</title>
		<author>
			<persName><forename type="first">Jianbo</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lecheng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yada</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingrui</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="11595" to="11603" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Diffusion Convolutional Recurrent Neural Network: Data-Driven Traffic Forecasting</title>
		<author>
			<persName><forename type="first">Yaguang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rose</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cyrus</forename><surname>Shahabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Naomi: Non-autoregressive multiresolution sequence imputation</title>
		<author>
			<persName><forename type="first">Yukai</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rose</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yisong</forename><surname>Yue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Multivariate time series imputation with generative adversarial networks</title>
		<author>
			<persName><forename type="first">Yonghong</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangrui</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="2018-06">Jun Xu, et al. 2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">E2gan: Endto-end generative adversarial network for multivariate time series imputation</title>
		<author>
			<persName><forename type="first">Yonghong</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangrui</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojie</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th international joint conference on artificial intelligence</title>
		<meeting>the 28th international joint conference on artificial intelligence<address><addrLine>Alto, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press Palo</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3094" to="3100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A Look into Causal Effects under Entangled Treatment in Graphs: Investigating the Impact of Contact on MRSA Infection</title>
		<author>
			<persName><forename type="first">Jing</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anil</forename><surname>Vullikanti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ritwick</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Madden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Borrajo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jundong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="4584" to="4594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Clear: Generative counterfactual explanations on graphs</title>
		<author>
			<persName><forename type="first">Jing</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruocheng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saumitra</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jundong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="25895" to="25907" />
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning causal effects on hypergraphs</title>
		<author>
			<persName><forename type="first">Jing</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengting</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Longqi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jundong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brent</forename><surname>Hecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><surname>Teevan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1202" to="1212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning to reconstruct missing data from spatiotemporal graphs with sparse observations</title>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Marisca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Cini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cesare</forename><surname>Alippi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="32069" to="32082" />
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Generative semi-supervised learning for multivariate time series imputation</title>
		<author>
			<persName><forename type="first">Xiaoye</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangyang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunjun</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xudong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianwei</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="8983" to="8991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Machine learning: a probabilistic perspective</title>
		<author>
			<persName><forename type="first">Kevin P</forename><surname>Murphy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">The statistical recurrent unit</title>
		<author>
			<persName><forename type="first">B</forename><surname>Junier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">BarnabÃ¡s</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>PÃ³czos</surname></persName>
		</author>
		<author>
			<persName><surname>Schneider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2671" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">The book of why: the new science of cause and effect</title>
		<author>
			<persName><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dana</forename><surname>Mackenzie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note>Basic books</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Robert J Prill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julio</forename><surname>Marbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">K</forename><surname>Saez-Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonidas</forename><forename type="middle">G</forename><surname>Sorger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaowei</forename><surname>Alexopoulos</surname></persName>
		</author>
		<author>
			<persName><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregoire</forename><surname>Neil D Clarke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gustavo</forename><surname>Altan-Bonnet</surname></persName>
		</author>
		<author>
			<persName><surname>Stolovitzky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Towards a rigorous assessment of systems biology models: the DREAM3 challenges</title>
		<imprint>
			<date type="published" when="2010">2010. 2010</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">9202</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Two causal principles for improving visual dialog</title>
		<author>
			<persName><forename type="first">Jiaxin</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulei</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="10860" to="10869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">CANON: Complex Analytics of Network of Networks for Modeling Adversarial Activities</title>
		<author>
			<persName><forename type="first">Shane</forename><surname>Roach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Connie</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexei</forename><surname>Kopylov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsai-Ching</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiejun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Si</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boxin</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lihui</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Big Data (Big Data)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1634" to="1643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Latent ordinary differential equations for irregularly-sampled time series</title>
		<author>
			<persName><forename type="first">Yulia</forename><surname>Rubanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricky Tq</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Causal inference using potential outcomes: Design, modeling, decisions</title>
		<author>
			<persName><surname>Donald B Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Amer. Statist. Assoc</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="page" from="322" to="331" />
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Detecting and quantifying causal associations in large nonlinear time series datasets</title>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Runge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peer</forename><surname>Nowack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marlene</forename><surname>Kretschmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seth</forename><surname>Flaxman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dino</forename><surname>Sejdinovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science advances</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">4996</biblScope>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Grad-cam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Ramprasaath R Selvaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhishek</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramakrishna</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE interna</title>
		<meeting>the IEEE interna</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="618" to="626" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Nrtsi: Non-recurrent time series imputation</title>
		<author>
			<persName><forename type="first">Siyuan</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junier B</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Multi-Time Attention Networks for Irregularly Sampled Time Series</title>
		<author>
			<persName><forename type="first">Satya</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shukla</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Marlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains</title>
		<author>
			<persName><surname>David I Shuman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Sunil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Frossard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Ortega</surname></persName>
		</author>
		<author>
			<persName><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE signal processing magazine</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="83" to="98" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Xiangnan He, and Tat-Seng Chua. 2022. Causal attention for interpretable and generalizable graph classification</title>
		<author>
			<persName><forename type="first">Yongduo</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiancan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<biblScope unit="page" from="1696" to="1705" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Neural granger causality</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Tank</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Covert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Foti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Shojaie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><forename type="middle">B</forename><surname>Fox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="4267" to="4279" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Csdi: Conditional score-based diffusion models for probabilistic time series imputation</title>
		<author>
			<persName><forename type="first">Yusuke</forename><surname>Tashiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="24804" to="24816" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Åukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Petar</forename><surname>VeliÄkoviÄ‡</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">Graph attention networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Networked time series imputation via position-aware graph enhanced variational autoencoders</title>
		<author>
			<persName><forename type="first">Dingsu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruizhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yada</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiyu</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Margenot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanghang</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="2256" to="2268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<author>
			<persName><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenjie</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keli</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenjia</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxuan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingsong</forename><surname>Wen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.04059</idno>
		<title level="m">Deep learning for multivariate time series imputation: A survey</title>
		<imprint>
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Multiple imputation using chained equations: issues and guidance for practice</title>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Ian R White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><forename type="middle">M</forename><surname>Royston</surname></persName>
		</author>
		<author>
			<persName><surname>Wood</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics in medicine</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="377" to="399" />
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Granger causal inference on DAGs identifies genomic loci regulating transcription</title>
		<author>
			<persName><forename type="first">Rohit</forename><surname>Alexander P Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bonnie</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><surname>Berger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">SLOG: An Inductive Spectral Graph Neural Network Beyond Polynomial Filter</title>
		<author>
			<persName><forename type="first">Haobo</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dingsu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhichen</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Tarek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Abdelzaher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanghang</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Forty-first International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">From trainable negative depth to edge heterophily in graphs</title>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuzhong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huiyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minghua</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mahashweta</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanghang</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Pacer: Network embedding from positional to structural</title>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongyi</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinghai</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lihui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhichen</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuzhong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Menghai</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huiyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mahashweta</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanghang</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM on Web Conference 2024</title>
		<meeting>the ACM on Web Conference 2024</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="2485" to="2496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Reconciling competing sampling strategies of network embedding</title>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baoyu</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lihui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruijie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinning</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tarek</forename><surname>Abdelzaher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanghang</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">Dynamic knowledge graph alignment</title>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lihui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yikun</forename><surname>Ban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baoyu</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanghang</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="4564" to="4572" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Bright: A bridging algorithm for network alignment</title>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Si</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanghang</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the web conference 2021</title>
		<meeting>the web conference 2021</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3907" to="3917" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Dissecting cross-layer dependency inference on multi-layered interdependent networks</title>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinghai</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinning</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tarek</forename><surname>Abdelzaher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanghang</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st ACM International Conference on Information &amp; Knowledge Management</title>
		<meeting>the 31st ACM International Conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2341" to="2351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Causal attention for vision-language tasks</title>
		<author>
			<persName><forename type="first">Xu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guojun</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfei</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="9847" to="9857" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">ST-MVL: filling missing values in geo-sensory time series data</title>
		<author>
			<persName><forename type="first">Xiuwen</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junbo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianrui</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 25th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Multivariate time series imputation with transformers</title>
		<author>
			<persName><forename type="first">Emirhan</forename><surname>YarkÄ±n YÄ±ldÄ±z</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aykut</forename><surname>KoÃ§</surname></persName>
		</author>
		<author>
			<persName><surname>KoÃ§</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="2517" to="2521" />
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Temporal regularized matrix factorization for high-dimensional time series prediction</title>
		<author>
			<persName><forename type="first">Hsiang-Fu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikhil</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inderjit S</forename><surname>Dhillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Counterfactual zero-shot and open-set visual recognition</title>
		<author>
			<persName><forename type="first">Zhongqi</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qianru</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian-Sheng</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="15404" to="15414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Hierarchical multi-marginal optimal transport for network alignment</title>
		<author>
			<persName><forename type="first">Zhichen</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boxin</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Si</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinglong</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhining</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanghang</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="16660" to="16668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Graph Mixup on Approximate Gromov-Wasserstein Geodesics</title>
		<author>
			<persName><forename type="first">Zhichen</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruizhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhining</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianxin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingrui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanghang</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Forty-first International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Parrot: Position-aware regularized optimal transport for network alignment</title>
		<author>
			<persName><forename type="first">Zhichen</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Si</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinglong</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanghang</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Web Conference 2023</title>
		<meeting>the ACM Web Conference 2023</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="372" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">The impact of circulation patterns on regional transport pathways and air quality over Beijing and its surroundings</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Atmospheric Chemistry and Physics</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="5031" to="5053" />
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Modeling temporal-spatial correlations for crime prediction</title>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Conference on Information and Knowledge Management</title>
		<meeting>the 2017 ACM on Conference on Information and Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="497" to="506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">MULAN: Multi-modal Causal Structure Learning and Root Cause Analysis for Microservice Systems</title>
		<author>
			<persName><forename type="first">Lecheng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengzhang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingrui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM on Web Conference 2024</title>
		<meeting>the ACM on Web Conference 2024</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="4107" to="4116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<monogr>
		<author>
			<persName><forename type="first">Lecheng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongqi</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Maciejewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingrui</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.13798</idno>
		<title level="m">Deeper-GXX: deepening arbitrary GNNs</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
		<author>
			<persName><forename type="first">Lecheng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baoyu</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanghang</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingrui</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2404.00225</idno>
		<title level="m">Heterogeneous Contrastive Learning for Foundation Models and Beyond</title>
		<imprint>
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Forecasting fine-grained air quality based on big data</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiuwen</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiyuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangqing</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianrui</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining</title>
		<meeting>the 21th ACM SIGKDD international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2267" to="2276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">A data-driven graph generative model for temporal interaction networks</title>
		<author>
			<persName><forename type="first">Dawei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lecheng</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingrui</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="401" to="411" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<monogr>
		<title level="m" type="main">Closed-loop network anomaly detection</title>
		<author>
			<persName><forename type="first">Qinghai</forename><surname>Zhou</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
		<respStmt>
			<orgName>University of Illinois at Urbana-Champaign</orgName>
		</respStmt>
	</monogr>
	<note>Ph. D. Dissertation</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
