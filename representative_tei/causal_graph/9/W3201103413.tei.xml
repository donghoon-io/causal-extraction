<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Graph Regularized Point Process Model For Event Propagation Sequence</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2022-11-21">21 Nov 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Siqiao</forename><surname>Xue</surname></persName>
							<email>siqiao.xsq@alibaba-inc.com</email>
						</author>
						<author>
							<persName><forename type="first">Hongyan</forename><surname>Hao</surname></persName>
							<email>hongyanhao.hhy@alibaba-inc.com</email>
						</author>
						<author>
							<persName><forename type="first">Lintao</forename><surname>Ma</surname></persName>
							<email>lintao.mlt@antgroup.com</email>
						</author>
						<author>
							<persName><forename type="first">Shijun</forename><surname>Wang</surname></persName>
							<email>shijun.wang@antgroup.com</email>
						</author>
						<author>
							<persName><forename type="first">James</forename><surname>Zhang</surname></persName>
							<email>james.z@antgroup.com</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Intelligent Engine Technologies Ant Group Hangzhou</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Xiaoming Shi Intelligent Engine Technologies Ant Group Hangzhou</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Intelligent Engine Technologies Ant Group Hangzhou</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Intelligent Engine Technologies Ant Group Hangzhou</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">th Shiyu Wang Digital Finance Technologies Ant Group Hangzhou</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department">Intelligent Engine Technologies Ant Group Seattle</orgName>
								<address>
									<country>US</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff6">
								<orgName type="department">Intelligent Engine Technologies Ant Group New York</orgName>
								<address>
									<region>US</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Graph Regularized Point Process Model For Event Propagation Sequence</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-11-21">21 Nov 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2211.11758v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>event sequence</term>
					<term>graph regularization</term>
					<term>point process</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Point process is the dominant paradigm for modeling event sequences occurring at irregular intervals. In this paper we aim at modeling latent dynamics of event propagation in graph, where the event sequence propagates in a directed weighted graph whose nodes represent event marks (e.g., event types). Most existing works have only considered encoding sequential event history into event representation and ignored the information from the latent graph structure. Besides they also suffer from poor model explainability, i.e., failing to uncover causal influence across a wide variety of nodes. To address these problems, we propose a Graph Regularized Point Process (GRPP) that can be decomposed into: 1) a graph propagation model that characterizes the event interactions across nodes with neighbors and inductively learns node representations; 2) a temporal attentive intensity model, whose excitation and time decay factors of past events on the current event are constructed via the contextualization of the node embedding. Moreover, by applying a graph regularization method, GRPP provides model interpretability by uncovering influence strengths between nodes. Numerical experiments on various datasets show that GRPP outperforms existing models on both the propagation time and node prediction by notable margins.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Multitudes of irregular event sequences, labeled with event timestamps and types, are being generated in digital world at every moment. For example, activities of transactions in financial markets, purchases in e-commerce platforms, visits to hospitals and postings in social medias can all be formulated as discrete event sequences happening at irregular intervals. It The 4th to 7th authors are corresponding authors. is essential to model these complex event sequence dynamics so that accurate prediction or intervention can be carried out subsequently depending on the context.</p><p>Point process <ref type="bibr" target="#b0">[1]</ref>, characterized by the intensity function that measures the instantaneous probability of occurrence, has long been the standard tool in modeling event sequences. Poisson process <ref type="bibr" target="#b8">[9]</ref>, a simple type of point process, has a constant intensity function and Hawkes process <ref type="bibr" target="#b3">[4]</ref> constructs the conditional intensity function to recognize self-excitation between events as an enhancement to Poisson process. Recently, RNN-based models, e.g.,Recurrent Marked Temporal Point Process (RMTPP) <ref type="bibr" target="#b1">[2]</ref> and Neural Hawkes Process (NHP) <ref type="bibr" target="#b6">[7]</ref>, are proposed to model the dynamics of event sequences, which achieve significant progresses in learning and prediction tasks. Besides the variants of attention-based Hawkes process <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b18">[19]</ref> use deep attention structure to capture the long-term dependencies of the sequences.</p><p>In this paper, we focus on the problem of modeling the dynamics of graph event sequence, where the event propagates in a directed weighted graph and the mark is denoted as the node in the graph. The examples of such sequences include meme (news articles and blog posts) cascade sequences over public media, where the websites correspond to the nodes in the graph network. As denoted in <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b14">[15]</ref>, abstracting the information from the mark (i.e., event type) space helps discover the structure in the point process effectively and efficiently. Sharing the same motivation, we build a graph propagation process that encodes evolving structural properties of the observed sequence into the node embedding of the graph, and construct a latent intensity model, which uses Fig. <ref type="figure">1</ref>: An example of two event propagation sequences: (v 1 , v 2 , v 3 , v 5 ) and (v 4 , v 3 , v 2 ) in the graph, where each node corresponds to a specific event type and each edge represents influences between the two event types. v i denotes the event propagating to the i-th node in the graph.</p><p>the learnt node embedding to capture both short-term and long-term complex dependencies of the sequences. Besides, motivated by the work in <ref type="bibr" target="#b17">[18]</ref> that incorporates social structure as constraints to learn influence patterns between users in social networks, we integrate the prior structural knowledge into the modeling process to learn a more reasonable and interpretable influence pattern between nodes.</p><p>We propose a graph regularized point process (GRPP) to model event propagation sequences. Compared with existing works, we make two main contributions:</p><p>• To capture the mutual effects of the changing graph structure and temporal interactions between nodes, we employ two sub-modules to model separately the structural and temporal components of the event propagation process in graph: 1) a graph propagation model that characterizes the event interactions across nodes with neighbors and inductively learns node representations; 2) a temporal attentive intensity model, whose excitation and time decay factors of the past events imposing on the current event are constructed via contextualization of the node embedding, which greatly facilitates the learning of dependencies from the distant past.</p><p>• By leveraging a regularization method, which imposes a prior connection graph to guide the learning of the mutually-exciting coefficients, a.k.a, infectivity matrix, GRPP is able to maintain the model interpretability <ref type="bibr" target="#b15">[16]</ref> by uncovering causal influence between nodes. The uncovered strength of influence is critical as it provides the understanding of how the model works internally and facilitates decision making.</p><p>The proposed GRPP model demonstrates consistent improvement over state-of-the-art baselines on both synthetic and real datasets on both the learning and prediction tasks. The code will be released on Github upon the internal approval of the company.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. BACKGROUND AND PRELIMINARIES</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Point Process</head><p>A point process <ref type="bibr" target="#b0">[1]</ref> is a stochastic process whose realization consists of a list of discrete events localized in times {t i } i∈N * where ∀i ∈ N * , t i &lt; t i+1 . A point process can be best characterized by a conditional intensity function</p><formula xml:id="formula_0">λ t := lim ∆→0 E[Nt+∆-Nt|Ht] ∆</formula><p>, where</p><formula xml:id="formula_1">E [N t+∆ -N t |H t ]</formula><p>represents the expected number of events that occur in the interval (t, t + ∆] given history H t . The univariate Hawkes process has a conditional intensity λ t = µ + ti&lt;t g(t -t i ) , where µ is the base intensity, t i is the i-th event's occurrence time and g(•) is a kernel function measuring the influence of the previous events on the current event. The univariate Hawkes process can be extended to multi-dimensional Hawkes Process (MHP) to handle multiple types of events happening sequentially. Specifically, for a Kdimensional event sequence, the conditional intensity function of the i-th dimension is</p><formula xml:id="formula_2">λ i (t) = µ i + K j=1 α ij tj &lt;t g(t -t j ),<label>(1)</label></formula><p>where µ i is the base intensity of the dimension i, α ij ∈ A measures the causal influence of dimension j to dimension i, and A ∈ R K×K is called the infectivity matrix across event dimensions <ref type="bibr" target="#b15">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Difference and Connection to Existing Works</head><p>From the perspective of the methodology, we consider existing works <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b18">[19]</ref> to be related to our work as they leverage the information of graph structures. However, there are both differences and similarities between our work and the existing works.</p><p>Specifically, <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b18">[19]</ref> propose a spatial-temporal point process by encoding spatial graph information from the extra mark of the event tuple which denotes geographic locations, i.e., (t i , v i , ξ i ), where t i , v i , ξ i signify the event timestamp, the event type and the geographic information, respectively. Meanwhile, the event description in our problem is confined to only the timestamp and type, i.e., (t i , v i ) without introducing any extra mark, while the graph we build has nodes that correspond to event types instead of geographic locations. While the recent work <ref type="bibr" target="#b14">[15]</ref> proposes a Graph Biased Temporal Point Process (GBTPP) which employs the combination of RNN and graph embedding, there are essential differences between GBTPP and our work. Specifically, we propose a graph model with second-order proximity preserved, via a graph attention mechanism, to incorporate the influence between nodes and the node embedding is dynamically updated while GBTPP learns a graph only with first-order proximity and the node embedding is statically learnt in a separate stage, where the information of the dynamics between the events in different nodes is lost. We also utilize the latent attentive intensity model to incorporate historical influence of events while GBTPP constructs the intensity function without considering the complex dependencies.</p><p>Furthermore, different from the concurrent work <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b18">[19]</ref>, which applies the transformer structure to model the point process, the intensity mechanism in our model evolves in a latent space instead of the actual event space whose dimension equals to the number of event types, which is often hard to process under large or sparse feature space. Lastly, <ref type="bibr" target="#b11">[12]</ref> focuses on graph representation learning over temporal dynamic graph as a latent mediation process while our model targets at learning a point process with interpretability with strong prediction accuracy by utilizing the graph information.</p><p>Table <ref type="table" target="#tab_0">I</ref> provides qualitative comparison between the most related state-of-the-art models and our framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Notations and Problem Formulation</head><p>We assume a system with K event types, denoted as K = {1, 2, ..., K} and there exists a latent directed graph G = (V, E) with node set V = K and edge set E = {e ij } K×K . The nodes in G correspond to the dimensions of the event space and the edges signify the strength of the endogenous influence across different nodes. Specifically, edge e uv indicates that the node u is the parent of the node v (i.e., an event with type u could cause an event with type v). Following the previous work <ref type="bibr" target="#b13">[14]</ref>, we only consider the first parent as the true parent. Suppose we are given an observed event propagation sequence</p><formula xml:id="formula_3">O t = {(t i , v i ) : t i &lt; t} N</formula><p>i=1 of N events up to time t, where each event propagates to node v i ∈ K at time t i . For the convenience of notation, we use k to denote the index of nodes in graph G, or equivalently the index of event types in K.</p><p>Given the observed event sequence O t , we aim to recover the latent graph G, and learn a predictor that predicts next event's propagation time and node by leveraging the structural information of G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PROPOSED MODEL: GRPP</head><p>The main idea of GRPP is to build a unified architecture that ingests evolving information over graphs to uncover the latent structure of the point process. Therefore, inspired by the work in <ref type="bibr" target="#b11">[12]</ref>, we design a point process model which is parameterized with inductive node representations. By leveraging the node representations, we construct an attentive intensity model that captures both the temporal dynamics and structural dependencies in the event sequence. Specifically, GRPP consists of a positional encoder for producing the orderdependent embedding for the event sequence and two specially designed modules:</p><p>• A graph propagation model for parameterizing the event propagation across nodes and learning node representations.</p><p>• An attentive intensity model for capturing the longterm and complex dependencies intrinsic to the event propagation sequence.</p><p>A. Model Architecture 1) Sequential Encoder: For an event propagating from node u to v at t i , we first use a linear embedding layer to obtain a unique dense embedding for event type x v ti ∈ R m , and then pass it to a positional encoding layer composed of LSTM cells, to capture the order-dependent information of the event sequences. Specifically, the encoding layer maps the event type embedding x ti ∈ R m to a positional encoding vector</p><formula xml:id="formula_4">d v ti ∈ R d by d v ti = LSTM([d u ti-1 ; x v ti ])</formula><p>We prefer LSTM to other structure because the cell state sums activities over time, which overcomes the problem of diminishing gradients and at the same time better captures long-term dependencies of the event sequences than vanilla RNNs.</p><p>2) Graph Propagation: The rationale of this sub-model is that the observed propagation sequences are the realizations of a latent point process that governs the changes in the topological structure of graph and interactions between the nodes in the graph. After the occurrence of an event, we update its corresponding node representation to capture the effect of propagation based on the principles of localized embedding propagation as well as self-propagation <ref type="bibr" target="#b11">[12]</ref>.</p><p>Given the sequential embedding d v ti , we evolve its node representation vector h v ∈ R d as:</p><formula xml:id="formula_5">h v ti = W 1 s u ti-1 Local propagation + W 2 h v ti-1 Self-propagation + W 3 d v ti Exogenous drive<label>(2)</label></formula><p>W 1 s u ti-1 corresponds to the localized propagation process and s u ti-1 = z∈Nu q z h z ti-1 is obtained from an aggregation on node u's neighborhood N u with a graph attention method proposed in <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b19">[20]</ref>, where q z = exp(score(h</p><formula xml:id="formula_6">u t i-1 ,h z t i-1 ) p∈Nu exp(score(h u t i-1 ,h p t i-1</formula><p>)) signifies the attention weight for the neighbour z during the propagation and score is the attention score method <ref type="bibr" target="#b20">[21]</ref>. The localized propagation process, where the information propagates from u's neighbours to v, captures the influence from the nodes at second-order proximity passing through the other node involved in the event.</p><p>W 2 h v ti-1 refers to a self-propagation that updates its own previous node embedding.</p><p>W 3 d v ti serves as an exogenous influence from the current event.</p><p>With our new setup, the representation of the involved node is updated to capture the rich structural properties of the event sequence. We then define a latent intensity function characterized by the temporally evolving node representations as follows.</p><p>3) Latent Intensity: Different from the concurrent work <ref type="bibr" target="#b16">[17]</ref>, <ref type="bibr" target="#b18">[19]</ref>, we model the intensity dynamics in a latent space R d . The latent conditional intensity λ h evolves as Hawkes process, except that the base intensity, the activation and decay factors are characterized using neural networks and are learnt from data. For simplicity, in this section, we use h i to denote the node embedding vector updated in Equation 2 and ignore the notation of its corresponding node. Predicts Times Predicts Nodes (Types) Å Fig. <ref type="figure">2</ref>: Framework of GRPP: Assume an observed sequence propagating through nodes v 1 , v 2 , v 3 , v 5 sequentially. They are fed through an LSTM layer to obtain sequential encodings, and then fed into a graph propagation module characterizing the event interactions across nodes with neighbors, while inductively learning node representations. In the end, an attentive module captures the intensity dynamics in the latent space. The output is the prediction of the next propagation time and node. For the convenience of notation, we use v i to denote the event propagating to the i-th node in the graph. For t ∈ [t i , t i+1 ), we define the latent conditional intensity vector λ h t ∈ R d as</p><formula xml:id="formula_7">λ h t = i-1 j=0 α j,i δ j,i (t -t j ) + µ ti , i ≥ 1<label>(3)</label></formula><formula xml:id="formula_8">µ ti = σ(W µ h ti + b µ</formula><p>) is the base intensity vector, representing the probability of occurrence of event propagation without considering the history information.</p><p>α j,i = β j,i h tj represents the initial excitation of the jth event on the i-th event. It is computed as the weighted hidden representation with β j,i , which is the attention weight measuring the relevance of corresponding node representation of j-th event and the i-th event, calculated using a softmax transformation of ω j,i in the form of a score:</p><formula xml:id="formula_9">ω j,i = V tanh(W ω [h ti ; h tj ]), β j,i = ω j,i i-1 u=0 ω u,i<label>(4)</label></formula><p>Note that α j,i aims to capture the long-term dependencies of excitation effects over the propagation sequence. Besides, during the learning process, α j,i could become negative to capture the inhibition effect <ref type="bibr" target="#b6">[7]</ref> because β j,i ∈ (0, 1) and h ti ∈ (-1, 1).</p><formula xml:id="formula_10">δ j,i = σ W δ ([h tj ; h ti ]</formula><p>)) + b δ represents the time decay factor. Specifically, we directly encode the j-th event's and the i-th event's node representations, multiplied by the exponential decay, to measure the relative time decay influence of the j-th event on the i-th event.</p><p>Finally, we apply a Feed-Forward Network (FFN) with softplus activation to map the latent intensity λ h ti ∈ R d to the event intensity λ t ∈ R K .</p><p>4) Graph Regularization: Using H ∈ R K×d to denote the node embedding matrix and Ω ∈ R d×d a matrix to be learned. Given the learned node embedding, we compute the adjacent matrix A of G as A = HΩH T . By definition, the edges of G describe the influence between nodes, and therefore the adjacent matrix A coincides with the infectivity matrix defined in Equation <ref type="formula" target="#formula_2">1</ref>.</p><p>Adopting the learning framework proposed in spatialtemporal point process <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b18">[19]</ref>, we introduce a connection matrix E = {e ij } ∈ R K×K as a prior to represent the local causal relations which exists pervasively and agrees with human intuition across different event dimensions. For example, an intense sports game is more likely to cause a rest than going to the movies. As the common practice in previous works <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, given the observed sequence, we use the empirical estimation for the entries in E, i.e., e ij = Nij Nmax where N ij is the number of events propagated from node i to j and N max is the total number of event propagated between the two node. Finally we adopt the KL-divergence as the distance measure to minimize the difference between the E and A so that the connection matrix is used as a constraint to guide the learning of infectivity matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Training and Inference</head><p>We integrate the prior structural knowledge into modeling the event sequence by imposing the connection matrix E as constraints in learning infectivity matrix A, with which we formulate the loss function as follows:</p><formula xml:id="formula_11">min -L nll + γL graph<label>(5)</label></formula><p>, where L nll = -ti&lt;T log λ ui ti + T 0 λ t dt is the negative log-likelihood <ref type="bibr" target="#b0">[1]</ref>, <ref type="bibr" target="#b9">[10]</ref> for event sequences, γ is a hyperparameter and L graph = A -E KL is a regularization term clarified previously in the beginning of Section III-A4. Model parameters are learned by optimizing Equation ( <ref type="formula" target="#formula_11">5</ref>) with the backpropagation through time (BPTT) <ref type="bibr" target="#b14">[15]</ref> technique, which maintains the dependencies between events while avoiding gradient-related problems. The density of the next propagation time is p i+1 (t|H i+1 ) = λ(t) exp -t ti λ(s)ds . The predictions of the next propagation's time and node are obtained by taking the expectation, i.e., ti+1 =</p><formula xml:id="formula_12">∞ ti tp i+1 (t)dt, ki+1 = arg max k ∞ ti λ k (t) λ(t) p i+1 (t)dt.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTS</head><p>A. Experimental Setup 1) Datasets and Metrics: We conduct experiments using two synthetic datasets and two public datasets, i.e., Higgs <ref type="bibr" target="#b14">[15]</ref> and Meme <ref type="bibr" target="#b4">[5]</ref>, to evaluate the predictive performance of the proposed model. We use accuracy ratio and RMSE as the metrics for the propagation node and time prediction tasks. To clarify again, the marks correspond to the nodes in the graph.</p><p>• Synthetic datasets The synthetic data of graph event propagation process are generated from the multivariate Hawkes process (MHP), which has been widely used to model the generative process of user behavior of social networks <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b15">[16]</ref>. In our setting, each dimension of Hawkes process corresponds to an individual node and the causal influence between nodes are explicitly modeled. Following the same setup as <ref type="bibr" target="#b14">[15]</ref>, we simulate a 10-dimensional MHP and a 100-dimensional MHP (syn-10d and syn-100d). The base intensity is sampled from a uniform distribution between [0, 0.001] and the infectivity matrix is generated from A = UV T , where U and V are 10 × 1 (and 100 × 9) matrix with entries between [(i -1) + 1 : (i + 1), i] (and [10(i -1) + 1 : 10(i + 1), i] if U and V are 100 × 9), i = 1, ..., 9 sampled uniformly between [0, 0.1] and all the other entries are assigned zero.</p><p>• Higgs The Higgs dataset has been built after monitoring the spreading processes on Twitter before, during and after the announcement of the discovery of a new particle with the features of the elusive Higgs boson on 4th July 2012. As the common practice in previous work <ref type="bibr" target="#b14">[15]</ref>, we study the retweet propagation process by using the largest strongly connected component which contains 984 nodes (users) and 3,850 edges in the retweet network and train the model on the retweet activities. • Meme The dataset tracks meme diffusion process over public media, containing millions of news articles and blog posts. Following the same procedure as <ref type="bibr" target="#b14">[15]</ref>, we extract top 500 popular sites and 62,000 meme diffusion cascades among them. We also study the meme propagation process between websites.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2) Baselines and Training Details:</head><p>We herein evaluate the predictive performances of our model against • Multivariate Hawkes Process(MHP) <ref type="bibr" target="#b3">[4]</ref>: a self-exciting multivariate point process, which assumes that past events can additively increase the probability of subsequent events. • Recurrent Marked Temporal Point Process (RMTPP) <ref type="bibr" target="#b1">[2]</ref>: a RNN-based model to learn a representation of influences from past events. • Neural Hawkes Process (NHP) <ref type="bibr" target="#b6">[7]</ref>: a continous-LSTM model to capture the dependencies of events. • Graph Biased Temporal Point Process (GBTPP) <ref type="bibr" target="#b14">[15]</ref>: the most related model which leverages the structural information from graph representation learning to model graph propagation sequences. Each dataset is splited into train set, validation set and test set with a ratio of 3:1:1. We conduct all the experiments with a hyper-parameter grid search strategy and choose the hyperparameters that achieve the best results on the validation dataset. We use the Adam optimizer to train the model. The batch size is set to 256, and the learning rate is set as 0.001. The dropout rate in LSTM is set to 0.2. The dimension of hidden state of all LSTM units d was set as 128, and the embedding dimension of the event m is set as 128 as well. The regularization parameter µ l = 0.01.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Experiment Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1) Comparison of Prediction Accuracy on Synthetic and</head><p>Real Datasets: As shown in Fig. <ref type="figure" target="#fig_0">3</ref>, GRPP outperforms the baselines on both synthetic datasets and real datasets. The datasets we adopted vary significantly in the number of nodes, i.e., synthetic-10d has only 10 nodes while Higgs has 900 nodes. Specifically, in all datasets, GRPP improves upon GBTPP by 1.5% -2% for propagation node prediction and also a notable margin for propagation time prediction. The major innovations that contribute to the advantage of GRPP compared with GBTPP lie in the following:</p><p>• GRPP employs a graph propagation mechanism where the node representation preserves the second-order proximity, while GBTPP only preserves the first-order prox-syn-10d   imity. In short, GRPP captures richer structural properties than GBTPP. • GRPP uses an attentive intensity model to capture longterm dependencies intrinsic to a graph sequence while GBTPP applies a recurrent structure that may fail in modeling interactions between two events located far in the temporal domain.</p><p>Not surprisingly, neural models (RMTPP, NHP, GBTPP and GRPP) have better performance than the conventional model MHP which makes strong assumptions on the generative process of the data while neural models use more expressive recurrent networks to learn the process from the propagation history. We remark that among the neural models, GRPP and GBTPP outperform the others by modeling the structural information of the graph. Similar to the finding in <ref type="bibr" target="#b14">[15]</ref>, our result verifies the hypothesis that graph event sequence modeling, as a special case of event sequence modeling, requires a suitable model to incorporate the structural information.</p><p>2) Model Interpretability -Graph Structure Recovery: As described in the above section, the node representation contributes to uncover the infectivity matrix, or equivalently the adjacency matrix A of G, with the guidance of the graph regularization method. For the illustration purpose, we visualize the infectivity matrix recovered (right) during the learning of syn-10d data, compared with the ground-truth of the generative Hawkes process (left) in Fig. <ref type="figure" target="#fig_1">4</ref>. The figure shows that we closely recover the influence between nodes and therefore maintain the model interpretability.</p><p>3) Ablation Study: We conduct an ablation study on Meme to validate the effectiveness of key components that contribute to the improved outcomes of our proposed model. We name GRPP removing different components as follows:</p><p>• woGP: GRPP without graph propagation model or graph regularizer, i.e., we directly pass the sequence encoding vector into the intensity model without any regularization during the training. • woAT: GRPP without latent attentive intensity model.</p><p>Specially, the excitation and decay factor of the intensity process is directly determined by the closest node embedding instead of a weighted sum of distant node embedding.  see that both the graph propagation and the latent intensity mechanisms contribute to the model performance. Specifically, GRPP-woAT achieves the worst performance in both training and prediction, which proves the effectiveness of latent intensity model that captures the long-term dependencies. The effect of structural propagation is evident as well: GRPP-woGP achieves worse performance because it fails to incorporate the learnt node representation into the model. Consistent with the discussion above, building the attention over the positional encoder states is not the most suitable model for graph event sequences.</p><p>V. CONCLUSIONS</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Propagation prediction performances on synthetic and real datasets. Based on a same train-valid-test splitting ratio, each dataset is sampled five times to produce different train, validation and test sets. Error bars are generated according to these experiments.</figDesc><graphic coords="6,93.76,248.83,283.46,129.22" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Comparison of infectivity matrix on syn-10d dataset and recovered graph structure.</figDesc><graphic coords="6,394.17,318.63,127.56,59.42" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5</head><label>5</label><figDesc>summarizes our experimental results. As shown, we</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: Predictive performance of variants of GRPP fitted on Meme.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="4,138.93,175.67,334.14,198.99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE I :</head><label>I</label><figDesc>Comparison of our model with state-of-the-art approaches</figDesc><table><row><cell>Key Properties</cell><cell>Our Model</cell><cell>GBTPP</cell><cell>RMTPP</cell></row><row><cell></cell><cell></cell><cell></cell><cell>/NHP</cell></row><row><cell>Models Graph Propagation</cell><cell></cell><cell></cell><cell>×</cell></row><row><cell>Graph Information</cell><cell>2nd-order Proximity</cell><cell>1st-order Proximity</cell><cell>×</cell></row><row><cell>Learns Node Representation</cell><cell>Dynamic</cell><cell>Static</cell><cell>×</cell></row><row><cell>Attention Mechanism</cell><cell>Attentive Latent Intensity</cell><cell>×</cell><cell>×</cell></row><row><cell>Graph Recovery</cell><cell></cell><cell>×</cell><cell>×</cell></row><row><cell>(Causal Matrix Recovery)</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			
			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We presented a novel neural architecture with a graph regularized procedure to model the event propagation process. We employed a graph propagation model and a latent intensity model to capture the structural and temporal dynamics of the event sequence while maintaining the model interpretability. Extensive experiments over real-world datasets have proved the advantages of our model compared to conventional methods and state-of-the-art methods.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">An introduction to the theory of point process: general theory and structure</title>
		<author>
			<persName><forename type="first">D</forename><surname>Daley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Vera-Jones</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Recurrent marked temporal point processes: embedding event history to vector</title>
		<author>
			<persName><forename type="first">N</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Upadhyay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gomez-Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016-08">August 2016</date>
			<publisher>ACM SIGKDD</publisher>
			<biblScope unit="page" from="1555" to="1564" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Coevolve: A joint point process model for information diffusion and network co-evolution</title>
		<author>
			<persName><forename type="first">M</forename><surname>Farajtabar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">G</forename><surname>Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="page" from="1954" to="1962" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Spectra of some self-exciting and mutually exciting point process</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Hawkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="83" to="90" />
			<date type="published" when="1971">1971</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Snap datasets: Stanford large network dataset collection</title>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<pubPlace>Stanford</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Exploiting graph regularized multidimensional hawkes processes for modeling events with spatio-temporal characteristics</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<biblScope unit="page" from="2475" to="2482" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The neural hawkes process: A neurally selfmodulating multivariate point process</title>
		<author>
			<persName><forename type="first">H</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Eisner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6754" to="6764" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Lecture note: temporal point process and the conditional intensity function</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Rosmussen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="93" to="102" />
		</imprint>
		<respStmt>
			<orgName>Aalborg University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Intensitatsschwankungen im fernsprechverker, Ericsson Technics</title>
		<author>
			<persName><forename type="first">C</forename><surname>Palm</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1943">1943</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Bayesian inference for hawkes processes</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">G</forename><surname>Rasmussen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Methodology and Computing in Applied Probability</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="623" to="642" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Dyrep: learning representations over dynamic graphs</title>
		<author>
			<persName><forename type="first">R</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Farajtabar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Biswal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dyrep: learning representations over dynamic graphs</title>
		<author>
			<persName><forename type="first">R</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Farajtabar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Biswal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Time-dependent representation for neural sequence prediction</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning latent process from high-dimensional event sequences via efficient sampling</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3847" to="3856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Modeling event propagation via graph biased temporal point process</title>
		<author>
			<persName><forename type="first">W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning time series associated event sequences with recurrent point process networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Farajtabar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks And Learning Systems</title>
		<imprint>
			<biblScope unit="page" from="3124" to="3136" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Self-attentive hawkes processes</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lipani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Kirnap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Yilmaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2020-03">March, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning social infectivity in sparse lowrank networks using multi-dimensional hawkes processes</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Transformer hawkes process</title>
		<author>
			<persName><forename type="first">S</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2020-03">March, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Effective Approaches to Attention-based Neural Machine Translation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EMNLP</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
