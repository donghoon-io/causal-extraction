<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Causal Question Answering with Reinforcement Learning</title>
				<funder ref="#_HhgtEy5">
					<orgName type="full">Paderborn Center for Parallel Computing</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-03-25">25 Mar 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Lukas</forename><surname>BlÃ¼baum</surname></persName>
							<email>lukasbluebaumb94@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Paderborn University Paderborn</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Stefan</forename><surname>Heindorf</surname></persName>
							<email>heindorf@uni-paderborn.de</email>
							<affiliation key="aff1">
								<orgName type="institution">Paderborn University Paderborn</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Causal Question Answering with Reinforcement Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-03-25">25 Mar 2024</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3589334.3645610</idno>
					<idno type="arXiv">arXiv:2311.02760v2[cs.AI]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Question answering</term>
					<term>Causality graphs</term>
					<term>Reinforcement learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Causal questions inquire about causal relationships between different events or phenomena. They are important for a variety of use cases, including virtual assistants and search engines. However, many current approaches to causal question answering cannot provide explanations or evidence for their answers. Hence, in this paper, we aim to answer causal questions with a causality graph, a largescale dataset of causal relations between noun phrases along with the relations' provenance data. Inspired by recent, successful applications of reinforcement learning to knowledge graph tasks, such as link prediction and fact-checking, we explore the application of reinforcement learning on a causality graph for causal question answering. We introduce an Actor-Critic-based agent which learns to search through the graph to answer causal questions. We bootstrap the agent with a supervised learning procedure to deal with large action spaces and sparse rewards. Our evaluation shows that the agent successfully prunes the search space to answer binary causal questions by visiting less than 30 nodes per question compared to over 3,000 nodes by a naive breadth-first search. Our ablation study indicates that our supervised learning strategy provides a strong foundation upon which our reinforcement learning agent improves. The paths returned by our agent explain the mechanisms by which a cause produces an effect. Moreover, for each edge on a path, our causality graph provides its original source allowing for easy verification of paths.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Causal question answering addresses the problem of determining the causal relations between given causes and effects <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b18">19]</ref>. This involves examining whether a causal relation exists and how causal relations can be explained in terms of intermediate steps. Examples of such questions include "Does pneumonia cause anemia?" and "How does pneumonia cause death?" Nowadays, the necessity to answer causal questions arises in various domains. For example, users often seek answers to causal questions from virtual assistants like Alexa or from search engines <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b27">28]</ref>. Reasoning via chains of causal relations is crucial for argumentation <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b43">44]</ref> and automated decision-making <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b18">19]</ref>, too, e.g., to arrive at better answers and to gain a deeper understanding.</p><p>The literature started to introduce approaches for causal question answering <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b37">38]</ref>. However, they lack explanations and verifiability of their answers. Only recently, the introduction of CauseNet <ref type="bibr" target="#b12">[13]</ref>, a large-scale knowledge graph (KG) with causal relations between noun phrases and with context information, provides new opportunities to build effective, verifiable causal question answering systems that we exploit in this work.</p><p>Inspired by the successful application of reinforcement learning to KGs on different tasks such as link prediction <ref type="bibr" target="#b48">[49]</ref>, factchecking <ref type="bibr" target="#b5">[6]</ref>, conversational question answering <ref type="bibr" target="#b17">[18]</ref>, and multihop reasoning <ref type="bibr" target="#b44">[45]</ref>, in this paper, we explore whether we can model the causal question answering task as a sequential decision problem over a causality graph. We train a reinforcement learning agent that learns to walk over the graph to find good inference paths to answer binary causal questions. We implement the agent via the Synchronous Advantage Actor-Critic (A2C) algorithm <ref type="bibr" target="#b26">[27]</ref> and use generalized advantage estimation (GAE) <ref type="bibr" target="#b35">[36]</ref> to compute the advantage. To address the challenge of a large action space, we bootstrap the agent with a supervised learning procedure <ref type="bibr" target="#b48">[49]</ref> where the agent receives expert demonstrations to understand what good paths look like.</p><p>We evaluate our approach both on causal questions from Se-mEval <ref type="bibr" target="#b13">[14]</ref> as well as a novel dataset constructed from the MS MARCO <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b27">28]</ref> dataset, which consists of questions asked to search engines. While the latter is skewed to questions with the answer "yes", the former is balanced and contains an equal number of questions that are to be answered with "yes" or "no". Our evaluation demonstrates that on both datasets, our agent can effectively prune the search space, considering only a small number of nodes per question-on average, less than 30 nodes per question. For comparison, a breadth-first search (BFS) visits over 3,000 nodes per question. Furthermore, our agent minimizes false positives achieving a precision of 0.89, whereas BFS achieves a precision of only 0.75 and the language model UnifiedQA <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref> only a precision of 0.5. Our experiments confirm that bootstrapping the agent via supervised learning establishes a strong foundation decreasing uncertainty and accelerating the learning process. The paths found by our agent can be used to explain the relations between cause and effect, including the option to report the original source of the causal relation <ref type="bibr" target="#b12">[13]</ref>.</p><p>To summarize our contributions: <ref type="bibr" target="#b0">(1)</ref> We introduce the first reinforcement learning approach for causal question answering on KGs;</p><p>(2) we introduce a supervised learning procedure for causal question answering to handle the challenge of the large action space and sparse rewards and accelerate the learning process; (3) we introduce a new causal question dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>In the following, we summarize related work regarding causal knowledge graphs, approaches for causal question answering, and approaches that apply reinforcement learning to reasoning tasks on knowledge graphs.</p><p>Causal Knowledge Graphs. ConceptNet <ref type="bibr" target="#b41">[42]</ref> is a general knowledge graph (KG) consisting of 36 relations between natural language terms, including a Causes relation. CauseNet <ref type="bibr" target="#b12">[13]</ref> and Cause Effect Graph <ref type="bibr" target="#b21">[22]</ref> specifically focus on causal relations extracted via linguistic patterns from web sources like Wikipedia and ClueWeb12. ATOMIC <ref type="bibr" target="#b34">[35]</ref> focuses on inferential knowledge of commonsense reasoning in everyday life. It consists of "If-Event-Then-X" relations based on social interactions or real-world events. ATOMIC 20  20 <ref type="bibr" target="#b15">[16]</ref> selects relations from ATOMIC and ConceptNet to create an improved graph while adding more relations via crowdsourcing. Instead, West et al. <ref type="bibr" target="#b46">[47]</ref> automate the curation of causal relations by clever prompting of a language model. Finally, CSKG <ref type="bibr" target="#b16">[17]</ref> builds a consolidated graph combining seven KGs, including ConceptNet and ATOMIC. We use CauseNet because most of the other KGs are smaller and less focused on causal relations, e.g., CauseNet contains many more causal relationships than ConceptNet <ref type="bibr" target="#b12">[13]</ref>. Future work may involve combining multiple KGs.</p><p>Causal Question Answering. As of now, only few approaches tackle the causal question answering task. Most of them focus on binary questions, i.e., questions such as "Does X cause Y?" which expect a "yes" or "no" answer. Kayesh et al. <ref type="bibr" target="#b18">[19]</ref> model the task as a transfer learning approach. They extract cause-effect pairs from news articles via causal cue words. Subsequently, they transform the pairs into sentences of the form "X may cause Y " and use them to finetune BERT <ref type="bibr" target="#b7">[8]</ref>. Similarly, Hassanzadeh et al. <ref type="bibr" target="#b11">[12]</ref> employ large-scale text mining to answer binary causal questions introducing multiple unsupervised approaches ranging from string matching to embeddings computed via BERT. Sharp et al. <ref type="bibr" target="#b37">[38]</ref> consider multiple-choice questions of the form "What causes X?". First, they mine cause-effect pairs from Wikipedia via syntactic patterns and train an embedding model to capture the semantics between them. At inference time, they compute the embedding similarity between the question and each answer candidate. Dalal <ref type="bibr" target="#b3">[4]</ref>, Dalal et al. <ref type="bibr" target="#b4">[5]</ref> combine a language model with CauseNet <ref type="bibr" target="#b12">[13]</ref>. Given a question, they apply string matching to extract relevant causal relations from CauseNet. Subsequently, they provide the question with the causal relations as additional context to a language model. As other language model-based approaches, too, they cannot produce verifiable answers. None of them selects relevant paths in a causality graph via reinforcement learning.</p><p>Knowledge Graph Reasoning with Reinforcement Learning. In recent years, reinforcement learning on knowledge graphs has been successfully applied to link prediction <ref type="bibr" target="#b5">[6]</ref>, fact-checking <ref type="bibr" target="#b48">[49]</ref>, or question answering <ref type="bibr" target="#b32">[33]</ref>. Given a source and a target entity, Deep-Path <ref type="bibr" target="#b48">[49]</ref> learns to find paths between them. The training of Deep-Path involves two steps. First, it is trained via supervised learning and afterward via REINFORCE <ref type="bibr" target="#b47">[48]</ref> policy gradients. During inference time, the paths are used to predict links between entities or check the validity of triples. Subsequently, MINERVA <ref type="bibr" target="#b5">[6]</ref> improves on DeepPath by introducing an LSTM <ref type="bibr" target="#b14">[15]</ref> into the policy network to account for the path history. Moreover, MINERVA does not require knowledge of the target entity and is trained end-toend without supervision at the start. Lin et al. <ref type="bibr" target="#b22">[23]</ref> propose two improvements for MINERVA. First, they apply reward shaping by scoring the paths with a pre-trained KG embedding model <ref type="bibr" target="#b6">[7]</ref> to reduce the problem of sparse rewards. Second, they introduce a technique called action dropout, which randomly disables edges at each step. Action dropout serves as additional regularization and helps the agent learn diverse paths. M-Walk <ref type="bibr" target="#b38">[39]</ref> applies model-based reinforcement learning techniques. Like AlphaZero <ref type="bibr" target="#b40">[41]</ref>, M-Walk applies Monte Carlo Tree Search (MCTS) as a policy improvement operator. Thus, at each step, M-Walk applies MCTS to produce trajectories of an improved policy and subsequently trains the current policy to imitate the improved one. GaussianPath <ref type="bibr" target="#b44">[45]</ref> takes a Bayesian view of the problem and represents each entity by a Gaussian distribution to better model uncertainty.</p><p>Previous approaches for reinforcement learning on KGs <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b48">49]</ref> were designed for KGs with multiple relation types and used structural KG embeddings. In contrast, we tailor our approach to causality graphs where nodes are noun phrases, edges are all of the same type ("causes"), and each edge has provenance data associated with it. This results in the following key differences: (1) Different action space: As edge types do not provide any learning signal, our action space encompasses all entities in the causality graph.</p><p>(2) Different entity encoding: We use text embeddings instead of structural KG embeddings because causality graphs are text-centric, e.g., nodes are noun phrases. (3) Different action encoding: An action corresponds to the traversal of an edge that leads to an adjacent node. We encode an action as a = [s; e] where s is the sentence embedding of he sentence that the edge was extracted from and e is the embedding of the adjacent entity. (4) Different RL algorithm: We train our agent with the more advanced Synchronous Advantage Actor-Critic (A2C) algorithm instead of the REINFORCE algorithm used in previous works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">CAUSAL QUESTION ANSWERING WITH REINFORCEMENT LEARNING</head><p>In the following, we formulate the question-answering task as a sequential decision problem on a causality graph and define the environment of the reinforcement learning agent. Afterward, we present our reinforcement learning agent, including its network architecture, training procedure, search strategy, and bootstrapping approach via supervised learning.  <ref type="bibr" target="#b12">[13]</ref> showing the entity pneumonia together with its neighborhood containing causes and effects, where each edge depicts a cause relation. The numbers on the edges show the probability of taking this edge under the current policy ğœ‹ ğœƒ (ğ‘ ğ‘¡ |ğ‘  ğ‘¡ ). For brevity, we only show the relevant probabilities for the given paths. The lower part of the figure shows the possibility to combine our agent with a language model. In that setup, we provide the paths the agent learned as additional context to the language model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Definition</head><p>We are given a binary causal question ğ‘ in natural language that contains a cause, an effect, and is to be answered with "yes" or "no". An example is "Can X cause Y?" where X and Y represent a cause and effect, respectively. Moreover, we are given a causality graph K = {(â„, ğ‘Ÿ, ğ‘¡)} âŠ† E Ã— R Ã— E, where â„, ğ‘¡ âˆˆ E denote entities and ğ‘Ÿ âˆˆ R denotes a relation. In this paper, we assume that (1) entities are noun phrases, (2) edges are all of the same type (R = {ğ‘ğ‘ğ‘¢ğ‘ ğ‘’}), and (3) for each edge, we have the sentence the edge was originally extracted from. The task of the agent is to walk over the graph to answer the question.</p><p>In the following, we elucidate the binary causal question answering task on the knowledge graph K on the basis of the example in Figure <ref type="figure" target="#fig_0">1</ref>. The example shows an excerpt of a causal knowledge graph (CauseNet) and the binary causal question "Does pneumonia cause anemia?". In this question, pneumonia takes the role of the cause, and anemia the role of the effect. First, the cause and effect are linked to the graph. Therefore, we find entities ğ‘’ ğ‘ , ğ‘’ ğ‘’ âˆˆ E such that pneumonia maps to ğ‘’ ğ‘ and anemia to ğ‘’ ğ‘’ . Currently, we link them via exact string matching. However, more sophisticated strategies can be considered in future work <ref type="bibr" target="#b17">[18]</ref>. Consequently, starting from ğ‘’ ğ‘ , the agent has to find a path (ğ‘’ ğ‘ , ğ‘’ 1 , ğ‘’ 2 , . . . , ğ‘’ ğ‘’ ) with ğ‘’ ğ‘– âˆˆ E, where the agent arrives at the effect ğ‘’ ğ‘’ . 1 If the agent finds such a path, the question is answered with "yes" and otherwise with "no". For the example, a possible path is (pneumonia, sepsis, kidney failure, anemia). Afterward, we can inspect the path to get further insights into the relationship between cause and effect. 1 The path only shows the entities, because the graph contains only one relation type.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Environment</head><p>As done by related work <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b48">49]</ref>, we formulate the causal question answering task as a sequential decision problem on the knowledge graph K. The agent walks over the graph and decides which edge to take at each entity. Therefore, we define a Markov Decision Process (MDP) as a 4-tuple (S, A, ğ›¿, R). The MDP consists of the state space S, the action space A, the transition function ğ›¿ : S Ã— A â†’ S, and the reward function R : S Ã— A â†’ R. States. At each time step t, we define state ğ‘  ğ‘¡ = (q, ğ‘’ ğ‘¡ , e t , h t , ğ‘’ ğ‘’ ) âˆˆ S, where q represents the embedding of the question ğ‘, ğ‘’ ğ‘¡ âˆˆ E the current entity, and e t its embedding. The entity ğ‘’ ğ‘¡ is needed to define the action space, and its embedding e t is used as input to the agent's networks. Additionally, h t represents the path history of the agent and ğ‘’ ğ‘’ the entity corresponding to the effect found in the question ğ‘. Moreover, ğ‘’ 0 = ğ‘’ ğ‘ and h 0 = 0, where ğ‘’ ğ‘ is the entity corresponding to the cause of the question (e.g., pneumonia in the example in Figure <ref type="figure" target="#fig_0">1</ref>). The path history is represented by the hidden states of an LSTM.</p><p>Actions. The action space at each time step ğ‘¡ consists of all neighboring entities of the current entity in state ğ‘  ğ‘¡ . Therefore, the set of possible actions in state ğ‘  ğ‘¡ = (q, ğ‘’ ğ‘¡ , e t , h t , ğ‘’ ğ‘’ ) is defined as ğ´(ğ‘  ğ‘¡ ) = {ğ‘’ |(ğ‘’ ğ‘¡ , ğ‘Ÿ, ğ‘’) âˆˆ K} where ğ‘Ÿ = ğ‘ğ‘ğ‘¢ğ‘ ğ‘’. So only the current entity ğ‘’ ğ‘¡ is needed to define the action space ğ´(ğ‘  ğ‘¡ ). Note that while the additional components inside the state are not needed to define the action space, they are needed for other parts of the learning algorithm, as described below in Sections 3.3 and 3.4.</p><p>Our causality graph contains additional meta-information for each edge (ğ‘’ ğ‘¡ , ğ‘Ÿ, ğ‘’), e.g., the original sentence ğ‘  the edge was extracted from. When computing the embedding a t for an action ğ‘ ğ‘¡ , we concatenate the sentence embedding s with the embedding of the entity e, yielding the action embedding</p><formula xml:id="formula_0">a t = [s; e].</formula><p>As done by prior works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b32">33]</ref>, we add a special STAY action at each step, so the action space becomes ğ´(ğ‘  ğ‘¡ ) = ğ´(ğ‘  ğ‘¡ ) âˆª {ğ‘†ğ‘‡ ğ´ğ‘Œ }. When selecting this action, the agent stays at the current entity. This way, we can keep all episodes to the same length, even though different questions might require a different number of hops. Another option would be to add a stop action. However, in that case, we would have episodes of different lengths. <ref type="foot" target="#foot_0">2</ref> Moreover, we add inverse edges to the graph because our experiments showed that their addition increases the performance. In general, inverse edges allow the agent to undo wrong decisions and to reach nodes that could otherwise not be reached under a given episode length. We discuss some implications and tradeoffs of inverse edges in Section 5.</p><p>Transitions. As described above, the transition function is deterministic, so the next state is fixed after the agent selects an action. Let ğ‘  ğ‘¡ = (q, ğ‘’ ğ‘¡ , e t , h t , ğ‘’ ğ‘’ ) be the current state and ğ‘ ğ‘¡ âˆˆ ğ´(ğ‘  ğ‘¡ ) be the selected action in ğ‘  ğ‘¡ . Subsequently, the environment evolves via ğ›¿ (ğ‘  ğ‘¡ , ğ‘ ğ‘¡ ) to ğ‘  ğ‘¡ +1 = (q, ğ‘’ ğ‘¡ +1 , e t+1 , h t+1 , ğ‘’ ğ‘’ ), where ğ‘’ ğ‘¡ +1 = ğ‘ ğ‘¡ .</p><p>Rewards. The agent only receives a terminal reward at the final time step ğ‘‡ -1. Specifically, the agent receives a reward of</p><formula xml:id="formula_1">R (ğ‘  ğ‘‡ -1 , ğ‘ ğ‘‡ -1 ) = 1 if ğ‘  ğ‘‡ = (q, ğ‘’ ğ‘‡ , e T , h T , ğ‘’ ğ‘’ ) with ğ‘’ ğ‘‡ = ğ‘’ ğ‘’ . Con- versely, the agent receives a reward of R (ğ‘  ğ‘‡ -1 , ğ‘ ğ‘‡ -1 ) = 0 if ğ‘’ ğ‘‡ â‰  ğ‘’ ğ‘’ .</formula><p>Similarly, for all other time steps ğ‘¡ &lt; ğ‘‡ -1 the reward is 0 as well.</p><p>Path Rollouts -Episodes. We define a path rollout or episode as a sequence of triples containing a state, action, and reward. Assuming a path rollout length of ğ‘‡ , an example path rollout is: ((ğ‘  0 , ğ‘ 0 , ğ‘Ÿ 0 ), . . . , (ğ‘  ğ‘‡ -1 , ğ‘ ğ‘‡ -1 , ğ‘Ÿ ğ‘‡ -1 )). For brevity, the triples (ğ‘  ğ‘¡ , ğ‘ ğ‘¡ , ğ‘Ÿ ğ‘¡ ) might be written as pairs (ğ‘  ğ‘¡ , ğ‘ ğ‘¡ ) omitting the rewards ğ‘Ÿ ğ‘¡ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Network Architecture</head><p>We use a Long Short-Term Memory (LSTM) <ref type="bibr" target="#b14">[15]</ref> to parametrize our agent. Additionally, we experimented with a simple feedforward architecture but found that incorporating the path history is crucial for our needs. This aligns with previous research, where approaches such as MINERVA <ref type="bibr" target="#b5">[6]</ref> and SRN <ref type="bibr" target="#b32">[33]</ref> also used LSTMs and GRUs. Just CONQUER <ref type="bibr" target="#b17">[18]</ref> used a feedforward architecture, but they only considered paths of length one.</p><p>Let q âˆˆ R ğ‘‘ be the embedding of the question ğ‘ and E âˆˆ R | E | Ã—ğ‘‘ the embedding matrix containing the embeddings for each entity ğ‘’ âˆˆ E of the knowledge graph K. The parameter ğ‘‘ specifies the dimension of the embeddings. The LSTM is then applied as</p><formula xml:id="formula_2">h t = ğ¿ğ‘†ğ‘‡ ğ‘€ (0; [q, e c ]), if ğ‘¡ = 0 ğ¿ğ‘†ğ‘‡ ğ‘€ (h t-1 , [q; e t ]), otherwise<label>(1)</label></formula><p>where h t âˆˆ R 2ğ‘‘ represents the hidden state vector (history) of the LSTM, and [; ] is the vector concatenation operator. At each time step, the LSTM takes the previous history h t-1 and the concatenation of the question embedding q and the current node embedding e t âˆˆ R ğ‘‘ to produce h t . In the first time step, h 0 is initialized with the zero vector and e 0 = e c , where e c is the embedding of the entity corresponding to the cause found in the current question.</p><p>On top of the LSTM, we stack two feedforward networks: one for the policy network ğœ‹ ğœƒ (ğ‘ ğ‘¡ |ğ‘  ğ‘¡ ) and one for the value network ğ‘‰ ğœ“ (ğ‘  ğ‘¡ ). In Section 3.2, we defined the action space A (ğ‘  ğ‘¡ ) at time step ğ‘¡ and state ğ‘  ğ‘¡ = (q, ğ‘’ ğ‘¡ , e t , h t , ğ‘’ ğ‘’ ) to contain all neighbors of the entity ğ‘’ ğ‘¡ . Therefore, we introduce an embedding matrix A t âˆˆ R |ğ´(ğ‘  ğ‘¡ ) | Ã—2ğ‘‘ , where the rows contain the embeddings of the actions ğ‘ ğ‘¡ âˆˆ A (ğ‘  ğ‘¡ ). The output of the policy network ğœ‹ ğœƒ (ğ‘ ğ‘¡ |ğ‘  ğ‘¡ ) is computed as</p><formula xml:id="formula_3">ğœ‹ ğœƒ (ğ‘ ğ‘¡ |ğ‘  ğ‘¡ ) = ğœ (A t Ã— ğ‘Š 2 Ã— ğ‘…ğ‘’ğ¿ğ‘ˆ (ğ‘Š 1 Ã— h t )) ğ‘ ğ‘¡ âˆ¼ ğ¶ğ‘ğ‘¡ğ‘’ğ‘”ğ‘œğ‘Ÿğ‘–ğ‘ğ‘ğ‘™ (ğœ‹ ğœƒ (ğ‘ ğ‘¡ |ğ‘  ğ‘¡ ))<label>(2)</label></formula><p>where ğ‘Š 1 âˆˆ R â„Ã—2ğ‘‘ and ğ‘Š 2 âˆˆ R 2ğ‘‘ Ã—â„ are weight matrices with hidden dimension â„ and ğœ is the softmax operator. The final output of the policy network is a categorical probability distribution over all actions ğ‘ ğ‘¡ âˆˆ A(ğ‘  ğ‘¡ ). Similarly, the output of the value network</p><formula xml:id="formula_4">ğ‘‰ ğœ“ (ğ‘  ğ‘¡ ) is computed with the feedforward network ğ‘‰ ğœ“ (ğ‘  ğ‘¡ ) = ğ‘Š 4 Ã— ğ‘…ğ‘’ğ¿ğ‘ˆ (ğ‘Š 3 Ã— h t )<label>(3)</label></formula><p>whereğ‘Š 3 âˆˆ R â„Ã—2ğ‘‘ andğ‘Š 4 âˆˆ R 1Ã—â„ are weight matrices with hidden dimension â„, and the output is a scalar that estimates the future reward from state ğ‘  ğ‘¡ onwards. Overall, the weights of the LSTM are shared between the policy and value network, while each network has its own weights in the form of its feedforward head.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Training the Reinforcement Learning Agent</head><p>The training process involves pre-processing questions and linking them to entities of the causality graph. As causality graphs such as CauseNet <ref type="bibr" target="#b12">[13]</ref> do not contain negative information (see Section A.1 in appendix), we only train the agent on positive causal questions, i.e., questions whose answer is "yes". We remove all questions where the cause, effect, or both cannot be found in the causality graph.</p><p>Then we obtain embeddings for the entities from their textual representation, initialize agent weights, and sample path rollouts with the policy network. The training utilizes the Synchronous Advantage Actor-Critic (A2C) algorithm, with the policy network acting as the actor and the value network as the critic. The policy network update rule includes the generalized advantage estimate (GAE). As commonly done, we add an entropy regularization term to help the agent with the exploitation vs. exploration tradeoff. Simultaneously, the value network is updated. Further details on our training procedure for the policy network including the pseudocode can be found in Section A.3 in the appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Search Strategy</head><p>At inference time, the agent receives both positive and negative questions. To answer a given question, we sample multiple paths ğ‘ of length ğ‘‡ from the agent. If any path contains the entity ğ‘’ ğ‘’ , the agent answers the question with "yes", otherwise with "no". In case the cause, effect, or both cannot be found in the causality graph, the question is answered with "no" per default.</p><p>For each path rollout ((ğ‘  0 , ğ‘ 0 ), (ğ‘  1 , ğ‘ 1 ), . . . , (ğ‘  ğ‘‡ -1 , ğ‘ ğ‘‡ -1 )), the path that was taken on the graph consists of the entity ğ‘’ 0 in ğ‘  0 and the actions taken at each time step ğ‘¡, i.e., ğ‘ = (ğ‘’ 0 , ğ‘’ 1 , . . . , ğ‘’ ğ‘‡ ) where ğ‘ ğ‘¡ -1 = ğ‘’ ğ‘¡ âˆˆ E for ğ‘¡ &gt; 0. The probability of path ğ‘ is the product</p><formula xml:id="formula_5">P(ğ‘) = ğ‘‡ -1 ğ‘¡ =0 ğœ‹ ğœƒ (ğ‘ ğ‘¡ |ğ‘  ğ‘¡ )<label>(4)</label></formula><p>of the probabilities of taking action ğ‘ ğ‘¡ at state ğ‘  ğ‘¡ under the current policy ğœ‹ ğœƒ (ğ‘ ğ‘¡ |ğ‘  ğ‘¡ ) for ğ‘¡ âˆˆ {0, . . . ,ğ‘‡ -1}. Figure <ref type="figure" target="#fig_0">1</ref> shows an excerpt from CauseNet where each edge is annotated with the probability of taking this edge under the current policy.</p><p>To sample paths from the agent, we apply greedy decoding or beam search. Greedy decoding takes the action with the highest probability at each time step, i.e., ğ‘ğ‘Ÿğ‘” ğ‘šğ‘ğ‘¥ ğ‘ ğ‘¡ âˆˆ A (ğ‘  ğ‘¡ ) ğœ‹ ğœƒ (ğ‘ ğ‘¡ |ğ‘  ğ‘¡ ). In Figure <ref type="figure" target="#fig_0">1</ref>, the agent would select sepsis in the first time step, kidney failure in the second time step, and anemia in the third time step. However, one disadvantage of greedy decoding is its myopic behavior, i.e., it might miss high-probability actions in later time steps. Beam search tries to alleviate this problem by always keeping a set of the best partial solutions up to the current timestep. In our case, partial solutions are paths of length ğ‘¡, where ğ‘¡ is the current timestep. Furthermore, the paths are ranked by their probability, as defined in Equation <ref type="formula" target="#formula_5">4</ref>. Assuming a beam width of two, Figure <ref type="figure" target="#fig_0">1</ref> shows the two paths that are found by beam search for the example. After obtaining these paths, the agent examines each one to determine whether it includes the effect. If that is the case, the agent answers the question with "yes", otherwise with "no". In this example, anemia is found, so the agent answers with "yes".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Bootstrapping via Supervised Learning</head><p>Reinforcement learning algorithms often take a long time to converge due to their trial-and-error nature combined with large action spaces and sparse rewards <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b48">49]</ref>. Thus, the reinforcement learning agent can be bootstrapped, by first training it on a series of expert demonstrations. For example, AlphaGo <ref type="bibr" target="#b39">[40]</ref> trained the agent on demonstrations from expert Go players before continuing with their reinforcement learning algorithm. In our case, the expert demonstrations come from a breadth-first search (BFS) on the causality graph. First, we randomly select a subset ğ‘„ of size ğ›¼ â€¢ |ğ‘ğ‘¢ğ‘’ğ‘ ğ‘¡ğ‘–ğ‘œğ‘›ğ‘  | of the training questions, where ğ›¼ is a hyperparameter. Subsequently, we run a BFS on the cause ğ‘’ ğ‘ and effect ğ‘’ ğ‘’ of each question in ğ‘„ and build a path rollout for each found path. If a path rollout is shorter than the path rollout length ğ‘‡ , it is padded with the STAY action. <ref type="foot" target="#foot_2">3</ref>Next, we train the policy network ğœ‹ ğœƒ (ğ‘ ğ‘¡ |ğ‘  ğ‘¡ ) via REINFORCE</p><formula xml:id="formula_6">âˆ‡ ğœƒ ğ½ (ğœƒ ) = - 1 ğµ ğµ âˆ‘ï¸ ğ‘– ğ‘‡ -1 âˆ‘ï¸ ğ‘¡ =0 âˆ‡ ğœƒ log(ğœ‹ ğœƒ (ğ‘ ğ‘¡ |ğ‘  ğ‘¡ )) ğ‘Ÿ ğ‘¡ + ğ›½ğ» ğœ‹ ğœƒ (<label>5</label></formula><formula xml:id="formula_7">)</formula><p>where ğµ is the batch size, ğ‘‡ the path rollout length, and ğ» ğœ‹ ğœƒ the entropy regularization from Section 3.4. During supervised training, the reward ğ‘Ÿ ğ‘¡ is set to 1 at each step. Note that we only train the policy network ğœ‹ ğœƒ (ğ‘ ğ‘¡ |ğ‘  ğ‘¡ ) during supervised learning. Afterward, we further train both the policy and value networks via Algorithm 1, as explained in Section A.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EVALUATION</head><p>In this section, we present the evaluation of our approach. We start by providing an overview of the experimental setup, including the datasets, the baselines, evaluation measures, hyperparameter settings, and implementation details. Afterward, we compare our agent to two baselines on the binary causal question answering task. Next, we conduct an ablation analysis to evaluate the effectiveness of the different components of our approach and evaluate the effects of initial supervised learning. The code and data to reproduce our results are publicly available.<ref type="foot" target="#foot_3">foot_3</ref>,<ref type="foot" target="#foot_4">foot_4</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>Datasets. As causality graph, we employ CauseNet <ref type="bibr" target="#b12">[13]</ref>. As question-answering datasets, we employ subsets of causal questions from MS MARCO <ref type="bibr" target="#b27">[28]</ref> and SemEval <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b37">38]</ref>. To extract binary causal question from MS MARCO we extended an extraction mechanism from Heindorf et al. <ref type="bibr" target="#b12">[13]</ref> by including additional causal cue words <ref type="bibr" target="#b9">[10]</ref>. SemEval was curated by Sharp et al. <ref type="bibr" target="#b37">[38]</ref> by selecting a subset of 1730 word pairs from the semantic relation classification Baselines. We compare our agent with two kinds of baselines: (1) breadth-first search (BFS) on CauseNet and (2) direct question answering with language models. BFS performs an exhaustive search in the graph up to a certain depth and serves as a strong baseline. However, it must be noted that BFS can be applied effectively only to binary causal questions. Moreover, it needs to traverse many nodes in the graph whereas our reinforcement learning agent visits much fewer nodes. As a second baseline, we use UnifiedQA-v2 <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref> and OpenAI's GPT-v4 <ref type="bibr" target="#b28">[29]</ref>. UnifiedQA-v2 is a text-to-text language model based on the T5 architecture <ref type="bibr" target="#b33">[34]</ref> and achieved state-ofthe-art performance on multiple datasets. We chose UnifiedQA-v2 because it was used by the CausalQA <ref type="bibr" target="#b2">[3]</ref> benchmark for their evaluation. Its input consists of a question and additional contextual information as shown in Figure <ref type="figure" target="#fig_0">1</ref>. We experimented with three variants of UnifiedQA-v2: (1) with an empty context (UnifiedQA-v2), (2) with causal triples as context (UnifiedQA-v2-T) as done by <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>, (3) by using the provenance data available in CauseNet along paths from the cause to the effect (UnifiedQA-v2-P). For (2), all triples from CauseNet are obtained where the cause in the question matches the cause in CauseNet and the effect in the question matches the effect in CauseNet. For (3), we take advantage of the additional meta-information available in CauseNet <ref type="bibr" target="#b12">[13]</ref>. For each causal pair, CauseNet contains the original sentence from which the pair was extracted. Given a question ğ‘ and a path (ğ‘’ 1 , . . . , ğ‘’ ğ‘› ) our agent found for that question, we extract the original sentence for each causal pair (ğ‘’ ğ‘– , ğ‘’ ğ‘–+1 ), with 0 â‰¤ ğ‘– &lt; ğ‘›, on the path ğ‘. We concatenate the sentences for all paths and all pairs therein and input the sequence into the language model as context. For the GPT-v4 baseline, we use the prompt "Answer binary causal questions with 'Yes' or 'No"' and experiment with the three variants, too.</p><p>Evaluation Measures. We evaluate our agent for binary question answering using the standard classification measures accuracy, ğ¹ 1score, precision, and recall. Additionally, we evaluate our agent and the BFS baselines w.r.t. the number of unique nodes (entities) that are visited per question on average. Hyperparameter Optimization. We optimized hyperparameters using Optuna <ref type="bibr" target="#b0">[1]</ref> on the validation sets and subsequently retrained on the combined training and validation sets. We trained each agent for 2000 steps with a batch size of 128 and a learning rate of 0.0001. The hidden dimension of the feedforward heads was set to 2048. Additionally, we set the discount factor ğ›¾ to 0.99 and the ğœ† parameter of GAE to 0.95 <ref type="bibr" target="#b35">[36]</ref>. The weight ğ›½ for the entropy regularization was set to 0.01. During supervised learning, we used 300 training steps, a batch size of 64, and a supervised ratio ğ›¼ of 0.8. Moreover, we used a beam width of 50.</p><p>Implementation Details. We used the AdamW <ref type="bibr" target="#b24">[25]</ref> optimizer with gradient norm clipping <ref type="bibr" target="#b29">[30]</ref> at a value of 0.5. As knowledge graph we used CauseNet-Precision <ref type="bibr" target="#b12">[13]</ref> and to embed the entities and questions we used GloVe embeddings <ref type="bibr" target="#b31">[32]</ref>. We also experimented with BERT <ref type="bibr" target="#b8">[9]</ref>, RoBERTa <ref type="bibr" target="#b23">[24]</ref>, and E5 <ref type="bibr" target="#b45">[46]</ref>, but this did not lead to improved results in terms of accuracy and ğ¹ 1 -score. For the UnifiedQA-v2 baseline, we chose the base model <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>. All experiments were run on an NVIDIA A100 40GB. Additional details can be found in our GitHub repository (URL see above).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation of the RL Agent</head><p>The last column of Table <ref type="table">2</ref>, compares the number of visited nodes by our approach with a brute-force BFS. Our approach visits less than 30 nodes per question on average whereas a BFS visits over 3,000 nodes to answer binary questions with 3 or 4 hops. Thus, our approach effectively prunes the search space by 99%. The fact that the number of nodes visited by BFS barely increases from 3 to 4 hops can be attributed to the topology of CauseNet.</p><p>The first columns of Table <ref type="table">2</ref> compare the question-answering performance of our agent with the BFS and the language models UnifiedQA-v2 and GPT-v4. Our agent consistently achieves a high precision above or around 0.9. In terms of precision, our agent outperforms BFS, comes close to GPT-v4 on MS MARCO, and outperforms GPT-v4 on SemEval. In terms of recall, BFS achieves a slightly higher recall than our agent, and UnifiedQA-v2 and GPT-v4 achieve an even higher recall. The accuracy of BFS and our agent is higher on SemEval than on MS MARCO.</p><p>Unlike language models, our graph-based approach yields highprecision and verifiable answers as for each edge on a path, we can provide its original source on the web. Our recall is naturally lower than BFS as BFS performs an exhaustive search and positively answers a question if it can find a path, whereas our agent drastically prunes the search space and hence, occasionally misses a path. The fact that language models have an even higher recall can Table <ref type="table">2</ref>: Evaluation results of our agent on the MS MARCO and SemEval test sets compared to the BFS baseline and UnifiedQA-v2 baseline. The table reports the accuracy: A, F 1 -Score: F 1 , recall: R, precision: P, and the average number of nodes that were visited per question |Nodes|. The results for UnifiedQA-v2 using triples or paths of the agent as context are denoted as UnifiedQA-v2-T and UnifiedQA-v2-P. shows a strong tendency to provide the answer "yes" instead of "no", thus increasing recall (see Table <ref type="table">5</ref> in appendix). This works particularly well on the MS MARCO dataset that is skewed towards positive questions (85% positive, 15% negative) and less so on Se-mEval which is balanced at around 50%. The fact that the accuracy of BFS and our agent is higher on SemEval than on MS MARCO is mainly due to false positives. BFS and our agent default to a "no" answer when cause or effect cannot be linked to the causality graph, Comparing varying number of hops of our agent and the BFS, we can observe that additional hops increase recall while decreasing precision. The best trade-off in terms of ğ¹ 1 -score and accuracy is achieved for 4 hops on MS MARCO and 2 or 3 hops on SemEval. On SemEval, the precision of BFS drops from 0.937 in the 2-hop setting to 0.750 in the 3-hop setting due to the introduction of many false positives through inverse edges. <ref type="foot" target="#foot_5">6</ref> In contrast, our agent mitigates this problem pruning the search space and avoiding paths leading to wrong answers, still achieving a precision of 0.930. Compared to BFS, our agent has several advantages: (1) it prunes the search space and decreases the number of visited nodes by around 99%, (2) it can avoid false positives introduced by inverse edges and errors in CauseNet as shown above, (3) it can be extended to open-ended causal questions as discussed in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MS MARCO</head><p>Moreover, as described in Section 4.1, we experimented with providing triples (UnifiedQA-v2-T) from CauseNet as additional context to the language model as well as providing paths found by our agent (UnifiedQA-v2-P). The results indicate that UnifiedQA-v2-T slightly outperforms the vanilla UnifiedQA-v2 without context as well as UnifiedQA-v2-T with triples. Adding context to GPT-v4 did hardly improve results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Study</head><p>In our ablation study in Table <ref type="table" target="#tab_3">3</ref>, we investigate the performance impact of the components of our approach. We try out the following configurations: (1) without supervised learning, i.e., we run Algorithm 1 directly and train the policy and value network with policy gradients from scratch, (2) without Actor-Critic, we remove the critic and only run the REINFORCE algorithm, (3) we remove the beam search and use greedy decoding to only sample the most probable path, (4) without inverse edges in the graph, (5) without LSTM only using the two feedforward networks.</p><p>Overall, beam search has the biggest impact on performance. When beam search is exchanged for greedy decoding, the accuracy drops from 0.460 to 0.293 on MS MARCO and from 0.769 to 0.613 on SemEval. Notably, greedy decoding slightly increases the precision on MS MARCO and does reach a precision of 1.0 compared to the 0.943 of beam search on SemEval. Thus, the number of false positives decreases when only using the most probable path. Supervised learning has the second highest impact and we analyze it in more detail below. Next, the Actor-Critic algorithm only has a minor impact with a difference of around 0.02-0.03 points accuracy on both datasets. Similarly, the removal of inverse edges results in a slight decrease in overall performance but an increase in precision on the SemEval dataset to 1.0. That is because the removal of inverse edges reduces the probability of finding false positives, as discussed in Section 5. Using a feedforward network instead of an LSTM slightly decreases performance, too. We attribute this to the fact that a pure feedforward network can only encode a single node whereas an LSTM can capture a broader context of a node, namely the previously visited nodes on the path. In future work, inspired by Almasan et al. <ref type="bibr" target="#b1">[2]</ref>, we would like to capture an even larger context of nodes (e.g., 1-hop or 2-hop neighborhood) by employing a graph neural network (GNN).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Effects of Supervised Learning</head><p>We compare the performance of the agent when using different numbers of supervised training steps. Figure <ref type="figure" target="#fig_1">2</ref> shows the accuracy of the agent on the SemEval <ref type="bibr" target="#b37">[38]</ref> test set depending on the number of reinforcement learning training steps. Each of the three runs was bootstrapped with a different number of supervised training steps. Thus, at step 0, we can see the accuracy directly after supervised learning without any training via reinforcement learning. We observe that the run with 100 steps is significantly worse than the runs with 200 and 300 steps. It starts at around 0.67 directly after supervised learning and increases to around 0.72. Whereas the difference between 200 to 300 steps is already a lot smaller. Both start between 0.72 and 0.73 and follow similar trajectories afterward to reach an accuracy of around 0.76 after 2000 reinforcement learning steps. To maintain the clarity of the figures, we did not include a run with 400 steps, but the trend of diminishing returns on the number of supervised steps continues. This suggests that increasing the number of supervised steps beyond 300 does not improve performance. Likewise, we observed similar results on the MS MARCO <ref type="bibr" target="#b27">[28]</ref> dataset.</p><p>Moreover, Figure <ref type="figure" target="#fig_2">3</ref> illustrates the number of unique paths explored during training on the left and the mean entropy of the action distribution of the policy network on the right. Notably, with an increasing number of supervised steps, the entropy of the policy network drops significantly. Hence, the number of explored paths during reinforcement learning also decreases as on the left in Figure <ref type="figure" target="#fig_2">3</ref>. These findings indicate that supervised learning effectively establishes a strong foundation for the reinforcement learning agent. For example, an agent trained with 300 supervised steps only explores 21.6% of the paths of an agent without any supervised steps at the start. Accordingly, the agent trained with 300 supervised steps can exploit the knowledge acquired during supervised learning to follow better paths. In contrast, the agent without any supervised learning steps requires more exploration and fails to achieve the same performance, as shown in Table <ref type="table" target="#tab_3">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">DISCUSSION</head><p>Inverse Edges. The addition of inverse edges implies that the agent can also walk from an effect to its cause. In general, their addition has a few benefits, like the possibility to undo wrong actions and to reach nodes that otherwise could not be reached under a given path length constraint. Conversely, they also introduce the possibility to make mistakes through false positives. While these mistakes will always happen for the BFS, our agent can minimize them by pruning the search space, as demonstrated in our experiments in Section 4.2. As our ablation study in Section 4.3 shows, adding inverse edges improves performance in practice, so their benefits seem to outweigh the pitfalls. This is in line with prior works that also added inverse edges to improve performance <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b48">49]</ref>.</p><p>Open-ended Causal Questions. Currently, our approach only supports binary causal questions. One idea to answer open-ended questions like "What causes X?" or "What are the effects of X?" would be the following: At inference time, multiple paths are sampled by the agent. The entities that occur most often as endpoints of these paths are selected as the answer.</p><p>Explainability of Causal Questions. Our approach has the advantage of not only being able to answer binary causal questions but also providing explanations in the form of paths. The paths indicate mechanisms by which the cause produces the effect, potentially through a chain of multiple entities. Each path might indicate a different mechanism. Moreover, we can use additional provenance data that is part of each relation <ref type="bibr" target="#b12">[13]</ref>. For example, we can reference the original sentence from which the relation was extracted, which may provide additional insights. Furthermore, each relation contains the URL of the original web source, which can be checked to verify the causal relations and receive further information.</p><p>Negative Information. Dealing with negative information both during training and for explanation is an interesting open research challenge. Current causality graphs such as CauseNet do not contain negative edges <ref type="bibr" target="#b12">[13]</ref> and we train our agent on positive causal question, i.e., questions whose answer is "yes". If the ground-truth answer to a question "Does X cause Y?" is "no", and we assume a causality graph with only positive edges, then it does not matter how our agent walks in the graph, as it will never reach Y.</p><p>Regarding explainability, it turns out to be surprisingly difficult to explain why the answer to a causal question is "no": (1) One possibility might be KGs with negative edges. However, this only works for 1-hop negative paths as negative edges are not transitive. For example, on a 2-hop negative path like "A does not cause B does not cause C", we cannot conclude that "A does not cause C". <ref type="bibr" target="#b1">(2)</ref> Instead of materializing negative edges, link prediction approaches might be used to predict negative edges on the fly. However, link prediction approaches come with their own challenges, e.g., they rarely make perfect predictions. (3) Complex background knowledge might be encoded in the form of an ontology that allows to express negative statements (e.g., by using negation and forall quantifiers in description logic). However, ontological reasoning comes with its own challenges, e.g., it is hardly scalable to real-world, large-scale knowledge graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper, we propose the first reinforcement learning approach for answering binary causal questions on causality graphs. Given a question "Does X cause Y?", we model the problem of finding causal paths as a sequential decision process over the graph. We evaluate our approach on two causal question answering datasets. The results show that our reinforcement learning agent efficiently prunes the search space by 99% compared to a breadth-first search. Unlike language model-based approaches, our graph-based approach yields high-precision and verifiable answers: for each single edge on a path, we can provide its original source on the web. In future work, we will extend the approach to open-ended causal question answering, as discussed in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A APPENDICES</head><p>We give further details on our causality graph, our QA dataset, pseudocode for training our RL agent, and the confusion table of results. Finally, we report additional experiments regarding the beam width in the decoding phase, show example paths found by our agent, and perform a manual analysis of the found paths.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Causality Graph CauseNet</head><p>CauseNet <ref type="bibr" target="#b12">[13]</ref> is a large-scale causality graph extracted from web sources like Wikipedia and ClueWeb12. The following table shows its precision according to a manual evaluation, number of entities, and number of relations as reported by Heindorf et al. <ref type="bibr" target="#b12">[13]</ref>: We construct two datasets of binary causal questions for our experiments: MS MARCO and SemEval. For our MS MARCO dataset, we extracted all binary causal questions from the MS MARCO subset of the Webis-CausalQA-22 corpus <ref type="bibr" target="#b2">[3]</ref>. 7 To do so, we build upon Heindorf et al. <ref type="bibr" target="#b12">[13]</ref> to extract questions via patterns of the form where the [question word] placeholder either represents one of the question words from Table <ref type="table" target="#tab_5">4</ref> (bottom) or is empty. The [cue word] placeholder represents words that are good indicators for causal relations together with their appropriate prepositions. The original approach only considers cause in different verb forms, e.g., infinitive, past, or progressive. We extend this to a greater number of causal cue words. Specifically, we use the collection from Girju and Moldovan <ref type="bibr" target="#b9">[10]</ref> who curated a collection of causal cue words and ranked them by their frequency and ambiguity, i.e., how often they appear in text and how often they refer to a causal relation. Among these, we selected the ones that were ranked with high frequency and low ambiguity. Table <ref type="table" target="#tab_5">4</ref> shows the full list of 23 words. Moreover, the [cause/effect] placeholder represents causal concepts, where one takes the role of the cause and the other the role of the effect. The order depends on the question word and the causal cue word. As done by Heindorf et al. <ref type="bibr" target="#b12">[13]</ref>, we place a few restrictions on the questions to keep the concepts simple and increase the probability that they can be found in CauseNet. The restrictions are enforced by filtering questions based on POS-Tags from the Stanford CoreNLP <ref type="bibr" target="#b25">[26]</ref>, e.g., we disallow coordinating conjunctions and subordinating conjunctions. The full list is shown in Table <ref type="table" target="#tab_5">4</ref> on the bottom right. Finally, we check whether the questions are answered with "yes" or "no" and remove any further explanation.</p><formula xml:id="formula_8">Graph</formula><p>The SemEval dataset was processed as described in Section 4.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Pseudocode for Training the RL Agent</head><p>In the following, we describe the training procedure of our reinforcement learning agent. This includes the pre-processing of the questions, the sampling of path rollouts, and the update rules  for the weights of the agent. We start with the observation that CauseNet <ref type="bibr" target="#b12">[13]</ref> does not contain negative information. Thus, we only train the agent on positive causal questions, i.e., questions whose answer is "yes". Similarly, we must remove all questions where the cause, effect, or both cannot be found in CauseNet. Algorithm 1 displays the pseudocode of the whole training phase. The pseudocode assumes that only positive causal questions, where cause and effect can be found in CauseNet, remain in the given ğ‘ğ‘¢ğ‘’ğ‘ ğ‘¡ğ‘–ğ‘œğ‘›ğ‘ . First, we pre-process the questions by linking the cause and effect to the corresponding entities ğ‘’ ğ‘ and ğ‘’ ğ‘’ in CauseNet. This is followed by the computation of embeddings for the question and entities. <ref type="foot" target="#foot_6">8</ref> Next, the weights ğœƒ and ğœ“ of the agent are initialized. In the default setup, both are initialized randomly. In the case of a preceding supervised learning phase (Section 3.6), the weights of the policy network ğœƒ are initialized with the resulting weights from the supervised learning.</p><p>Afterward, we start sampling path rollouts from the environment via the current policy network ğœ‹ ğœƒ (ğ‘ ğ‘¡ |ğ‘  ğ‘¡ ). Each path rollout has the same length ğ‘‡ . Hence, the agent should learn to use the STAY action in case it arrives at the target entity before a length of ğ‘‡ is reached. Given a pre-processed question ğ‘, we construct the first state ğ‘  0 . Subsequently, the agent interacts with the environment for ğ‘‡ time steps. At each time step, the agent applies an action ğ‘ ğ‘¡ and receives a reward ğ‘Ÿ ğ‘¡ while the environment evolves via the transition function ğ›¿ (ğ‘  ğ‘¡ , ğ‘ ğ‘¡ ) to the next state ğ‘  ğ‘¡ +1 . This procedure is continued until a full batch of path rollouts is accumulated.</p><p>The training of the agent is facilitated via the Synchronous Advantage Actor-Critic (A2C) <ref type="bibr" target="#b26">[27]</ref> algorithm. The policy network ğœ‹ ğœƒ (ğ‘ ğ‘¡ |ğ‘  ğ‘¡ ) takes the role of the actor while the value network ğ‘‰ ğœƒ (ğ‘  ğ‘¡ ) takes the role of the critic. We briefly experimented with Proximal Policy Optimization (PPO) <ref type="bibr" target="#b36">[37]</ref> but found no significant performance improvements. Thus, the update rule for the policy network   âˆ‡ ğœƒ log(ğœ‹ ğœƒ (ğ‘ ğ‘¡ |ğ‘  ğ‘¡ )) A ğœ“ ğ‘¡ <ref type="bibr" target="#b5">(6)</ref> where ğµ is the batch size, T the path rollout length, and A ğœ“ ğ‘¡ the generalized advantage estimate (GAE) <ref type="bibr" target="#b35">[36]</ref>. GAE introduces two hyperparameters, the discount factor ğ›¾ and a smoothing factor ğœ† which controls the trade-off between bias and variance <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b42">43]</ref>.</p><formula xml:id="formula_9">(ğ‘  ğ‘‡ -1 , ğ‘ ğ‘‡ -1 , ğ‘Ÿ ğ‘‡ -1 , A ğœ“ ğ‘‡ -1 , ğ‘… ğ‘‡ -1 (ğœ†))) in path_rollouts do 30 ğ‘ğ‘œğ‘™ğ‘–ğ‘ğ‘¦_ğ‘¢ğ‘ğ‘‘ğ‘ğ‘¡ğ‘’ += ğ‘‡ -1 ğ‘¡ =0 âˆ‡ ğœƒ log ğœ‹ ğœƒ (ğ‘ ğ‘¡ |ğ‘  ğ‘¡ ) A ğœ“ ğ‘¡ 31 ğ‘£ğ‘ğ‘™ğ‘¢ğ‘’_ğ‘¢ğ‘ğ‘‘ğ‘ğ‘¡ğ‘’ += ğ‘‡ -1 ğ‘¡ =0 âˆ‡ ğœ“ (ğ‘… ğ‘¡ (ğœ†) -ğ‘‰ ğœ“ (ğ‘  ğ‘¡ )) 2</formula><p>As commonly done, we add an entropy regularization term to the objective <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b17">18]</ref>. The entropy regularization should help the agent with the exploitation vs. exploration tradeoff. Specifically, it should encourage exploration during training and the resulting policy should be more robust and have a higher diversity of explored actions. We compute the average entropy of the action distribution of ğœ‹ ğœƒ (ğ‘ ğ‘¡ |ğ‘  ğ‘¡ ) over all actions ğ‘ ğ‘¡ âˆˆ A (ğ‘  ğ‘¡ ) at each time step ğ‘¡ and take the average over the whole batch: The final update for the policy network becomes</p><formula xml:id="formula_10">ğ» ğœ‹ ğœƒ =</formula><formula xml:id="formula_11">ğœƒ = ğœƒ -ğ‘™ğ‘Ÿ â€¢ (âˆ‡ ğœƒ ğ½ (ğœƒ ) + ğ›½ğ» ğœ‹ ğœƒ ) (<label>8</label></formula><formula xml:id="formula_12">)</formula><p>where ğ‘™ğ‘Ÿ is the learning rate and ğ›½ is a hyperparameter that determines the weight of the entropy regularization term. Simultaneously, we update the value network ğ‘‰ ğœ“ (ğ‘  ğ‘¡ ) via the mean-squared error between the ğœ†-return and the predictions of the value network âˆ‡ ğœ“ (ğ‘… ğ‘¡ (ğœ†) -ğ‘‰ ğœ“ (ğ‘  ğ‘¡ )) 2 <ref type="bibr" target="#b8">(9)</ref> where ğ‘… ğ‘¡ (ğœ†) is the ğœ†-return <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b42">43]</ref>. Therefore, the update for the value network becomes:</p><formula xml:id="formula_13">ğœ“ = ğœ“ -ğ‘™ğ‘Ÿ â€¢ âˆ‡ ğœ“ ğ½ (ğœ“ )<label>(10)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Confusion Table</head><p>Table <ref type="table">5</ref> shows the confusion table corresponding to our evaluation results in Section 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Decoding Analysis</head><p>In the following, we investigate the effects of different beam widths on the MS MARCO test set. We experiment with beam widths of 1, 5, 10, and 50 and present the results in Figure <ref type="figure" target="#fig_9">4</ref>. As the beam width increases, accuracy also increases. For example, the difference between a width of 1 and a width of 50 is 0.09 points accuracy after 2000 steps. In general, beam search can be viewed as an interpolation between greedy decoding and BFS. If the beam width is set high, the agent performs close to an exhaustive search. We can already observe this effect when looking at step 0. As the beam width increases, the accuracy without any learning also increases slightly, e.g., for widths of 10 and 50 from 0.18 to 0.21. Having said this, even beam width 50 only starts at 0.21 and still has a lot of learning progress afterwards and the performance of width 50 does not reach the performance of supervised learning suggesting that 50 is still a reasonable beam width.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6 Example Paths</head><p>Table <ref type="table">6</ref> illustrates a few example paths found by our agent. The paths can be used to follow the complete reasoning chain to examine the mechanisms of how a cause produces an effect. Moreover, the first example demonstrates the agent's ability to utilize the STAY action; the third example shows how the agent learned to use inverse edges to recover from mistakes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.7 Manual Path Analysis</head><p>It has previously been found that 0.96% of triples in CauseNet are correct as judged by a human annotator <ref type="bibr" target="#b12">[13]</ref>. Similarly, we investigated manually in how far the paths found by our 3-hop agent are correct. For each question, we inspected the top path according to its probability as returned by the agent. A path is rated as correct if evidence for it could be found via a web search; a path is wrong if counter-evidence could be found; a path is borderline if the human annotator found it difficult to make a definite decision, e.g., because conflicting evidence was found or because the path contained overly generic intermediate nodes.</p><p>Across datasets, about 53%-64% of answers were rated as correct, 25%-27% as borderline, and 11%-20% as wrong. Many of the wrong answers are due to inverse edges, namely 16 out of 23 for MS MARCO and 2 out of 6 for SemEval. Nevertheless, as discussed in Section 5, inverse edges often help to improve accuracy and ğ¹ 1 -score. The higher percentage of correct answers for SemEval compared to MS MARCO may be explained by the fact that answering MS MARCO questions tends to require more hops, increasing the risk of errors accumulating (cf. Table <ref type="table">2</ref>). Overall, the manual analysis highlights the need to develop improved causality graphs and question-answering approaches in future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: An excerpt from CauseNet<ref type="bibr" target="#b12">[13]</ref> showing the entity pneumonia together with its neighborhood containing causes and effects, where each edge depicts a cause relation. The numbers on the edges show the probability of taking this edge under the current policy ğœ‹ ğœƒ (ğ‘ ğ‘¡ |ğ‘  ğ‘¡ ). For brevity, we only show the relevant probabilities for the given paths. The lower part of the figure shows the possibility to combine our agent with a language model. In that setup, we provide the paths the agent learned as additional context to the language model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Accuracy of the agent on the SemEval test set depending on the number of reinforcement learning training steps. Each run was bootstrapped with a different number of supervised training steps. Step 0 shows the performance directly after supervised learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Number of unique paths explored during reinforcement learning training on the left and the mean entropy of the action distribution of the policy network on the right. Each run was bootstrapped with a different number of supervised training steps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>[</head><label></label><figDesc>question word]? [cause/effect] [cue word] [cause/effect]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>7 https://doi.org/10.5281/zenodo.7476615</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>4 ğ‘ğ‘Ÿğ‘œğ‘ğ‘’ğ‘ ğ‘ ğ‘’ğ‘‘_ğ‘ğ‘¢ğ‘’ğ‘ ğ‘¡ğ‘–ğ‘œğ‘›ğ‘  = [ ] 5 for 7 25 if</head><label>45725</label><figDesc>each ğ‘ in ğ‘ğ‘¢ğ‘’ğ‘ ğ‘¡ğ‘–ğ‘œğ‘›ğ‘  do 6 Link the cause and effect of ğ‘ to entities ğ‘’ ğ‘ , ğ‘’ ğ‘’ in K Compute embeddings q, e c for ğ‘, ğ‘’ ğ‘ 8 ğ‘ğ‘Ÿğ‘œğ‘ğ‘’ğ‘ ğ‘ ğ‘’ğ‘‘_ğ‘ğ‘¢ğ‘’ğ‘ ğ‘¡ğ‘–ğ‘œğ‘›ğ‘ .ğ‘ğ‘ğ‘ğ‘’ğ‘›ğ‘‘ ((q, ğ‘’ ğ‘ , e c , ğ‘’ ğ‘’ )) ğ‘’ ğ‘ , e c , ğ‘’ ğ‘’ ) = ğ‘†ğ‘ğ‘šğ‘ğ‘™ğ‘’ğ‘ˆ ğ‘›ğ‘– ğ‘“ ğ‘œğ‘Ÿğ‘š(ğ‘ğ‘Ÿğ‘œğ‘ğ‘’ğ‘ ğ‘ ğ‘’ğ‘‘_ğ‘ğ‘¢ğ‘’ğ‘ ğ‘¡ğ‘–ğ‘œğ‘›ğ‘ ) 17 ğ‘  0 = (q, ğ‘’ ğ‘ , e c , 0, ğ‘’ ğ‘’ ) 18 for ğ‘¡ = 0 to ğ‘‡ -1 do 19 ğ‘ ğ‘¡ = ğ‘†ğ‘ğ‘šğ‘ğ‘™ğ‘’ğ¶ğ‘ğ‘¡ğ‘’ğ‘”ğ‘œğ‘Ÿğ‘–ğ‘ğ‘ğ‘™ (ğœ‹ ğœƒ (ğ‘ ğ‘¡ |ğ‘  ğ‘¡ )) 20 Receive state ğ‘  ğ‘¡ +1 = ğ›¿ (ğ‘  ğ‘¡ , ğ‘ ğ‘¡ ) and reward ğ‘Ÿ ğ‘¡ = R (ğ‘  ğ‘¡ , ğ‘ ğ‘¡ ) 21 ğ‘ğ‘ğ‘¡â„_ğ‘Ÿğ‘œğ‘™ğ‘™ğ‘œğ‘¢ğ‘¡ .ğ‘ğ‘ğ‘ğ‘’ğ‘›ğ‘‘ ((ğ‘  ğ‘¡ , ğ‘ ğ‘¡ , ğ‘Ÿ ğ‘¡ )) |ğ‘ğ‘ğ‘¡â„_ğ‘Ÿğ‘œğ‘™ğ‘™ğ‘œğ‘¢ğ‘¡ğ‘  | = ğµ then 26 Compute the GAE A ğœ“ ğ‘¡ and ğœ†-returns ğ‘… ğ‘¡ (ğœ†) for each episode in ğ‘ğ‘ğ‘¡â„_ğ‘Ÿğ‘œğ‘™ğ‘™ğ‘œğ‘¢ğ‘¡ğ‘  using ğœ†, ğ›¾ 27 ğ‘ğ‘œğ‘™ğ‘–ğ‘ğ‘¦_ğ‘¢ğ‘ğ‘‘ğ‘ğ‘¡ğ‘’, ğ‘£ğ‘ğ‘™ğ‘¢ğ‘’_ğ‘¢ğ‘ğ‘‘ğ‘ğ‘¡ğ‘’ = 0 28 for each ((ğ‘  0 , ğ‘ 0 , ğ‘Ÿ 0 , A ğœ“ 0 , ğ‘… 0 (ğœ†)), . . . , 29</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>35 ğœƒ 37 ğ‘ğ‘ğ‘¡â„_ğ‘Ÿğ‘œğ‘™ğ‘™ğ‘œğ‘¢ğ‘¡ğ‘  = [ ] 38 ğ‘ ğ‘¡ğ‘’ğ‘Algorithm 1 :</head><label>3537381</label><figDesc>= ğœƒ -ğ‘™ğ‘Ÿ â€¢ (ğ‘ğ‘œğ‘™ğ‘–ğ‘ğ‘¦_ğ‘¢ğ‘ğ‘‘ğ‘ğ‘¡ğ‘’ + ğ›½ğ» ğœ‹ ğœƒ ) 36 ğœ“ = ğœ“ -ğ‘™ğ‘Ÿ â€¢ ğ‘£ğ‘ğ‘™ğ‘¢ğ‘’_ğ‘¢ğ‘ğ‘‘ğ‘ğ‘¡ğ‘’ Pseudocode of the training procedure for the policy network ğœ‹ ğœƒ (ğ‘ ğ‘¡ |ğ‘  ğ‘¡ ) and value network ğ‘‰ ğœ“ (ğ‘  ğ‘¡ ). ğœ‹ ğœƒ (ğ‘ ğ‘¡ |ğ‘  ğ‘¡ ) becomes âˆ‡ ğœƒ ğ½ (ğœƒ ) = -</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>1 ğµ</head><label>1</label><figDesc>ğ‘ ğ‘¡ âˆˆ A (ğ‘  ğ‘¡ ) ğœ‹ ğœƒ (ğ‘ ğ‘¡ |ğ‘  ğ‘¡ ) log ğœ‹ ğœƒ (ğ‘ ğ‘¡ |ğ‘  ğ‘¡ )) (7)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>âˆ‡ 1 ğµ</head><label>1</label><figDesc>ğœ“ ğ½ (ğœ“ ) =</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Accuracy on the MS MARCO test set depending on the number of RL training steps and beam width.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Hence, at each state ğ‘  ğ‘¡ âˆˆ S the agent selects an action ğ‘ ğ‘¡ âˆˆ A, which changes the current state via ğ›¿ (ğ‘  ğ‘¡ , ğ‘ ğ‘¡ ) to ğ‘  ğ‘¡ +1 . Additionally, the agent receives a reward R (ğ‘  ğ‘¡ , ğ‘ ğ‘¡ ) = ğ‘Ÿ ğ‘¡ . Note that the transition function ğ›¿ is known and deterministic because the graph entirely defines ğ›¿. So, for each action ğ‘ ğ‘¡ in state ğ‘  ğ‘¡ , the next state ğ‘  ğ‘¡ +1 is known. Agent. Our agent consists of a policy network ğœ‹ ğœƒ (ğ‘ ğ‘¡ |ğ‘  ğ‘¡ ) (Actor) parameterized with ğœƒ and a value network ğ‘‰ ğœ“ (ğ‘  ğ‘¡ ) (Critic) parameterized with ğœ“ . The policy network ğœ‹ ğœƒ (ğ‘ ğ‘¡ |ğ‘  ğ‘¡ ) generates a distribution over actions ğ‘ ğ‘¡ at the current state ğ‘  ğ‘¡ . The value network ğ‘‰ ğœ“ (ğ‘  ğ‘¡ ) generates a scalar to estimate the value of the state ğ‘  ğ‘¡ . Specifically, the value network should predict the future reward from ğ‘  ğ‘¡ onwards.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Number of questions for training, validation, and testing for the MS MARCO<ref type="bibr" target="#b27">[28]</ref> and SemEval<ref type="bibr" target="#b37">[38]</ref> datasets. The "E. Train" column shows the number of questions the agent has effectively available for learning. Task 8<ref type="bibr" target="#b13">[14]</ref>. Among the 1730 word pairs, there are 865 causal pairs and 865 non-causal pairs, i.e., the dataset is balanced, whereas MS MARCO is imbalanced. The first three columns of Table1show the original numbers of training, validation, and test questions of both datasets. However, as discussed in Sections 3.4 and A.3, for training our reinforcement learning agent, we remove negative questions and questions where either the cause or effect cannot be found in CauseNet. The "E. Train" column shows the number of questions effectively available for learning when combining the training and validation sets. That leaves us with 1350 questions for MS MARCO and 812 questions for SemEval. Each causal question contains one cause and one effect.</figDesc><table><row><cell>Dataset</cell><cell>Train</cell><cell cols="2">Validation</cell><cell>Test</cell><cell></cell><cell>E. Train</cell></row><row><cell></cell><cell cols="5">Pos. Neg. Pos. Neg. Pos. Neg.</cell><cell>Pos.</cell></row><row><cell cols="3">MS MARCO 1837 332 194</cell><cell cols="2">47 223</cell><cell>40</cell><cell>1350</cell></row><row><cell>SemEval</cell><cell>694 690</cell><cell>84</cell><cell>89</cell><cell>87</cell><cell>86</cell><cell>812</cell></row><row><cell cols="2">benchmark SemEval 2010</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p>On average, a causal question has a length of 30.2 characters and 4.6 words (averaged over the training, validation, and test splits of both datasets after filtering). For testing question answering, we use both positive and negative questions regardless of whether the cause/effect can be mapped to CauseNet. If a cause or effect cannot be found in CauseNet, the question is answered with "no". Further details regarding Causenet and our dataset construction are given in Sections A.1 and A.2 in the appendix, respectively.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Results of the ablation study, where we compare different configurations of our approach by removing (-) different components. The evaluation measures are abbreviated as follows: accuracy: A, F 1 -Score: F 1 , recall: R, precision: P.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">MS MARCO</cell><cell></cell><cell></cell><cell cols="2">SemEval</cell></row><row><cell></cell><cell>A</cell><cell>F 1</cell><cell>R</cell><cell>P</cell><cell>A</cell><cell>F 1</cell><cell>R</cell><cell>P</cell></row><row><cell>Agent 2-Hop</cell><cell cols="8">0.460 0.562 0.408 0.901 0.769 0.714 0.575 0.943</cell></row><row><cell>-Beam Search</cell><cell cols="8">0.293 0.306 0.184 0.911 0.613 0.374 0.230 1.000</cell></row><row><cell cols="9">-Supervised Learn. 0.342 0.397 0.256 0.891 0.682 0.538 0.368 1.000</cell></row><row><cell>-Actor-Critic</cell><cell cols="8">0.437 0.535 0.381 0.895 0.740 0.656 0.494 0.977</cell></row><row><cell>-Inverse Edges</cell><cell cols="8">0.418 0.508 0.354 0.898 0.740 0.651 0.483 1.000</cell></row><row><cell>-LSTM</cell><cell cols="8">0.426 0.518 0.363 0.900 0.738 0.646 0.475 1.000</cell></row><row><cell cols="9">which leads to 21 false negatives on SemEval (out of 173 predictions)</cell></row><row><cell cols="8">and 80 false negatives on MS MARCO (out of 263 predictions).</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Causal cue words for detecting binary causal questions (top), question words for binary question extraction (bottom left) and POS tags for exclusion (bottom right).</figDesc><table><row><cell></cell><cell></cell><cell cols="3">Causal Cue Words</cell></row><row><cell>induce</cell><cell cols="2">provoke</cell><cell></cell><cell>relate (to)</cell><cell>trigger off</cell></row><row><cell>give rise (to)</cell><cell cols="2">arouse</cell><cell></cell><cell>link (to)</cell><cell>bring on</cell></row><row><cell>produce</cell><cell cols="2">elicit</cell><cell></cell><cell>stem (from)</cell><cell>result (from)</cell></row><row><cell>generate</cell><cell cols="2">lead (to)</cell><cell></cell><cell>originate</cell><cell>trigger</cell></row><row><cell>effect</cell><cell cols="2">derive (from)</cell><cell></cell><cell>bring forth</cell><cell>cause</cell></row><row><cell>bring about</cell><cell cols="3">associate (with)</cell><cell>lead up</cell></row><row><cell cols="2">Question Words</cell><cell cols="3">POS-Tag Description</cell></row><row><cell>is</cell><cell>do</cell><cell>CC</cell><cell cols="2">Coordinating conjunction</cell></row><row><cell>can</cell><cell>does</cell><cell>IN</cell><cell cols="2">Preposition or subordinating conj.</cell></row><row><cell>might</cell><cell>did</cell><cell>TO</cell><cell cols="2">To-prepositions</cell></row><row><cell>would</cell><cell>will</cell><cell>WDT</cell><cell cols="2">Wh-determiner</cell></row><row><cell>could</cell><cell>are</cell><cell>WP</cell><cell cols="2">Wh-pronoun</cell></row><row><cell>may</cell><cell></cell><cell>WRB</cell><cell cols="2">Wh-adverb</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>Knowledge graph K, questions ğ‘ğ‘¢ğ‘’ğ‘ ğ‘¡ğ‘–ğ‘œğ‘›ğ‘ , optimization steps ğ‘ ğ‘¡ğ‘’ğ‘ğ‘ , batch size ğµ, path rollout length ğ‘‡ , learning rate ğ‘™ğ‘Ÿ , entropy weight ğ›½, discount factor ğ›¾, GAE lambda ğœ† 2 Output: Trained Agent ğœƒ , ğœ“ 3 Function Training(K, ğ‘ğ‘¢ğ‘’ğ‘ ğ‘¡ğ‘–ğ‘œğ‘›ğ‘ , ğ‘ ğ‘¡ğ‘’ğ‘ğ‘ , ğµ, ğ‘‡ , ğ‘™ğ‘Ÿ , ğ›½, ğ›¾, ğœ†):</figDesc><table /><note><p>1 Input:</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 7 :</head><label>7</label><figDesc>Manual evaluation of top path found by 3-hop agent.</figDesc><table><row><cell></cell><cell cols="2">MS MARCO</cell><cell cols="2">SemEval</cell></row><row><cell></cell><cell>n</cell><cell>Fraction</cell><cell>n</cell><cell>Fraction</cell></row><row><cell>Correct</cell><cell>60</cell><cell>53%</cell><cell>34</cell><cell>64%</cell></row><row><cell>Borderline</cell><cell>31</cell><cell>27%</cell><cell>13</cell><cell>25%</cell></row><row><cell>Wrong</cell><cell>23</cell><cell>20%</cell><cell>6</cell><cell>11%</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>In principle, episodes of different lengths are not a problem. However, keeping them to the same length simplifies the implementation. We chose this simplification because it worked well in prior works[6,  </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p><ref type="bibr" target="#b32">33]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>A question ğ‘ is discarded, if a path of length less than or equal to ğ‘‡ cannot be found between its cause ğ‘’ ğ‘ and its effect ğ‘’ ğ‘’ .</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>https://github.com/ds-jrg/causal-qa-rl</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>https://doi.org/10.5281/zenodo.10683046</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>The results of BFS 3-Hop on SemEval are improved when not using inverse edges. However, using inverse edges improves the results for all other configurations. Thus, we included them in the graph as they also improve the performance of our agent, as shown in the ablation study in Section 4.3.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_6"><p>We use GloVe<ref type="bibr" target="#b31">[32]</ref> embeddings to embed the questions and entities. For more details, see Section 4.1.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>The authors gratefully acknowledge the funding of this project by computing time provided by the <rs type="funder">Paderborn Center for Parallel Computing</rs> (<rs type="grantNumber">PC 2</rs> ).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_HhgtEy5">
					<idno type="grant-number">PC 2</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Optuna: A Next-generation Hyperparameter Optimization Framework</title>
		<author>
			<persName><forename type="first">Takuya</forename><surname>Akiba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shotaro</forename><surname>Sano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toshihiko</forename><surname>Yanase</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takeru</forename><surname>Ohta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masanori</forename><surname>Koyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2623" to="2631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning meets graph neural networks: Exploring a routing optimization use case</title>
		<author>
			<persName><forename type="first">JosÃ©</forename><surname>Paul Almasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krzysztof</forename><surname>SuÃ¡rez-Varela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pere</forename><surname>Rusek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Albert Cabellos-Aparicio</forename><surname>Barlet-Ros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Commun</title>
		<imprint>
			<biblScope unit="volume">196</biblScope>
			<biblScope unit="page" from="184" to="194" />
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">CausalQA: A Benchmark for Causal Question Answering</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Bondarenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Magdalena</forename><surname>Wolska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Heindorf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukas</forename><surname>BlÃ¼baum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Axel-Cyrille Ngonga</forename><surname>Ngomo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benno</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavel</forename><surname>Braslavski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Hagen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Potthast</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING. International Committee on Computational Linguistics</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="3296" to="3308" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Knowledge augmented language models for causal question answering</title>
		<author>
			<persName><forename type="first">Dhairya</forename><surname>Dalal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CEUR Workshop Proceedings</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
			<biblScope unit="volume">3005</biblScope>
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Enhancing Multiple-Choice Question Answering with Causal Knowledge</title>
		<author>
			<persName><forename type="first">Dhairya</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Mihael Arcan</surname></persName>
		</author>
		<author>
			<persName><surname>Buitelaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">DeeLIO@NAACL-HLT. ACL</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="70" to="80" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Go for a Walk and Arrive at the Answer: Reasoning Over Paths in Knowledge Bases using Reinforcement Learning</title>
		<author>
			<persName><forename type="first">Rajarshi</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shehzaad</forename><surname>Dhuliawala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishan</forename><surname>Durugkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akshay</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convolutional 2D Knowledge Graph Embeddings</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pasquale</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1811" to="1818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT. ACL</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/V1/N19-1423</idno>
		<ptr target="https://doi.org/10.18653/V1/N19-1423" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019</title>
		<title level="s">Long and Short Papers</title>
		<editor>
			<persName><forename type="first">Jill</forename><surname>Burstein</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Christy</forename><surname>Doran</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Thamar</forename><surname>Solorio</surname></persName>
		</editor>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019<address><addrLine>Minneapolis, MN, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-06-02">2019. June 2-7, 2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Text Mining for Causal Relations</title>
		<author>
			<persName><forename type="first">Roxana</forename><surname>Girju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><forename type="middle">I</forename><surname>Moldovan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FLAIRS</title>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="360" to="364" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The Argument Reasoning Comprehension Task: Identification and Reconstruction of Implicit Warrants</title>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Habernal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henning</forename><surname>Wachsmuth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iryna</forename><surname>Gurevych</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benno</forename><surname>Stein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT. ACL</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1930" to="1940" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Answering Binary Causal Questions Through Large-Scale Text Mining: An Evaluation Using Cause-Effect Pairs from Human Experts</title>
		<author>
			<persName><forename type="first">Oktie</forename><surname>Hassanzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Debarun</forename><surname>Bhattacharjya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Feblowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kavitha</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Perrone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirin</forename><surname>Sohrabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Katz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">IJCAI. ijcai.org</title>
		<imprint>
			<biblScope unit="page" from="5003" to="5009" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">CauseNet: Towards a Causality Graph Extracted from the Web</title>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Heindorf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Scholten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henning</forename><surname>Wachsmuth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3023" to="3030" />
		</imprint>
	</monogr>
	<note>Axel-Cyrille Ngonga Ngomo, and Martin Potthast</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">SemEval-2010 Task 8: Multi-Way Classification of Semantic Relations between Pairs of Nominals</title>
		<author>
			<persName><forename type="first">Iris</forename><surname>Hendrickx</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nam</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zornitsa</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Preslav</forename><surname>Kozareva</surname></persName>
		</author>
		<author>
			<persName><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ã“</forename><surname>Diarmuid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>SÃ©aghdha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>PadÃ³</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorenza</forename><surname>Pennacchiotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stan</forename><surname>Romano</surname></persName>
		</author>
		<author>
			<persName><surname>Szpakowicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SemEval@ACL. ACL</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="33" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Long Short-Term Memory</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">JÃ¼rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997. 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Atomic 2020: On Symbolic and Neural Commonsense Knowledge Graphs</title>
		<author>
			<persName><forename type="first">Jena</forename><forename type="middle">D</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandra</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keisuke</forename><surname>Da</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="6384" to="6392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">CSKG: The CommonSense Knowledge Graph</title>
		<author>
			<persName><forename type="first">Filip</forename><surname>Ilievski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pedro</forename><forename type="middle">A</forename><surname>Szekely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ESWC</title>
		<imprint>
			<biblScope unit="volume">12731</biblScope>
			<biblScope unit="page" from="680" to="696" />
			<date type="published" when="2021">2021</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Reinforcement Learning from Reformulations in Conversational Question Answering over Knowledge Graphs</title>
		<author>
			<persName><forename type="first">Magdalena</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rishiraj</forename><surname>Saha Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerhard</forename><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="459" to="469" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Answering Binary Causal Questions: A Transfer Learning Based Approach</title>
		<author>
			<persName><forename type="first">Md</forename><forename type="middle">Saiful</forename><surname>Humayun Kayesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junhu</forename><surname>Islam</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S M</forename><surname>Shikha Anirban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">A</forename><surname>Kayes</surname></persName>
		</author>
		<author>
			<persName><surname>Watters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In IJCNN. IEEE</title>
		<imprint>
			<biblScope unit="page" from="1" to="9" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">UnifiedQA-v2: Stronger Generalization via Broader Cross-Format Training</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yeganeh</forename><surname>Kordi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
		<idno>CoRR abs/2202.12359</idno>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">UnifiedQA: Crossing Format Boundaries With a Single QA System</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sewon</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Sabharwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oyvind</forename><surname>Tafjord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannaneh</forename><surname>Hajishirzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP (Findings) (Findings of ACL</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="1896" to="1907" />
		</imprint>
	</monogr>
	<note>ACL</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Guided Generation of Cause and Effect</title>
		<author>
			<persName><forename type="first">Zhongyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Edward</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI. ijcai.org</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3629" to="3636" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multi-Hop Knowledge Graph Reasoning with Reward Shaping</title>
		<author>
			<persName><forename type="first">Victoria</forename><surname>Xi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiming</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP. ACL</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3243" to="3253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">RoBERTa: A Robustly Optimized BERT Pretraining Approach</title>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<ptr target="http://arxiv.org/abs/1907.11692" />
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Decoupled Weight Decay Regularization</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR (Poster)</title>
		<imprint>
			<publisher>OpenReview</publisher>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>net</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihai</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jenny</forename><forename type="middle">Rose</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Finkel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Bethard</surname></persName>
		</author>
		<author>
			<persName><surname>Mcclosky</surname></persName>
		</author>
		<title level="m">The Stanford CoreNLP Natural Language Processing Toolkit. In ACL (System Demonstrations). ACL</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="55" to="60" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Asynchronous Methods for Deep Reinforcement Learning</title>
		<author>
			<persName><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">AdriÃ </forename><surname>PuigdomÃ¨nech Badia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML (JMLR Workshop and Conference Proceedings</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="1928" to="1937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">MS MARCO: A Human Generated MAchine Reading COmprehension Dataset</title>
		<author>
			<persName><forename type="first">Tri</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mir</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Tiwary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rangan</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
		<ptr target="CEUR-WS.org" />
	</analytic>
	<monogr>
		<title level="m">CoCo@NIPS (CEUR Workshop Proceedings</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1773</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName><surname>Openai</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2303.08774</idno>
		<idno type="arXiv">arXiv:2303.08774</idno>
		<ptr target="https://doi.org/10.48550/ARXIV.2303.08774" />
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">GPT-4 Technical Report. CoRR</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">TomÃ¡s</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML (JMLR Workshop and Conference Proceedings</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1310" to="1318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">DeepMimic: example-guided deep reinforcement learning of physics-based character skills</title>
		<author>
			<persName><forename type="first">Xue Bin</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michiel</forename><surname>Van De Panne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page">143</biblScope>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Glove: Global Vectors for Word Representation</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP. ACL</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Stepwise Reasoning for Multi-Relation Question Answering over Knowledge Graph with Weak Supervision</title>
		<author>
			<persName><forename type="first">Yunqi</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanzhuo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolong</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="474" to="482" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page">67</biblScope>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">ATOMIC: An Atlas of Machine Commonsense for If-Then Reasoning</title>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandra</forename><surname>Allaway</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannah</forename><surname>Lourie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brendan</forename><surname>Rashkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Roof</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3027" to="3035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">High-Dimensional Continuous Control Using Generalized Advantage Estimation</title>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR (Poster)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Proximal Policy Optimization Algorithms</title>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Filip</forename><surname>Wolski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleg</forename><surname>Klimov</surname></persName>
		</author>
		<idno>CoRR abs/1707.06347</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Creating Causal Embeddings for Question Answering with Minimal Supervision</title>
		<author>
			<persName><forename type="first">Rebecca</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihai</forename><surname>Surdeanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Hammond</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP. ACL</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="138" to="148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">M-Walk: Learning to Walk over Graphs using Monte Carlo Tree Search</title>
		<author>
			<persName><forename type="first">Yelong</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianshu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Po-Sen</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuqing</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="6787" to="6798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Mastering the game of Go with deep neural networks and tree search</title>
		<author>
			<persName><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aja</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vedavyas</forename><surname>Panneershelvam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sander</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Grewe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Nham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madeleine</forename><surname>Leach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nat</title>
		<imprint>
			<biblScope unit="volume">529</biblScope>
			<biblScope unit="page" from="484" to="489" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note>Thore Graepel, and Demis Hassabis</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Thore Graepel, et al. 2018. A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play</title>
		<author>
			<persName><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dharshan</forename><surname>Kumaran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">362</biblScope>
			<biblScope unit="page" from="1140" to="1144" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">ConceptNet 5.5: An Open Multilingual Graph of General Knowledge</title>
		<author>
			<persName><forename type="first">Robyn</forename><surname>Speer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Catherine</forename><surname>Havasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4444" to="4451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Reinforcement learning -an introduction</title>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Dialog theory for critical argumentation</title>
		<author>
			<persName><forename type="first">Douglas</forename><forename type="middle">N</forename><surname>Walton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Controversis</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<date type="published" when="2007">2007</date>
			<pubPlace>Benjamin/Cummings</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">GaussianPath: A Bayesian Multi-Hop Reasoning Framework for Knowledge Graph Reasoning</title>
		<author>
			<persName><forename type="first">Guojia</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4393" to="4401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Text Embeddings by Weakly-Supervised Contrastive Pre-training</title>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Binxing</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linjun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rangan</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Furu</forename><surname>Wei</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2212.03533</idno>
		<idno type="arXiv">ARXIV.2212.03533arXiv:2212.03533</idno>
		<ptr target="https://doi.org/10.48550/" />
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Symbolic Knowledge Distillation: from General Language Models to Commonsense Models</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>West</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandra</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Hessel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jena</forename><forename type="middle">D</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liwei</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronan</forename><surname>Le Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ximing</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Welleck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL-HLT. ACL</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="4602" to="4625" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning</title>
		<author>
			<persName><forename type="first">Ronald</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992">1992. 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">DeepPath: A Reinforcement Learning Method for Knowledge Graph Reasoning</title>
		<author>
			<persName><forename type="first">Wenhan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thien</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP. ACL</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="564" to="573" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
