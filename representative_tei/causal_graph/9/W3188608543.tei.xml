<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Counterfactual Mean Embeddings</title>
				<funder ref="#_fhwq2Ym">
					<orgName type="full">European Research Council</orgName>
				</funder>
				<funder>
					<orgName type="full">Faculty of Science, Mahidol University</orgName>
				</funder>
				<funder ref="#_pahxGJT">
					<orgName type="full">VILLUM FONDEN</orgName>
				</funder>
				<funder ref="#_pW5BrEb">
					<orgName type="full">Thailand Research Fund</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2021-07-10">10 Jul 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Krikamol</forename><surname>Muandet</surname></persName>
							<email>krikamol@tuebingen.mpg.de</email>
						</author>
						<author>
							<persName><forename type="first">Motonobu</forename><surname>Kanagawa</surname></persName>
							<email>motonobu.kanagawa@eurecom.fr</email>
						</author>
						<author>
							<persName><surname>Copenhagen</surname></persName>
						</author>
						<author>
							<persName><surname>Denmark</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Max Planck Institute for Intelligent Systems Tübingen</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Data Science Department</orgName>
								<orgName type="institution">EURECOM Sophia Antipolis</orgName>
								<address>
									<country key="FR">France</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">National Electronics and Computer Technology Center National Science and Technology Development Agency Pathumthani</orgName>
								<orgName type="institution">University of Copenhagen</orgName>
								<address>
									<country key="TH">Thailand</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="department">Department of Mathematics</orgName>
								<orgName type="institution">Mahidol University</orgName>
								<address>
									<country key="TH">Thailand</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">Institute of Statistical Mathematics</orgName>
								<orgName type="institution">University of Tübingen</orgName>
								<address>
									<country key="JP">Japan</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department">Max Planck Institute for Intelligent Systems</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Counterfactual Mean Embeddings</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-07-10">10 Jul 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1805.08845v4[stat.ML]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>counterfactual inference</term>
					<term>kernel mean embedding</term>
					<term>potential outcome framework</term>
					<term>reproducing kernel Hilbert space</term>
					<term>causality</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Counterfactual inference has become a ubiquitous tool in online advertisement, recommendation systems, medical diagnosis, and econometrics. Accurate modelling of outcome distributions associated with different interventions-known as counterfactual distributions-is crucial for the success of these applications. In this work, we propose to model counterfactual distributions using a novel Hilbert space representation called counterfactual mean embedding (CME). The CME embeds the associated counterfactual distribution into a reproducing kernel Hilbert space (RKHS) endowed with a positive definite kernel, which allows us to perform causal inference over the entire landscape of the counterfactual distribution. Based on this representation, we propose a distributional treatment effect (DTE) which can quantify the causal effect over entire outcome distributions. Our approach is nonparametric as the CME can be estimated under the unconfoundedness assumption from observational data without requiring any parametric assumption about the underlying distributions. We also establish a rate of convergence of the proposed estimator which depends on the smoothness of the conditional mean and the Radon-Nikodym derivative of the underlying marginal distributions. Furthermore, our framework allows for more complex outcomes such as images, sequences, and graphs. Our experimental results on synthetic data and off-policy evaluation tasks demonstrate the advantages of the proposed estimator.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>To make a rational decision, a decision maker must be able to anticipate the effects of a decision to the outcomes of interest, before committing to that decision. For instance, before building a certain facility in a city, e.g., a dam, policymakers and citizens must seek to understand its environmental effects. In medicine, a doctor has some prior knowledge about the effects a certain drug will have on a patient's health, before actually prescribing it. In business, a company needs to understand the effects of a certain strategy of advertisement to its revenue. One approach to addressing these questions is counterfactual inference.</p><p>Counterfactual inference we consider in this work consists of the following three main ingredients. Suppose that there exists a hypothetical subject (e.g., a patient in medical treatment), and let X be covariates representing the features of the subject (e.g., age, weight, medical record, etc.), T be a treatment indicator representing the treatment assigned to the subject (a drug of interest or a placebo), and Y be the observed outcome representing the post-treatment quantity of interest (e.g., whether the patient is recovered or not). Given certain realizations of these variables {(x i , t i , y i )} n i=1 , in which each index i represents the identity of a subject, an analyst wishes to know how the treatment affects the outcome.</p><p>This problem is called counterfactual since for each subject i, we only observe the outcome y i resulting from the assigned treatment t i and can never observe the outcome (say y i ) that would have been realized under an alternative treatment t i = t i . For example, if a patient receives an active treatment (e.g., a drug of interest), we can never observe the outcome from the same patient under a control treatment (e.g., a placebo). This is known as the fundamental problem of causal inference <ref type="bibr" target="#b35">(Holland, 1986)</ref> and also as bandit feedback in the bandit literature <ref type="bibr" target="#b18">(Dudík et al., 2011)</ref>. One way to partially address this issue is a randomized experiment <ref type="bibr" target="#b21">(Fisher, 1935)</ref>, in which treatments are randomly assigned to subjects. Although considered a gold standard, in practice randomization can be too expensive, time-consuming, or unethical. In most cases, therefore, analysis about treatment effects needs to be done on the basis of observational data {(x i , t i , y i )} n i=1 in which the treatment assignment t i may depend on covariates x i and possibly on some hidden confounders; this setting is commonly known as observational studies <ref type="bibr" target="#b58">(Rosenbaum, 2002;</ref><ref type="bibr" target="#b62">Rubin, 2005)</ref>.</p><p>A fundamental framework for observational studies is the potential outcome framework <ref type="bibr" target="#b53">(Neyman, 1923;</ref><ref type="bibr" target="#b61">Rubin, 1974)</ref>. It provides a clear notation for potential outcomes, i.e., the outcomes that would have been observed under different treatments, and elucidates the conditions required for making a valid inference about treatment effects; see Section 3.1. The framework has been studied extensively in statistics, and has a wide range of applications in biomedical and social sciences; see, e.g., <ref type="bibr" target="#b38">Imbens and Rubin (2015)</ref>. Moreover, important applications of machine learning such as off-policy evaluation for online advertisement and recommendation systems can be reformulated under this framework <ref type="bibr" target="#b63">(Schnabel et al., 2016;</ref><ref type="bibr" target="#b43">Kallus and Zhou, 2018)</ref>. We argue, however, that there exist the following challenges: Average treatment effects. Many of existing works focus on estimating the average treatment effect (ATE), which is the difference between the means of the outcome distributions; see Section 3.1 for details. However, the ATE does not inform changes in higher-order moments, even when they exist. For instance, if a treatment of interest has an effect only in the variance of the distribution of outcomes, then the analysis of average treatment effects cannot capture such effects. Suppose that the treatment is whether to provide a certain drug, and the outcome is the blood pressure of a patient; just analyzing the average treatment effects may lead to an incorrect conclusion, if the drug increases/decreases the blood pressure of a patient whose blood pressure was already high/low. This highlights the importance of analyzing the outcome distribution as a whole.</p><p>In this work, we focus on the distributional treatment effect (DTE), which involves the entire outcome distributions. This scenario often arises in several real-world socioeconomic applications; see, e.g., <ref type="bibr" target="#b60">Rothe (2010)</ref>; <ref type="bibr" target="#b13">Chernozhukov et al. (2013)</ref>.</p><p>Parametric models. Many of the classical approaches in causal inference make parametric assumptions about relationships between covariates X, treatment assignment T , and observed (or potential) outcomes. However, if the imposed parametric assumption is incorrect, i.e., model misspecification, then the conclusion about treatment effects can be wrong or misleading. To overcome this limitation, there is a recent surge in applying nonparametric machine learning models to causal inference problems, e.g., <ref type="bibr" target="#b65">Shalit et al. (2016)</ref> and <ref type="bibr" target="#b0">Alaa and van der Schaar (2017)</ref> among others. This paper also contributes to this endeavour.</p><p>Overparameterized models. Deep learning has become the first choice in many applied fields due to its excellent empirical performance, and thus has also been applied to counterfactual inference, e.g., <ref type="bibr" target="#b40">Johansson et al. (2016)</ref>; <ref type="bibr" target="#b31">Hartford et al. (2017)</ref>. Unfortunately, such approaches based on deep learning lack theoretical guarantees, because arguably deep learning itself lacks an established theory as a learning method (at least until now). This is problematic when consequential decisions are based on the analysis of treatment effects (e.g., political decisions and medical treatments). Having better theoretical grounding, kernel methods have recently become popular tools for causal inference <ref type="bibr" target="#b0">(Alaa and van der Schaar, 2017;</ref><ref type="bibr" target="#b69">Singh et al., 2019;</ref><ref type="bibr">Muandet et al., 2020a,b)</ref>.</p><p>Multivariate and structured outputs. Existing works often deal with outcomes that are discrete or real-valued. However, depending on the application, outcome variables may be multivariate (possibly high-dimensional) or structured, such as images and graphs. For example, in medical data analysis, outcomes may be fMRI data taken from a subject after receiving a certain treatment. Thus, it is not straightforward to apply existing approaches.</p><p>In this work, we propose a novel approach to counterfactual inference that addresses the above challenges, which we term counterfactual mean embedding (CME). Our approach is built on kernel mean embedding <ref type="bibr" target="#b6">(Berlinet and Thomas-Agnan, 2004;</ref><ref type="bibr" target="#b71">Smola et al., 2007;</ref><ref type="bibr" target="#b50">Muandet et al., 2017)</ref>, a framework for representing probability distributions as elements in a reproducing kernel Hilbert space (RKHS), so that each element representing a distribution maintains all of its information (cf. Section 2.2 and 2.3). We define an element representing a counterfactual distribution, for which we propose a nonparametric estimator. Notable advantages of the proposed approach are summarized as follows:</p><p>1. The proposed estimator can be computed based only on linear algebraic operations involving kernel matrices. Being a kernel method, it can be applied to not only standard domains (such as the Euclidean space), but also more complex and structured covariates and/or outcomes such as images, sequences, and graphs, by using off-theshelf kernels designed for such data <ref type="bibr" target="#b27">(Gärtner, 2003)</ref>; this widens possible applications of counterfactual inference in general (cf. Section 3.4). Thus our work offers more flexibility than the existing approaches by <ref type="bibr" target="#b60">Rothe (2010)</ref> and <ref type="bibr" target="#b13">Chernozhukov et al. (2013)</ref>, who focused on estimating the cumulative distribution functions of counterfactual distributions by assuming real-valued outcomes.</p><p>2. The proposed estimator can be used for computing a distance between the counterfactual and controlled distributions, thereby providing a way of quantifying the effect of a treatment to the distribution of outcomes; we define this distance as the maximum mean discrepancy (MMD) <ref type="bibr" target="#b7">(Borgwardt et al., 2006;</ref><ref type="bibr" target="#b29">Gretton et al., 2012)</ref> between the counterfactual and controlled distributions. It also provides a way to sample points from a counterfactual distribution based on kernel herding <ref type="bibr" target="#b12">(Chen et al., 2010)</ref>, a kernelbased deterministic sampling method (cf. Section 3.5).</p><p>3. The proposed estimator is nonparametric, and has theoretical guarantees. Specifically, we prove the consistency of the proposed estimator under a very mild condition (cf. Theorem 8), and derive its convergence rates under certain regularity assumptions involving kernels and underlying distributions (cf. Theorem 13). Both results hold without assuming any parametric assumption.</p><p>The rest of the paper is organized as follows. After summarizing related work in Section 1.1, we review in Section 2 the potential outcome framework as well as kernel mean embedding of distributions. Section 3 introduces counterfactual learning and then provides a generalization of Hilbert space embedding to counterfactual distributions. This section also presents how we can quantify and estimate distributional treatment effects (DTEs) with our approach. We subsequently provide the detailed convergence analysis in Section 4, followed by examples of the important applications in Section 5 (sampling and testing) and Section 6 (off-policy evaluation). Finally, we demonstrate the effectiveness of the proposed estimator on simulated data as well as real-world policy evaluation tasks in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Related Work</head><p>We summarize below related works on counterfactual inference.</p><p>Treatment effect estimation. Estimating treatment effects is one of the most fundamental tasks in counterfactual inference <ref type="bibr" target="#b61">(Rubin, 1974;</ref><ref type="bibr" target="#b66">Shalit et al., 2017)</ref>. This task is hindered by the fact that one cannot observe all potential outcomes at the same time for each subject. Moreover, the data is usually biased by a non-randomized treatment assignment. Modern approaches attempt to resolve these problems by usingo state-of-the-art ML algorithms. For example, <ref type="bibr" target="#b33">Hill (2011)</ref> develops a nonparametric method for estimating the ITE based on Bayesian additive regression tree (BART). <ref type="bibr" target="#b3">Athey and Imbens (2016)</ref> and Wager and <ref type="bibr" target="#b85">Athey (2018)</ref> adapt tree-based methods to treatment effect estimation. <ref type="bibr" target="#b65">Shalit et al. (2016)</ref> and <ref type="bibr" target="#b40">Johansson et al. (2016)</ref> formulate the problem as a domain adaptation problem and propose to balance the covatiates using representation learning. <ref type="bibr" target="#b31">Hartford et al. (2017)</ref> develop a two-step regression method based on deep neural networks for instrumental variable regression. Adversarial training of neural networks for causal inference have also been considered in <ref type="bibr" target="#b88">Yoon et al. (2018)</ref>, for example.</p><p>Off-policy evaluation and learning from observational data. In many circumstances, evaluating and learning a policy by interacting directly with an environment may not be possible due to practical constraints (e.g., monetary costs, safety and ethics). As a result, several works have attempted to leverage historical data collected using a logging policy in off-policy evaluation and learning, e.g., <ref type="bibr" target="#b46">Langford et al. (2008)</ref>; <ref type="bibr" target="#b2">Atan et al. (2018)</ref>. Most methods rely on importance weighting <ref type="bibr" target="#b46">(Langford et al., 2008;</ref><ref type="bibr" target="#b8">Bottou et al., 2013;</ref><ref type="bibr" target="#b80">Swaminathan and Joachims, 2015)</ref>. <ref type="bibr" target="#b18">Dudík et al. (2011)</ref> uses a doubly robust estimator to reduce the variance of off-policy evaluation. <ref type="bibr" target="#b80">Swaminathan and Joachims (2015)</ref> presents a framework for policy learning called counterfactual risk minimization (CRM) based on empirical variance regularization. In this work, we also demonstrate the application of our estimator in off-policy evaluation.</p><p>Causal inference with kernel mean embeddings. Hilbert space embedding of distributions has been applied extensively in causal inference. For instance, in causal discovery, <ref type="bibr" target="#b23">Fukumizu et al. (2008)</ref>; <ref type="bibr" target="#b89">Zhang et al. (2011)</ref>; <ref type="bibr" target="#b17">Doran et al. (2014)</ref> develop powerful kernelbased tests of conditional independence which allow for the recovery of causal graphs up to the Markov equivalence class. See <ref type="bibr">Muandet et al. (2017, Section 4.8)</ref> for a review of many other applications. In treatment effect estimation, kernel methods have become a popular approach to covariate balancing between treatment and control groups <ref type="bibr" target="#b65">(Shalit et al., 2016;</ref><ref type="bibr" target="#b40">Johansson et al., 2016;</ref><ref type="bibr" target="#b87">Wong and Chan, 2017;</ref><ref type="bibr" target="#b42">Kallus, 2017)</ref>. Our work, on the contrary, focuses on characterizing the representation of counterfactual distribution of outcomes using the kernel mean embedding and provides nonparametric inference tools.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Preliminaries</head><p>The counterfactual mean embedding relies on the potential outcome framework as well as the concepts of kernels, reproducing kernel Hilbert spaces (RKHSs), and kernel mean embedding of distributions. We review these concepts in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Kernels and Reproducing Kernel Hilbert Spaces (RKHSs)</head><p>We first review kernels and RKHSs, details of which can be found in, e.g., <ref type="bibr" target="#b64">Schölkopf and Smola (2002)</ref>, <ref type="bibr" target="#b6">Berlinet and</ref><ref type="bibr" target="#b6">Thomas-Agnan (2004), and</ref><ref type="bibr" target="#b71">Smola et al. (2007)</ref>.</p><p>Let X be a nonempty set. Let H be a Hilbert space consisting of functions on X with •, • H and • H being its inner-product and norm, respectively. The Hilbert space H is called a reproducing kernel Hilbert space (RKHS), if there exists a symmetric function k : X × X → R, called the reproducing kernel of H , satisfying the following properties:</p><p>1. For all x ∈ X , we have k(•, x) ∈ H . Here k(•, x) is the function of the first argument with x being fixed, such that x → k(x , x).</p><p>2. For all f ∈ H and x ∈ X , we have</p><formula xml:id="formula_0">f (x) = k(•, x), f H . This is called the reproducing property of H (or of k).</formula><p>It is known that the linear span of functions k(•, x), denoted by span(k(</p><formula xml:id="formula_1">•, x) | x ∈ X ), is dense in H , i.e., H = span(k(•, x) | x ∈ X ),</formula><p>where the closure on the right hand side is taken with respect to the norm of H . In other words, any f ∈ H can be written as</p><formula xml:id="formula_2">f = ∞ i=1 α i k(•, x i ) for some (α i ) ∞ i=1 ⊂ R and (x i ) ∞ i=1 ⊂ X such that ∞ i=1 α i k(•, x i ) 2 H = ∞ i,j=1 α i α j k(x i , x j ) &lt; ∞.</formula><p>Any RKHS is uniquely associated with its reproducing kernel k, which is positive definite: a symmetric function k : X × X → R is called positive definite, if for all n ∈ N, α 1 , . . . , α n ∈ R, and all x 1 , . . . , x n ∈ X , we have n i=1 n j=1 α i α j k(x i , x j ) ≥ 0. On the other hand, for any positive definite kernel k : X × X → R, there exists an RKHS H for which k is the reproducing kernel <ref type="bibr" target="#b1">(Aronszajn, 1950)</ref>. Therefore, by defining a positive definite kernel, one always implicitly defines its RKHS.</p><p>As indicated from the definition of positive definiteness, kernels can be defined on any nonempty set X . Therefore, they have been defined not only for the real vector space R d , but also for non-standard domains such as those of images and graphs. Popular kernels on</p><formula xml:id="formula_3">X ⊂ R d include linear kernels k(x, x ) = x x , polynomial kernels k(x, x ) = (x x +c) p , c &gt; 0, p ∈ N + , Gaussian kernels k(x, x ) = exp(-x -x 2</formula><p>2 /2σ 2 ), σ &gt; 0, and Laplace (or more generally Matérn) kernels k(x, x ) = exp(xx 2 /2σ 2 ), σ &gt; 0. More examples of positive definite kernels can be found in <ref type="bibr" target="#b28">Genton (2002)</ref> and <ref type="bibr" target="#b34">Hofmann et al. (2008)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Kernel Mean Embedding of Distributions</head><p>In this work, we use kernels and RKHSs to represent, compare, and estimate probability distributions. This is enabled by the approach known as kernel mean embedding of distributions <ref type="bibr" target="#b6">(Berlinet and Thomas-Agnan, 2004;</ref><ref type="bibr" target="#b71">Smola et al., 2007;</ref><ref type="bibr" target="#b50">Muandet et al., 2017)</ref>, which we review here. In what follows, we assume that X is a measurable space with some sigma algebra B X .</p><p>Definition 1 (Kernel mean embedding (KME)) Let P be the set of all probability measures on a measurable space (X , B X ) and k : X × X → R be a measurable positive definite kernel with associated RKHS H , such that sup x∈X k(x, x) &lt; ∞. Then, the kernel mean embedding (KME) of P ∈ P is defined as the Bochner integral 1 of k(•, x) with respect to P:</p><p>µ : P → H , P → µ P := k(•, x) dP(x).</p><p>(1)</p><p>The element µ P may be alternatively called the kernel mean of P. For a random variable X ∼ P, the kernel mean may also be written as µ X .</p><p>The kernel mean µ P serves as a representation of P ∈ P in the RKHS H . This is justified if H is characteristic <ref type="bibr" target="#b24">(Fukumizu et al., 2004)</ref>: the RKHS H (and the associated kernel k) is defined to be characteristic, if the mapping µ : P → H in (1) is injective. In other words, H is characteristic, if for any P, Q ∈ P, we have µ P = µ Q if and only if P = Q. That is, µ P is uniquely associated with P ∈ P, and thus µ P becomes a unique representation of P in H , maintaining all information about P. Examples of characteristic kernels on X = R d include Gaussian, Matérn and Laplace kernels <ref type="bibr" target="#b75">(Sriperumbudur et al., 2010)</ref>. On the other hand, linear and polynomial kernels are not characteristic, since their RKHSs are finite dimensional and only provide unique representations of distributions up to certain moments.</p><p>The kernel mean embedding (1) is the key ingredient of a well-known metric on probability measures called maximum mean discrepancy (MMD) <ref type="bibr" target="#b7">(Borgwardt et al., 2006;</ref><ref type="bibr" target="#b29">Gretton et al., 2012)</ref>. For two distributions P, Q ∈ P, their MMD is given as the RKHS distance between the corresponding kernel means µ P , µ Q :</p><formula xml:id="formula_4">MMD[H , P, Q] := µ P -µ Q H = sup f ∈H , f H ≤1 f (x) dP(x) -f (x) dQ(x) ,<label>(2)</label></formula><p>where the second identity follows from the reproducing property and H being a vector space <ref type="bibr">(Gretton et al., 2012, Lemma 4)</ref>. The right expression is the maximum discrepancy between the means of functions from the unit ball of the RKHS H , and is the original definition of MMD. Being defined via the RKHS distance, MMD is a pseudo-metric on P. Moreover, if H is characteristic, MMD[H , P, Q] = 0 holds if and only if P = Q, and thus MMD becomes a proper metric on probability measures. See <ref type="bibr" target="#b75">Sriperumbudur et al. (2010)</ref>; Simon-Gabriel and <ref type="bibr" target="#b68">Schölkopf (2018)</ref> for details and relationships to other popular metrics on probability measures. Given an i.i.d. (identically and independently distributed) sample x 1 , . . . , x n from P, the kernel mean µ P can be estimated simply by the empirical average</p><formula xml:id="formula_5">μP := 1 n n i=1 k(•, x i ). (3) The √ n-consistency of (3), that is µ P -μP H = O p (n -1/2 ) as n → ∞, has been established</formula><p>in <ref type="bibr">Song (2008, Theorem 27)</ref> and also in <ref type="bibr" target="#b29">Gretton et al. (2012)</ref>; <ref type="bibr" target="#b49">Lopez-Paz et al. (2015)</ref>; <ref type="bibr" target="#b82">Tolstikhin et al. (2017)</ref>. Importantly, this holds without any parametric assumption about the underlying distribution P. Given another i.i.d. sample x 1 , . . . , x m from Q, and defining μQ := 1 m m j=1 k(•, x j ) as an estimate of the kernel mean µ Q , the (squared) MMD (2) can be estimated as</p><formula xml:id="formula_6">MMD 2 [H , P, Q] = μP -μQ 2 H = 1 n 2 n i=1 n j=1 k(x i , x j ) - 2 nm n i=1 m j=1 k(x i , x j ) + 1 m 2 m i=1 m j=1 k(x i , x j ),</formula><p>where the right expression follows from the reproducing property <ref type="bibr">(Gretton et al., 2012, Eq. 5)</ref>. Applying the triangle inequality, it follows that</p><formula xml:id="formula_7">| µ P -µ Q H -μP -μQ H | ≤ µ P -μP H + µ Q -μQ H = O p (n -1/2 ) + O p (m -1/2</formula><p>) as n, m → ∞, implying the consistency of the above estimator of MMD with a parametric convergence rate. This estimator only requires evaluations of the kernel, and therefore is easy to implement in practice. We note that the above MMD estimator is biased, while being consistent; an unbiased estimator is also available for MMD <ref type="bibr">(Gretton et al., 2012, Eq. 3)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Kernel Mean Embedding of Conditional Distributions</head><p>Finally, the notion of KME can be extended to conditional distributions <ref type="bibr" target="#b73">(Song et al., 2009;</ref><ref type="bibr" target="#b30">Grünewälder et al., 2012;</ref><ref type="bibr" target="#b74">Song et al., 2013;</ref><ref type="bibr" target="#b25">Fukumizu et al., 2013)</ref>. To describe this, let (X, Y ) be a random variable taking values in the product space X × Y, where X and Y are measurable spaces. We define a measurable kernel k on X and let H be the associated RKHS. Similarly, we define a measurable kernel on Y and let F be the associated RKHS.</p><p>Let P XY be the joint distribution of (X, Y ), and P Y |X=x be the conditional distribution of Y given X = x.</p><p>The KME of the conditional distribution P Y |X=x is then defined as the conditional expectation of (•, y) with respect to P Y |X=x :</p><formula xml:id="formula_8">µ Y |X=x := (•, y) dP Y |X=x (y) ∈ F (x ∈ X ).<label>(4)</label></formula><p>Again, if F is characteristic, this kernel mean maintains all information about P Y |X=x , thus being qualified as its representation. It is instructive to note that µ Y |X=x is defined for each x ∈ X individually. Given an i.i.d. sample (x 1 , y 1 ), . . . , (x n , y n ) from the joint distribution P XY , the conditional mean embedding (4) can be estimated as</p><formula xml:id="formula_9">μY |X=x := n i=1 w i (x) (•, y i ),<label>(5)</label></formula><p>where</p><formula xml:id="formula_10">(w 1 (x), . . . , w n (x)) := (K + nεI) -1 k(x) ∈ R n , k(x) := (k(x, x 1 ), . . . , k(x, x n )) ∈ R n .</formula><p>Here, K ∈ R n×n is the kernel matrix such that K i,j = k(x i , x j ), and ε &gt; 0 is a regularization constant. As pointed out by <ref type="bibr" target="#b30">Grünewälder et al. (2012)</ref>, this estimator can be interpreted as that of function-valued kernel ridge regression, where the task is to estimate the mapping x → (•, y) dP Y |X=x (y) from training data (x 1 , (•, y 1 )), . . . , (x n , (•, y n )) ∈ X × F . In fact, the weights w 1 (x), . . . , w n (x) in (5) are identical to those of kernel ridge regression (or Gaussian process regression). As such, the regularization constant ε should decay to 0 at an appropriate speed as n → ∞, in order to ensure a good convergence rate of the estimator (5), see, e.g., <ref type="bibr" target="#b9">Caponnetto and Vito (2007)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Counterfactual Mean Embeddings</head><p>In this section, we formulate our problem of estimating distributional treatment effects and describe our approach. In Section 3.1, we review the potential outcome framework and, based on it, we define distributional treatment effects. The key concepts here are counterfactual distributions on outcomes. In Section 3.2, we describe our approach, counterfactual mean embeddings, as the kernel mean embeddings of counterfactual distributions. Section 3.3 provides details of the distributional effects of covariate distributions, which are essential for applications in off-policy evaluation. We then define their empirical estimators in Section 3.4. Finally, we introduce the kernel treatment effect (KTE) as a way to evaluate the distributional treatment effect in Section 3.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Potential Outcome Framework and Distributional Causal Effects</head><p>We pose our problem based on the potential outcome framework, also known as the Neyman-Rubin causal model, which is a classic and widely used approach to estimating causal effects of treatments from observational data <ref type="bibr" target="#b53">(Neyman, 1923;</ref><ref type="bibr" target="#b61">Rubin, 1974</ref><ref type="bibr" target="#b62">Rubin, , 2005))</ref>.</p><p>We consider a hypothetical subject (e.g., a patient) in a population. Let X ∈ X be a covariate random variable representing the subject's features (e.g., age, weight, blood pressure, etc.), where X is a measurable space. Let T ∈ T a random variable that indicates the treatment assigned to the subject, where T denotes the set of treatments of interest. We call T treatment indicator or treatment assignment. In this work, we focus on binary treatments T := {0, 1} for simplicity, but an extension to multiple treatments is straightforward. For instance, T = 1 may represent that the subject is assigned an active treatment (e.g., a drug of interest), and T = 0 a control treatment (e.g., placebo).</p><p>Let Y * 0 , Y * 1 ∈ Y be random variables representing potential outcomes, where Y is a measurable space. That is, Y * 1 represents the outcome of interest after the subject is exposed to treatment 1, and Y * 0 the outcome after the subject is exposed to treatment 0. For instance, Y * 1 may be the blood pressure of the patient measured after the patient had the drug, and Y * 0 be that after having nothing. The problem here, known as the fundamental problem of causal inference, is that one can only observe either Y * 1 or Y * 0 , but not both. For instance, if one gave the drug to the patient and measured the resulting blood pressure, it is no longer possible to measure the blood pressure of the same patient without the drug. Thus, the observed outcome Y ∈ Y can be defined as</p><formula xml:id="formula_11">Y := 1(T = 0)Y * 0 + 1(T = 1)Y * 1 ,</formula><p>where 1(T = j) := 1 if T = j, and zero otherwise. Note that in observational studies, the treatment assignment may not be completely random, i.e., T depends on Y * 0 , Y * 1 and X. Assume that there are N subjects, and that each subject i = 1, . . . , N is associated with random variables (x i , t i , y * 0i , y * 1i ) that are distributed as (X, T, Y * 0 , Y * 1 ) independently to the other subjects,<ref type="foot" target="#foot_0">foot_0</ref> i.e.,</p><formula xml:id="formula_12">(x i , t i , y * 0i , y * 1i ) N i=1 ∼ (X, T, Y * 0 , Y * 1 ), i.i.d.<label>(6)</label></formula><p>Note that for each subject i, only one of y * 0i or y * 1i can be observed. Thus, observational data given to the analyst are</p><formula xml:id="formula_13">(x i , t i , y i ) N i=1 , y i := 1(t i = 0)y * 0i + 1(t i = 1)y * 1i ,<label>(7)</label></formula><p>which are i.i.d. with (X, T, Y ). We write n := N i=1 1(t i = 0) the number of subjects receiving treatment T = 0, and m := N i=1 1(t i = 1) that of treatment T = 1.</p><p>We consider three kinds of distributional causal effect, as described below. For ease of understanding, we also present the corresponding expressions based on the sample (6). Nevertheless, these sample expressions are also counterfactual quantities due to the fundamental problem of causal inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Distributional Treatment Effect (DTE)</head><p>Let P Y * 0 and P Y * 1 be the distributions of the potential outcomes Y * 0 and Y * 1 , respectively. Then we define the distributional treatment effect (DTE) as the difference between these two distributions:</p><formula xml:id="formula_14">P Y * 0 (•) -P Y * 1 (•).<label>(8)</label></formula><p>The corresponding sample expression is given by</p><formula xml:id="formula_15">1 N N i=1 δ(• -y * 0i ) - 1 N N i=1 δ(• -y * 1i ),</formula><p>where δ is the Dirac distribution. As mentioned, this sample expression cannot be obtained from observational data (7), since for each subject i, we only have either y * 0i or y * 1i . The DTE (8) can capture the treatment effects on the potential outcomes that may not be identified only by the average treatment effect (ATE) <ref type="bibr" target="#b37">(Imbens, 2004)</ref>, the difference between the expectations of Y * 0 and Y * 1 :</p><formula xml:id="formula_16">ATE := E[Y * 0 ] -E[Y * 1 ]<label>(9)</label></formula><p>or its corresponding sample version</p><formula xml:id="formula_17">ATE N := 1 N N i=1 y * 0i - 1 N N i=1 y * 1i .</formula><p>For instance, even when the ATE is 0, the higher order moments of P Y * 0 and P Y * 1 , such as their variances, may differ. The DTE can capture such a difference, while the ATE cannot.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Distributional Treatment Effects on the Treated</head><p>This is defined as the difference in two conditional distributions as</p><formula xml:id="formula_18">P Y * 1 |T (• | t) -P Y * 0 |T (• | t), t ∈ {0, 1}.<label>(10)</label></formula><p>For t = 1, this can be understood as the distributional treatment effect for the treated, and the corresponding sample expression is given by</p><formula xml:id="formula_19">1 m N i=1 1(t i = 1)δ(• -y * 1i ) - 1 m N i=1 1(t i = 1)δ(• -y * 0i ),</formula><p>where the second term is counterfactual. The details of the conditional treatment effect (10) can be found, for example, in <ref type="bibr" target="#b13">Chernozhukov et al. (2013</ref><ref type="bibr">Chernozhukov et al. ( , p.2214))</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Distributional Effects of the Covariate Distributions</head><p>This is defined as the difference between the conditional distribution of Y * 0 given T = 0 and that of Y * 0 given T = 1:</p><formula xml:id="formula_20">P Y * 0 |T (• | 0) -P Y * 0 |T (• | 1)<label>(11)</label></formula><p>where P Y * 0 |T is the conditional distribution of Y * 0 given T . A similar definition can be given for Y * 1 . The corresponding sample expression is given by</p><formula xml:id="formula_21">1 n N i=1 1(t i = 0)δ(• -y * 0i ) - 1 m N i=1 1(t i = 1)δ(• -y * 0i ).<label>(12)</label></formula><p>Note that the second term in ( <ref type="formula" target="#formula_20">11</ref>) and ( <ref type="formula" target="#formula_21">12</ref>) are counterfactual in the sense that the potential outcome y * 0i of subject i with t i = 1 is not observable. The above distributional differences capture the effects caused by the difference in the characteristics (i.e., covariates) of subjects exposed to different treatments, e.g., selection bias, rather than the effects caused by the treatment itself. For instance, let us assume that a drug was assigned to subjects whose blood pressures were already high (t i = 1) and not assigned to subjects with low blood pressures (t i = 0). Then the counterfactual blood pressures y * 0i of the subjects with t i = 1, which would have been observed if they had not taken the drug, would be higher than those y * 0i with t i = 0. This kind of "selection bias" can be captured in the above distributional difference, and this helps understand how the difference in observed outcome distributions arises. To explain this more precisely, however, we need the notation, definitions and assumptions introduced in the next subsection. Thus, we defer further explanations to Section 3.3. There, we also explain that this distributional difference is useful in studying counterfactual effects of a policy defined as a specification of a covariate distribution. In fact, this is how we formulate the problem of off-policy evaluation in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Counterfactual Distributions</head><p>To deal with distributional treatment effects discussed in the previous subsection, we need to introduce the notion of counterfactual distributions <ref type="bibr" target="#b13">(Chernozhukov et al., 2013)</ref>. We first summarize the notation defined above and introduce new ones, which we follow <ref type="bibr">Chernozhukov et al. (2013, Appendix C)</ref>.</p><p>Definition 2 Let Y * 0 and Y * 1 be random variables taking values in Y, and X and T be random variables taking values in X and T = {0, 1}, respectively. The random variables Y , Y t and X t (t = 0, 1) are defined as</p><formula xml:id="formula_22">Y := 1(T = 0)Y * 0 + 1(T = 1)Y * 1 , Y t := Y | T = t (t = 0, 1), X t := X | T = t (t = 0, 1).</formula><p>In Definition 2, Y is the observed outcome variable. Thus, Y t is Y given that the treatment assignment is T = t (t = 0, 1). By the definition of Y , this implies that Y t = Y * t | (T = t), that is, Y t is the potential outcome conditional on T = t. Note that, since Y * t and T may be dependent, Y * t | (T = t) may differ from Y * t as a random variable. The variable X t is the covariate variable X conditional on T = t. The pair of variables (X t , Y t ) can thus be seen as observed random variables conditional on the treatment assignment</p><formula xml:id="formula_23">T = t.</formula><p>The following is a key assumption, which is needed in general for counterfactual inference with observational data.</p><formula xml:id="formula_24">Assumption 1 (A1) Conditional exogeneity: Y * 0 , Y * 1 ⊥ ⊥ T | X almost surely for X.</formula><p>(A2) Support condition: X 0 = X 1 , where X j is the support of the distribution P X j of X j for j = 0, 1.</p><formula xml:id="formula_25">T Y * 1 Y * 0 X Potential Outcomes</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Covariates Treatment Indicator</head><p>Figure <ref type="figure">1</ref>: A graphical representation of the conditional exogeneity assumption. An edge between two random variables indicates that they are dependent. The conditional exogeneity assumption states that given the covariates X, the potential outcomes Y * 0 , Y * 1 and the treatment assignment T are conditionally independent. The assumption does not hold if there exists an edge between Y * 0 , Y * 1 and T , which is the case, for instance, when there exists a hidden confounder Z that is dependent to both Y * 0 , Y * 1 and T .</p><p>The conditional exogeneity (A1), also known as the unconfoundedness or ignorability, is a common assumption in observational studies to guarantee the identifiability of causal effects from observational data <ref type="bibr" target="#b59">(Rosenbaum and Rubin, 1983;</ref><ref type="bibr" target="#b37">Imbens, 2004;</ref><ref type="bibr" target="#b62">Rubin, 2005)</ref>. It requires that there is no hidden confounder, say Z, that affects both the treatment assignment T and potential outcomes Y * 0 , Y * 1 . In other words, the covariates X include all important characteristics regarding the potential outcomes. This assumption is described further in Figure <ref type="figure">1</ref>, where the graphical model represents the conditional independence structure between the random variables. The support condition (A2) is needed to make the counterfactual distribution (introduced in (13) below) well-defined, and is also made in <ref type="bibr">Chernozhukov et al. (2013, Eq. 2.3)</ref>. It is analogous to the overlap assumption required for propensity score methods (e.g. <ref type="bibr">Imbens, 2004, Assumption 2.2)</ref>.</p><p>We now define counterfactual distributions. Let P X 0 and P X 1 be the probability distributions of X 0 and X 1 , respectively. Denote by P Y 0|0 and P Y 1|1 the corresponding marginal distributions of outcomes defined by</p><formula xml:id="formula_26">P Y 0|0 (y) := P Y 0 |X 0 (y|x) dP X 0 (x) = P Y 0 (y) P Y 1|1 (y) := P Y 1 |X 1 (y|x) dP X 1 (x) = P Y 1 (y)</formula><p>where P Y 0 |X 0 (y|x) is the conditional distribution of Y 0 given X 0 , and <ref type="bibr">Following Chernozhukov et al. (2013)</ref>, counterfactual distributions are then defined as</p><formula xml:id="formula_27">P Y 1 |X 1 (y|x) is that of Y 1 given X 1 .</formula><formula xml:id="formula_28">P Y 0|1 (y) := P Y 0 |X 0 (y|x) dP X 1 (x),<label>(13)</label></formula><formula xml:id="formula_29">P Y 1|0 (y) := P Y 1 |X 1 (y|x) dP X 0 (x),<label>(14)</label></formula><p>which are well-defined as long as the support condition in Assumption 1 is satisfied. The distributions introduced above are defined in terms of the observed random variables (X t , Y t ) t=0,1 . We now see how these distributions are related to the distributions on potential outcomes that appear in distributional causal effects <ref type="bibr">(10)</ref>  , which play the key role in analyzing distributional treatment effects (10) (11), can be obtained by estimating the corresponding counterfactual distributions P Y 0|1 and P Y 1|0 defined in terms of observed random variables (X t , Y t ) t=0,1 . The key assumption in this regard is the conditional exogeneity in Assumption 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Further Explanation on the Distributional Effects of Covariate Distributions</head><p>We are now in a position to provide further explanation on the distributional difference introduced in Section 3.1.3. To this end, let us assume that the conditional exogeneity in Assumption 1 is satisfied. Then, by Lemmas 3 and 4, the distributional difference in (11) can be written as</p><formula xml:id="formula_30">P Y * 0 |T (y | 0) -P Y * 0 |T (y | 1) = P Y 0|0 (y) -P Y 0|1 (y) = P Y 0 |X 0 (y|x) dP X 0 (x) -P Y 0 |X 0 (y|x) dP X 1 (x). (<label>15</label></formula><formula xml:id="formula_31">)</formula><p>The rhs of (15) shows that this distributional difference (if it exists) is due to the difference between the covariate distributions P X 0 and P X 1 . In what follows, we provide two distinct interpretations. First, it quantifies a selection bias that affects the difference in observed outcome distributions. Second, it quantifies the causal effect for a policy implemented as a specification of a covariate distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1">Quantifying a Selection Bias</head><p>We can decompose the difference in the observed outcome distributions P Y 0 and P Y 1 as</p><formula xml:id="formula_32">P Y 0 (y) -P Y 1 (y) = P Y * 0 |T (y | 0) -P Y * 1 |T (y | 1) = P Y * 0 |T (y | 0) -P Y * 0 |T (y | 1) (A) + P Y * 0 |T (y | 1) -P Y * 1 |T (y | 1) (B)</formula><p>,</p><p>where the first term (A) is the distributional effect of covariate distributions (15), and the second term (B) is the distributional treatment effect on the treated. Thus, the difference in the observed outcome distributions can arise from (A) and/or (B), and the estimation of (A) and (B) is useful in studying the origin of the difference in observed outcome distributions. For instance, if we find that (A) is zero, the difference between the observed outcome distributions is originated from the distributional difference on the treated (B). On the other hand, if (B) is zero, then the difference between the observed outcome distributions is due to (A), i.e., by the selection bias, and is not due to the effects of the treatment.</p><p>Note that this difference is different from the difference between the potential outcome distributions P Y * 0 , P Y * 1 , which accounts for the effects of the treatments 0 and 1 and thus is of primary interest. The observed outcome distributions P Y 0 , P Y 1 are biased approximations to the potential outcome distributions, if the treatment assignment is not randomized (i.e., if X and T are not independent).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2">Policy as a Specification of a Covariate Distribution</head><p>The distributional difference (15) can also be used to quantify the effects of a policy that specifies a covariate distribution. Recall that we introduced the random variables X 0 , X 1 as the covariate random variable X conditioned on T = t, t ∈ {0, 1}, i.e., X t := X | (T = t). We can instead directly define two random variables X 0 , X 1 by specifying their probability distributions P X 0 = P X|T (• | 0) and P X 1 = P X|T (• | 1), respectively. In this case, the conditioning T = t for t ∈ {0, 1} may be regarded as specifying the covariate distribution P Xt = P X|T (• | t) on the space of covariates X . This specification of the covariate distribution P Xt = P X|T (• | t) itself can be regarded as a certain policy. 3  For instance, Rothe (2010, Section 5.2) used this formulation to study the effects of smoking of a pregnant mother on the birth weight of the baby. There, the observed outcome Y &gt; 0 is the birth weight of the baby, and covariates X := (X 1 , X 2 , X 3 , X 4 ) ∈ R 4 are relevant features of the mother: X 1 is the number of cigarettes per day, X 2 is the age, X 3 is the weight gain and X 4 is the marital status. The distribution P X 0 is the covariate distribution of available data of smoking mothers, while P X 1 is a transformation of P X 0 so that the number of cigarettes per day, X 1 0 , is reduced to 75%. Thus, T = 1 or P X 1 may be regarded as a hypothetical policy that reduces the amount of cigarettes of smoking pregnant women. Then,</p><formula xml:id="formula_33">P 0|1 (y) = P Y 0 |X 0 (y | x) dP X 1 (x)</formula><p>is the counterfactual distribution of the birth weights of babies that would have been observed if the mothers had smoked 75 % less amount of cigarettes than they actually did. The comparison to the observed outcome distribution P 0|0 (y) = P Y 0 |X 0 (y | x) dP X 0 (x) then enables studying the effects of the amount of cigarettes on birth weights.</p><p>Another important instance is the off-policy evaluation task, which will be discussed further in Section 6.</p><p>3. Here we use the terminology "policy" instead of "treatment" not to confuse the two notions. In our paper, a "treatment" t ∈ {0, 1} specifies the corresponding potential outcome Y * t and its distribution P Y * t ; thus, the difference between P Y * 0 and P Y * 1 characterizes the treatment effects. On the other hand, a "policy" here t ∈ {0, 1} specifies the corresponding covariate random variable Xt and its distribution PX t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Kernel Mean Embeddings for Counterfactual Distributions</head><p>We now define counterfactual mean embeddings. Let be a positive definite kernel on Y with RKHS F , and assume that the support condition in Assumption 1 is satisfied. We then refer to the kernel mean embeddings of the counterfactual distributions ( <ref type="formula" target="#formula_28">13</ref>) and ( <ref type="formula" target="#formula_29">14</ref>)</p><formula xml:id="formula_34">µ Y 0|1 := (•, y) dP Y 0|1 (y) ∈ F ,<label>(16)</label></formula><formula xml:id="formula_35">µ Y 1|0 := (•, y) dP Y 1|0 (y) ∈ F ,<label>(17)</label></formula><p>as counterfactual mean embeddings (CME). Lemma 4 implies that, under Assumption 1, these CMEs are respectively identical to the kernel mean embeddings of P Y * 0 |T (y|1) and</p><formula xml:id="formula_36">P Y * 1 |T (y|0) defined as µ Y * 0 |T =1 := (•, y) dP Y * 0 |T (y|1), µ Y * 1 |T =0 := (•, y) dP Y * 1 |T (y|0).</formula><p>Therefore, by defining an empirical estimator of the CME ( <ref type="formula" target="#formula_34">16</ref>), one can hope to estimate the distributional treatment effects in ( <ref type="formula" target="#formula_18">10</ref>) and ( <ref type="formula" target="#formula_20">11</ref>), which will be done below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Estimating counterfactual mean embeddings.</head><p>In what follows, we introduce our estimator of the CME µ Y 0|1 defined in ( <ref type="formula" target="#formula_34">16</ref>); one can define an estimator of ( <ref type="formula" target="#formula_35">17</ref>) in a similar manner. In practice, it is not possible to obtain a sample from P Y 0|1 , and therefore the counterfactual mean embedding µ Y 0|1 cannot be estimated directly. Instead, we propose an estimator that uses samples from P X 0 Y 0 and P X 1 to estimate µ Y 0|1 . To this end, first note that µ Y 0|1 in ( <ref type="formula" target="#formula_34">16</ref>) can be written in terms of the conditional mean embedding (4) of</p><formula xml:id="formula_37">P Y 0 |X 0 =x : µ Y 0|1 = µ Y 0 |X 0 =x dP X 1 (x) ∈ F , where µ Y 0 |X 0 =x := (•, y) dP Y 0 |X 0 =x (y) ∈ F .</formula><p>This formulation suggests that µ Y 0|1 can be estimated by i) constructing an estimator of the conditional mean embedding µ Y 0 |X 0 =x and then ii) taking its average over P X 1 (x). This is how our estimator is derived below.</p><p>Suppose that we are given independent samples (x 1 , y 1 ), . . . , (x n , y n ) from P Y 0 X 0 (x, y) and x 1 , . . . , x m from P X 1 (x). For x ∈ X , let μY 0 |X 0 =x denote the estimate (5) of the conditional mean embedding µ Y 0 |X 0 =x based on (x 1 , y 1 ), . . . , (x n , y n ). Then, an empirical estimator of µ Y 0|1 is defined and expressed as</p><formula xml:id="formula_38">μY 0|1 := 1 m m j=1 μY 0 |X 0 =x j = n i=1 β i (•, y i ) with (β 1 , . . . , β n ) = (K + nεI) -1 K1 m , (<label>18</label></formula><formula xml:id="formula_39">)</formula><p>where ε &gt; 0 is a regularization constant,</p><formula xml:id="formula_40">1 m = (1/m, . . . , 1/m) ∈ R m , K ∈ R n×n with K ij = k(x i , x j ), and K ∈ R n×m with K ij = k(x i , x j ).</formula><p>The proposed estimator ( <ref type="formula" target="#formula_38">18</ref>) is nonparametric, and can be implemented without knowledge about parametric forms of the conditional P Y 0 |X 0 and marginal P X 1 . Thus, the estimator is useful when such knowledge is not available. In Section 4, we theoretically analyze the asymptotic behavior of the estimator, proving its consistency and deriving convergence rates. In doing so, we elucidate conditions required for the consistency of the proposed estimator.</p><p>The computational complexity of our estimator ( <ref type="formula" target="#formula_38">18</ref>) is O(n 3 ) because of the matrix inversion, which may be expensive when the sample size n is huge. To reduce the complexity, one can adopt existing approximation methods such as Nyström method and random Fourier features <ref type="bibr" target="#b86">(Williams and Seeger, 2001;</ref><ref type="bibr" target="#b57">Rahimi and Recht, 2008)</ref>.</p><p>We note that the form of the estimator is identical to the kernel sum rule <ref type="bibr">(Song et al., 2013, Section 4.1)</ref>, a mean embedding approach to computing forward probabilities in Bayesian inference. The way we use the estimator is different from this previous approach, however. That is, we use our estimator to estimate the counterfactual distribution and distributional causal effects (11), and this requires Assumption 1 to hold for data (or for the population random variables), as shown in Lemma 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Kernel Treatment Effects</head><p>We quantify distributional treatment effects by using the RKHS distance between the mean embeddings of potential outcome distributions under consideration. We call this approach Kernel Treatment Effects (KTE). We show below how KTEs can be defined for the different distributional treatment effects discussed in Section 3.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.1">KTE for Distributional Treatment Effects</head><p>As before, let be a kernel on the output space Y and F be its RKHS. For the distributional treatment effect (8) discussed in Section 3.1.1, the corresponding KTE is defined as</p><formula xml:id="formula_41">KTE(Y * 0 , Y * 1 , F ) := µ Y * 0 -µ Y * 1 F ,<label>(19)</label></formula><p>where µ Y * 0 and µ Y * 1 are the kernel mean embeddings of the distributions of potential outcomes P Y * 0 and P Y * 1 , respectively, i.e.,</p><formula xml:id="formula_42">µ Y * 0 := (•, y) dP Y * 0 (y), µ Y * 1 := (•, y) dP Y * 1 (y).<label>(20)</label></formula><p>The KTE (19) may be regarded as a generalization of the ATE (9) in the sense that, if is the linear kernel (y, y ) = y, y on Y = R d , then the KTE only distinguishes the means of the two outcome distributions. By using a different kernel , the KTE may capture the differences between higher-order statistics of the outcome distributions <ref type="formula" target="#formula_41">19</ref>) is equal to 0 if and only if P Y * 0 and P Y * 1 have the same moments up to degree m (see, e.g., <ref type="bibr">Muandet et al. 2017, Chapter 3)</ref>.</p><formula xml:id="formula_43">P Y * 0 and P Y * 1 . For instance, if is a polynomial kernel (y, y ) = ( y, y + c) m of degree m ∈ N with c &gt; 0, then the KTE (</formula><p>If is a characteristic kernel, such as Gaussian and Matérn kernels, then the KTE ( <ref type="formula" target="#formula_41">19</ref>) is equal to 0 if and only if the two distributions P Y * 0 and P Y * 1 are the same. In this case, the KTE takes a positive value if and only if there is a difference between P Y * 0 and P Y * 1 . This means that the KTE informs the existence of any difference in the potential outcome distributions, quantifying the distributional treatment effect.</p><p>The question is how to estimate the KTE (19) from data. As in (7), let (x i , t i , y i ) N i=1 be observational data, which are i.i.d. with the random variables (X, T, Y ). Recall that Y is the observed outcome and thus given by Y</p><formula xml:id="formula_44">= 1(T = 0)Y * 0 + 1(T = 1)Y *</formula><p>1 where Y * 0 and Y * 1 are the potential outcomes. In observational studies, it is common to use the propensity score e(x) := E[T | X = x], the conditional probability of the treatment assignment T being made given that the covariates are X = x, to define an unbiased estimator of the average treatment effect <ref type="bibr" target="#b59">and Rubin, 1983)</ref>. We show here that the same strategy of inverse propensity weighting <ref type="bibr" target="#b37">(Imbens, 2004</ref>, Section III-C) can be straightforwardly used to define unbiased estimators of the mean embeddings µ Y * 1 and µ Y * 0 of potential outcome distributions P Y * 1 and P Y * 0 , respectively, thus providing a way of estimating the KTE. That is, assuming that the propensity e(x) is available, we define</p><formula xml:id="formula_45">E[Y * 1 ] -E[Y * 0 ] (Rosenbaum</formula><formula xml:id="formula_46">μY * 1 := 1 m N i=1 t i (•, y i ) e(x i ) , μY * 0 := 1 n N j=1 (1 -t j ) (•, y j ) 1 -e(x j ) ,<label>(21)</label></formula><p>where m := N i=1 t i and n := N j=1 (1-t j ) are the populations of treated and control groups, respectively.</p><p>In the special case of a completely randomized experiment where X and T are independent and thus the propensity is e(x) = 1/2 for all x ∈ X , the above estimators reduce to the standard empirical estimators of mean embeddings:</p><formula xml:id="formula_47">μY * 1 := 2 m N i=1 t i (•, y i ) and μY * 0 := 2 n N j=1 (1 -t j ) (•, y j ).</formula><p>Note that these uniformly-weighted empirical estimators are biased if the experiment is not completely randomized, i.e., in observational studies. This is because, for instance, the sample y i contributing to μY * 1 follows the distribution of</p><formula xml:id="formula_48">Y * 1 |T = 1, which is different from the unconditional Y *</formula><p>1 . Thus, we need the inverse propensity weighting to obtain unbiased estimators in the case of observational studies.</p><p>The following result shows that the estimators (21) are indeed unbiased estimators of the corresponding mean embeddings µ Y * 1 and µ Y * 0 of potential outcome distributions. The proof is presented in Appendix C.3.</p><p>Theorem 5 Suppose that 0 &lt; e(x) &lt; 1 for all x ∈ X and that the conditional exogeneity in Assumption 1 is satisfied. Let (x i , t i , y i ) N i=1 be i.i.d. with (X, T, Y ), and let μY * 1 and μY * 0 be the estimators (21) of the mean embeddings µ Y * 1 and µ Y * 0 of the potential outcome distributions P Y * 1 and P Y * 0 in (20). Then, we have</p><formula xml:id="formula_49">E[μ Y * 1 ] = µ Y * 1 , E[μ Y * 0 ] = µ Y * 0 .</formula><p>Theorem 5 shows that the estimators (21) are unbiased, but does not say anything about their convergence rates as the sample size goes to infinity. The following result provides this; it essentially shows that the estimators (21) converge to the mean embeddings µ Y * 1 and µ Y * 0 at the same rates as the standard kernel mean estimators, which are minimax optimal <ref type="bibr" target="#b82">(Tolstikhin et al., 2017)</ref>. The key assumption here is that the propensity e(x) is uniformly lower-and upper-bounded away from 0 and 1. The proof is presented in Appendix C.4.</p><p>Theorem 6 Suppose the propensity score e(x) satisfies inf x∈X e(x) &gt; 0 and sup x∈X e(x) &lt; 1, that sup y∈Y (y, y) &lt; ∞, and that the conditional exogeneity in Assumption 1 is satisfied. Let (x i , t i , y i ) N i=1 be i.i.d. with (X, T, Y ), and let μY * 1 and μY * 0 be the estimators (21) of the mean embeddings µ Y * 1 and µ Y * 0 of the potential outcome distributions P Y * 1 and P Y * 0 in (20). Then, we have</p><formula xml:id="formula_50">E μY * 1 -µ Y * 1 2 F = O(m -1 ), E μY * 0 -µ Y * 0 2 F = O(n -1 ), (N → ∞),</formula><p>where m := N i=1 t i and n := N i=1 (1t i ).</p><p>Based on the estimators (21), we can define a consistent estimator of ( <ref type="formula" target="#formula_41">19</ref>) as</p><formula xml:id="formula_51">KTE 2 b (Y * 0 , Y * 1 , F ) := μY * 1 -μY * 0 2 F = μY * 1 2 F -2 μY * 1 , μY * 0 F + μY * 0 2 F (22) = 1 m 2 N i,j=1 t i t j (y i , y j ) e(x i )e(x j ) - 2 mn N i,j=1 t i (1 -t j ) (y i , y j ) e(x i )(1 -e(x j )) + 1 n 2 N i,j=1</formula><p>(1</p><formula xml:id="formula_52">-t i )(1 -t j ) (y i , y j ) (1 -e(x i ))(1 -e(x j )) ,</formula><p>where the last equality follows from the reproducing property of the kernel . By the triangle inequality, we have</p><formula xml:id="formula_53">| KTE b (Y * 0 , Y * 1 , F ) -KTE(Y * 0 , Y * 1 , F )|= | μY * 1 -μY * 0 F -µ Y * 0 - µ Y * 1 F |≤ | μY * 1 -µ Y * 1 F + μY * 0 -µ Y * 0 F |= O p (m -1/2 +n -1/2</formula><p>) as n, m → ∞, which shows that the estimator ( <ref type="formula">22</ref>) is asymptotically unbiased.</p><p>Note that ( <ref type="formula">22</ref>) is a biased estimator, while being asymptotically unbiased. This bias is caused by the terms with identical indices (i = j) in the first and third summations of ( <ref type="formula">22</ref>). Thus, by subtracting these terms, an unbiased estimator of the KTE can be defined as</p><formula xml:id="formula_54">KTE 2 u (Y * 0 , Y * 1 , F ) := 1 m(m -1) i =j t i t j (y i , y j ) e(x i )e(x j ) (23) - 2 mn N i,j=1 t i (1 -t j ) (y i , y j ) e(x i )(1 -e(x j )) + 1 n(n -1) i =j (1 -t i )(1 -t j ) (y i , y j ) (1 -e(x i ))(1 -e(x j ))</formula><p>.</p><p>By similar arguments as in the proof of Theorem 6, it can be shown that this is indeed an unbiased estimator of (the square of) <ref type="bibr">KTE (19)</ref>. Moreover, since it can be shown that</p><formula xml:id="formula_55">KTE 2 u (Y * 0 , Y * 1 , F ) -KTE 2 b (Y * 0 , Y * 1 , F ) = O p (m -1 + n -1 ) (n, m → ∞)</formula><p>given that the assumptions in Theorem 6 hold, this unbiased estimator ( <ref type="formula">23</ref>) enjoys the same convergence rate as the biased one ( <ref type="formula">22</ref>):</p><formula xml:id="formula_56">| KTE u (Y * 0 , Y * 1 , F ) -KTE(Y * 0 , Y * 1 , F )|= O p (m -1/2 + n -1/2 ) as n, m → ∞.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.2">KTE for Distributional Treatment Effects on the Treated</head><p>We define KTE for the distributional effect</p><formula xml:id="formula_57">P Y * 1 |T (• | t) -P Y * 0 |T (• | t) introduced in Section 3.1.2,</formula><p>where t ∈ {0, 1}. We only consider the case t = 1 here, which is interpreted as the distributional treatment effect for the treated; the case t = 0 can be defined similarly. The definition is</p><formula xml:id="formula_58">KTE(Y * 1 |(T = 1), Y * 0 |(T = 1), F ) := µ Y * 1 |T =1 -µ Y * 0 |T =1 F ,<label>(24)</label></formula><p>where</p><formula xml:id="formula_59">µ Y * 1 |T =1 and µ Y * 0 |T =1 are the kernel mean embeddings of P Y * 1 |T (• | 1) and P Y * 0 |T (• | 1), respectively: µ Y * 1 |T =1 := (•, y) dP Y * 1 |T (y | 1), µ Y * 0 |T =1 := (•, y) dP Y * 0 |T (y | 1).</formula><p>Lemma 3 shows that P Y 1|1 (y) = P Y * 1 |T (y | 1), while Lemma 4 implies that P Y 0|1 = P Y * 0 |T =1 under Assumption 1. Thus, we can define an estimator of the above KTE (24) as follows. Let μY 0|1 = n i=j β i (•, y j ) be the estimator (18) of the CME, and let μY 1|1 := 1 m m i=1 (y i , •) where y1 , . . . , ym is a sample from P Y 1|1 . Note that such y1 , . . . , ym can be obtained in practice, as P Y 1|1 is the distribution of the observed outcome Y given that the treatment assignment is T = 1. Then, we can define an empirical estimator of the KTE in (24) as follows:</p><formula xml:id="formula_60">KTE 2 (Y * 1 |(T = 1), Y * 0 |(T = 1), F ) := μY 1|1 -μY 0|1 2 F = 1 m 2 m i,j=1 (y i , yj ) - 2 m m i=1 n j=1 β j (y i , y j ) + n i,j=1</formula><p>β i β j (y i , y j ).</p><p>(25)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.3">KTE for Distributional Effects of the Covariate Distributions</head><p>Similarly, we can define a KTE for the distributional effect</p><formula xml:id="formula_61">P Y * 0 |T (• | 0) -P Y * 0 |T (• | 1) defined in (11) by KTE(Y * 0 |(T = 0), Y * 0 |(T = 1), F ) := µ Y * 0 |T =0 -µ Y * 0 |T =1 F ,<label>(26)</label></formula><p>where µ Y * 0 |T =0 and µ Y * 0 |T =1 are the kernel mean embeddings of</p><formula xml:id="formula_62">P Y * 0 |T (• | 0) and P Y * 0 |T (• | 1), respectively, i.e., µ Y * 0 |T =0 := (•, y) dP Y * 0 |T (y | 0), µ Y * 0 |T =1 := (•, y) dP Y * 0 |T (y | 1).</formula><p>Lemma 3 shows that P Y 0|0 (y) = P Y * 0 |T (y | 0), and Lemma 4, under Assumption 1, implies that P Y 0|1 = P Y * 0 |T =1 . Therefore, we define an estimator of (26) in the following way. Let μY 0|1 be the estimator (18) of the CME, and let μY 0|0 := 1 n n i=1 (•, ỹi ) where ỹ1 , . . . , ỹn is a sample from P Y 0|0 . Note that such ỹ1 , . . . , ỹn can be obtained in practice, as P Y 0|0 is the distribution of the observed outcome Y given that the treatment assignment is T = 0. Then, we can define an empirical estimator of the KTE in (26) as follows:</p><formula xml:id="formula_63">KTE 2 (Y * 0 |(T = 0), Y * 0 |(T = 1), F ) := μY 0|0 -μY 0|1 2 F = 1 n 2 n i,j=1 (ỹ i , ỹj ) - 2 n n i,j=1 β j (ỹ i , y j ) + n i,j=1 β i β j (y i , y j ).<label>(27)</label></formula><p>In the next section, we analyze the convergence behavior of the proposed CME estimator in (18) as the sample size n goes to infinity. Readers who are interested in applications may skip the next section and jump to Section 5 and Section 6 directly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Convergence Analysis of the CME Estimator</head><p>We provide here a convergence analysis of the CME estimator introduced in Section 3.4. In Section 4.1, we first establish its consistency under mild assumptions. In Section 4.2, we derive its convergence rates by making a quantitative assumption on the smoothness of certain functions involved.</p><p>We first introduce necessary notation and definitions. We assume that the covariate space X and the outcome space Y are measurable spaces, and that the kernels k and are measurable on X and Y, respectively, with H and F being their respective RKHSs. Let P X 0 and P X 1 be the probability distributions of the random variables X 0 ∈ X and X 1 ∈ X , respectively (see Definition 2 for the definition of these random variables).</p><p>Let L 2 (P X 0 ) be the Hilbert space of square-integral functions<ref type="foot" target="#foot_1">foot_1</ref> with respect to P X 0 :</p><formula xml:id="formula_64">L 2 (P X 0 ) := f : X → R | f 2 (x) dP X 0 (x) &lt; ∞ ,</formula><p>which is equipped with the inner product f, g L 2 (P X 0 ) := f (x)g(x) dP X 0 (x) and the re-</p><formula xml:id="formula_65">sulting norm f L 2 (P X 0 ) := f, f L 2 (P X 0 )</formula><p>. Let P X 0 ⊗ P X 0 be the product measure of P X 0 and P X 0 on the product space X × X .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Consistency</head><p>To establish the consistency of the CME estimator, we require the following conditions.</p><p>Assumption 2 Assume that the following conditions are satisfied:</p><p>(i) The kernels k and are bounded on X and Y, respectively, that is, sup x∈X k(x, x) &lt; ∞ and sup y∈Y (y, y) &lt; ∞.</p><p>(ii) The RKHS H of k is dense in L 2 (P X 0 ).</p><p>(iii) The distribution P X 1 is absolutely continuous with respect to P X 0 with the Radon-Nikodym derivative g := dP X 1 / dP X 0 satisfying g ∈ L 2 (P X 0 ).</p><p>(iv) (x 1 , y 1 ), . . . , (x n , y n ) are i.i.d. observations of the random variables (X 0 , Y 0 ), and x 1 , . . . , x m are i.i.d. observations of the random variable X 1 , with n = m.</p><p>Remark 7 We make the following comments on Assumption 2.</p><p>• The boundedness condition (i) is satisfied, for instance, if k and are shift-invariant kernels, such as Gaussian and Matérn kernels.</p><p>• The condition (ii) requires that the RKHS be rich enough to approximate squareintegrable functions with respect to P X 0 . For instance, this is satisfied by the Gaussian kernel <ref type="bibr">(Steinwart and Christmann, 2008, Theorem 4.63)</ref>, and therefore by any kernel whose RKHS is larger than that of the Gaussian kernel, such as Laplace and Matèrn kernels <ref type="bibr">(Steinwart and Christmann, 2008, Theorem 4.48</ref>).</p><p>• The condition (iii) requires the support of P X 1 be included in that of P X 0 , and thus is related to the common support assumption in Assumption 1. If both P X 1 and P X 0 have density functions p X 1 and p X 0 , respectively, with respect to a common reference measure (e.g., the Lebesgue measure in the case of X ⊂ R d ), then the Radon-Nikodym derivative becomes the density ratio or the importance weight function g(x) = p X 1 (x)/p X 0 (x). Thus, the square-integrability of g requires, intuitively, that p X 1 should not be very different from p X 0 .</p><p>• In the condition (iv), we assume n = m for simplicity of presentation.</p><p>Before presenting the result, we introduce a function θ : X × X → R defined by</p><formula xml:id="formula_66">θ(x, x) := (y, ỹ) dP Y 0 |X 0 (y|x) dP Y 0 |X 0 (ỹ|x). (<label>28</label></formula><formula xml:id="formula_67">)</formula><p>This function appears in the proof of consistency, and also is needed to derive convergence rates in Section 4.2. Note that the assumption in (i) that being bounded implies that θ ∈ L 2 (P X 0 ⊗ P X 0 ); this property is used in the proof of consistency. Theorem 8 below shows the consistency of the CME estimator (18). The proof can be found in Appendix E.2.</p><p>Theorem 8 (Consistency) Suppose that Assumption 2 is satisfied. Let μY 0|1 be the estimator defined in (18) with a regularization constant</p><formula xml:id="formula_68">ε n &gt; 0. Then if ε n → 0 and n 1/2 ε n → ∞ as n → ∞, we have μY 0|1 -µ Y 0|1 F → 0 in probability as n → ∞.</formula><p>Remark 9 As discussed, the form of the CME estimator (18) is the same as that of the kernel sum rule, and <ref type="bibr">Fukumizu et al. (2013, Theorem 8)</ref> proves its consistency. Unlike ours, however, <ref type="bibr" target="#b25">Fukumizu et al. (2013)</ref> assume that the function θ in (28) belongs to the tensor-product RKHS H ⊗ H , which is a rather strong assumption for proving just the consistency. For instance, if H is the RKHS of the Gaussian kernel, then this assumption requires that θ be infinitely differentiable. The theoretical contribution of our analysis is in removing this condition.</p><p>Recall that by Lemma 4 we have µ Y 0|1 = µ Y * 0 |T =1 under the conditional exogeneity condition (Assumption 1). Thus, Theorem 8 implies that the CME estimator is consistent in estimating µ Y * 0 |T =1 , as summarized in the following corollary. This justifies the use of the CME estimator in dealing with counterfactual questions, as will be described in Section 5.</p><p>Corollary 10 Suppose that Assumptions 1 and 2 are satisfied. Let μY 0|1 be the estimator defined in (18) with a regularization constant</p><formula xml:id="formula_69">ε n &gt; 0. Then, if ε n → 0 and n 1/2 ε n → ∞ as n → ∞, we have μY 0|1 -µ Y * 0 |T =1 F → 0</formula><p>in probability as n → ∞.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Convergence Rates</head><p>Next, we present a result on the convergence rate of the CME estimator (18). This result is obtained based on certain smoothness assumptions on the Radon-Nikodym derivative g = dP X 1 / dP X 0 and the function θ defined in (28). To state these assumptions, we need to introduce the following concepts, details of which can be found in Appendix D.1.</p><p>In the sequel, I ⊂ N denotes a set of indices, which is a finite set or an infinite set depending on whether the RKHS H is finite dimensional (e.g., if k is a linear or polynomial kernel) or infinite dimensional (e.g., if k is a Gaussian or Matérn kernel). We define an integral operator T : L 2 (P X 0 ) → L 2 (P X 0 ) by</p><formula xml:id="formula_70">T f := k(•, x)f (x) dP X 0 (x), f ∈ L 2 (P X 0 ).</formula><p>Intuitively, the output function T f is a smoother version of the input function f , as T f can be seen as a convolution between f and the kernel k.</p><p>Under Assumption 2 (i) and (ii), there exist at most countable families of functions (e i ) i∈I ⊂ H and the associated positive constants</p><formula xml:id="formula_71">(µ i ) i∈I ⊂ (0, ∞) such that i) µ 1 ≥ µ 2 ≥ • • • &gt; 0, that ii) (µ 1/2 i e i ) i∈I is an orthonormal basis (ONB) in H , that iii) (e i ) i∈I is an ONB in L 2 (P X 0 )</formula><p>, and that iv) the integral operator can be written as</p><formula xml:id="formula_72">T f = i∈I µ i f, e i L 2 (P X 0 ) e i ,</formula><p>with convergence in L 2 (P X 0 ); see Lemmas 16 and 18 in Appendix D.1. In other words, the pairs (µ i , e i ) i∈I are eigenvalues and eigenfunctions of the integral operator: T e i = µ i e i for i ∈ I. Based on this eigendecomposition, one can define a power of the integral operator T : for a constant α ≥ 0, the α-th power of T is defined as</p><formula xml:id="formula_73">T α f := i∈I µ α i f, e i L 2 (P X 0 ) e i , f ∈ L 2 (P X 0 ).</formula><p>We now make the following assumption about the smoothness of the Radon-Nikodym derivative g = dP X 1 / dP X 0 , where Range(T α ) denotes the range or image of T α . This way of stating a smoothness condition is common in learning theory for kernel methods, e.g., <ref type="bibr" target="#b9">Caponnetto and Vito (2007)</ref>; <ref type="bibr" target="#b70">Smale and Zhou (2007)</ref>; <ref type="bibr" target="#b25">Fukumizu et al. (2013)</ref>.</p><p>Assumption 3 There exists a constant 0 ≤ α ≤ 1 such that the Radon-Nikodym derivative g = dP X 1 / dP X 0 satisfies g ∈ Range(T α ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Remark 11</head><p>• Assumption 3 quantifies the smoothness of the Radon-Nikodym derivative g = dP X 1 / dP X 0 by the constant 0 ≤ α ≤ 1. That is, g is smoother if α is close to 1 and less smooth if α is close to 0. Since we have dP X 1 (x) = g(x) dP X 0 (x), a larger α may therefore be understood as that P X 0 and P X 1 are more similar. This interpretation can be obtained as follows.</p><p>• The assumption implies that there exists a square-integrable function f ∈ L 2 (P X 0 ) that g = T α f . As mentioned, T acts as a smoother, outputting a smoothed version T f of an input function f . Similarly, its power T α acts as a smoother, but now α determines the degree of smoothness of the output function. For α = 0, T α is just the identity map, and there is no effect of smoothing. As α increases, the degree of smoothness increases. In fact, <ref type="bibr">Steinwart and Scovel (2012, Theorem 4.6)</ref> shows that Range(T α ) for 0 &lt; α ≤ 1/2 is equal to an interpolation space between L 2 (P X 0 ) and the RKHS H as a set of functions. In particular, we have Range(T α ) = H for α = 1/2, and thus the assumption implies g ∈ H . Thus, the case α &gt; 1/2 is that g is smoother than the least smooth functions in H .</p><p>We next state a smoothness assumption about the function θ(x, x ) defined in (28). To simplify the presentation, let X 2 := X × X . Define a kernel on X 2 as the product kernel</p><formula xml:id="formula_74">k prod : X 2 × X 2 → R such that k prod ((x 1 , x 2 ), (x 1 , x2 )) := k(x 1 , x1 )k(x 2 , x2 ), (x 1 , x 2 ), (x 1 , x2 ) ∈ X 2 .</formula><p>We then define an integral operator</p><formula xml:id="formula_75">T prod : L 2 (P X 0 ⊗ P X 0 ) → L 2 (P X 0 ⊗ P X 0 ) by T prod η := k prod (•, (x 1 , x2 )) η ((x 1 , x2 )) d (P X 0 ⊗ P X 0 ) ((x 1 , x2 )) , η ∈ L 2 (P X 0 ⊗ P X 0 ).</formula><p>By Assumption 2 (i) and (ii), this can be written in terms of the eigensystem (µ i , e i ) i∈I as</p><formula xml:id="formula_76">T prod η = i,j∈I µ i µ j η, e i ⊗ e j L 2 (P X 0 ⊗P X 0 ) e i ⊗ e j ,</formula><p>where e i ⊗ e j : X 2 → R denotes the tensor product of e i and e j , and the convergence is in L 2 (P X 0 ⊗ P X 0 ); see Lemma 19 in Appendix D. That is, each e i ⊗ e j is an eigenfunction of T prod with the corresponding eigenvalue µ i µ j . The β-th power of T prod for 0 ≤ β ≤ 1 is then defined as</p><formula xml:id="formula_77">T β prod η = i,j∈I (µ i µ j ) β η, e i ⊗ e j L 2 (P X 0 ⊗P X 0 ) e i ⊗ e j .<label>(29)</label></formula><p>Similar to Assumption 3, we make the following smoothness assumption for the function θ : X × X → R defined in (28), based on the range of the power T β prod .</p><p>Assumption 4 There exists a constant 0 ≤ β ≤ 1 such that the function θ defined in (28) satisfies θ ∈ Range(T β prod ).</p><p>Remark 12 As for Assumption 3, we can interpret Assumption 4 as quantifying the smoothness of θ by the constant β. That is, larger β implies that θ is smoother. Note that θ can be written as θ</p><formula xml:id="formula_78">(x, x ) = µ Y 0 |X 0 =x , µ Y 0 |X 0 =x F , where µ Y 0 |X 0 =x := (•, y) dP Y 0 |X 0 (y|x) is the kernel mean of P Y 0 |X 0 (•|x). Therefore, θ is smooth if the mapping x → µ Y 0 |X 0 =x is smooth.</formula><p>Thus, β may be interpreted as quantifying the smoothness of this mapping.</p><p>We are now ready to state Theorem 13 below, which establishes the convergence rate of the CME estimator. The rate is given in terms of the constants α and β introduced in the above assumptions. The proof is given in Appendix E.3.</p><p>Theorem 13 (Convergence rates) Suppose that Assumptions 2, 3 and 4 hold with α + β ≤ 1. Let μY 0|1 be the estimator defined in (18) with a regularization constant ε n &gt; 0. Let c &gt; 0 be an arbitrary constant, and set ε n = cn -1/(1+β+max(1-α,α)) . Then we have</p><formula xml:id="formula_79">μY 0|1 -µ Y 0|1 F = O p n -(α+β)/2(1+β+max(1-α,α))</formula><p>(n → ∞).</p><p>Remark 14 Let us interpret the rate of Theorem 13.</p><p>• The exponent (α+β)/2(1+β +max(1-α, α)) in the rate is smaller than 1/2 for any α and β, and thus the rate is always slower than the parametric rate n -1/2 . For instance, the rate becomes n -1/4 if α = β = 1/2. This is due to the CME estimator being nonparametric, as for other nonparametric statistical estimators in general <ref type="bibr" target="#b83">(Tsybakov, 2008)</ref>. We are not aware of, however, whether the obtained rate is minimax optimal.</p><p>We leave this question for future research.</p><p>• An important interpretation of the rate is as follows: if either the Radon-Nikodym derivative g = dP X 1 / dP X 0 or the function θ is smooth, then the CME estimator converges reasonably fast. For instance, the rate becomes n -1/6 if α = 0 and β = 1, and n -1/4 if α = 1 and β = 0 (recall that α and β quantify the smoothness of g and θ, respectively). Therefore, even in the situation where the change from P X 1 to P X 0 is large, we may still expect a good performance for the CME estimator if the relationship between X 0 and Y 0 is smooth (and vice versa).</p><p>As for Corollary 10, we obtain the following corollary from Theorem 13.</p><p>Corollary 15 Suppose that Assumptions 1, 2, 3 and 4 hold with α + β ≤ 1. Let μY 0|1 be the estimator defined in (18) with a regularization constant ε n &gt; 0. Let c &gt; 0 be an arbitrary constant, and set ε n = cn -1/(1+β+max(1-α,α)) . Then we have</p><formula xml:id="formula_80">μY 0|1 -µ Y * 0 |T =1 F = O p n -(α+β)/2(1+β+max(1-α,α)) (n → ∞).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Applications to Sampling and Testing</head><p>In this section, we discuss important applications of the proposed framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Sampling from Counterfactual Distributions</head><p>While a CME estimate can be seen as a weighted sample (β i , y i ) n i=1 , the coefficients β 1 , . . . , β n may in general include negative values; thus it is not straightforward to interpret them as importance weights. If one can generate sample points ỹ1 , . . . , ỹm from the CME estimate, then these unweighted points may be more useful for an analyst. For instance, we might use them for the purpose of visualization (e.g., scatter plot and histogram). Moreover, as will be described below, such unweighted points can be straightforwardly used for testing hypotheses regarding distributional treatment effects.</p><p>We propose a method for sampling from the counterfactual distribution based on the CME estimator and the kernel herding algorithm <ref type="bibr" target="#b12">(Chen et al., 2010;</ref><ref type="bibr" target="#b44">Kanagawa et al., 2016;</ref><ref type="bibr" target="#b41">Kajihara et al., 2018)</ref>. The method is summarized in Algorithm 1, which generates sample Algorithm 1 Sampling from a counterfactual mean embedding estimate 1: Compute ỹt := arg max y∈Y n i=1 β i (y i , y) -</p><formula xml:id="formula_81">Input: A CME estimate μY 0|1 = n i=1 β i (y i , •) with (β i , y i ) n i=1 ⊂ R × Y</formula><formula xml:id="formula_82">1 t t-1</formula><p>i=1 (ỹ i , y). 5: end for 6: Output: ỹ1 , . . . , ỹm . points ỹ1 , . . . , ỹm from μY 0|1 in (18). When the kernel is shift-invariant (e.g., Gaussian), the procedure in Algorithm 1 to generate ỹ1 , . . . , ỹt for t = 1, . . . , m ∈ N is equivalent to the greedy minimization of the RKHS distance between the CME estimate μY 0|1 and the empirical kernel mean</p><formula xml:id="formula_83">1 t t i=1 (ỹ i , •): μY 0|1 - 1 t t i=1 (ỹ i , •) F = sup f F ≤1 n i=1 β i f (y i ) - 1 t t j=1 f (ỹ j ) . (<label>30</label></formula><formula xml:id="formula_84">)</formula><p>See <ref type="bibr" target="#b12">Chen et al. (2010)</ref> for details. In other words, the points ỹ1 , . . . , ỹm are those greedily minimizing the worst case error to the weighted points (β i , y i ) n i=1 in the unit ball of the RKHS F ; thus, this algorithm is a greedy variant of Quasi Monte Carlo methods <ref type="bibr" target="#b14">(Dick et al., 2013)</ref>. Notice that therefore these points are, of course, not independent to each other. The convergence rate O(n -1/2 ) is guaranteed for 1 t t i=1 (•, ỹi ) <ref type="bibr" target="#b4">(Bach et al., 2012)</ref>, which may hold even when the optimization problem (30) is solved approximately <ref type="bibr" target="#b45">(Lacoste-Julien et al., 2015;</ref><ref type="bibr" target="#b44">Kanagawa et al., 2016)</ref>.</p><p>Lastly, to obtain high dimensional samples, e.g., images, from the counterfactual distribution, one can train deep generative models using MMD-GAN <ref type="bibr" target="#b48">(Li et al., 2015;</ref><ref type="bibr" target="#b19">Dziugaite et al., 2015;</ref><ref type="bibr" target="#b79">Sutherland et al., 2017;</ref><ref type="bibr" target="#b47">Li et al., 2017)</ref> based the CME estimate. We defer this promising application to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Counterfactual Inference as Two-sample Testing</head><p>One can also identify distributional treatment effects by formulating the problem as that of hypothesis testing, or more specifically, two-sample testing. To describe this, we assume that we are given data   <ref type="formula">23</ref>). To decide a critical region, we need the distribution of the test statistic under the null hypothesis H 0 . One way to approximate this distribution is to use a bootstrap procedure <ref type="bibr" target="#b20">(Efron and Tibshirani, 1993)</ref>, as follows. Let B ∈ N be the number of bootstrap samples. For each b = 1, . . . , B, we randomly permute the indices 1, . . . , N to, say, π b (1), . . . , π b (N ) ⊂ {1, . . . , N }. Then we compute the test statistic</p><formula xml:id="formula_85">(x i , t i , y i ) N i=1 , which are i.i.d. with random variables (X, T, Y ) with Y = Y * 0 1(T = 0) + Y * 1 1(T = 1)</formula><formula xml:id="formula_86">, Y * 1 , F ) of the kernel treat- ment effect, KTE(Y * 0 , Y * 1 , F ) = µ Y * 0 -µ Y * 1 2 F ,</formula><formula xml:id="formula_87">η b := KTE(Y * 0 , Y * 1 , F ) based on the permuted data (x i , t π b (i) , y i ) N i=1 .</formula><p>We then approximate the null distribution by the histogram of η 1 , . . . , η B , and determine a critical tail region for rejecting the null hypothesis (e.g., with significance level α = 0.05).</p><p>Distributional effects of the covariate distributions. Here, we are interested in whether the two distributions</p><formula xml:id="formula_88">P Y * 0 |T (• | 0) and P Y * 0 |T (• | 1</formula><p>) are equal or not (see Section 3.1.3). If they are different, there is a distributional effect on the outcomes arising from the difference in covariate distributions. The identification of such an effect can be phrased as a hypothesis test with the null and alternative hypotheses being</p><formula xml:id="formula_89">H 0 : P Y * 0 |T (• | 0) = P Y * 0 |T (• | 1), H 1 : P Y * 0 |T (• | 0) = P Y * 0 |T (• | 1),<label>(31)</label></formula><p>To describe the approach, we rearrange the data (x i , t i , y i ) N i=1 so that t i = 0 for i = 1, . . . , n and t i = 1 for i = n + 1, . . . , n + m = N . Note that y 1 , . . . , y n are a sample from</p><formula xml:id="formula_90">P Y * 0 |T (• | 0), while the distribution P Y * 0 |T (• | 1</formula><p>) is counterfactual and we do not have a sample from it. However, we can estimate the kernel mean of P Y * 0 |T (• | 1) by the CME estimator (18) using the data (x i , y i ) n i=1 and (x j ) m j=1 := (x n+j ) m j=1 under the conditional exogeneity assumption, and let μ 0|1 be the resulting estimate. We then apply kernel herding to μ 0|1 for obtaining sample points ỹ1 , . . . , ỹm approximating P Y * 0 |T (• | 1), as described in Algorithm 1. We can then apply any method for two-sample test, e.g., those in <ref type="bibr" target="#b29">Gretton et al. (2012)</ref>, to the two samples y 1 , . . . , y n and ỹ1 , . . . , ỹm to test the hypotheses (31). We note that this is a rather heuristic approach, since ỹ1 , . . . , ỹm are not drawn from P Y * 0 |T (• | 1), but generated deterministically so as to approximate P Y * 0 |T (• | 1). We leave a further theoretical study regarding the validity of this approach for future research.</p><p>Finally, one can develop a testing procedure for identifying distributional treatment effects on the treated (see Section 3.1.2) in the same way as described here, and thus we omit the explanation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Discussion</head><p>Here we discuss how the above sampling and testing procedures may be used in practice. Suppose that an analyst is interested in the distributional effects of the difference in covariate distributions, i.e., the hypotheses in (31), and that the null hypothesis has been rejected as a result of applying the above testing procedure. This suggests the existence of a difference in the two distributions,</p><formula xml:id="formula_91">P Y * 0 |T (• | 0) and P Y * 0 |T (• | 1)</formula><p>. This is usually not the end of the analysis, but is the starting point of a further exploratory analysis. There are several ways to proceed, to understand how the two outcome distributions differ.</p><p>One way is to use the samples y 1 , . . . , y n and ỹ1 , . . . , ỹm (the latter being a counterfactual sample generated from Algorithm 1) approximating the distributions P Y * 0 |T (• | 0) and</p><formula xml:id="formula_92">P Y * 0 |T (• | 1)</formula><p>, respectively. The analyst can use any available statistical method for finding the source of the difference in the two distributions. For instance, she may compute summary statistics of both samples (e.g., mean, variance, etc.) and compare them. It is also possible to just plot both samples, or to estimate the densities, to visualize the difference (as we demonstrate in Section 7.1). Another useful method in this context is the approach of <ref type="bibr" target="#b39">Jitkrittum et al. (2016)</ref> and their follow-up works, which returns interpretable features for explaining the difference in the two samples, such as the sample locations on which (smoothed version) of the two density values differ substantially.</p><p>Another important point for discussion is the use of non-characteristic kernels. If the kernel is not characteristic, such as polynomial kernels, rejecting the null hypothesis implies that there exist a certain kind of difference in the two distributions. For instance, if the kernel is a polynomial kernel of order 2, then rejecting the null hypothesis implies that there exists a difference in the mean or in the variance of the two outcome distributions. In this sense, if one is interested in the existence of a specific difference in the outcome distributions (such as the mean and variance), non-characteristic kernels may be more useful.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Application to Off-Policy Evaluation (OPE)</head><p>We describe here how our approach can be applied to the off-policy evaluation task (OPE), e.g., <ref type="bibr" target="#b18">Dudík et al. (2011)</ref>, which aims at evaluating the performance of a given target policy of deciding a certain action given a context. The performance is measured in terms of the resulting rewards. For instance, consider a recommendation system, where an action is a list of items to be recommended to a user, and a policy determines which action to take, given the features of the user. There will be a positive reward if the user clicks or buys one of the recommended items, and no reward otherwise. The goal of OPE is to estimate how a given policy would work, without actually implementing the policy. Instead, the evaluation is to be done relying only on logged (or historical) data obtained from a possibly unknown initial policy, which is different from the target policy. This task is important when actually implementing a new policy is expensive or difficult, with wide applications including ad placement, recommendation systems, and health care.</p><p>We first describe the OPE problem more formally in Section 6.1. We then interpret it with the potential outcome framework and formulate the OPE problem as an estimation of a counterfactual distribution in Section 6.2. Finally, we present a concrete algorithm in Section 6.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Problem Description</head><p>Formally, the OPE task may be defined as follows. Let U be a space of context features, A be a space of actions and R be a space of rewards. For instance, in the case of recommendation systems, each u ∈ U represents a user's features, a ∈ A a recommendation (e.g., a list of items), and r ∈ R the number of clicks on the recommendation. A policy π(a|u) is a conditional distribution on the action space A given context features u ∈ U. In a recommendation system, π(a|u) determines the probability of providing a recommendation a to a user having features u.</p><p>Assume that tuples {(u i , a i , r i )} n i=1 ⊂ U × A × R of context features u i ∈ U, action a i ∈ A and reward r i ∈ R are available as logged (or historical) data. We assume that they were independently generated from a joint distribution P 0 (u, a, r) := q 0 (u)π 0 (a|u)P 0 (r|u, a) in the data collection phase, where q 0 (u) is a marginal distribution on U, π 0 (a|u) is an initial (or logging/behavior) policy, and P 0 (r | u, a) is a conditional distribution of a reward r ∈ R given (u, a) ∈ U × A. In a recommendation system, for instance, P 0 (r | u, a) describes whether a user with features u who has been recommended a list of items a would choose one of the items. As such, it is typically unknown a priori. Similarly, q 0 (u) and π 0 (a|u) may be unknown in practice, if {(u i , a i , r i )} n i=1 are given as historical data. Let π * (a|u) be another conditional distribution of actions a ∈ A given context features u ∈ U, which represents the target policy that one wants to evaluate. By design, the target policy is known and sampling from it is possible. Let q * (u) be a probability distribution on U, which represents the distribution of context features under the target environment (e.g., the distribution of user features when a recommendation system is deployed). In the standard OPE setting, it is typically assumed that q 0 (u) = q * (u), i.e., the historical and target environments are the same; but in general these can be different, q 0 (u) = q * (u). The latter situation is not uncommon in practice and has been recently studied by <ref type="bibr" target="#b84">Uehara et al. (2020)</ref>. Finally, let P * (r | u, a) be the conditional distribution of a reward r given context features u and action a under the target environment. We assume that this remains the same as in the data collection phase, i.e.,</p><formula xml:id="formula_93">P * (r | u, a) = P 0 (r | u, a).</formula><p>This assumption may be understood as the policy invariance assumption commonly made in the econometric policy evaluation literature, e.g., <ref type="bibr" target="#b32">Heckman and Vytlacil (2007)</ref>. 5  The task of off-policy evaluation is then to estimate the expected reward under the target environment:</p><formula xml:id="formula_94">R * := U ×A R r dP * (r|u, a) dπ * (u, a) = U ×A R r dP 0 (r|u, a) dπ * (u, a),<label>(32)</label></formula><p>where the identity follows from the assumption P * (r | u, a) = P 0 (r | u, a), and π * (u, a) := π * (a|u)q * (u) is the joint distribution on U × A given by π * (a|u) and q * (u). This estimation is to be done using logged data {(u i , a i , r i )} n i=1 and the target policy π * (u|a).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">OPE as Counterfactual Inference</head><p>We explain below how our CME estimator (18) can be applied to the OPE task. To this end, we consider the marginal distribution of a reward under the target environment:</p><formula xml:id="formula_95">P * (r) := U ×A P * (r | u, a) dπ * (u, a) = U ×A P 0 (r | u, a) dπ * (u, a). (<label>33</label></formula><formula xml:id="formula_96">)</formula><p>Note that the expected reward (32) is the mean of this distribution. We first show that the distribution P * (r) can be interpreted as a counterfactual distribution, by formulating the OPE task using the potential outcome framework 6 in Section 3.</p><p>5. More precisely, this assumption may be identified with the policy invariance assumptions PI-1 and PI-2 in Section 2.2 of <ref type="bibr" target="#b32">Heckman and Vytlacil (2007)</ref>, where s and ω there correspond to a and u in our setting, respectively, and tuple (a, b, τ ) there essentially corresponds to a policy in our setting. 6. Our formulation of the OPE task using the potential outcome framework is different from the existing formulation, e.g., <ref type="bibr" target="#b43">Kallus and Zhou (2018)</ref>, where each action a ∈ A is defined as a treatment, and for Random variables. Consider a hypothetical subject in population. This subject is associated with covariates X := (U, A), where U ∈ U is context features and A ∈ A is an action taken. As such, we define the covariate space as X := U × A, the product of the context feature space U and action space A. In a recommendation system, for instance, U is the user features and A is a recommended list of items. We define two treatments 0 and 1 as exposing the subject to the environment during the data collection phase and that during the evaluation phase, respectively; and the associated potential outcomes Y * 0 and Y * 1 as the rewards under the respective treatments 0 and 1. Let T ∈ {0, 1} be a treatment indicator.</p><p>In a recommendation system, for example, an environment may refer to the situation where a user is about to choose an item, such as the calendar year when this takes place. For instance, treatment 0 may refer to the environment in the year 2000, and treatment 1 the environment in year 2020. Consider a user with the features U and the recommended items A, and suppose that A consists of items which were popular in 2000 but are outdated in 2020. Then this user may have chosen an item from A if it were in 2000, but may not choose any item in 2020, i.e., we may have Y * 0 = Y * 1 . For ease of understanding, consider a finite population of N subjects with</p><formula xml:id="formula_97">(y * i0 , y * i1 , x i , t i ) N i=1 (<label>34</label></formula><formula xml:id="formula_98">)</formula><p>being i.i.d. realizations of the random variables (Y * 0 , Y * 1 , X, T ). That is, the i-th subject is associated with covariates x i := (u i , a i ) consisting of context features u i and action a i . The treatment assignment t i ∈ {0, 1} indicates which environment the i-th subject is exposed to. Thus, the potential outcomes y * i0 and y * i1 are the rewards from the i-th subject (associated with x i := (u i , a i )) that would have been observed if she was exposed to the environment during the data collection phase (treatment 0) and that during the evaluation phase (treatment 1), respectively.</p><p>Distributions of the potential outcomes. The distributions of the potential outcomes Y * 0 , Y * 1 are defined via their conditional distributions given X = (U, A), i.e., P Y * 0 |X (y|x) and P Y * 1 |X (y|x) where y = r and x = (u, a). These are the conditional distributions of rewards r ∈ R given context features u ∈ U and action a ∈ A, under the environment during the data collection phase (treatment 0) and that during the evaluation phase (treatment 1), respectively:</p><formula xml:id="formula_99">P Y * 0 |X (y|x) = P 0 (r|u, a), P Y * 1 |X (y|x) = P * (r|u, a) (y = r, x = (u, a)</formula><p>). Note that our assumption P 0 (r|u, a) = P * (r|u, a) implies that</p><formula xml:id="formula_100">P Y * 0 |X (y|x) = P Y * 1 |X (y|x),<label>(35)</label></formula><p>each action a there is a corresponding potential outcome Y * a . In our formulation, on the other hand, an action a taken for the subject is defined as a part of covariates x = (u, a), and binary treatments, 0 and 1, are considered, each of which is defined as exposing the subject to a certain environment. Our formulation enables us to interpret the OPE task as counterfactual inference of changing the covariate distribution, so that our CME estimator can be naturally applied. Thus, our motivation of introducing this formulation is rather pragmatic, and we do not argue whether it is more reasonable than the existing one. One benefit may exist, however: In our formulation, we explicitly model the assumption on the conditional distributions of rewards being the same for the data collection and evaluation phases via the potential outcome notation (35), while this assumption is implicitly made in the existing formulation. This explicit statement of the assumption helps a researcher to understand when the OPE may be justified.</p><p>i.e., the conditional distributions of the potential outcomes (rewards) Y * 0 and Y * 1 are the same, given the covariates X = x := (u, a).</p><p>For instance, consider a recommendation system with a finite population (34) with the i-th user equipped with covariates x i = (u i , a i ) consisting of features u i and recommended items a i . The identity (35) then implies that, for the i-th user, the distributions of the potential outcomes (rewards) y * i0 and y * i1 are the same. This means that this user should have the same stochastic behavior in choosing (or not choosing) an item from the recommended ones during the data collection (treatment 0) and evaluation (treatment 1) phases. In other words, the environmental factors that affect the user behavior should be the same for the data collection and evaluation phases. This excludes, for instance, the above example where the data collection phase is in year 2000 and the evaluation phase is in 2020, in which case user preferences are different.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Distributions of covariates.</head><p>As in Section 3.3.2, we interpret here a policy as specifying a distribution on the covariate space X = U ×R. More specifically, consider the conditioning T = 0 or T = 1, which imply that the hypothetical subject is exposed to the environment of the data collection phase (T = 0) or to that of the evaluation phase (T = 1). Then the corresponding distributions of the covariates X 0 = X|(T = 0) and X 1 = X|T = 1 are given by the logging policy π 0 (u|a) and the target policy π * (u|a), respectively:</p><formula xml:id="formula_101">P X 0 (x) = π 0 (a|u)q 0 (u), P X 1 (x) = π * (a|u)q * (u) (x = (u, a) ∈ U × A).<label>(36)</label></formula><p>To describe this, consider the finite population (34), and assume that for the i-th user the treatment indicator is t i = 0, which implies that her data are given in the data collection phase. In this case, her covariates x i = (u i , a i ) are generated according to the joint distribution π 0 (a|u)q 0 (u) involving the logging policy π 0 (a|u). On the other hand, if t i = 1, her covariates x i = (u i , a i ) are generated from the joint distribution π * (a|u)q * (u) given by the target policy π * (a|u). Note that in the OPE setting, if t i = 1 we have access to neither y * i0 nor y * 1i ; note also that in this case this "user" may be imaginary, with covariates x i = (u i , a i ) generated artificially.</p><p>Distributions of observed outcomes. The observed outcome (reward) from the hypothetical subject is defined as</p><formula xml:id="formula_102">Y = 1(T = 0)Y * 0 + 1(T = 1)Y * 1 . Let Y 0 = Y |(T = 0) = Y * 0 |(T = 0) and Y 1 = Y |(T = 1) = Y * 1 |(T = 1)</formula><p>be the observed outcome Y conditioned on T = 0 or T = 1, respectively. Then Y 0 conditioned on X 0 = X|(T = 0) and Y 1 conditioned on X 1 = X|(T = 1) can be written in terms of the potential outcomes Y * 0 and Y * 1 as </p><formula xml:id="formula_103">Y 0 |X 0 = Y * 0 |X, (T = 0), Y 1 |X 1 = Y * 1 |X, (T =</formula><formula xml:id="formula_104">P Y 0 |X 0 (y|x) = P Y * 0 |X (y|x) = P 0 (r|u, a), P Y 1 |X 1 (y|x) = P Y * 1 |X (y|x) = P * (r|u, a),<label>(</label></formula><p>37) where y = r and x = (u, a). Therefore, the assumption P 0 (r|u, a) = P * (r|u, a) (or ( <ref type="formula" target="#formula_100">35</ref>)) implies that</p><formula xml:id="formula_105">P Y 0 |X 0 (y|x) = P Y 1 |X 1 (y|x).</formula><p>Reward distribution as a counterfactual distribution. Finally, the reward distribution (33) under the target environment can be written as a counterfactual distribution using the identities (36) and (37) (assuming the conditional exogeneity) as</p><formula xml:id="formula_106">P * (r) = P Y 0 |X 0 (y|x) dP X 1 (x) = P Y 0|1 (y).</formula><p>with y = r and x = (u, a). Thus, we can use the CME estimator (18) to estimate the kernel mean of this reward distribution, which is described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Off-Policy Evaluation by the CME Estimator</head><p>Let be a kernel on the outcome (reward) space Y = R with F its RKHS. Then the mean embedding of the reward distribution under the target environment is defined as</p><formula xml:id="formula_107">µ P * := (•, r) dP * (r) ∈ F . (<label>38</label></formula><formula xml:id="formula_108">)</formula><p>Note that this becomes the expected reward (32) if we define as a linear kernel: (r, r ) := rr , which we use in our experiments in Section 7.2. In principle, however, the use of other nonlinear kernels (in particular characteristic kernels) makes the mean embedding more informative, and this may be beneficial in assessing the effectiveness of the target policy.</p><p>Kernel on covariates. To use the CME estimator, we also need to define a kernel k on the covariate space X = U × A. To this end, we first define kernels k U and k A on the context feature space U and the action space A, respectively. Then we can define k as the product kernel of k U and k</p><formula xml:id="formula_109">A : k((u, a), (u , a )) := k U (u, u )k A (a, a ) for (u, a), (u , a ) ∈ U × A.</formula><p>Joint sample. Recall that logged data {(u i , a i , r i )} n i=1 are i.i.d. with the joint distribution P 0 (u, a, r) = P 0 (r|u, a)π 0 (a|u)q 0 (u), which is identified as P X 0 Y 0 (x, y) = P Y 0 |X 0 (y|x)P X 0 (x) for x = (u, a) and y = r because of ( <ref type="formula" target="#formula_101">36</ref>) and (37). Thus, by defining x i := (u i , a i ) and y i := r i , we have an i.i.d. sample (x i , y i ) n i=1 from the joint distribution P X 0 Y 0 (x, y). Covariate sample. We also need to express P X 1 in terms of a sample in the form (x j ) m j=1 . As in (36), the covariate distribution P X 1 is the joint distribution given by the target policy π * (a|u) and the marginal distribution q * (u) of context features. Thus, if we can sample from both π * (a|u) and q * (u) (the former is typically possible because it is defined by the designer of the target policy, while the latter depends on the problem), then (x j ) m j=1 may be given by</p><formula xml:id="formula_110">x j := (u * j , a * j ), where u * j ∼ q * (u), a * j ∼ π * (a|u * j ), j = 1, . . . , m.<label>(39)</label></formula><p>In the particular case where q * (u) = q 0 (u), we can use the sample (u i ) n i=1 in the logged data {(u i , a i , r i )} n i=1 , which are from q 0 (u), as a sample from q * (u): u * j := u j for j = 1, . . . , m := n. Note that even if q * (u) = q 0 (u), we can use the CME estimator as long as we have a sample of context features (u * j ) m j=1 from the target environment, i.e., the covariate shift setting of <ref type="bibr" target="#b84">Uehara et al. (2020)</ref>.</p><p>Algorithm. The resulting algorithm is described in Algorithm 2, which only requires matrix operations and thus is simple to implement. We note that the expected reward (32) under the target environment can be estimated as μP * (r) = n i=1 β i r i ; this is obtained by setting as a linear kernel on R.</p><p>Algorithm 2 Off-Policy Evaluation using the CME estimator <ref type="bibr">(18)</ref> 1: Requirement: A kernel k U on the context space U, a kernel k A on the action space A, a kernel on the reward space R, and a regularization constant ε &gt; 0. 2: Input: Logged data (u i , a i , r i ) n i=1 , a target policy π * (u|a) and a sample of context features (u * j ) m j=1 . (If q * (u) = q 0 (u), set u * j := u j , j = 1, . . . , m := n.) 3: for j = 1 to n do 4:</p><formula xml:id="formula_111">a * j ∼ π * (a | u * j ) 5: end for 6: Compute K ∈ R n×n with K ij := k U (u i , u j )k A (a i , a j ), i, j = 1, . . . , n. 7: Compute K ∈ R n×m with K ij := k U (u i , u * j )k A (a i , a * j ), i = 1, . . . , n, j = 1, . . . , m. 8: Compute β := (β 1 , . . . , β n ) = (K+n I) -1 K1 m ∈ R n , where 1 m := 1 m (1, . . . , 1) ∈ R m .</formula><p>9: Output: An estimate μP * = n i=1 β i (•, r i ) of the mean embedding ( <ref type="formula" target="#formula_107">38</ref>) or an estimate R * := n i=1 β i r i of the expected reward (32).</p><p>Extensions. Note that the above method of approximating the covariate distribution P X 1 via the sampling procedure in (39) does not fully exploit the information of the target policy π * (a|u), since for each u * j we only sample one action a * j ∼ π * (a|u * j ). In Appendix A, we discuss extensions of Algorithm 2 to make use of more information from the target policy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Experiments</head><p>This section provides empirical results that demonstrate the advantages of the proposed framework. The codes to reproduce the experiments are available at <ref type="url" target="https://github.com/sorawitj/counterfactual-mean-embedding">https://github.com/ sorawitj/counterfactual-mean-embedding</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Simulations: Distributional Treatment Effects</head><p>We first conduct simulation experiments on distributional treatment effects in Section 7.1.1 and on distributional effects of covariate distributions in Section 7.1.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.1">Distributional Treatment Effects (DTE)</head><p>We first deal with the identification of DTE (Sections 3.1.1), defined as the difference between the distributions P Y * 0 and</p><formula xml:id="formula_112">P Y * 1 of two potential outcomes Y * 0 , Y * 1 ∈ R.</formula><p>As discussed in Section 5.2, this identification problem can be formulated as hypothesis testing of the null hypothesis</p><formula xml:id="formula_113">H 0 : P Y * 1 = P Y * 0 against the alternative H 1 : P Y * 1 = P Y * 0 .</formula><p>For this purpose, we assume that i.i.d. observations {(x i , t i , y i )} N i=1 of random variables (X, T, Y ) are available, where X ∈ R 5 is covariates, T ∈ {0, 1} is a treatment indicator, and</p><formula xml:id="formula_114">Y = 1(T = 0)Y * 0 + 1(T = 1)Y * 1 ∈ R is the observed outcome.</formula><p>The purpose here is to demonstrate the validity of our approach to identifying DTE, described in Sections 3.5.1 and Section 5.2. To this end, we compare it with a baseline approach that uses an estimate of ATE (9) as a test statistic. For simplicity, we call here our approach "DTE" and the baseline "ATE". We consider the following three scenarios: Scenario I. There exists no treatment effect so that the distributions of the potential outcomes Y * 0 , Y * 1 are the same:</p><formula xml:id="formula_115">P Y * 0 = P Y * 1 .</formula><p>Hence, we expect that both ATE and DTE do not detect any treatment effect.</p><p>Scenario II. There exists a treatment effect that only makes the means of P Y * 0 and P Y * 1 different: i.e., the mean-shift scenario. Hence, we expect that both ATE and DTE can detect the treatment effect.</p><p>Scenario III. There exists a treatment effect that does not change the means of P Y * 0 and P Y * 1 , but changes their higher order moments. Hence, we expect that ATE fails to detect any treatment effect, whereas DTE with non-linear kernels can detect the difference.</p><p>To realize these scenarios, we define the random variables X, T , Y * 0 and Y * 1 as</p><formula xml:id="formula_116">X ∼ N (0, σ x I 5 ), T ∼ Bernoulli 1 1 + exp (-α X -α 0 ) Y * 0 = β X + ε 0 , Y * 1 = β X + b + ε 1 ,</formula><p>where ε 0 , ε 1 ∼ N (0, σ 2 ε ) are independent noises. Throughout the experiment, we set β = [0.1, 0.2, 0.3, 0.4, 0.5] , α = [0.05, 0.04, 0.03, 0.02, 0.01] , α 0 = 0.05, and σ 2 ε = σ 2 x = 0.1. We set b = 0 for the Scenario I and b = 2 for the Scenario II. For Scenario III, we set b = 2z -1, where z ∈ {0, 1} is an independent Bernoulli random variable z ∼ Bernoulli(0.5) generated for every observation. By construction, the conditional exogeneity For DTE and ATE, we perform the following tests using data {(x i , t i , y i , e i )} n i=1 augmented with propensity scores e i := e(x i ) :</p><formula xml:id="formula_117">Y * 0 , Y * 1 ⊥ ⊥ T |X in Assumption 1 is satisfied. For each scenario, we generate data {(x i , t i , y i )} N i=1 as i.i.d. observations of X, T and Y = 1(T = 0)Y * 0 + 1(T = 1)Y * 1 , with N ∈ {50, 100}.</formula><formula xml:id="formula_118">= E[T | X = x i ].</formula><p>For DTE, we use the unbiased KTE estimate in (23) as a test statistic, with the Gaussian kernel (y, y ) = exp(xy 2 2 /2σ 2 ) whose bandwidth parameter σ is chosen using the median heuristic  <ref type="table">1</ref>: The frequencies of rejecting the null hypothesis H 0 : P Y * 1 = P Y * 0 when the null hypothesis is true (i.e., the probability of the Type-I error in Scenario I) and when the alternative hypothesis H 1 : P Y * 1 = P Y * 0 is true (i.e., the power of the test in Scenario II &amp; III), computed from 1000 repetitions. The significance level α is 0.01. <ref type="bibr" target="#b26">(Garreau et al., 2017)</ref>. For ATE, we also use ( <ref type="formula">23</ref>) as a test statistic, but with the linear kernel (y, y ) = y y , resulting in a test that distinguishes only the means of two distributions. We use the bootstrap procedure described in Section 5.2 to construct the distribution of the test statistic under the null H 0 : P Y * 0 = P Y * 1 , with B = 10, 000 bootstrap samples. The significance level α is set to 0.01 in all experiments.</p><p>Table <ref type="table">1</ref> reports the frequencies of rejecting the null hypothesis H 0 : P Y * 0 = P Y * 1 over 1000 repetitions, for each of the three scenarios. When the null hypothesis H 0 is true (Scenario I), these are the frequencies of Type-I errors, which are well calibrated approximately at the designed level α = 0.01 for both ATE and DTE. When the alternative hypothesis H 1 : P Y * 0 = P Y * 1 is true (Scenarios II and III), these represent test powers (i.e., one minus the probability of Type II error). In Scenario II, both ATE and DTE successfully reject the null hypothesis, capable of detecting the mean shift effect in the potential outcome distributions. In Scenario III, where the treatment effects do not appear in the mean but in the higher order moments, DTE has significantly higher power than ATE, demonstrating that DTE can identify higher order distributional effects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.2">Distributional Effects of Covariate Distributions</head><p>We next consider the identification of distributional effects of covariate distributions (see <ref type="bibr">Sections 3.1.3,</ref><ref type="bibr">3.3 and 3.5.3)</ref></p><formula xml:id="formula_119">. As before, let Y * 0 , Y * 1 ∈ R be potential outcomes, T ∈ {0, 1} be a treatment indicator, Y = 1(T = 0)Y * 0 +1(T = 1)Y * 1 be the observed outcome, and X ∈ R 5 be covariates. Let X 0 = X|(T = 0), X 1 = X|(T = 1) and Y 0 = Y |(T = 0) = Y * 0 |(T = 0).</formula><p>Here we are interested in the distributional effects defined as</p><formula xml:id="formula_120">P Y * 0 | T (• | 0) -P Y * 0 | T (• | 1) = P Y 0|0 -P Y 0|1 = P Y 0 |X 0 (•|x)P X 0 (x) -P Y 0 |X 0 (•|x)P X 1 (x)</formula><p>where the first identity holds under the conditional exogeneity. As discussed in Section 5.2, the identification of this distributional effect can be cast as testing the null hypothesis</p><formula xml:id="formula_121">H 0 : P Y * 0 |T (• | 0) = P Y * 0 |T (• | 1) against the alternative H 1 : P Y * 0 |T (• | 0) = P Y * 0 |T (• | 1).</formula><p>For this experiment, we define the joint distribution of T , X and Y * 0 by first specifying the distribution of T , and then specifying the conditional distributions of X, Y * 0 and Y * 1 given</p><p>T (note that Y * 1 is not relevant in this experiment). To this end, we define the distribution P T of T as P T (0) = P T (1) = 1/2. Then, we define the conditional distributions of X and Y * 0 given T as</p><formula xml:id="formula_122">Y * 0 | (T = 0) = β X 0 + ε 0 , X 0 = X|(T = 0) ∼ N (0, σ x I 5 ), Y * 0 | (T = 1) = β X 1 + ε 1 , X 1 = X|(T = 1) ∼ 3 j=1 γ j N (ν j , σ x I 5 ),</formula><p>where ε 0 , ε 1 ∼ N (0, σ 2 ε ) are independent, β = [0.1, 0.2, 0.3, 0.4, 0.5] , σ ε = σ x = 0.1, and γ 1 = γ 2 = γ 3 = 1/3. We set ν 1 = [-5, 2.5, 0, 0, 2.5], ν 2 = [2.5, 2.5, 0, 0, -5], and ν 3 = [2.5, -5, 0, 0, 2.5], so that X 0 and X 1 have the same zero mean. By construction, Y * 0 | T = 0 and Y * 0 | T = 1 have the same mean, which is zero, while their higher-order moments differ. In other words, the distributional effects of the covariate distributions appear only in the higher-order moments. We generate data</p><formula xml:id="formula_123">(x i , y i ) n i=1 as i.i.d. observations of (X 0 , Y 0 ) (recall that Y 0 = Y * 0 |(T = 0)</formula><p>) and (x j ) m j=1 as i.i.d. observations of X 1 , where n = m (which amounts to P T (0) = P T (1) = 1/2).</p><p>We estimate the embedding</p><formula xml:id="formula_124">µ Y 0 |T =1 = (•, y) dP Y 0 |T (y|1) of the counterfactual distri- bution P Y 0 |T (•|1) = P Y 0|1 with the CME estimator (18) based on (x i , y i ) n</formula><p>i=1 and (x j ) m j=1 . We set the kernel on the outcome space as the Gaussian kernel (y, y ) = exp(yy 2 2 /2σ 2 Y ) whose bandwidth parameter σ Y is chosen by the median heuristic using (y i ) n i=1 . We also set the kernel k on the covariate space as the Gaussian kernel k(x, x ) = exp(xx 2 2 /2σ 2 X ), whose parameter σ X as well as the regularization constant ε in the CME estimator are chosen by 5-fold cross validation from σ X ∈ {0.01, 0.1, 1, 10} and ε ∈ {0.01, 0.1, 1, 10}. This cross validation is done by regarding the joint sample (x i , y i ) n i=1 as training data for regression from x i to y i , and by performing kernel ridge regression with kernel k and regularization parameter ε, motivated by the interpretation of conditional mean embedding as kernel ridge regression <ref type="bibr" target="#b30">(Grünewälder et al., 2012)</ref>.</p><p>We apply Algorithm 1 to the resulting CME estimate μY 0|1 = n i=1 β i (•, y i ) to generate counterfactual samples (y j ) n j=1 . We now have (y i ) n i=1 as an i.i.d. sample from P Y * 0 |T (• | 0) and (y j ) n j=1 as an approximate sample of the counterfactual distribution P Y * 0 |T (• | 1). As discussed in Section 5.2, we can test the null hypothesis H 0 :</p><formula xml:id="formula_125">P Y * 0 |T (• | 0) = P Y * 0 |T (• | 1) against the alternative H 1 : P Y * 0 |T (• | 0) = P Y * 0 |T (• | 1</formula><p>) by performing a two sample test using the samples (y i ) n i=1 and (y j ) n j=1 . For this purpose, we perform the kernel two-sample test with the unbiased MMD statistic <ref type="bibr">(Gretton et al., 2012, Eq. 3)</ref>, with permutation-based bootstrapping using B = 10, 000 bootstrap samples and with significance level α = 0.01. For comparison, we also perform the same kernel two-sample test, but with linear kernel (y, y ) = y y , resulting in a test that only uses the means of (y i ) n i=1 and (y j ) n j=1 . Figure <ref type="figure" target="#fig_4">3</ref> describes the experimental results. Figure <ref type="figure" target="#fig_4">3</ref></p><formula xml:id="formula_126">(a) illustrates the observed out- comes (y i ) n i=1 of Y * 0 |(T = 0) (red), a counterfactual sample of Y * 0 |(T = 1) (blue)</formula><p>and the approximate counterfactual sample (y j ) n j=1 generated with Algorithm 1 applied to our CME estimate (green). Note that the sample of Y * 0 |(T = 1) is shown here for an illustration purpose; in practice we never have access to such a sample, but we can generate it here as we know the ground-truth model. For illustration, we also show the corresponding density curves obtained from the respective samples using kernel density estimation. The approxi- </p><formula xml:id="formula_127">(y i ) n i=1 of Y * 0 |(T = 0) (red), a counterfactual sample of Y * 0 |(T = 1) (blue)</formula><p>and the approximate counterfactual sample (y j ) n j=1 generated with Algorithm 1 applied to our CME estimate (green), obtained from data with size n = 500. For illustration, we also show density curves obtained from the corresponding samples of the same colors, estimated with kernel density estimation. (b) Powers of the two sample tests based on the generated counterfactual sample (y j ) n j=1 and observed outcomes (y i ) n i=1 , using the unbiased MMD statistic with the Gaussian or the linear kernel with significance level α = 0.01. The powers are obtained from 1,000 repetitions, for each of different sample sizes. mate counterfactual sample (y j ) n j=1 resembles that from the ground-truth model, supporting the validity of our CME estimator (18) and the sampling method (Algorithm 1).</p><p>Figure <ref type="figure" target="#fig_4">3</ref>(b) describes the test powers (i.e., the frequencies of rejecting the null hypothesis</p><formula xml:id="formula_128">H 0 : P Y * 0 |T (• | 0) = P Y * 0 |T (• | 1)</formula><p>) over 1,000 repetitions of the above testing procedure for each case of using the Gaussian or the linear kernel for computing the test statistic, for different sample sizes. The test with the linear kernel has very low power. This implies that the mean of the generated counterfactual sample (y j ) n j=1 is close to the mean of the observed sample (y i ) n i=1 from Y * 0 |(T = 0) since the kernel two-sample test with the linear kernel only uses the information of the sample means. On the other hand, the power of the test with the Gaussian kernel increases as the size n of observed data increases, suggesting that the higher-order moments of the generated counterfactual sample (y j ) n j=1 differ substantially from those of the observed sample (y i ) n i=1 . These observations suggest that the approximate counterfactual sample (y j ) n j=1 has properties consistent with the ground-truth counterfactual distribution Y * 0 |T = 1. Thus our CME estimator (18) and Algorithm 1 are capable of producing an approximate counterfactual sample based on which a test for distributional effects can be constructed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Off-Policy Evaluation</head><p>We conduct experiments on the off-policy evaluation (OPE) task for a recommendation system, described in Section 6. Let η : U × A → R be the regression function that takes a pair (u, a) of user features u ∈ U and recommendation a ∈ A as an input and outputs the conditional expectation of the reward r:</p><formula xml:id="formula_129">η(u, a) := E[r | u, a] := R r dP * (r | u, a) = R r dP 0 (r | u, a),</formula><p>where P * (r | u, a) = P 0 (r | u, a) is the conditional distribution of the reward r given the pair (u, a), which is assumed to be invariant under the target and logging environments.</p><p>For a given target policy π * (a|u), the OPE task is to estimate the expected reward under the target environment defined by</p><formula xml:id="formula_130">R * := U ×A R r dP * (r | u, a) dπ * (u, a) = U ×A η(u, a) dπ * (u, a), where π * (u, a) = π * (a | u)q * (u) = π * (a | u)q 0 (u)</formula><p>is the joint distribution of context features u ∈ U and action a ∈ A. Here we consider the standard setting where the marginal distributions of the user features u are the same under the target and logging environments: q * (u) = q 0 (u). The above estimation is to be done based on the logged data D init := {(u i , a i , r i )} n i=1 obtained from the joint distribution P 0 (u, a, r) = P 0 (r | u, a)π 0 (a, u) during the data collection phase, where π 0 (u, a) = π 0 (u|a)q 0 (u).</p><p>We compare our approach in Algorithm 2, which we call CME below, to the following benchmark estimators using both simulated and real-world data.</p><p>Direct method with a parametric regressor (DM). The direct method <ref type="bibr" target="#b18">(Dudík et al., 2011)</ref> first learns the regression function η based on the logged data D init with a regression model of one's choice. Let η : U × A → R be the learned regressor. Then the expected reward R * is estimated as</p><formula xml:id="formula_131">R DM = 1 n n i=1 E a∼π * (a | u i ) [η(u i , a)].</formula><p>The direct method obtains the approximation η based on the logged data</p><formula xml:id="formula_132">D init = {(u i , a i , r i )} n i=1</formula><p>, in which input pairs (u i , a i ) are generated from the covariate distribution π 0 (u, a) = π 0 (u|a)q 0 (u) that is different from the target covariate distribution π * (u, a) = π * (u|a)q 0 (u). Recall that we interpret the paired variables (u, a) as "covariates" in our discussion. This situation is known as covariate shift in the literature. It is well known that under the covariate shift, a parametric regression model may produce a significant bias <ref type="bibr" target="#b67">(Shimodaira, 2000)</ref>. That is, the approximation quality of the learned model η obtained with the logged data D init may be good with respect to the covariate distribution π 0 (u, a) under the data collection environment, but can be poor with respect to the target covariate distribution π * (u, a), e.g., ηη 2 L 2 (π * ) := (η(u, a)η(u, a)) 2 dπ * (u, a) may be large. This in turn may induce a large bias in the estimation of the expected reward R * = η(u, a) dπ * (u, a). To demonstrate this, we use a 3-layer feedforward neural network, which is an (overparametrized) parametric model, as a regressor for the direct method.</p><p>Weighted inverse propensity score (wIPS). The wIPS estimator obtains an unbiased estimate of the target reward by re-weighting each observation in the logged dataset by the ratio of the propensity scores under the target and initial policies <ref type="bibr" target="#b36">(Horvitz and Thompson, 1952;</ref><ref type="bibr" target="#b55">Precup et al., 2000)</ref>. The wIPS estimator is defined by</p><formula xml:id="formula_133">R wIPS = n i=1 w i r i n i=1 w i ,</formula><p>where w i := π * (a i , u i )/π 0 (a i , u i ) = π * (a i |u i )/π 0 (a i |u i ) are the propensity weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Doubly robust (DR).</head><p>The DR estimator combines the two aforementioned estimators by exploiting both the regression model η(u, a) and the propensity scores <ref type="bibr" target="#b10">(Cassel et al., 1976;</ref><ref type="bibr" target="#b18">Dudík et al., 2011)</ref>. The estimator is given by</p><formula xml:id="formula_134">R DR = 1 n n i=1 E a∼π * (a | u i ) [η(u i , a)] + w i (r i -η(u i , a i )) .</formula><p>It has been proved to be unbiased if at least one of the estimators, η and π * /π 0 is correctly specified; see, e.g., <ref type="bibr" target="#b10">Cassel et al. (1976)</ref>; <ref type="bibr" target="#b18">Dudík et al. (2011)</ref>.</p><p>Slate estimator. The Slate estimator, proposed for recommendation systems, makes use of the structure within a recommendation (= action) by assuming a certain linearity assumption on the regression function with respect to the recommendation <ref type="bibr" target="#b81">(Swaminathan et al., 2017)</ref>. More precisely, Swaminathan et al. ( <ref type="formula">2017</ref>) consider a recommendation system in which an action a ∈ A is an ordered list (called slate) of K ∈ N items chosen from M ∈ N possible items. Let 1 a ∈ R KM be the indicator vector whose (k, m)-th element is 1 if a contains the item m ∈ {1, . . . , M } in the slot k ∈ {1, . . . , K}, and 0 otherwise. <ref type="bibr" target="#b81">Swaminathan et al. (2017,</ref> Assumption 1) then model the regression function η(u, a) as a linear function of this indicator vector: η(u, a) = w u 1 a , where w u is an unknown feature vector of the context u (Note that w u can be a nonlinear function of u ∈ U). Under this assumption, the authors derive the slate estimator as</p><formula xml:id="formula_135">R slate = 1 n n i=1 r i • q u i Γ † u i 1 a i ,</formula><p>where Γ † u i is the Moore-Penrose pseudoinverse of the matrix</p><formula xml:id="formula_136">Γ u i := E a∼π * (a | u i ) [1 a 1 a ] ∈ R KM ×KM , and q u i := E a∼π * (a | u i ) [1 a ] ∈ R KM .</formula><p>Thanks to the linearity assumption, the slate estimator may enjoy a lower variance than the wIPS estimator, while the assumption may also lead to a non-vanishing bias if it does not hold.</p><p>For the CME, we use a kernel defined as k((u, a), (u , a</p><formula xml:id="formula_137">)) := k U (u, u )k A (a, a ) where k U (u, u ) := exp -u -u 2 2 /2σ 2 u and k A (a, a ) := exp -a -a 2 2 /2σ 2 a .</formula><p>For this experiment, the linear kernel (r, r ) := rr is used as a reward kernel since we only compare the estimation of the expected reward. The regularization parameter ε is selected by the cross validation procedure in Appendix B, while we determined σ u and σ a by the median heuristic, i.e., σ 2 u = median{ u iu j 2 2 } 1≤i&lt;j≤n and σ 2 a = median{ a ia j 2 2 } 1≤i&lt;j≤n . Before proceeding, we point out here a connection between our approach and the direct method. Assume that we use kernel ridge regression to obtain the approximation η of the regression function η using the logged data: η(u, a) = r (K + n I) -1 k(u, a), where r := (r 1 , . . . , r n ) ∈ R n and k(u, a) := (k((u j , a j ), (u, a))) n j=1 ∈ R n Then, the estimate of the direct method can be related to the CME estimate as</p><formula xml:id="formula_138">R DM = 1 n n i=1 E a∼π * (a | u i ) [η(u i , a)] ≈ 1 n n i=1 η(u i , a * i ) = 1 n n i=1 r (K + n I) -1 k(u i , a * i ) = r (K + n I) -1 1 n n i=1 k(u i , a * i ),<label>(40)</label></formula><p>where the approximation in the first line is a Monte Carlo approximation based on a single draw a * i from the target policy π * (a | u i ) for each i. As we can see from ( <ref type="formula" target="#formula_138">40</ref>), the estimate has the same form as the CME estimate given in Algorithm 2 when the output kernel is a linear kernel, i.e., when we are only interested in the expected reward.</p><p>In this sense, the CME estimate can be interpreted as the direct method with kernel ridge regressor. Note that the kernel ridge regression is a nonparametric method (as long as the RKHS of the kernel on covariates is infinite dimensional such as the RKHS of the Gaussian kernel), and thus less prone to the effects of covariate shift. In fact, our convergence results in Section 4 show that the CME is consistent and thus asymptotically unbiased. This explains why our method, even if it can be related to the direct method, works well in the off-policy evaluation task compared to the direct method using a parametric model (as we will show shortly).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.1">Simulated Data</head><p>We consider the following setting for our simulation experiment. When a user visits a website, the system provides a recommendation as an ordered list of K ∈ N items out of M ∈ N available items to that user. Each item m ∈ {1, . . . , M } is represented by a feature vector v m ∈ R d generated randomly as v m ∼ N (0, I d ), where d ∈ N. Hence, a recommendation is an ordered list a = (v m 1 , v m 2 , ..., v m K ) ∈ R d×K , where m 1 , m 2 , . . . , m K ⊂ {1, . . . , M }. Likewise, each user j ∈ {1, . . . , N } has a feature vector u j ∈ R d generated as u j ∼ N (0, I d ), where N ∈ N is the number of users. The reward from a user is 1 if the user clicks any of the recommended items and 0 otherwise. Specifically, for each (a i , u j ) pair, let θ ij = P(click | a i , u j ) = 1/(1 + exp(-a i u j + ij )) be the probability of a click, where a i is the mean vector of the item vectors listed in a i , and ij ∼ N (0, 1) is an independent noise. The reward from user j receiving recommendation a i is defined as r ij ∼ Bernoulli(θ ij ).</p><p>In this experiment, we consider the following policy setup. For each user j, a policy π(a|u) generates a list a = (v m 1 , v m 2 , ..., v m K ) of K recommended items by sampling without replacement with respect to a multinomial distribution over all items. The probability of item l ∈ {1, . . . , M } being selected for user j is exp(b j v l )/ M k=1 exp(b j v k ), where {b j } N j=1 are parameter vectors of the policy π(a|u). Note that we obtain an optimal policy if b j = u j for all j ∈ {1, . . . , N }. To construct initial policy π 0 (a|u) and target policy π * (a|u), we first randomly generate user feature vectors u 1 , . . . , u N . Then, for the target policy π * , we set b * j = p j u j for j = 1, . . . , N where p j := (p jk ) d k=1 with p jk ∼ Bernoulli(0.5). That is, the parameter vector b * j is equal to the user feature vector with about half of its entries randomly set to zero. For the initial policy π 0 , we set b j = αb * j where α ∈ [-1, 1]. The parameter α controls how similar the policies are. If α = 1, we obtain π 0 = π * , whereas π 0 and π * differ the most when α = -1.</p><p>We generate two datasets D init = {(u i , a i , r i )} n i=1 and D target = {(u * i , a * i , r * i )} n i=1 using π 0 (a|u) and π * (a|u), respectively, where u i = u * i for i = 1, . . . , n. Note that the target rewards r * 1 , r * 2 , . . . , r * n are only used for evaluation. Our task is to estimate the expected reward of the target policy from the remaining information. We perform 5-fold CV over parameter grids, i.e., the number of hidden units n h ∈ {50, 100, 150, 200} for the Direct and DR estimators, and the regularization parameter ε ∈ {10 -8 , . . . , 10 0 } for our CME. We repeat the experiments 30 times independently to obtain the mean square errors (MSE) and their 95% confidence intervals in the estimation of the expected reward for each estimator.</p><p>We investigate the behavior of different estimators as we vary different experimental conditions including the degree of difference between initial and target policies (α), the context dimensionality (d), the number of items (M ), the number of users (N ), the number of recommended items (K), and the number of observations (n). Figure <ref type="figure" target="#fig_5">4</ref> depicts the experimental results (note that vertical axis is in log scale). In brief, we find that a) the performance of all estimators degrade as the difference between π 0 and π * increases (i.e., as α tends to -1), but the CME is least susceptible to this difference, b) the Slate estimator does not perform well in this setting because its linearity assumption does not hold, c) all estimators deteriorate as the context dimension increases, but the effect appears to be more pronounced for the Direct, DR, and CME estimators than for the wIPS and Slate estimators as they do not rely directly on the covariates, d) the opposite effect is observed if we increase the number of available items M , as illustrated in Figure <ref type="figure" target="#fig_5">4</ref>(c), and e) the CME estimator achieves better performance than other estimators in most experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.2">Real Data</head><p>For our real data experiment, we use the data from the Microsoft Learning to Rank Challenge dataset (MSLR-WEB30K) <ref type="bibr" target="#b56">(Qin and Liu, 2013)</ref> and treat them as an off-policy evaluation problem. We follow the same experiment setting as described in <ref type="bibr">Swaminathan et al. (2017, Section 4.1)</ref>. The data contains a set of queries and the corresponding URLs. Each pair of query q and URL u is represented by a feature vector f q,u and accompanied by a relevance judgment ρ(q, u) ∈ {0, ..., 4}. We consider the expected reciprocal rank (ERR) <ref type="bibr" target="#b11">(Chapelle et al., 2009)</ref> as our reward function, which is defined as ERR(q, u)</p><formula xml:id="formula_139">:= K k=1 1 k k-1</formula><p>j=1 (1 -R(q, u j ))R(q, u k ), where R(q, u) := 2 ρ(q,u) -1 2 maxrel with maxrel := 4. In order to obtain distinct initial and target policies π 0 (a|u) and π * (a|u), the feature vector f q,u is split into URL features f url q,u and body features f body q,u , which are used to train two regression models to predict the relevance score ρ(q, u): a Lasso regression model is trained to predict ρ(q, u) from f url q,u (denoted by lasso url ), and a regression tree model is trained to predict ρ(q, u) from f body q,u (denoted by tree body ). These two regression models are then used in the initial and target policies as will be described below.</p><p>In order to generate logged data D init , we first sample a query q uniformly from the dataset, and select top M candidate URLs based on the relevance scores predicted by the tree body model. We then generate K recommended URLs out of these top M candidates using an initial policy π 0 and compute its corresponding reward. The initial policy is a stochastic policy which recommends K URLs by sampling without replacement according to a multinomial distribution parameterized by p α (u|q) ∝ 2 -α[log 2 rank(u,q)] , where rank(u, q) is the ERR of the relevance score ρ(q, u) predicted by the tree body model and α ≥ 0 is an exploration rate parameter. For target data D target , we consider both stochastic and deterministic target policies. The stochastic target policy π * (a|u) is similar to the initial policy described earlier except that it employs lasso url model for the predicted relevance scores, but this makes the two policies distinct; their top-10 rankings are only overlapping by 2.5 URLs, on average. On the other hand, the deterministic target policy directly selects top-K URLs ranked by the predicted relevance scores obtained from the lasso url model. In this experiment, we set the exploration rate parameter α = 1 for the stochastic initial policy, and set α = 2 for the stochastic target policy.</p><p>We compare our estimator (CME) with the benchmark estimators Direct, wIPS, DR and Slate. In addition, we include the OnPolicy method as a baseline, which estimates rewards directly from the target policies (and thus, this baseline should always perform the best). To accelerate the computation of the CME, we make use of the Nyström approximation method <ref type="bibr" target="#b86">(Williams and Seeger, 2001)</ref>. We repeat the experiments 10 times independently to obtain the mean square errors (MSEs) and their 95% confidence intervals in the estimation of the expected reward for each estimator.</p><p>Figure <ref type="figure" target="#fig_6">5</ref> depicts the results. In short, our CME dominates other estimators in most of experimental conditions (note that the vertical axis is in log scale, so the margins are significantly large). The wIPS clearly suffers from high variance, especially in the deterministic target policy. The reason is that, in the deterministic setting, the propensity score adjustment requires that the treatments picked by logged and target policies match exactly. Otherwise, the propensity ratio π * (a|u)/π 0 (a|u) will be zero. The exact match is likely not to happen in practice and leads to higher variance in estimation of rewards. The Slate, Direct and CME are relatively robust across different conditions. The Direct method and CME perform particularly well when sample size is small, regardless of the action space, while the Slate estimator is less sample-efficient, especially in the large action space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Discussion</head><p>This paper presents a general-purpose kernel mean representation of counterfactual distributions called the counterfactual mean embedding (CME). It draws insights and tools from kernel methods in machine learning and the potential outcome framework in causal inference. We show that our estimator of counterfactual distributions exhibits appealing theoretical properties, and also serves as a practical tool for causal inference. Ultimately, we hope that our work will be useful not only for researchers in disciplines that rely on the potential outcome framework, such as social and biomedical sciences, but also for researchers in machine learning and statistics to develop novel methodology for counterfactual inference, since several important open questions still remain: e.g., the use of high-order moments of counterfactual distributions, and how to handle a hidden confounder and an instrumental variable.</p><p>One promising application of our framework is in generating a sample from the counterfactual distribution. For instance, neuroscientists can visualize the fMRI images of subjects under alternative setups without explicitly conducting invasive experiments. In this case, the outcome variable corresponds to an fMRI image. Let G θ be a generative model over the outcome parametrized by a parameter vector θ. The choice of G θ can range from a mixture of Gaussians to deep generative models, e.g., generative adversarial networks (GAN). An estimate of the counterfactual distribution, denoted by G * θ , can be obtained via an optimization problem: G * θ = arg min θ μY 0|1 -μG θ 2 F , where μY 0|1 is our CME estimate and μG θ denotes the mean embedding of G θ in the RKHS F . Counterfactual sample generation is ubiquitous for qualitative analysis in many application domains. We leave it as an open problem to future research. Thus, a new approximation of µ X 1 may be defined as</p><formula xml:id="formula_140">μX 1 (x) := μπ * (u, a) := 1 m m j=1 1 M M ν=1 k A (a, a * jν ) k U (u, u * j ),</formula><p>where x := (u, a). Note that M = 1 recovers Algorithm 2. Another approach is to exactly compute the integral A k A (a, a ) dπ * (a |u * j ) when it is possible, and define a new approximation of µ X 1 as</p><formula xml:id="formula_141">μX 1 (x) := μπ * (u, a) := 1 m m j=1 A k A (a, a ) dπ * (a |u * j ) k U (u, u * j )</formula><p>The algorithm above follows the standard cross validation procedure, except the bias correction step on validation sets. In the bias correction step, we re-weight the sample in the validation set so that the performance estimate computed from this set is unbiased. Nevertheless, the estimate may have high variance, e.g., when the propensity weights are used. This pitfall is alleviated by the variance reduction step.  </p><formula xml:id="formula_142">:= Y | (T = 0) = 1 t=0 1(T = t)Y * t | (T = 0) = Y * 0 | (T = 0). From this and Definition 2, it follows that P Y 0|0 (y) = P Y 0 |X 0 (y|x) dP X 0 (x) = P Y |T,X (y|0, x) dP X|T (x|0) = P Y * 0 |T,X (y|0, x) dP X|T (x|0) = P Y * 0 |T (y|0).</formula><formula xml:id="formula_143">that Y = 1(T = 0)Y * 0 + 1(T = 1)Y * 1 , we have Y | (T = 0) = Y * 0 | (T = 0); (c) By Definition 2, we have P Y 0 |X 0 (y|x) = P (Y |T =0)|(X|T =0) (y|x) = P Y |T,X<label>(</label></formula><p>y|0, x). Using these, we have</p><formula xml:id="formula_144">P Y * 0 |T (y|1) = P Y * 0 |T,X (y|1, x) dP X|T (x|1) (a) = P Y * 0 |T,X (y|0, x) dP X|T (x|1) (b) = P Y |T,X (y|0, x) dP X|T (x|1) (c) = P Y 0 |X 0 (y|x) dP X 1 (x) = P Y 0|1 (y),</formula><p>as required.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Proof of Theorem 5</head><p>Proof We prove here</p><formula xml:id="formula_145">E[μ Y * 1 ] = µ Y * 1 ; the proof of E[μ Y * 0 ] = µ Y * 0 is similar and thus omitted. First note that, since (x i , t i , y i ) N i=1 are i.i.d. with (X, T, Y ) and m := N i=1 t i , we have E[μ Y * 1 ] = E 1 m N i=1 t i (•, y i ) e(x i ) = E T (•, Y ) e(X)</formula><p>.</p><p>By the definition of Y , the conditional exogeneity and the definition of the propensity e(x), we then have</p><formula xml:id="formula_146">E T (•, Y ) e(X) = E T (•, Y * 1 ) e(X) = E X E T (•, Y * 1 ) e(X) X = E X E[T |X]E[ (•, Y * 1 )|X] e(X) = E X e(X)E[ (•, Y * 1 )|X] e(X) = E X [E[ (•, Y * 1 )|X]] = E[ (•, Y * 1 )] = µ Y * 1</formula><p>, where E X denotes the expectation with respect to X.</p><p>C.4 Proof of Theorem 6 . Proof We derive the convergence rate of μY * 1 ; the rate of μY * 0 can be derived in a similar way, and thus is omitted. Let Ỹ * 1 be an independent copy of Y * 1 .</p><formula xml:id="formula_147">E μY * 1 -µ Y * 1 2 F = E   1 m N i=1 t i (•, y i ) e(x i ) -µ Y * 1 2 F   = E   1 m 2 i,j t i t j (y i , y j ) e(x i )e(x j )   (A) -2 E 1 m i t i E Y * 1 [ (y i , Y * 1 )] e(x i ) (B) +E[ (Y * 1 , Ỹ * 1 )].<label>(43)</label></formula><p>We first deal with the term (A). It can be expanded as</p><formula xml:id="formula_148">(A) = 1 m 2 E   i =j t i t j (y i , y j ) e(x i )e(x j )   + 1 m 2 E i t 2 i (y i , y i ) e 2 (x i ) .</formula><p>Let ( X, T , Ỹ * 0 , Ỹ * 1 ) be an independent copy of (X, T, Y * 0 , Y * 1 ), and write Ỹ = 1( T = 0) Ỹ * 0 + 1( T = 1) Ỹ * 1 . Since in the first term of (A), (x i , t i , y i ) and (x j , t j , y j ) are independent but distributed as (X, T, Y ), the first term can be written as</p><formula xml:id="formula_149">1 m 2 E   i =j T T (Y, Ỹ ) e(X)e( X)   = m -1 m E T T (Y, Ỹ ) e(X)e( X)<label>.</label></formula><p>For the right hand side, we have</p><formula xml:id="formula_150">E T T (Y, Ỹ ) e(X)e( X) (a) = E T T (Y * 1 , Ỹ * 1 ) e(X)e( X) = E X, T , Ỹ * 1 T e( X) E X,T,Y * 1 T (Y * 1 , Ỹ * 1 ) e(X) | Ỹ * 1 = E X, T , Ỹ * 1 T e( X) E X E T,Y * 1 [T (Y * 1 , Ỹ * 1 ) | X] e(X) | Ỹ * 1 (b) = E X, T , Ỹ * 1 T e( X) E X E T [T | X]E Y * 1 [ (Y * 1 , Ỹ * 1 ) | X] e(X) | Ỹ * 1 (c) = E X, T , Ỹ * 1 T e( X) E X E Y * 1 [ (Y * 1 , Ỹ * 1 ) | X] | Ỹ * 1 ] = E X, T , Ỹ * 1 T e( X) E Y * 1 [ (Y * 1 , Ỹ * 1 )] | Ỹ * 1 ] = E X 1 e( X) E T , Ỹ * 1 T E Y * 1 [ (Y * 1 , Ỹ * 1 )] | Ỹ * 1 ] | X (d) = E X 1 e( X) E T [ T | X]E Y * 1 , Ỹ * 1 [ (Y * 1 , Ỹ * 1 ) | X] (e) = E X E Y * 1 , Ỹ * 1 [ (Y * 1 , Ỹ * 1 ) | X] = E Y * 1 , Ỹ * 1 [ (Y * 1 , Ỹ * 1 )].</formula><p>where On the other hand, the second term of (A) can be written as</p><formula xml:id="formula_152">1 m 2 E i t 2 i (y i y i ) e 2 (x i ) = 1 m E T 2 (Y, Y ) e 2 (X) .</formula><p>For the last expression, we have</p><formula xml:id="formula_153">E T 2 (Y, Y ) e 2 (X) (a) = E T (Y, Y ) e 2 (X) (b) = E T (Y * 1 , Y * 1 ) e 2 (X) = E X E T,Y * 1 [T (Y * 1 , Y * 1 ) | X] e 2 (X) (c) = E X E T [T |X]E Y * 1 [ (Y * 1 , Y * 1 ) | X] e 2 (X) (d) = E X E Y * 1 [ (Y * 1 , Y * 1 ) | X] e(X) ≤ sup y∈Y (y, y) inf x∈X e(x) =: C ,e<label>(e)</label></formula><p>&lt; ∞,</p><formula xml:id="formula_154">where (a) follows from T taking values in {0, 1}, (b) from Y being Y * 1 if T = 1, (c) from the conditional exogeneity, (d) from E[T |X] = e(X)</formula><p>, and (e) from our assumptions that sup y∈Y (y, y) &lt; ∞ and inf x∈X e(x) &gt; 0. Thus, the term (A) is upper-bounded as</p><formula xml:id="formula_155">(A) ≤ m -1 m E Y * 1 , Ỹ * 1 [ (Y * 1 , Ỹ * 1 )] + 1 m C ,e .</formula><p>Next we deal with the term (B), which can be written as</p><formula xml:id="formula_156">E 1 m i t i E Y * 1 [ (y i , Y * 1 )] e(x i ) = E T E Y * 1 [ ( Ỹ , Y * 1 )] e( X)</formula><p>,</p><p>where, as before, ( X, T , Ỹ * 0 , Ỹ * 1 ) is an independent copy of (X, T, Y * 0 , Y * 1 ) and Ỹ := 1( T = 0) Ỹ * 0 + 1( T = 1) Ỹ * 1 . The right expression can be expanded as</p><formula xml:id="formula_157">E T E Y * 1 [ ( Ỹ , Y * 1 )] e( X) (a) = E T E Y * 1 [ ( Ỹ * 1 , Y * 1 )] e( X) = E X   E T , Ỹ * 1 T E Y * 1 [ ( Ỹ * 1 , Y * 1 )] | X e( X)   (b) = E X E T [ T | X]E Y * 1 , Ỹ * 1 [ ( Ỹ * 1 , Y * 1 )] | X] e( X) (c) = E X E Y * 1 , Ỹ * 1 [ ( Ỹ * 1 , Y * 1 )] | X] = E Y * 1 , Ỹ * 1 [ ( Ỹ * 1 , Y * 1 )],</formula><p>where Using the obtained results for (A) and (B) in ( <ref type="formula" target="#formula_147">43</ref>), we now have</p><formula xml:id="formula_159">E μY * 1 -µ Y * 1 2 F ≤ m -1 m E[ (Y * 1 , Ỹ * 1 )] + 1 m C ,e -2E[ (Y * 1 , Ỹ * 1 )] + E[ (Y * 1 , Ỹ * 1 )]</formula><p>Lemma 16 (Spectral decomposition of integral operators) Let X , k and P X 0 be such that Assumption 2 (i) is satisfied. Then there exist at most countable families (e i ) i∈I ⊂ H and</p><formula xml:id="formula_160">(µ i ) i∈I ⊂ (0, ∞) such that µ 1 ≥ µ 2 ≥ • • • &gt; 0, (µ 1/2 i e i ) i∈I is an ONS in H , ([e i ] ∼ ) i∈I is an ONS in L 2 (P X 0 ), and T f = i∈I µ i [e i ] ∼ , f L 2 (P X 0 ) [e i ] ∼ , f ∈ L 2 (P X 0 ),<label>(47)</label></formula><formula xml:id="formula_161">Sf = i∈I µ i [e i ] ∼ , f L 2 (P X 0 ) e i , f ∈ L 2 (P X 0 ),<label>(48)</label></formula><formula xml:id="formula_162">C XX g = i∈I µ i µ 1/2 i e i , g H µ 1/2 i e i , g ∈ H , (<label>49</label></formula><formula xml:id="formula_163">)</formula><p>where the convergence is in L 2 (P X 0 ) for (47), and in H for (48) and (49).</p><p>Proof Since k and P X 0 satisfy Assumption 2 (i), it follows from <ref type="bibr">Steinwart and Scovel (2012, Lemma 2.3</ref>) that H is compactly embedded into L 2 (P X 0 ). As a result, Steinwart and Scovel (2012, Lemma 2.12) implies that there exist at most countable families (e) i∈I ⊂ H and</p><formula xml:id="formula_164">(µ i ) i∈I ⊂ (0, ∞) such that µ 1 ≥ µ 2 ≥ • • • &gt; 0, ([e i ] ∼ ) i∈I is an ONS in L 2 (P X 0 ), (µ 1/2 i e i )</formula><p>i∈I is an ONS in H , and (47) holds with convergence in L 2 (P X 0 ).</p><p>To show (48), since ([e i ] ∼ ) i∈I is an ONS in L 2 (P X 0 ), any f ∈ L 2 (P X 0 ) can be written as</p><formula xml:id="formula_165">f = i∈I [e i ] ∼ , f L 2 (P X 0 ) [e i ] ∼ + f ⊥ ,</formula><p>with convergence in L 2 (P X 0 ), where f ⊥ ∈ L 2 (P X 0 ) is such that [e i ] ∼ , f ⊥ L 2 (P X 0 ) = 0 for all i ∈ I. By Steinwart and Scovel (2012, Lemma 2.12, Eq.15) we have µ i e i = S[e i ] ∼ for all i ∈ I. It then holds that</p><formula xml:id="formula_166">Sf = i∈I µ i [e i ] ∼ , f L 2 (P X 0 ) e i + Sf ⊥ ,</formula><p>where the convergence is in H since S is continuous. Note that we have T f ⊥ = 0, since we have (47) and [e i ] ∼ , f ⊥ L 2 (P X 0 ) = 0 for all i ∈ I. That is, f ⊥ is in the null space of T . Since the null spaces of S and T are equal <ref type="bibr">(Steinwart and Scovel, 2012, Lemma 2.12, Eq.16)</ref>, it follows that Sf ⊥ = 0, which implies (48).</p><p>Finally we show (49). First note that C XX e i = SS * e i = S[e i ] ∼ = µe i for all i ∈ I. Using this and (48), for any g ∈ H we have</p><formula xml:id="formula_167">C XX g = SS * g = i∈I µ i [e i ] ∼ , S * g L 2 (P X 0 ) e i = i∈I µ i SS * e i , g L 2 (P X 0 ) e i = i∈I µ i C XX e i , g H e i = i∈I µ i µ i e i , g H e i ,</formula><p>where the convergence is in H , which implies (49).</p><p>Definition 17 Let X , k and P X 0 be such that Assumption 2 (i) is satisfied. Let (e i ) i∈I ⊂ H and (µ i ) i∈I ⊂ (0, ∞) be as in Lemma 16. Then for a constant β &gt; 0, the β-th power of T , S and C XX are respectively defined by</p><formula xml:id="formula_168">T β f := i∈I µ β i [e i ] ∼ , f L 2 (P X 0 ) [e i ] ∼ , f ∈ L 2 (P X 0 ), S β f := i∈I µ β i [e i ] ∼ , f L 2 (P X 0 ) e i , f ∈ L 2 (P X 0 ), C β XX f := i∈I µ β i µ 1/2 i e i , f H µ 1/2 i e i , f ∈ H .</formula><p>Lemma 18 Let X , k and P X 0 be such that Assumption 2 (i) and (ii) hold. Let (e i ) i∈I ⊂ H and (µ i ) i∈I ⊂ (0, ∞) be as in Lemma 16. Then, ([e] ∼ ) i∈I is an ONB of L 2 (P X 0 ).</p><p>Proof Since k and P X 0 satisfy Assumption 2 (i), it follows from <ref type="bibr">Steinwart and Scovel (2012, Lemma 2.3</ref>) that H is compactly embedded into L 2 (P X 0 ). Then one can use <ref type="bibr">Steinwart and Scovel (2012, Theorem 3.1)</ref>, which states that the assertion is equivalent to the Assumption 2 (ii) that the embedding S * : H → L 2 (P X 0 ) has a dense image in L 2 (P X 0 ).</p><p>We provide a condition for the integral operator T prod defined in Section 4.2 to admit an eigen-decomposition, which is needed for its power T β prod in (29) to be well-defined. Lemma 19 Let X , k and P X 0 be such that Assumption 2 (i) and (ii) are satisfied. Let T prod : L 2 (P X 0 ⊗ P X 0 ) → L 2 (P X 0 ⊗ P X 0 ) be the integral operator defined as in Section 4.2, and let (e i ) i∈I ⊂ H and (µ i ) i∈I ⊂ (0, ∞) be as in Lemma 16. Then we have</p><formula xml:id="formula_169">T prod η = i,j∈I µ i µ j η, [e i ] ∼ ⊗ [e j ] ∼ L 2 (P X 0 ⊗P X 0 ) [e i ] ∼ ⊗ [e j ] ∼ , η ∈ L 2 (P X 0 ⊗ P X 0 ),</formula><p>where the convergence is in L 2 (P X 0 ⊗ P X 0 ).</p><p>Proof By Assumption 2 (ii) that S * has a dense image in L 2 (P X 0 ) and Lemma 18, ([e i ] ∼ ) i∈I is an ONB in L 2 (P X 0 ). This implies that ([e i ] ∼ ⊗ [e j ] ∼ ) i,j∈I is an ONB in L 2 (P X 0 ⊗ P X 0 ); see e.g., <ref type="bibr">Folland (1999, Ex. 61, p.178)</ref>. Therefore any η ∈ L 2 (P X 0 ⊗ P X 0 ) can be written as</p><formula xml:id="formula_170">η = i,j∈I η, [e i ] ∼ ⊗ [e j ] ∼ L 2 (P X 0 ⊗P X 0 ) [e i ] ∼ ⊗ [e j ] ∼</formula><p>with convergence in L 2 (P X 0 ⊗P X 0 ). Note that [e i ] ∼ ⊗[e j ] ∼ for any i, j ∈ I is an eigenfunction of T prod with the corresponding eigenvalue being µ i µ j , since</p><formula xml:id="formula_171">T prod ([e i ] ∼ ⊗ [e j ] ∼ ) = k(•, x)[e i ] ∼ (x) dP X 0 (x) ⊗ k(•, x)[e j ] ∼ (x) dP X 0 (x) = (T [e i ] ∼ ) ⊗ (T [e j ] ∼ ) = µ i µ i [e i ] ∼ ⊗ [e j ] ∼ .</formula><p>The assertion follows from this and the above eigendecomposition of η in Assumption 4.</p><p>As a direct corollary of Lemma 19, we have the following result, which provides an eigenbasis expression of the range assumption θ ∈ Range(T β prod ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Note that ε(µ</head><formula xml:id="formula_172">i + ε) -1 2 ≤ 1 for all i ∈ I, that i∈I | g, [e i ] ∼ L 2 (P X 0 ) | 2 = g 2 L 2 (P X 0 )</formula><p>&lt; ∞, and that lim ε→0 ε(µ i + ε) -1 2 = 0 (which follows from µ i &gt; 0 for all i ∈ I). These facts enable the use of the dominated convergence theorem, from which we have</p><formula xml:id="formula_173">lim ε→0 (T + εI) -1 T g -g 2 L 2 (P X 0 ) = lim ε→0 i∈I ε(µ i + ε) -1 2 | g, [e i ] ∼ L 2 (P X 0 ) | 2 = i∈I lim ε→0 ε(µ i + ε) -1 2 | g, [e i ] ∼ L 2 (P X 0 ) | 2 = 0.</formula><p>This completes the proof.</p><p>Lemma 26 Let X , k and P X 0 be such that Assumption 2 (i) is satisfied. Let g ∈ L 2 (P X 0 ) be such that g ∈ Range(T α ) for a constant 0 ≤ α ≤ 1. Then, for all ε &gt; 0, we have</p><formula xml:id="formula_174">(T + εI) -1 T g -g L 2 (P X 0 ) ≤ c α ε α ,</formula><p>where c α := h L 2 (P X 0 ) with h ∈ L 2 (P X 0 ) being such that g = T α h.</p><p>Proof Let (e i ) i∈I ⊂ H and (µ i ) i∈I ⊂ (0, ∞) be as in Lemma 16. From g ∈ Range(T α ) there exists h ∈ L 2 (P X 0 ) such that g = T α h. Therefore g can be written as</p><formula xml:id="formula_175">g = T α h = i∈I µ α i b i [e i ] ∼ ,<label>(51)</label></formula><p>where the convergence is in L 2 (P X 0 ), and b i := h, [e i ] ∼ L 2 (P X 0 ) . It then follows that</p><formula xml:id="formula_176">(T + εI) -1 T g -g = i∈I (µ i + ε) -1 µ i µ α i b i [e i ] ∼ - i∈I µ α i b i [e i ] ∼ = i∈I -ε(µ i + ε) -1 µ α i b i [e i ] ∼ .</formula><p>Therefore, by Parseval's identity, we have</p><formula xml:id="formula_177">(T + εI) -1 T g -g 2 L 2 (P X 0 ) = i∈I ε 2 (µ i + ε) -2 µ 2α i b 2 i .</formula><p>The rhs of the above equation can be bounded from above as</p><formula xml:id="formula_178">ε 2 (µ i + ε) -2 µ 2α i b 2 i = ε 2 (µ i + ε) -2+2α (µ i + ε) -2α µ 2α i b 2 i ≤ ε 2 (µ i + ε) -2+2α b 2 i = ε 2α ε 2-2α (µ i + ε) -2+2α b 2 i ≤ ε 2α b 2 i ,</formula><p>where the above two inequalities follow from ε &gt; 0 and µ i &gt; 0, and the last inequality uses α ≤ 1. Thus, we have</p><formula xml:id="formula_179">(T + εI) -1 T g -g 2 L 2 (P X 0 ) ≤ ε 2α i∈I b 2 i = ε 2α i∈I h, [e i ] ∼ L 2 (P X 0 ) 2 ≤ ε 2α h 2 L 2 (P X 0 ) ,</formula><p>where the last inequality follows from ([e i ] ∼ ) i∈I being an ONS in L 2 (P X 0 ).</p><p>Remark 27 Different from Lemma 25, Lemma 26 does not require Assumption 2 (ii) that S * has a dense image in L 2 (P X 0 ). In Lemma 25, this condition is required to guarantee that ([e i ] ∼ ) i∈I is an ONB in L 2 (P X 0 ), so that g can be expanded by this ONB. On the other hand, in Lemma 26, g can be written as (51), thanks to the assumption g ∈ Range(T α ); this is the reason why Lemma 26 does not need Assumption 2 (ii).</p><p>The following is a key lemma, based on which we show the consistency and convergence rates of our estimator.</p><p>Lemma 28 Let X , Y, k, , P X 0 , P X 1 , g := dP X 1 / dP X 0 and θ : X × X → R defined in (28) be such that Assumption 2 (i) and (iii) are satisfied. Then for any ε &gt; 0, we have</p><formula xml:id="formula_180">C YX (C XX + εI) -1 µ X 1 -µ Y 0|1 2 F = g ε ⊗ g ε , θ L 2 (P X 0 ⊗P X 0 ) -2 g, (T + εI) -1 T θ(•, x) dP X 1 (x) L 2 (P X 0 ) + θ(x, x) dP X 1 (x) dP X 1 (x)</formula><p>where g ε := (T + εI) -1 T g. In the second term of the right hand side, the inner product is well defined, since we have (T + ε n I) -1 T θ(•, x) dP X 1 (x) ∈ L 2 (P X 0 ).</p><p>Proof First note that, because is bounded (Assumption 2 (i)), the function θ in (28) satisfies θ ∈ L 2 (P X 0 ⊗ P X 0 ). Therefore, the right hand side of the assertion is well defined.</p><p>The left hand side of the assertion can be written as</p><formula xml:id="formula_181">C YX (C XX + εI) -1 µ X 1 -µ Y 0|1 2 F (52) = C YX (C XX + εI) -1 µ X 1 2 F -2 C YX (C XX + εI) -1 µ X 1 , µ Y 0|1 F + µ Y 0|1 2 F .</formula><p>As in the proof of <ref type="bibr">Fukumizu et al. (2013, Thm. 8)</ref>, the third term in (52) can be written as</p><formula xml:id="formula_182">µ Y 0|1 2 F = θ(x, x) dP X 1 (x) dP X 1 (x).<label>(53)</label></formula><p>We thus derive the expressions for the first two terms in (52) in the sequel.</p><p>The first term in (52). Let f ∈ H be arbitrary, and let ( X0 , Ỹ0 ) denote an independent copy of (X 0 , Y 0 ). By the property of C YX that C YX f, h F = E X 0 ,Y 0 [f (X 0 )h(Y 0 ))] for any h ∈ F and the expression θ(x, x) = E Y 0 , Ỹ0 [ (Y 0 , Ỹ0 )|X 0 = x, X0 = x], we have</p><formula xml:id="formula_183">C YX f 2 F = C YX f, C YX f F = E X 0 ,Y 0 [f (X 0 )(C YX f )(Y 0 ))] = E X 0 ,Y 0 [f (X 0 )E X0 , Ỹ0 [ (Y 0 , Ỹ0 )f ( X0 )]] = E X 0 , X0 [f (X 0 )f ( X0 )E Y 0 , Ỹ0 [ (Y 0 , Ỹ0 )|X 0 , X0 ]] (∵ Fubini theorem) = E X 0 , X0 [f (X 0 )f ( X0 )θ(X 0 , X0 )],<label>(54)</label></formula><p>where the use of Fubini's theorem is enabled by and f being bounded, the latter implied by k being bounded. Now define f := (C XX + εI) -1 µ X 1 ∈ H . With this choice of f , the quantity C YX f 2 F is equal to the first term in (52). From (54), it follows that C YX f 2 F = E X 0 , X0 [f (X 0 )f ( X0 )θ(X 0 , X0 )] = f (x)f (x)θ(x, x) dP X 0 (x) dP X 0 (x) = S * f ⊗ S * f, θ L 2 (P X 0 ⊗P X 0 )</p><p>= S * (C XX + εI) -1 µ X 1 ⊗ S * (C XX + εI) -1 µ X 1 , θ L 2 (P X 0 ⊗P X 0 )</p><p>= S * (C XX + εI) -1 Sg ⊗ S * (C XX + εI) -1 Sg, θ L 2 (P X 0 ⊗P X 0 ) (∵ Lemma 21) = (T + εI) -1 T g ⊗ (T + εI) -1 T g, θ L 2 (P X 0 ⊗P X 0 ) (∵ Lemma 22) = g ε ⊗ g ε , θ L 2 (P X 0 ⊗P X 0 ) ,</p><p>where g ε := (T + εI) -1 T g.</p><p>The second term in (52). First we have</p><formula xml:id="formula_185">E Y 0 [µ Y 0|1 (Y 0 )|X 0 = x] = E Y 0 E Ỹ0 [ (Y 0 , Ỹ0 )| X0 = x] dP X 1 (x)|X 0 = x = E Y 0 , Ỹ0 (Y 0 , Ỹ0 )|X 0 = x, X0 = x dP X 1 (x) (∵ Fubini) = θ(x, x) dP X 1 (x)<label>(56)</label></formula><p>where ( X0 , Ỹ0 ) is an independent copy of (X 0 , Y 0 ). Note that for the first expression in (56), we have</p><formula xml:id="formula_186">E Y 0 [µ Y 0|1 (Y 0 )|X 0 = •] ∈ L 2 (P X 0</formula><p>) since is bounded. Using this and (56), we have</p><formula xml:id="formula_187">C XY µ Y 0|1 = E X 0 ,Y 0 [k(•, X 0 )µ Y 0|1 (Y 0 )] = E X 0 k(•, X 0 )E Y 0 [µ Y 0|1 (Y 0 )|X 0 ] = SE Y 0 [µ Y 0|1 (Y 0 )|X 0 = •] = S θ(•, x) dP X 1 (x).<label>(57)</label></formula><p>Now for the second term in (52), we have</p><formula xml:id="formula_188">C YX (C XX + ε n I) -1 µ X 1 , µ Y 0|1 F = µ X 1 , (C XX + ε n I) -1 C XY µ Y 0|1 H = Sg, (C XX + ε n I) -1 S θ(•, x) dP X 1 (x)</formula><p>H (∵ Lemma 21 and (57))</p><p>= g, S * (C XX + ε n I) -1 S θ(•, x) dP X 1 (x)</p><p>L 2 (P X 0 )</p><p>= g, (T + ε n I) -1 T θ(•, x) dP X 1 (x)</p><p>L 2 (P X 0 ) (∵ Lemma 22).</p><p>This completes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix E. Proofs for Section 4</head><p>We provide proofs for the convergence results presented in Section 4 of the main paper.</p><p>The proofs rely on several lemmas collected and proved in Appendix D. The notation and definitions follow those in these sections. In the following, for any bounded linear operator </p><formula xml:id="formula_189">≤ C YX ( C XX + ε n I) -1 (μ X 1 -µ X 1 ) F + ( C YX -C YX )(C XX + ε n I) -1 µ X 1 F + C YX ( C XX + ε n I) -1 (C XX -C XX )(C XX + ε n I) -1 µ X 1 F<label>(</label></formula><formula xml:id="formula_190">C YX ( C XX + ε n I) -1 = C 1/2 YY W YX C 1/2 XX ( C XX + ε n I) -1 ≤ C 1/2 YY ( C XX + ε n I) -1/2 ≤ C 1/2 YY ε -1/2 n . (<label>60</label></formula><formula xml:id="formula_191">)</formula><p>Thus, the rate of the first term in ( <ref type="formula">59</ref>) is</p><formula xml:id="formula_192">C YX ( C XX + ε n I) -1 (μ X 1 -µ X 1 ) F ≤ C 1/2 YY ε -1/2 μX 1 -µ X 1 H = O p (ε -1/2 n -1/2 ).</formula><p>Next, the rate of the second term in ( <ref type="formula">59</ref>) is given by</p><formula xml:id="formula_193">( C YX -C YX )(C XX + ε n I) -1 µ X 1 F ≤ C YX -C YX (C XX + ε n I) -1 µ X 1 H ≤ C YX -C YX c α ε min(-1/2+α,0) n (∵ Lemma 24) = O p n -1/2 ε min(-1/2+α,0) n ,</formula><p>where c α is a constant depending only on α and g. Finally, for the third term in (59), the rate is given as</p><formula xml:id="formula_194">C YX ( C XX + ε n I) -1 (C XX -C XX )(C XX + ε n I) -1 µ X 1 F ≤ C YX ( C XX + ε n I) -1 C XX -C XX (C XX + ε n I) -1 µ X 1 H ≤ C 1/2 YY ε -1/2</formula><p>n C XX -C XX c α ε min(-1/2+α,0) n (∵ (60) and Lemma 24)</p><p>= O p n -1/2 ε min(-1+α,-1/2) n .</p><p>Since we will set ε n so that ε n → 0 as n → ∞, the rate of the third term is the slowest in the three terms in (59). This completes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 Proof of Theorem 8</head><p>Proof By the triangle inequality, we can bound the error of our estimator as</p><formula xml:id="formula_195">C YX ( C XX + ε n I) -1 μX 1 -µ Y 0|1 F ≤ C YX ( C XX + ε n I) -1 μX 1 -C YX (C XX + ε n I) -1 µ X 1 F (61) + C YX (C XX + ε n I) -1 µ X 1 -µ Y 0|1 F ,<label>(62)</label></formula><p>where ( <ref type="formula">61</ref>) can be interpreted as the stochastic error and (62) as the approximation error. Note that the assumption g ∈ L 2 (P X 0 ) enables the use of Theorem 30 with α = 0, which implies that the estimation error (61) converges to 0 at rate O p (n -1/2 ε -1 n ) as n → ∞, provided that ε n → 0 and n 1/2 ε n → ∞ as n → ∞.</p><p>Here we aim to prove that the approximation error (62) goes to zero as ε n → 0. Note that to this end, we cannot apply the proof of Theorem 8 in <ref type="bibr" target="#b25">Fukumizu et al. (2013)</ref>, since it relies on stronger assumptions than ours. We do this by using Lemma 28, which shows that the approximation error can be written as</p><formula xml:id="formula_196">C YX (C XX + ε n I) -1 µ X 1 -µ Y 0|1 2 F = g εn ⊗ g εn , θ L 2 (P X 0 ⊗P X 0 )<label>(63)</label></formula><p>-2 g, (T + ε n I) -1 T θ(•, x) dP X 1 (x)</p><p>L 2 (P X 0 )</p><p>(64)</p><formula xml:id="formula_197">+ θ(x, x) dP X 1 (x) dP X 1 (x),</formula><p>where g εn := (T + ε n I) -1 T g with g = dP X 1 / dP X 0 being the Radon-Nikodym derivative.</p><p>Below we show the convergence limits of ( <ref type="formula" target="#formula_196">63</ref>) and ( <ref type="formula">64</ref>) as ε n → 0, which conclude the proof.</p><p>Convergence of (63). We will show that g εn ⊗ g εn , θ L 2 (P X 0 ⊗P X 0 ) → θ(x, x) dP X 1 (x) dP X 1 (x) (ε n → 0).</p><p>(65)</p><p>Note that we have g ⊗ g, θ L 2 (P X 0 ⊗P X 0 ) = g(x)g(x)θ(x, x) dP X 0 (x) dP X 0 (x) = θ(x, x) dP X 1 (x) dP X 1 (x).</p><p>Therefore it suffices to show that g εn ⊗ g εn , θ L 2 (P X 0 ⊗P X 0 ) → g ⊗ g, θ L 2 (P X 0 ⊗P X 0 ) (ε n → 0).</p><p>Note that by the Cauchy-Schwartz inequality, we have g εn ⊗ g εn , θ L 2 (P X 0 ⊗P X 0 )g ⊗ g, θ L 2 (P X 0 ⊗P X 0 ) = g εn ⊗ g εng ⊗ g, θ L 2 (P X 0 ⊗P X 0 ) ≤ g εn ⊗ g εng ⊗ g L 2 (P X 0 ⊗P X 0 ) θ L 2 (P X 0 ⊗P X 0 ) .</p><p>Thus we focus on showing that g εn ⊗ g εng ⊗ g L 2 (P X 0 ⊗P X 0 ) → 0 (ε n → 0).</p><p>By the triangle inequality we have g εn ⊗ g εng ⊗ g L 2 (P X 0 ⊗P X 0 ) ≤ g εn ⊗ g εng ⊗ g εn L 2 (P X 0 ⊗P X 0 ) + g ⊗ g εng ⊗ g L 2 (P X 0 ⊗P X 0 ) .</p><p>The first term of (67) can be written as g εn ⊗ g εng ⊗ g εn L 2 (P X 0 ⊗P X 0 ) = (g εng) ⊗ g εn L 2 (P X 0 ⊗P X 0 )</p><p>= g εng L 2 (P X 0 ) g εn L 2 (P X 0 ) → 0 (ε n → 0) (∵ Lemma 25),</p><p>Similarly, the second term of (67) can be written as g ⊗ g εng ⊗ g L 2 (P X 0 ⊗P X 0 ) = g ⊗ (g εng) L 2 (P X 0 ⊗P X 0 ) = g L 2 (P X 0 ) g εng L 2 (P X 0 ) → 0 (ε n → 0) (∵ Lemma 25).</p><p>We have shown (66), which concludes (65).</p><p>= g εn ⊗ g εn , θ L 2 (P X 0 ⊗P X 0 ) -2 g, (T + ε n I) -1 T θ(•, x) dP X 1 (x)</p><p>L 2 (P X 0 ) + θ(x, x) dP X 1 (x) dP X 1 (x) ≤ g εn ⊗ g εn , θ L 2 (P X 0 ⊗P X 0 )g ⊗ g, θ L 2 (P X 0 ⊗P X 0 ) (74)</p><p>+2 g ⊗ g, θ L 2 (P X 0 ⊗P X 0 )g, (T + ε n I) -1 T θ(•, x) dP X 1 (x)</p><p>L 2 (P X 0 )</p><p>, T β (g εng)</p><formula xml:id="formula_200">Bound</formula><formula xml:id="formula_201">L 2 (P X 0 ) = i,j∈I</formula><p>a 2 i,j T β (g εng)</p><p>L 2 (P X 0 )</p><p>T β g εn L 2 (P X 0 )</p><formula xml:id="formula_202">+ T β g L 2 (P X 0 )<label>(75)</label></formula><p>Note that T β g ∈ Range(T α+β ) holds because of the assumption g ∈ Range(T α ). Therefore by Lemma 26 (which can be used because α + β ≤ 1), we have</p><formula xml:id="formula_203">T β g εn -T β g L 2 (P X 0 ) = (T + ε n I) -1 T T β g -T β g L 2 (P X 0 ) ≤ c α+β ε α+β n , (<label>76</label></formula><formula xml:id="formula_204">)</formula><p>where c α+β is a constant depending only on α, β and g. We also have</p><formula xml:id="formula_205">T β g εn L 2 (P X 0 ) ≤ T β g εn -T β g L 2 (P X 0 ) + T β g L 2 (P X 0 ) ≤ c α+β ε α+β n + T β g L 2 (P X 0 ) (∵ (76))</formula><p>Therefore ( <ref type="formula" target="#formula_202">75</ref>), and thus the first term in (74), is bounded by </p><p>Bound on the second term in (74). From the equivalence (73), (the half of) the second term in (74) can be written as g, θ(•, x) dP X 1 (x)</p><p>L 2 (P X 0 )</p><p>g, (T + ε n I) -1 T θ(•, x) dP X 1 (x)</p><p>L 2 (P X 0 )</p><p>.( <ref type="formula">78</ref>)</p><p>Note that, using θ = i,j∈I a i,j (µ β i [e i ] ∼ ) ⊗ (µ β j [e j ] ∼ ), we can write</p><formula xml:id="formula_207">θ(•, x) dP X 1 (x) = i,j∈I a i,j (µ β i [e i ] ∼ ) ⊗ (µ β j [e j ] ∼ (x)) dP X 1 (x) = i,j∈I a i,j (µ β i [e i ] ∼ ) (µ β j [e j ] ∼ (x)) dP X 1 (x).</formula><p>Therefore, g, θ(•, x) dP X 1 (x)</p><p>L 2 (P X 0 ) = g, i,j∈I a i,j (µ β i [e i ] ∼ ) (µ β j [e j ] ∼ (x)) dP X 1 (x)</p><p>L 2 (P X 0 ) = i,j∈I a i,j µ β i g, [e i ] ∼ L 2 (P X 0 ) (µ β j [e j ] ∼ (x)) dP X 1 (x).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>and kernel : Y × Y → R; the number m ∈ N of sample points to generate. 2: Compute ỹ1 := arg max y∈Y n i=1 β i (y i , y). 3: for t = 2 to m do 4:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>1). Note that the potential outcomes Y * 0 and Y * 1 are independent to the treatment indicator T given the covariates X under the conditional exogeneity in Assumption 1. Therefore, Y * 0 |X, (T = 0) = Y * 0 |X and Y * 1 |X, (T = 1) = Y * 1 |X. Thus, we can identify the conditional distributions P Y 0 |X 0 and P Y 1 |X 1 as P 0 (r|u, a) and P * (r|u, a), respectively:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Histograms of observed outcomes (y i ) N i=1 from the data {(x i , t i , y i )} N i=1 generated under the three scenarios in Section 7.1.1, with N = 500. For each scenario, the green histogram consists of outcomes y i with t i = 0, which are i.i.d. with Y 0 = Y |(T = 0)), and the blue histogram consists of y i with t i = 1, which are i.i.d. with Y 1 = Y |(T = 1). Note that Y 0 = Y * 0 |(T = 0) and Y 1 = Y * 1 |(T = 1), so the distributions of Y 0 and Y 1 (described here) are slightly different from those of the potential outcomes Y * 0 and Y * 1 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Figure 2 describes the empirical distributions of observed outcomes for the three scenarios, where Y 0 = Y |(T = 0) and Y 1 = Y |(T = 1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Results of the experiments in Section 7.1.2. (a) Histograms of observed outcomes(y i ) n i=1 of Y * 0 |(T = 0) (red), a counterfactual sample of Y * 0 |(T = 1) (blue)and the approximate counterfactual sample (y j ) n j=1 generated with Algorithm 1 applied to our CME estimate (green), obtained from data with size n = 500. For illustration, we also show density curves obtained from the corresponding samples of the same colors, estimated with kernel density estimation. (b) Powers of the two sample tests based on the generated counterfactual sample (y j ) n j=1 and observed outcomes (y i ) n i=1 , using the unbiased MMD statistic with the Gaussian or the linear kernel with significance level α = 0.01. The powers are obtained from 1,000 repetitions, for each of different sample sizes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Mean square error (MSE) of the expected reward estimated by different estimators as we vary the value of (a) the multiplier α, (b) the context dimension d, (c) the number of available items M , (d) the number of users N , (e) the number of recommended items K, and (f) the number of observations n. Each error bar represents a 95% confidence interval.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The performance of different estimators on the MSLR-WEB30K dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Proof</head><label></label><figDesc>We only show P Y 0|0 (y) = P Y * 0 |T (y|0); the other identity P Y 1|1 (y) = P Y * 1 |T (y|1) can be shown similarly. First notice that Y 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>C. 2</head><label>2</label><figDesc>Proof of Lemma 4 Proof We only show here P Y 0|1 = P Y * 0 |T =1 ; the other identity P Y 1|0 = P Y * 1 |T =0 can be shown similarly. We first derive basic identities: (a) The conditional exogeneity implies that P Y * 0 |T,X (y|1, x) = P Y * 0 |X (y|x) = P Y * 0 |T,X (y|0, x) for almost every x with respect to the distribution of X; (b) Recalling</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>a) follows from the definitions of Y and Ỹ , (b) from the conditional exogeneity, (c) from E[T | X] = e(X), (d) from the conditional exogeneity, and (e) from E[ T | X] = e( X).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>a) follows from Ỹ being Ỹ * 1 if T = 1, (b) from the conditional exogeneity and (c) from E T [ T | X] = e( X).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>59)ByBaker (1973, Theorem 1), C YX can be decomposed asC YX = C linear operator W YX : H → F with W YX ≤ 1, where C 1/2 YY : F → F and C 1/2 XX : H → H are such that C YY = C</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>β g L 2 (P X 0 ) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>being the observed outcome and Y * 0 , Y * 1 being the potential outcomes. Let n := N i=1 (1t i ) and m := N i=1 t i . See Section 3.5 for the notation. Distributional treatment effects. Here we are interested in testing whether P Y * 0 and P Y * 1 are equal or not (see Section 3.1.1). The null hypothesis H 0 and the alternative hypothesis H 1 are thus defined as H 0 : P Y * 0 = P Y * 1 , H 1 : P Y * 0 = P Y</figDesc><table /><note><p>* 1 , As a test statistic, we propose to use an estimate KTE(Y * 0</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>introduced in Section 3.5.1. This esti-mate KTE(Y * 0 , Y * 1 , F ), computed from the data (x i , t i , y i ) N i=1 , can either be the biased one, KTE b (Y * 0 , Y * 1 , F ), defined in (22), or the unbiased one, KTE u (Y * 0 , Y * 1 , F ), in (</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>on the first term in (74). By Corollary 20 (which follows from Assumption 4),θ = i,j∈I a i,j (µ β i [e i ] ∼ ) ⊗ (µ β j [e j ] ∼ ) with i,j∈I a 2 ij &lt; ∞.Using this, we haveg εn ⊗ g εn , θ L 2 (P X 0 ⊗P X 0 )g ⊗ g, θ L 2 (P X 0 ⊗P X 0 ) = g εn ⊗ g εn , i,j∈I a i,j (µ β i [e i ] ∼ ) ⊗ (µ β j [e j ] ∼ ) εn ⊗ g εn , θ L 2 (P X 0 ⊗P X 0 )g ⊗ g, θ L 2 (P X 0 ⊗P X 0 ) T β (g εng)</figDesc><table><row><cell></cell><cell></cell><cell cols="2">i,j∈I</cell><cell cols="6">a 2 i,j L 2 (P X 0 )</cell><cell>T β g εn</cell><cell>L 2 (P X 0 )</cell></row><row><cell></cell><cell>+</cell><cell cols="5">i,j∈I</cell><cell cols="3">a 2 i,j T β g</cell><cell>L 2 (P X 0 )</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="3">i,j∈I</cell><cell>a i,j g, µ β i [e i ] ∼</cell><cell>L 2 (P X 0 )</cell><cell>g, µ β j [e j ] ∼</cell><cell>L 2 (P X 0 )</cell></row><row><cell></cell><cell cols="2">=</cell><cell cols="5">i,j∈I</cell><cell cols="2">a i,j g εn -g, µ β i [e i ] ∼</cell><cell>L 2 (P X 0 )</cell><cell>g εn , µ β j [e j ] ∼</cell><cell>L 2 (P X 0 )</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">+</cell><cell cols="3">i,j∈I</cell><cell>a i,j g, µ β i [e i ] ∼</cell><cell>L 2 (P X 0 )</cell></row><row><cell>≤</cell><cell cols="2">i,j∈I</cell><cell cols="7">a i,j g εn -g, µ β i [e i ] ∼</cell><cell>L 2 (P X 0 )</cell><cell>g εn , µ β j [e j ] ∼</cell><cell>L 2 (P X 0 )</cell></row><row><cell></cell><cell>+</cell><cell cols="3">i,j∈I</cell><cell cols="5">a i,j g, µ β i [e i ] ∼</cell><cell>L 2 (P X 0 )</cell><cell>g εn -g, µ β j [e j ] ∼</cell><cell>L 2 (P X 0 )</cell></row><row><cell>≤</cell><cell></cell><cell cols="2">i,j∈I</cell><cell cols="4">a 2 i,j</cell><cell></cell><cell>i∈I</cell><cell>g εn -g, µ β i [e i ] ∼</cell><cell>2 L 2 (P X 0 )</cell><cell>j∈I</cell><cell>g εn , µ β j [e j ] ∼</cell><cell>2 L 2 (P X 0 )</cell></row><row><cell></cell><cell>+</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">a 2 i,j</cell><cell>g, µ β</cell></row><row><cell></cell><cell></cell><cell cols="5">i,j∈I</cell><cell></cell><cell></cell><cell>i∈I</cell></row></table><note><p><p><p>L 2 (P X 0 ⊗P X 0 ) g ⊗ g, i,j∈I a i,j (µ β i [e i ] ∼ ) ⊗ (µ β j [e j ] ∼ ) L 2 (P X 0 ⊗P X 0 ) = i,j∈I a i,j g εn , µ β i [e i ] ∼ L 2 (P X 0 ) g εn , µ β j [e j ] ∼ L 2 (P X 0 ) g εng, µ β j [e j ] ∼ L 2 (P X 0 )</p>. Therefore,</p>g i [e i ] ∼ 2 L 2 (P X 0 ) j∈I g εng, µ β j [e j ] ∼ 2 L 2 (P X 0 ) =</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>This independence assumption may be seen as a version of the Stable Unit Treatment Value Assumption (SUTVA), which requires that the potential outcomes of any subject i are independent of the treatments tj assigned to the other subjects j = i.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1"><p>More precisely, each element in L2(PX 0 ) is a PX 0 -equivalent class of functions; see Appendix D.1.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We express our sincere gratitude to the Action Editor and the anonymous reviewers, whose comments greatly helped improve the quality of the paper. We also thank <rs type="person">Ricardo Silva</rs>, <rs type="person">Joris Mooij</rs>, <rs type="person">Adith Swaminathan</rs>, <rs type="person">Evan Robin</rs>, <rs type="person">David Lopez-Paz</rs>, <rs type="person">Wittawat Jitkrittum</rs>, <rs type="person">Kenji Fukumizu</rs>, and <rs type="person">Bernhard Schölkopf</rs> for fruitful discussions. Krikamol Muandet would like to acknowledge fundings from the <rs type="funder">Thailand Research Fund</rs> Grant No. <rs type="grantNumber">MRG6080206</rs> and additional funding from the <rs type="funder">Faculty of Science, Mahidol University</rs>. Motonobu Kanagawa has been partially supported by the <rs type="funder">European Research Council</rs> (<rs type="projectName">StG</rs> project <rs type="projectName">PANAMA</rs>). Sorawit Saengkyongam is supported by a research grant (<rs type="grantNumber">18968</rs>) from <rs type="funder">VILLUM FONDEN</rs>.</p></div>
			</div>
			<div type="funding">
<div><p>This essentially is the case of M = ∞. For instance, the integral can be computed analytically when the kernel k A is Gaussian and the target policy π * (a|u) is an additive Gaussian noise model of the form π * (a|u) = N (a|F (u), σ 2 ) for some function F : <rs type="person">U → A</rs> and σ 2 &gt; 0. This way of using an analytic integral for approximating the kernel mean is studied in <ref type="bibr" target="#b54">Nishiyama et al. (2020)</ref>; we refer to this paper for details and other examples.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_pW5BrEb">
					<idno type="grant-number">MRG6080206</idno>
				</org>
				<org type="funded-project" xml:id="_fhwq2Ym">
					<orgName type="project" subtype="full">StG</orgName>
				</org>
				<org type="funded-project" xml:id="_pahxGJT">
					<idno type="grant-number">18968</idno>
					<orgName type="project" subtype="full">PANAMA</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A. Possible Extensions for Off-Policy Evaluation</head><p>Here we describe possible extensions of the proposed approach to the off-policy evaluation task described in Section 6.3 (Algorithm 2). As mentioned there, Algorithm 2 only generates one action a * j ∼ π * (a|u * j ) for each u * j , which does not fully exploit the information of the target policy π * (a|u * j ). We show a possible approach to using more information from the target policy, thereby improving the quality of the algorithm.</p><p>First, notice that the weight vector β ∈ R n in Algorithm 2 depends on the sample (x j ) m j=1 = (u * j , a * j ) m j=1 only through the vector K1 m ∈ R m , where K = (k(x i , x j )) ∈ R n×m and 1 m = 1 m (1, . . . , 1) ∈ R m . This vector can be written as</p><p>where (x i ) n i=1 = (u i , a i ) n i=1 are from the logged data, and μP X 1 is an empirical approximation of the mean embedding µ P X 1 of the covariate distribution P X 1 , given by</p><p>This implies that the role of the sample (x j ) m j=1 is essentially to approximate the kernel mean µ P X 1 . In fact, the quality of the CME estimator (based on which Algorithm 2 is constructed) depends on the sample (x j ) m j=1 only through the the approximation error μX 1µ X 1 F (see e.g., the proof of Theorem 30).</p><p>Thus, Algorithm 2 may be improved by constructing a better approximation, say μX 1 , of the kernel mean μX 1 , and replace (41) in the computation of the weight vector β by the evaluations of this new approximation:</p><p>where K := (k(x i , x j )) n i,j=1 ∈ R n×n . To construct μX 1 , recall that the kernel k is given as a product kernel k(x, x ) = k A (a, a )k U (u, u ) for x = (u, a), x = (u , a ), and rewrite the kernel mean µ X 1 as</p><p>Thus, if we can approximate the integral in the last expression accurately, we can obtain a good approximation for the kernel mean.</p><p>One approach is to generate M &gt; 1 actions a j1 , . . . , a jM ∼ π * (a|u * j ) from the target policy for each u * j , and the approximate the integral as</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B. Cross Validation Procedure for Counterfactual Prediction</head><p>Here we describe an approach to cross validation for model selection in counterfactual prediction, focusing on the problem of off-policy evaluation (OPE). We use below the notation defined in Section 6. Unlike the standard situation in machine learning, performing cross validation directly on the logged data D := {(u i , a i , r i )} n i=1 may lead to a biased estimate on the performance measure for counterfactual prediction, resulting in sub-optimal model selection. This is due to the covariate shift -the change of the covariate distribution from π 0 (u, a) in the data collection environment to π * (u, a) in the target environment. To correct for the bias due to this distributional shift, we propose the following procedure for cross validation. A similar cross validation approach has been proposed by <ref type="bibr" target="#b78">Sugiyama et al. (2007)</ref>.</p><p>Let M := {1, . . . , M } be a set of indicators for candidate models. (e.g., each m ∈ M may represent a specific choice of the kernel k and regularization constant ε in our CME estimator.) 1. Split D into K folds: D k = {(u j , a j , r j )} qk j=q(k-1)+1 for k = 1, . . . , K and q = n/K . 2. For each model m ∈ M:</p><p>(a) For each fold k = 1, 2, . . . , K: i. Calculate w 1 , . . . , w q ≥ 0 using propensity scores or covariate matching. ii. Re-weight the validation reward r * k = q j=1 w j r q(k-1)+j (bias correction). iii. Use the remaining logged data D ¬k and validation data {(x * j , s * j )} qk j=q(k-1)+1</p><p>to compute the estimated reward rk and corresponding error e k = (r kr * k ) 2 . (b) Calculate the mean CV error ēp = 1 K K k=1 e k (variance reduction).</p><p>3. Pick the m-th parameter setting whose ēm is the smallest.</p><p>which completes the proof.</p><p>Appendix D. Preliminaries to the Proofs for Section 4</p><p>We collect here preliminary results required for proving the theoretical results in Section 4. Thus, the interested reader may first look at Appendix E, where the proofs for the main theoretical results are presented. We use here the notation and basic definitions provided in Section 4 of the main body. In Appendix D.1, we introduce certain integral operators and collect basic facts regarding them. Based on them, in Appendix D.2 we present various lemmas needed for the proofs of the convernce results in Appendix E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Integral Operators</head><p>To be rigorous, we employ the following notation used in <ref type="bibr" target="#b77">Steinwart and Scovel (2012)</ref>: For a measurable function f : X → R, [f ] ∼ denotes the class of measurable functions that are</p><p>Note that while these operators look similar, they are different in their domains and ranges.</p><p>In particular, C XX is the covariance operator. Under Assumption 2 (i), i.e., sup x∈X k(x, x) &lt; ∞, Steinwart and Scovel (2012, Lemma 2.3) implies that the operator</p><p>is compact, and thus continuous. This operator S * is the adjoint of the operator S defined in (45). Since S * is continuous, by <ref type="bibr">Steinwart and Scovel (2012, Lemma 2.3)</ref>, the operators T and C XX can be written as</p><p>The following lemma summarizes conditions required for eigen-decompositions of ( <ref type="formula">44</ref>), ( <ref type="formula">45</ref>) and ( <ref type="formula">46</ref>). In the sequel, "ONS" and "ONB" mean "orthonormal series" and "orthonormal basis," respectively. The set I ⊂ N is a set of indices, which is finite if the RKHS H is finite dimensional, and infinite if H is infinite dimensional.</p><p>Corollary 20 Let X , k and P X 0 be such that Assumption 2 (i) and (ii) are satisfied, and let (e i ) i∈I ⊂ H and (µ i ) i∈I ⊂ (0, ∞) be as in Lemma 16. Suppose that Assumption 4 is satisfied, i.e., θ ∈ Range(T β prod ) holds for θ ∈ L 2 (P X 0 ⊗ P X 0 ) and 0 ≤ β ≤ 1, where T β prod is defined in (29). Then there exist (a i,j ) i,j∈I ⊂ R such that i,j∈I a 2 i,j &lt; ∞ and</p><p>where the convergence is in L 2 (P X 0 ⊗ P X 0 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof</head><p>The assumption θ ∈ Range(T β prod ) implies that there exists some η ∈ L 2 (P</p><p>Lastly, let C Y X : H → F be the covariance operator of the random variables X 0 and Y 0 defined as (see e.g., <ref type="bibr" target="#b25">Fukumizu et al. (2013)</ref>)</p><p>where P X 0 Y 0 is the joint distribution of X 0 and Y 0 . Under Assumption 2 (i), this covariance operator satisfies</p><p>The conjugate operator of C Y X is denoted by C XY : F → H and given by</p><p>since for any f ∈ H and g ∈ F it holds that</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Lemmas</head><p>We collect here lemmas used in the proofs for the convergence results in Appendix E.</p><p>Lemma 21 Let X , k, P X 0 , P X 1 and g := dP X 1 / dP X 0 be such that Assumption 2 (i) and (iii) are satisfied. Then we have µ X 1 = Sg.</p><p>Proof By definitions of the kernel mean µ X 1 and the Radon-Nikodym derivative g, we have</p><p>where the expression Sg is justified from Assumption 2 (iii) that g ∈ L 2 (P X 0 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Counterfactual Mean Embeddings</head><p>Lemma 22 Let X , k and P X 0 be such that Assumption 2 (i) is satisfied. Then for any f ∈ L 2 (P X 0 ) and ε &gt; 0, we have S * (C XX + εI) -1 Sf = (T + εI) -1 T f.</p><p>Proof Let (e i ) i∈I ⊂ H and (µ i ) i∈I ⊂ (0, ∞) as in Lemma 16. Then we have</p><p>Lemma 23 Let X , k and P X 0 be such that Assumption 2 (i) is satisfied. Then for any f ∈ L 2 (P X 0 ) and α ≥ 0, we have</p><p>Proof Let (e i ) i∈I ⊂ H and (µ i ) i∈I ⊂ (0, ∞) as in Lemma 16. Then we have</p><p>Lemma 24 Let X , k, P X 0 , P X 1 and g := dP X 1 / dP X 0 be such that Assumption 2 (i) and (iii) are satisfied. Assume also that Assumption 3 holds, i.e., g ∈ Range(T α ) for a constant α ≥ 0. Then for any ε &gt; 0, we have</p><p>where c α := h L 2 (P X 0 ) is a constant defined by h ∈ L 2 (P X 0 ) such that g = T α h.</p><p>Proof As in the assertion, write g = T α h for h ∈ L 2 (P X 0 ), which exists from the assumption g ∈ Range(T α ). By Lemmas 21 and 23, we can then write µ X 1 as</p><p>and thus S 1/2 h ∈ H . Therefore we have</p><p>Below we focus on bounding the first term in the above bound.</p><p>This completes the proof.</p><p>Lemma 25 Let X , k and P X 0 be such that Assumption 2 (i) and (ii) are satisfied. Then, for any g ∈ L 2 (P X 0 ), we have</p><p>Proof Let (e i ) i∈I ⊂ H and (µ i ) i∈I ⊂ (0, ∞) be as in Lemma 16. By Lemma 18, ([e i ] ∼ ) i∈I is an ONB of L 2 (P X 0 ), which implies that g can be expanded using ([e i ] ∼ ) i∈I . From this and Lemma 16, we then have</p><p>Thus, by Parseval's identity,</p><p>A : V → W between normed vector spaces V and W , we denote by A its operator norm:</p><p>, where • V and • W denote the norms of V and W , respectively. We first show that the CME estimator μ 0|1 in ( <ref type="formula">18</ref>) can be expressed in terms of certain empirical covariance operators. Given an i.i.d. sample (x i , y i ) n i=1 from P X 0 Y 0 , the covariance operators C XX : H → H in ( <ref type="formula">46</ref>) and C Y X : H → F in ( <ref type="formula">50</ref>) can be respectively approximated by C XX : H → H and C YX : H → F , defined as</p><p>Under Assumption 2 (i) that the kernels k and are bounded, these satisfy</p><p>Proposition 29 Let μ 0|1 be the CME estimator in (18). Then we have</p><p>Proof Define g := (</p><p>n (Kg) + εg for all = 1, . . . , n, where K ∈ R n×n with K i,j = k(x i , x j ) and g = (g(x 1 ), . . . , g(x n )) ∈ R n . Therefore µ = 1 n (K + nεI)g, where µ := (μ X 1 (x 1 ), . . . , μX 1 (x n )) = K1 m , where 1 m = (1/m, . . . , 1/m) and K ∈ R n×m with K ij = k(x i , x j ). Thus g = n(K + nεI) -1 µ. Lastly, the right hand side of (58) can be expressed as</p><p>, where β = (β 1 , . . . , β n ) = n -1 g = (K + nεI) -1 µ, which is the expression of the CME estimator μ 0|1 in (18).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.1 Convergence Rates of the Stochastic Error</head><p>The proofs of Theorems 8 and 13 rely on the following result, which characterizes the "stochastic error" of the CME estimator. As stated in Assumption (iv), we assume m = n in the following.</p><p>Theorem 30 Let X be a measurable space, k be a measurable kernel on X and P X 0 be a probability measure on X such that Assumption 2 (i), (iii) and (iv) are satisfied. Assume that the Radon-Nikodym derivative g := dP X 1 / dP X 0 satisfies g ∈ Range(T α ) for a constant α ≥ 0. Then for any ε n &gt; 0 such that ε n → 0 as n → ∞, we have</p><p>Proof As in the proof of <ref type="bibr">Fukumizu et al. (2013, Theorem 11)</ref>, the lhs can be bounded as</p><p>Convergence of (64). We show that as ε n → 0,</p><p>From Lemma 25, as ε n → 0, the lhs converges to</p><p>Thus we have shown ( <ref type="formula">68</ref>). The proof completes by substituting ( <ref type="formula">65</ref>) and ( <ref type="formula">68</ref>) in ( <ref type="formula">63</ref>) and ( <ref type="formula">64</ref>) respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.3 Proof of Theorem 13</head><p>Proof By the triangle inequality we can bound the error of our estimator as</p><p>where ( <ref type="formula">69</ref>) is the estimation error, and (70) is the approximation error. By Theorem 30, the estimation error decays at the rate</p><p>as n → ∞. Hence we focus below on deriving a convergence rate for the approximation error. We then determine the optimal schedule for the decay of the regularization constant ε n as n → ∞ in order to derive a convergence rate for the overall error.</p><p>Rate for the approximation error (70). We will show that the approximation error decays at the rate</p><p>First note that, by the definition of g = dP X 1 / dP X 0 , we have</p><p>Therefore, using Lemma 28 and the notation g εn := (T +ε n I) -1 T g, we can bound the square of the approximation error (70) as</p><p>Because of the properties that</p><p>Therefore, using the Cauchy-Schwartz, the above identities and inequality, we have , where the last inequality follows from ([e j ] ∼ ) j∈I being an ONS in L 2 (P X 0 ) and the definition of T β . Note that we have T β g ∈ Range(T α+β ) from the assumption g ∈ Range(T α ).</p><p>Therefore by Lemma 26,</p><p>where c α+β &gt; 0 is a constant depending only on α, β and g. Thus, we finally obtain (78) ≤ i,j∈I</p><p>Resulting approximation error rate. Using ( <ref type="formula">77</ref>) and ( <ref type="formula">79</ref>) in ( <ref type="formula">74</ref>), the rate ( <ref type="formula">72</ref>) is finally obtained as</p><p>Balancing the estimation and approximation error rates. For an arbitrary constant c &gt; 0 independent of n, let ε n = cn -b for some constant b &gt; 0. We determine b by balancing the two rates ( <ref type="formula">71</ref>) and ( <ref type="formula">72</ref>). This yields b = 1/(2α + β) for α ≤ 1/2, and b = 1/(1 + α + β) for α ≥ 1/2; equivalently, b = 1/(1 + β + max(1α, α)) for 0 ≤ α ≤ 1. The proof completes by substituting the resulting ε n = n -b in ( <ref type="formula">71</ref>) and ( <ref type="formula">72</ref>).</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Bayesian inference of individualized treatment effects using multi-task gaussian processes</title>
		<author>
			<persName><forename type="first">Ahmed</forename><forename type="middle">M</forename><surname>Alaa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihaela</forename><surname>Van Der Schaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3427" to="3435" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Theory of reproducing kernels</title>
		<author>
			<persName><forename type="first">Nachman</forename><surname>Aronszajn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the American Mathematical Society</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="337" to="404" />
			<date type="published" when="1950">1950</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep-treat: Learning optimal personalized treatments from observational data using neural networks</title>
		<author>
			<persName><forename type="first">James</forename><surname>Onur Atan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihaela</forename><surname>Jordon</surname></persName>
		</author>
		<author>
			<persName><surname>Van Der Schaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Recursive partitioning for heterogeneous causal effects</title>
		<author>
			<persName><forename type="first">Susan</forename><surname>Athey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guido</forename><surname>Imbens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="issue">27</biblScope>
			<biblScope unit="page" from="7353" to="7360" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">On the equivalence between herding and conditional gradient algorithms</title>
		<author>
			<persName><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lacoste-Julien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Obozinski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Conference on Machine Learning (ICML2012)</title>
		<meeting>the 29th International Conference on Machine Learning (ICML2012)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1359" to="1366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Joint measures and cross-covariance operators</title>
		<author>
			<persName><forename type="first">Charles</forename><forename type="middle">R</forename><surname>Baker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the American Mathematical Society</title>
		<imprint>
			<biblScope unit="volume">186</biblScope>
			<biblScope unit="page" from="273" to="289" />
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">Alain</forename><surname>Berlinet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christine</forename><surname>Thomas-Agnan</surname></persName>
		</author>
		<title level="m">Reproducing Kernel Hilbert Spaces in Probability and Statistics</title>
		<imprint>
			<publisher>Kluwer Academic Publishers</publisher>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Integrating structured biological data by kernel maximum mean discrepancy</title>
		<author>
			<persName><forename type="first">Karsten</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Malte</forename><surname>Rasch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hans-Peter</forename><surname>Kriegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="49" to="57" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Counterfactual reasoning and learning systems: The example of computational advertising</title>
		<author>
			<persName><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joaquin</forename><surname>Quiǹonero-Candela</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Denis</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Chickering</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elon</forename><surname>Portugaly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dipankar</forename><surname>Ray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<editor>
			<persName><forename type="first">Patrice</forename><surname>Simard</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ed</forename><surname>Snelson</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="3207" to="3260" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Optimal rates for regularized least-squares algorithm</title>
		<author>
			<persName><forename type="first">A</forename><surname>Caponnetto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">De</forename><surname>Vito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Found. Comput. Math. J</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="331" to="368" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Some results on generalized difference estimation and generalized regression estimation for finite populations</title>
		<author>
			<persName><forename type="first">M</forename><surname>Claes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><forename type="middle">E</forename><surname>Cassel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><forename type="middle">H</forename><surname>Särndal</surname></persName>
		</author>
		<author>
			<persName><surname>Wretman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="615" to="620" />
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Expected reciprocal rank for graded relevance</title>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Metlzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Grinspan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM Conference on Information and Knowledge Management</title>
		<meeting>the 18th ACM Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="621" to="630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Super samples from kernel-herding</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the 26th Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="109" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Inference on counterfactual distributions</title>
		<author>
			<persName><forename type="first">Victor</forename><surname>Chernozhukov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iván</forename><surname>Fernández-Val</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Blaise</forename><surname>Melly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econometrica</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2205" to="2268" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">High dimensional numerical integration -the Quasi-Monte Carlo way</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">Y</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">H</forename><surname>Sloan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Numerica</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="133" to="288" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Vector Measures</title>
		<author>
			<persName><forename type="first">J</forename><surname>Diestel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uhl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1977">1977</date>
			<publisher>American Mathematical Society</publisher>
			<pubPlace>Providence</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Vector Integration and Stochastic Integration in Banach Spaces</title>
		<author>
			<persName><forename type="first">N</forename><surname>Dinculeanu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>Wiley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A permutation-based kernel conditional independence test</title>
		<author>
			<persName><forename type="first">G</forename><surname>Doran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Muandet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the 30th Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="132" to="141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Doubly Robust Policy Evaluation and Learning</title>
		<author>
			<persName><forename type="first">Miroslav</forename><surname>Dudík</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lihong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning</title>
		<meeting>the 28th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>Omnipress</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1097" to="1104" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Training generative neural networks via maximum mean discrepancy optimization</title>
		<author>
			<persName><forename type="first">Gintare</forename><surname>Karolina Dziugaite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><forename type="middle">M</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Number 57 in Monographs on Statistics and Applied Probability</title>
		<author>
			<persName><forename type="first">Bradley</forename><surname>Efron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Tibshirani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
			<publisher>Chapman &amp; Hall/CRC</publisher>
		</imprint>
	</monogr>
	<note>An Introduction to the Bootstrap</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">The Design of Experiments</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Fisher</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1935">1935</date>
			<publisher>Oliver and Boyd</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">B</forename><surname>Folland</surname></persName>
		</author>
		<title level="m">Real Analysis: Modern Techniques and Their Applications, 2nd Edition</title>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Kernel measures of conditional dependence</title>
		<author>
			<persName><surname>Fukumizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 20</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="489" to="496" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Dimensionality reduction for supervised learning with reproducing kernel Hilbert spaces</title>
		<author>
			<persName><forename type="first">Kenji</forename><surname>Fukumizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francis</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="73" to="99" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Kernel Bayes&apos; rule: Bayesian inference with positive definite kernels</title>
		<author>
			<persName><forename type="first">Kenji</forename><surname>Fukumizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="3753" to="3783" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Large sample analysis of the median heuristic</title>
		<author>
			<persName><forename type="first">Damien</forename><surname>Garreau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wittawat</forename><surname>Jitkrittum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Motonobu</forename><surname>Kanagawa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.07269</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A survey of kernels for structured data</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Gärtner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGKDD Explor. Newsl</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="49" to="58" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Classes of kernels for machine learning: A statistics perspective</title>
		<author>
			<persName><forename type="first">Marc</forename><surname>Genton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="299" to="312" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A kernel two-sample test</title>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karsten</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Malte</forename><surname>Rasch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="723" to="773" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Conditional mean embeddings as regressors</title>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Grünewälder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guy</forename><surname>Lever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Baldassarre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Patterson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Massimiliano</forename><surname>Pontil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Conference on Machine Learning</title>
		<meeting>the 29th International Conference on Machine Learning<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Omnipress</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1823" to="1830" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep IV: A flexible approach for counterfactual prediction</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Hartford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Leyton-Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Taddy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1414" to="1423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Econometric evaluation of social programs, part I: Causal models, structural models and econometric policy evaluation</title>
		<author>
			<persName><forename type="first">J</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><forename type="middle">J</forename><surname>Heckman</surname></persName>
		</author>
		<author>
			<persName><surname>Vytlacil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Handbook of Econometrics</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="4779" to="4874" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Bayesian nonparametric modeling for causal inference</title>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Hill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational and Graphical Statistics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="217" to="240" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Kernel methods in machine learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1171" to="1220" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Statistics and causal inference</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Holland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">396</biblScope>
			<biblScope unit="page" from="945" to="960" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A generalization of sampling without replacement from a finite universe</title>
		<author>
			<persName><forename type="first">D</forename><surname>Horvitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">260</biblScope>
			<biblScope unit="page" from="663" to="685" />
			<date type="published" when="1952">1952</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Nonparametric estimation of average treatment effects under exogeneity: A review</title>
		<author>
			<persName><forename type="first">Guido</forename><surname>Imbens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Review of Economics and Statistics</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4" to="29" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Causal Inference for Statistics, Social, and Biomedical Sciences: An Introduction</title>
		<author>
			<persName><forename type="first">Guido</forename><forename type="middle">W</forename><surname>Imbens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>Cambridge University Press</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Interpretable distribution features with maximum testing power</title>
		<author>
			<persName><forename type="first">Wittawat</forename><surname>Jitkrittum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zoltán</forename><surname>Szabó</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kacper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Chwialkowski</surname></persName>
		</author>
		<author>
			<persName><surname>Gretton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="181" to="189" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning representations for counterfactual inference</title>
		<author>
			<persName><forename type="first">Fredrik</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uri</forename><surname>Shalit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on Machine Learning</title>
		<meeting>the 33rd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3020" to="3029" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Kernel recursive ABC: Point estimation with intractable likelihood</title>
		<author>
			<persName><forename type="first">Takafumi</forename><surname>Kajihara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Motonobu</forename><surname>Kanagawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keisuke</forename><surname>Yamazaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenji</forename><surname>Fukumizu</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Machine Learning</title>
		<meeting>the 35th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="2400" to="2409" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A framework for optimal matching for causal inference</title>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Kallus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the 20th International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="372" to="381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Policy evaluation and optimization with continuous treatments</title>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Kallus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angela</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics (AISTATS)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1243" to="1251" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Filtering with state-observation examples via kernel monte carlo filter</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kanagawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Nishiyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fukumizu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="382" to="444" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Sequential kernel herding: Frank-wolfe optimization for particle filtering</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lacoste-Julien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Lindsten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. AISTATS 2015</title>
		<meeting>AISTATS 2015</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Exploration scavenging</title>
		<author>
			<persName><forename type="first">John</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Strehl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Wortman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th International Conference on Machine Learning</title>
		<meeting>the 25th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="528" to="535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">MMD GAN: Towards deeper understanding of moment matching network</title>
		<author>
			<persName><forename type="first">Chun-Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Cheng</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barnabás</forename><surname>Póczos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2203" to="2213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Generative moment matching networks</title>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rich</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1718" to="1727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Towards a learning theory of cause-effect inference</title>
		<author>
			<persName><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krikamol</forename><surname>Muandet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Tolstikhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning (ICML)</title>
		<meeting>the 32nd International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1452" to="1461" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Kernel mean embedding of distributions: A review and beyond</title>
		<author>
			<persName><forename type="first">Krikamol</forename><surname>Muandet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenji</forename><surname>Fukumizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bharath</forename><surname>Sriperumbudur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Machine Learning</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="1" to="141" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Kernel conditional moment test via maximum moment restriction</title>
		<author>
			<persName><forename type="first">Krikamol</forename><surname>Muandet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wittawat</forename><surname>Jitkrittum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Kübler</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the 36th Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="page" from="41" to="50" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Dual instrumental variable regression</title>
		<author>
			<persName><forename type="first">Krikamol</forename><surname>Muandet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arash</forename><surname>Mehrjou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Si</forename><forename type="middle">Kai</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anant</forename><surname>Raj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Sur les applications de la theorie des probabilites aux experiences agricoles: Essai des principes</title>
		<author>
			<persName><forename type="first">Jerzy</forename><surname>Neyman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistical Science</title>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Dabrowska</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Speed</surname></persName>
		</editor>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="463" to="472" />
			<date type="published" when="1923">1923</date>
		</imprint>
	</monogr>
	<note type="report_type">Master&apos;s thesis</note>
	<note>Excerpts reprinted in English. Translators.</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Model-based kernel sum rule: kernel bayesian inference with probabilistic models</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Nishiyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Motonobu</forename><surname>Kanagawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenji</forename><surname>Fukumizu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Eligibility traces for off-policy policy evaluation</title>
		<author>
			<persName><forename type="first">Doina</forename><surname>Precup</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Satinder</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th International Conference on Machine Learning</title>
		<meeting>the 17th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="759" to="766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<idno>LETOR 4.0 datasets. CoRR, abs/1306.2597</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Random features for large-scale kernel machines</title>
		<author>
			<persName><forename type="first">Ali</forename><surname>Rahimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 20</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1177" to="1184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Observational Studies</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Rosenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Springer Series in Statistics</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
	<note>2nd edition</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">The central role of the propensity score in observational studies for causal effects</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="55" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Nonparametric estimation of distributional policy effects</title>
		<author>
			<persName><forename type="first">C</forename><surname>Rothe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Econometrics</title>
		<imprint>
			<biblScope unit="volume">155</biblScope>
			<biblScope unit="page" from="56" to="70" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Estimating causal effects of treatments in randomized and nonrandomized studies</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Educational Psychology</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="688" to="701" />
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Causal inference using potential outcomes</title>
		<author>
			<persName><forename type="first">Donald</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="issue">469</biblScope>
			<biblScope unit="page" from="322" to="331" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Recommendations as treatments: Debiasing learning and evaluation</title>
		<author>
			<persName><forename type="first">Tobias</forename><surname>Schnabel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adith</forename><surname>Swaminathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashudeep</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navin</forename><surname>Chandak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 33rd International Conference on Machine Learning</title>
		<meeting>The 33rd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="1670" to="1679" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<monogr>
		<title level="m" type="main">Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond</title>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Smola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
			<publisher>MIT Press</publisher>
			<pubPlace>Cambridge, MA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<title level="m" type="main">Bounding and minimizing counterfactual error</title>
		<author>
			<persName><forename type="first">Uri</forename><surname>Shalit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fredrik</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.03976</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>Preprint</note>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Estimating individual treatment effect: generalization bounds and algorithms</title>
		<author>
			<persName><forename type="first">Uri</forename><surname>Shalit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fredrik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName><surname>Sontag</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="3076" to="3085" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Improving predictive inference under covariate shift by weighting the log-likelihood function</title>
		<author>
			<persName><forename type="first">Hidetoshi</forename><surname>Shimodaira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Planning and Inference</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="227" to="244" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Kernel distribution embeddings: Universal kernels, characteristic kernels and kernel metrics on distributions</title>
		<author>
			<persName><forename type="first">C.-J</forename><surname>Simon-Gabriel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">44</biblScope>
			<biblScope unit="page" from="1" to="29" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Kernel instrumental variable regression</title>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maneesh</forename><surname>Sahani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="4593" to="4605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Learning theory estimates via integral operators and their approximations</title>
		<author>
			<persName><forename type="first">S</forename><surname>Smale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Constructive Approximation</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="153" to="172" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">A Hilbert space embedding for distributions</title>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th International Conference on Algorithmic Learning Theory (ALT)</title>
		<meeting>the 18th International Conference on Algorithmic Learning Theory (ALT)</meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="13" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<monogr>
		<title level="m" type="main">Learning via Hilbert Space Embedding of Distributions</title>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
		<respStmt>
			<orgName>The University of Sydney</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Hilbert space embeddings of conditional distributions with applications to dynamical systems</title>
		<author>
			<persName><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenji</forename><surname>Fukumizu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th International Conference on Machine Learning (ICML)</title>
		<meeting>the 26th International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2009-06">June 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Kernel embeddings of conditional distributions: A unified kernel framework for nonparametric inference in graphical models</title>
		<author>
			<persName><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenji</forename><surname>Fukumizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="98" to="111" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Hilbert space embeddings and metrics on probability measures</title>
		<author>
			<persName><forename type="first">Bharath</forename><surname>Sriperumbudur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenji</forename><surname>Fukumizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gert</forename><surname>Lanckriet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="1517" to="1561" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Steinwart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Christmann</surname></persName>
		</author>
		<title level="m">Support Vector Machines</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Mercer&apos;s theorem on general domains: on the interaction between measures, kernels, and RKHS. Constructive Approximation</title>
		<author>
			<persName><forename type="first">I</forename><surname>Steinwart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Scovel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="363" to="417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Covariate shift adaptation by importance weighted cross validation</title>
		<author>
			<persName><forename type="first">Masashi</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Krauledat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Klaus-Robert</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="985" to="1005" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">Generative models and model criticism via optimized maximum mean discrepancy</title>
		<author>
			<persName><forename type="first">Dougal</forename><surname>Sutherland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hsiao-Yu</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heiko</forename><surname>Strathmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumyajit</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaditya</forename><surname>Ramdas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Counterfactual risk minimization: Learning from logged bandit feedback</title>
		<author>
			<persName><forename type="first">Adith</forename><surname>Swaminathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">JMLR Workshop and Conference Proceedings</title>
		<imprint>
			<publisher>JMLR.org</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="814" to="823" />
		</imprint>
	</monogr>
	<note>ICML</note>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Off-policy evaluation for slate recommendation</title>
		<author>
			<persName><forename type="first">Adith</forename><surname>Swaminathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Akshay</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alekh</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miro</forename><surname>Dudik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Langford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Damien</forename><surname>Jose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Imed</forename><surname>Zitouni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3632" to="3642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Minimax estimation of kernel mean embeddings</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Tolstikhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bharath</forename><surname>Sriperumbudur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krikamol</forename><surname>Muandet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">47</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title level="m" type="main">Introduction to Nonparametric Estimation</title>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Tsybakov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Springer Publishing Company</publisher>
		</imprint>
	</monogr>
	<note>Incorporated, 1st edition</note>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Off-policy evaluation and learning for external validity under a covariate shift</title>
		<author>
			<persName><forename type="first">Masatoshi</forename><surname>Uehara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masahiro</forename><surname>Kato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shota</forename><surname>Yasui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020">NeurIPS 2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Estimation and inference of heterogeneous treatment effects using random forests</title>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Wager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susan</forename><surname>Athey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="issue">523</biblScope>
			<biblScope unit="page" from="1228" to="1242" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Using the Nyström method to speed up kernel machines</title>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Seeger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 13</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="682" to="688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Kernel-based covariate functional balancing for observational studies</title>
		<author>
			<persName><forename type="first">K W</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kwun</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gary</forename><surname>Chuen</surname></persName>
		</author>
		<author>
			<persName><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="199" to="213" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">GANITE: Estimation of individualized treatment effects using generative adversarial nets</title>
		<author>
			<persName><forename type="first">Jinsung</forename><surname>Yoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Jordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihaela</forename><surname>Van Der Schaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Kernel-based conditional independence test and application in causal discovery</title>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the 27th Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="804" to="813" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
