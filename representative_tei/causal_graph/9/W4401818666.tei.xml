<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">TSLiNGAM: DirectLiNGAM under heavy tails</title>
				<funder ref="#_xCKJc6n">
					<orgName type="full">Fonds Wetenschappelijk onderzoek -Vlaanderen (FWO)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2023-08-11">August 11, 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Sarah</forename><surname>Leyder</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Mathematics</orgName>
								<orgName type="department" key="dep2">Jakob Raymaekers Department of Quantitative Economics</orgName>
								<orgName type="institution" key="instit1">University of Antwerp</orgName>
								<orgName type="institution" key="instit2">Maastricht University</orgName>
								<address>
									<addrLine>Tim Verdonck</addrLine>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Mathematics</orgName>
								<orgName type="department" key="dep2">imec Department of Mathematics</orgName>
								<orgName type="institution" key="instit1">University of Antwerp</orgName>
								<orgName type="institution" key="instit2">KU Leuven</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">TSLiNGAM: DirectLiNGAM under heavy tails</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-08-11">August 11, 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2308.05422v1[stat.ME]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:22+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Causal discovery</term>
					<term>Efficiency</term>
					<term>LiNGAM</term>
					<term>Structural causal models</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>One of the established approaches to causal discovery consists of combining directed acyclic graphs (DAGs) with structural causal models (SCMs) to describe the functional dependencies of effects on their causes. Possible identifiability of SCMs given data depends on assumptions made on the noise variables and the functional classes in the SCM. For instance, in the LiNGAM model, the functional class is restricted to linear functions and the disturbances have to be non-Gaussian.</p><p>In this work, we propose TSLiNGAM, a new method for identifying the DAG of a causal model based on observational data. TSLiNGAM builds on DirectLiNGAM, a popular algorithm which uses simple OLS regression for identifying causal directions between variables. TSLiNGAM leverages the non-Gaussianity assumption of the error terms in the LiNGAM model to obtain more efficient and robust estimation of the causal structure. TSLiNGAM is justified theoretically and is studied empirically in an extensive simulation study. It performs significantly better on heavy-tailed and skewed data and demonstrates a high small-sample efficiency. In addition, TSLiNGAM also shows better robustness properties as it is more resilient to contamination.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Over the last decades, statistics and machine learning have proven to be very strong tools for finding and modeling associations in data sets. However, in recent years, it has become clear that when analyzing and using data, causal relations are often more valuable than associations. As a result, there has been a growing interest in causal inference in statistics and machine learning, and it has become a crucial tool in many empirical sciences including medicine, social sciences, neuroinformatics and biology.</p><p>One of the established approaches to causal inference builds on the directed acyclic graph (DAG) framework, studied in great depth by <ref type="bibr" target="#b16">Pearl [2009]</ref>. DAGs represent the variables of interest as nodes in a graph where directed edges between nodes correspond with causal relations. As DAGs are acyclic, i.e. there are no cycles in the network, they represent a one-directional causal order of the variables, such that no variable with a later causal order can influence an earlier variable. DAGS are typically complemented by structural causal models (SCMs), which are used to describe the functional dependence of an effect on its causes in a DAG. This combined theoretical framework then bestows us with the necessary tools to compute observational, interventional and counterfactual distributions for answering all possible causal queries.</p><p>SCMs describe the causal relationships among random variables X 1 , . . . , X p as a set of equations X i = f i (P i , e i ), i = 1, . . . , p where the variable set P i ⊂ {X 1 , . . . , X p } denotes the parent variables of X i , i.e. the variables that determine X i , and e i represents the random noise variable disturbing X i . A central question in causal inference is whether the SCM can be identified from data. In other words, given enough data, can we find the DAG (and the functional dependencies)</p><p>underlying the data generating process? It is known that this is impossible in full generality.</p><p>However, under appropriate conditions on the behavior of the noise variables and the functional class of the f i , it is indeed possible to recover the SCM from observational data alone <ref type="bibr" target="#b18">[Peters et al., 2017]</ref>. This process is called causal discovery. It provides a very valuable addition to (randomized) controlled experiments, which are often difficult or impossible due to prohibitive costs or ethical objections.</p><p>One prime example of conditions under which identifiability is possible, is the LiNGAM model <ref type="bibr" target="#b24">[Shimizu et al., 2006]</ref>. In LiNGAM, the functional class is restricted to linear functions and the disturbances have to be non-Gaussian and mutually independent. Under these assumptions, identifiability is provable and given continuous data, the complete causal structure can be recovered. The above described model is known as the linear, non-Gaussian, acyclic model (LiNGAM) and the original discovery algorithm is based on independent component analysis (ICA), justified by the assumption of non-Gaussianity <ref type="bibr" target="#b24">[Shimizu et al., 2006]</ref>. Several extension to the LiNGAM model have been made. LvLiNGAM <ref type="bibr" target="#b9">[Hoyer et al., 2008]</ref> includes the presence of hidden variables or latent confounders by using overcomplete ICA, other methods resilient against latent confounders are ParceLiNGAM <ref type="bibr" target="#b30">[Tashiro et al., 2014]</ref> and MLCLiNGAM <ref type="bibr" target="#b3">[Chen et al., 2022]</ref>. <ref type="bibr" target="#b5">Dai et al. [2022]</ref> consider causal discovery of the LiNGAM model in the presence of measurement errors. <ref type="bibr" target="#b10">Hyvärinen et al. [2010]</ref> discuss the integration of LiNGAM in autoregressive models for time series.</p><p>In this work, we propose TSLiNGAM, a new algorithm for estimating the causal structure in a LiNGAM model. TSLiNGAM builds on DirectLiNGAM <ref type="bibr" target="#b25">[Shimizu et al., 2011]</ref>,</p><p>which is a popular method to obtain the causal LiNGAM model based on simple OLS regressions, but relies on regression estimators which are more efficient under heavy tails and skewness. These alternative regression estimators are more natural given the nongaussianity assumption in the LiNGAM model, and their appropriateness is further motivated theoretically and empirically.</p><p>The remainder of the article is organized as follows. Section 2 briefly reviews Di- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Method</head><p>We start by reviewing the LiNGAM model and the DirectLiNGAM algorithm, before introducing TSLiNGAM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Preliminaries</head><p>The LiNGAM structural causal model postulates that the functional dependencies are linear and the external influences are independent and non-Gaussian. More precisely, it relies on the following three assumptions:</p><p>1. The generating process can be described by a directed acyclic graph such that the variables {X 1 , . . . , X p } can be arranged in a causal order. The causal order of the variable X i is denoted by k(i).</p><p>2. Each variable is a linear combination of other variables with a lower causal order, plus an external influence:</p><formula xml:id="formula_0">X i = k(j)&lt;k(i) b ij X j + e i</formula><p>The coefficients b ij , called the connection strengths, can be arranged into a matrix B, which can be permuted to strict lower triangularity since the generating process concerns a DAG. The noise terms e i can be placed into a vector e. Hence we obtain the matrix notation:</p><formula xml:id="formula_1">X = BX + e (1)</formula><p>We call X i an exogenous variable if X i is equal to e i , so no variable X j has a directed path to X i . In the DAG framework, there is always at least one exogenous variable.</p><p>Non-exogenous variables are called endogenous variables.</p><p>3. The external influences e i are continuous random variables following a non-Gaussian distribution with zero mean and non-zero variance and all the e i for i ∈ {1, . . . , p} are independent of each other.</p><p>Equation (1) as:</p><formula xml:id="formula_2">X = Ae (2)</formula><p>where A = (I -B) -1 . Since the disturbance vector e contains mutually independent, non-Gaussian variables, Equation (2) corresponds to the well-known linear independent component analysis model (ICA). The matrix A is called the mixing matrix and efficient ICA-algorithms exist to estimate it for a given data set. Subsequently scaling and permutation steps can be performed to produce a strictly lower triangular matrix B, from which the corresponding causal order can then easily be derived. More information on the LiNGAM discovery algorithm can be found in the original LiNGAM paper <ref type="bibr" target="#b24">[Shimizu et al., 2006]</ref>.</p><p>The LiNGAM algorithm using ICA-estimation does, however, have some drawbacks.</p><p>First, the optimization used for ICA can get trapped in a local minimum and hence we have no guaranteed computational stability for the method. Second, for the gradient-based algorithm, appropriate parameters must be selected which is not easily done.</p><p>In 2011, a direct method was proposed to estimate causal ordering in the linear non-Gaussian context, namely DirectLiNGAM <ref type="bibr" target="#b25">[Shimizu et al., 2011]</ref>. In contrast to ICA-LiNGAM, this new method has guaranteed convergence and requires no parameter specification. DirectLINGAM uses two main ingredients. The first is OLS regression to remove the effect of an exogenous variable from the other variables. The second is an independence measure to identify the next exogenous variable. Denote with r (j)</p><formula xml:id="formula_3">i := X i - cov(X i ,X j )</formula><p>var(X j ) X j the ordinary least squares residual when X i is regressed on X j . Further denote the kernel-based estimator of mutual information <ref type="bibr" target="#b0">[Bach and Jordan, 2002]</ref> with M I kernel . For each variable X j we sum the mutual information of it with each of its ordinary least squares residuals r (j) i to obtain the kernel-based independence measure (KBI):</p><formula xml:id="formula_4">T kernel (X j , U ) = i∈U, i̸ =j M I kernel (X j , r (j) i ) (3)</formula><p>Here U is the set of indices of the remaining variables. The variable with the lowest T kernel is then the most independent and will be used as the next exogenous variable.</p><p>In summary, the DirectLiNGAM algorithm proceeds as follows:</p><p>Algorithm 1 DirectLiNGAM algorithm <ref type="bibr" target="#b25">[Shimizu et al., 2011]</ref> Require: n × p data set X U ← {1, . . . , p} ▷ Initialize the set of variable subscripts K ← ∅ ▷ Initialize an empty ordered list of variable subscripts while K contains less than p -1 indices do</p><formula xml:id="formula_5">for j ∈ U \ K do ▷ Cycle through the variables in U \ K for i ∈ U \ (K ∪ j) do ▷ Cycle through the variables in U \ (K ∪ j) R (j) •,i ← r (j) i</formula><p>▷ Store the OLS residuals of variable X i on X j end for</p><formula xml:id="formula_6">T j ← T kernel (X j , U \ K) end for m = argmin j∈U \K T j ▷ Find the next exogenous variable K ← {K, m} ▷ Append m to K X ← r (m) , X ← R (m)</formula><p>▷ Consider the residuals as new input end while K ← {K, (U \ K)} ▷ Append the final variable to obtain the complete causal ordering B ← OLS(X, K) ▷ perform OLS on X following the order in K</p><p>The algorithm above can be extended to make use of prior knowledge on the structure if this is available. For more details, we refer to the DirectLiNGAM paper <ref type="bibr" target="#b25">[Shimizu et al., 2011]</ref>. As is clear from the pseudo-code description in Algorithm 1, DirectLiNGAM relies crucially on least squares regression. In addition, the proofs for the identification of the LiNGAM structure by DirectLiNGAM also rely on the use of the least squares estimator <ref type="bibr" target="#b25">[Shimizu et al., 2011]</ref>.</p><p>The DirectLiNGAM algorithm as introduced so far only identifies the causal ordering and returns a fully connected DAG, which is the focus of this paper. In order to drop redundant edges, it can be followed by a sparse regression estimator, for which the adaptive lasso <ref type="bibr" target="#b34">[Zou, 2006]</ref> was used by <ref type="bibr" target="#b25">Shimizu et al. [2011]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">TSLiNGAM</head><p>The reliance of DirectLiNGAM on OLS regression is counterintuitive. OLS is known to perform extremely well under independent Gaussian errors, but loses its superiority when the errors are skewed, heavy tailed or heteroscedastic, especially when data samples are small <ref type="bibr" target="#b33">[Wilcox, 1998]</ref>. Given that the LiNGAM model assumes non-Gaussianity of the error terms, OLS is potentially a weak point of the algorithm.</p><p>In order to study this hypothesis, we propose the use of a different slope estimator to identify exogenous variables, namely the Theil-Sen regression estimator. This is motivated by its favorable properties on heavy-tailed and skewed distributions. Theil-Sen regression was first introduced by <ref type="bibr" target="#b31">Theil [1950]</ref> and later extended by <ref type="bibr" target="#b23">Sen [1968]</ref>. It is defined as follows:</p><p>Definition 1 (Theil-Sen slope). For the linear regression of a random variable Y on X, Y = βX + e, the Theil-Sen slope estimator is defined as</p><formula xml:id="formula_7">β = med i,j y j -y i x j -x i , for x j ̸ = x i (4)</formula><p>for data pairs {(x i , y i ) : i = 1, . . . , n}.</p><p>The Theil-Sen slope estimator is unbiased, regression equivariant, robust with a breakdown value of 0.293 and a bounded influence function <ref type="bibr" target="#b23">[Sen, 1968</ref><ref type="bibr" target="#b17">, Peng et al., 2008]</ref>. Compared to OLS, it has a high small-sample efficiency and it is super-efficient when combined with discontinuous or discrete errors <ref type="bibr" target="#b33">[Wilcox, 1998</ref><ref type="bibr" target="#b17">, Peng et al., 2008]</ref>. Also, when the errors are (close to) normal, Theil-Sen only loses little efficiency compared to OLS.</p><p>To apply the Theil-Sen slope in the DirectLiNGAM algorithm, we need to justify its use theoretically by generalizing the lemmas in <ref type="bibr" target="#b25">Shimizu et al. [2011]</ref>. For this, we need the functional form of the Theil-Sen given by</p><formula xml:id="formula_8">T (X, Y ) = med X,X ′ ,Y,Y ′ Y -Y ′ X -X ′ = F -1 (0.5),<label>(5)</label></formula><p>where Before proving the validity of the Theil-Sen slope in the LiNGAM model we need the additional concept of correlation-faithfulness <ref type="bibr">[Shimizu et al., 2009]</ref>:</p><formula xml:id="formula_9">F denotes the distribution of Y -Y ′ X-X ′ with Y d = Y ′ and X d = X ′ .</formula><p>Definition 3 (Correlation-faithfulness). The distribution of (X 1 , . . . , X p ) is said to be correlation-faithful to the underlying graph if and only if the (conditional) correlations of the X i 's are implicated by the graph structure.</p><p>Now suppose the data are realizations of a p-variate random vector (X 1 , . . . , X p ) ∼ F p .</p><p>Lemma 1 then states that the Theil-Sen slope can successfully identify exogenous variables and generalizes Lemma 1 of <ref type="bibr" target="#b25">Shimizu et al. [2011]</ref>.</p><p>Lemma 1 (Generalization of Lemma 1 of <ref type="bibr" target="#b25">Shimizu et al. [2011]</ref>). Suppose that the random variables X 1 , . . . , X p strictly follow the LiNGAM assumptions and that their distribution is correlation-faithful. We consider slope estimators as functionals T acting on bivariate distributions (X, Y ) ∼ F 2 . Assume that following properties hold for these slope functionals</p><formula xml:id="formula_10">T (F 2 ) = T (X, Y ) when Y is regressed on X: 1. T is regression equivariant: ∀γ ∈ R : T (X, Y + Xγ) = T (X, Y ) + γ 2. If Xand Y are independent: T (F 2 ) = T (X, Y ) = 0 (6) 3. T (X, Y ) ̸ = 0 =⇒ T (Y, X) ̸ = 0</formula><p>Define the residual when X i is regressed on X j using slope functional T as the following random variable: r</p><formula xml:id="formula_11">(j) i := X i -T (X j , X i )X j (i ̸ = j). Then the variable X j is exogenous if and only if X j is independent of r (j)</formula><p>i for all i ̸ = j. In particular, this holds for the Theil-Sen slope.</p><p>Next, Lemma 2 shows that the LiNGAM model holds on the residuals after an exogenous variable is regressed out using the Theil-Sen slope.</p><p>Lemma 2 (Generalization of Lemma 2 of <ref type="bibr" target="#b25">Shimizu et al. [2011]</ref>). Suppose that the random variables X = (X 1 , . . . , X p ) T strictly follow the LiNGAM assumptions and that their distribution is correlation-faithful. Assume that the variable X j is exogenous and denote by r (j)   the (p -1)-dimensional vector holding all the residuals when the X i , i ̸ = j, are regressed on X j using the a slope estimator satisfying the properties in (6). Then a LiNGAM holds for the residual random variables r (j) : r (j) = B (j) r (j) + e (j) or r (j) = A (j) e (j) . Moreover, the causal order is preserved:</p><formula xml:id="formula_12">k r (j) (l) &lt; k r (j) (m) ⇐⇒ k(l) &lt; k(m).</formula><p>We conclude that using the Theil-Sen slope is effective at identifying exogenous variables and regressing out their effect, and hence our new method correctly identifies the underlying causal model under the LiNGAM assumptions. We will refer with TSLiNGAM <ref type="bibr">(Theil-Sen LiNGAM)</ref> to the resulting algorithm which uses Theil-Sen regression for identifying exogenous variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Robustness</head><p>Theil-Sen regression is not only more efficient than OLS at heavy-tailed and skewed distributions, it is also more robust in the sense that it is more resilient against contamination in the data. As discussed, the Theil-Sen slope has a bounded influence function <ref type="bibr" target="#b8">[Hampel et al., 1986]</ref>, implying that the effect that a single outlying observation can have on the measure is limited. Furthermore, the considered slope has a breakdown value of 0.293, meaning that the slope is robust up to 29.3% contamination in the data <ref type="bibr" target="#b22">[Rousseeuw and Leroy, 2005]</ref>. In contrast, the OLS has an unbounded influence function and a breakdown value of 0%. To further explore the effect of the robustness-efficiency trade-off, we additionally consider the repeated median, defined by <ref type="bibr" target="#b26">Siegel [1982]</ref>, for the identification of the exogenous variables. The repeated median is defined as: Definition 4 (Repeated median slope). For the linear regression of a random variable Y on X, Y = βX + e, the repeated median slope estimator is defined as</p><formula xml:id="formula_13">β = med i med j̸ =i y i -y j x i -x j , for x j ̸ = x i (7)</formula><p>for data pairs {(x i , y i ) : i = 1, . . . , n}.</p><p>The functional form of the repeated median is given by:</p><formula xml:id="formula_14">T (X, Y ) = med X,Y med X ′ ,Y ′ Y -Y ′ X -X ′ (8) with Y d = Y ′ , X d = X ′</formula><p>Just as the Theil-Sen slope, the repeated median is unbiased and regression equivariant.</p><p>However, the repeated median is more robust, with a bounded influence function and a breakdown value of 0.5. In order to use the repeated median for the identification of exogenous variables, we need to verify its plug-in theoretically. As the Fisher consistency of the repeated median has only been proven for symmetric errors, see <ref type="bibr" target="#b26">Siegel [1982]</ref>, we proof this property for general error distributions.</p><p>Theorem 2 (Fisher consistency of the repeated median). For a simple linear regression model Y = βX + ε such that X and ε are independent, continuous random variables, the repeated median slope estimator is Fisher consistent.</p><p>As the repeated median is regression equivariant, Fisher consistent and makes use of medians, Lemma 1 and 2 also hold for this slope estimator. Hence it can thus also be used to identify exogenous variables in a LiNGAM structure.</p><p>We now illustrate the effect that a single outlier can have on LiNGAM methods in the following robustness experiment. The objective is to estimate the causal order of a simple two node DAG in the LiNGAM family:</p><formula xml:id="formula_15">     X 1 = e 1 X 2 = X 1 + e 2</formula><p>Here e 1 and e 2 are distributed according to a Student-t distribution with 5 degrees of freedom. We generate n = 500 observations of this bivariate causal model and replace one observation by an outlier of value (±2 i , ±2 j ) for i, j ∈ {0, 1, 2, . . . , 10}. The causal direction is then estimated from the contaminated data using the original ICA-LiNGAM algorithm from the pcalg package <ref type="bibr" target="#b12">[Kalisch, 2022]</ref>, the DirectLiNGAM algorithm and our adapted versions using Theil-Sen and the repeated median. We iterate this process 100 times to get representative results. The outcome is shown in Figure <ref type="figure" target="#fig_2">1</ref>. For ICA-LiNGAM and DirectLiNGAM, we observe that a single outlying observation has the ability to distort the discovery of the causal order, even in the simplest of causal models. Both plots show different regions for the outlier such that the obtained causal order is the inverse of the true causal direction. In contrast, when Theil-Sen (TSLiNGAM) or the repeated median are used to identify the exogenous variables, we notice that the causal order is always correctly estimated, regardless of the values of the added outlier.  According to Figure <ref type="figure" target="#fig_2">1</ref>, using the repeated median to identify the exogenous variables seems promising. As discussed, it is a robust slope estimator, but at a cost of lower efficiency. Notwithstanding, the repeated median is still more efficient than OLS for various error distributions, e.g. skewed, heavy-tailed or discrete distributions. Therefore from now on, we will also consider the repeated median as an alternative slope estimator and we will compare its use to TSLiNGAM in an extensive simulation study in Section 3.</p><formula xml:id="formula_16">2 -10 2 -8 2 -6 2 -4 2 -2 2 0 2 2 2 4 2 6 2 8 2 10 2 -10 2 -8 2 -6 2 -4 2 -</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Computational considerations</head><p>Finally, we discuss the computational cost of DirectLiNGAM and the proposed TSLiNGAM.</p><p>As Theil-Sen regression and the repeated median can be computed in O(n log(n)) time <ref type="bibr" target="#b4">[Cole et al., 1989</ref><ref type="bibr" target="#b13">, Katz and Sharir, 1993</ref><ref type="bibr" target="#b28">, Stein and Werman, 1992</ref><ref type="bibr" target="#b15">, Matoušek et al., 1993]</ref>,</p><p>they do not add much to the computational cost when used instead of simple OLS (which requires O(n) time). Therefore, TSLiNGAM has a computational cost that is similar to that of DirectLiNGAM.</p><p>The computational costs of both algorithms is in fact dominated by the independence measure for each remaining variable per iteration in the algorithm. In the original paper <ref type="bibr" target="#b25">[Shimizu et al., 2011]</ref>, the independence measure used in Equation ( <ref type="formula">3</ref>) is the kernel-based estimator of mutual information defined by <ref type="bibr" target="#b0">Bach and Jordan [2002]</ref>. Although this measure performs well, its accumulated computational cost can become rather large, as lots of Gram matrices with Gaussian kernels have to be computed.</p><p>Therefore, as an alternative independence measure, we consider the use of the distance correlation (dcorr) between random variables <ref type="bibr" target="#b29">[Székely et al., 2007]</ref>. This measure, unlike</p><p>Pearson's correlation, is zero if and only if the variables are independent. One of the advantages is that it can be computed in O(nlog(n)) time <ref type="bibr" target="#b2">[Chaudhuri and Hu, 2019]</ref>. In contrast, the kernel-based independence measure computed on two variables has a computational complexity of O(nM 2 + M 3 ), where M (≪ n) is the maximal rank found by the low-rank approximations of the Gram matrices used in the algorithm for the independence measure. Therefore, only if M = O( log(n)), we obtain the same computational complexity as dcorr. Empirically, M often seems to grow substantially faster than this rate.</p><p>Hence, when the data sets are too large to use the kernel-based independence measure, it can be beneficial to use the distance correlation instead to speed up DirectLiNGAM and TSLiNGAM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Simulation</head><p>In this section we compare the proposed method with direct competitors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Setup and methods</head><p>We compare TSLiNGAM with the original DirectLiNGAM algorithm and with extremal ancestral search (EASE) <ref type="bibr" target="#b6">[Gnecco et al., 2021]</ref>. The latter method is designed for heavytailed data, and is thus a highly relevant competitor to TSLiNGAM. In addition, we also compare with three variations of the proposed algorithm. The first uses TS regression with dcorr as independence measure. The other two use RM regression paired with KBI and dcorr respectively.</p><p>The data is generated by the following procedure, inspired by the simulation setup of <ref type="bibr" target="#b6">Gnecco et al. [2021]</ref>:</p><p>1. We simulate data of dimension p = 2 with sample sizes n = {5, 10, 25, 50, 100}, of dimension p = 5 with sample sizes {n = 30, 50, 100, 200} and of dimension p = 10 with sample sizes {n = 50, 100, 200, 300}.</p><p>2. We generate a linear structural causal model X = BX + e with X = (X 1 , . . . , X p ) T , e = (e 1 , . . . , e p ) T and B ∈ R p×p as follows:</p><p>(a) First, we generate a random causal order between the variables X 1 , . . . , X p as a permutation π of {1, . . . , p}.</p><p>(b) Per variable X i with π(i) &gt; 1, the number of parents is distributed as Bin(π(i)-1, q) with q = {1, 0.6, 0.5} respectively for dimension p = {2, 5, 10}.</p><p>(c) Next, we select those parents randomly from the variables with a lower causal order, such that cycles are ruled out.</p><p>(d) Then, we sample the connection strengths B ij per variable per parent uniformly from [-0.9, -0.1] ∪ [0.1, 0.9]. This yields the matrix B.</p><p>(e) Finally, we sample the noise variables randomly from following distributions:</p><p>Student-t with 1, 2 or 5 degrees of freedom, a centered lognormal distribution, a centered Pareto distribution and a centered exponential distribution. Combining</p><p>B and e, we obtain X.</p><p>For each setting, we generate 1000 data sets. To compare performance among the different methods, we count the number of times the algorithm returns the correct causal order. All implementations are done in R. For the Theil-Sen slope and the repeated median we use the corresponding functions from the robslopes package <ref type="bibr" target="#b19">[Raymaekers, 2022</ref><ref type="bibr" target="#b21">[Raymaekers, , 2023]]</ref>. For the distance correlation we use the dccpp package <ref type="bibr" target="#b1">Berrisch [2022]</ref> and for EASE we use the implementation in the causalXtreme package <ref type="bibr" target="#b6">[Gnecco, 2021]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Results</head><p>We discuss the results for p = 10 variables here. The results for p = 2 and p = 5 are qualitatively similar and can be found in Section B of the Appendix. The results for p = 10 are presented in Table <ref type="table" target="#tab_1">1</ref>.</p><p>We observe that in almost every scenario the proposed TSLiNGAM achieves the best result. TSLiNGAM strongly outperforms DirectLiNGAM when the data is heavy-tailed.</p><p>For skewed distributions, TSLiNGAM also performs better than DirectLiNGAM, although the difference is somewhat smaller. As the distribution moves closer to normality, such as for the t 5 distribution, DirectLiNGAM becomes the preferred method. However, note that the difference in performance is almost negligible and perhaps more importantly, the absolute performance is very poor. This is explained by the fact that the identifiability of the LiNGAM structure is lost when there are Gaussian errors, and as we move closer to that scenario, it becomes increasingly difficult to identify the underlying structure. EASE does not perform well here. This is probably explained by the fact that EASE only looks at the tails and therefore needs bigger sample sizes in order to perform well. Finally, we consider the variations of the TSLiNGAM algorithm. The repeated median performs well on heavytailed distributions, but does not offer an improvement over TSLiNGAM. Using dcorr as independence measure becomes a viable strategy when the sample size is reasonably large.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Computation time</head><p>In addition to the results on the recovery of the underlying LiNGAM structure, we study the computation times of the methods. We discuss the computation times for the simulation study with p = 10 and for the t 5 distribution here. The other distributions had similar computational costs. The computation times for p = 2 and p = 5 are qualitatively similar and can be found in Section B of the Appendix.</p><p>Table <ref type="table">2</ref> presents the computation times for p = 10 and the t 5 distribution. The first thing to note is that TSLiNGAM and its variants have essentially the same computational cost as DirectLiNGAM. This is explained by the fact that the computation time of both algorithms is dominated by the kernel-based independence measure. As a result, when using dcorr as a measure of independence, we see a substantial speedup of about one order of magnitude. This suggests that dcorr is useful when the sample size gets larger, which is precisely the scenario in which its performance is also competitive with TSLiNGAM.</p><p>Finally, we note that EASE is by far the fastest method here. However, as pointed out before, it is not competitive in these relatively small-sample scenarios. chronic conditions and a person's age are causes of the number of hospital stays. Additionally, years of schooling should have an impact on a persons income. In contrast, the causal order found by TSLiNGAM is <ref type="bibr">(age, school, income, chronic, visits, hospital)</ref>. This order is very logical and corresponds with our intuition. Furthermore, the causal graph found by TSLiNGAM consists of edges that match our understanding of the variables. The better result obtained by TSLiNGAM can be explained by studying the underlying variables of the data set. We know that TSLiNGAM tends to outperform DirectLiNGAM on heavy-tailed and skewed data, and it turns out that the variables visits, hospital and income are leptokurtic and have very fat tails, see for instance the boxplots in Figure <ref type="figure" target="#fig_4">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hospital</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">GAGurine data</head><p>The second data set considered in this work is the GAGurine data from the package MASS in R <ref type="bibr" target="#b32">[Venables and Ripley, 2002]</ref>. This data contains the concentration of the chemical GAG in the urine of 314 children between 0 and 17 years old, where it is known that age is a dominant cause of the concentration of GAG. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GAG Age</head><p>Figure <ref type="figure">4</ref>: Ground truth: age is a cause of GAG concentration in urine.</p><p>On this data set, both DirectLiNGAM and TSLiNGAM succeed in discovering the right causal order. However, as we would like to demonstrate the small-sample efficiency of TSLiNGAM, we sample 1000 data subsets of sizes {5, 10, 15, . . . , 50} from the original data. The number of times TSLiNGAM and DirectLiNGAM find the right causal order on these subsamples are shown in Figure <ref type="figure" target="#fig_5">5</ref>. Overall we observe that TSLiNGAM has a 10% higher small-sample efficiency compared to DirectLiNGAM. For example, for sample size n = 45, TSLiNGAM finds the right causal order 762 times, while DirectLiNGAM only succeeds 657 times. This increase in efficiency can be explained by observing that the distribution of the variable GAG is right-skewed and tailed, a scenario where the Theil-Sen slope is better suited than OLS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">FMRI data</head><p>As a third data set, we study the functional magnetic resonance imaging (FMRI) data simulated in <ref type="bibr" target="#b27">Smith et al. [2011]</ref>. This data was previously studied within the LiNGAM framework <ref type="bibr" target="#b27">[Smith et al., 2011</ref><ref type="bibr" target="#b11">, Hyvärinen and Smith, 2013</ref><ref type="bibr" target="#b30">, Tashiro et al., 2014]</ref>, however we now use the FMRI data in a robustness context. We take the first simulation data set from the paper which contains 10.000 continuous observations of 5 variables and has a causal structure as demonstrated in Figure <ref type="figure" target="#fig_6">6</ref>. If we run DirectLiNGAM or TSLiNGAM on the original data set, both methods succeed in discovering the correct causal order. However, to demonstrate the robustness of TSLiNGAM, we artificially contaminate 20 observations by changing the value of the first variable to a number generated from a Student-t distribution with 3 degrees of freedom, centered around 25. We then run both algorithms 100 times again and observe that Di-rectLiNGAM never finds the right causal order. TSLiNGAM, in contrast, recovers the ground truth 72 times and is therefore much less influenced by the 20 outlying observations. This shows that TSLiNGAM using the Theil-Sen slope instead of OLS is more resilient towards contamination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Sociological data</head><p>As a final real data example, we try to replicate the example performed in the original DirectLiNGAM paper on sociology data. The data are publicly available at the General Social Survey (GSS: <ref type="url" target="https://gssdataexplorer.norc.org/gssdata">https://gssdataexplorer.norc.org/gss data</ref>). We study the variables father's occupation level, son's income, father's education, son's occupation level, son's education and number of siblings. We take the same subset of the data as studied by <ref type="bibr" target="#b25">Shimizu et al. [2011]</ref>: non-farm background, ages 35 to 44, white, male, in the labor force at the time of the survey and years 1972 to 2006, see  Domain knowledge on the causal relations between these variables suggests the causal structure shown in Figure <ref type="figure" target="#fig_7">7</ref> [ <ref type="bibr" target="#b25">Shimizu et al., 2011]</ref>. On this data we then run the DirectLiNGAM algorithm and our TSLiNGAM. To prune redundant edges in the resulting adjacency matrices B, we again perform Adaptive Lasso.</p><p>This yields the directed acyclic graphs shown in Figures <ref type="figure">8</ref> and<ref type="figure">9</ref>. The two discovered DAGs are fairly good. DirectLiNGAM finds 5 correct edges, 2 wrongly directed edges and 2 redundant edges. TSLiNGAM finds 4 correct edges, 1 wrongly directed edge and no redundant edges. Overall, both methods perform equally well on the sociological data.</p><p>Additionally we remark that the Theil-Sen estimator combined with the distance correlation gave a better outcome, see the resulting DAG in Figure <ref type="figure" target="#fig_10">10</ref> of the Appendix. This combination of slope estimator and independence measure yields 6 correct edges, 1 wrongly directed edge and 2 redundant edges, the best result yet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we proposed TSLiNGAM, which builds on the popular DirectLiNGAM algorithm for causal discovery in LiNGAM structures. We proved that TSLiNGAM recovers the Theil-Sen slope. By leveraging the attractive properties of the Theil-Sen slope estimator, we obtain improved recovery under heavy tailed and skewed data models, without sacrificing performance in models with symmetric distributions which are close to normal. This improved performance was illustrated in an extensive simulation study. Furthermore, we suggested considering a different independence measure in the algorithm. More precisely, using the distance correlation instead of the original kernel-based independence measure reduces the overall computational cost of the method without sacrificing performance, pro-vided the data set is large enough.</p><p>We additionally illustrated the TSLiNGAM on four real data sets. These applications confirm better performance under skewness and heavy tails, an improved small-sample efficiency and increased robustness to outliers compared to the original DirectLiNGAM algorithm.</p><p>In summary, we conclude that the newly developed method can be considered a premium alternative to DirectLiNGAM. TSLiNGAM performs significantly better on heavy-tailed data and discovers the right causal order on smaller sample sizes, without increasing the computational cost. In addition, TSLiNGAM also showed better robustness properties as it is more resilient to contamination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Proofs</head><p>A.1 Proof of Lemma 1</p><p>Proof.</p><p>1) Assume X j is exogenous:</p><p>If X j is exogenous we have X j = e j . From X = Ae we have X i = a ij X j + h̸ =j a ih e h (i ̸ = j).</p><p>Here X j is independent of h̸ =j a ih e h since X j = e j and all the e i are mutually independent.</p><p>Then:</p><formula xml:id="formula_17">r (j) i = X i -T (X j , X i )X j = a ij X j + h̸ =j a ih e h -T (X j , a ij X j + h̸ =j a ih e h )X j = a ij X j + h̸ =j a ih e h -(a ij + T (X j , h̸ =j a ih e h ))X j = a ij X j + h̸ =j a ih e h -(a ij + 0)X j = h̸ =j a ih e h</formula><p>where in the second equality we substituted X i , in the third equality we used regression equivariance and in the fourth equality we used the independence property. We obtain that X j is independent of r (j) i for all i ̸ = j as X j is independent of h̸ =j a ih e h .</p><p>2) Assume X j is not exogenous:</p><p>If X j is endogenous, then there exists an exogenous variable X h = e h such that X h has a directed path to X j :</p><formula xml:id="formula_18">r (j) h = X h -T (X j , X h )X j = e h -T (X j , X h )   k(t)&lt;k(j) a jt e t + e j   = (1 -T (X j , X h )a jh )e h -T (X j , X h ) k(t)&lt;k(j),t̸ =h</formula><p>a jt e t -T (X j , X h )e j and</p><formula xml:id="formula_19">X j = k(t)&lt;k(j)</formula><p>a jt e t + e j = k(t)&lt;k(j),t̸ =h a jt e t + a jh e h + e j</p><p>If we now proof that T (X j , X h ) is nonzero, then the Darmois-Skitovitch theorem gives us that r (j)</p><p>h and X j are dependent as all the e k are independent and non-Gaussian. For this, we proceed as follows. First we have that T (X h , X j ) = T (e h , k(t)&lt;k(j),t̸ =h a jt e t + a jh e h + e j ) = a jh using independence and regression equivariance. Second one has that cov(X h , X j ) = cov(e h , k(t)&lt;k(j),t̸ =h a jt e t + a jh e h + e j ) = a jh var(e h ) =⇒ a jh = cov(X h , X j )/var(e h ) as var(e h ) &gt; 0 Here cov(X h , X j ) cannot be zero under the correlation-faithfulness assumption as we have a directed path connecting them. Hence T (X h , X j ) = a jh is nonzero. If we now use the third assumption in (6) we have that T (X j , X h ) is also nonzero and we are done.</p><p>For the Theil-Sen slope assumptions 1 and 2 from (6) are immediately satisfied by using the stronger property of Fisher consistency. For the third assumption we proceed as follows. First we note that the Theil-Sen slope uses medians to estimate the regression slope between variables Y and X. For these we have that the median of Y -Y ′ X-X ′ is nonzero if and only if the median of X-X ′ Y -Y ′ is nonzero. To see this: suppose without loss of generality that the median of the slopes Y -Y ′ X-X ′ is larger than zero. Then, as all slopes larger than zero yield an inverse slope larger than zero, and likewise all slopes smaller than zero yield an inverse slope smaller than zero, the median of X-X ′ Y -Y ′ is also larger than zero. Hence the median preserves the sign when swapping numerator and denominator and as we assume continuous variables, division by zero when swapping occurs with a negligible probability of zero. Hence T (X, Y ) ̸ = 0 =⇒ T (Y, X) ̸ = 0 and we are done.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Proof of Lemma 2</head><p>Proof.</p><p>Assume that the mixing matrix A in X = Ae has already been permuted to lower triangularity with ones on the diagonal and assume without loss of generality that X j = X 1 = e 1 .</p><p>Since X 1 is exogenous, we have that the a i1 for i ̸ = 1 are the slope coefficients when X i is regressed on X 1 using T :</p><formula xml:id="formula_20">T (X 1 , X i ) = T (e 1 , t≤i a it e t ) = a i1 + T (e 1 , t≤i,t̸ =1 a it e t ) = a i1 + 0 = a i1</formula><p>where we used regression equivariance in the second equality and independence in the third equality. Hence when we remove the effect of X 1 from X i by switching to the residuals</p><formula xml:id="formula_21">r (1) i = X i -T (X 1 , X i )X 1 = X i -a i1 e 1 ,</formula><p>the first column of A becomes a zero vector. As r (1) is independent of X 1 , we get for r (1) a new (p-1)×(p-1) dimensional lower triangular matrix A (1) = [A] 2:p,2:p with ones on the diagonal: r (1) = A (1) e (1) with e (1) = [e] 2:p . Therefore a LiNGAM holds for the residual vector r (1) . Also, when switching to r (1) , the corresponding matrix A (1) is the lower triangular submatrix formed by removing the first row and the first column of A. Hereby the causal order is not altered and hence switching to the residuals preserves the causal order.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Proof of Theorem 1</head><p>Proof.</p><p>We have that:</p><formula xml:id="formula_22">Y -Y ′ = βX + ε -βX ′ -ε ′ = β(X -X ′ ) + ε -ε ′ , with ε d = ε ′ =⇒ Y -Y ′ X -X ′ = β + ε -ε ′ X -X ′</formula><p>Hence the Theil-Sen slope is Fisher-consistent:</p><formula xml:id="formula_23">⇐⇒ med ε -ε ′ X -X ′ = 0 ⇐⇒ P ε -ε ′ X -X ′ ≤ 0 = 0.5 Now for ε-ε ′ X-X ′ holds that ε -ε ′ and X -X ′ are symmetric about zero. Therefore ε-ε ′ X-X ′</formula><p>is also symmetric about zero as numerator and denominator are independent and symmetric about zero, and thus we obtain that P ε-ε ′ X-X ′ ≤ 0 = 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Proof of Theorem 2</head><p>Proof.</p><p>For</p><formula xml:id="formula_24">X d = X ′ , ε d = ε ′ independent, continuous random variables, we have that Y -Y ′ = βX + ε -βX ′ -ε ′ = β(X -X ′ ) + ε -ε ′ =⇒ Y -Y ′ X -X ′ = β + ε -ε ′ X -X ′</formula><p>Hence, for Fisher consistency of the repeated median, it is needed that: med</p><formula xml:id="formula_25">X,ε med X ′ ,ε ′ ε -ε ′ X -X ′ = 0 ⇐⇒ P X,ε med X ′ ,ε ′ ε -ε ′ X -X ′ ≤ 0 = 0.5</formula><p>⇐⇒ P X,ε x, e : med</p><formula xml:id="formula_26">X ′ ,ε ′ e -ε ′</formula><p>x -X ′ ≤ 0 = 0.5</p><p>We compute: med</p><formula xml:id="formula_27">X ′ ,ε ′ e -ε ′ x -X ′ ≤ 0 ⇐⇒ P X ′ ,ε ′ e -ε ′</formula><p>x -X ′ ≤ 0 ≥ 0.5 ⇐⇒ P(e -ε ′ ≤ 0 ∩ x -X ′ ≥ 0) + P(e -ε ′ ≥ 0 ∩ x -X ′ ≤ 0) ≥ 0.5 ⇐⇒ P(e -ε ′ ≤ 0) • P(x -X ′ ≥ 0) + P(e -ε ′ ≥ 0) • P(x -X ′ ≤ 0) ≥ 0.5 [independence]</p><p>⇐⇒ (1 -F ε (e)) • F X (x) + F ε (e) • (1 -F X (x)) ≥ 0.5 <ref type="bibr">[continuous r.v.]</ref> This implies: P X,ε x, e : med</p><formula xml:id="formula_28">X ′ ,ε ′ e -ε ′</formula><p>x -X ′ ≤ 0 = P X,ε x, e : (1 -F ε (e)) • F X (x) + F ε (e) • (1 -F X (x)) ≥ 0.5 </p><formula xml:id="formula_29">= P (1 -F ε (ε)) • F X (X) + F ε (ε) • (1 -F X<label>(</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Additional simulation results</head><p>Table <ref type="table" target="#tab_4">4</ref> and 5 present the simulation results for p = 2 and p = 5 variables respectively. It is clear that TSLiNGAM performs best overall. In particular, it outperforms DirectLiNGAM on heavy-tailed and skewed distributions, and the outperformance is more pronounced as the tails gets heavier. For lighter tails, the performance becomes similar to DirectLiNGAM.</p><p>Using the repeated median works well, but provides no improvement over TSLiNGAM. The dcorr independence measure performs somewhat worse than the kernel-based independence measure, but becomes competitive at larger sample sizes. EASE is not doing very well, and needs larger sample sizes to become competitive.</p><p>The computation times for the simulation with p = 2 and p = 5 variables with t 5 distributions are given in Tables <ref type="table">6</ref> and<ref type="table">7</ref>  DirectLiNGAM have similar computational costs. Using dcorr as independence measure decreases the computational costs by a factor of roughly 3 for p = 2 and a factor of 10 for p = 5. EASE is again by far the fastest method. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>rectLiNGAM and introduces TSLiNGAM. It also discusses the theoretical and computational properties of TSLiNGAM. Section 3 then demonstrates the advantage of TSLiNGAM over DirectLiNGAM in an extensive simulation study. Lastly, in Section 4, we compare TSLiNGAM to DirectLiNGAM on four real data examples. Finally, Section 5 concludes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Using this form, we first proof the Fisher Consistency of the Theil-Sen estimator, a property we will need later for generalizing the lemmas. It is defined as follows Definition 2 (Fisher consistency). The functional T estimating a parameter Θ is Fisher consistent for a distribution F if, when calculating the functional on the whole population, it equals the estimated parameter: for the distribution F : T (F ) = Θ It turns out that the Theil-Sen estimator is indeed Fisher consistent as shown in the result below Theorem 1 (Fisher consistency of Theil-Sen slope). For a simple linear regression model Y = βX +ε such that X and ε are independent, continuous random variables, the Theil-Sen slope is a Fisher consistent slope estimator.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Number of correctly estimated causal orders out of 100 runs per discovery method given the (x, y)-coordinate of added outlier.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Directed acyclic graphs found by DirectLiNGAM (left) and TSLiNGAM (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Boxplots of the heavy-tailed variables in the physician data set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Number of correctly found causal orders by TSLiNGAM and DirectLiNGAM for 1000 runs on subsamples of the GAGurine data with specified sample sizes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure6: The causal order of the FMRI data is(1,2,3,4,5).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>(Figure 7 :</head><label>7</label><figDesc>Figure 7: Ground truth based on domain knowledge: a directed arrow indicates a possible causal relation, a bidirected purple arrow signifies an unknown causal relation (there might be a relation, a latent confounder or nothing).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>(Figure 8 :FatherFigure 9 :</head><label>89</label><figDesc>Figure 8: Causal graph found by DirectLiNGAM with correct arrows in black, wrongly directed arrows in red, unverifiable arrows in purple and redundant arrows in blue.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>X)) ≥ 0.5 = P (1 -U ′ )U + U ′ (1 -U ) ≥ 0.5 with U := F X (X), U ′ := F ε (ε) uniform and independent (1 -U ′ )U + U ′ (1 -U ) ≥ 0.5 ⇐⇒ U ≥ 0.5 ∩ U ′ ≤ 0.5 or U ≤ 0.5 ∩ U ′ ≥ 0-Median is Fisher-consistent for continuous, random errors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>FatherFigure 10 :</head><label>10</label><figDesc>Figure 10: Causal graph found by Theil-Sen and distance correlation with correct arrows in black, wrongly directed arrows in red, unverifiable arrows in purple and redundant arrows in blue.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Number of correct causal orders out of 1000 trials for p = 10.</figDesc><table><row><cell></cell><cell></cell><cell>t 1</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Pareto</cell></row><row><cell cols="8">TSLiNGAM 477 806 942 984 539 892 983</cell></row><row><cell cols="8">DirectLiNGAM 286 432 555 640 368 552 733</cell></row><row><cell>EASE</cell><cell cols="4">21 124 307 450</cell><cell>3</cell><cell>10</cell><cell>17</cell></row><row><cell cols="8">Repeated Median &amp; KBI 379 733 915 969 365 759 961</cell></row><row><cell cols="8">Theil-Sen &amp; dcorr 232 530 747 841 361 740 942</cell></row><row><cell cols="8">Repeated Median &amp; dcorr 152 425 657 791 178 548 877</cell></row><row><cell></cell><cell></cell><cell>t 2</cell><cell></cell><cell></cell><cell></cell><cell cols="2">lognormal</cell></row><row><cell cols="8">TSLiNGAM 167 387 601 711 457 798 959</cell></row><row><cell cols="8">DirectLiNGAM 149 333 539 642 351 649 859</cell></row><row><cell>EASE</cell><cell>3</cell><cell>13</cell><cell>52</cell><cell>81</cell><cell>0</cell><cell>3</cell><cell>8</cell></row><row><cell cols="8">Repeated Median &amp; KBI 128 348 592 663 309 675 919</cell></row><row><cell>Theil-Sen &amp; dcorr</cell><cell cols="7">38 189 519 733 273 703 932</cell></row><row><cell>Repeated Median &amp; dcorr</cell><cell cols="7">21 133 427 651 156 514 848</cell></row><row><cell></cell><cell></cell><cell>t 5</cell><cell></cell><cell></cell><cell></cell><cell cols="2">exponential</cell></row><row><cell>TSLiNGAM</cell><cell>17</cell><cell>35</cell><cell cols="5">88 151 236 601 858</cell></row><row><cell>DirectLiNGAM</cell><cell>16</cell><cell>37</cell><cell cols="5">90 158 245 575 812</cell></row><row><cell>EASE</cell><cell>0</cell><cell>1</cell><cell>5</cell><cell>1</cell><cell>0</cell><cell>0</cell><cell>1</cell></row><row><cell>Repeated Median &amp; KBI</cell><cell>11</cell><cell>34</cell><cell cols="5">72 132 155 482 818</cell></row><row><cell>Theil-Sen &amp; dcorr</cell><cell>2</cell><cell>1</cell><cell>7</cell><cell cols="4">11 140 542 867</cell></row><row><cell>Repeated Median &amp; dcorr</cell><cell>0</cell><cell>2</cell><cell>4</cell><cell>16</cell><cell cols="3">66 360 746</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Table 3 for details. After omitting observations which contain missing values, this results in a data set with 2117 observations.</figDesc><table><row><cell></cell><cell>studied variables</cell><cell>GSS codebook name</cell><cell>selection variables</cell><cell>GSS codebook name</cell></row><row><cell cols="2">X 1 Father's occupation level</cell><cell>PAPRES10</cell><cell>non-farm background</cell><cell>RES16</cell></row><row><cell>X 2</cell><cell>Son's income</cell><cell>REALRINC</cell><cell>age</cell><cell>AGE</cell></row><row><cell>X 3</cell><cell>Father's education</cell><cell>PAEDUC</cell><cell>white</cell><cell>RACE</cell></row><row><cell>X 4</cell><cell>Son's occupation level</cell><cell>PRESTG10</cell><cell>sex</cell><cell>SEX</cell></row><row><cell>X 5</cell><cell>Son's education</cell><cell>EDUC</cell><cell>in the labor force</cell><cell>WRKSTAT</cell></row><row><cell>X 6</cell><cell>Number of siblings</cell><cell>SIBS</cell><cell>year</cell><cell>YEAR</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Studied variables taken from the GSS repository and which variables we selected our sample on.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Number of correct causal orders out of 1000 trials for p = 2.</figDesc><table><row><cell>respectively. It is clear that TSLiNGAM and</cell></row></table></figure>
		</body>
		<back>

			<div type="funding">
<div><p>* First author is supported by <rs type="funder">Fonds Wetenschappelijk onderzoek -Vlaanderen (FWO)</rs> as a PhD fellow Fundamental Research (PhD fellowship <rs type="grantNumber">11K5523N</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_xCKJc6n">
					<idno type="grant-number">11K5523N</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Real data applications</head><p>In this section we illustrate the application of TSLiNGAM on four data sets from medical and social sciences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Physician data</head><p>As a first real data example, we consider data originating from the US National Medical Expenditure Survey conducted in 1987 and 1988. This data contains health-related information on 4406 individuals and can be found at <ref type="url" target="http://qed.econ.queensu.ca/jae/1997-v12.3/deb-trivedi/">http://qed.econ.queensu.ca/jae/1997-v12.3/deb-trivedi/</ref> or in the R-package AER <ref type="bibr" target="#b14">[Kleiber and Zeileis, 2008]</ref> as the data set NMES1988. We work with the following variables: age, school (years of education), income (family income), chronic (number of chronic conditions), visits (number of physician office visits) and hospital (number of hospital stays).</p><p>We compare TSLiNGAM with the standard DirectLingam to find the causal structure.</p><p>To prune redundant edges in the resulting adjacency matrices B, we perform Adaptive Lasso, as is done in <ref type="bibr" target="#b25">Shimizu et al. [2011]</ref>. This results in the directed acyclic graphs shown in Figure <ref type="figure">2</ref>.</p><p>The causal order found by DirectLiNGAM is (hospital, chronic, visits, age, income, school). This order is not very logical. We would for example expect that the number of</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Disclosure statement</head><p>The authors report there are no competing interests to declare. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SUPPLEMENTARY MATERIAL</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Kernel independent component analysis</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">R</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Berrisch</surname></persName>
		</author>
		<ptr target="https://CRAN.R-project.org/package=dccpp" />
		<title level="m">Fast Computation of Distance Correlations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note>R package version 0.0.2</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A fast algorithm for computing distance correlation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Statistics &amp; Data Analysis</title>
		<imprint>
			<biblScope unit="volume">135</biblScope>
			<biblScope unit="page" from="15" to="24" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Causal discovery in linear non-gaussian acyclic model with multiple latent confounders</title>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Hao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="2816" to="2827" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An optimal-time algorithm for slope selection</title>
		<author>
			<persName><forename type="first">R</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Salowe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Steiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Szemerédi</surname></persName>
		</author>
		<idno type="DOI">10.1137/0218055</idno>
		<ptr target="https://doi.org/10.1137/0218055" />
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Computing</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="792" to="810" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Independence testing-based approach to causal discovery under measurement error and linear non-gaussian models</title>
		<author>
			<persName><forename type="first">H</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2210.11021" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">causalXtreme: Causal discovery in heavy-tailed models</title>
		<author>
			<persName><forename type="first">N</forename><surname>Gnecco</surname></persName>
		</author>
		<ptr target="https://github.com/nicolagnecco/causalXtreme" />
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>R package version 0.0.0.9000</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Causal discovery in heavy-tailed models</title>
		<author>
			<persName><forename type="first">N</forename><surname>Gnecco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Meinshausen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Engelke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="6" to="2021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Robust statistics: the approach based on influence functions</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">R</forename><surname>Hampel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">M</forename><surname>Ronchetti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Rousseeuw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">A</forename><surname>Stahel</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986">1986</date>
			<publisher>Wiley-Interscience</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Estimation of causal effects using linear non-gaussian causal models with hidden variables</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">O</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shimizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Kerminen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Palviainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Approximate Reasoning</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="362" to="378" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Estimation of a structural vector autoregression model using non-gaussianity</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shimizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">O</forename><surname>Hoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">56</biblScope>
			<biblScope unit="page" from="1709" to="1731" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Pairwise likelihood ratios for estimation of non-gaussian structural equation models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<author>
			<persName><forename type="first">M</forename><surname>Kalisch</surname></persName>
		</author>
		<ptr target="https://cran.r-project.org/web/packages/pcalg/" />
	</analytic>
	<monogr>
		<title level="m">Methods for Graphical Models and Causal Inference</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="7" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Optimal slope selection via expanders</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sharir</surname></persName>
		</author>
		<idno type="DOI">10.1016/0020-0190(93)90234-Z</idno>
		<ptr target="https://doi.org/10.1016/0020-0190(93)90234-Z" />
	</analytic>
	<monogr>
		<title level="j">Information Processing Letters</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="115" to="122" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Applied Econometrics with R (AER)</title>
		<author>
			<persName><forename type="first">C</forename><surname>Kleiber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zeileis</surname></persName>
		</author>
		<ptr target="https://CRAN.R-project.org/package=AER" />
		<imprint>
			<date type="published" when="2008">2008</date>
			<publisher>Springer-Verlag</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Efficient randomized algorithms for the repeated median line estimator</title>
		<author>
			<persName><forename type="first">J</forename><surname>Matoušek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Mount</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">S</forename><surname>Netanyahu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth ACM-SIAM Annual Symposium on Discrete Algorithms</title>
		<meeting>the Fourth ACM-SIAM Annual Symposium on Discrete Algorithms</meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="74" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Causality: Models, Reasoning and Inference</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
	<note>2 edition</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Consistency and asymptotic distribution of the theil-sen estimator</title>
		<author>
			<persName><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Planning and Inference</title>
		<imprint>
			<biblScope unit="volume">138</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1836" to="1850" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Elements of causal inference: foundations and learning algorithms</title>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">robslopes: Fast Algorithms for Robust Slopes</title>
		<author>
			<persName><forename type="first">J</forename><surname>Raymaekers</surname></persName>
		</author>
		<ptr target="https://CRAN" />
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">R-Project</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The r journal: robslopes: Efficient computation of the (repeated) median slope</title>
		<author>
			<persName><forename type="first">J</forename><surname>Raymaekers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The R Journal</title>
		<idno type="ISSN">2073-4859</idno>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="38" to="49" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Robust regression and outlier detection</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Rousseeuw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Leroy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
			<publisher>John wiley &amp; sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Estimates of the regression coefficient based on kendall&apos;s tau</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Sen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">324</biblScope>
			<biblScope unit="page" from="1379" to="1389" />
			<date type="published" when="1968">1968</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A linear non-gaussian acyclic model for causal discovery</title>
		<author>
			<persName><forename type="first">S</forename><surname>Shimizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">O</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvarinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kerminen</surname></persName>
		</author>
		<ptr target="//arxiv.org/ftp/arxiv/papers/1408/1408.2038.pdf" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<date type="published" when="2003">2003-2030, 10 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Directlingam: A direct method for learning a linear non-gaussian structural equation model</title>
		<author>
			<persName><forename type="first">S</forename><surname>Shimizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Inazumi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sogawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvarinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kawahara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Washio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">O</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bollen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1225" to="1248" />
			<date type="published" when="2011-04">04 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Robust regression using repeated medians</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">F</forename><surname>Siegel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="242" to="244" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Network modelling methods for fmri</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">M</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Salimi-Khorshidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Webster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Beckmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">E</forename><surname>Nichols</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Ramsey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Woolrich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeuroImage</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="875" to="891" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Finding the repeated median regression line</title>
		<author>
			<persName><forename type="first">A</forename><surname>Stein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Werman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SODA &apos;92</title>
		<imprint>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Measuring and testing dependence by correlation of distances</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Székely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Rizzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">K</forename><surname>Bakirov</surname></persName>
		</author>
		<idno type="DOI">10.1214/009053607000000505</idno>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2769" to="2794" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">ParceLiNGAM: A Causal Ordering Method Robust Against Latent Confounders</title>
		<author>
			<persName><forename type="first">T</forename><surname>Tashiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Shimizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Washio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="57" to="83" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A rank-invariant method of linear and polynomial regression analysis</title>
		<author>
			<persName><forename type="first">H</forename><surname>Theil</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Koninklijke Nederlandse Akademie van Wetenschappen</title>
		<meeting>Koninklijke Nederlandse Akademie van Wetenschappen</meeting>
		<imprint>
			<date type="published" when="1950">1950</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">N</forename><surname>Venables</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">D</forename><surname>Ripley</surname></persName>
		</author>
		<ptr target="https://www.stats.ox.ac.uk/pub/MASS4/" />
		<title level="m">Modern Applied Statistics with S, fourth edition</title>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Simulation results on extensions of the theil-sen regression estimator</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">R</forename><surname>Wilcox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications in Statistics -Simulation and Computation</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1117" to="1126" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The adaptive lasso and its oracle properties</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American statistical association</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">476</biblScope>
			<biblScope unit="page" from="1418" to="1429" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
