<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Crake: Causal-Enhanced Table-Filler for Question Answering over Large Scale Knowledge Base</title>
				<funder ref="#_Pq2VAua">
					<orgName type="full">National Key R&amp;D Program of China</orgName>
				</funder>
				<funder ref="#_SW4mNCm">
					<orgName type="full">NSFC</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Minhao</forename><surname>Zhang</surname></persName>
							<email>zhangminhao@pku.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Ruoyu</forename><surname>Zhang</surname></persName>
							<email>ry_zhang@pku.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Yanzeng</forename><surname>Li</surname></persName>
							<email>liyanzeng@stu.pku.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Lei</forename><surname>Zou</surname></persName>
							<email>zoulei@pku.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Wangxuan Institute of Computer Technology (WICT)</orgName>
								<orgName type="institution">Peking University</orgName>
								<address>
									<country>China;</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Beijing Academy of Artificial Intelligence</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country>China;</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Crake: Causal-Enhanced Table-Filler for Question Answering over Large Scale Knowledge Base</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Semantic parsing solves knowledge base (KB) question answering (KBQA) by composing a KB query, which generally involves node extraction (NE) and graph composition (GC) to detect and connect related nodes in a query. Despite the strong causal effects between NE and GC, previous works fail to directly model such causalities in their pipeline, hindering the learning of subtask correlations. Also, the sequencegeneration process for GC in previous works induces ambiguity and exposure bias, which further harms accuracy. In this work, we formalize semantic parsing into two stages. In the first stage (graph structure generation), we propose a causal-enhanced table-filler to overcome the issues in sequence-modelling and to learn the internal causalities. In the second stage (relation extraction), an efficient beamsearch algorithm is presented to scale complex queries on large-scale KBs. Experiments on LC-QuAD 1.0 indicate that our method surpasses previous state-of-the-arts by a large margin (17%) while remaining time and space efficiency. The code and models are available at <ref type="url" target="https://github.com/AOZMH/Crake">https://github.com/AOZMH/Crake</ref>.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>To incorporate knowledge in real-world questionanswering systems, knowledge base question answering (KBQA) utilizes a background knowledge base (KB) as the source of answers to factoid natural language questions. Leveraging the versatility of KB query languages like SPARQL <ref type="bibr" target="#b20">(Prud'hommeaux, 2008)</ref>, many previous works <ref type="bibr" target="#b24">(Unger et al., 2012;</ref><ref type="bibr">Yahya et al., 2012)</ref> adopted a semantic parsing paradigm for KBQA, in which questions are converted to equivalent SPARQL queries and answers are given by executing the queries in KB. Regarding the intrinsic graph structure of SPARQLs, some works further reduced such procedure as generating the query graph of SPARQLs w.r.t. questions. However, these methods either 0 0 1 ‚ñû 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 ‚ñû 0 1 ‚ñü 0 0 1 ‚ñõ 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 ‚ñõ 0 0 0 0 1 ‚ñö 1 ‚ñö 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 ‚ñö 0 0 0 0 0 0 0 0 1 ‚ñö 0 0 0 Node Mention Tag Linksto ‚ô† Swinhoe's Crake Entity dbr:Swinhoe's _Crake ‚ô¶ person Type dbo:person require auxiliary tools (e.g. AMR in <ref type="bibr" target="#b21">Kapanipathi et al., 2021</ref>, constituency tree in <ref type="bibr" target="#b12">Hu et al., 2021</ref>, dependency tree in <ref type="bibr" target="#b10">Hu et al., 2017)</ref> causing potential cascading errors, or rely on predefined templates <ref type="bibr" target="#b8">(Cui et al., 2017;</ref><ref type="bibr" target="#b2">Athreya et al., 2021)</ref> limiting their expressiveness and generalization abilities.</p><formula xml:id="formula_0">‚ô£ class Variable / ‚ô• person Variable / üççÔºå‚ô¶Ôºå‚ô•Ôºå‚ö™Ôºå‚ô¶Ôºå‚ñ†Ôºå‚ñ≤Ôºå‚òÖÔºåüåôÔºåüçïÔºå‚Ä¢Ôºå‚ÄªÔºåÊòØüòÇü§£üòÉüòçüßßüíã‚ô¶üçÇüéàüéØüÄÑ‚ô†‚ô£‚ô•‚ô¶üîàüîâüö©‚òÅ‚ù§üß°üíõüíöüñ§üíîüí§‚õî ‚ùå‚ùó‚ùì‚ùî‚úÖÛæ†∑Ûæ†ÆÛæ†¨‚Ä¢üî¥‚ûï‚ûñ‚ûó‚úñüîòüî¥üîµ‚Ä¢‚¨ú‚ö™‚¨õ‚ñ™‚ñ´‚óæ‚óªüó®ÔºåüçéÔºåüççÔºåüçáÔºåü••</formula><p>To address these, efforts were made on devising independent pipelines for query graph construction <ref type="bibr" target="#b16">(Lin et al., 2021)</ref>. As in Figure <ref type="figure" target="#fig_1">1</ref>, these pipelines usually involve a node extraction (NE) module to detect the mentions of all nodes in query graph and link entity mentions, a graph composition (GC) module to connect related nodes given by NE, and a relation extraction (RE) module deciding the KB predicate corresponding to each edge added in GC. In this framework, two drawbacks exist in previous works: 1) we observe strong causal effects between NE and GC, e.g. edges connected by GC are valid only between the node mentions extracted in NE, making GC decisions highly dependent on NE. To this regard, previous works <ref type="bibr">(Zhang et al., 2021;</ref><ref type="bibr" target="#b21">Ravishankar et al., 2021)</ref> that perform NE and GC separately without causal-modelling may fall short in deeply comprehending the correlated tasks and accurately generating query graphs. 2) GC is commonly modelled as a sequence-generation in prior methods, either through generative decoder <ref type="bibr" target="#b22">(Shen et al., 2019;</ref><ref type="bibr" target="#b7">Chen et al., 2021)</ref> or via stagetransition <ref type="bibr">(Yih et al., 2015;</ref><ref type="bibr" target="#b11">Hu et al., 2018)</ref>. However, sequence-modelling generally undergoes sequence ambiguity and exposure bias <ref type="bibr">(Zhang et al., 2019)</ref> that harms model accuracy.</p><p>In this work, we formalize the generation of query graph in a two-staged manner as in Figure <ref type="figure" target="#fig_1">1</ref>. At the first stage, we tackle the aforesaid weaknesses by a novel causal-enhanced table-filling model to jointly complete NE and GC, resulting in a query graph structure representing the connectivity of all nodes. More specifically, inspired by <ref type="bibr">Chen et al. (2020a)</ref>, we utilize a label transfer mechanism to facilitate the acquisition of causality between NE and GC (which solves drawback 1 above). Further, we apply a table-filler to decode all edges simultaneously, which naturally circumvents the ambiguity and bias of iterative decoding (and solves drawback 2). For the second stage, we propose a beam-search-based relation extraction algorithm to determine the predicate that binds to each graph edge. Differ from prior works, we perform candidate predicate retrieval and ranking alternately for each edge, limiting the candidate scale linearly w.r.t. KB degree and making the algorithm scalable for large-scale KBs like DBpedia.</p><p>In short, the major contributions of this paper are: 1) to our knowledge, we are the first to model GC as a table-filling process, which prevents the ambiguity and bias in prior works; 2) we model the intrinsic causal effects in KBQA to grasp subtask correlations and improve pipeline integrity; 3) our method outperforms previous state-of-the-arts on LC-QuAD 1.0, a prominent KBQA benchmark, by a large margin (‚àº17%), further experiments verifies the effectiveness of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Problem Setting</head><p>We solve KBQA in a semantic parsing way, given a question (left-top in Figure <ref type="figure" target="#fig_1">1</ref>), we generate a SPARQL query (right-top in Figure <ref type="figure" target="#fig_1">1</ref>) to represent its semantics and answer the question by executing the query in KB. By definition, SPARQL describes a query graph with each triple in its body referring to a graph edge; by matching the graph pattern in KB, certain KB entries binding to the query graph can be processed as query results (e.g. in   for SELECT queries, all entries binding to the "select" node are results; for JUDGE queries, the existence of matched entries determines the boolean result). Hence, our task is further specified as constructing the query graph (bottom of Figure <ref type="figure" target="#fig_1">1</ref>) of a question to represent its corresponding SPARQL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Methodology Overview</head><p>Illustrated by Figure <ref type="figure" target="#fig_1">1</ref>, we construct the query graph in two stages. In the graph structure generation stage (bottom-left in Figure <ref type="figure" target="#fig_1">1</ref>), we extract all graph nodes by finding the mention of each node in question and its tag among {variable, entity, type}, e.g. the mention and tag for the node ?class is "class" and variable, respectively. Further, we link all non-variable nodes to KB entries, e.g. the type node with mention "person" links to dbo:person in Figure <ref type="figure" target="#fig_1">1</ref>. Also, we decide the target ("select") node of the graph and add undirected edges between the nodes that are connected in the query graph, resulting in a graph structure representing the connectivity of all nodes. Since all edges above are undirected and unlabeled, we fill in the exact KB predicate of each edge in the relation extraction stage (bottom-right in Figure <ref type="figure" target="#fig_1">1</ref>) to construct a complete query graph.</p><p>Finally, we compose a SPARQL w.r.t. the query graph as output. Note that the body of the SPARQL exactly corresponds to the query graph, so only the SPARQL header is yet undetermined. Like Hu et al., 2021, we collect frequent trigger words in the train data to classify questions into COUNT, JUDGE or SELECT queries as in Table <ref type="table" target="#tab_2">1</ref> (e.g. a question beginning with "is" triggers JUDGE). Thus, an entire SPARQL can now be formed. In the following sections, we expatiate our methodology for the two aforementioned stages.</p><p>Figure <ref type="figure">2</ref>: Causal-enhanced table-filling model for graph structure generation. The label-to-node and table-to-edge correspondence is illustrated by the poker and fruit symbols respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Node Extraction (NE)</head><p>Node extraction discovers all nodes in the query graph, i.e. {?person, ?class, dbr:Swinhoe's_Crake, dbo:person} in Figure <ref type="figure" target="#fig_1">1</ref>. We represent a node by its mention and tag, i.e. ("person", variable), ("class", variable), ("Swinhoe's Crake", entity) and ("person", type) for each node respectively.</p><p>This goal can naturally be achieved by multiclass sequence labeling. More specifically, let Q ‚àà N n be the question (token ids) with length n, we first encode it into hidden features H rb by a RoBERTa <ref type="bibr" target="#b17">(Liu et al., 2019)</ref> encoder</p><formula xml:id="formula_1">E rb ‚à∂ N n ‚Üí R</formula><p>n√óh rb with hidden size h rb :</p><formula xml:id="formula_2">H rb = E rb (Q) ‚àà R n√óh rb</formula><p>Then, H rb is projected by a fully-connectednetwork (FCN)</p><formula xml:id="formula_3">E ne ‚à∂ R n√óh rb ‚Üí R n√ó|L| into Y ne in label space: Y ne = E ne (H rb ) ‚àà R n√ó|L| L = {O} ‚à™ {B, I} √ó {V, E, T, V T }</formula><p>is the label set denoting the mention span of variables (V), entities (E), types (T), or overlapping variable and type (VT). Now, the label prediction of each token can be given by P ne = argmax(Y ne ); also, given the gold token labels G ne ‚àà N n (Figure <ref type="figure">2</ref> top), a model for NE can be trained by optimizing:</p><formula xml:id="formula_4">‚Ñì ne = - 1 n n ‚àë i=1 log(softmax(Y ne )[i; G ne [i]])</formula><p>Where [‚ãÖ] denotes tensor indexing.</p><p>After detecting all node mentions and tags, we link each non-variable node to KB entries by DBpedia Lookup and a mention-to-type dictionary built on train data to align the graph structure with KB. See Appendix A for more details in node linking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Graph Composition (GC)</head><p>After node extraction, all nodes in the query graph remain unconnected. To form the structure of the query graph, graph composition inserts unlabeled and undirected edges between the nodes that are related in the query graph, leaving the specific predicate of each edge yet unresolved. Formerly, graph composition is commonly modelled as a edgesequence-generation process via stage-transition <ref type="bibr">(Yih et al., 2015;</ref><ref type="bibr" target="#b11">Hu et al., 2018)</ref> or generative decoders <ref type="bibr" target="#b22">(Shen et al., 2019;</ref><ref type="bibr" target="#b7">Chen et al., 2021)</ref>. Despite the strong expressiveness, modelling graph composition by a sequence usually suffers from two issues: 1) while the edge sequence is ordered, edges in the query graph are a set without order. For a graph with two edges e 1 and e 2 , both sequence e 1 -e 2 and e 2 -e 1 correctly represents the edges in the graph, but they are distinct from the perspective of sequence-generation. As a result, the edge set itself becomes ambiguous for the sequence, which confuses the model when comprehending a sequence and potentially decelerates the convergence. 2) As discussed by <ref type="bibr">Zhang et al., 2019</ref>, without extra augmentation, sequence-generation generally endures an exposure bias between training and inference, harming the model's accuracy when predicting. Hence, a robust model should address the issues above properly.</p><p>Here, we model graph composition by a tablefilling process to decide all edges simultaneously involving no sequence-generation, which naturally circumvents all issues above. Let H gc ‚àà R n√óh gc be the hidden features for graph composition (the full definition of H gc with causal-modelling is given in Section 3.3; without causal-modelling, we simply have H gc = H rb ), we adopt a biaffine attention model <ref type="bibr" target="#b9">(Dozat and Manning, 2017;</ref><ref type="bibr" target="#b27">Wang et al., 2021)</ref> to convert H gc into a table denoting the relationship between each token pair. More specifically, through two multi-layer-perceptrons (MLP) E head and E tail ‚à∂ R n√óh gc ‚Üí R n√óh bi , we first project H gc into head (H head ) and tail (H tail ) features:</p><formula xml:id="formula_5">H {head,tail} = E {head,tail} (H gc ) ‚àà R n√óh bi</formula><p>Then, for ‚àÄ1 ‚â§ i, j ‚â§ n, the biaffine attention is performed between the head features of the i th token h</p><formula xml:id="formula_6">(i)</formula><p>head and the tail features of the j th token h</p><formula xml:id="formula_7">(j) tail , producing s i,j ‚àà R</formula><p>2 representing the probability that an edge exists between the i th and j th token:</p><formula xml:id="formula_8">s i,j = softmax(Biaff(h (i) head , h (j) tail )) Biaff(x, y) ‚à∂= x T U 1 y + U 2 (x ‚äï y) + b As U 1 ‚àà R 2√óh bi √óh bi , U 2 ‚àà R 2√ó2h bi and b ‚àà R 2 are trainable parameters, ‚äï denotes concatenation.</formula><p>Combining all scores by Y gc = (s i,j ) (1‚â§i,j‚â§n) ‚àà R n√ón√ó2 , we now have a table describing the edge existence likelihood between any two tokens.</p><p>At training, we first obtain the boolean gold table G gc ‚àà B n√ón , for every connected node pair in the query graph, the element in G gc corresponding to any pair of tokens belonging to the mentions of the two nodes respectively is set to 1 (resulting in several rectangles of 1s). Also, we prefix the question with a special [CLS] token and connect it with the target node to represent the "select" edge; for ASK queries without target nodes, a [SEP] token is suffixed and connected with <ref type="bibr">[CLS]</ref>. Note that since the graph structure is undirected, G gc is a symmetric matrix. An example of G gc can be found in Figure <ref type="figure">2</ref>. With G gc , we can train the table-filler by ‚Ñì tb : Following <ref type="bibr" target="#b27">Wang et al., 2021</ref>, we also introduce ‚Ñì sym to grasp the table symmetry. Finally, we optimize ‚Ñì gc = ‚Ñì tb + ‚Ñì sym to train a model for GC.</p><formula xml:id="formula_9">‚Ñì tb = - 1 n 2 n ‚àë i=1 n ‚àë j=1 log(Y gc [i; j; G gc [i; j]]) (a) (b) Y GC X Y NE Y GC X Y NE RoBERTaEncoder Gumbel Softmax Label Embedding X Y NE Y GC LabelTransfer Biaffine Attention Sequence Labeling (c)</formula><formula xml:id="formula_10">‚Ñì sym = 1 n 2 n ‚àë i=1 n ‚àë j=1 2 ‚àë k=1 |Y gc [i; j; k] -Y gc [j; i; k]|</formula><p>At inference, for each pair of nodes given by NE, we average the rectangle area in Y gc corresponding to the mentions of the node pair as its edge existence probability. The node pairs with a probability higher than 0.5 are connected. This threshold is selected intuitively to denote an edge is more likely to exist against to not exist, though we argue that the prediction is insensitive to any threshold in reasonable range (e.g. 0.3‚àº0.7).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Causal Modelling NE and GC</head><p>Up to now, NE and GC are treated as separate tasks that fail to model the intrinsic causal effects between them (e.g. edges in Y GC only exist between the mentions detected in NE). Here, we model such causality by a mediation assumption in Figure <ref type="figure" target="#fig_2">3(b)</ref> denoting the causal dependence of GC on both question and NE prediction by edge X‚ÜíY GC and Y NE ‚ÜíY GC respectively. To grasp this causal graph, we devise a label transfer <ref type="bibr">(Chen et al., 2020a)</ref> module to enable the transfer of NE predictions to GC, i.e. representing Y NE ‚ÜíY GC , in Figure <ref type="figure" target="#fig_2">3(c)</ref>.</p><p>In detail, we sample NE predictions ·ª∏ne by gumbel softmax <ref type="bibr" target="#b18">(Nie et al., 2019)</ref>  </p><formula xml:id="formula_11">H gc = H rb ‚äï ( ·ª∏ne W le ) ‚àà R n√óh gc</formula><p>Now, by minimizing ‚Ñì gsg = ‚Ñì ne +‚Ñì gc , a joint model for NE and GC can be obtained. In this model, GC receives NE labels to learn the causal effects from NE, while NE gets feedback through differentiable label transfer to further aid GC decision. In this sense, our model improves the integrity of graph structure generation compared with separately modelling each subtask or simple multitasking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Relation Extraction (RE)</head><p>As shown in Figure <ref type="figure">4</ref>, relation extraction (RE) conducts candidate retrieval and ranking in turn for each edge in graph structure S to decide its predicate. For a question q, an edge e connecting nodes n 1 and n 2 with mention m 1 ,m 2 respectively, candidate retrieval recalls a set of predicates P that can be bound to e. Note that unlike e, each predicate in P is directional. Then, candidate ranking Rank(P ,q,m 1 ,m 2 ) gives each predicate a score. This section details this procedure.</p><p>Candidate Ranking For each p i ‚àà P , we encode it together with q,m 1 ,m 2 by a RoBERTa encoder and pool them to 0 ‚â§ s i ‚â§ 1 to score the predicate. If the direction of p i is n 1 ‚Üí n 2 , we join q, m 1 , m  is a non-variable node, the predicates around that node in KB are viewed as candidates; otherwise, they trace n 1 or n 2 in other graph edges with non-variable nodes and view the predicates k-hop away from that node in KB as candidates (e.g. predicates 2-hop away from dbo:person are candidates for ?class-?person in Figure <ref type="figure">4</ref>). We view this as the baseline in latter experiments.</p><formula xml:id="formula_12">e = (n 1 , n 2 ) ‚ÜêSample (S pend ); 6 for G ‚àà B do 7 P ‚ÜêRetrieve (G, n 1 , n 2 ); // n 1 /n 2 has mention m 1 /m 2 8 C = {(p i , s i )} ‚ÜêRank (P, q, m 1 , m 2 ); // Extend previous beams 9 for (p i , s i ) ‚àà C do 10 B ‚Ä≤ ‚Üê B ‚Ä≤ ‚à™ {G ‚à™ {(n 1 , n 2 , p i , s i )}}; 11 B ‚Üê B ‚Ä≤ .</formula><p>However, this results in a candidate scale O(n k )<ref type="foot" target="#foot_1">foot_1</ref> , making it unscalable to multi-hop queries (k‚Üë) and large KBs (n‚Üë). Here, we propose Algorithm 1 to limit the scale to O(n). We start by selecting an edge between n a 1 and n b 1 containing a non-variable node (e.g. edge ?class-dbr:Swinhoe's_Crake in Figure <ref type="figure">4</ref>), retrieving all adjacent predicates of that node in KB and use Rank to select the most proper predicate p 1 (e.g. dbp:named_by) of score s 1 , this forms a subgraph G={(n I NSQA <ref type="bibr" target="#b21">(Kapanipathi et al., 2021)</ref> .448 .458 .445 EDGQA <ref type="bibr" target="#b12">(Hu et al., 2021)</ref> .505 .560 .531</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II</head><p>QAmp <ref type="bibr" target="#b25">(Vakulenko et al., 2019)</ref> .250 .500 .330 NAMER <ref type="bibr">(Zhang et al., 2021)</ref> .438 .438 .435 STaG-QA <ref type="bibr" target="#b21">(Ravishankar et al., 2021)</ref> .745 .548 .536 Crake (ours)</p><p>.722 .731 .715</p><p>Table <ref type="table">2</ref>: End-to-end performance on LC-QuAD 1.0 test set. I/II stands for methods with/without aux tools. We re-implement NAMER since its results on LC-QuAD is not provided; however, NAMER suffers from severe timeout issues on DBpedia to limit its performance, so we restrict each candidate query to run at most 45s in practice (which already requires ‚àº15h for a complete evaluation run).</p><p>lect p 2 of score s 2 from P , add (n a 2 ,n b 2 ,p 2 ) to subgraph G and update its score as s 1 * s 2 . Repeating this loop until all edges are bound with a predicate, we finally form a query graph.</p><p>Note that for each edge, the candidate scale given by Retrieve is O(n), since it is always among the neighbors of one or several KB nodes. Also, to improve the recall of query graphs, this process can trivially be extended as a beam search with each step maintaining a beam of subgraphs B, ordering each subgraph by ‚àè i s i as in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>Dataset We adopt LC-QuAD 1.0 <ref type="bibr" target="#b23">(Trivedi et al., 2017)</ref>, a predominant open-domain English KBQA benchmark based on DBpedia <ref type="bibr" target="#b3">(Auer et al., 2007)</ref> 2016-04, to test the performance of our system. We randomly sample 200 questions from train data as dev set and follow the raw test set, resulting in a 4800/200/1000 train/dev/test split. More details on the dataset can be found in Appendix C. Like Zhang et al., 2021, we do not experiment on multiple datasets due to the high annotation cost involved, however, we conduct no dataset-specific optimizations in this work, so we consider the large improvements on LC-QuAD and detailed discussions sufficient to prove our effectiveness.</p><p>Annotation We annotate the dataset with the mention of each node in query graph, e.g. the mention "class" and "person" for the node ?class and dbo:person respectively in Figure <ref type="figure" target="#fig_1">1</ref>. With the annotation, we obtain the gold data (G ne ,G gc ) to train our models. Appendix D details the annotation process.  <ref type="table" target="#tab_7">3</ref>.</p><p>Baselines We evaluate our method against existing works both with and without auxiliary tools. With aux tools, <ref type="bibr" target="#b21">Kapanipathi et al., 2021</ref>  Setup We utilize the RoBERTa-large released by huggingface <ref type="bibr" target="#b28">(Wolf et al., 2020)</ref> as our encoder. All experiments are averaged on two runs on an NVIDIA A40 GPU. For the GSG model, we train for at most 500 epochs (~6 GPU-hours) and report the best checkpoint on dev set; for the RE model, we train for 20 epochs (~16 GPU-hours) and report the final checkpoint. For hyperparameters, we find no apparent performance variance on dev set as long as the values are in reasonable range (e.g. 64 ‚â§ h le ‚â§ 1024, 1e-6 ‚â§ lr gsg ‚â§ 2e-5) so no further tuning is involved. See the full setting in Appendix E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">End-to-end Evaluation</head><p>As shown in Table <ref type="table">2</ref>, our method, Crake, outperforms all former methods by a large‚àº17% margin on F1, becoming the new SoTA of LC-QuAD 1.0. Surpassing methods requiring aux tools (I) on all metrics, we present the effectiveness of independent pipelines (II) that avoid cascading errors. Also, we achieve consistent answer precision and recall to surpass other methods in II on F1, showing the superiority of our pipeline design, which is further discussed in the sections below. ) co-trains NE and GC by directly adding losses without modelling their intrinsic causal effects. TF+Causal denotes our full approach which models the causal effects between NE and GC by label transfer. We report the node-level P/R/F1 in NE, the exact-match (EM) and actual accuracy (that ignores variable mentions in judging accuracy) in GSG, and the overall answer-level P/R/F1 on LC-QuAD 1.0 dev set for comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Effects of Tabel-Filling</head><p>As explained in Section 3.2, modelling GC as a sequence-generation causes a few issues that can be overcome by table-filling. Specifically, the sequence ambiguity confuses the learning process and requires large decoders to grasp the sequence generation policy, which may slow down the convergence. Besides, the exposure bias harms the decoding accuracy of the model at inference. This section, we try to verify such effects by experiments. To enable the comparison with sequencegeneration, we construct a generative decoder as in Zhang et al., 2021 as the baseline, which sequentially generates the connected node pairs in the graph structure to represent the edges. We train the generative model under the same settings (e.g. learning rate, warmup, epochs, etc.), resulting in the performance of Seq2seq in Table <ref type="table" target="#tab_7">3</ref>.</p><p>Comparing with the table-filling model (i.e. TF in Table <ref type="table" target="#tab_7">3</ref>), Seq2seq comes short in the accuracy of graph structure, indicating the negative effects of the exposure bias on predicting accuracy. Meanwhile, TF requires only 1/100 of Seq2seq's parameters to achieve comparable or better results, we attribute this to the removal of sequence ambiguity which frees the model from acquiring the complex and ambiguous scheme of sequencegeneration. This speculation is further verified in Figure <ref type="figure" target="#fig_4">5</ref>, in which TF converges distinctly quicker than Seq2seq since the simultaneous decision of all edges is well-defined and easier to learn. Thus, compared with sequence-modelling, handling GC via table-filling reduces model size and boosts training, which is essential for real-world applications. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Effects of Causal-Modelling</head><p>We propose a joint model to learn the NE-GC causalities in Section 3.3, to discuss its effects, we compare it with two alternatives in Table <ref type="table" target="#tab_7">3</ref>: 1) using two separate models for NE and GC (TF in Table 3) like <ref type="bibr">Ravishankar et al., 2021, 2)</ref> co-training NE and GC by sharing encoder and adding losses (TF+SMTL in Table <ref type="table" target="#tab_7">3</ref>) like <ref type="bibr" target="#b22">Shen et al., 2019.</ref> As shown, co-training consistently surpasses separate models by grasping the shared knowledge between NE and GC, nevertheless, our causal-modelling approach (TF+Causal) further outperforms cotraining. In detail, though TF+Causal has similar results with TF+SMTL in NE, it achieves better accuracy for overall GSG (NE+GC) and excels in end-to-end metrics. Therefore, we infer that causalmodelling improves the integrity of the GSG stage   <ref type="bibr">4:</ref> Performance comparison between our beamsearch RE algorithm and its baseline in Section 4. Accuracy refers to the answer-level P/R/F1, efficiency is measured by the average run time on 1/2/3-hop queries.</p><p>by expressing the internal causalities between its subtasks. To better understand this, we perform a case study in Figure <ref type="figure" target="#fig_5">6</ref>, in which TF fails to realize that "skier" also corresponds to a type node; in contrast, TF+SMTL extract all nodes correctly by learning both NE and GC labels, but it still fails in generating a correct graph structure. Finally, TF+Causal utilizes the VT tag of "skier" in NE predictions and correctly connects the II-IV edge in GC. Thus, Figure <ref type="figure" target="#fig_5">6</ref> demonstrates the usage of causal effects to reach higher accuracy in GSG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Analysis on Beam-Search RE</head><p>In this section, we compare our beam-search RE algorithm with its baseline. As stated in Section 4, performing retrieval and ranking on each edge (rather than retrieving the candidates of every edge before ranking), our approach lowers the scale of candidate predicates on multi-hop queries to get better efficiency, which is verified in Table <ref type="table">4</ref>. In detail, BeamSearch costs substantially less time than Baseline in 2 and 3-hop queries (note that for 1-hop queries, two methods reduce to a same process with similar time costs). Since BeamSearch only operates on the neighbors of certain KB nodes, it avoids the retrieval of 2-hop neighbors, which requires considerable time on DBpedia, to improve efficiency. In addition, by pruning off useless candidates in Baseline, BeamSearch also achieves higher overall KBQA accuracy in Table <ref type="table">4</ref>. Therefore, Algorithm 1 transcends previous methods to reveal an efficient and accurate solution for ranking-based RE scalable to KB size and query complexity.</p><p>6 Related Works KBQA via Semantic Parsing A mainstream to solve KBQA is semantic parsing <ref type="bibr">(Yih et al., 2016)</ref> which converts a question to a KB query to get answers. Due to the graph-like structure of KB queries, prior works construct query graphs to represent queries in semantic parsing. Among them, some works 2018; <ref type="bibr">Chen et al., 2020b)</ref> only focus on predicting the graph structure given node inputs. To perform end-to-end QA, Hu et al., 2017 leverages the dependency parsing tree to match KB subgraphs for answers; <ref type="bibr" target="#b21">Kapanipathi et al., 2021</ref> builds the query graph by transforming and linking the AMR <ref type="bibr" target="#b4">(Banarescu et al., 2012)</ref> of the question; <ref type="bibr" target="#b12">Hu et al., 2021</ref> uses the constituency tree to compose an entity description graph representing the query graph structure. Requiring aux tools or data structures, these works may be subjected to cascading errors. Yih et al., 2015 overcomes this by an independent stage-transition framework to generate the query graph, <ref type="bibr" target="#b11">Hu et al., 2018</ref> extends the transitions to express more complex graphs. Besides, Zhang et al., 2021 adopts a pointer generator to decode graph structure, <ref type="bibr" target="#b21">Ravishankar et al., 2021</ref> generates the query skeleton by a seq2seq decoder. Unlike these methods that model the query graph as a sequence (by state-transition or generative decoder), we decode all edges at once via a table-filler in graph structure generation.</p><p>Modelling causal effects Causality occurs in various deep-learning scenarios between multiple channels or subtasks, existing works models the causality for better performance. <ref type="bibr" target="#b19">Niu et al., 2021</ref> mitigates the false causal effects in VQA <ref type="bibr" target="#b1">(Antol et al., 2015)</ref> to overcome language bias; Zeng et al., 2020 dispels the incorrect causalities from different input channels of NER by generating counterfacts. Chen et al., 2020a utilizes the inter-subtask causalities to improve multitask learning for JERE <ref type="bibr" target="#b15">(Li and Ji, 2014)</ref>, <ref type="bibr">ABSA (Kirange et al., 2014)</ref>, and LJP. Unlike them, we formulate and utilize the internal causal effects in KBQA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this work, we formalize the generation of query graphs in KBQA by two stages, namely graph structure generation (GSG) and relation extraction (RE).</p><p>In GSG, we propose a table-filling model for graph composition to avoid the ambiguity and bias of sequence-modelling, meanwhile, we encode the inherent causal effects among GSG by a labeltransfer block to improve the stage integrity. In RE, we introduce an effective beam-search algorithm to retrieve and rank predicates in order for each edge, which turns out to be scalable for large KBs and multi-hop queries. Consequently, our approach substantially surpasses previous state-of-the-arts in KBQA, revealing the effectiveness of our pipeline design. Detailed experiments also validate the effects of all our contributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Limitation</head><p>Admittedly, our approach endures certain limitations as discussed below.</p><p>Query Expressiveness Like most semantic parsing systems, we fail to cover all the operations of SPARQL, limiting our capability to compose queries with complex filter or property path. For the conciseness of our system, we only focus on constructing triples in the multi-hop query graph in this paper, while we plan to incorporate more functions into Crake in the future to improve the expressiveness of the system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Annotation Cost</head><p>Training models with node mentions require expensive manual annotations, which is impractical for us to conduct on every popular KBQA dataset. As explained in Section 5, without data-oriented optimization, we believe the significant gain presented adequate to verify our contributions. Further, we expect to extenuate such costs in two directions for the future: 1) some modules of our framework (e.g. NE) is generalizable to other English questions, gifting it the potential to be transferred to other datasets without re-training; 2) few-shot <ref type="bibr" target="#b26">(Wang et al., 2020)</ref> and active <ref type="bibr" target="#b0">(Aggarwal et al., 2014)</ref> learning techniques aids the model to reach competitive performance with a small portion of annotated data, which can be explored in our framework to reduce annotation cost. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Details in Entity and Type Linking</head><p>We link each non-variable node to a KB entry by its mention. For entity nodes, we directly link it to an entity with the same name as its mention if such entity exists in the KB (e.g. link mention "New York" to dbr:New_York); otherwise, we recall entities by DBpedia Lookup<ref type="foot" target="#foot_2">foot_2</ref> and further prioritize ones whose lower-cased name is the same as the lower-cased mention (e.g.dbr:new_york). Then, the prioritized entity with the highest lookup score is linked to the node; if no entity is prioritized, the entity with the highest score is selected.</p><p>For type nodes, we build a dictionary D based on the mention-type pairs (e.g. authors-dbo:Writer) in train data and directly use the link result from D if the mention exists in D. Otherwise, we singularize and capitalize the mention to construct an URI with prefix dbo (e.g. bands‚Üídbo:Band), if this URI presents in the KB, the type node is linked to this URI. If no entry is found for either an entity or a type afterall, we simply discard the node from our query graph.</p><p>Note that although we involve no extra disambiguation step, DBpedia Lookup itself has certain mention-level disambiguation abilities to refine mention-relevant candidates. Admittedly, sentence context also contributes to a precise linking decision, leaving such context-level disambiguation a future direction to improve our work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Training Details of the Candidate Ranking Model</head><p>We mainly follow NAMER <ref type="bibr">(Zhang et al., 2021)</ref> in training the candidate ranking model mentioned in Section 4. Basically, the positive and negative training samples are obtained from the gold query. For instance, in Figure <ref type="figure" target="#fig_1">1</ref>, we obtain the candidates between ?class (m 1 ="class") and ?person (m 2 ="person") by constructing "select ?r { ?person dbp:type dbo:person. ?class ?r ?person. dbr:Swinhoe's_Crake dbp:class ?class }" and "select ?r { ?person dbp:type dbo:person. ?person ?r ?class. dbr:Swinhoe's_Crake dbp:class ?class }" with query results P p and P r respectively. Let p * = dbp ‚à∂ named_by be the correct predicate, we collect model inputs {(q, m 1 , m 2 , p * )} as a positive sample (i.e. of label 1) and {(q, m 1 , m 2 , p i )|p i ‚àà P pos \{p * } ‚à™ {(q, m 2 , m 1 , p i )|p i ‚àà P rev } as negative samples (i.e. of gold label 0).</p><p>Further, we follow the augmentation process in NAMER to learn the effects of mention order on model predictions. Specifically, we add {(q, m 1 , m 2 , p i )|p i ‚àà P rev \{p * } ‚à™ {(q, m 2 , m 1 , p i )|p i ‚àà P pos } to negative samples when training. With the aforesaid process repeated on each query graph edge, we get the full training samples to train a ranking model.</p><p>Besides, similar with NAMER, we observe a performance decay when forcibly co-training RE and GSG module, in this regard, we leave RE a separate module alongside GSG in the system. As discussed in NAMER, the different input channels between RE and GSG may result in unequal semantic spaces for the model. Thus, despite the causal association between RE and GSG, we conjecture that the model fails to acquire beneficial causalities between incompatible semantic spaces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Details of the Dataset</head><p>LC-QuAD 1.0 is an English open-domain KBQA dataset widely used to evaluate KBQA systems. With a GPL-3.0 licence, this dataset is intended for training and testing models to answer a question via querying the knowledge base, permitting modifications on the dataset for experiments, which is consistent with the way we use the dataset (annotating node mentions for each data entry, train several models for KBQA on the train data and test the system performance on the test data).</p><p>Due to the nature of KBQA tasks, LC-QuAD 1.0 involves questions about certain real-world entities usually including persons, organizations or objects (which is exactly the conditions where KBQA is applied to real-world applications). However, most information about the individuals (e.g. name, team, etc.) are publicly available (since the dataset utilizes DBpedia as background KB while DBpedia mainly collects data from publicly available Wikipedia). Further, when annotating the dataset, we perform a brief manual check on potential offensive or biased contents, to the best of our efforts, we find no apparent offensive hints in the questions and SPARQL queries. Hence, we believe that LC-QuAD 1.0 under intended KBQA use has minor potential to offend others or cause privacy issues.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Details of Data Annotation</head><p>Annotation Guidelines We adopt the same annotation format as Zhang et al., 2021 to annotate the LC-QuAD 1.0 dataset. Specifically, for each node in the query graph corresponding to the SPAR-QLs in the dataset, the mention of such node in the question is annotated. All annotated mentions are required as whole-words (e.g. including the 's' for plural words), the mention is left as "None" when no mention of a node can be found. There are certain cases where multiple mentions co-refer a node, we encourage annotators to choose a mention containing more concrete semantics, while all of these mentions are acceptable (e.g. for the question "Who is Jack's dad?", both "Who" and "dad" are correct mentions but the latter is encouraged since it indicates more semantics of the node). We provide a detailed guideline<ref type="foot" target="#foot_3">foot_3</ref> to annotators with extra discussions on marginal cases to further aid the annotation. Also, we discuss the potential risks and the overall usage of such annotations to get agreements from the annotators in the guideline.</p><p>Annotation Process We recruit 9 annotators with necessary background knowledge from school, consisting of 5 undergraduate and 4 graduate students, to fulfil the annotation task. By completing the annotation, we provide essential payments for each annotator. Finally, we use a script to auto-check the collected annotations and perform basic corrections (e.g. align all mentions to whole-words). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Hyperparameter Settings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Ethical Statements</head><p>Considering the nature of NLP-based QA systems, our method keeps the risk to output false (e.g. incorrect answers to factoid questions) or biased (e.g. imprecise count of answer numbers) answers, which might cause issues in trustworthy or practical uses. However, we'd like to clarify that this work is intended for discovering more accurate and efficient systems on KBQA regardless of the exact content in a KB, the answers to specific questions given by our method does not reflect the authors' point of view.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Generating a query graph (bottom) by two stages to represent the SPARQL (right-top). At graph structure generation stage, node-extraction generates all graph nodes while graph-composition adds unlabeled edges between proper nodes. Then, the relation extraction stage decides the specific predicate of each edge.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Modelling NE and GC with (a) and without (b) causality, as X, Y NE , and Y GC denotes question, NE predictions, and GC predictions. Model (c) learns the causal effects by a label transfer module.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>1 )} with only one edge whose score is s 1 . Then, we sample another edge between n a 2 and n b 2 (e.g. ?class-?person) and retrieve its candidates P based on G (e.g. G already entails ?class=dbr:bird, so all neighbors of dbr:bird forms P ), this process is denoted as Retrieve(G, n a 2 , n b 2 ). Now, we use Rank to se-</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: EM accuracy of GSG during training. See the meaning of each series in Table3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Case study on the effects of causal-modelling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="3,71.09,70.84,453.09,226.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Supported query types.</figDesc><table><row><cell>Type</cell><cell>Example SPARQL</cell></row><row><cell cols="2">JUDGE ask {dbr:New_York a dbo:City}</cell></row><row><cell cols="2">COUNT select count(?x) {?x a dbo:City}</cell></row><row><cell>SELECT</cell><cell>select ?x {?x a dbo:City}</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>with g‚àºGumbel(0,1)</figDesc><table><row><cell cols="2">QueryGraphStructure</cell><cell cols="2">CompleteQueryGraph</cell></row><row><cell cols="2">CandidateRetrieval</cell><cell cols="2">CandidateRanking</cell></row><row><cell>select</cell><cell>?person</cell><cell>select</cell><cell>?person</cell></row><row><cell></cell><cell>dbo:person CandidatePredicates ?class</cell><cell>dbp:class</cell><cell>dbo:person Candidate Score ?class</cell></row><row><cell>dbr:Swinh-oe's_Crake</cell><cell>dbp:fossilRange dbp:class</cell><cell>dbr:Swinh-oe's_Crake</cell><cell>dbp:class 0.99 dbo:species 0.92</cell></row><row><cell></cell><cell>dbo:wikiPageID</cell><cell></cell><cell>dbo:class 0.66</cell></row><row><cell></cell><cell>......</cell><cell></cell><cell>dbo:basedOn 0.05</cell></row><row><cell></cell><cell>dbp:genus</cell><cell></cell><cell>dbo:type 0.02</cell></row><row><cell></cell><cell cols="2">Iterateoneachedge</cell><cell></cell></row><row><cell cols="4">Figure 4: Candidate retrieval and raking framework for relation extraction.</cell></row></table><note><p><p><p>and temperature œÑ .</p>·ª∏ne = softmax((Y ne + g)/œÑ ) ‚àà</p>R n√ó|L| ·ª∏ne is then embedded by label embedding W le ‚àà R |L|√óh le and concatenated with H rb to form H gc in Section 3.2 with h gc = h rb + h le :</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Query graph structure S, beam width b Output: A beam of query graphs B 1 B ‚Üê {{}};// Start with an empth graph 2 S pend ‚Üê S;// All edges are pending 3 while S pend ‚â† ‚àÖ do</figDesc><table><row><cell>4</cell><cell>B</cell><cell>‚Ä≤ ‚Üê {};</cell></row><row><cell></cell><cell cols="2">// Select a pending edge</cell></row><row><cell>5</cell><cell></cell><cell></cell></row></table><note><p>2 , p i sequentially by [SEP] token as model input; otherwise (direction n 2 ‚Üí n 1 ), the Algorithm 1: BeamSearchRE Input: Question q,</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>topk(b);// Set up new beams // Mark e as determined 12 S pend ‚Üê S pend \ {e}; join order is q, m 2 , m 1 , p i . By giving s i to each candidate, we can get the most proper predicates for e by selecting those with highest scores. More details on training the ranking model can be found in Appendix B. Candidate Retrieval Zhang et al., 2021 proposed a straightforward way to retrieve candidates: if either n 1 or n 2</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>constructs query graphs based on the AMR of questions; Hu et al., 2021 designs rules on constituency tree to aid query graph formation. For independent pipelines without aux tools, Vakulenko et al., 2019 parses URI mentions from the question to match with KB via confidence score passing; Ravishankar et al., 2021 combines a generative graph-skeleton decoder with entity and relation detector to form a query; Zhang et al., 2021 co-trains a pointer generator with the node extractor to build a query graph, it's worth to note that this work also requires the node-to-mention Annotation for training.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>Experiments on table-filling and causal-modelling. Seq2seq and TF adopt a generative decoder and table-filler in GC respectively, while both deal with NE and GC by separate models. TF+SMTL (simple multitask learning</figDesc><table><row><cell></cell><cell>Decoder Parameters</cell><cell>NE Accuracy P R F1</cell><cell cols="2">GSG Accuracy EM Actual</cell><cell>P</cell><cell>End-to-end R F1</cell></row><row><cell>Seq2seq</cell><cell>76.67M (√ó1)</cell><cell cols="2">.895 .901 .897 .695</cell><cell>.768</cell><cell>.653 .674 .654</cell></row><row><cell>TF</cell><cell>0.66M (√ó1/100)</cell><cell cols="2">.895 .901 .897 .728</cell><cell>.795</cell><cell>.655 .674 .657</cell></row><row><cell cols="2">TF+SMTL 0.66M (√ó1/100)</cell><cell cols="2">.901 .904 .902 .735</cell><cell>.805</cell><cell>.665 .684 .667</cell></row><row><cell cols="2">TF+Causal 3.03M (√ó1/25)</cell><cell cols="2">.909 .914 .911 .755</cell><cell>.828</cell><cell>.677 .696 .680</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table</head><label></label><figDesc></figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 5 :</head><label>5</label><figDesc>Table 5 details our hyperparameter settings. Detailed hyperparameter settings in this work.</figDesc><table><row><cell>Name</cell><cell>Description</cell><cell>Setting</cell></row><row><cell>h rb h bi h le œÑ</cell><cell>Hidden size of the RoBERTa encoder Hidden size of the biaffine model Dimension of the label embedding Gumbel-softmax temperature</cell><cell>1024 256 256 0.05</cell></row><row><cell>optim Œ≤ 1 /Œ≤ 2 wd</cell><cell>Optimizer to train both GSG and RE models Betas of the AdamW optimizer Weight decay rate of the AdamW optimizer</cell><cell>AdamW 0.9 / 0.9 1e-5</cell></row><row><cell cols="2">Learning rate of the RoBERTa encoder in GSG Learning rate of other parameters in GSG batch gsg Batch size of the GSG model lr rb lr gsg Learning rate of the RE model lr re batch re Batch size of the RE model b Beam width in RE</cell><cell>1e-5 5e-5 64 1e-5 100 4</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_0"><p>Graph Structure Generation (GSG)The overview of the model proposed for graph structure generation is illustrated by Figure2. As discussed in Section 1, the model jointly deals with node extraction and graph composition via causalmodelling, which is detailed in this section below.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1"><p>n is the node degree in KB, k is the edge number in S</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2"><p>http://wiki.dbpedia.org/projects/dbpedia-lookup</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3"><p>See the guideline in supplementary materials.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work was supported by <rs type="funder">National Key R&amp;D Program of China</rs> (<rs type="grantNumber">2020AAA0105200</rs>) and <rs type="funder">NSFC</rs> under grant <rs type="grantNumber">U20A20174</rs>. The corresponding author of this work is <rs type="person">Lei Zou</rs> (zoulei@pku.edu.cn). We would like to thank <rs type="person">Zhen Niu</rs> and <rs type="person">Sen Hu</rs> for their kind assistance on this work. We also appreciate anonymous reviewers for their valuable comments and advises.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_Pq2VAua">
					<idno type="grant-number">2020AAA0105200</idno>
				</org>
				<org type="funding" xml:id="_SW4mNCm">
					<idno type="grant-number">U20A20174</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Active learning: A survey</title>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>Charu C Aggarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanquan</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Classification</title>
		<imprint>
			<publisher>Chapman and Hall/CRC</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="599" to="634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">VQA: visual question answering</title>
		<author>
			<persName><forename type="first">Stanislaw</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aishwarya</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2015.279</idno>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision, ICCV 2015</title>
		<meeting><address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015-12-07">2015. December 7-13, 2015</date>
			<biblScope unit="page" from="2425" to="2433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Template-based question answering using recursive neural networks</title>
		<author>
			<persName><surname>Ram G Athreya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Srividya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Axel-Cyrille Ngonga</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Ngomo</surname></persName>
		</author>
		<author>
			<persName><surname>Usbeck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE 15th International Conference on Semantic Computing (ICSC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="195" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Dbpedia: A nucleus for a web of open data</title>
		<author>
			<persName><forename type="first">S√∂ren</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Bizer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgi</forename><surname>Kobilarov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jens</forename><surname>Lehmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Cyganiak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Ives</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The semantic web</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="722" to="735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Abstract meaning representation (amr) 1.0 specification. In Parsing on Freebase from Question-Answer Pairs</title>
		<author>
			<persName><forename type="first">Laura</forename><surname>Banarescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claire</forename><surname>Bonial</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madalina</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kira</forename><surname>Griffitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ulf</forename><surname>Hermjakob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Schneider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing. Seattle: ACL</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing. Seattle: ACL</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1533" to="1544" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Exploring logically dependent multi-task learning with causal inference</title>
		<author>
			<persName><forename type="first">Wenqing</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jidong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaohui</forename><surname>Jin</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.173</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2213" to="2225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Formal query building with query structure prediction for complex question answering over knowledge base</title>
		<author>
			<persName><forename type="first">Yongrui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huiying</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuncheng</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guilin</forename><surname>Qi</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2020/519</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI 2020</title>
		<meeting>the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJCAI 2020</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3751" to="3758" />
		</imprint>
	</monogr>
	<note>ijcai.org</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Outlining and filling: Hierarchical query graph generation for answering complex questions over knowledge graph</title>
		<author>
			<persName><forename type="first">Yongrui</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huiying</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guilin</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianxing</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tenggou</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.00732</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Kbqa: Learning question answering over qa corpora and knowledge bases</title>
		<author>
			<persName><forename type="first">Wanyun</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanghua</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haixun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangqiu</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seung-Won</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the VLDB Endowment</title>
		<meeting>the VLDB Endowment</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep biaffine attention for neural dependency parsing</title>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Dozat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<title level="s">Conference Track Proceedings. Open-Review</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24">2017. 2017. April 24-26, 2017</date>
		</imprint>
	</monogr>
	<note>net</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Answering natural language questions by subgraph matching over knowledge graphs</title>
		<author>
			<persName><forename type="first">Sen</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><forename type="middle">Xu</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haixun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="824" to="837" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A statetransition framework to answer complex questions over knowledge base</title>
		<author>
			<persName><forename type="first">Sen</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinbo</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-1234</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2098" to="2108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Edg-based question decomposition for complex question answering over knowledge bases</title>
		<author>
			<persName><forename type="first">Xixin</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiheng</forename><surname>Shu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuzhong</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Semantic Web Conference</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="128" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Achille Fokoue-Nkoutche, et al. 2021. Leveraging abstract meaning representation for knowledge base question answering</title>
		<author>
			<persName><forename type="first">Pavan</forename><surname>Kapanipathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ibrahim</forename><surname>Abdelaziz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srinivas</forename><surname>Ravishankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ram√≥n</forename><surname>Fernandez Astudillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cristina</forename><surname>Cornelio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saswati</forename><surname>Dana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</title>
		<imprint>
			<biblScope unit="page" from="3884" to="3894" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Aspect based sentiment analysis semeval-2014 task 4</title>
		<author>
			<persName><surname>Dk Kirange</surname></persName>
		</author>
		<author>
			<persName><surname>Ratnadeep R Deshmukh</surname></persName>
		</author>
		<author>
			<persName><surname>Kirange</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Asian Journal of Computer Science and Information Technology (AJCSIT</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Incremental joint extraction of entity mentions and relations</title>
		<author>
			<persName><forename type="first">Qi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/P14-1038</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="402" to="412" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep-ganswer: A knowledge based question answering system</title>
		<author>
			<persName><forename type="first">Yinnian</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruoyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asia-Pacific Web (APWeb) and Web-Age Information Management (WAIM) Joint International Conference on Web and Big Data</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="434" to="439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.11692</idno>
		<title level="m">Roberta: A robustly optimized bert pretraining approach</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Relgan: Relational generative adversarial networks for text generation</title>
		<author>
			<persName><forename type="first">Weili</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nina</forename><surname>Narodytska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankit</forename><surname>Patel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019-05-06">2019. 2019. May 6-9, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Counterfactual vqa: A cause-effect look at language bias</title>
		<author>
			<persName><forename type="first">Yulei</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaihua</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiwu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian-Sheng</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="12700" to="12710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Sparql query language for rdf</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Prud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">'</forename><surname>Hommeaux</surname></persName>
		</author>
		<ptr target="http://www.w3.org/TR/rdf-sparql-query/" />
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Srinivas</forename><surname>Ravishankar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">June</forename><surname>Thai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ibrahim</forename><surname>Abdelaziz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nandana</forename><surname>Mihidukulasooriya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tahira</forename><surname>Naseem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pavan</forename><surname>Kapanipathi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaetano</forename><surname>Rossilleo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Achille</forename><surname>Fokoue</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.05825</idno>
		<title level="m">A two-stage approach towards generalization in knowledge base question answering</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multi-task learning for conversational question answering over a large-scale knowledge base</title>
		<author>
			<persName><forename type="first">Tao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiubo</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daya</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daxin</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1248</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2442" to="2451" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Lc-quad: A corpus for complex question answering over knowledge graphs</title>
		<author>
			<persName><forename type="first">Priyansh</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Maheshwari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohnish</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jens</forename><surname>Lehmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Semantic Web Conference</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="210" to="218" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Axel-Cyrille Ngonga Ngomo, Daniel Gerber, and Philipp Cimiano</title>
		<author>
			<persName><forename type="first">Christina</forename><surname>Unger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorenz</forename><surname>B√ºhmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jens</forename><surname>Lehmann</surname></persName>
		</author>
		<idno type="DOI">10.1145/2187836.2187923</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st World Wide Web Conference</title>
		<meeting>the 21st World Wide Web Conference<address><addrLine>Lyon, France</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012-04-16">2012. 2012. 2012. April 16-20, 2012</date>
			<biblScope unit="page" from="639" to="648" />
		</imprint>
	</monogr>
	<note>Template-based question answering over RDF data</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Message passing for complex question answering over knowledge graphs</title>
		<author>
			<persName><forename type="first">Svitlana</forename><surname>Vakulenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Javier</forename><surname>David Fernandez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Axel</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Polleres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>De Rijke</surname></persName>
		</author>
		<author>
			<persName><surname>Cochez</surname></persName>
		</author>
		<idno type="DOI">10.1145/3357384.3358026</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Information and Knowledge Management, CIKM 2019, Beijing</title>
		<meeting>the 28th ACM International Conference on Information and Knowledge Management, CIKM 2019, Beijing<address><addrLine>China</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2019-11-03">2019. November 3-7, 2019</date>
			<biblScope unit="page" from="1431" to="1440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Generalizing from a few examples: A survey on few-shot learning</title>
		<author>
			<persName><forename type="first">Yaqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanming</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">T</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lionel</forename><forename type="middle">M</forename><surname>Ni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM computing surveys (csur)</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="34" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Unire: A unified label space for entity relation extraction</title>
		<author>
			<persName><forename type="first">Yijun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changzhi</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanbin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="220" to="231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Transformers: State-of-the-art natural language processing</title>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lysandre</forename><surname>Debut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Sanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Chaumond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clement</forename><surname>Delangue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Moi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierric</forename><surname>Cistac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Rault</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Remi</forename><surname>Louf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Funtowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joe</forename><surname>Davison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Shleifer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clara</forename><surname>Patrick Von Platen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yacine</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Jernite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Canwen</forename><surname>Plu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Teven</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Le Scao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mariama</forename><surname>Gugger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quentin</forename><surname>Drame</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Lhoest</surname></persName>
		</author>
		<author>
			<persName><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-demos.6</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="38" to="45" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
