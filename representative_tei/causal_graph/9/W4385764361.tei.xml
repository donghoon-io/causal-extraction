<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Some General Identification Results for Linear Latent Hierarchical Causal Structure</title>
				<funder ref="#_KbsXNvv">
					<orgName type="full">National Key R&amp;D Program of China</orgName>
				</funder>
				<funder ref="#_ZmwhtmT">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_ZvCxBEY #_r8yfNue">
					<orgName type="full">Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_Jmt6yU4">
					<orgName type="full">National Science Fund for Excellent Young Scholars</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhengming</forename><surname>Chen</surname></persName>
							<email>chenzhengming1103@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Guangdong University of Technology</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Feng</forename><surname>Xie</surname></persName>
							<email>fengxie@btbu.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Applied Statistics</orgName>
								<orgName type="institution">Beijing Technology and Business University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jie</forename><surname>Qiao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Guangdong University of Technology</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhifeng</forename><surname>Hao</surname></persName>
							<email>haozhifeng@stu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Guangdong University of Technology</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">College of Science</orgName>
								<orgName type="institution">Shantou University</orgName>
								<address>
									<settlement>Shantou Guangdong</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ruichu</forename><surname>Cai</surname></persName>
							<email>cairuichu@gdut.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Guangdong University of Technology</orgName>
								<address>
									<settlement>Guangzhou</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="laboratory">Peng Cheng Laboratory</orgName>
								<address>
									<postCode>518066</postCode>
									<settlement>Shenzhen</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Some General Identification Results for Linear Latent Hierarchical Causal Structure</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We study the problem of learning hierarchical causal structure among latent variables from measured variables. While some existing methods are able to recover the latent hierarchical causal structure, they mostly suffer from restricted assumptions, including the tree-structured graph constraint, no "triangle" structure, and non-Gaussian assumptions. In this paper, we relax these restrictions above and consider a more general and challenging scenario where the beyond tree-structured graph, the "triangle" structure, and the arbitrary noise distribution are allowed. We investigate the identifiability of the latent hierarchical causal structure and show that by using second-order statistics, the latent hierarchical structure can be identified up to the Markov equivalence classes over latent variables. Moreover, some directions in the Markov equivalence classes of latent variables can be further identified using partially non-Gaussian data. Based on the theoretical results above, we design an effective algorithm for learning the latent hierarchical causal structure. The experimental results on synthetic data verify the effectiveness of the proposed method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In the traditional causal discovery task, researchers focus on discovering the causal relationships between the measured (observed) variables <ref type="bibr" target="#b10">[Pearl, 2009;</ref><ref type="bibr" target="#b14">Spirtes et al., 2000;</ref><ref type="bibr" target="#b11">Peters et al., 2017]</ref>. There have been a number of attempts to address this issue <ref type="bibr" target="#b14">[Spirtes and Glymour, 1991;</ref><ref type="bibr" target="#b3">Chickering, 2002;</ref><ref type="bibr" target="#b12">Shimizu et al., 2006;</ref><ref type="bibr" target="#b5">Hoyer et al., 2009;</ref><ref type="bibr" target="#b17">Zhang and Hyvärinen, 2009;</ref><ref type="bibr" target="#b14">Spirtes et al., 1995;</ref><ref type="bibr" target="#b0">Zhang, 2008;</ref><ref type="bibr" target="#b4">Colombo et al., 2012]</ref> (also see <ref type="bibr" target="#b14">[Spirtes and Zhang, 2016;</ref><ref type="bibr" target="#b6">Kitson et al., 2021]</ref>). However, in some empirical studies, scientists are interested in inferring causal relationships between latent (hidden) variables that they cannot directly measure, e.g., the relationship between industrialization and political democracy <ref type="bibr" target="#b1">[Bollen, 1989;</ref><ref type="bibr" target="#b0">Bartholomew et al., 2008]</ref>.</p><p>Thus, it is necessary to develop statistical methods for learning the causal relationships between latent variables.</p><p>By the measured variables that are influenced by the latent variables, much effort has been made to recover causal structure among latent variables, such as Tetrad constraint-based methods <ref type="bibr" target="#b14">[Silva et al., 2006;</ref><ref type="bibr" target="#b6">Kummerfeld et al., 2014;</ref><ref type="bibr" target="#b6">Kummerfeld and Ramsey, 2016;</ref><ref type="bibr" target="#b16">Xie et al., 2023]</ref>, non-Gaussianity based-approaches <ref type="bibr" target="#b13">[Shimizu et al., 2009;</ref><ref type="bibr" target="#b1">Cai et al., 2019;</ref><ref type="bibr" target="#b15">Xie et al., 2020;</ref><ref type="bibr" target="#b17">Zeng et al., 2021;</ref><ref type="bibr" target="#b0">Adams et al., 2021;</ref><ref type="bibr" target="#b2">Chen et al., 2022]</ref>, expansion property-based method <ref type="bibr" target="#b0">[Anandkumar et al., 2013]</ref>, copula model-based method <ref type="bibr" target="#b4">[Cui et al., 2018]</ref>, and mixture oracle-based method <ref type="bibr">[Kivva et al., 2021]</ref>. However, these methods assume that each latent variable has some certain measured variables as children and fail to work when latent variables have no measured variables.</p><p>There exist several works in the literature that tried to recover the latent hierarchical causal structure (i.e., the children of some latent variables may still be latent variables). One classical framework for inferring latent hierarchical structure is the latent tree model <ref type="bibr">[Pearl, 1988a;</ref><ref type="bibr" target="#b17">Zhang, 2004]</ref>. Many contributions along this line include <ref type="bibr" target="#b12">[Poon et al., 2010;</ref><ref type="bibr">Choi et al., 2011a;</ref><ref type="bibr" target="#b7">Mourad et al., 2013;</ref><ref type="bibr" target="#b4">Drton et al., 2017;</ref><ref type="bibr" target="#b19">Zhou et al., 2020]</ref>. However, these methods assume that the underlying structure is a tree-structured graph (there is only one path between every pair of variables in the graph). In realworld scenarios, they may not be tree-structured graphs. Recently, <ref type="bibr" target="#b15">[Xie et al., 2022]</ref> relaxed the tree-structured assumption and provided a sufficient graphical condition for identifying the latent hierarchical causal structure in the Linear Non-Gaussian Latent Hierarchical Model (LiNGLaM). They proposed a principled method to learn the latent causal structure by using the non-Gaussianity of noise variables. However, this method is inapplicable to partially Gaussian data. More recently, <ref type="bibr" target="#b6">[Huang et al., 2022]</ref> relax the non-Gaussianity assumption and provided a new sufficient condition for recovering the structure in the linear latent hierarchical structure. They proposed a rank-deficiency constraint-based method to search the latent hierarchical structure. Though the proposed method does not restrict the non-Gaussinaity assumption, it assumes that there is no "triangle" structure, i.e., the child of the latent variable must be pure (without other parents), and cannot distinguish between Markov equivalent models over latent variables. Moreover, if the data is partially Gaussian, e.g., the setting described in Fig. <ref type="figure" target="#fig_0">1</ref>, the above two methods will give incorrect or uninformative answers. We seek to find out new general identifiability conditions of linear latent hierarchical causal structure in the case where the structure is not limited to a tree-structured graph without "triangle", and moreover, the data can be partially Gaussian. Meanwhile, we develop an efficient algorithm with theoretical guarantees to answer the following questions. (1) How can we locate the latent variables and recover the causal skeleton between them only from measured variables? (2) How can we infer the causal direction among latent variables by capturing the partially non-Gaussianity? Interestingly, these questions can be well addressed under appropriate conditions, by combining the Tetrad conditions and Generalized Independent Noise (GIN) conditions in specific ways.</p><formula xml:id="formula_0">L 1 L 2 L 3 X 16 X 17 L 4 L 5 L 6 L 7 L 8 X 18 X 19 X 1 X 2 X 3 X 4 X 5 X 6 X 7 X 8 X 9 X 10 X 11 X 12 X 13 X 14 X 15</formula><p>Our contributions are summarised as follows:</p><p>• We develop new sufficient identifiability conditions that relax the existing assumptions, e.g., no "triangle" structure, and non-Gaussian assumption. • We design an algorithm that can efficiently locate latent variables and identify the latent hierarchical structure up to a Markov equivalent class by leveraging Tetrad constraints, and meanwhile, can infer the causal direction in the causal skeleton by testing GIN conditions. • We theoretically show that the proposed algorithm can find the correct hierarchical structure asymptotically under mild conditions.</p><p>2 Problem Statement</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Linear Latent Hierarchical Model</head><p>In this paper, we focus on a linear latent hierarchical causal model with graph G, where both measured (observed) variables X G and latent variable L G are generated by their latent parents in a directed acyclic graph (DAG) with the following linear structural equation models:</p><formula xml:id="formula_1">X i = ∑ L j ∈P a(X i ) β ij L j + ε X i , L j = ∑ L k ∈P a(L j ) α jk L k + ε L j ,</formula><p>(1) where β ij and α jk represent the causal strength from L j to X i and from L k to L j , respectively, and ε X i and ε L j are noise terms that are independent of each other. Without loss of generality, we assume that all variables in X G and L G have zero mean. Furthermore, Let V G denote all variables in a graph G, V, X and L be a set of variables, set of measured variable and a set of latent variables, respectively. We use P a(</p><formula xml:id="formula_2">V i ) = {V j |V j → V i }, Ch(V i ) = {V j |V i → V j }, Anc(V i ) = {V j |V j ↝ V i }, Des(V i ) = {V j |V i ↝ V j }</formula><p>to denote the set of parents, children, ancestors, descendants of V i , respectively. Definition 1 (Linear Latent Hierarchical Model (LHM)). A graphical model, with its graph</p><formula xml:id="formula_3">G = (V G , E G ), is a linear latent hierarchical model if: 1. V G = X G ∪ L G</formula><p>, where X G is the set of measured variables and L G is the set of latent variables, 2. there is at least one undirected path between every pair of variables, and 3. each variable in X G and L G are generated by the structural equation models in Eq. 1. Goal. In this paper, we aim to establish new general sufficient conditions of the latent hierarchical causal structure and also design an efficient algorithm for learning the latent hierarchical causal structure only from measured variables X G .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">General Identifiability Conditions for LHM</head><p>It is worth noting that, without further assumptions, there is no hope to locate latent variables in an LHM. Recent evidence suggests that under certain assumptions, e.g., tree-structured graph assumption <ref type="bibr">[Pearl, 1988b;</ref><ref type="bibr" target="#b17">Zhang, 2004;</ref><ref type="bibr">Choi et al., 2011b]</ref>, no "triangle" structure assumption <ref type="bibr" target="#b6">[Huang et al., 2022]</ref>, or non-Gaussian model assumption <ref type="bibr" target="#b15">[Xie et al., 2022]</ref>, the latent hierarchical structure is identifiable. However, these assumptions are restrictive and may not hold in practice, e.g., for the example given in Figure <ref type="figure" target="#fig_0">1</ref>. Below, we describe sufficient general conditions under which the LHM becomes identifiable. Specifically, the task of identifiability of LHM can be divided into two sub-problem: identifiability of causal skeleton under Irreducible condition, and identifiability of causal direction under Distribution condition, as shown in Fig. <ref type="figure" target="#fig_1">2</ref>. Before giving the details of these two identifiability conditions, we first introduce a concept, Children sets, which will be used in the identifiability condition. Definition 2 (Children Set). A variable set, denote by</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Identifiability of LHM</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Identifiability of Causal Skeleton Irreducible Condition</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Identifiability of Causal Direction Distribution Condition</head><formula xml:id="formula_4">V child , is a children set of latent variable L in a graph G if V child ⊂ Ch(L).</formula><p>We now give the condition for structural identifiability from measured variable X G .</p><p>Condition 1 (Irreducible Condition). The irreducible condition of linear latent hierarchical structure G satisfies: (1). for each latent variable L p ∈ L G , there exists a children set V child of L p with its partition</p><formula xml:id="formula_5">V child = V 1 ∪ V 2 ∪ V 3 with |V i | ≥ 1, such that (a) ∀L q ∈ L G , L q / ∈ Des(V child ), V child ⫫ {L q } ∪ Des(L q )|L p and (b) V 1 ⫫ V 2 ⫫ V 3 |L p ,</formula><p>(2). there exists a neighbour set B to L s.t. B ∩ V child = ∅ and Dim(B) ≥ 1.</p><p>The key difference to existing researchers introducing the similar "irreducible condition", such as <ref type="bibr" target="#b6">[Huang et al., 2022]</ref>, is that we relax this assumption and allow the triangle structure (i.e., children of a latent variable may not be entirely pure). Fig. <ref type="figure" target="#fig_0">1</ref> shows a simple example that satisfies the proposed irreducible condition<ref type="foot" target="#foot_1">foot_1</ref> whereas violates the "irreducible condition" given in <ref type="bibr" target="#b6">[Huang et al., 2022]</ref> (the reason is that the children of L 1 , i.e., {L 2 , L 5 , L 6 , L 7 , L 3 }, are not fully pure due to the edge L 2 → L 5 .).</p><p>The irreducible condition only ensures structural identifiability up to a Markov equivalent class, i.e., there are some edges that are undirected (see the Theorem 2). To further identify the causal direction of the undirected edge in the Markov equivalent class, the non-Gaussinaity of noise terms has been shown to be needed <ref type="bibr" target="#b12">[Shimizu et al., 2006;</ref><ref type="bibr" target="#b1">Cai et al., 2019]</ref>. However, those works need to assume that all noise terms are non-Gaussian distributions. Interestingly, we found that the non-Gaussianity assumption can be relaxed to only a (hopefully small) subset of variables that are non-Gaussian (see the Theorem 3). We refer to the Distribution condition in Condition 2. Condition 2 (Distribution Condition). For each pair of adjacent latent variables L i , L j in the causal skeleton G ′ , (1) at least one of latent variables L i , L j has non-Gaussian noise or (2) there exists a latent variable</p><formula xml:id="formula_6">L k ∈ {Anc(L i )∪Anc(L j )} that has non-Gaussian component ε such that ε is not condi- tional independent from {L i , L j } given the confounder set S = {P a(L i ) ∩ P a(L j )}, i.e., ε / ⫫ (L i , L j )|S.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Compared to existing work introducing the non-</head><p>Gaussianity assumption, such as <ref type="bibr" target="#b15">[Xie et al., 2022]</ref>, Condition 2 allows for identifying the causal direction in the more general non-Gaussian setting. For example, as shown in Fig. <ref type="figure" target="#fig_0">1</ref>, the triangle structure among L 1 , L 2 , and L 5 are identifiable even if there is only L 2 has non-Gaussian noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Algorithm for Estimating LHM</head><p>In this section, we propose a two-step efficient algorithm (Algorithm 1) to discover the structure of the linear latent hierarchical structure from measured variables. The algorithm covers two aspects of the identifiability problem that are shown in  </p><formula xml:id="formula_7">5: L, G ′ = IntroduceLatentVariables(C, G ′ ); // Step 1.2 6: A, G ′ = UpdateCausalSkeleton(L, G ′ ); //</formula><formula xml:id="formula_8">G = OrientEdges(X G , G ′ ); 10: return Graph G.</formula><p>Below, we provide the technical details of the two steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Step I: Identify Causal Skeleton</head><p>We adopt a recursive procedure to identify the causal skeleton (Step 1.1 ∼ Step 1.3). More specifically, in Step 1.1, the causal cluster will be learned from the active variable set. In Step 1.2, according to the learned causal cluster, the new latent variable set will be introduced without redundancy. In Step 1.3, the causal skeleton is reconstructed from the output of the previous phase, and the active variable is updated such that the new causal cluster can be learned in the next iteration. By repeating the above phases until no new causal cluster is learned or no new latent variable is introduced, the skeleton will be recovered (Line 3 ∼ 7 in Algorithm 1).</p><p>Before giving the details of Step I, we first give a theorem that relates the latent hierarchical structure to the Tetrad constraints over measured variables. Theorem 1 (Graphical Implication of Tetrad Constraint in LHM). Suppose G satisfies a linear latent hierarchical model and the irreducible condition holds. Then two measured variables set</p><formula xml:id="formula_9">X A and X B in G (with Dim(X A ), Dim(X B ) ≥ 2) is d-separated by a latent variable in G if and only if ∀X i , X j ∈ X A , ∀X k , X s ∈ X B , {X i , X j } and {X k , X s } follows Tetrad Constraints, i.e., σ X i X k σ X j X s = σ X i X s σ X j X k , where σ V i V j is the co-variance between V i and V j and it is not equal to zero. Example 1. Consider the structure in Fig. 1. Let X = {X 7 , X 8 , X 9 }. We can verify that ∀X i , X j ∈ X, ∀X k , X s ∈ X G \X, where X G = {X 1 , ..., X 19 }, {X i , X j } and {X k , X s } follows Tetrad Constraints. This is because L 1 d-separates X from X G \ X.</formula><p>The classical Tetrad constraint <ref type="bibr">[Spearman, 1928;</ref><ref type="bibr" target="#b12">Shafera et al., 1993]</ref> has been used in the past to investigate linear latent variable models under pure measurement model assumption <ref type="bibr" target="#b14">[Silva et al., 2006;</ref><ref type="bibr" target="#b6">Kummerfeld and Ramsey, 2016]</ref>. The current study extends the application scenario and utilizes Tetrad constraints to analyze the causal structure of the linear latent hierarchical model under some general conditions (beyond the pure measurement model).</p><p>Step 1.1: Find Causal Clusters</p><p>Step I begins with examining the existence of latent variables by identifying the causal clusters in the active variable set 2 . We denote the active variable set as A, which is set to X G initially. Next, we give the definition of the causal cluster and provide a method to identify the causal cluster from the active variable set (Proposition 1). Definition 3 (Causal Cluster &amp; Minimal Causal Cluster). Let A be the active variable set that is under investigation. We say a set C ⊂ A is a causal cluster if there exists a latent variable L such that L d-separates C from A\C. Furthermore, we say C is a minimal causal cluster if no proper subset</p><formula xml:id="formula_10">C ⊂ C (with Dim( C) ≥ 2) is a causal cluster.</formula><p>It is worth noting that for a minimal causal cluster C, if C is a pure child set of a latent variable L, then the dimension of C is two and</p><formula xml:id="formula_11">∀V i , V j ∈ C, V i ⫫ V j |L.</formula><p>To distinguish the property of the causal clusters, we give a brief definition of the pure (impure) causal cluster, which will be used in the next phase. Generally, we use the pure (impure) causal cluster to refer to any pair variable of C that is (not) d-separated by a latent variable L, i.e., C is a pure causal cluster if there exists a latent variable L such that ∀V i , V j ∈ C, V i ⫫ V j |L. For example, consider the structure in Fig. <ref type="figure" target="#fig_0">1</ref>. Suppose the active variable set</p><formula xml:id="formula_12">A = {L 2 , L 5 , L 6 , L 7 , L 8 }. C 1 = {L 6 , L 7 } is a pure causal cluster while C 2 = {L 2 , L 5 } is a impure causal cluster.</formula><p>We next show that the minimal causal cluster, fortunately, will help us to find the existence of the latent variables, which can be identified by appropriately testing for the Tetrad constraint, as formally stated in the following proposition. Proposition 1 (Identify Minimal Causal Cluster). Let A be the active variable set and C be a proper subset of A. Then C is a minimal causal cluster if and only if the following two conditions hold 1) ∀V i , V j ∈ C, ∀V k , V s ∈ A \ C, {V i , V j } and {V k , V s } follows Tetrad Constraints, and 2) no proper subset of C satisfies condition 1).</p><p>According to Proposition 1, given an active variable set, one may identify all minimal causal clusters in the current active variable set. The detailed search procedure of Step 1.1 is given in Algorithm 2 (named FindCausalClusters). Specifically, given an active variable set A, we first identify the causal cluster with size CLen = 2 based on Proposition 1 from all possible combinations. Then we increase the size of finding causal cluster C i until no causal cluster of the active variable set is found. An illustrative example is given below. Example 2. Consider the causal structure in Fig. <ref type="figure" target="#fig_0">1</ref>. Suppose active variable set A = {X 1 , ..., X 19 }. Let the size of causal cluster CLen = 2, one can find seventeen clusters, i.e., {X 1 , X 2 }, {X 1 , X 3 }, {X 2 , X 3 } ..., {X 18 , X 19 } .</p><p>Step 1.2: Introduce Latent Variables By Algorithm 2 (FindCausalClusters), we already find all minimal causal clusters in active set A. This will tell us that there exist latent variables for those minimal causal clusters. However, if we directly introduce the latent variable for the 2 We say a set is active if selected in the current iteration. learned cluster, some of the latent variables may be redundant because there are some clusters that share a common latent variable, or even the latent variable of the learned cluster is introduced in the previous iteration. To ensure the introduced latent variable is irredundant, there are two issues that we need to address.</p><p>• (Merging causal clusters): which causal clusters share a common latent variable, and</p><p>• (Identifying previously introduced latent variables): whether the latent variable of the learned cluster is introduced previously.</p><p>First issue: merging causal clusters. We now discuss the first issue. To consider all merging case that two causal cluster shares a common latent variable, we first provide a method (Proposition 2) to identify the pure (impure) causal cluster we learned, which will help us to address the different cases of merging cluster.</p><p>Proposition 2 (Identifying Pure (Impure) Cluster). Given a graph G and the active varaible set A. Suppose the irreducible condition holds. A minila causal cluster</p><formula xml:id="formula_13">C = {V i , V j } is a pure causal cluster in G if ∃V k , V s ∈ A \ C such that {V i , V k } and {V j , V s } follows the tetrad constraint, otherwise C is an impure causal cluster.</formula><p>By applying Proposition 2, the learned cluster set can be classified into the pure or impure cluster set. Based on such results, we now provide the conditions under which the clusters of variables share a common latent variable and should be merged.</p><p>Proposition 3 (Merge Cluster). Let A be the active variable set and C 1 and C 2 be two causal clusters. Then C 1 and C 2 share a common latent variable if one of the following rules hold.</p><p>• Rule 1. Both C 1 and C 2 are pure causal cluster, for ∀V i , V j ∈ C 1 and ∀V k , V s ∈ C 2 , {V i , V k } and {V j , V s } follows the Tetrad Constraint.</p><p>• Rule 2. One of the clusters is a pure cluster and the other is not, e.g., C 1 is a pure causal cluster and C 2 is an impure causal cluster, ∀V i , V j ∈ C 1 and</p><formula xml:id="formula_14">∀V k ∈ C 2 such that {V i , V k } and {V j , V s } follows Tetrad Constraint for all V s ∈ A \ {V i , V j } ∪ C 2 • Rule 3. C 1 and C 2 both are impure clusters, ∀V i ∈ C 1 and ∀V j ∈ C 2 such that {V i , V j } and {V k , V s } follows Tetrad Constraints for all V k , V s ∈ A \ C 1 ∪ C 2</formula><p>We give an example to illustrate Rule 2 of Proposition 3 using the graph in Fig. <ref type="figure" target="#fig_0">1</ref>. Example 3. Suppose the current active variable set is A = {L 2 , L 5 , L 6 , L 7 , L 3 }. By applying Algorithm 2 (FindCausal-Clusters) to A, one may learn four causal clusters : {L 2 , L 5 }, {L 6 , L 7 }, {L 6 , L 3 } and {L 3 , L 7 }. According to Rule 2 of Proposition 3, the impure causal cluster {L 2 , L 5 } (that is identified by Proposition 2) should be merged into {L 6 , L 7 } because they share a common latent variable L 1 .</p><p>Proposition 3 shows that two causal clusters that share a common latent variable can be identified by testing the proper Tetrad constraint. Thus, by checking these rules among the learning causal cluster and merging these clusters that shares a common latent variable into one causal cluster, the first issue is solved. Second issue: identifying previously introduced latent variables. Next, we consider the second issue that the latent variable of the learned cluster may be introduced in the previous iteration. An example of this issue is shown in Fig. <ref type="figure" target="#fig_0">1</ref>. For the left structure with blue ellipses, one may see that the latent variable L 2 and L 4 are introduced for the learned causal cluster {X 1 , X 2 , X 3 } and {X 16 , X 17 } in the same iteration. In the next iteration, a redundant latent variable L ′ 2 would be introduced for the learned causal cluster {L 2 , L 4 }. Interestingly, by checking the merge rule among {X 16 , X 17 } and {L 4 }, one may find that {L 4 } and {X 16 , X 17 } share a common latent variable, which can reject introducing redundant L ′ 2 and merge {L 4 } into {X 16 , X 17 } as the causal cluster of L 2 . To formalize the solution of the identifying redundant latent variables problem, we present Proposition 4 as follows. Proposition 4 (Identify Previously Introducing Latent Variables). Let L 1 be a latent variable that was introduced in previous iterations, C 2 ⊂ A be a learned cluster, where A be the active variable set in the current iteration. Suppose cluster C 1 was a causal cluster of L 1 that is found in previous iterations. Let A ′ = A ∪ C 1 \L 1 be a new active variable set, then C 1 and C 2 share the common latent parent L 1 if one of the following rules holds. // Identifying previously introducd latent variables;</p><formula xml:id="formula_15">• Rule 4. If L 1 ∈ C 2 and C ′ 2 = C 2 \ {L 1 }, Dim(C 2 ) = 1, then ∀V i , V j ∈ C 1 and V k ∈ C ′ 2 , {V i , V k } and {V j , V s } follows Tetrad constraint for all V s ∈ A ′ \C 1 ∪ C 2 . Otherwise, for Dim(C 2 ) ≥ 2,</formula><formula xml:id="formula_16">5: if ∃L j ∈ G</formula><p>′′ such that C i and L j satisfy the conditions of Proposition 4 then 6:</p><formula xml:id="formula_17">G ′ = G ′ ∪ {L j -V i |V i ∈ C i }; 7:</formula><p>else if C i is a pure cluster or merged cluster then 8:</p><p>Introduce a new latent variable L k into L;</p><p>9:</p><formula xml:id="formula_18">G ′ = G ′ ∪ {L k -V i |V i ∈ C i }; 10:</formula><p>end if 11: end for 12: return L, G</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>′</head><p>Step 1.3: Update Causal Skeleton After Algorithm 2 (FindCausalClusters) and Algorithm 3 (In-troduceLatentVariables), the number of latent variables is identified correctly. To ensure the complete causal skeleton can be identified correctly, in this phase, we deal with the following two problems: (i) reconstructing the causal skeleton from the learned causal cluster and newly introduced latent variables and (ii) updating the active variable to include the latent variable that is learned in the current iteration.</p><p>Let us consider the first problem. In the previous phase, the skeleton is constructed by adding an edge between the newly learned latent variable L and their corresponding causal cluster C (Line 7-10 in Algorithm 3). However, the reconstruction may suffer from a redundant edge problem. For example, the variable V i of an impure causal cluster may not directly connect with the corresponding latent variable. In other words, the relations across the causal cluster, including their corresponding latent variable remain unclear. To solve this problem, one efficient way is to test the d-separated relations across the causal cluster by rank constraint <ref type="bibr" target="#b14">[Silva et al., 2006;</ref><ref type="bibr" target="#b15">Xie et al., 2020;</ref><ref type="bibr" target="#b6">Huang et al., 2022]</ref> therefore correcting the edges over each latent variable and its causal cluster.</p><p>For the second problem, we consider the problem of updating the active variable to include learned latent variables such that the new latent variable can be found in the updated active variable set. The challenge is that, in a latent hierarchical structure, some children of latent variables still are latent, which hinders using the observed children of latent variables as surrogates. Thanks to the linear transitivity, we show that the observed descendent of the latent variable can also be selected as the surrogate of latent variables to update the active variable set. We provide the updated principle in the following Proposition 5. Proposition 5 (Update Active Variable Set). For a graph G, let A be the current active variable set and L be the latent variable sets discovered in the current iteration with the learned causal cluster C. if the new active variable set A ′ = A ∪ L\C, where the value of L sets to their observed descendant, then the Tetrad constraints over variables in A ′ are equal to the Tetrad constraints implied by the corresponding subgraph of G with the node set A ′ .</p><p>The above proposition shows that for the Tetrad constraints over latent variables, we can initialize the value of the latent variable with the value of any variable in its corresponding observed descendant that may be found in the previous iteration, without recovering the distribution of latent variables. We give an illustrative example as follows. Example 4. Consider the structure G in Fig. <ref type="figure" target="#fig_0">1</ref>, for the introduced latent variable L 3 , L 7 , L 6 and L 2 , L 5 , one may set the values of {L 3 , L 7 , L 6 , L 2 , L 5 } to their corresponding observed descentant {X 13 , X 10 , X 8 , X 16 , X 4 }, respectively. Then the Tetrad constraints among {X 13 , X 10 , X 8 , X 16 , X 4 } is equal to</p><formula xml:id="formula_19">{L 3 , L 7 , L 6 , L 2 , L 5 } in G.</formula><p>The solution procedure of two problems is summarized in Algorithm 4, which ensures the correct causal skeleton of linear latent hierarchical structure can be reconstructed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 4 UpdateCausalSkeleton</head><p>Input: Latent set L and skeleton G ′ from Algorithm 3 Output: Causal skeleton G ′ 1: for each learned (or updated) latent variable L i ∈ L and their causal cluster</p><formula xml:id="formula_20">C i in G ′ do 2:</formula><p>Remove the redundant edges and find the colliders structure in G ′ by testing conditional independence among {L i } ∪ C i ; 3: end for 4: apply Meek's rule to G ′ ; 5: Update active variable set according to Proposition 5; 6: return G ′</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Step II: Identify Causal Direction</head><p>As the recursive procedure (Step I) is finished, the hierarchical structure is identified up to a Markov equivalent class (see Theorem 2). There still remains an unclear identifiability problem, i.e., how to identify the causal direction among the latent variable on the hierarchical skeleton? It will be discussed under the distribution condition where the non-Gaussianity requirements are hopefully small.</p><p>We first introduce the GIN condition, which can be used to capture the partial non-Gaussianity in the linear hierarchical structure and identify the causal direction. Definition 4 (GIN condition <ref type="bibr" target="#b15">[Xie et al., 2020]</ref>). Let Y and Z be two observed random vectors. Suppose the variables follow the linear non-Gaussian acyclic causal model. Define the surrogate-variable of Y relative to Z, as</p><formula xml:id="formula_21">E Y||Z ≔ ω ⊺ Y,<label>(2)</label></formula><p>where ω satisfies ω ⊺ E[YZ ⊺ ] = 0 and ω ≠ 0. We say that (Z, Y) follows GIN condition if and only if E Y||Z is independent from Z.</p><p>Intuitively, GIN implies that 'surrogate variable' relative to Z, i.e., ω ⊺ Y, shares no common non-Gaussian exogenous noise components with Z. Based on the GIN condition, we will show that the causal direction among the latent variable is identifiable under the distribution condition. For notational convenience, we use notation GIN (L i , L j ) to show that ({X i2 }, {X i1 , X j1 }) satisfy GIN condition, i.e., E (X i1 ,X j1 )||(X i2 ) ⫫ X i2 , where {X i1 , X i2 } and {X j1 } are the measured variable of L i and L j , respectively. Furthermore, we use notation GIN (L i , L j |L k ) to show that ({X i2 , X k2 }, {X i1 , X j1 , X k1 }) satisfy GIN condition, where X k1 , X k2 are measured variables of L k .</p><p>For a skeleton of latent hierarchical structure, the undirect edges among latent variables can be divided into two cases, i.e., the edge in the pure causal cluster and the edge in the impure causal cluster. To identify these undirected edges, we give the following Proposition 6. Proposition 6 (Orientation). Suppose the distribution condition hold, for a latent variable L p and its causal cluster C = {L 1 , ..., L n }, and for each latent variable </p><formula xml:id="formula_22">L i ∈ C ∪ {L p }, let {X i , X j } be a measured variable set of L i that satisfies (1) {X i , X j } ⊂ Des(L i ) and (2) X i ⫫ X j |L i , if</formula><formula xml:id="formula_23">∀L i , L j ∈ C ∪ L p , ∃L ⊂ C ∪ L p and L ⊂ Adj(L i ) ∩ Adj(L j ), such that GIN (L i , L j |L) hold</formula><p>and GIN (L j , L i |L) does not hold, then L i → L j . Example 5. Consider the triangle structure L 1 , L 2 and L 5 in Fig. <ref type="figure" target="#fig_0">1</ref>, where the noise of L 2 and L 5 are Gaussian. Let {X 7 , X 8 } and {X 4 , X 5 } be the measured variable of L 1 and L 5 , respectively. The causal direction from L 1 to L 5 satisfies condition (2) of the distribution condition, where the non-Gaussian noise ε L 2 is absorbed into L 5 . Thus, the direction is identifiable by GIN (L 1 , L 5 ) hold and GIN (L 5 , L 1 ) does not hold.</p><p>Below, we propose the orientation algorithm that orients the undirected edges among the latent variable that satisfies the distribution condition, as shown in Algorithm 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 5 OrientEdges</head><formula xml:id="formula_24">Input: Causal skeleton G ′ and dataset X G Output: Causal structure G 1: for ∀L i ∈ G ′ and the causal cluster C of L i do 2: for ∀C i ⊂ C do 3:</formula><p>if C i is a pure causal cluster then 4:</p><p>Oriente the causal direction according to Rule 6; 5:</p><p>else if C i is an impure causal cluster then 6:</p><p>Oriente the causal direction according to Rule 7; </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Theoretical Results</head><p>In this section, we provide the theoretical results of the identification algorithm. We first show that the causal structure is identified up to a Markov equivalence class under the irreducible condition (Thm. 2), and the causal direction is identifiable under the distribution condition (Thm. 3). Then we provide the complete identifiability of LHM (Thm. 4).</p><p>Before discussing the identifiability of the algorithm, we first give the definition of the identification equivalent class.</p><p>Definition 5 (Markov Equivalence Class of LHM graphs). Two LHM graphs G 1 and G 2 are in the same Markov equivalence class iif (1) they have the same set of variables (both measured and latent variables), (2) have the same causal skeleton, and (3) have the same V-structures L i → L k ← L j among latent variables.</p><p>As we discussed in Step 1.1 ∼ 1.2, latent variables are learned correctly by finding the corresponding cluster under certain merging rules. Furthermore, in Step 1.3, we show the causal skeleton would be reconstructed from bottom to top by correcting redundant edges and updating active data. Thus, we conclude that Step I can correctly identify the Markov equivalent classes, which is given in the following theorem.</p><p>Theorem 2 (Identifiability of Causal Skeleton). Suppose G is an LHM graph with measured variables X G and irreducible condition holds, Step I of Algorithm 1 can asymptotically identify the Markov equivalence class of G.</p><p>We also provide the identification of the causal direction of this skeleton with a general distribution condition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Theorem 3 (Identifiability of Causal Direction). Given the causal skeleton G</head><p>′ of an LHM graph G, for each pair of adjacent latent variables L i , L j in the Markov equivalence class G ′ , the causal direction between L i and L j is identifiable by Step II of Algorithm 1 iif the distribution condition holds.</p><p>Combining two identifiability results, the full structure of the linear latent hierarchy is identifiable under the irreducible condition and the distribution condition.</p><p>Theorem 4 (Identifiability of LHM). Suppose G is an LHM graph with measured variables X G and the irreducible condition and the distribution condition holds. Algorithm 1 over X G can identify the correctly causal structure of G.</p><p>It is worth noting that algorithm 1 only requires Condition 1 to ensure the correctness of the learned Markov equivalent class no matter if Condition 2 is violated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Results</head><p>In this section, we applied the proposed algorithm to synthetic data to learn the latent hierarchical causal graph. Specifically, we considered different types of latent graphs and different sample sizes (with N = 2k, 5k, 10k), where structures are provided in Fig. <ref type="figure" target="#fig_5">3</ref> (Measurement Model and Latent Tree) and Fig. <ref type="figure" target="#fig_0">1</ref> (Hierarchical Model). The causal strength was generated uniformly from [-2.5, -0.5] ∪ [0.5, 2.5], and the noise term either follows a Gaussian distribution (represented by circular in the graph) or a uniform distribution U (-2, 2) X 1 X 2 X 3 X 4 X 5 X 6 X 7 X 8 X 9 X 1 X 2 X 3 X 4 X 5 X 6 X 7 X 8 X 9 X 10 X 11 X 12  (represented by the triangle in the graph). Each experiment was repeated ten times with randomly generated data. We compare our method with the hierarchical-model-based method, Latent Hierarchical Causal Structure Discovery (LHD) <ref type="bibr" target="#b6">[Huang et al., 2022]</ref> and Linear Non-Gaussian Latent Hierarchical Model (LNG) <ref type="bibr" target="#b15">[Xie et al., 2022]</ref>. Furthermore, we used the percentage of correctly identified causal clusters (RCC) <ref type="bibr" target="#b6">[Huang et al., 2022]</ref> and the F1 score over the latent structure to evaluate the performance.</p><p>The experimental results were reported in Table <ref type="table" target="#tab_3">1</ref>. Our method gives the best results on all types of graphs, indicating that it can handle not only the tree-based and measurementbased structures but also the latent hierarchical structure. The LHD method has a poor F1 score because it can not identify the causal direction between latent variables, while LNG has poor performance in two metrics because there is not enough non-Gaussianity to ensure the correctness of learned clusters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We proposed new sufficient identifiability conditions of linear latent hierarchical causal structure. Theoretically, we show that under the mild restriction of the graph structure, i.e., the irreducible condition, and partial distribution condition, the linear latent hierarchical structure is identifiable. Our theoretical results relax the application scope of the linear latent hierarchical model and contribute to the general latent structure research. Future research directions include extending the one-factor model assumption to an n-factor model setting and allowing non-linear relations, existing techniques, e.g., <ref type="bibr" target="#b6">[Kummerfeld et al., 2014;</ref><ref type="bibr" target="#b17">Zhang and Hyvärinen, 2009;</ref><ref type="bibr" target="#b14">Squires et al., 2022]</ref>, may help to mitigate this issue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contribution Statement</head><p>Authors Zhengming Chen and Feng Xie contributed equally to this work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: A hierarchical causal structure involving 8 latent variables (shaded nodes) and 19 observed variables (unshaded nodes), where the red edge represents a triangle structure and the blue ellipses represent a structure that may cause the problem of previously introducing latent variables (discuss in Section 3.1.2). Moreover, the rectangles represent node with non-Gaussian noise, while the circles represent the nodes with Gaussian noise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Identifiability of the Linear Latent hierarchical Structure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Fig. 2. Specifically, it first discovers the causal skeleton of LHM up to a Markov equivalent class in a recursive manner (Step I), and then infers the causal direction among the latent variable in the Markov equivalent class (Step II). The complete procedure is summarized in Algorithm 1. Algorithm 1 Causal Discovery in LHM Input: Data from a set of measured variables X G Output: Partial causal structure G1 1: Initialize the active variable set A ≔ X G , and G ′ = ∅; 2: // Step I: Identify Causal Skeleton 3: Begin the recursive procedure 4: C = FindCausalClusters(A); // Step 1.1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Algorithm 2</head><label>2</label><figDesc>FindCausalClusters Input: A set of active variables A Output: Causal Cluster set C 1: Let C = ∅ be a causal cluster set, and B is a copy of active variable set A; 2: Initialize the finding causal size CLen = 2; 3: for |B| ≥ CLen + 2 do 4: for draw a set of test variables C i ⊂ B with |C i | = CLen do for 13: C ← A if |C| = 0; 14: return Causal Cluster set C.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>one of the three rules in Proposition 3 between C ′ 2 and C 1 holds. • Rule 5. If L 1 / ∈ C 2 , one of the three rules in Proposition 3 holds. Based on Proposition 4, the second problem can be solved. By combining the solution to two problems, the latent vari-able can be learned correctly and irredundantly. The complete procedure of introducing latent variables for the current active variable set is summarized in Algorithm 3. Algorithm 3 IntroduceLatentVariables Input: Causal cluster set C and causal skeleton G ′ Output: Latent set L and graph G ′ 1: Initialize G ′′ = G ′ , L = ∅; 2: C ←Merge clusters from C according to Proposition 2 and Rules 1 ∼ 3 of Proposition 3; 3: for each C i ∈ C do 4:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure3: Latent structures used in our simulation studies (measurement model and latent tree respectively), where the rectangles represent non-Gaussian noise, while the circles represent Gaussian noise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Step 1.3 7: End the recursive procedure Until no causal cluster is found or new latent variable is introduced 8: // Step II: Identify Causal Direction 9:</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>•</head><label></label><figDesc>Rule 6. (Identify Causal Direction in Pure Cluster) ∀L i ∈ C, GIN (L p , L i ) hold and GIN (L i , L p ) does not hold, then L p → L i . • Rule 7. (Identify Causal Direction in Impure Cluster)</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Performance on learning different types of latent graphs.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>RCC ↑</cell><cell></cell><cell></cell><cell>F1 ↑</cell><cell></cell></row><row><cell cols="2">Algorithm</cell><cell cols="6">Ours LNG LHD Ours LNG LHD</cell></row><row><cell></cell><cell>2k</cell><cell>1.0</cell><cell>0.76</cell><cell>1.0</cell><cell>0.87</cell><cell>0.66</cell><cell>0.44</cell></row><row><cell>MM</cell><cell>5k</cell><cell>1.0</cell><cell>0.86</cell><cell>1.0</cell><cell>1.0</cell><cell>0.86</cell><cell>0.51</cell></row><row><cell></cell><cell>10k</cell><cell>1.0</cell><cell>0.93</cell><cell>1.0</cell><cell>1.0</cell><cell>0.9</cell><cell>0.51</cell></row><row><cell></cell><cell>2k</cell><cell>0.98</cell><cell>0.36</cell><cell>0.96</cell><cell>0.87</cell><cell>0.21</cell><cell>0.63</cell></row><row><cell>LT</cell><cell>5k</cell><cell>1.0</cell><cell>0.51</cell><cell>1.0</cell><cell>0.97</cell><cell>0.46</cell><cell>0.66</cell></row><row><cell></cell><cell>10k</cell><cell>1.0</cell><cell>0.6</cell><cell>1.0</cell><cell>1.0</cell><cell>0.55</cell><cell>0.66</cell></row><row><cell></cell><cell>2k</cell><cell>0.87</cell><cell>0.5</cell><cell>0.625</cell><cell>0.8</cell><cell>0.47</cell><cell>0.27</cell></row><row><cell>HM</cell><cell>5k</cell><cell>0.91</cell><cell>0.56</cell><cell>0.68</cell><cell>0.86</cell><cell>0.52</cell><cell>0.38</cell></row><row><cell></cell><cell cols="2">10k 0.96</cell><cell>0.66</cell><cell>0.75</cell><cell>0.93</cell><cell>0.58</cell><cell>0.47</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1"><p>For the latent variable L 1 , (1) there is a children set V child = {L</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_2"><p>, L 5 , L 6 , L 7 } with its partition {L 2 , L 5 }, {L 6 } and {L 7 }, and (2) there exist a neighbor set B = {L</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_3"><p>} satisfies the above two conditions.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This research was supported in part by <rs type="funder">National Key R&amp;D Program of China</rs> (<rs type="grantNumber">2021ZD0111501</rs>), <rs type="funder">National Science Fund for Excellent Young Scholars</rs> (<rs type="grantNumber">62122022</rs>), <rs type="funder">Natural Science Foundation of China</rs> (<rs type="grantNumber">61876043</rs>, <rs type="grantNumber">61976052</rs>), the major key project of PCL (PCL2021A12). FX acknowledges the support by <rs type="funder">National Natural Science Foundation of China</rs> (<rs type="grantNumber">NSFC 62006051</rs>). We appreciate the comments from anonymous reviewers, which greatly helped to improve the paper.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_KbsXNvv">
					<idno type="grant-number">2021ZD0111501</idno>
				</org>
				<org type="funding" xml:id="_Jmt6yU4">
					<idno type="grant-number">62122022</idno>
				</org>
				<org type="funding" xml:id="_ZvCxBEY">
					<idno type="grant-number">61876043</idno>
				</org>
				<org type="funding" xml:id="_r8yfNue">
					<idno type="grant-number">61976052</idno>
				</org>
				<org type="funding" xml:id="_ZmwhtmT">
					<idno type="grant-number">NSFC 62006051</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Identification of partially observed linear causal models: Graphical conditions for the non-gaussian and heterogeneous cases</title>
		<author>
			<persName><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">Daniel</forename><surname>Anandkumar</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Adel</forename><surname>Hsu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Sham</forename><surname>Javanmard</surname></persName>
		</editor>
		<editor>
			<persName><surname>Kakade</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2008">2021. 2021. 2013. 2013. 2008. 2008</date>
			<biblScope unit="page" from="249" to="257" />
		</imprint>
	</monogr>
	<note>The analysis and interpretation of multivariate data for social scientists. Routledge (2 edition</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Triad constraints for learning causal structure of latent variables</title>
		<author>
			<persName><forename type="first">Kenneth</forename><forename type="middle">A</forename><surname>Bollen</surname></persName>
		</author>
		<author>
			<persName><surname>Bollen</surname></persName>
		</author>
		<author>
			<persName><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Structural Equations with Latent Variable</title>
		<editor>
			<persName><surname>Neurips</surname></persName>
		</editor>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="1989">1989. 1989. 2019. 2019</date>
			<biblScope unit="page" from="12863" to="12872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Identification of linear latent variable model with arbitrary distribution</title>
		<author>
			<persName><forename type="first">Chen</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings 36th AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>36th AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Optimal structure identification with greedy search</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">Maxwell</forename><surname>Chickering</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Chickering</surname></persName>
		</author>
		<author>
			<persName><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">49</biblScope>
			<biblScope unit="page" from="1771" to="1812" />
			<date type="published" when="2002-11">2002. Nov. 2002. 2011. 2011</date>
		</imprint>
	</monogr>
	<note>JMLR</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning high-dimensional directed acyclic graphs with latent and selection variables</title>
		<author>
			<persName><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Fourth Conference on Uncertainty in Artificial Intelligence, UAI 2018</title>
		<editor>
			<persName><surname>Cui</surname></persName>
		</editor>
		<meeting>the Thirty-Fourth Conference on Uncertainty in Artificial Intelligence, UAI 2018</meeting>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2011">2011. 2011. 2012. 2012. 2018. 2018. 2017. 2017</date>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1202" to="1232" />
		</imprint>
	</monogr>
	<note>Bernoulli</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Nonlinear causal discovery with additive noise models</title>
		<author>
			<persName><surname>Hoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
			<biblScope unit="page" from="689" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Latent hierarchical causal structure discovery with rank constraints</title>
		<author>
			<persName><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.11415</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014th European Conference on Machine Learning and Knowledge Discovery in Databases-Volume Part II</title>
		<editor>
			<persName><surname>Kivva</surname></persName>
		</editor>
		<meeting>the 2014th European Conference on Machine Learning and Knowledge Discovery in Databases-Volume Part II<address><addrLine>Erich Kummerfeld, Joe Ramsey, Renjie Yang, Peter Spirtes, and Richard Scheines</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014">2022. 2022. 2021. 2021. 2016. 2016. 2014</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="34" to="49" />
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Advances in Neural Information Processing Systems</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A survey on latent tree models and applications</title>
		<author>
			<persName><surname>Mourad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="157" to="203" />
			<date type="published" when="2013">2013. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><surname>Pearl</surname></persName>
		</author>
		<title level="m">Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference</title>
		<meeting><address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="1988">1988. 1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><surname>Pearl</surname></persName>
		</author>
		<title level="m">Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference</title>
		<meeting><address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="1988">1988. 1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><surname>Pearl</surname></persName>
		</author>
		<title level="m">Judea Pearl. Causality: Models, Reasoning, and Inference</title>
		<meeting><address><addrLine>New York, 2nd edition</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<author>
			<persName><surname>Peters</surname></persName>
		</author>
		<title level="m">Jonas Peters, Dominik Janzing, and Bernhard Schölkopf. Elements of Causal Inference</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Variable selection in model-based clustering: to do or to facilitate</title>
		<author>
			<persName><surname>Poon</surname></persName>
		</author>
		<idno>93-68</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on International Conference on Machine Learning</title>
		<meeting>the 27th International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="1993-10">2010. 2010. 1993. 1993. Oct. 2006</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2003" to="2030" />
		</imprint>
	</monogr>
	<note type="report_type">DIMACS Technical Report</note>
	<note>A linear non-Gaussian acyclic model for causal discovery</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Estimation of linear non-gaussian acyclic models for latent factors</title>
		<author>
			<persName><surname>Shimizu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="issue">7-9</biblScope>
			<biblScope unit="page" from="2024" to="2027" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Spirtes and Glymour, 1991] Peter Spirtes and Clark Glymour. An algorithm for fast recovery of sparse causal graphs</title>
		<author>
			<persName><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Applied informatics</title>
		<imprint>
			<publisher>Peter Spirtes, Clark Glymour, and Richard Scheines. Causation, Prediction, and Search. MIT press</publisher>
			<date type="published" when="1928">2006. Feb. 2006. 1928. 1991. 2016. 2016. 1995. 1995. 2000. 2022</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="499" to="506" />
		</imprint>
	</monogr>
	<note>CLeaR</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Generalized independent noise conditionfor estimating latent variable causal graphs</title>
		<author>
			<persName><surname>Xie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020. 2020. 2022. 2022</date>
			<biblScope unit="page" from="24370" to="24387" />
		</imprint>
	</monogr>
	<note>NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Causal discovery of 1-factor measurement models in linear latent variable models with arbitrary noise distributions</title>
		<author>
			<persName><surname>Xie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note>Neurocomputing</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Causal discovery with multi-domain lingam for latent factors</title>
		<author>
			<persName><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2004">2021. 2021. 2009. 2009. 2004. 2004</date>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="697" to="723" />
		</imprint>
	</monogr>
	<note>Hierarchical latent class models for cluster analysis</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">On the completeness of orientation rules for causal discovery in the presence of latent confounders and selection bias</title>
		<author>
			<persName><forename type="first">Jiji</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">172</biblScope>
			<biblScope unit="issue">16-17</biblScope>
			<biblScope unit="page" from="1873" to="1896" />
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning mixed latent tree models</title>
		<author>
			<persName><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">249</biblScope>
			<biblScope unit="page" from="1" to="35" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
