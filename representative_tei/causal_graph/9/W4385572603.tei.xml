<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Discriminative Reasoning with Sparse Event Representation for Document-level Event-Event Relation Extraction</title>
				<funder ref="#_xVHpdVq">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_aZqmgDa">
					<orgName type="full">Singapore Ministry of Education (MOE) Academic Research Fund (AcRF)</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Changsen</forename><surname>Yuan</surname></persName>
							<email>yuanchangsen@bit.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Beijing Institute of Technology</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Heyan</forename><surname>Huang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Beijing Institute of Technology</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yixin</forename><surname>Cao</surname></persName>
							<email>caoyixin2011@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Singapore Management University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yonggang</forename><surname>Wen</surname></persName>
							<email>ygwen@ntu.edu.sg</email>
							<affiliation key="aff2">
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Discriminative Reasoning with Sparse Event Representation for Document-level Event-Event Relation Extraction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Document-level Event-Event Relation Extraction (DERE) aims to extract relations between events in a document. It challenges conventional sentence-level task (SERE) with difficult long-text understanding. In this paper, we propose a novel DERE model (SENDIR) for better document-level reasoning. Different from existing works that build an event graph via linguistic tools, SENDIR does not require any prior knowledge. The basic idea is to discriminate event pairs in the same sentence or span multiple sentences by assuming their different information density: 1) low density in the document suggests sparse attention to skip irrelevant information. Our module 1 designs various types of attention for event representation learning to capture long-distance dependence. 2) High density in a sentence makes SERE relatively easy. Module 2 uses different weights to highlight the roles and contributions of intra-and intersentential reasoning, which introduces supportive event pairs for joint modeling. Extensive experiments demonstrate great improvements in SENDIR and the effectiveness of various sparse attention for document-level representations. Codes will be released later. * Corresponding author 1 Event is defined as the trigger word in this area.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Event-Event Relation Extraction (ERE) is the task of identifying the relation between two events in texts. As shown in Figure <ref type="figure">1</ref>, for any pair of events 1 , e.g., (Services, downtime), it shall make the classifications for which relation type it holds. Clearly, event pairs may be in the same sentence (SERE) <ref type="bibr">(Kadowaki et al., 2019a;</ref><ref type="bibr" target="#b15">Liu et al., 2020;</ref><ref type="bibr">Kadowaki et al., 2019b)</ref>, or scattered across the entire document (DERE) <ref type="bibr" target="#b25">(Phu and Nguyen, 2021)</ref>. In practice, DERE can benefit a wider range of applications, such as knowledge graph construction <ref type="bibr">(Chen et al., 2019)</ref> and future event forecasting  <ref type="bibr">14,21,35,41,38 (6, 11)</ref> 6-2-Serv (6, 9)9-2-down (6,7)7-2-disrup (7, 24) 24-2-co (7, 14)14-4-dam (7, 12)12-3-dam 13-4-affected 14-4-damage</p><p>[2] Services on', "Egypt's", 'three telecom operators were disrupted in parts of Alexandria on Tuesday afternoon , and some users in Cairo experienced slower internet connections due to "downtime".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>â€¦</head><p>Figure <ref type="figure">1</ref>: Example of document-level ERE from EventStoryLine. Solid lines denote causal relations; some events and relations are omitted for clarity. The red boxes are indicator words for causal relations. <ref type="bibr" target="#b8">(Hashimoto, 2019)</ref> but it remains challenging due to the difficulty in long-text understanding.</p><p>In this paper, we propose to improve the reasoning ability among events spanning the entire document for DERE. Different from conventional methods, which build an event graph based on linguistic tools <ref type="bibr" target="#b25">(Phu and Nguyen, 2021;</ref><ref type="bibr" target="#b6">Gao et al., 2019;</ref><ref type="bibr" target="#b32">Xu et al., 2023;</ref><ref type="bibr" target="#b33">Zeng et al., 2021)</ref>, we focus more on the nature of document itself and do not rely on any prior knowledge. To do this, we highlight the following key questions:</p><p>â€¢ How to capture events' dependence that may be far away?</p><p>â€¢ Should we treat all event pairs equally considering the essential difference between SERE and DERE?</p><p>To address them, we propose a novel DERE model that learns Sparse EveNt representations for Discriminating Intraand intersentential Reasoning, namely SENDIR. Inspired by MAE <ref type="bibr" target="#b9">(He et al., 2022)</ref>, we observe a different information density between sentences and documents -for an event, most parts of the document are irrelevant, leading to a low information density. By contrast, the sentence has a high density and usually contains related words as causal indicators. As shown in Figure <ref type="figure">1</ref>, problem 3 causal ----â†’ damage 3 in the third sentence has clear causal word (due to) 2 , making the prediction much easier. While, for problem 3 causal ----â†’ damage 4 across the third and fourth sentences, there is no such pattern. This motivates us the design two modules as follows.</p><p>The goal of module 1 is to shorten the dependence distance among events to learn high-quality local and global context representations. The basic idea is to learn event-specific sentence embeddings as local features. Based on that, we further utilize sparse self-attention globally to skip irrelevant information. We have defined various types of attention masks to reflect specific dependence among sentences. In addition to conventional random sparse attention <ref type="bibr" target="#b29">(Tay et al., 2021)</ref>, we have also explored Narrative, Flashback, Globalâ†’, Globalâ†, and Banded attention, to reflect specific language bias according to human writing habits. We name this module sparse event representation learning, while these sparse attentions shall also benefit other long-text understanding tasks.</p><p>Module 2 aims to discriminate intra-and intersentential reasoning to help difficult cross-sentence events with relatively easy within-sentence-level events. As shown in Figure <ref type="figure">1</ref>, it is easy to predict (problem 3 causal ----â†’ damage 3 ) with high confidence, which is part of the path problem</p><formula xml:id="formula_0">3 causal ----â†’ damage 3 causal ----â†’ af f ected 4 causal ----â†’ damage 4</formula><p>for prediction of (problem 3 causal ----â†’ damage 4 ). Thus, for each event pair, we take the outputs of module 1 as intra-sentential features. We then enhance them with selected supportive event pairs to constitute possible reasoning chain, and utilize Gated Attention Unit (GAU) <ref type="bibr" target="#b12">(Hua et al., 2022)</ref> to conduct inter-sentential reasoning. Finally, we combine these two types of features with varying weights to differentiate their confidence and roles in ERE. Thus, we can improve the prediction of event pairs across sentences without hurting the performance of the pairs within a sentence.</p><p>We summarize the contributions as follows:</p><p>â€¢ We propose to discriminate intra-and intersentential reasoning considering the essential difference between SERE and DERE.</p><p>â€¢ We propose a novel DERE model SENDIR without any prior knowledge or external tools.</p><p>2 We use subscript to denote the sentence index.</p><p>â€¢ Experimental results on three public datasets demonstrate the effectiveness of SENDIR. Further studies also verify our proposed sparse attentions and discriminative reasoning in long-text understanding.</p><p>2 Related Work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Sentence-level ERE</head><p>Early ERE methods focus on SERE and exploit various textual features to represent the relation, such as syntactic features <ref type="bibr" target="#b31">(Venkatachalam et al., 2021;</ref><ref type="bibr" target="#b23">Ning et al., 2019)</ref>, causal patterns <ref type="bibr">(Riaz and Girju, 2010;</ref><ref type="bibr" target="#b10">Hidey and McKeown, 2016)</ref>, and statistical features of causal information <ref type="bibr">(Mirza et al., 2014;</ref><ref type="bibr" target="#b11">Hu et al., 2017;</ref><ref type="bibr" target="#b28">Tan et al., 2022)</ref>. Following the success of pre-trained language models (PLMs) <ref type="bibr" target="#b5">(Devlin et al., 2019;</ref><ref type="bibr" target="#b16">Liu et al., 2019)</ref>, recent works tend to enhance PLMs with external knowledge, so that SERE models can obtain high-quality contextualized event representations for classification. <ref type="bibr" target="#b8">Hashimoto (2019)</ref> exploited the cause and effect entities in Wikipedia and the multilingual inter-wiki links as weak supervision. <ref type="bibr">Zuo et al. (2021a)</ref> introduced external causal statements and adapted a contrastive transfer strategy to incorporate them into a target model. <ref type="bibr" target="#b1">Cao et al. (2021)</ref> utilized ConceptNet <ref type="bibr" target="#b27">(Speer et al., 2017)</ref> to learn latent structure of event causal relation, and <ref type="bibr">Zuo et al. (2021b</ref><ref type="bibr" target="#b36">Zuo et al. ( , 2020) )</ref> designed a knowledge-guided method to generate new samples based on several knowledge sources, such as WordNet <ref type="bibr" target="#b19">(Miller, 1998)</ref> and VerbNet <ref type="bibr" target="#b26">(Schuler, 2005)</ref>. Although the SERE has achieved great success, events usually scatter the entire document in real scenarios. Therefore, DERE has attracted more and more research attention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Document-level ERE</head><p>Compared with SERE, DERE has a wider range of applications but is more challenging due to the difficult long-text understanding. Thus, researchers tend to consider event-event structures for global reasoning. <ref type="bibr" target="#b6">Gao et al. (2019)</ref>  the node classification, which captures the global interactions and alleviates the spurious correlation between events. <ref type="bibr" target="#b17">Man et al. (2022)</ref> proposes to model the important context sentences to identify relations. However, to capture long-distance information, these approaches typically construct an additional document-level graph to assist global reasoning. The graph introduces unnecessary noise and decreases efficiency. Instead, we explore the information in documents without requiring prior knowledge or external tools. Certainly, our method can be further improved by incorporating structural knowledge. We leave it in the future.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>SENDIR aims to learn high-quality event representations to facilitate both intra-and inter-sentential reasoning. As shown in Figure <ref type="figure">2</ref>, our framework has four main components: Encoder to encode a document into vector, Sparse Event Representation Learning (SER) that further learns event representations based on document embeddings, Discriminating Intra-and inter sentential Reasoning (DIR) that conducts joint inference based on each pair of event representations, and Classification to make final predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Encoder</head><p>We utilize the BERT <ref type="bibr" target="#b5">(Devlin et al., 2019)</ref> with Bi-LSTM to encode the document for long documents (more than 512 tokens). Given a document D with n sentences and N events,</p><formula xml:id="formula_1">D = [X 1 , X 2 , . . . , X n ],</formula><p>and the sentence (X i = [x 1 , x 2 , . . . , x l ]) contains l words, the encoder is expressed as follows:</p><formula xml:id="formula_2">H = Bi-LSTM([s 1 , . . . , s i , . . . , s n ]) s i = BERT(X i ) ,<label>(1)</label></formula><p>where</p><formula xml:id="formula_3">H = [h 1 , h 2 , . . . , h n * l ]</formula><p>is the embedding of token, and</p><formula xml:id="formula_4">h i âˆˆ R d .</formula><p>For event e i,p , where i denotes i th event and p denotes the index of sentence, we define its embeddings as e i,p = h k , if the event mention word is x k , the position of the event in the document is k.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Sparse Event Representation Learning</head><p>SER explores various types of attention to capture long-distance dependence between sentences for high-quality document representation, which will be used to enhance the event representation. In specific, SER first learns event-specific sentence embedding as the local context. Based on them, we then apply sparse self-attention to skip irrelevant information as global contexts. Particularly, we introduce various types of long-distance dependency assumptions. Finally, we define event representations based on local and global contexts. We highlight the following differences and advantages of SER from the previous document representation:</p><p>(1) The global contexts taking sentences as nodes shortens the token-level distance between events.</p><p>(2) Well-designed sparse attention mechanism will bring useful language bias that further alleviates the difficulty of modeling long-distance dependence.</p><p>Event-specific Sentence Embedding. Given event and its sentence embeddings (i.e., e i,p and s p ), we use the event as a query and compute event-specific sentence embedding c i as follows:</p><formula xml:id="formula_5">w i = Softmax(s p e i,p ) c i = w i s p ,<label>(2)</label></formula><p>where c i is defined as local context of event e i,p , and w i is the trainable parameters. Sparse Attention Mask.</p><p>We apply selfattention technique to fuse information from the above event-specific sentence embeddings</p><formula xml:id="formula_6">C 1 = [c 1 , c 2 , . . . , c N ] âˆˆ R N Ã—d as the global context.</formula><p>Note that N is the number of events rather than sentences, because each event has its own sentence embedding, even if two events are located in the same sentence.</p><p>Inspired by <ref type="bibr" target="#b4">Child et al. (2019)</ref>, we design the sparse mask matrix to deal with the long-distance dependency issue. We have designed six types of masks: Globalâ†’, Globalâ†, Random, Banded, Narrative, and Flashback. Note that the assumptions behind them are not always true and we will give a discussion later. Their intuitive impression is shown in Figure <ref type="figure">2</ref>. Formally, we define the mask matrix as G âˆˆ R N Ã—N , where each element G i,j âˆˆ {0, 1} denotes if the information of the j th event in the q th sentence e j,q can be seen by the i th event in the p th sentence e i,p . We define the following masks:</p><p>â€¢ Globalâ†’. We assume the events in the first several sentences (e.g., the first two sentences) are core topics of the document and should see all of the other events:</p><formula xml:id="formula_7">G i,j = 1, if p &lt; 3, or q &lt; 3 0, otherwise .<label>(3)</label></formula><p>â€¢ Globalâ†. We assume the events in the last several sentences (e.g., the last two sentences) are conclusion topics of the document and should see all of the other events:</p><formula xml:id="formula_8">G i,j = 1, if p &gt; N -3, or q &gt; N -3 0, otherwise .<label>(4)</label></formula><p>â€¢ Random. Random sparse masks are usually used to increase the ability of non-local interactions-we randomly sample 20% matrix element as 0, and others are 1.</p><p>â€¢ Banded. We assume that the related information is narrowed down into neighbor sentences only. That is, each event can only see the events in neighbor sentences:</p><formula xml:id="formula_9">G i,j = 1, if |p -q| &lt; 3 0, otherwise .<label>(5)</label></formula><p>â€¢ Narrative. We assume that the events are mostly described in narrative order, so that the former event can see the latter one:</p><formula xml:id="formula_10">G i,j = 1, if q -p &gt; 0 0, otherwise .<label>(6)</label></formula><p>â€¢ Flashback. We assume that events are sequentially written, and thus the latter one should see the former one:</p><formula xml:id="formula_11">G i,j = 1, if p -q &gt; 0 0, otherwise .<label>(7)</label></formula><p>Discussion of sparsity assumptions. Due to the complexity of language, we have designed the above six types of attention masks to capture different linguistic biases. Although these masks have patterns suitable for specific settings, they also have unsuitable cases where the underlying assumptions shall fail. However, all of them are designed based on our core assumption -the information density in documents is lower than that in sentences. Thus, we capture long-distance dependence via the sparse attention mechanism. In the experiment (Section 4.7), we demonstrate that even the random attention mask can improve document-level performance, and other types of attention masks (e.g., Narrative) have achieved further improvements by introducing additional language bias. Event Representation. Given local context C and attention mask G, we now use selfattention <ref type="bibr" target="#b30">(Vaswani et al., 2017)</ref> to obtain global context, which is further combined with local context as event representations. The global context</p><formula xml:id="formula_12">C = [c 1 , c 2 , . . . , c N ] âˆˆ R N Ã—d can be computed as follows: C = Î±C (<label>8</label></formula><formula xml:id="formula_13">)</formula><p>where Î± is the sparse self-attention matrix and each element is defined as follows:</p><formula xml:id="formula_14">ï£± ï£² ï£³ att i,j = (c i W i )(c j W j ) T âˆš d * G i,j Î± i,j = exp(att i,j ) zâˆˆN -1 exp(att i,z ) , (<label>9</label></formula><formula xml:id="formula_15">)</formula><p>where d is the dimension of hidden states for scaling, W i and W j are the trainable parameters. To capture different representation subspaces, we also use multi-head attention <ref type="bibr" target="#b30">(Vaswani et al., 2017)</ref>. Now, we define the sparse contextualized event representation as follows:</p><formula xml:id="formula_16">e i = ReLU([e i , c i , c i ]W e ),<label>(10)</label></formula><p>where W e âˆˆ R 3dÃ—d is the trainable parameters, and e i âˆˆ R d . Given each pair of events (e i , e j ), we define their representation as follows 3 :</p><formula xml:id="formula_17">v i,j = ReLU([e i + e j , |e i -e j |]W c ),<label>(11)</label></formula><p>where W c âˆˆ R 2dÃ—d is the trained parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Discriminating Intra-and inter-sentential Reasoning</head><p>Section 3.2 defines event pair representations based on local and global contexts. In this section, DIR takes them as intra-sentential features, indicating that they have not considered the event pairs in other sentences to form a reasoning chain. To further obtain inter-sentential features for each pair of events, we first select supportive event pairs for each event pair and use GAU <ref type="bibr" target="#b12">(Hua et al., 2022)</ref> for information fusion. Then, we combine two types of features with different weights to differentiate two types of reasoning -the relatively easy intrasentential tasks can help cross-sentence relation identification without the loss of performance. First, instead of using all event pairs as supports, we assume that only the pairs sharing at least one common event can contribute to the reasoning chain. For example, given four events <ref type="bibr">(a, b, c, d)</ref> to predict the relation between (a, c), the event pairs (a, b) and (b, c) are clearly related, but (b, d) is clearly irrelevant. By stacking more layers, our proposed model can implicitly deal with longer reasoning chains, as any length of chains can be decomposed into several shorter chains. Take the chain a -â†’ b -â†’ c -â†’ d for predicting event pair (a, d) as an example, we indeed will use a -â†’ c and c -â†’ d as supportive evidence, while a -â†’ c shall be supported as illustrated above. This is, the longer chain has been decomposed into two sub-chains, modeled using multiple layers, and optimized jointly. Based on the assumption, we build a set of supportive event pairs for query (e i , e j ), T 1 = [v i,j , v i,1 , . . . , v N,j ]. Note that we include the representations of query event pair v i,j . Next, we utilize GAU to conduct reasoning over the sup-3 For clarity, we delete the index of the sentence. portive set as follows:</p><formula xml:id="formula_18">ï£± ï£² ï£³ T 2 = (U AV)W o U = T 1 W u , V = T 1 W v , Z = T 1 W z A = (ReLU((ZW q )(ZW k ) T + b)) 2 , (12) where W o , W u , W v , W z , W q , W k</formula><p>, and b are the trainable parameters, and T 2 = [v i,j , v i,1 , . . . , v N,j ] are the output event pair representations enhanced by reasoning chains. We take v i,j as the inter-sentential reasoning features of query (e i , e j ). Now, we are to combine two types of features with different weights. The basic idea is that event pairs within the same sentence is relatively easy to predict with high confidence, e.g., causal indicator words (e.g., due to or lead to) in Figure <ref type="figure">1</ref> provide clear patterns. We thus leverage intra-sentential features to facilitate the event pairs spanning different sentences. To avoid the harm of easier predictions, we assign higher weights to intra-sentential features if the event pair is within the same sentence. By contrast, we assign higher weights to inter-sentential features for events from different sentences to highlight the inter-sentential reasoning. Finally, the query event pair representations for relation between (e i , e j ) are defined as follows:</p><formula xml:id="formula_19">o = ï£± ï£´ ï£² ï£´ ï£³ v i,j + Î² 1 v i,j , if p -q &lt; 0( ---â†’ inter) v i,j + Î² 2 v i,j , if p -q = 0(intra) v i,j + Î² 3 v i,j , if p -q &gt; 0( â† --- inter) ,<label>(13)</label></formula><p>where Î² 1 , Î² 2 , Î² 3 are the weights that highlight different type of features for different event distribution. We determine these hyper-parameters by the heuristic experiments. Note that we separate the two cases of events in different sentences:</p><p>---â†’ inter and â† --inter.</p><p>---â†’ inter indicates that the event pairs are located in narrative order, while â† --inter denotes a flashback order. The separation considers the different attention assumptions in Section 3.2, which captures various language biases. In experiments, we indeed found such a bias that â† --inter has a high probability of negative relation, we thus set Î² 3 = 0 and Î² 1 = 0.8, Î² 2 = 0.2 heuristically, but this may be varying in different scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Classification</head><p>Given the final representation of an event pair, we use the linear function to predict relations as follows:</p><formula xml:id="formula_20">Model (%)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Intra-sentence</head><p>Inter-sentence Intra+Inter P R F1 P R F1 P R F1 LSIN <ref type="bibr" target="#b1">(Cao et al., 2021)</ref> 47.9 58.1 52.5 ------KnowDis <ref type="bibr" target="#b36">(Zuo et al., 2020)</ref> 39.7 66.5 49.7 ------LR+ <ref type="bibr" target="#b6">(Gao et al., 2019)</ref> 37.0 45.2 40.7 25.2 48.1 33.1 27.9 47.2 35.1 LIP <ref type="bibr" target="#b6">(Gao et al., 2019)</ref> 38.8 52.4 44.6 35.1 48.2 40.6 36.2 49.5 41.9 KMMG <ref type="bibr" target="#b15">(Liu et al., 2020)</ref> 41.9 62.5 50.1 ------LearnDA <ref type="bibr">(Zuo et al., 2021b)</ref> 42 Intra-sentence denotes that the event pair is in the same sentence, and inter-sentence denotes that the event pair is in different sentences.</p><formula xml:id="formula_21">Å· = Ïƒ(oW + b),<label>(14)</label></formula><p>where W and b are the trainable parameters, Å· is the probability of being positive, and Ïƒ is an activation function. For training, we adopt crossentropy as the loss function: L =e i ,e j (1y) log(1Å·) + y log(Å·) (y is the golden). We use dropout to prevent overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Metrics</head><p>To demonstrate the performance of our model, we evaluate the model on two domains three datasets. EventStoryLine 4 <ref type="bibr" target="#b22">(Mostafazadeh et al., 2016)</ref> and Causal-TimeBank 5 <ref type="bibr">(Mirza and Tonelli, 2014)</ref> are event causal relation extraction (RE) dataset, and MATRES 6 <ref type="bibr" target="#b24">(Ning et al., 2018)</ref> is event temporal RE dataset. And we use Precision (P), Recall (R), and F1-score (F1) as evaluation metrics.</p><p>EventStoryLine annotates 258 documents, 22 topics, 4, 316 sentences, 5, 334 event mentions, 7, 805 intra-sentential event pairs, and 46, 521 intersentential event pairs. Following <ref type="bibr" target="#b6">(Gao et al., 2019)</ref>, we put them in order based on their topic IDs. Causal-TimeBank (Causal-TB) annotates 184 documents, 6, 813 events, and 7, 608 event pairs. MATRES annotates 275 documents for four temporal relations, i.e., BEFORE, AFTER, EQUAL, and VAGUE. 4 <ref type="url" target="https://github.com/tommasoc80/EventStoryLine">https://github.com/tommasoc80/EventStoryLine</ref> 5 <ref type="url" target="https://github.com/paramitamirza/CATENA/tree/master/data">https://github.com/paramitamirza/CATENA/tree/master/data</ref> 6 <ref type="url" target="https://github.com/qiangning/MATRES">https://github.com/qiangning/MATRES</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Parameter Settings</head><p>We choose the most widely used BERT-base as basic PLMs, to avoid exhaustive parameter tuning. The learning rate is set to 1e -5 for pretraining and 1e -4 for others. We optimize our model with AdamW. We conduct the grid search to tune hyperparameters: the size of embedding is in {64; 128; 256; 512; 768}, where bold font denotes the best setup. The batch size for pre-trained model is set to 2. The dropout rate is set to 0.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Baseline</head><p>We compare SENDIR with state-of-the-art methods for Event Causal RE and Event Temporal RE. Event Causal RE. (1) KMMG <ref type="bibr" target="#b15">(Liu et al., 2020)</ref> that proposes a mention masking generalization and use external knowledge databases; (2) KnowDis <ref type="bibr" target="#b36">(Zuo et al., 2020)</ref> that investigates a data augmentation to solve the data lacking; (3) LSIN <ref type="bibr" target="#b1">(Cao et al., 2021)</ref> that employ ConceptNet to capture the latent causal relational structure; (4) LearnDA <ref type="bibr">(Zuo et al., 2021b)</ref> that augments data to solve the data lacking; (5) LR+ and LIP <ref type="bibr" target="#b6">(Gao et al., 2019)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Overall Performance</head><p>Table <ref type="table" target="#tab_1">1</ref> and 2 show the overall performance on EventStoryLine, Causal-TB, and MATRES, respectively. We can see that: (1) SENDIR achieves better F1 scores on EventStoryLine and Causal-TB, it also has a competitive result on MATRES, which demonstrates the effectiveness and generalization ability of our model. (2) On the MATRES, SENDIR is slightly lower than SCS-EERE. Because event temporal RE is particularly sensitive to direction between events. (3) All models perform better on intra-sentence than inter-sentence in Table <ref type="table" target="#tab_1">1</ref>. This is consistent with our claim that intra-sentence is easier to identify. (4) Particularly, SENDIR has much higher precision on intrasentence. Because the discriminative reasoning scheme alleviates the negative impacts of more difficult cross-sentence reasoning.</p><p>(5) On intersentence setting, the improvements are mainly from higher recall. We attribute this to the enhanced long-distance modeling ability and the supportive query set -it tends to find relation clues from broader contexts and other event pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Ablation Study</head><p>To further analyze SENDIR, we also conduct an ablation analysis to illustrate the effectiveness of our main modules. We show the results of the ablation study in Table <ref type="table" target="#tab_3">3</ref> <ref type="foot" target="#foot_0">foot_0</ref> . SER. We examined the impacts of SER in Table 3. w/o Sparse Att, w/o Event Repre, and w/o SER denote that we gradually remove the key designs in Section 3.2: remove sparse attention mask, use local context only as event representations, and remove the entire SER module.</p><p>(1) w/o SER. The performance becomes sharply poor without SER, especially on intra-sentence. Specifically, the experimental results are reduced by 5.3%/2.2%/2.4% F1 on intra-sentence, intersentence, and intra+inter. This demonstrates that SER can capture high-quality document representation, and intra-sentence event pairs rely more on high quality event representation, so intra-sentence drops more. (2) w/o Sparse Att. The experimental results are reduced. Specifically, the sparse attention mask has a more significant impact on intrasentence, and this sparsity is the key to improving the quality of the document representation. We will do further analysis of multiple attention masks in Section 4.7. (3) w/o Event Repre. The main performance is the decline in inter-sentence, while intra-sentence is even slightly up. We attribute the reason that the inter-sentence relies more on global context and the intra-sentence relies more on local context. Moreover, the number of inter-sentence is much more than intra-sentence. Therefore, when inter-sentence drops, results of inter+inter drops, even though intra-sentence arises.</p><p>DIR. In Table <ref type="table" target="#tab_3">3</ref>, we can also observe the following insights. Note that w/o DIR, w/o Selection, and w/o Weight denote the deletion of our entire DIR module, taking all other event pairs as supports, and regarding intra-and inter-sentential reasoning the same by using the same weight 1. decrease on inter-sentence, which is consistent with the method's motivation and validates the DIR's effectiveness.</p><p>(2) w/o Selection. The experimental results are reduced. The major reason is that considering all event pairs as candidates will increase the noisy information.</p><p>(3) w/o Weight. The drop on intra-sentence is very significant. Because of the lack of distinction in cross-sentence reasoning, the over-reasoning of simple event pairs leads to poor results. And using different weights facilitates simple tasks and global enhancements. We will follow up with a detailed discussion in Section 4.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Effects of Weights</head><p>As shown in Figure <ref type="figure" target="#fig_0">3</ref>, we show the performance of Intra+Inter with different values for Î² 1 ( ---â†’ inter) and Î² 2 (intra). The darker the color, the higher the F1 value. We did not include Î² 3 due to the serious bias in datasets (Î² 3 = 0 always leads to better results), which makes further investigation trivial. The darker the color, the higher the F1 value. We can find that: (1) The scores in the bottom right part is better than in other parts, where the weight of inter-sentence increases and the weight of intra-sentence decreases. This agrees with our assumption that inter-sentential event pairs rely more on cross-sentence reasoning than intra-sentential event pairs. (2) In general, the performance of the right part is better than the left. There are two reasons: first, as the weight of inter-sentence increases, we highlight more cross-sentence reasoning, which improves the performance of intersentence; second, there are far more inter-sentence than intra-sentence, and the improvements of intersentential results will significantly improve the overall experimental performance.</p><p>(3) The subdiagonal elements indicate that intra-sentence and inter-sentence have the same weight. Clearly, the performance is unsatisfactory. The reason is that inter-sentential event pairs are usually difficult to predict and have lower confidence than intrasentential event pairs. Thus, treating them equally negatively impacts the easier sentence-level tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Effects of Sparse Attention Mask</head><p>To investigate the impacts of various sparse attention masks on the SER, we report the results using different sparse attention masks: Narrative, Flashback, Globalâ†’, Globalâ†, Random, and Banded.</p><p>From the Figure <ref type="figure" target="#fig_2">4</ref>, we can find that: (1) On intrasentence, these sparse attention masks have similar results except for Globalâ†’. This result is consistent with the previous results that event pairs rely more on local contexts rather than long-distance dependence on global contexts.</p><p>(2) Random is unexpectedly good, indicating there is numerous redundant information in the document, and the sparse mask matrix can mitigate the effect of noise.</p><p>(3) Narrative has achieved the best performance, which reflects a language bias from human writing habits -always talk about the main topic at first.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8">Case Study</head><p>As shown in Figure <ref type="figure">5</ref>, we present a case study to better understand the effect of SENDIR compared to the baseline model (i.e., BERT). We can notice that: BERT and SENDIR can identify the intra-sentential event pair (dead causal ----â†’6.1 magnitude quake) and some easy inter-sentential event pairs (dead causal ----â†’6.1 magnitude earthquake and 6.1 magnitude quake causal ----â†’ rescue). However, for some complex event pairs that span multiple sentences, SENDIR can employ DIR to infer causal relations, but BERT fails, such as dead causal ----â†’ quake and 6.1 magnitude quake causal ----â†’ injured. These demonstrate the significant advantages of SENDIR in dealing with inter-sentential event pairs to identify hard-to-identify causal relations by cross-sentence reasoning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we exploit a novel Discriminative Reasoning with Sparse Event Representation for DERE. It can learn high-quality event representation and facilitate inter-sentential reasoning for document-level understanding. Experimental results show that our method is effective and significantly better than competitive baselines, improving inter-sentence cases without harming intrasentence event pairs. The extensive analysis also provides interesting insights about various language biases for sparse long-text representation learning. In the future, we will combine various sparse assumptions for high-quality document representations and incorporate graph reasoning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Limitation</head><p>The limitations of SENDIR include the following two points: (1) It has not extended to documentlevel entity-centric relations tasks. Our work is event-centric, and future work extends it with entity-centric cases. Document-level entity-centric RE needs to consider multiple mentions of an entity and different relations in different directions of the same entity pair. (2) It does not bring in external commonsense knowledge. Knowledge can be used to enrich events and improve the accurate ERE.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>[ 3 ]</head><label>3</label><figDesc>Vodafone Egypt said some users are continuing to face slight problems due to damage in Telecom', "Egypt's", 'cables which began in central Cairo . [4] Vodafone , along with the country's three telecom providers has been affected by a damage in one of Telecom Egypt's fiber cables â€¦</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Impacts of the weight (Î²) on the EventStory-Line. Weight (Inter) is the weight of ---â†’ inter.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Impacts of Sparse Attention Mask on the EventStoryLine.</figDesc><graphic coords="8,100.42,80.87,138.68,138.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>[ 0 ]Figure 5 :</head><label>05</label><figDesc>Figure5: The case study of our proposed SENDIR and BERT models on EventStoryLine, where GT denotes ground truth. Red numbers are the sentence numbers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>6 Sparse Event Representation Learning Discriminating Intra-and inter-sentential Reasoning</head><label></label><figDesc></figDesc><table><row><cell>Document</cell><cell>ğ‘‹ğ‘‹ 0 ğ‘‹ğ‘‹ 1 ğ‘‹ğ‘‹ ğ‘›ğ‘›</cell><cell>â‹®</cell><cell>Encoder</cell><cell></cell><cell></cell><cell></cell><cell cols="4">Event c ğ‘–ğ‘– Sentence</cell><cell></cell><cell></cell><cell>ğ‘½ğ‘½ ğ‘²ğ‘² ğ‘¸ğ‘¸</cell><cell>ğ‘®ğ‘®</cell><cell>â‹®</cell><cell>ğ‘£ğ‘£</cell><cell cols="2">ğ’—ğ’— Indonesia,dead â‹® ğ’—ğ’— Indonesia,rescue ğ’—ğ’— landslides,dead ğ’—ğ’— rescue,dead</cell><cell>GAU</cell><cell>ğ’—ğ’— ğ’—ğ’— â€²</cell><cell>â‹®</cell><cell>ğ‘œğ‘œ</cell><cell>Classification</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>ğ’—ğ’— ğŸğŸ,ğŸğŸ</cell><cell cols="2">ğ’—ğ’— ğŸğŸ,ğŸğŸ ğ’—ğ’— ğŸğŸ,ğŸ‘ğŸ‘</cell><cell>â‹¯</cell><cell>ğ‘£ğ‘£ ğŸ’ğŸ’,ğŸğŸ ğ’—ğ’— ğŸ“ğŸ“,ğŸğŸ</cell><cell>ğ‘œğ‘œ</cell></row><row><cell>Flashback</cell><cell></cell><cell>Narrative</cell><cell>0</cell><cell>1</cell><cell>1</cell><cell>2</cell><cell></cell><cell>3</cell><cell>4</cell><cell>2</cell><cell>5</cell><cell>6</cell><cell>3</cell><cell>ğ‘£ğ‘£ 0,2</cell><cell></cell><cell></cell><cell>â‹¯</cell><cell>â‹¯</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>ğ‘£ğ‘£ 0,3</cell><cell></cell><cell></cell><cell>â‹¯</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0</cell><cell>1</cell><cell>1</cell><cell>2</cell><cell></cell><cell>3</cell><cell>4</cell><cell>2</cell><cell>5</cell><cell>6</cell><cell>3</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>ğ›½ğ›½1</cell><cell>inter</cell></row><row><cell>Globalïƒ </cell><cell></cell><cell>GlobalïƒŸ</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>â‹®</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>ğ‘£ğ‘£ 0,4</cell><cell></cell><cell></cell><cell>â‹¯</cell><cell>ğ›½ğ›½</cell><cell>ğ›½ğ›½2</cell><cell>intra</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>â‹®</cell><cell></cell><cell></cell><cell>â‹®</cell></row><row><cell>Random</cell><cell></cell><cell>Banded</cell><cell>0</cell><cell>1</cell><cell>1</cell><cell>2</cell><cell>â‹®</cell><cell>3</cell><cell>4</cell><cell>2</cell><cell>5</cell><cell></cell><cell>3</cell><cell>ğ‘£ğ‘£ 5,4</cell><cell></cell><cell></cell><cell>â‹¯</cell><cell>ğ’—ğ’—</cell><cell>ğ’—ğ’— â€²</cell><cell>ğ›½ğ›½3</cell><cell>inter</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Event</cell><cell></cell><cell></cell><cell cols="4">First Sentence</cell><cell></cell><cell cols="2">Second Sentence</cell><cell cols="3">Third Sentence</cell><cell>&amp;</cell><cell>Event Representation</cell></row><row><cell cols="19">Figure 2: Architecture of proposed model. The white dotted box is zero embedding, which can supplement matrices.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>used integer linear pro-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">gramming (ILP) to model causal information by</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">designing constraints and modifying the objective</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">function to encourage causal structures and discour-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">age the opposite. To build the connections among</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">the events, Phu and Nguyen (2021) designed var-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">ious document-level graphs and used graph con-</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">volutional networks to learn structure-preserved</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">features. Chen et al. (2022) proposed to build an</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="4">event pair relational graph and converted DERE to</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>Main results on the EventStoryLine. The best results are in bold and the second-best results are underlined.</figDesc><table><row><cell>.2 69.8 52.6</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell cols="7">RichGCN (Phu and Nguyen, 2021) 49.2 63.0 55.2 39.2 45.7 42.2 42.6 51.3 46.6 ERGO (Chen et al., 2022) 49.7 72.6 59.0 43.2 48.8 45.8 46.3 50.1 48.1 SENDIR 65.8 66.7 66.2 33.0 90.0 48.3 37.8 82.8 51.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>The results on the Causal-TB and MATRES.</figDesc><table><row><cell>that models rich causal structures via</cell></row><row><cell>designing constraints and objection function; (6)</cell></row><row><cell>RichGCN (Phu and Nguyen, 2021) that builds multi-level graphs to capture structure-preserved</cell></row><row><cell>features; (6) ERGO (Chen et al., 2022) that de-signs an event relational graph and converts the</cell></row><row><cell>event causal identify to a node classification frame-</cell></row><row><cell>work.</cell></row><row><cell>Event Temporal RE. (1) DEER (Han et al., 2020) that constructs many training samples to simulate</cell></row><row><cell>the machine reading comprehension for event tem-</cell></row><row><cell>poral understanding; (2) SMTL (Ballesteros et al.,</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>An ablation study for our model on the EventStoryLine.</figDesc><table><row><cell cols="4">Model (F1 %) Intra Inter Intra+Inter SENDIR 66.2 48.3 51.9</cell></row><row><cell>w/o SER</cell><cell>60.9</cell><cell>46.1</cell><cell>49.5</cell></row><row><cell>w/o Sparse Att</cell><cell>61.6</cell><cell>48.1</cell><cell>50.9</cell></row><row><cell cols="2">w/o Event Repre 66.9</cell><cell>46.2</cell><cell>50.2</cell></row><row><cell>w/o DIR</cell><cell>64.0</cell><cell>44.9</cell><cell>48.5</cell></row><row><cell>w/o Selection</cell><cell>66.0</cell><cell>46.5</cell><cell>50.5</cell></row><row><cell>w/o Weight (Î²)</cell><cell>63.9</cell><cell>45.7</cell><cell>49.3</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_0"><p>Intra and inter indicate intra-sentence and inter-sentence setting, respectively.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>Mehwish Riaz and Roxana Girju. 2010. Another look at causality: Discovering scenario-specific contingency relationships with no supervision. In Proceedings of ICSC, pages 361-368. IEEE Computer Society.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_2"><p>The Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing assistance.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>We appreciate the comments from anonymous reviewers which will help further improve our work.This study is supported by <rs type="funder">National Natural Science Foundation of China</rs> (No.<rs type="grantNumber">U19B2020</rs>), <rs type="funder">Singapore Ministry of Education (MOE) Academic Research Fund (AcRF)</rs> Tier 1 grant, and <rs type="projectName">Beijing Institute of Technology Southeast Academy of Information Technology</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_xVHpdVq">
					<idno type="grant-number">U19B2020</idno>
				</org>
				<org type="funded-project" xml:id="_aZqmgDa">
					<orgName type="project" subtype="full">Beijing Institute of Technology Southeast Academy of Information Technology</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified? For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)? No response.</p><p>B4. Did you discuss the steps taken to check whether the data that was collected / used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect / anonymize it? No response.</p><p>B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.? No response.</p><p>B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits, etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the number of examples in train / validation / test splits, as these provide necessary context for a reader to understand experimental results. For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not be. No response.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Did you run computational experiments?</head><p>Section 4 C1. Did you report the number of parameters in the models used, the total computational budget (e.g., GPU hours), and computing infrastructure used? Section 4.2 C2. Did you discuss the experimental setup, including hyperparameter search and best-found hyperparameter values? Section 4 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary statistics from sets of experiments), and is it transparent whether you are reporting the max, mean, etc. or just a single run? Section 4</p><p>C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE, etc.)? Section 4 D Did you use human annotators (e.g., crowdworkers) or research with human participants? Left blank.</p><p>D1. Did you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators, etc.? No response.</p><p>D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students) and paid participants, and discuss if such payment is adequate given the participants' demographic (e.g., country of residence)? No response.</p><p>D3. Did you discuss whether and how consent was obtained from people whose data you're using/curating? For example, if you collected data via crowdsourcing, did your instructions to crowdworkers explain how the data would be used? No response.</p><p>D4. Was the data collection protocol approved (or determined exempt) by an ethics review board? No response.</p><p>D5. Did you report the basic demographic and geographic characteristics of the annotator population that is the source of the data? No response.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Severing the edge between before and after: Neural architectures for temporal ordering of events</title>
		<author>
			<persName><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rishita</forename><surname>Anubhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nima</forename><surname>Pourdamghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yogarshi</forename><surname>Vyas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parminder</forename><surname>Bhatia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathleen</forename><forename type="middle">R</forename><surname>Mckeown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaser</forename><surname>Al-Onaizan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.436</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-11-16">2020. 2020. November 16-20, 2020</date>
			<biblScope unit="page" from="5412" to="5417" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Knowledge-enriched event causality identification via latent structure induction networks</title>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yubo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuguang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Peng</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.376</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL/IJCNLP</title>
		<meeting>ACL/IJCNLP</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4862" to="4872" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">ERGO: event relational graph transformer for document-level event causality identification</title>
		<author>
			<persName><forename type="first">Meiqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kunquan</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mukai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.48550/arXiv.2204.07434</idno>
		<idno>CoRR, abs/2204.07434</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Uhop: An unrestricted-hop relation extraction framework for knowledge-based question answering</title>
		<author>
			<persName><forename type="first">Chih-Hung</forename><surname>Zi-Yuan Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi-Pei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jijnasa</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lun-Wei</forename><surname>Nayak</surname></persName>
		</author>
		<author>
			<persName><surname>Ku</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n19-1031</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="345" to="356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Generating long sequences with sparse transformers</title>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno>CoRR, abs/1904.10509</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">BERT: pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Modeling document-level causal structures for event causal relation identification</title>
		<author>
			<persName><forename type="first">Lei</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Kumar Choubey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruihong</forename><surname>Huang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/n19-1179</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1808" to="1817" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">DEER: A data efficient language model for event temporal reasoning</title>
		<author>
			<persName><forename type="first">Rujun</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<idno>CoRR, abs/2012.15283</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Weakly supervised multilingual causality extraction from wikipedia</title>
		<author>
			<persName><forename type="first">Chikara</forename><surname>Hashimoto</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1296</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP-IJCNLP</title>
		<meeting>EMNLP-IJCNLP</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2986" to="2997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Masked autoencoders are scalable vision learners</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinlei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanghao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><surname>DollÃ¡r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR52688.2022.01553</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022</title>
		<meeting><address><addrLine>New Orleans, LA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022-06-18">2022. June 18-24, 2022</date>
			<biblScope unit="page" from="15979" to="15988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Identifying causal relations using parallel wikipedia articles</title>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Hidey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kathy</forename><surname>Mckeown</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/p16-1135</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL. The Association for Computer Linguistics</title>
		<meeting>ACL. The Association for Computer Linguistics</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Inference of fine-grained event causality from blogs and films</title>
		<author>
			<persName><forename type="first">Zhichao</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elahe</forename><surname>Rahimtoroghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marilyn</forename><forename type="middle">A</forename><surname>Walker</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/w17-2708</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Events and Stories in the News Workshop@ACL 2017</title>
		<meeting>the Events and Stories in the News Workshop@ACL 2017<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017-08-04">2017. August 4, 2017</date>
			<biblScope unit="page" from="52" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Transformer quality in linear time</title>
		<author>
			<persName><forename type="first">Weizhe</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName><surname>Le</surname></persName>
		</author>
		<idno>CoRR, abs/2202.10447</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Event causality recognition exploiting multiple annotators&apos; judgments and background knowledge</title>
		<author>
			<persName><forename type="first">Kazuma</forename><surname>Kadowaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryu</forename><surname>Iida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kentaro</forename><surname>Torisawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong-Hoon</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Kloetzer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1590</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP-IJCNLP</title>
		<meeting>EMNLP-IJCNLP</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5815" to="5821" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Event causality recognition exploiting multiple annotators&apos; judgments and background knowledge</title>
		<author>
			<persName><forename type="first">Kazuma</forename><surname>Kadowaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryu</forename><surname>Iida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kentaro</forename><surname>Torisawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong-Hoon</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Kloetzer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1590</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EMNLP-IJCNLP</title>
		<meeting>EMNLP-IJCNLP<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5816" to="5822" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Knowledge enhanced event causality identification with mention masking generalizations</title>
		<author>
			<persName><forename type="first">Jian</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yubo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2020/499</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI</title>
		<meeting>IJCAI</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3608" to="3614" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Roberta: A robustly optimized BERT pretraining approach</title>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno>CoRR, abs/1907.11692</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Selecting optimal context sentences for event-event relation extraction</title>
		<author>
			<persName><surname>Hieu Man</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trung</forename><surname>Nghia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linh</forename><forename type="middle">Ngo</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thien Huu</forename><surname>Van</surname></persName>
		</author>
		<author>
			<persName><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Sixth AAAI Conference on Artificial Intelligence, AAAI 2022, Thirty-Fourth Conference on Innovative Applications of Artificial Intelligence, IAAI 2022, The Twelveth Symposium on Educational Advances in Artificial Intelligence, EAAI 2022 Virtual Event</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2022-02-22">2022. February 22 -March 1, 2022</date>
			<biblScope unit="page" from="11058" to="11066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">TIMERS: document-level temporal relation extraction</title>
		<author>
			<persName><forename type="first">Puneet</forename><surname>Mathur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajiv</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franck</forename><surname>Dernoncourt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vlad</forename><forename type="middle">I</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quan</forename><surname>Hung Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinesh</forename><surname>Manocha</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-short.67</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021-08-01">2021. August 1-6, 2021</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="524" to="533" />
		</imprint>
	</monogr>
	<note>Short Papers), Virtual Event</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">WordNet: An electronic lexical database</title>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Annotating causality in the TempEval-3 corpus</title>
		<author>
			<persName><forename type="first">Paramita</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rachele</forename><surname>Sprugnoli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sara</forename><surname>Tonelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manuela</forename><surname>Speranza</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/W14-0702</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of EACL)</title>
		<meeting>EACL)<address><addrLine>Gothenburg, Sweden</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="10" to="19" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">An analysis of causality between events and its relation to temporal information</title>
		<author>
			<persName><forename type="first">Paramita</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sara</forename><surname>Tonelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLING 2014, 25th International Conference on Computational Linguistics, Proceedings of the Conference: Technical Papers</title>
		<meeting><address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>ACL</publisher>
			<date type="published" when="2014-08-23">2014. August 23-29, 2014</date>
			<biblScope unit="page" from="2097" to="2106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Caters: Causal and temporal relation scheme for semantic annotation of event structures</title>
		<author>
			<persName><forename type="first">Nasrin</forename><surname>Mostafazadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alyson</forename><surname>Grealish</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathanael</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">F</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucy</forename><surname>Vanderwende</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W16-1007</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Workshop on Events, EVENTS@HLT-NAACL 2016</title>
		<meeting>the Fourth Workshop on Events, EVENTS@HLT-NAACL 2016<address><addrLine>San Diego, California, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016-06-17">2016. June 17, 2016</date>
			<biblScope unit="page" from="51" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">An improved neural baseline for temporal relation extraction</title>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1642</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019-11-03">2019. November 3-7, 2019</date>
			<biblScope unit="page" from="6202" to="6208" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A multiaxis annotation scheme for event temporal relations</title>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1122</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018-07-15">2018. July 15-20, 2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1318" to="1328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Graph convolutional networks for event causality identification with rich document-level structures</title>
		<author>
			<persName><forename type="first">Minh</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phu</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Thien</forename><surname>Huu Nguyen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.273</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3480" to="3490" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">VerbNet: A broadcoverage, comprehensive verb lexicon</title>
		<author>
			<persName><forename type="first">Karin Kipper</forename><surname>Schuler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
		<respStmt>
			<orgName>University of Pennsylvania</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Conceptnet 5.5: An open multilingual graph of general knowledge</title>
		<author>
			<persName><forename type="first">Robyn</forename><surname>Speer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Chin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Catherine</forename><surname>Havasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI</title>
		<meeting>AAAI</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4444" to="4451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Fiona Anting</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>HÃ¼rriyetoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommaso</forename><surname>Caselli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nelleke</forename><surname>Oostdijk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tadashi</forename><surname>Nomoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hansi</forename><surname>Hettiarachchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iqra</forename><surname>Ameer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Onur</forename><surname>Uca</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Farhana</forename><surname>Ferdousi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liza</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Tiancheng</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.11714</idno>
		<title level="m">The causal news corpus: Annotating causal relations in event sentences from news</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Synthesizer: Rethinking self-attention for transformer models</title>
		<author>
			<persName><forename type="first">Yi</forename><surname>Tay</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dara</forename><surname>Bahri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Metzler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Da-Cheng</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Che</forename><surname>Zheng</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning Research</title>
		<meeting>Machine Learning Research</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="10183" to="10192" />
		</imprint>
	</monogr>
	<note>Proceedings of ICML</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Long Beach, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-09">2017. 2017. December 4-9, 2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">SERC: syntactic and semantic sequence based event relation classification</title>
		<author>
			<persName><forename type="first">Kritika</forename><surname>Venkatachalam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raghava</forename><surname>Mutharaju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sumit</forename><surname>Bhatia</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICTAI52525.2021.00208</idno>
	</analytic>
	<monogr>
		<title level="m">33rd IEEE International Conference on Tools with Artificial Intelligence, ICTAI 2021</title>
		<meeting><address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021-11-01">2021. November 1-3, 2021</date>
			<biblScope unit="page" from="1316" to="1321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Document-level relation extraction with path reasoning</title>
		<author>
			<persName><forename type="first">Wang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kehai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiejun</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.1145/3572898</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Asian Low Resour. Lang. Inf. Process</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">SIRE: separate intra-and inter-sentential reasoning for document-level relation extraction</title>
		<author>
			<persName><forename type="first">Shuang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuting</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-acl.47</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021</title>
		<imprint>
			<date type="published" when="2021-08-01">2021. August 1-6, 2021</date>
			<biblScope unit="page" from="524" to="534" />
		</imprint>
	</monogr>
	<note>ACL/IJCNLP 2021 of Findings of ACL. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">2021a. Improving event causality identification via selfsupervised representation learning on external causal statement</title>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yubo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuguang</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-acl.190</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL/IJCNLP 2021</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2021">August 1-6, 2021</date>
			<biblScope unit="page" from="2162" to="2172" />
		</imprint>
	</monogr>
	<note>ACL/IJCNLP 2021 of Findings of ACL</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">2021b. Learnda: Learnable knowledge-guided data augmentation for event causality identification</title>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yubo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuguang</forename><surname>Chen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.276</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACL/IJCNLP</title>
		<meeting>ACL/IJCNLP</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="3558" to="3571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Knowdis: Knowledge enhanced data augmentation for event causality detection via distant supervision</title>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yubo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.coling-main.135</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING</title>
		<meeting>COLING</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1544" to="1550" />
		</imprint>
	</monogr>
	<note>International Committee on Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
