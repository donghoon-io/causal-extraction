<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Extended Gray-Wyner System with Complementary Causal Side Information</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2017-01-12">12 Jan 2017</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ting</forename><surname>Cheuk</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Abbas</forename><forename type="middle">El</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><surname>Gamal</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical Engineering</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Extended Gray-Wyner System with Complementary Causal Side Information</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2017-01-12">12 Jan 2017</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:1701.03207v1[cs.IT]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:26+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Gray-Wyner system</term>
					<term>side information</term>
					<term>complementary delivery</term>
					<term>Körner graph entropy</term>
					<term>privacy funnel</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We establish the rate region of an extended Gray-Wyner system for 2-DMS (X, Y ) with two additional decoders having complementary causal side information. This extension is interesting because in addition to the operationally significant extreme points of the Gray-Wyner rate region, which include Wyner's common information, Gács-Körner common information and information bottleneck, the rate region for the extended system also includes the Körner graph entropy, the privacy funnel and excess functional information, as well as three new quantities of potential interest, as extreme points. To simplify the investigation of the 5-dimensional rate region of the extended Gray-Wyner system, we establish an equivalence of this region to a 3-dimensional mutual information region that consists of the set of all triples of the form (I(X; U ), I(Y ; U ), I(X, Y ; U )) for some p U |X,Y . We further show that projections of this mutual information region yield the rate regions for many settings involving a 2-DMS, including lossless source coding with causal side information, distributed channel synthesis, and lossless source coding with a helper.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>The lossless Gray-Wyner system <ref type="bibr" target="#b0">[1]</ref> is a multi-terminal source coding setting for two discrete memoryless source (2-DMS) (X, Y ) with one encoder and two decoders. This setup draws some of its significance from providing operational interpretation for several information theoretic quantities of interest, namely Wyner's common information <ref type="bibr" target="#b1">[2]</ref>, the Gács-Körner common information <ref type="bibr" target="#b2">[3]</ref>, the necessary conditional entropy <ref type="bibr" target="#b3">[4]</ref>, and the information bottleneck <ref type="bibr" target="#b4">[5]</ref>.</p><p>In this paper, we consider an extension of the Gray-Wyner system (henceforth called the EGW system), which includes two new individual descriptions and two decoders with causal side information as depicted in Figure <ref type="figure" target="#fig_3">1</ref>. The encoder maps sequences from a 2-DMS (X, Y ) into five indices M i ∈ [1 : 2 nRi ], i = 0, . . . , 4. Decoders 1 and 2 correspond to those of the Gray-Wyner system, that is, decoder 1 recovers X n from (M 0 , M 1 ) and decoder 2 recovers Y n from (M 0 , M 2 ). At time i ∈ [1 : n], decoder 3 recovers X i causally from (M 0 , M 3 , Y i ) and decoder 4 similarly recovers Y i causally from (M 0 , M 4 , X i ). Note that decoders 3 and 4 correspond to those of the complementary delivery setup studied in <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref> with causal (instead of noncausal) side information and with two additional private indices M 3 and M 4 . This extended Gray-Wyner system setup is lossless, that is, the decoders recover their respective source sequences with probability of error that vanishes as n approaches infinity. The rate region R of the EGW system is defined in the usual way as the closure of the set of achievable rate tuples (R 0 , R 1 , R 2 , R 3 , R 4 ).</p><p>The first contribution of this paper is to establish the rate region of the EGW system. Moreover, to simplify the study of this rate region and its extreme points, we show that it is equivalent to the 3-dimensional mutual information region for (X, Y ) defined as</p><formula xml:id="formula_0">I XY = p U |XY {(I(X; U ), I(Y ; U ), I(X, Y ; U ))} ⊆ R 3<label>(1)</label></formula><p>in the sense that we can express R using I and vice versa. As a consequence and of particular interest, the extreme points of the rate region R (and its equivalent mutual information region I XY ) for the EGW system include, in addition to the aforementioned extreme points of the Gray-Wyner system, the Körner graph entropy <ref type="bibr" target="#b7">[8]</ref>, privacy funnel <ref type="bibr" target="#b8">[9]</ref> and excess functional information <ref type="bibr" target="#b9">[10]</ref>, as well as three new quantities with interesting operational meaning, which we refer to as the maximal interaction information, the asymmetric private interaction information, and the symmetric private interaction information. These extreme points can be cast as maximizations of the interaction information <ref type="bibr" target="#b10">[11]</ref> I(X; Y |U ) -I(X; Y ) under various constraints. They can be considered as distances from extreme dependency, as they are equal to zero only under certain conditions of extreme dependency. In addition to providing operational interpretations to these information theoretic quantities, projections of the mutual information region yield the rate regions for many settings involving a 2-DMS, including lossless source coding with causal side information <ref type="bibr" target="#b11">[12]</ref>, distributed channel synthesis <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref>, and lossless source coding with a helper <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>.</p><p>A related extension of lossy Gray-Wyner system with two decoders with causal side information was studied by Timo and Vellambi <ref type="bibr" target="#b17">[18]</ref>. If we only consider decoders 3 and 4 in EGW, then it can be considered as a special case of their setting PSfrag replacements (where the side information does not need to be complementary). Other related source coding setups to the EGW can be found in <ref type="bibr" target="#b18">[19]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>. A related 3-dimensional region, called the region of tension, was investigated by Prabhakaran and Prabhakaran <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b23">[24]</ref>. We show that this region can be obtained from the mutual information region, but the other direction does not hold in general.</p><formula xml:id="formula_1">X n , Y n Y i X i</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Encoder</head><p>In the following section, we establish the rate region of the EGW system, relate it to the mutual information region, and show that the region of the original Gray-Wyner system and the region of tension can be obtained from the mutual information region. In Section III, we study the extreme points of the mutual information region. In Section IV we establish the rate region for the same setup as the EGW system but with noncausal instead of causal side information at decoders 3 and 4. We show that the rate region of the noncausal EGW can be expressed in terms of the Gray-Wyner region, hence it does not contain as many interesting extreme points as the causal EGW. Moreover, we show that this region is equivalent to the closure of the limit of the mutual information region for (X n , Y n ) as n approaches infinity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Notation</head><p>Throughout this paper, we assume that log is base 2 and the entropy H is in bits. We use the notation: X b a = (X a , . . . , X b ),</p><formula xml:id="formula_2">X n = X n 1 and [a : b] = [a, b] ∩ Z.</formula><p>For discrete X, we write the probability mass function as p X . For A ⊆ R n , we write the closure of A as cl(A) and the convex hull as conv(A). We write the support function as</p><formula xml:id="formula_3">ψ A (b) = sup a T b : a ∈ A .</formula><p>We write the one-sided directional derivative of the support function as</p><formula xml:id="formula_4">ψ ′ A (b; c) = lim t→0 + 1 t (ψ A (b + tc) -ψ A (b)) .</formula><p>Note that if A is compact and convex, then</p><formula xml:id="formula_5">ψ ′ A (b; c) = max d T c : d ∈ arg max a∈A a T b .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RATE REGION OF EGW AND THE MUTUAL INFORMATION REGION</head><p>The rate region of the EGW system is given in the following.</p><p>Theorem 1. The rate region the EGW system R is the set of rate tuples</p><formula xml:id="formula_6">(R 0 , R 1 , R 2 , R 3 , R 4 ) such that R 0 ≥ I(X, Y ; U ), R 1 ≥ H(X |U ), R 2 ≥ H(Y |U ), R 3 ≥ H(X |Y, U ), R 4 ≥ H(Y |X, U )</formula><p>for some p U|XY , where</p><formula xml:id="formula_7">|U| ≤ |X | • |Y| + 2.</formula><p>Note that if we ignore decoders 3 and 4, i.e., let R 3 , R 4 be sufficiently large, then this region reduces to the Gray-Wyner region.</p><p>Proof: The converse proof is quite straightforward and is given in Appendix A for completion. We now prove the achievability. Codebook generation. Fix p U|XY and randomly and independently generate 2 nR0 sequences u n (m 0 ), m 0 ∈ [1 : 2 nR0 ], each according to</p><formula xml:id="formula_8">n i=1 p U (u i ). Given u n (m 0 ), assign indices m 1 ∈ [1 : 2 nR1 ], m 2 ∈ [1 : 2 nR2</formula><p>] to the sequences in the conditional typical sets T Encoding. To encode the sequence x n , y n , find m 0 such that (u n (m 0 ), x n , y n ) ∈ T (n) ǫ is jointly typical, and find indices</p><formula xml:id="formula_9">m 1 , m 2 of x n , y n in T (n) ǫ (X|u n (m 0 )) and T (n) ǫ (Y |u n (m 0 )) given u n (m 0 ).</formula><p>For each x, y, let x n y,u be the subsequence of x n where x i is included if and only if y i = y and u i (m 0 ) = u. Note that since (u n (m 0 ),</p><formula xml:id="formula_10">y n ) ∈ T (n) ǫ</formula><p>, the length of x n y,u is not greater than n(1 + ǫ)p Y U (y, u). We then find an index m 3,y,u of xn(1+ǫ)pY U (y,u)</p><formula xml:id="formula_11">y,u ∈ T n(1+ǫ)pY U (y,u) ǫ (X|y, u) such that x n y,u is a prefix of xn(1+ǫ)pY U (y,u) y,u</formula><p>, and output m 3 as the concatenation of m 3,y,u for all y, u. Similar for m 4 .</p><p>Decoding. Decoder 1 outputs the sequence corresponding to the index m 1 in T</p><p>(n) ǫ (X|u n (m 0 )). Decoder 2 performs similarly using (m 0 , m 2 ). Decoder 3, upon observing y i , finds the sequence xn(1+ǫ)pY U (yi,ui(m0))</p><p>yi,ui(m0)</p><p>at the index m 3,yi,ui(m0) in T n(1+ǫ)pY U (yi,ui(m0)) ǫ (X|y i , u i (m 0 )), and output the next symbol in the sequence that is not previously used. Decoder 4 performs similarly using (m 0 , m 4 ).</p><p>Analysis of the probability of error. By the covering lemma, the probability that there does not exist m 0 such that (u n (m 0 ), . And T n(1+ǫ)pY U (y,u) ǫ (X|y, u) ≤ 2 nR3,y,upY U (y,u) for large n if R 3,y,u &gt; (1 + ǫ)H(X|Y = y, U = u) + δ(ǫ). Hence we can assign suitable R 3,y,u for each y, u if R 3 &gt; (1 + ǫ)H(X|Y, U ) + δ(ǫ).</p><formula xml:id="formula_12">x n , y n ) ∈ T (n) ǫ tends to 0 if R 0 &gt; I(X, Y ; U ). Also T (n) ǫ (X|u n (m 0 )) ≤ 2 nR1 for large n if R 1 &gt; H(X|U ) + δ(ǫ) (similar for R 2 &gt; H(Y |U ) + δ(ǫ)). Note that (u n (m 0 ), x n , y n ) ∈ T (n) ǫ implies |{i : x i = x, y i = y, u i (m 0 ) = u}| n(1 + ǫ)p Y U (y, u) ≤ (1 + ǫ)p XY U (x, y, u) (1 + ǫ)p Y U (y, u) ≤ p X|Y U (</formula><p>Although R is 5-dimensional, the bounds on the rates can be expressed in terms of three quantities: I(X; U ), I(Y ; U ) and I(X, Y ; U ) together with other constant quantities that involve only the given (X, Y ). This leads to the following equivalence of R to the mutual information region I XY defined in <ref type="bibr" target="#b0">(1)</ref>. We denote the components of a vector v ∈</p><formula xml:id="formula_13">I XY by v = (v X , v Y , v XY ).</formula><p>Proposition 1. The rate region for the EGW system can be expressed as</p><formula xml:id="formula_14">R = v∈IXY v XY , H(X) -v X , H(Y ) -v Y , H(X |Y ) -v XY + v Y , H(Y |X) -v XY + v X + [0, ∞) 5 ,<label>(2)</label></formula><p>where the last "+" denotes the Minkowski sum. Moreover, the mutual information region for (X, Y ) can be expressed as</p><formula xml:id="formula_15">I XY = v ∈ R 3 : v XY , H(X) -v X , H(Y ) -v Y , H(X |Y ) -v XY + v Y , H(Y |X) -v XY + v X ∈ R . (<label>3</label></formula><formula xml:id="formula_16">)</formula><p>Proof: Note that (2) follows from the definitions of R and I XY . We now prove (3). The ⊆ direction follows from (2).</p><formula xml:id="formula_17">For the ⊇ direction, let v ∈ R 3 satisfy v XY , H(X) -v X , H(Y ) -v Y , H(X |Y ) -v XY + v Y , H(Y |X) -v XY + v X ∈ R.</formula><p>Then by Theorem 1, there exists U such that</p><formula xml:id="formula_18">v XY ≥ I(X, Y ; U ),<label>(4)</label></formula><formula xml:id="formula_19">H(X) -v X ≥ H(X |U ),<label>(5)</label></formula><formula xml:id="formula_20">H(Y ) -v Y ≥ H(Y |U ),<label>(6)</label></formula><formula xml:id="formula_21">H(X |Y ) -v XY + v Y ≥ H(X |Y, U ),<label>(7)</label></formula><formula xml:id="formula_22">H(Y |X) -v XY + v X ≥ H(Y |X, U ).<label>(8)</label></formula><p>Adding ( <ref type="formula" target="#formula_18">4</ref>) and ( <ref type="formula" target="#formula_22">8</ref>), we have v X ≥ I(X; U ). Combining this with (5), we have v X = I(X; U ). Similarly v Y = I(Y ; U ). Substituting this into (7), we have v XY ≤ I(X, Y ; U ). Combining this with (4), we have v XY = I(X, Y ; U ). Hence v ∈ I XY .</p><p>In the following we list several properties of I XY .</p><p>Proposition 2. The mutual information region I XY satisfies: 1) Compactness and convexity. I XY is compact and convex.</p><p>2) Outer bound.</p><formula xml:id="formula_23">I XY ⊆ I o XY , where I o XY is the set of v such that v X , v Y ≥ 0, v X + v Y -v XY ≤ I(X; Y ), 0 ≤ v XY -v Y ≤ H(X |Y ), 0 ≤ v XY -v X ≤ H(Y |X).</formula><p>3) Inner bound.</p><formula xml:id="formula_24">I XY ⊇ I i XY , where I i XY is the convex hull of the points (0, 0, 0), (H(X), I(X; Y ), H(X)), (I(X; Y ), H(Y ), H(Y )), (H(X), H(Y ), H(X, Y )), (H(X|Y ), H(Y |X), H(X|Y ) + H(Y |X)). Moreover, there exists 0 ≤ ǫ 1 , ǫ 2 ≤ log I(X; Y ) + 4 such that (0, H(Y |X) -ǫ 1 , H(Y |X)), (H(X |Y ) -ǫ 2 , 0, H(X |Y )) ∈ I XY . 4) Superadditivity. If (X 1 , Y 1 ) is independent of (X 2 , Y 2 ), then I X1,Y1 + I X2,Y2 ⊆ I (X1,X2),(Y1,Y2) ,</formula><p>where + denotes the Minkowski sum. As a result, if</p><formula xml:id="formula_25">(X i , Y i ) ∼ p XY i.i.d. for i = 1, . . . , n, I XY ⊆ (1/n)I X n ,Y n . 5) Data processing. If X 2 -X 1 -Y 1 -Y 2 forms a Markov chain, then for any v ∈ I X1,Y1 , there exists w ∈ I X2,Y2 such that w X ≤ v X , w Y ≤ v Y , w XY ≤ v XY , I(X 2 ; Y 2 ) -w X -w Y + w XY ≤ I(X 1 ; Y 1 ) -v X -v Y + v XY .</formula><p>6) Cardinality bound.</p><formula xml:id="formula_26">I XY = p U |XY : |U |≤|X |•|Y|+2 {(I(X; U ), I(Y ; U ), I(X, Y ; U ))} .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>7) Relation to Gray-Wyner region and region of tension. The Gray-Wyner region can be obtained from</head><formula xml:id="formula_27">I XY as R GW = p U |XY I(X, Y ; U ), H(X |U ), H(Y |U ) + [0, ∞) 3 = v∈IXY v XY , H(X) -v X , H(Y ) -v Y + [0, ∞) 3 .</formula><p>The region of tension can be obtained from I XY as</p><formula xml:id="formula_28">T = p U |XY I(Y ; U |X), I(X; U |Y ), I(X; Y |U ) + [0, ∞) 3 = v∈IXY v XY -v X , v XY -v Y , I(X; Y ) -v X -v Y + v XY + [0, ∞) 3 .</formula><p>The proof of this proposition is given in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. EXTREME POINTS OF THE MUTUAL INFORMATION REGION</head><p>Many interesting information theoretic quantities can be expressed as optimizations over I XY (and R). Since I XY is convex and compact, some of these quantities can be represented in terms of the support function ψ IXY (x) and its one-sided directional derivative, which provides a representation of those quantities using at most 6 coordinates. To avoid conflicts and for consistency, we use different notation for some of these quantities from the original literature . We use semicolons, e.g., G(X; Y ), for symmetric quantities, and arrows, e.g., G(X → Y ), for asymmetric quantities.</p><p>Figures <ref type="figure" target="#fig_2">2,</ref><ref type="figure" target="#fig_4">3</ref> illustrate the mutual information region I XY and its extreme points, and Table I lists the extreme points and their corresponding optimization problems and support function representations.</p><p>We first consider the extreme points of I XY that correspond to previously known quantities.</p><p>Wyner's common information <ref type="bibr" target="#b1">[2]</ref> J(X; Y ) = min</p><formula xml:id="formula_29">X-U-Y I(X, Y ; U ) α γ K(X; Y ) I(X; Y ) α H(X † Y ) H(Y † X) β H(Y |X) H K (G XY , X) -H(X|Y ) -H(Y |X) -G NNI (X; Y ) α -G PPI (X; Y ) H(X|Y ) -G PNI (Y → X) Ψ(Y → X) I n f o r m a t i o n b o t t l e n e c k t r a d e o ff J(X; Y ) -I(X; Y ) G R * (Y → X)</formula><p>P ri v a cy fu n n el tr a d eo ff </p><formula xml:id="formula_30">H K (G Y X , Y ) -H(Y |X) G R * (X → Y ) Ψ(X → Y ) -G PNI (X → Y )</formula><formula xml:id="formula_31">|Y ) = v XY -v Y , β = I(Y ; U |X) = v XY -v X and γ = v X + v Y -v XY , i.</formula><p>e., the mutual information I(X; Y ; U ). Without loss of generality, we assume H(X) ≥ H(Y ). Note that the original Gray-Wyner region and the region of tension correspond to the upper-left corner. can be expressed as</p><formula xml:id="formula_32">J(X; Y ) = min {v XY : v ∈ I XY , v X + v Y -v XY = I(X; Y )} = min R 0 : R 4 0 ∈ R, R 0 + R 1 + R 2 = H(X, Y ) = -ψ ′ IXY (1, 1, -1; 0, 0, -1). Gács-Körner common information [3], [25] K(X; Y ) = max U: H(U|X)=H(U|Y )=0 H(U ) = max U: X-Y -U, U-X-Y I(X, Y ; U ) can be expressed as K(X; Y ) = max {v XY : v ∈ I XY , v X = v Y = v XY } = max R 0 : R 4 0 ∈ R, R 0 + R 1 = H(X), R 0 + R 2 = H(Y ) = ψ ′</formula><p>IXY (1, 1, -2; 0, 0, 1). Körner graph entropy <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b25">[26]</ref>. Let G XY be a graph with a set of vertices X and edges between confusable symbols upon observing Y , i.e., there is an edge (x 1 , x 2 ) if p(x 1 , y), p(x 2 , y) &gt; 0 for some y. The Körner graph entropy  </p><formula xml:id="formula_33">H K (G XY , X) = min U: U-X-Y, H(X|Y,U)=0 I(X; U ) α γ H(X † Y ) K(X; Y ) GR * (Y → X) H(X|Y ) I(X; Y ) α β H(X|Y ) H(Y |X) H(X † Y ) H(Y † X) J(X; Y ) -I(X; Y )</formula><formula xml:id="formula_34">I(X; Y ) 3. Plane α = H(X|Y ) HK(GXY , X) -H(X|Y ) Ψ(Y → X) -H(Y |X) -GNNI(X; Y ) α γ -GPPI(X; Y ) -H(Y |X) H(X|Y ) 4. Plane β + γ = 0 -GPNI(Y → X) GR * (Y → X)</formula><formula xml:id="formula_35">|Y ) = v XY -v Y , β = I(Y ; U |X) = v XY -v X and γ = v X + v Y -v XY . We assume H(X) ≥ H(Y ).</formula><p>can be expressed as</p><formula xml:id="formula_36">H K (G XY , X) = min {v X : v ∈ I XY , v X = v XY , v XY -v Y = H(X |Y )} = min R 0 : R 4 0 ∈ R, R 0 + R 1 = H(X), R 3 = 0 = -ψ ′</formula><p>IXY (1, -1, 0; -1, 0, 0). In the Gray-Wyner system with causal complementary side information, H K (G XY , X) corresponds to the setting with only decoders 1, 3 and M 3 = ∅, and we restrict the sum rate R 0 +R 1 = H(X). This is in line with the lossless source coding setting with causal side information <ref type="bibr" target="#b11">[12]</ref>, where the optimal rate is also given by H K (G XY , X). An intuitive reason of this equality is that R 0 + R 1 = H(X) and the recovery requirement of decoder 1 forces M 0 and M 1 to contain negligible information outside X n , hence the setting is similar to the case in which the encoder has access only to X n . This corresponds to lossless source coding with causal side information setting. <ref type="bibr" target="#b3">[4]</ref> (also see H(Y ց X|X) in <ref type="bibr" target="#b26">[27]</ref>, G(Y → X) in <ref type="bibr" target="#b27">[28]</ref>, private information in <ref type="bibr" target="#b28">[29]</ref> and <ref type="bibr" target="#b29">[30]</ref>)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Necessary conditional entropy</head><formula xml:id="formula_37">H(Y † X) = min U: H(U|Y )=0, X-U-Y H(U |X) = min U: X-Y -U, X-U-Y I(Y ; U ) -I(X; Y )</formula><p>can be expressed as</p><formula xml:id="formula_38">H(Y † X) = min {v XY : v ∈ I XY , v Y = v XY , v X = I(X; Y )} -I(X; Y ) = min R 0 : R 4 0 ∈ R, R 0 + R 2 = H(Y ), R 1 = H(X |Y ) = -ψ ′</formula><p>IXY (1, 2, -2; 1, 0, -1). Information bottleneck <ref type="bibr" target="#b4">[5]</ref> G IB (t, X → Y ) = min</p><formula xml:id="formula_39">U: X-Y -U, I(X;U)≥t I(Y ; U )</formula><p>can be expressed as</p><formula xml:id="formula_40">G IB (t, X → Y ) = min {v Y : v ∈ I XY , v Y = v XY , v X ≥ t} = min R 0 : R 4 0 ∈ R, R 0 + R 2 = H(Y ), R 1 ≤ H(X) -t .</formula><p>Note that the same tradeoff also appears in common randomness extraction on a 2-DMS with one-way communication <ref type="bibr" target="#b30">[31]</ref>, lossless source coding with a helper <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b16">[17]</ref>, and a quantity studied by Witsenhausen and Wyner <ref type="bibr" target="#b31">[32]</ref>. It is shown in <ref type="bibr" target="#b32">[33]</ref> that its slope is given by the chordal slope of the hypercontractivity of Markov operator <ref type="bibr" target="#b33">[34]</ref> s</p><formula xml:id="formula_41">* (Y → X) = sup U: X-Y -U I(X; U ) I(Y ; U ) = sup {v X /v Y : v ∈ I XY , v Y = v XY } .</formula><p>Privacy funnel <ref type="bibr" target="#b8">[9]</ref> (also see the rate-privacy function defined in <ref type="bibr" target="#b28">[29]</ref>)</p><formula xml:id="formula_42">G PF (t, X → Y ) = min U: X-Y -U, I(Y ;U)≥t I(X; U )</formula><p>can be expressed as</p><formula xml:id="formula_43">G PF (t, X → Y ) = min {v X : v ∈ I XY , v Y = v XY , v Y ≥ t} = min R 0 + R 4 -H(Y |X) : R 4 0 ∈ R, R 0 + R 2 = H(Y ), R 0 ≥ t .</formula><p>In particular, the maximum R for perfect privacy (written as g 0 (X; Y ) in <ref type="bibr" target="#b28">[29]</ref>, also see <ref type="bibr" target="#b34">[35]</ref>) is</p><formula xml:id="formula_44">G R * (X → Y ) = max {t ≥ 0 : G PF (t, X → Y ) = 0} = max {v Y : v ∈ I XY , v Y = v XY , v X = 0} = max R 0 : R 4 0 ∈ R, R 0 + R 2 = H(Y ), R 0 + R 4 = H(Y |X) = ψ ′</formula><p>IXY (-1, 1, -1; 0, 1, 0). The optimal privacy-utility coefficient <ref type="bibr" target="#b34">[35]</ref> is <ref type="bibr" target="#b9">[10]</ref> Ψ(X → Y ) = min</p><formula xml:id="formula_45">v * (X → Y ) = inf U: X-Y -U I(X; U ) I(Y ; U ) = inf {v X /v Y : v ∈ I XY , v Y = v XY } .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Excess functional information</head><formula xml:id="formula_46">U: U⊥ ⊥X H(Y |U ) -I(X; Y )</formula><p>is closely related to one-shot channel simulation <ref type="bibr" target="#b35">[36]</ref> and lossy source coding, and can be expressed as</p><formula xml:id="formula_47">Ψ(X → Y ) = H(Y |X) -max {v Y : v ∈ I XY , v X = 0} = min R 2 : R 4 0 ∈ R, R 0 + R 4 = H(Y |X) -I(X; Y ) = min R 2 : R 4 0 ∈ R, R 4 = 0, R 0 = H(Y |X) -I(X; Y ) = -ψ ′</formula><p>IXY (-2, 0, 1; 0, 1, -1). In the EGW system, Ψ(X → Y ) corresponds to the setting with only decoders 2, 4 and M 4 = ∅ (since it is better to allocate the rate to R 0 instead of R 4 ), and we restrict R 0 = H(Y |X). The value of Ψ(X → Y ) + I(X; Y ) is the rate of the additional information M 2 that decoder 2 needs, in order to compensate the lack of side information compared to decoder 4. Minimum communication rate for distributed channel synthesis with common randomness rate t <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref> </p><formula xml:id="formula_48">C(t, X → Y ) = min U: X-U-Y max {I(X; U ), I(X, Y ; U ) -t}</formula><p>can be expressed as</p><formula xml:id="formula_49">C(t, X → Y ) = min {max{v X , v XY -t} : v ∈ I XY , v X + v Y -v XY = I(X; Y )} = min max{H(X) -R 1 , R 0 -t} : R 4 0 ∈ R, R 0 + R 1 + R 2 = H(X, Y ) .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. New information theoretic quantities</head><p>We now present three new quantities which arise as extreme points of I XY . These extreme points concern the case in which decoders 3 and 4 are active in the EGW system. Note that they are all maximizations of the interaction information I(X; Y |U ) -I(X; Y ) under various constraints. They can be considered as distances from extreme dependency, in the sense that they are equal to zero only under certain conditions of extreme dependency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Maximal interaction information is defined as</head><formula xml:id="formula_50">G NNI (X; Y ) = max p U |XY I(X; Y |U ) -I(X; Y ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>It can be shown that</head><formula xml:id="formula_51">G NNI (X; Y ) = H(X |Y ) + H(Y |X) - min U: H(Y |X,U)=H(X|Y,U)=0 I(X, Y ; U ) = max {v XY -v X -v Y : v ∈ I XY } = H(X |Y ) + H(Y |X) -min R 0 + R 3 + R 4 : R 4 0 ∈ R = H(X |Y ) + H(Y |X) -min R 0 : R 4 0 ∈ R, R 3 = R 4 = 0 = ψ IXY (-1, -1, 1).</formula><p>The maximal interaction information concerns the sum-rate of the EGW system with only decoders 3,4. Note that it is always better to allocate the rates R 3 , R 4 to R 0 instead, hence we can assume R 3 = R 4 = 0 (which corresponds to H(Y |X, U ) = H(X|Y, U ) = 0). The quantity H(X|Y ) + H(Y |X) -G NNI (X; Y ) is the maximum rate in the lossless causal version of the complementary delivery setup <ref type="bibr" target="#b6">[7]</ref>. Asymmetric private interaction information is defined as</p><formula xml:id="formula_52">G PNI (X → Y ) = max U: U⊥ ⊥X I(X; Y |U ) -I(X; Y ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>It can be shown that</head><formula xml:id="formula_53">G PNI (X → Y ) = H(Y |X) - min U: U⊥ ⊥X, H(Y |X,U)=0 I(Y ; U ) = H(Y |X) -min {v Y : v ∈ I XY , v X = 0, v XY = H(Y |X)} = H(X |Y ) -min R 3 : R 4 0 ∈ R, R 0 + R 4 = H(Y |X) = H(X |Y ) -min R 3 : R 4 0 ∈ R, R 4 = 0, R 0 = H(Y |X) = ψ ′</formula><p>IXY (-1, 0, 0; 0, -1, 1). The asymmetric private interaction information is the opposite of excess functional information defined in <ref type="bibr" target="#b9">[10]</ref> in which I(Y ; U ) is maximized instead. Another operational meaning of G PNI is the generation of random variables with a privacy constraint. Suppose Alice observes X and wants to generate Y ∼ p Y |X (•|X). However, she does not have any private randomness and can only access public randomness W , which is also available to Eve. Her goal is to generate Y as a function of X and W , while minimizing Eve's knowledge on Y measured by I(Y ; W ). The minimum</p><formula xml:id="formula_54">I(Y ; W ) is H(Y |X)-G PNI (X → Y ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Symmetric private interaction information is defined as</head><formula xml:id="formula_55">G PPI (X; Y ) = max U: U⊥ ⊥X, U⊥ ⊥Y I(X; Y |U ) -I(X; Y ).</formula><p>It can be shown that</p><formula xml:id="formula_56">G PPI (X; Y ) = max U: U⊥ ⊥X, U⊥ ⊥Y I(X, Y ; U ) = max {v XY : v ∈ I XY , v X = v Y = 0} = max R 0 : R 4 0 ∈ R, R 0 + R 3 = H(X |Y ), R 0 + R 4 = H(Y |X) = ψ ′</formula><p>IXY (-1, -1, 0; 0, 0, 1). Intuitively, G PPI captures the maximum amount of information one can disclose about (X, Y ), such that an eavesdropper who only has one of X or Y would know nothing about the disclosed information. Another operational meaning of G PNI is the generation of random variables with a privacy constraint (similar to that for G PNI ). Suppose Alice observes X and wants to generate Y ∼ p Y |X (•|X). She has access to public randomness W , which is also available to Eve. She also has access to private randomness. Her goal is to generate Y using X, W and her private randomness such that Eve has no knowledge on Y (i.e., I(Y ; W ) = 0), while minimizing the amount of private randomness used measured by H(Y |X, W ) (note that if Alice Active decoders in EGW</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Information quantity</head><p>Objective and constraints in EGW Support fcn. rep.</p><formula xml:id="formula_57">(ψ = ψ I XY ) 1, 2 Wyner's CI [2] min R 0 : R 0 + R 1 + R 2 = H(X, Y ) -ψ ′ (1, 1, -1; 0, 0, -1)</formula><p>Gács-Körner CI <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b24">[25]</ref> max R 0 : R 0</p><formula xml:id="formula_58">+ R 1 = H(X), R 0 + R 2 = H(Y ) ψ ′ (1, 1, -2; 0, 0, 1)</formula><p>Necessary conditional entropy <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b26">[27]</ref> min</p><formula xml:id="formula_59">R 0 : R 0 + R 2 = H(Y ), R 1 = H(X|Y ) -ψ ′ (1, 2, -2; 1, 0, -1)</formula><p>Info. bottleneck <ref type="bibr" target="#b4">[5]</ref> min</p><formula xml:id="formula_60">R 0 : R 0 + R 2 = H(Y ), R 1 ≤ H(X) -t none</formula><p>Comm. rate for channel synthesis <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b13">[14]</ref> min max{H(X)-R 1 , R 0 -t} : R 0 +R</p><formula xml:id="formula_61">1 +R 2 = H(X, Y ) none 1, 3 or 2, 4</formula><p>Körner graph entropy <ref type="bibr" target="#b7">[8]</ref> min</p><formula xml:id="formula_62">R 0 : R 0 + R 1 = H(X), R 3 = 0 -ψ ′ (1, -1, 0; -1, 0, 0) Excess functional info. [10] min R 2 -I(X; Y ) : R 4 = 0, R 0 = H(Y |X) -ψ ′ (-2, 0, 1; 0, 1, -1)</formula><p>Max. rate for perfect privacy <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b28">[29]</ref> max</p><formula xml:id="formula_63">R 0 : R 0 + R 2 = H(Y ), R 0 + R 4 = H(Y |X) ψ ′ (-1, 1, -1; 0, 1, 0) Privacy funnel [9] min R 0 + R 4 -H(Y |X) : R 0 + R 2 = H(Y ), R 0 ≥ t none 3, 4 Maximal interaction info. max H(X|Y ) + H(Y |X) -R 0 : R 3 = R 4 = 0 ψ(-1, -1, 1)</formula><p>Asymm. private interaction info. max H(X|Y</p><formula xml:id="formula_64">) -R 3 : R 4 = 0, R 0 = H(Y |X) ψ ′ (-1, 0, 0; 0, -1, 1)</formula><p>Symm. private interaction info. max R 0 : R can flip fair coins for the private randomness, then by Knuth-Yao algorithm <ref type="bibr" target="#b36">[37]</ref> the expected number of flips is bounded by</p><formula xml:id="formula_65">0 + R 3 = H(X|Y ), R 0 + R 4 = H(Y |X) ψ ′ (-1, -1, 0; 0, 0, 1)</formula><formula xml:id="formula_66">H(Y |X, W ) + 2 ). The minimum H(Y |X, W ) is H(Y |X) -G PPI (X; Y ).</formula><p>We now list several properties of G NNI , G PNI and G PPI .</p><formula xml:id="formula_67">Proposition 3. G NNI , G PNI and G PPI satisfies 1) Bounds. 0 ≤ G PPI (X; Y ) ≤ G PNI (X → Y ) ≤ G NNI (X; Y ) ≤ min {H(X |Y ), H(Y |X)} .</formula><p>2) Conditions for zero.</p><p>• G NNI (X; Y ) = 0 if and only if the characteristic bipartite graph of X, Y (i.e. vertices X ∪ Y with edge (x, y) if p(x, y) &gt; 0) does not contain paths of length 3, or equivalently, p(x|y) = 1 or p(y|x) = 1 for all x, y such that p(x, y) &gt; 0.</p><formula xml:id="formula_68">• G PNI (X → Y ) = 0 if and only if G NNI (X; Y ) = 0. • G PPI (X; Y ) = 0 if</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>and only if the characteristic bipartite graph of X, Y does not contain cycles.</head><p>3) Condition for maximum. If H(X) = H(Y ), then the following statements are equivalent:</p><formula xml:id="formula_69">• G NNI (X; Y ) = H(Y |X). • G PNI (X → Y ) = H(Y |X). • G PPI (X; Y ) = H(Y |X). • p(x) = p(y) for all x, y such that p(x, y) &gt; 0. 4) Lower bound for independent X, Y . If X ⊥ ⊥ Y , G PPI (X; Y ) ≥ E [-log max{p(X), p(Y )}] -1. 5) Superadditivity. If (X 1 , Y 1 ) is independent of (X 2 , Y 2 ), then G NNI (X 1 , X 2 ; Y 1 , Y 2 ) ≥ G NNI (X 1 ; Y 1 ) + G NNI (X 2 ; Y 2 ).</formula><p>Similar for G PNI and G PPI .</p><p>The proof of this proposition is given in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXTENDED GRAY-WYNER SYSTEM WITH NONCAUSAL COMPLEMENTARY SIDE INFORMATION</head><p>In this section we establish the rate region R ′ for the EGW system with complementary noncausal side information at decoders 3 and 4 (noncausal EGW), that is, decoder 3 recovers X n from (M 0 , M 3 , Y n ) and decoder 4 similarly recovers Y n from (M 0 , M 4 , X n ). We show that R ′ can be expressed in terms of the Gray-Wyner region R GW , hence it contains fewer interesting extreme points compared to R. This is the reason we emphasized the causal side information in this paper. We further show that R ′ is related to the asymptotic mutual information region defined as</p><formula xml:id="formula_70">I ∞ XY = ∞ n=1 1 n I X n ,Y n , where (X n , Y n ) is i.i.d. with (X 1 , Y 1 ) ∼ p XY .</formula><p>Note that I ∞ XY may not be closed (unlike I XY which is always closed). The following gives the rate region for the noncausal EGW.</p><p>Theorem 2. The optimal rate region R ′ for the extended Gray-Wyner system with noncausal complementary side information is the set of rate tuples</p><formula xml:id="formula_71">(R 0 , R 1 , R 2 , R 3 , R 4 ) such that R 0 ≥ I(X, Y ; U ), R 1 ≥ H(X |U ), R 2 ≥ H(Y |U ), R 3 ≥ H(X |U ) -H(Y ), R 4 ≥ H(Y |U ) -H(X), R 0 + R 3 ≥ H(X |Y ), R 0 + R 4 ≥ H(Y |X), R 2 + R 3 ≥ H(X |U ), R 1 + R 4 ≥ H(Y |U ), R 0 + R 2 + R 3 ≥ H(X, Y ), R 0 + R 1 + R 4 ≥ H(X, Y ) for some p U|XY , where |U| ≤ |X | • |Y| + 2.</formula><p>The proof is given in Appendix D. Then we characterize the closure of I ∞ XY . We show that cl(I ∞ XY ), R ′ and the the Gray-Wyner region R GW can be expressed in terms of each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposition 4. The closure of I ∞</head><p>XY , the rate region R ′ for the noncausal EGW and the Gray-Wyner region R GW satisfy:</p><formula xml:id="formula_72">1) Characterization of cl(I ∞ XY ). cl(I ∞ XY ) = (I XY + (-∞, 0] × (-∞, 0] × [0, ∞)) ∩ I o XY = (I XY + {(t, t, t) : t ≤ 0}) ∩ [0, ∞) × [0, ∞) × R .</formula><p>2) Equivalence between cl(I ∞ XY ) and R ′ .</p><formula xml:id="formula_73">R ′ = v∈cl(I ∞ XY ) v XY , H(X) -v X , H(Y ) -v Y , H(X |Y ) -v XY + v Y , H(Y |X) -v XY + v X + [0, ∞) 5 ,<label>and</label></formula><formula xml:id="formula_74">cl(I ∞ XY ) = v ∈ R 3 : v XY , H(X) -v X , H(Y ) -v Y , H(X |Y ) -v XY + v Y , H(Y |X) -v XY + v X ∈ R ′ . 3) Equivalence between cl(I ∞ XY ) and R GW . R GW = v∈cl(I ∞ XY ) v XY , H(X) -v X , H(Y ) -v Y + [0, ∞) 3 , and cl(I ∞ XY ) = v ∈ I o XY : v XY , H(X) -v X , H(Y ) -v Y ∈ R GW .</formula><p>The proof is given in Appendix E. Note that Proposition 4 does not characterize I ∞ XY completely since it does not specify which boundary points are in I ∞ XY .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Proof of Proposition 2</head><p>1) To see that I XY is convex, for any U 0 , U 1 and λ ∈ [0, 1], let Q ∼ Bern(λ) be independent of X, Y, U 0 , U 1 , and let U = (Q, U Q ). Then I(X; U ) = (1λ)I(X; U 0 ) + λI(X; U 1 ) (similarly for the other two quantities). Compactness will be proved later.</p><p>2) The outer bound follows directly from the properties of entropy and mutual information.</p><p>3) For the inner bound, the first 4 points can be obtained by substituting U = ∅, X, Y, (X, Y ) respectively. For the last point, by the functional representation lemma <ref type="bibr">[40, p. 626</ref></p><formula xml:id="formula_75">], let V ⊥ ⊥ X such that H(Y |X, V ) = 0. Again by the functional representation lemma, let W ⊥ ⊥ (Y, V ) such that H(X|Y, V, W ) = 0. Let U = (V, W ), then I(X, Y ; U ) -I(X; U ) = H(Y |X) -H(Y |X, U ) = H(Y |X), I(X, Y ; U ) -I(Y ; U ) = H(X|Y ) -H(X|Y, U ) = H(X|Y ),<label>and</label></formula><formula xml:id="formula_76">I(X, Y ; U ) = I(X, Y ; V, W ) = I(X, Y ; V ) + I(X, Y ; W |V ) = I(Y ; V |X) + I(X; W |Y, V ) ≤ H(Y |X) + H(X |Y ).</formula><p>Hence  We then prove that if there exist a length 3 path in the bipartite graph, then G NNI (X; Y ) ≥ G PNI (X; Y ) &gt; 0. Assume p(x 1 , y 1 ), p(x 1 , y 2 ), p(x 2 , y 1 ) &gt; 0. Let U ∈ {1, 2},</p><formula xml:id="formula_77">i ; U i ), I(Y i ; U i ), I(X i , Y i ; U i )) ∈ I Xi,Yi<label>.</label></formula><formula xml:id="formula_78">p(u|x, y) =                1/2 + ǫ/p(x 1 , y 1 ) if (x, y, u) = (x 1 , y 1 , 1) 1/2 -ǫ/p(x 1 , y 1 ) if (x, y, u) = (x 1 , y 1 , 2) 1/2 -ǫ/p(x 1 , y 2 ) if (x, y, u) = (x 1 , y 2 , 1) 1/2 + ǫ/p(x 1 , y 2 ) if (x, y, u) = (x 1 , y 2 , 2) 1/2 otherwise,</formula><p>where ǫ &gt; 0 is small enough such that the above is a valid conditional pmf. One can verify that U ⊥ ⊥ X. Since p U|XY (1|x 1 , y 1 ) = 1/2 + ǫ/p(x 1 , y 1 ) = 1/2 = p U|XY (1|x 2 , y 1 ), X and U are not conditionally independent given Y . Hence I(X; Y |U ) -I(X; Y ) = I(X; U |Y ) &gt; 0.</p><p>We then prove that if G PPI (X; Y ) &gt; 0, then there exists a cycle in the bipartite graph. Let U satisfies U ⊥ ⊥ X, U ⊥ ⊥ Y and I(X; Y |U ) &gt; I(X; Y ). Since U is not independent of X, Y , there exists x 1 , y 1 , u such that p(x 1 , y 1 |u) &gt; p(x 1 , y 1 ). Since y ′ p(x 1 , y ′ |u) = p(x 1 |u) = p(x 1 ) = y ′ p(x 1 , y ′ ), there exists y 2 = y 1 such that p(x 1 , y 2 |u) &lt; p(x 1 , y 2 ). Since</p><p>x ′ p(x ′ , y 2 |u) = p(y 2 |u) = p(y 2 ) = x ′ p(x ′ , y 2 ), there exists x 2 = x 1 such that p(x 2 , y 2 |u) &gt; p(x 2 , y 2 ). Continue this process until we return to a visited x, y pair, i.e., (x a , y a ) = (x b , y b ) for a &lt; b. Then y a , x a , y a+1 , x a+1 , . . . , x b-1 , y b forms a cycle. We then prove that if there exist a cycle in the bipartite graph, then G PPI (X; Y ) &gt; 0. Let y 1 , x 1 , y 2 , x 2 , . . . , x a , y a+1 = y 1 be a cycle. Let U ∈ {1, 2},</p><formula xml:id="formula_79">p(u|x, y) =                1/2 + ǫ/p(x i , y i ) if (x, y, u) = (x i , y i , 1) 1/2 -ǫ/p(x i , y i ) if (x, y, u) = (x i , y i , 2) 1/2 -ǫ/p(x i , y i+1 ) if (x, y, u) = (x i , y i+1 , 1) 1/2 + ǫ/p(x i , y i+1 ) if (x, y, u) = (x i , y i+1 , 2) 1/2 otherwise,</formula><p>where ǫ &gt; 0 is small enough such that the above is a valid conditional pmf. One can verify that U ⊥ ⊥ X and U ⊥ ⊥ Y . Since p U|XY (1|x 1 , y 1 ) &gt; 1/2 &gt; p U|XY (1|x 1 , y 2 ), U is not independent of X, Y . Hence I(X; Y |U ) -I(X; Y ) = I(X, Y ; U ) &gt; 0. 3) We then prove that if H(X) = H(Y ) and p(x) = p(y) for all x, y such that p(x, y) &gt; 0, then G PPI (X; Y ) = H(Y |X). Let Q achieves the Gács-Körner common information, and let X q = {x : p(x|q) &gt; 0}, Y q = {y : p(y|q) &gt; 0}, then X|{Q = q} ∼ Unif(X q ), Y |{Q = q} ∼ Unif(Y q ) and |X q | = |Y q | for all q. Applying Birkhoff-von Neumann theorem on the submatrix of p(x, y) with rows X q and columns Y q , there exists U q such that p(x, y|q) = u p Uq (u)p XY |UqQ (x, y|u, q), p X|UqQ (x|u, q) = p Y |UqQ (y|u, q) = 1/|X q | and p XY |UqQ (x, y|u, q) ∈ {0, 1/|X q |} for all x, y, u. Let U = {U q } q∈Q , where U q are assumed to be independent across q. Then for any x and u = {u q }, p(x|{u q }) = p(x, q|{u q }) = p(q)p(x|u q , q) = p(q)/|X q | = p(x), where q = q(x) since H(Q|X) = 0. Hence U ⊥ ⊥ X. Similarly U ⊥ ⊥ Y . Also since there is only one non-zero in p XY |UqQ (x, y|u, q) for different x, we have  5) The superadditivity property follows from the superadditivity of mutual information region.</p><formula xml:id="formula_80">H(X|Y, U ) = 0. Similarly H(Y |X, U ) = 0. Hence I(X; Y |U ) -I(X; Y ) = I(Y ; U |X) -I(Y ; U ) = H(Y |X). We then prove that if H(X) = H(Y ) and G NNI (X; Y ) = H(Y |X), then p(x) = p(y) for all x, y such that p(x, y) &gt; 0. Let U satisfies I(X; Y |U ) = I(X; Y ) + H(Y |X) = H(Y ), then one can check that U ⊥ ⊥ X, U ⊥ ⊥ Y , H(X|Y, U ) = 0 and H(Y |X, U ) = 0.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) We then prove the lower bound when</head><formula xml:id="formula_81">X, Y independent. Assume X ⊥ ⊥ Y . Assume X = {1, . . . , |X |}, Y = {1, . . . , |Y|}, X = F -1 X (V ), Y = F -1 Y (W ), V, W ∼ Unif[0, 1] independent. Let U = V + W mod 1, then U ⊥ ⊥ X, U ⊥ ⊥ Y . H(Y |U, X) = x p(x) ˆ1 0 H(Y |U = u, X = x)du = x p(x) ˆ1 0 H(Y | W ∈ ([u -F X (x), u -F X (x -1)) mod 1))du = x p(x) ˆ1 0 H(Y | W ∈ ([u, u + p(x)] mod 1))du = x p(x) ˆ1 0 y l (P {Y = y | W ∈ ([u, u + p(x)] mod 1)}) du = x p(x) ˆ1 0 y l p(x) -1 |[F Y (y -1), F Y (y)] ∩ ([u, u + p(x)] mod 1)| du = x p(x) y ˆ1 0 l p(x) -1 |[0, p(y)] ∩ ([u, u + p(x)] mod 1)| du = -H(X) + x,y ˆ1 0 l (|[0, p(y)] ∩ ([u, u + p(x)]<label>mod</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Proof of Theorem 2</head><p>We first prove the achievability. Without loss of generality assume H(X) ≥ H(Y ). Fix any point v = (v X , v Y , v XY ) ∈ I XY . Consider the region</p><formula xml:id="formula_82">I (v) = ((-∞, v X ] × (-∞, v Y ] × [v XY , ∞)) ∩ I o XY .</formula><p>is achievable. The region can be written as w XY ≥ I(X, Y ; U ), w X ≤ I(X; U ), w Y ≤ I(Y ; U ),</p><formula xml:id="formula_83">w X ≥ 0, w Y ≥ 0, w XY -w Y ≤ H(X |Y ), w XY -w X ≤ H(Y |X), R 0 ≥ w XY + ǫ, R 1 ≥ H(X) -w X + ǫ, R 2 ≥ H(Y ) -w Y + ǫ, R 3 ≥ H(X |Y ) -w XY + w Y + ǫ, R 4 ≥ H(Y |X) -w XY + w X + ǫ</formula><p>for some U, w X , w Y , w XY . The final rate region can be obtained by eliminating w X , w Y , w XY using Fourier-Motzkin elimination.</p><p>We then prove the converse. Since decoder 3 observes M 0 , M 3 , Y n and has to recover X n with vanishing error probability, R 0 + R 3 ≥ H(X|Y ). Similarly R 0 + R 4 ≥ H(Y |X). Note that decoders 2 and 3 together can recover X n , Y n with vanishing error probability (decoder 3 uses the output of decoder 2 as the side information), and hence</p><formula xml:id="formula_84">R 0 + R 2 + R 3 ≥ H(X, Y ). Similarly R 0 + R 1 + R 4 ≥ H(X, Y ).</formula><p>Let U i = (M 0 , X i-1 , Y i-1 ). Using the same arguments in the proof of Theorem 1, we have R 0 ≥ I(X, Y ; U ), R 1 ≥ H(X|U ), R 2 ≥ H(Y |U ).</p><formula xml:id="formula_85">nR 3 ≥ H(M 3 |M 0 ) ≥ I(X n ; M 3 |M 0 ) = H(X n |M 0 ) -H(X n |M 0 , M 3 ) = n i=1</formula><p>H(X i |M 0 , X i-1 ) -H(X n |M 0 , M 3 )</p><formula xml:id="formula_86">≥ n i=1 H(X i |M 0 , X i-1 , Y i-1 ) -H(Y n ) -H(X n |M 0 , M 3 , Y n ) ≥ n i=1 H(X i |U i ) -H(Y n ) -o(n),</formula><p>where the last inequality is due to Fano's inequality. Similarly</p><formula xml:id="formula_87">nR 4 ≥ i H(Y i |U i ) -H(X n ) -o(n). n(R 2 + R 3 ) ≥ H(M 2 , M 3 |M 0 ) ≥ I(X n ; M 2 , M 3 |M 0 ) = H(X n |M 0 ) -H(X n |M 0 , M 2 , M 3 ) = n i=1 H(X i |M 0 , X i-1 ) -H(X n |M 0 , M 2 , M 3 ) ≥ n i=1 H(X i |M 0 , X i-1 , Y i-1 ) -H(Y n |M 0 , M 2 , M 3 ) -H(X n |M 0 , M 2 , M 3 , Y n ) ≥ n i=1 H(X i |U i ) -o(n),</formula><p>where the last inequality follows by Fano's inequality. Similarly n(R 1 + R 4 ) ≥ i H(Y i |U i )o(n). Hence the point (R 0 + ǫ, . . . , R 4 + ǫ) is in the convex hull of R ′ for any ǫ &gt; 0. We have seen in the achievability proof that (for ǫ = 0) R ′ = R ((I XY + (-∞, 0] × (-∞, 0] × [0, ∞)) ∩ I o XY ) + [0, ∞) 5 is the increasing hull of an affine transformation of a convex set. Therefore R ′ is convex.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>3 M 4 Figure 1 .</head><label>341</label><figDesc>Figure 1. Extended Gray-Wyner system.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>(</head><label></label><figDesc>X|u n (m 0 )) and T (n) ǫ (Y |u n (m 0 )), respectively. For each y ∈ Y, u ∈ U, assign indices m 3,y,u ∈ [1 : 2 nR3,y,upY U (y,u) ] to the sequences in T n(1+ǫ)pY U (y,u) ǫ (X|y, u), where y,u R 3,y,u p Y U (y, u) ≤ R 3 . Define m 4,x,u similarly.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Illustration of I XY (yellow), I i XY (green) and I o XY (grey) defined in Proposition 2. The axes are α = I(X; U|Y ) = v XYv Y , β = I(Y ; U |X) = v XYv X and γ = v X + v Yv XY , i.e., the mutual information I(X; Y ; U ). Without loss of generality, we assume H(X) ≥ H(Y ). Note that the original Gray-Wyner region and the region of tension correspond to the upper-left corner.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>2. Plane β = 0 1 .</head><label>1</label><figDesc>Plane γ = I(X; Y ) I n f o r m a t i o n b o t t l e n e c k t r a d e o ff P r iv a c y f u n n e l t r a d e o ff β γ H(Y |X)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Illustration of I XY (yellow), I i XY (green) and I o XY (grey) restricted to different planes. The axes are α = I(X; U|Y ) = v XYv Y , β = I(Y ; U |X) = v XYv X and γ = v X + v Yv XY .We assume H(X) ≥ H(Y ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>5 ) 1 )</head><label>51</label><figDesc>The data processing property can be obtained from considering U where (I(X 1 ; U ), I(Y 1 ; U ), I(X 1 , Y 1 ; U )) ∈ I X1,Y1 . 6) The cardinality bound can be proved using Fenchel-Eggleston-Carathéodory theorem using the same arguments as in the converse proof of Theorem 1. Compactness follows from the fact that mutual information is a continuous function, and the set of conditional pmfsp U|XY with |U| ≤ |X | • |Y| + 2 is a compact set.7) The relation to Gray-Wyner region and region of tension follows from the definitions of the regions. C. Proof of Proposition 3 To prove the bound, note that I(X; Y |U ) ≤ H(X), hence I(X; Y |U ) -I(X; Y ) ≤ H(X|Y ), G NNI ≤ H(X|Y ). 2) We first prove that if there does not exist length 3 paths in the bipartite graph, then G NNI (X; Y ) = G PNI (X; Y ) = 0. Let Q achieves the Gács-Körner common information, i.e., Q represents which connected component the edge (X, Y ) lies in. If the bipartite graph does not contain length 3 paths, every connected component is a star, i.e., for each q, either H(X|Q = q) = 0 or H(Y |Q = q) = 0. Then I(X; Y ) = H(Q) + I(X; Y |Q) = H(Q), and I(X; Y |U ) = H(Q|U ) + I(X; Y |Q, U ) = H(Q|U ) ≤ H(Q) for any U . Hence G NNI (X; Y ) = G PNI (X; Y ) = 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>For any x, y such that p(x, y) &gt; 0, let u such that p(x, y, u) &gt; 0, then p(x) = p(x|u) = p(x|u)p(y |x, u) = p(y |u)p(x|y, u) = p(y).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>2 = ab log 1 b + b 2 ≤ ab log 1 b-a 2 ≤ 2 ≤</head><label>22122</label><figDesc>1)|) duwhere we write A mod 1 = {a mod 1 : a ∈ A} and |A| for the Lebesgue measure forA ⊆ R, l(t) = -t log t. Consider f (a, b) = ˆ1 0 l (|[0, b] ∩ ([u, u + a] mod 1)|) du. If b ≤ a ≤ 1 and a + b ≤ 1, f (a, b) = (ab)l(b) + 2 ˆb 0 l (u) du ≤ (ab)l(b) + 2bl (b/2) = al(b) + b + ab. If b ≤ a and a + b &gt; 1, f (a, b) = (ab)l(b) + (a + b -1)l(a + b -1) + 2 ˆb b+a-1 l(u)du ≤ (ab)l(b) + (a + b -1)l(a + b -1) + 2(1a)l b -1 (ab)l(b) + (1 + ba)l b 2 1 + ba = (ab)l(b) + b 2 log 1 + ba b (ab)l(b) + b 2 log 2b b 2 = al(b) + b 2 ≤ ab log 1 b + ab. Hence I(X, Y ; U ) = H(X, Y ) -H(Y |U, X) -H(X |U ) = H(X, Y )x,y f (p(x), p(y)) ≥ H(X, Y ) -x,y p(x)p(y) log 1 min{p(x), p(y)} + p(x)p(y) = E log 1 max{p(X), p(Y )} -1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table I EXTREME</head><label>I</label><figDesc>POINTS OF I XY AND THE CORRESPONDING EXTREME POINTS IN THE EGW, AND THEIR SUPPORT FUNCTION REPRESENTATIONS.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>there exists t ≤ H(Y |X) + H(X|Y ) such that (t -H(Y |X), t -H(X|Y ), t) ∈ I XY (by substituting t = I(X, Y ; U )). Taking convex combination of this point and (H(X), H(Y ), H(X, Y )) ∈ I XY , we have (H(X|Y ), H(Y |X), H(X|Y )+ H(Y |X)) ∈ I XY . The existence of 0 ≤ ǫ 1 ≤ log I(X; Y ) + 4 such that (0, H(Y |X)ǫ 1 , H(Y |X)) ∈ I XYcan be proved by substituting ǫ 1 = Ψ(X → Y ) and invoking the strong functional representation lemma<ref type="bibr" target="#b9">[10]</ref>. 4) The superadditivity property can be obtained from considering U = (U 1 , U 2 ), where (I(X</figDesc><table /></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>APPENDIX</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Proof of the converse of Theorem 1</head><p>To prove the converse, let U i = (M 0 , X i-1 , Y i-1 ). Consider</p><p>where the last inequality follows by Fano's inequality. Similarly</p><p>where the last inequality follows by Fano's inequality since</p><p>Hence the point (R 0 + ǫ, . . . , R 4 + ǫ) is in the convex hull of R for any ǫ &gt; 0. From (2), R is the increasing hull of an affine transformation of I XY , and thus is convex.</p><p>To prove the cardinality bound, we apply Fenchel-Eggleston-Carathéodory theorem <ref type="bibr" target="#b37">[38]</ref>, <ref type="bibr" target="#b38">[39]</ref> on the (|X ||Y|+2)-dimensional vectors with entries H(X|U = u), H(Y |U = u), H(X, Y |U = u) and p(x, y|u) for u ∈ {1, . . . , |U|}, (x, y) ∈ {1, . . . , |X |} × {1, . . . , |Y|}\(|X |, |Y|); see <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b18">[19]</ref>.</p><p>It can be seen from Figure <ref type="figure">2</ref> that I (v) is a subset of the convex hull of the following 9 points:</p><p>i.e., v together with the corner points of I o XY except (I(X; Y ), I(X; Y ), I(X; Y )). We will prove that for any w</p><p>is achievable in the extended Gray-Wyner system with noncausal complementary side information for ǫ &gt; 0. It suffices to prove the corner points R(v), R(p 1 ), . . . , R(p 8 ) are achievable. R(v) is achievable using the causal scheme in Theorem 1. To achieve R(p 1 ), R(p 2 ), R(p 3 ) and R(p 4 ), apply the causal scheme in Theorem 1 on U ← ∅, U ← X, U ← Y and U ← (X, Y ), respectively.</p><p>To achieve R(p 5 ), applying the strong functional representation lemma <ref type="bibr" target="#b9">[10]</ref>, there exists</p><p>for n large enough. We then apply the causal scheme on X ← X n , Y ← Y n and U ← V n . Similar for R(p 6 ).</p><p>We now prove the achievability of R(p 7 ). To generate the codebook, randomly partition</p><p>To encode x n , y n , find m 0 , m 3 such that (x n , y n ) ∈ B 3 (m 0 , m 3 ). Directly encode x n , y n into m 1 and m 2 respectively. Decoder 3 receives m 0 , m 3 , y n and output the unique xn such that (x n , y n ) ∈ B 3 (m 0 , m 3 ). The probability of error vanishes if H(Y ) &gt; H(X, Y ) + ǫ/2 -R 0 -R 3 , which is guaranteed by the definition of R(p 7 ). Decoder 4 receives m 0 , x n and output the unique ŷn such that (x n , ŷn ) ∈ B 0 (m 0 ). The probability of error vanishes if H(X) &gt; H(X, Y ) + ǫ/2 -R 0 , which is guaranteed by the definition of R(p 7 ).</p><p>The achievability of R(p 8 ) is similar to that of R(p 7 ). To generate the codebook, randomly partition</p><p>To encode x n , y n , find m 0 such that (x n , y n ) ∈ B 0 (m 0 ) and find the index m 1 . Directly encode y n into m 2 . Decoder 1 receives m 0 , m 1 and output x n where (x n , y n ) ∈ B 0 (m 0 ) with index m 1 . Decoder 3 receives m 0 , y n and output the unique xn such that (x n , y n ) ∈ B 0 (m 0 ). The probability of error vanishes if H(Y ) &gt; H(X, Y ) + ǫ/2 -R 0 , which is guaranteed by the definition of R(p 8 ). Decoder 4 receives m 0 , x n and output the unique ŷn such that (x n , ŷn ) ∈ B 0 (m 0 ). The probability of error vanishes if H(X) &gt; H(X, Y )+ ǫ/2 -R 0 , which follows from the definition of R(p 8 ) and H(X) ≥ H(Y ).</p><p>Hence we have proved that for any point v ∈ I XY and</p><p>XY , the rate tuple R(w) is achievable. In other words, the region</p><p>XY , then by Theorem 2, the following rate tuple is achievable</p><p>i.e. for the source X l , Y l , the probability of error P e (l) → 0 as l → ∞. Apply this scheme n times on the source X nl , Y nl . This can be considered as a causal scheme on the source sequence (X l 1 , Y l 1 ), (X 2l l+1 , Y 2l l+1 ), . . . , (X nl (n-1)l+1 , Y nl (n-1)l+1 ) with rate tuple lR(w) and symbol error probability P e (l). Hence by ( <ref type="formula">9</ref>) and <ref type="bibr" target="#b9">(10)</ref> in the proof of Theorem 1,</p><p>and similar for the other 3 dimensions, which implies vw ∞ ≤ 2ǫ ′ . The result follows from taking l → ∞, ǫ → 0. To show</p><p>note that they are both equal to the union of the convex hulls of {v, p 1 , . . . , p 8 } for v ∈ I XY (as in the proof of Theorem 2).</p><p>2) The equivalence between cl(I ∞ XY ) and R ′ is proved in the Fourier-Motzkin elimination step in the proof of Theorem 2. 3) By Proposition 2,</p><p>For the other direction, </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Source coding for a simple network</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Wyner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bell Syst. Tech. J</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1681" to="1721" />
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The common information of two dependent random variables</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Wyner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Theory</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="163" to="179" />
			<date type="published" when="1975-03">Mar. 1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Common information is far less than mutual information</title>
		<author>
			<persName><forename type="first">P</forename><surname>Gács</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Körner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Probl. Control Inf. Theory</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="149" to="162" />
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Coordination capacity</title>
		<author>
			<persName><forename type="first">P</forename><surname>Cuff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Permuter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">M</forename><surname>Cover</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Info. Theory</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="4181" to="4206" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Tishby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">C</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Bialek</surname></persName>
		</author>
		<idno>arXiv preprint physics/0004057</idno>
		<title level="m">The information bottleneck method</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Communicating via a processing broadcast satellite</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Wyner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">M J</forename><surname>Willems</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1243" to="1249" />
			<date type="published" when="2002-06">Jun 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multiterminal source coding with complementary delivery</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kimura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Uyematsu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Symp</title>
		<meeting>IEEE Int. Symp<address><addrLine>Seoul, Korea</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006-10">October 2006</date>
			<biblScope unit="page" from="189" to="194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Coding of an information source having ambiguous alphabet and the entropy of graphs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Körner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1973">1973</date>
			<biblScope unit="page" from="411" to="425" />
		</imprint>
	</monogr>
	<note>in 6th Prague conference on information theory</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">From the information bottleneck to the privacy funnel</title>
		<author>
			<persName><forename type="first">A</forename><surname>Makhdoumi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Salamatian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Fawaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Medard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Information Theory Workshop (ITW)</title>
		<imprint>
			<date type="published" when="2014-11">2014. Nov 2014</date>
			<biblScope unit="page" from="501" to="505" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Strong functional representation lemma and applications to coding theorems</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">El</forename><surname>Gamal</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1701.02827" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multivariate information transmission</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">J</forename><surname>Mcgill</surname></persName>
		</author>
		<idno type="DOI">10.1007/BF02289159</idno>
		<ptr target="http://dx.doi.org/10.1007/BF02289159" />
	</analytic>
	<monogr>
		<title level="j">Psychometrika</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="97" to="116" />
			<date type="published" when="1954">1954</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Source coding with limited-look-ahead side information at the decoder</title>
		<author>
			<persName><forename type="first">T</forename><surname>Weissman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">El</forename><surname>Gamal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Theory</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5218" to="5239" />
			<date type="published" when="2006-12">Dec. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Distributed channel synthesis</title>
		<author>
			<persName><forename type="first">P</forename><surname>Cuff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Info. Theory</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="7071" to="7096" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The quantum reverse shannon theorem and resource tradeoffs for simulating quantum channels</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Devetak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Harrow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">W</forename><surname>Shor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Winter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Info. Theory</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2926" to="2959" />
			<date type="published" when="2014-05">May 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A theorem on the entropy of certain binary sequences and applications-II</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Wyner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Theory</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="772" to="777" />
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Source coding with side information and a converse for degraded broadcast channels</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ahlswede</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Körner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Theory</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="629" to="637" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">On source coding with side information at the decoder</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Wyner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Theory</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="294" to="300" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Two lossy source coding problems with causal side-information</title>
		<author>
			<persName><forename type="first">R</forename><surname>Timo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">N</forename><surname>Vellambi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE International Symposium on Information Theory</title>
		<imprint>
			<date type="published" when="2009-06">June 2009</date>
			<biblScope unit="page" from="1040" to="1044" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The rate-distortion function for source coding with side information at the decoder</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">D</forename><surname>Wyner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ziv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Inf. Theory</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Rate distortion when side information may be absent</title>
		<author>
			<persName><forename type="first">C</forename><surname>Heegard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Berger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="727" to="734" />
			<date type="published" when="1985-11">Nov 1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">On successive refinement for the Wyner-Ziv problem</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Steinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Merhav</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium onInformation Theory</title>
		<imprint>
			<date type="published" when="2004">2004. 2004</date>
			<biblScope unit="page" from="364" to="364" />
		</imprint>
	</monogr>
	<note>ISIT 2004. Proceedings</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Side-information scalable source coding</title>
		<author>
			<persName><forename type="first">C</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">N</forename><surname>Diggavi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5591" to="5608" />
			<date type="published" when="2008-12">Dec 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Assisted common information with an application to secure two-party sampling</title>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">M</forename><surname>Prabhakaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Prabhakaran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="3413" to="3434" />
			<date type="published" when="2014-06">June 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">M</forename><surname>Prabhakaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">M</forename><surname>Prabhakaran</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.6285</idno>
		<title level="m">Tension bounds for information complexity</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">On sequences of pairs of dependent random variables</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">S</forename><surname>Witsenhausen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="100" to="113" />
			<date type="published" when="1975">1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Source coding and graph entropies</title>
		<author>
			<persName><forename type="first">N</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Orlitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1329" to="1339" />
			<date type="published" when="1996-09">Sep 1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">New monotones and lower bounds in unconditional two-party computation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wullschleger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2792" to="2797" />
			<date type="published" when="2008-06">June 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A new dual to the Gács-Körner common information defined via the Gray-Wyner system</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kamath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Anantharam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Communication, Control, and Computing (Allerton), 2010 48th Annual Allerton Conference on</title>
		<imprint>
			<date type="published" when="2010-09">Sept 2010</date>
			<biblScope unit="page" from="1340" to="1346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Notes on information-theoretic privacy</title>
		<author>
			<persName><forename type="first">S</forename><surname>Asoodeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Alajaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Linder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Communication, Control, and Computing (Allerton), 2014 52nd Annual Allerton Conference on</title>
		<imprint>
			<date type="published" when="2014-09">Sept 2014</date>
			<biblScope unit="page" from="1272" to="1278" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Griffith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.03706</idno>
		<title level="m">Synergy, redundancy and common information</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Common randomness in information theory and cryptography. II. CR capacity</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ahlswede</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Csiszar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="225" to="240" />
			<date type="published" when="1998-01">Jan 1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A conditional entropy bound for a pair of discrete random variables</title>
		<author>
			<persName><forename type="first">H</forename><surname>Witsenhausen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Wyner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="493" to="501" />
			<date type="published" when="1975-09">Sep 1975</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">On maximal correlation, hypercontractivity, and the data processing inequality studied by erkip and cover</title>
		<author>
			<persName><forename type="first">V</forename><surname>Anantharam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Gohari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kamath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Nair</surname></persName>
		</author>
		<idno>abs/1304.6133</idno>
		<ptr target="http://arxiv.org/abs/1304.6133" />
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Spreading of sets in product spaces and hypercontraction of the Markov operator</title>
		<author>
			<persName><forename type="first">R</forename><surname>Ahlswede</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Gács</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The annals of probability</title>
		<imprint>
			<date type="published" when="1976">1976</date>
			<biblScope unit="page" from="925" to="939" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Fundamental limits of perfect privacy</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">P</forename><surname>Calmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Makhdoumi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Medard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Symposium on Information Theory (ISIT)</title>
		<imprint>
			<date type="published" when="2015-06">June 2015</date>
			<biblScope unit="page" from="1796" to="1800" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">The communication complexity of correlation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Harsha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Radhakrishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Info. Theory</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="438" to="449" />
			<date type="published" when="2010-01">Jan 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Algorithms and complexity: new directions and recent results</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">E</forename><surname>Knuth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Yao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1976">1976</date>
			<biblScope unit="page" from="357" to="428" />
		</imprint>
	</monogr>
	<note>The complexity of nonuniform random number generation</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">G</forename><surname>Eggleston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Convexity</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1958">1958</date>
			<publisher>Cambridge University Press</publisher>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>Rockafellar</surname></persName>
		</author>
		<title level="m">Convex Analysis</title>
		<meeting><address><addrLine>Princeton, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Princeton University Press</publisher>
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">El</forename><surname>Gamal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-H</forename><surname>Kim</surname></persName>
		</author>
		<title level="m">Network information theory</title>
		<imprint>
			<publisher>Cambridge university press</publisher>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
