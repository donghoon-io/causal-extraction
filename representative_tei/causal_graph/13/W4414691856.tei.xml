<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Causal Reasoning Elicits Controllable 3D Scene Generation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2025-09-18">18 Sep 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Shen</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Zhejiang University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ruiyu</forename><surname>Zhao</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">East China</orgName>
								<orgName type="institution">University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jiale</forename><surname>Zhou</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">East China</orgName>
								<orgName type="institution">University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zongkai</forename><surname>Wu</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Skai Intelligence</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jenq-Neng</forename><surname>Hwang</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">University of Washington</orgName>
								<address>
									<addrLine>5 VitaSight</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lei</forename><surname>Li</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">University of Washington</orgName>
								<address>
									<addrLine>5 VitaSight</addrLine>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Causal Reasoning Elicits Controllable 3D Scene Generation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-09-18">18 Sep 2025</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2509.15249v1[cs.GR]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-21T20:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Existing 3D scene generation methods often struggle to model the complex logical dependencies and physical constraints between objects, limiting their ability to adapt to dynamic and realistic environments. We propose CausalStruct, a novel framework that embeds causal reasoning into 3D scene generation. Utilizing large language models (LLMs), We construct causal graphs where nodes represent objects and attributes, while edges encode causal dependencies and physical constraints. CausalStruct iteratively refines the scene layout by enforcing causal order to determine the placement order of objects and applies causal intervention to adjust the spatial configuration according to physics-driven constraints, ensuring consistency with textual descriptions and real-world dynamics. The refined scene causal graph informs subsequent optimization steps, employing a Proportional-Integral-Derivative(PID) controller to iteratively tune object scales and positions. Our method uses text or images to guide object placement and layout in 3D scenes, with 3D Gaussian Splatting and Score Distillation Sampling improving shape accuracy and rendering stability. Extensive experiments show that CausalStruct generates 3D scenes with enhanced logical coherence, realistic spatial interactions, and robust adaptability.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Introduction</head><p>In recent years, 3D scene generation has advanced significantly in computer vision, graphics, and content creation. However, traditional methods still rely heavily on manual modeling and expert knowledge, making multi-object scene construction time-consuming and costly. Existing text-to-3D approaches have attempted to address this by using 2D diffusion models for optimizing 3D representations <ref type="bibr" target="#b37">(Tang et al. 2024;</ref><ref type="bibr" target="#b46">Yi et al. 2023;</ref><ref type="bibr" target="#b11">Fridman et al. 2024)</ref> or employing 3D diffusion models for direct asset generation <ref type="bibr" target="#b14">(Hong et al. 2024;</ref><ref type="bibr" target="#b52">Zhou, Zhang, and Liu 2025)</ref>. While these approaches have demonstrated success in synthesizing individual objects, they struggle with multi-object scene composition, often resulting in geometric distortion, spatial inconsistencies, and object drift.</p><p>A well-structured spatial layout is essential for generating coherent 3D scenes, as it dictates object placement. Previous layout-generation approaches <ref type="bibr" target="#b51">(Zhou et al. 2024;</ref><ref type="bibr" target="#b36">Sun et al. 2023;</ref><ref type="bibr">Feng et al. 2024b;</ref><ref type="bibr" target="#b50">Zheng et al. 2024;</ref><ref type="bibr" target="#b44">Yang, Hu, and Ye 2021;</ref><ref type="bibr">Lin et al. 2023c</ref>) rely on data-driven heuristics or LLM-based inference to ensure semantic and structural consistency. However, these methods primarily focus on static spatial constraint placement and overlook the interactions between objects on the overall scene.</p><p>Causal reasoning in 3D scene generation establishes a hierarchy of directed dependencies among objects, ensuring their spatial and functional interactions adhere to realworld physical principles. Rooted in structural causal models (SCMs) <ref type="bibr" target="#b27">(Neuberg 2003)</ref>, it transcends statistical correlations by explicitly defining how the presence or state of one object causally influences the placement, orientation, or existence of others. Without causal reasoning, layout methods are unable to dynamically model object interactions, resulting in scenes with misaligned spatial relationships, improper functional placements, or floating objects that violate realworld physics.</p><p>To overcome these challenge, we propose CausalStruct, a novel framework that integrates causal reasoning into scene graph optimization. Using LLMs <ref type="bibr" target="#b15">(Hurst et al. 2024;</ref><ref type="bibr">Cai et al. 2025b)</ref>, we construct a scene graph where nodes rep-resent objects and attributes, and edges encode relationships and physical dependencies. However, the LLM alone cannot accurately construct scene graphs due to its neglect of node properties and edge interactions, resulting in unrealistic scenes that defy physical laws. Inspired by causal reasoning in structure discovery and relationship modeling <ref type="bibr" target="#b18">(Kıcıman et al. 2023;</ref><ref type="bibr" target="#b39">Vashishtha et al. 2023)</ref>, we introduce a causal order mechanism to enforce logical sequencing in object placement. By computing a causal precedence through pairwise LLM reasoning, we ensure that objects follow physically consistent dependencies. Furthermore, to address uncertain or inconsistent edges, we compute the confidence of each edge using a Bayesian estimation <ref type="bibr">(Cai et al. 2025a</ref>) and determine whether to apply a causal intervention based on this confidence, where interventions on object states validate relationships through their physical impact on other scene elements, guiding whether to modify or retain the edge.</p><p>To refine the attributes of nodes (objects) in the causal scene graph and adjust them to realistic spatial proportions, we optimize attributes using a Multimodal Large Language Model (MLLM) <ref type="bibr" target="#b15">(Hurst et al. 2024;</ref><ref type="bibr" target="#b20">Li 2024</ref>) that assesses spatial relationships. Each edge in the causal graph is assigned an attribute correction score, quantifying discrepancies in size and position. To ensure physically plausible adjustments, we apply a Proportional-Integral-Derivative (PID) controller <ref type="bibr" target="#b42">(Willis 1999;</ref><ref type="bibr" target="#b6">Crowe et al. 2005)</ref>, iteratively refining object attributes while maintaining scene stability. For enhanced geometric consistency and rendering stability, we integrate 3D Gaussian Splatting (3DGS) <ref type="bibr">(Kerbl et al. 2023;</ref><ref type="bibr" target="#b5">Chen, Zhou, and Li 2025)</ref> with Score Distillation Sampling (SDS) <ref type="bibr" target="#b30">(Poole et al. 2022</ref>) at both the object and scene levels. Additionally, our method supports text-only and text-image combinations, enabling improved physical simulations and generating coherent 3D scenes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Related Work</head><p>Text-Driven 3D Generation NeRF-based methods, such as DreamFusion <ref type="bibr" target="#b30">(Poole et al. 2022)</ref> and Score Jacobian Chaining <ref type="bibr" target="#b40">(Wang et al. 2023)</ref>, leverage 2D diffusion models <ref type="bibr" target="#b32">(Rombach et al. 2022;</ref><ref type="bibr" target="#b33">Saharia et al. 2022</ref>) to synthesize single objects. Subsequent works, including Magic3D <ref type="bibr">(Lin et al. 2023a</ref>), Latent-NeRF <ref type="bibr" target="#b26">(Metzer et al. 2023)</ref>, and 3DFuse <ref type="bibr" target="#b35">(Seo et al. 2023)</ref>, aim to enhance 3D generation quality under SDS constraints. ProlificDreamer <ref type="bibr" target="#b41">(Wang et al. 2024</ref>) models 3D parameters as random variables, and introduces Variational Score Distillation (VSD) for improved op-timization. While NeRF-based approaches effectively generate high-quality 3D objects, they suffer from inefficiency.</p><p>To improve efficiency, 3DGS-based approaches <ref type="bibr" target="#b37">(Tang et al. 2024;</ref><ref type="bibr" target="#b46">Yi et al. 2023)</ref> have been proposed for text-to-3D generation by integrating diffusion models with Gaussian splatting. Recent methods <ref type="bibr" target="#b52">(Zhou, Zhang, and Liu 2025;</ref><ref type="bibr">Zhang et al. 2024a;</ref><ref type="bibr" target="#b5">Chen et al. 2024;</ref><ref type="bibr" target="#b16">Jiang et al. 2024</ref>) utilize textto-3DGS pipelines to facilitate object synthesis, achieving faster generation. While these methods enable diverse 3D generation from text prompts, they struggle to produce photorealistic multi-object scenes with complex geometry and high-fidelity textures due to their reliance on high-level semantic priors.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LLMs for Causal Discovery</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>As shown in Fig. <ref type="figure" target="#fig_1">2</ref>, our method constructs causal-driven 3D Gaussian representations by integrating causal reasoning, PID-based optimization, and layout-guided representation. First, given a text description, we generate an initial scene graph using LLM and refine object relationships through causal reasoning. Second, to ensure spatial balance, PID Control optimizes object scale and position while preventing abrupt changes. Finally, Layout-Guided Representation builds the scene with 3DGS and optimizes it using Diffusion and SDS for spatial consistency and high-fidelity rendering.</p><p>Preliminaries 3D Gaussian Splatting (3DGS) represents 3D scenes using anisotropic Gaussian primitives, denoted as</p><formula xml:id="formula_0">{G n | n = 1, . . . , N }, with parameters including position µ n ∈ R 3 , covariance Σ n ∈ R 7 , color c n ∈ R 3 ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>and opacity</head><p>α n ∈ R. The Gaussian function is defined as:</p><formula xml:id="formula_1">G n (p) = e -1 2 (p-µn) T Σ -1 n (p-µn) ,<label>(1)</label></formula><p>where Σ n is parameterized by a rotation matrix R n ∈ R 4 and a scaling matrix S n ∈ R 3 . For rendering, differential splatting projects the Gaussians onto camera planes, using a viewing transformation W n and the Jacobian matrix J n to obtain a transformed covariance. The color for a ray r is computed as:</p><formula xml:id="formula_2">C r (x) = i∈M c i σ i i-1 j=1 (1 -σ j ), σ i = α i G 2D i (x). (2)</formula><p>An adaptive density control mechanism dynamically adjusts the number of Gaussians to balance computational efficiency and scene detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Causal Reasoning for Scene Graph</head><p>Causal Order for Scene Graph Optimization Ensuring a consistent and physically plausible scene layout, we define causal precedence among objects to establish a logical placement sequence. Given a set of objects O = {o 1 , o 2 , ..., o n } and their spatial relations E = {e i,j |o i , o j ∈ O}, directly from a LLM, we define the causal order ≺ as:</p><formula xml:id="formula_3">o i ≺ o j ⇐⇒ C i,j &gt; C j,i ,<label>(3)</label></formula><p>where o i → o j represents a directed causal edge indicating that the placement of o j depends on o i .</p><formula xml:id="formula_4">C i,j = P(o i ≺ o j | LLM(o i , o j )) represents the probability of o i causally pre- ceding o j . If C i,j &gt; C j,i</formula><p>, we adjust the scene such that o i is placed before o j in spatial reasoning, updating the edge set E to the refined set E order by enforcing the inferred causal order.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bayesian Edge Estimation</head><p>The hallucinations of LLM lead to causal graphs containing false or physically unreasonable edges during linguistic reasoning. We introduce Bayesian Edge Estimation to assess the confidence of each edge before Causal Intervention.</p><p>Prior: The prior probability p(e i,j ) of an edge e i,j represents the inherent plausibility of the causal relationship between objects o i and o j , before observing any additional evidence. Instead of assuming a uniform prior, we obtain the prior directly from LLM.</p><p>Posterior: For a candidate edge e i,j , its posterior probability given the Causal Order edges E order is computed using Bayes' rule:</p><formula xml:id="formula_5">p(e i,j | E order ) = p(E order | e i,j )p(e i,j ) p(E order ) ,<label>(4)</label></formula><p>where p(E order | e i,j ) is the likelihood that the LLMgenerated edge set is observed given that e i,j is correct, and p(E order ) is the normalizing factor ensuring a valid probability distribution. We assume that the likelihood of E order given e i,j is decomposable across all edges in E order :</p><formula xml:id="formula_6">p(E order | e i,j ) = (oi,oj )∈Eorder p(LLM(o i , o j ) | e i,j ),<label>(5)</label></formula><p>where p(LLM(o i , o j ) | e i,j ) represents the probability that the LLM correctly predicts the causal relationship between objects o i and o j , obtained by querying the model multiple times and aggregating its responses.</p><p>Causal Intervention for Edge Refinement For edges with uncertain posterior probabilities, we conduct causal interventions using an MLLM to verify their correctness.</p><p>Given two objects o i and o j connected by an edge e i,j , we evaluate the impact of modifying their spatial relationship on the entire scene. This intervention is performed by iterating over a set of candidate placements for the object and analyzing their effect using the rendered scene image. The MLLM determines whether each placement results in a physically and semantically plausible configuration.</p><p>To quantify the likelihood of each possible adjustment, we define the probability of D i,j as:</p><formula xml:id="formula_7">p(D i,j | e i,j ) = r,s∈S p(D i,j = s | do(e i,j = r)), (6)</formula><p>where S is the set of all candidate placements for object o j , determined by a predefined position list. do(e i,j = r)) means to interfere with edge e i,j to force its state to be set to r. p(D i,j = s | do(e i,j = r)) represents the probability of state s being the modification for do(e i,j = r)), which is estimated using an MLLM evaluation:</p><formula xml:id="formula_8">p(D i,j = s | do(e i,j = r)) = 1 K K k=1 I MLLM(o i , o j , I r ei,j ) → s ,<label>(7)</label></formula><p>where I(•) is an indicator function that returns 1 if the MLLM selects state s for edge e i,j in the k-th trial, otherwise returns 0. I r ei,j is the rendered scene image when object o j is placed at candidate position r ∈ S, and the MLLM evaluates whether this placement is reasonable. The intervention decision is determined by selecting the placement modification:</p><formula xml:id="formula_9">s * = arg max s∈S p(D i,j = s | do(e i,j = r)). (<label>8</label></formula><formula xml:id="formula_10">)</formula><p>Update Strategy Edges are classified into three categories based on their posterior probability and causal intervention results:</p><formula xml:id="formula_11">e i,j =    e i,j p ei,j &gt; τ 1 s * p ei,j ≤ τ 1 &amp;p(E order | s * ) &gt; τ 2 ∅ p ei,j ≤ τ 1 &amp;p(E order | s * ) ≤ τ 2 ,<label>(9)</label></formula><p>where τ 1 and τ 2 are confidence thresholds, p ei,j represents p(E order | e i,j ), p(E order | s * ) represents the posterior probability of s * , and ∅ means remove the edge. This ensures that high-confidence edges remain, mediumconfidence edges are validated through interventions, and low-confidence edges are discarded, leading to a reliable causal scene graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PID Control Object Optimization</head><p>Causal reasoning determines the initial placement of objects, while the PID controller fine-tunes their positions and scales to ensure physical plausibility and spatial accuracy. Each object pair (edge) in the scene graph is evaluated using an MLLM to accurately adjust edges, generating a scale correction score and position scores for precise spatial refinement.</p><p>We propose an optimization method based on a Proportional-Integral-Derivative (PID) controller to ensure proportional accuracy and visual balance in reconstructed scenes. The PID controller can effectively handle the nonindependent relationships between edges by dynamically adjusting the proportional, integral, and derivative parameters to to ensure that the objects in the edges do not overlap and ensure overall spatial coordination and accuracy.</p><p>The error signal, defined as the negation of the score, drives the PID controller to adjust the scale and position. The control signal u is computed using the error e, accumulated error e dt, and change in error over time de dt :</p><formula xml:id="formula_12">u = K p e + K i e dt + K d de dt ,<label>(10)</label></formula><p>where K p , K i and K d are the proportional, integral, and derivative coefficients.</p><p>To implement PID control effectively, we introduce an actuator that converts the control signal u into practical adjustments of the scene's scale and position. The actuator ensures that adjustments remain within a predefined range, preventing instability or produce unrealistic results. The actuator output ∆ is formulated as a nonlinear function of u, employing the hyperbolic tangent function to achieve smooth scaling:</p><formula xml:id="formula_13">Γ = ∆ • tanh( u γ ),<label>(11)</label></formula><p>where ∆ denotes the maximum permissible adjustment, and γ modulates the steepness of the response. This transformation constrains the output within the range -∆ to ∆, ensuring smooth and controlled adjustments while preventing abrupt shifts in scale or position.</p><p>Once the nodes and edges are optimized, they are organized into a graph based on inter-object relationships, such as aligned spatial attributes and semantic associations. The LLM then directs the placement of these subgraphs by interpreting high-level prompts that specify the expected sizes, relationships, and spatial context of the objects. Additionally, based on the attribute values of each node, MLLM is utilized to finely adjust the scene graph, ensuring that the final structure accurately aligns with the textual description.</p><p>Prompt: A woman walks her dog on a dirt path as a cyclist passes her on the left, both heading toward a wooden pavilion. The path is lined with shrubs on one side and a wooden fence on the other. A jogger passes by on the opposite side and a pigeon sits on a park bench.</p><p>Prompt: A study with a desk and a chair, and three bookshelves side by side, a laptop and a lamp on the desk.</p><p>Prompt: A modern living room with a gray sofa, a wooden coffee table, a tv stand with a flat-screen television, a floor air conditioner and a floor lamp on the floor.</p><p>Prompt: A kitchen with a white cabinet, a black countertop, a stainless steel refrigerator, an oven, a dining table with four chairs, and a light on table. A cozy study room features a large wooden desk with a big monitor, a keyboard, a touchpad, and a small potted plant; a desktop case sits on the left side, and an office chair near the desk ,while a file cabinet and a trash bin are on the right. Another   </p><note type="other">DreamGaussian GaussianDreamer MVDream Gala3D GraphDreamer Ours GSGEN</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Layout-guided Representation</head><p>Based on causal reasoning for scene optimization and PID controller for object state adjustments, we obtain a structured layout, where each object is assigned a position based on its inferred dependencies. The layout provides center coordinates, and object sizes.</p><p>Object Representation Each object is optimized independently using MVDream <ref type="bibr" target="#b19">(Lee and Kim 2023)</ref> or Zero123 <ref type="bibr" target="#b25">(Liu et al. 2023)</ref> with Score Distillation Sampling (SDS):</p><formula xml:id="formula_14">∇ Gobj L = E ϵ,η λ obj (ϵ ϕ (I obj ; t, β, η) -ϵ) ∂I ∂G obj ,<label>(12)</label></formula><p>where G obj represents object parameters, I obj is the rendered object image, and t, β, η correspond to time step, camera parameters, and noise conditioning, respectively.</p><p>Scene Representation Simply placing objects directly in the scene makes it difficult to maintain overall coherence.</p><p>To address this, we optimize the entire scene using Stable Diffusion <ref type="bibr" target="#b32">(Rombach et al. 2022)</ref> with SDS, ensuring consistency in object interactions. The full-scene optimization follows as:</p><formula xml:id="formula_15">∇ Gscene L = E ϵ,p λ scene (ϵ ϕ (I Fscene ; t, β, p) -ϵ) ∂I ∂G scene ,<label>(13)</label></formula><p>where p represents the description of the entire scene, G scene represents the global scene parameters, I Fscene is the rendered full-scene image, and F scene encodes the structured layout with 3D Gaussian properties and spatial parameters for all objects:</p><formula xml:id="formula_16">F scene = {G i , x i , y i , z i , s i } N i=1 ,<label>(14)</label></formula><p>where G i represents the 3D Gaussian attributes of object i, while (x i , y i , z i ) denote the center coordinates, and s i define the object scale. This joint optimization ensures that objects are not only individually refined but also integrated into a spatially coherent and semantically meaningful scene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiments</head><p>Experimental Setup Our approach is implemented in Py-Torch <ref type="bibr" target="#b29">(Paszke et al. 2019</ref>). We employ GPT-4o <ref type="bibr">(Hurst et</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison of Methods</head><p>Quantitative Comparison We report quantitative results in Table <ref type="table" target="#tab_3">1</ref>. We assess our approach on the Text-to-3D task by comparing it with mainstream methods, including DreamGaussian <ref type="bibr" target="#b37">(Tang et al. 2024)</ref>, GaussianDreamer <ref type="bibr" target="#b12">(Gao et al. 2024)</ref>, MVDream (Lee and Kim 2023), GSGen <ref type="bibr" target="#b5">(Chen et al. 2024</ref>), GALA3D (Doe and Smith 2023), and Graph-Dreamer <ref type="bibr" target="#b12">(Gao et al. 2024</ref>). To ensure a fair evaluation, we adopt CLIP Score, following prior works, to measure the alignment between generated images and their corresponding textual descriptions. Given the inherent randomness in MLLMs, we utilize multiple MLLMs to assess the semantic consistency between generated scenes and input descriptions from different perspectives. Notably, higher CLIP or MLLM scores indicate better performance. By aggregating evaluations from CLIP and various MLLMs, our approach achieves the highest performance, demonstrating superior scene-object alignment and semantic coherence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Qualitative Comparison</head><p>We report qualitative comparisons on text-to-3D generation in Fig. <ref type="figure" target="#fig_2">3</ref>. Compared to existing methods, our approach demonstrates superior spa-tial consistency and causal alignment. While prior methods primarily focus on single-object generation or data-driven scene synthesis, they often struggle with incorrect object relationships and spatial inconsistencies. In contrast, our method leverages causal order and intervention optimization to refine object interactions, ensuring that generated scenes adhere to real-world semantics and physical constraints. Additionally, our PID-based optimization maintains proportional accuracy, while diffusion-guided 3DGS refinement enhances overall rendering quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scene Editing</head><p>As illustrated in Fig. <ref type="figure" target="#fig_4">4</ref>, scene editing in our approach facilitates flexible and controllable modifications via text descriptions. LLMs translate user descriptions into layout transformations, such as adding, removing, or repositioning objects. The Layout-Guided Representation is subsequently optimized within the edited regions, maintaining stability in the unchanged areas. Notably, our editing process accounts for the causal relationships that govern typical object placement, ensuring that modifications to position and scale align with real world spatial logic. This approach supports spatial adjustments, object interactions, and style modifications, offering a seamless and intuitive 3D scene editing experience grounded in causal reasoning.</p><p>In addition to generating 3D scenes from text descriptions, our method also supports text-image-based generation. Each node in the scene can receive image inputs, which guide the output of the node, enabling image-to-3D conversion. This integration enables text and image modes to work synergistically, where text provides global semantic constraints while images inject local geometric priors, thereby enhancing both the realism and physical plausibility of the synthesized scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Study</head><p>Adaptability and Robustness Our framework incorporates localized, distilled LLMs and MLLMs, facilitating</p><formula xml:id="formula_17">prompt &amp; nodes DeepSeek8b &amp; LLava34b DeepSeek14b &amp; LLava34b DeepSeek-R1 &amp; GPT4o GPT4o &amp; GPT4o</formula><p>A cozy children park features a red slide, two swings, a mini climbing wall, a sandpit with small shovel and bucket, a picnic table with bench, a small tree.   <ref type="table" target="#tab_4">2</ref>, we assess multiple model configurations, demonstrating that our causal graph-based framework effectively captures object relationships and spatial dependencies. Leveraging a diverse set of models, our approach ensures robustness and stability in scene generation under varying computational constraints. Moreover, our framework efficiently adapts to varying input complexities, ensuring consistent spatial reasoning across diverse scenarios.</p><p>Causal Reasoning To evaluate the impact of causal order and causal intervention on scene generation, we conduct ablation studies comparing scene graphs constructed from standard LLM parsing with those refined using causal reasoning. As shown in Fig. <ref type="figure">5</ref>, Fig. <ref type="figure" target="#fig_6">6</ref> and Table <ref type="table" target="#tab_4">2</ref>, removing the causal order mechanism leads to missing essential object relationships, resulting in incomplete or incorrect connections. Without Causal intervention to validate edges, the scene graph retains erroneous relationships, causing misaligned objects, floating placements, and unrealistic spatial arrangements. Integrating causal ordering and causal intervention validation enhances spatial consistency, object in-teractions, and physical plausibility. These results highlight the need for causal reasoning in ensuring well-structured 3D representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PID Controller</head><p>The PID controller regulates node attributes (object scales and positions) to maintain spatial consistency and enhance structural precision. As shown in Fig. <ref type="figure" target="#fig_6">6</ref>, proportional optimization without PID control relies on fixed value updates, which are highly sensitive to erroneous LLM scores. These errors often propagate as perturbations in the system, causing fluctuations, directional misalignment, or even inverted object orientations-ultimately destabilizing scene layouts. In contrast, PID-based refinement addresses this limitation through a dynamic errorcorrection mechanism: the proportional term responds to immediate discrepancies, the integral term compensates for accumulated historical errors, and the derivative term anticipates abrupt changes. This multi-component control strategy effectively dampens noise from LLM evaluations, enabling smoother convergence. Through multi-term error compensation, PID dynamically regulates node attributes to generate physically coherent scenes in the case of abnormal LLM output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, we proposed CausalStruct, a causal-driven framework for 3D scene generation, integrating causal reasoning, PID control, and Diffusion refinement. By leveraging LLMs and MLLMs, our method constructs a causal scene graph, ensuring that object relationships align with real-world semantics and physical constraints. Moreover, our approach adapts to varying scene complexities, ensuring stable optimization across different generation tasks.</p><p>Through PID control, we maintain proportional accuracy and spatial consistency, while 3DGS with SDS optimization enhances object fidelity and rendering quality. Experimental results show that CausalStruct improves scene composition, object interactions, and multi-view consistency, generating structured and semantically coherent 3D scenes. Our work demonstrates the potential of causal reasoning and PID control in 3D generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Clarify of Causal Reasoning</head><p>In the introduction and related works, we define "Causal Reasoning (CR)" as explicitly modeling relationships between objects (e.g., spatial or functional relationships) while excluding irrelevant variables to address ambiguity in object placement during scene generation. This aligns with the definition of CR in LLMs <ref type="bibr" target="#b43">(Xiong et al. 2024)</ref>. In image generation, consistent with Image Content Generation with Causal Reasoning <ref type="bibr" target="#b21">(Li et al. 2024)</ref>, CR in generative tasks aims to structure latent relationships between objects, compensating for missing details of objects during the generation process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Details of Bayesian Edge Estimation</head><p>In our Bayesian Edge Estimation framework, we assume that each edge in the causal order edge set E order is independent given e i,j . However, edges may remain not independent in scenarios. The PID controller effectively addresses these dependencies by dynamically adjusting its parameters, ensuring comprehensive spatial coordination and precision across the framework. This assumption allows us to apply the product rule for independent events in probability theory. we compute the probability of the entire edge set occurring under the condition e i,j , referring to formula 5 in the main text. Under the condition that e i,j is valid, the probability of each side can be calculated separately, without being affected by the presence of other sides in the set. This simplifies the calculation and allows us to decompose the joint probability into the product of the individual probabilities:</p><p>p(E order | e i,j ) = p(e a,b | e i,j )p(e c,d | e i,j ) . . .</p><p>This calculation mode is particularly advantageous in LLMbased probability estimation, as it allows each edge to be evaluated independently without the need to explicitly model dependencies between them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Details of PID controller</head><p>In our framework, causal reasoning and PID control operate as two distinct stages to ensure accurate scene reconstruction. Causal reasoning is first used to generate an initial object placement by inferring spatial dependencies from textual descriptions. However, these placements lack precise spatial alignment, leading to minor positional errors, intersections, or unrealistic gaps between objects.</p><p>To refine these placements, we employ a PID controller, which fine-tunes object positions and scales to ensure that the spatial configuration remains physically plausible. The PID controller takes the initial positions inferred from causal reasoning and iteratively adjusts them to minimize spatial discrepancies.</p><p>Why Use an Actuator? A key challenge in applying PID adjustments is preventing instability and overcorrection. As illustrated in Fig. <ref type="figure" target="#fig_8">7</ref>, directly applying the PID output can lead to abrupt jumps, oscillations, or physically unrealistic movements, especially when the error signal fluctuates. To mitigate this, we introduce an actuator that smooths the adjustment process, ensuring controlled and realistic modifications to object placement. A bedroom with a bed, two nightstands, a square table with a table lamp on it, a wardrobe, and a TV stand with a flat-screen TV. d ← e -e prev {Compute derivative} 8:</p><formula xml:id="formula_19">u ← K p • e + K i • E + K d • d {PID control signal} 9:</formula><p>Γ ← ∆ • tanh (u/γ) {Actuator output} 10:</p><formula xml:id="formula_20">℘ current ← ℘ current + Γ {Update attribute} 11:</formula><p>e prev ← e 12:</p><p>i ← i + 1 13: until |e| ≤ ϵ or i ≥ N 14: return ℘ current Actuators are commonly used in control systems to translate control signals into gradual, physically constrained movements <ref type="bibr" target="#b6">(Crowe et al. 2005)</ref>. In our case, the actuator controls position and scale adjustments, mitigating abrupt, excessive corrections that may introduce additional inconsistencies.</p><p>Why Use tanh as the Actuator? To implement the actuator, we use a hyperbolic tangent (tanh) function to smoothly transform the control signal into practical spatial adjustments. The choice of tanh offers several advantages.</p><p>• Saturation Effect: The output of tanh(x) is bounded between -1 and 1, ensuring that extreme PID outputs do not result in excessively large movements, which could cause objects to shift too abruptly.</p><p>• Smooth Transitions: Unlike a linear function, tanh produces gradual transitions, which is essential for finetuning positions without introducing jerky or unnatural motion.</p><p>• Damping Small Adjustments: For small error values, tanh behaves approximately linearly, allowing precise</p><note type="other">Ours Ours Comp3D LI3D</note><p>A white king and a black queen chess piece on a chess board.</p><p>A simple garden with a tree, a bench, and a flower bed. micro-adjustments, while for large errors, it naturally limits the adjustment size, preventing excessive corrections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Compared with scene generation methods</head><p>We further evaluate our approach against recent methods in compositional scene generation, including Graph-Dreamer <ref type="bibr" target="#b12">(Gao et al. 2024</ref>), GALA3D(Doe and Smith 2023), LI3D <ref type="bibr">(Lin et al. 2023c</ref>), and CompoNeRF <ref type="bibr">(Lin et al. 2023b</ref>). Detailed comparisons of GraphDreamer and GALA3D are presented in the main text. GALA3D utilizes a LLM to generate layouts. However, its direct layout inference lacks reasoning about object placement logic, often leading to physical constraint violations, such as objects floating in mid-air. GraphDreamer employs a graph-based structure to model inter-object relationships. Despite this, experimental results demonstrate that its performance degrades significantly when generating complex scenes or environments with a large number of objects. Since LI3D and CompoN-eRF are not open-sourced, our comparison relies on the examples provided in their respective papers. As illustrated in Fig. <ref type="figure" target="#fig_9">8</ref> , our method outperforms these approaches in terms of visual clarity and offers more precise scene control, enabling intuitive and interactive editing tailored to user specifications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Failure case</head><p>While our LLM-based spatial evaluation is effective, distilled models with smaller architectures often struggle to establish certain relational edges and accurately adjust object positions due to reduced precision. This results in increased randomness in spatial adjustments and reduced accuracy in scene refinements. To mitigate this, we propose fine-tuning on scene layout datasets using contrastive learning and multi-view consistency constraints to enhance relational reasoning and positional accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Causal Order Prompt</head><p>You are an expert in computer graphics, computer vision, causal analysis, and scene design.</p><p>You will be provided with a scene layout graph containing objects (nodes) and their spatial relationships (edges). Your task is to analyze and refine this graph using physical constraints and causal reasoning. Follow these guidelines precisely:   The image shows a scene of {edge.create prompt()}.</p><p>Evaluation Task:</p><p>-The position of object {obj names[1]} is correct.</p><p>-Object {obj names[0]} may be misplaced in the scene. Evaluate its spatial deviation along three axes.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: CausalStruct optimizes and controls 3D scene generation using either pure text or a combination of text and image inputs, ensuring spatial coherence and realistic object interactions through causal reasoning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: Overview of our method. Given a scene description, our method constructs a causal scene graph using LLMs and MLLMs with causal reasoning. A PID controller refines object scales and positions, ensuring spatial consistency. Additionally, objects and the scene are represented with 3D Gaussian Splatting and optimized using Diffusion and SDS for high-fidelity rendering.</figDesc><graphic coords="3,406.25,144.63,136.34,136.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Qualitative Reconstruction Results. Compared to other methods, our approach produces high-quality reconstructions.</figDesc><graphic coords="5,239.05,177.81,186.34,97.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>to move the plants to the bookcase.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Scene Editing. Our method can add, remove, or move objects based on the causal relationship between their placement.</figDesc><graphic coords="5,117.10,377.66,114.04,114.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>al. 2024) to generate the initial scene graph. To optimize the causal scene graph, we integrate DeepSeek (Guo et al. 2025) and GPT-4o, where DeepSeek facilitates strong chain-ofthought reasoning to refine causal relationships, while GPT-4o incorporates multimodal analysis to maintain consistency between textual descriptions and the visual layout. During causal graph optimization, we employ Point-E (Nichol et al. 2022) to render edge images and generate the spatial layout. During the object scale and position adjustment stage, we set k p = 1, k i = 0.00001, k d = 5, ∆ = 0.02, and γ = 500 for scale control, while for position control, we set k p = 1, k i = 0.00001, k d = 5, ∆ = 0.4, and γ = 800.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Visual results of key Components. The experiments validate the necessity of each component in our framework, highlighting their critical roles in ensuring coherent spatial relationships and physical plausibility</figDesc><graphic coords="7,43.29,290.95,110.23,110.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Visual results of actuator.</figDesc><graphic coords="11,311.50,69.42,116.49,116.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Visual results of compositional scene generation methods.</figDesc><graphic coords="12,212.30,122.77,73.50,73.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>Scoring Criteria: Assign a score from -100 to 100 along each axis: 1. Left-Right (X-Axis): -Positive score: Too close to {obj names[1]}. -Negative score: Too far from {obj names[1]}. 2. Forward-Backward (Y-Axis): -Positive score: Too close to {obj names[1]}. -Negative score: Too far from {obj names[1]}. 3. Up-Down (Z-Axis): -Positive score: Too high above {obj names[1]}. -Negative score: Too low below {obj names[1]}. Output Format: &lt;Answer&gt;The score-1 is: XX. The score-2 is: YY. The score-3 is: ZZ&lt;/Answer&gt;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>bed, two wooden nightstands, a square wooden table with a table lamp on it, a wooden wardrobe, and a TV stand with a flat- screen TV</head><label></label><figDesc></figDesc><table><row><cell>A bedroom with a</cell><cell>Causal Scene Graph</cell></row><row><cell></cell><cell>Layout</cell></row><row><cell>Order</cell><cell>Intervention</cell></row><row><cell></cell><cell>LLMs have significantly ad-</cell></row><row><cell></cell><cell>vanced causal inference by combining text-based depen-</cell></row><row><cell></cell><cell>dency extraction with reasoning capabilities to uncover</cell></row><row><cell></cell><cell>causal relationships. LLMs enhance causal inference by</cell></row><row><cell></cell><cell>performing pairwise causal reasoning to identify relation-</cell></row><row><cell></cell><cell>causal</cell></row><row><cell></cell><cell>discovery into this process ensures coherent spatial layouts</cell></row><row><cell></cell><cell>and semantically consistent object interactions.</cell></row><row><cell></cell><cell>Layout Generation Scene layout is fundamental to 3D</cell></row><row><cell></cell><cell>scene generation, as it dictates the spatial arrangement,</cell></row><row><cell></cell><cell>scale, and interactions of objects, directly influencing re-</cell></row><row><cell></cell><cell>alism and coherence. Various methods have explored dif-</cell></row><row><cell></cell><cell>ferent strategies for scene composition and object place-</cell></row><row><cell></cell><cell>ment. SceneSuggest (Savva, Chang, and Agrawala 2017)</cell></row><row><cell></cell><cell>utilizes spatial constraints to infer supporting surfaces, while</cell></row><row><cell></cell><cell>Physcene (Yang et al. 2024) and Text2nerf (Zhang et al.</cell></row><row><cell></cell><cell>2024b) integrate diffusion models to enforce physically</cell></row><row><cell></cell><cell>plausible layouts. Graph-based approaches have gained pop-</cell></row><row><cell></cell><cell>ularity in structuring object relationships and layout within</cell></row><row><cell></cell><cell>scenes. PlanIT (Wang et al. 2019) and SceneGraphNet</cell></row><row><cell></cell><cell>(Zhou, While, and Kalogerakis 2019) encode spatial and</cell></row><row><cell></cell><cell>functional dependencies, guiding object placement based on</cell></row><row><cell></cell><cell>predefined constraints. GraphDreamer (Gao et al. 2024) and</cell></row><row><cell></cell><cell>SceneWiz3D (Zhang et al. 2024c) incorporates LLMs with</cell></row><row><cell></cell><cell>layout-based NeRF to further enhance scene composition,</cell></row><row><cell></cell><cell>while GALA3D (Doe and Smith 2023) and LayoutDreamer</cell></row><row><cell></cell><cell>(Zhou et al. 2025) introduces layout-guided 3D Gaussian</cell></row><row><cell></cell><cell>representation, leveraging adaptive constraints for geome-</cell></row><row><cell></cell><cell>try refinement and inter-object interactions. Despite their ef-</cell></row><row><cell></cell><cell>fectiveness in structuring layouts, these methods overlook</cell></row><row><cell></cell><cell>causal dependencies in object placement and attributes, of-</cell></row><row><cell></cell><cell>ten leading to misalignment or floating objects. In this paper,</cell></row><row><cell></cell><cell>we integrate causal reasoning to refine both object relation-</cell></row><row><cell></cell><cell>ships and attributes, ensuring a more coherent and physically</cell></row><row><cell></cell><cell>plausible scene.</cell></row></table><note><p><p><p><p><p><p><p><p><p>ships between variables</p><ref type="bibr" target="#b18">(Kıcıman et al. 2023)</ref></p>. Beyond pairwise inference, LLMs contribute to causal graph construction, leveraging causal ordering to orient undirected edges</p><ref type="bibr" target="#b39">(Vashishtha et al. 2023)</ref> </p>and refining predictions through iterative feedback mechanisms</p><ref type="bibr" target="#b2">(Ban et al. 2023</ref></p>). Additionally, LLMs enhance generalization by incorporating pretrained knowledge</p>(Feng et al. 2024a)</p>, making them wellsuited for capturing complex dependencies in structured reasoning. Since scene construction inherently involves causal relationships between objects, integrating LLMs with</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 :</head><label>1</label><figDesc>Comparison with additional metrics. CLIP<ref type="bibr" target="#b31">(Radford et al. 2021</ref>) &amp; MLLMs<ref type="bibr" target="#b38">(Team et al. 2023;</ref><ref type="bibr" target="#b15">Hurst et al. 2024;</ref><ref type="bibr" target="#b1">Bai et al. 2023;</ref><ref type="bibr" target="#b8">Du et al. 2021;</ref><ref type="bibr" target="#b0">Anthropic 2024</ref>).</figDesc><table><row><cell>Methods</cell><cell cols="10">Represent. ViT-B/32 ViT-L/14 ViT-bigL/14 Gemini GPT-4o Claude Qwen GLM</cell></row><row><cell>MVDream (Lee and Kim 2023)</cell><cell></cell><cell>Nerf</cell><cell>24.30</cell><cell>18.34</cell><cell>18.16</cell><cell>1.7</cell><cell>4.1</cell><cell>2.7</cell><cell>1.6</cell><cell>3.6</cell></row><row><cell>GraphDreamer (Gao et al. 2024)</cell><cell></cell><cell>Nerf</cell><cell>21.30</cell><cell>19.96</cell><cell>20.57</cell><cell>1.5</cell><cell>2.8</cell><cell>3.0</cell><cell>6.6</cell><cell>2.6</cell></row><row><cell cols="2">DreamGaussian (Tang et al. 2024)</cell><cell>3DGS</cell><cell>15.78</cell><cell>10.22</cell><cell>10.41</cell><cell>0.6</cell><cell>1.2</cell><cell>1.8</cell><cell>3.0</cell><cell>0.3</cell></row><row><cell>GaussianDreamer (Yi et al. 2023)</cell><cell></cell><cell>3DGS</cell><cell>20.57</cell><cell>18.04</cell><cell>18.72</cell><cell>0.8</cell><cell>1.8</cell><cell>1.7</cell><cell>5.2</cell><cell>1.0</cell></row><row><cell>GSGen (Chen et al. 2024)</cell><cell></cell><cell>3DGS</cell><cell>17.32</cell><cell>12.11</cell><cell>14.22</cell><cell>1.0</cell><cell>2.5</cell><cell>3.2</cell><cell>2.6</cell><cell>1.0</cell></row><row><cell>GALA3D (Doe and Smith 2023)</cell><cell></cell><cell>3DGS</cell><cell>22.29</cell><cell>16.68</cell><cell>17.60</cell><cell>1.6</cell><cell>3.5</cell><cell>2.6</cell><cell>4.3</cell><cell>0.6</cell></row><row><cell>Ours</cell><cell></cell><cell>3DGS</cell><cell>25.90</cell><cell>20.86</cell><cell>21.31</cell><cell>2.8</cell><cell>5.0</cell><cell>3.3</cell><cell>6.9</cell><cell>4.2</cell></row><row><cell>Methods</cell><cell cols="2">LLM</cell><cell>MLLM</cell><cell cols="6">ViT-B/32 ViT-L/14 ViT-bigL/14 Gemini GPT-4o Claude</cell></row><row><cell>w/o Causal Reasoning</cell><cell>-</cell><cell></cell><cell>-</cell><cell>25.55</cell><cell>22.27</cell><cell>23.81</cell><cell>1.78</cell><cell>4.06</cell><cell>3.19</cell></row><row><cell>DeepSeek(distilled)</cell><cell cols="3">R1-8b LLava-34b</cell><cell>27.48</cell><cell>23.05</cell><cell>24.88</cell><cell>2.39</cell><cell>4.83</cell><cell>3.75</cell></row><row><cell>DeepSeek(distilled)</cell><cell cols="3">R1-14b LLava-34b</cell><cell>28.17</cell><cell>23.46</cell><cell>25.80</cell><cell>2.44</cell><cell>5.13</cell><cell>3.64</cell></row><row><cell>DeepSeek</cell><cell>R1</cell><cell></cell><cell>4o</cell><cell>28.05</cell><cell>23.81</cell><cell>25.95</cell><cell>2.69</cell><cell>5.47</cell><cell>3.89</cell></row><row><cell>GPT</cell><cell>4o</cell><cell></cell><cell>4o</cell><cell>27.81</cell><cell>24.63</cell><cell>26.95</cell><cell>2.42</cell><cell>5.64</cell><cell>3.64</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Ablation Studies on Knowledge Distillation. Our experiments systematically evaluate the impact of knowledge distillation, while probing how original non-distilled models shape final performance outcomes.</figDesc><table><row><cell>During Gaussian optimization and generation, we employ</cell></row><row><cell>MVDream or Zero123 to refine individual objects and Sta-</cell></row><row><cell>ble Diffusion to optimize the overall scene, ensuring both</cell></row><row><cell>object-level quality and scene-level coherence. All exper-</cell></row><row><cell>iments were performed on an NVIDIA A100 GPU with</cell></row><row><cell>80GB memory.</cell></row></table><note><p><p><p><p><p>Evaluation Metrics We evaluate our method using CLIP</p><ref type="bibr" target="#b31">(Radford et al. 2021</ref></p>) Score and MLLMs</p><ref type="bibr" target="#b38">(Team et al. 2023;</ref><ref type="bibr" target="#b15">Hurst et al. 2024;</ref><ref type="bibr" target="#b1">Bai et al. 2023;</ref><ref type="bibr" target="#b0">Anthropic 2024;</ref><ref type="bibr" target="#b8">Du et al. 2021</ref></p>) Score, comparing them quantitatively with baseline models. CLIP Score computes similarity by comparing the visual and textual embeddings extracted from the same CLIP model. Additionally, we incorporate MLLMs Score, to further assess scene-object alignment and semantic coherence in the generated 3D representations.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>nodes Cw climbing wall Sh shovel Sa sandpit S1 swing1 Pt picnic table bu bucket Sl slide Tr tree S2 swing2 Be bench</head><label></label><figDesc>Ablation Study of causal reasoning and adaptability. The results show that causal reasoning enhances scene coherence, and experiments with distilled models demonstrate the robustness of our method.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Sl</cell><cell>Cw</cell><cell></cell><cell>bu</cell><cell>Sl</cell><cell>Cw</cell><cell></cell><cell>bu</cell><cell>Sl</cell><cell>Cw</cell><cell></cell><cell>bu</cell><cell>Sl</cell><cell>Cw</cell><cell>bu</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>S1</cell><cell cols="2">Tr</cell><cell>S2</cell><cell>S1</cell><cell>Tr</cell><cell>S2</cell><cell></cell><cell>S1</cell><cell>Tr</cell><cell>S2</cell><cell></cell><cell>S1</cell><cell>Tr</cell><cell>S2</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>Pt</cell><cell>Be</cell><cell>Sh</cell><cell>Sa</cell><cell>Pt</cell><cell>Be</cell><cell>Sh</cell><cell>Sa</cell><cell>Pt</cell><cell>Be</cell><cell>Sh</cell><cell>Sa</cell><cell>Pt</cell><cell>Be</cell><cell>Sh</cell><cell>Sa</cell></row><row><cell cols="2">Figure 5: monkey plants</cell><cell>stool table</cell><cell>salad juice</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>doll</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>prompt</cell><cell cols="3">Causal Scene Graph</cell><cell cols="3">Full Setting</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>w/o Causal Order</cell><cell cols="3">w/o Causal Intervention</cell><cell cols="3">w/o PID Control</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p><p><p>A monkey doll stood on a stool. There is juice and salad on the next table</p>.</p>The monkey looks at the plants.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>1. Allowed Spatial Relations:-All nodes need to have an edge.-Use only the following words to describe connections: {above, under, in, on, front, left, right, corner, behind, left front, right front, left back, right back, left on, right on}.-Only one word must be selected per edge.2. Causal Reasoning &amp; Edge Completion:-If two objects are closely related in real-world use but are not connected in the input graph, infer the missing edge and add it (e.g., add ["lamp", "on", "table"] if missing, but do not add "floor").-Ensure causal flow integrity: All edges must form a directed acyclic graph following causal order.3. Causal Order Principles:-Objects follow a causal flow: obj 2 comes after obj 1 if obj 1 's placement depends on obj 2 (e.g., ["cup", "on", "table"]).-Causal Rule: If obj 1 depends on obj 2 , reverse edge and adjust relation. Example: ["table", "under", "cup"] → ["cup", "on", "table"].-Size Rule: Larger objects should be obj 2 . Transform ["laptop", "left on", "mouse"] → ["mouse", "right on", "laptop"].-Causal order takes priority over size when they conflict (e.g., ["TV", "on", "stand"], even if TV is larger).</figDesc><table><row><cell>Output Format: The output must contain the following content:</cell></row><row><cell>&lt;Answer&gt;edges = [[obj 1, word 1, obj 2], [obj 2, word 4, obj 3], ...]&lt;/Answer&gt;.</cell></row><row><cell>Example Corrections:</cell></row></table><note><p><p><p><p><p><p><p>Input: [['laptop','left','mouse']</p>,</p>['cup','under','table']</p>] Output: edges = [</p>['mouse','right','laptop']</p>,</p>['cup','on','table']]    </p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>Prompt used for causal order inference in our framework. The LLM is guided to ensure spatial constraints, causal consistency, and object dependencies. Task:-Assess whether the given spatial relationship complies with physical laws and real-world scene consistency.-Based on the provided image and object interactions, determine the validity of the relation using the following criteria: -Gravity &amp; Support: Objects must adhere to realistic physical constraints (e.g., smaller objects should rest on larger ones, and unsupported objects should not float).-Spatial Positioning: The labeled relationship should match common spatial arrangements (e.g., a chair should be under a table, not above it).-Functional Affordance: Objects should maintain plausible real-world functionality (e.g., a monitor should be on a desk, not inside it).Decision Guidelines:-If the relationship is valid, return 'keep'.-If the relationship is incorrect but fixable, return 'modify' and suggest a new relation from the predefined set: {candidate relations}.Output Format: Provide the response strictly in JSON format as follows: { "action": "keep" | "modify", "updated relation": "new relation" }</figDesc><table><row><cell>Causal Intervention Prompt</cell></row><row><cell>I will provide an image of a scene.</cell></row></table><note><p>The image depicts {candidate edge.create prompt()} as part of the scene described as: {prompt}. In this scene, object '{obj names[0]}' is currently labeled as '{candidate edge.edge name}' relative to '{obj names[1]}'.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Prompt used for causal intervention in our framework. The LLM evaluates scene images across multiple perspectives to assess the correctness of object relationships, ensuring alignment with real-world physics, and spatial reasoning. Scale Evaluation Prompt I will provide an image of a scene. Object Dimensions: -The dimensions (length, width, height) of {obj names[0]} in the real world: -Length: {length0} cm, Width: {width0} cm, Height: {height0} cm. -The dimensions (length, width, height) of {obj names[1]} in the real world: -Length: {length1} cm, Width: {width1} cm, Height: {height1} cm. Task: -Evaluate the relative scale of the object {obj names[0]} compared to {obj names[1]}. -The scale of {obj names[1]} is assumed to be correct, but {obj names[0]} may have scaling inconsistencies in the scene with {edge.create prompt()}. Evaluation Criteria: Scale Comparison: Does {obj names[0]} appear appropriately scaled relative to {obj names[1]}? -Consider the effect on scene composition, ensuring it is neither too large nor too small. Scoring System: -A score from -100 to 100 is assigned based on scale consistency: -Score close to 0: The scale of {obj names[0]} is appropriate. -Positive score: {obj names[0]} is too large compared to {obj names[1]}, disrupting scene balance.-Negative score: {obj names[0]} is too small, making it insignificant in the scene. Output Format: Provide only the result in the following format, with no additional text: &lt;Answer&gt;The score is: X&lt;/Answer&gt;, where X is the evaluated score. For example, output: &lt;Answer&gt;The score is: 25&lt;/Answer&gt;.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Prompt used for scale evaluation and PID-based optimization in our framework. The LLM assesses object-relative scaling.</figDesc><table><row><cell>Spatial Position Evaluation Prompt</cell></row><row><cell>I will send you a sentence and images of a scene.</cell></row><row><cell>Scene Description:</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>Prompt used for spatial position evaluation. The LLM provides axis-aligned position corrections based on multi-view scene analysis.</figDesc><table /></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><surname>Anthropic</surname></persName>
		</author>
		<title level="m">The Claude 3 Model Family: Opus, Sonnet</title>
		<meeting><address><addrLine>Haiku</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Dang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2309.16609</idno>
		<title level="m">Qwen technical report</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">From query tools to causal architects: Harnessing large language models for advanced causal discovery from data</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.16902</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">2025a. Bayesian Optimization for Controlled Image Editing via LLMs</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-N</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">2025b. The Role of Deductive and Inductive Reasoning in Large Language Models</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-N</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL)</title>
		<meeting>the Annual Meeting of the Association for Computational Linguistics (ACL)</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Dense Point Clouds Matter: Dust-GS for Scene Reconstruction from Sparse Viewpoints</title>
		<author>
			<persName><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2025 -2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2024">2025. 2024</date>
			<biblScope unit="page" from="21401" to="21412" />
		</imprint>
	</monogr>
	<note>Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Crowe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ferdous</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Greenwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Grimble</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Katebi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kwong</surname></persName>
		</author>
		<title level="m">PID control: new identification and design methods</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">GALA3D: Generative Adversarial Layout Arrangement in 3D Spaces</title>
		<author>
			<persName><forename type="first">J</forename><surname>Doe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="12345" to="12353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Z</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.10360</idno>
		<title level="m">General language model pretraining with autoregressive blank infilling</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">2024a. From pre-training corpora to large language models: What factors influence llm performance in causal discovery tasks</title>
		<author>
			<persName><forename type="first">T</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Tandon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Haffari</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2407.19638</idno>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Layoutgpt: Compositional visual planning and generation with large language models</title>
		<author>
			<persName><forename type="first">W</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-J</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Akula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">E</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="page">36</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Scenescape: Text-driven consistent scene generation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Fridman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Abecasis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Kasten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Dekel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page">36</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Graphdreamer: Compositional 3d scene synthesis from scene graphs</title>
		<author>
			<persName><forename type="first">G</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="21295" to="21304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">D</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Bi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2501.12948</idno>
		<title level="m">Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning</title>
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">dtopia: Large text-to-3d generation model with hybrid diffusion priors</title>
		<author>
			<persName><forename type="first">F</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.02234</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Hurst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Goucher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Perelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ostrow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Welihinda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.21276</idno>
		<title level="m">Gpt-4o system card</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.11273</idno>
		<title level="m">Generic 3d gaussian generative framework for fast text-to-3d synthesis</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Kerbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Wraber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Egger</surname></persName>
		</author>
		<author>
			<persName><surname>Lugmayr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.08354</idno>
		<title level="m">A. 2023. 3D Gaussian Splatting for Efficient Scene Representation</title>
		<imprint/>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Kıcıman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.00050</idno>
		<title level="m">Causal reasoning and large language models: Opening a new frontier for causality</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">MVDream: Multi-View Consistent 3D Object Generation from Single-View Images</title>
		<author>
			<persName><forename type="first">A</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Computer Vision (ICCV)</title>
		<meeting>the International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="5678" to="5685" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Cpseg: Finer-grained image semantic segmentation via chain-of-thought language prompting</title>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="513" to="522" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Image content generation with causal reasoning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="13646" to="13654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Magic3d: High-resolution text-to-3d content creation</title>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Takikawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kreis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="300" to="309" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.09134</idno>
		<title level="m">Customizable Layouts for Compositional 3D Generation</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2305.15808</idno>
		<title level="m">Towards language-guided interactive 3d generation: Llms as layout interpreter with generative feedback</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Zero-1-to-3: Zero-shot one image to 3d object</title>
		<author>
			<persName><forename type="first">R</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Hoorick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tokmakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zakharov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF international conference on computer vision</title>
		<meeting>the IEEE/CVF international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="9298" to="9309" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Latent-nerf for shape-guided generation of 3d shapes and textures</title>
		<author>
			<persName><forename type="first">G</forename><surname>Metzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Patashnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Giryes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="12663" to="12673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Causality: models, reasoning, and inference, by judea pearl</title>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">G</forename><surname>Neuberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econometric Theory</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="675" to="685" />
			<date type="published" when="2000">2003. 2000</date>
			<publisher>cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2212.08751</idno>
		<title level="m">Point-e: A system for generating 3d point clouds from complex prompts</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Antiga</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">32</biblScope>
		</imprint>
	</monogr>
	<note>Advances in neural information processing systems</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mildenhall</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2209.14988</idno>
		<title level="m">DreamFusion: Text-to-3D using 2D Diffusion</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning transferable visual models from natural language supervision</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Hallacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Goh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Clark</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8748" to="8763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis with latent diffusion models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</title>
		<meeting>the IEEE/CVF conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="10684" to="10695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Photorealistic text-toimage diffusion models with deep language understanding</title>
		<author>
			<persName><forename type="first">C</forename><surname>Saharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Whang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Ghasemipour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gontijo Lopes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Karagol Ayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="36479" to="36494" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Agrawala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.00061</idno>
		<title level="m">Scenesuggest: Context-driven 3d scene design</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-S</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.07937</idno>
		<title level="m">Let 2d diffusion model know 3d-consistency for robust text-to-3d generation</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.12945</idno>
		<title level="m">Procedural 3d modeling with large language models</title>
		<meeting>edural 3d modeling with large language models</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">DreamGaussian: Generative Gaussian Splatting for Efficient 3D Content Creation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Twelfth International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Team</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Anil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Borgeaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-B</forename><surname>Alayrac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Soricut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schalkwyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hauth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Millican</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.11805</idno>
		<title level="m">Gemini: a family of highly capable multimodal models</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Causal Inference using LLM-Guided Discovery</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vashishtha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bachu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">N</forename><surname>Balasubramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI 2024 Workshop on &quot;Are Large Language Models Simply Causal Parrots?</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Score jacobian chaining: Lifting pretrained 2d diffusion models for 3d generation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="12619" to="12629" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-A</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Weissmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ritchie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">36</biblScope>
			<date type="published" when="2019">2019. 2024</date>
		</imprint>
	</monogr>
	<note>ACM Transactions on Graphics (TOG)</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Proportional-integral-derivative control</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Willis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">6</biblScope>
		</imprint>
		<respStmt>
			<orgName>Dept. of Chemical and Process Engineering University of Newcastle</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pan</surname></persName>
		</author>
		<idno>arXiv- 2410</idno>
		<title level="m">Improving causal reasoning in large language models: A survey</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Text to scene: a system of configurable 3D indoor scene synthesis</title>
		<author>
			<persName><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Multimedia</title>
		<meeting>the 29th ACM International Conference on Multimedia</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2819" to="2821" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Physcene: Physically interactable 3d scene synthesis for embodied ai</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="16262" to="16272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2310.08529</idno>
		<title level="m">Gaussiandreamer: Fast generation from text to 3d gaussian splatting with point cloud priors</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2403.19655</idno>
		<title level="m">GaussianCube: Structuring Gaussian Splatting using Optimal Transport for 3D Generative Modeling</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Text2nerf: Text-driven 3d scene generation with neural radiance fields</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Visualization and Computer Graphics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="7749" to="7762" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Towards Text-guided 3D Scene Composition</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Siarohin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tulyakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-Y</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="6829" to="6838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">E</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.12836</idno>
		<title level="m">EditRoom: LLM-parameterized Graph Diffusion for Composable 3D Room Layout Editing</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.15391</idno>
		<title level="m">Layoutyour-3D: Controllable and Precise 3D Generation with 2D Blueprint</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Diffgs: Functional gaussian splatting diffusion</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-S</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2025">2025</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="37535" to="37560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2502.01949</idno>
		<title level="m">LAY-OUTDREAMER: Physics-guided Layout for Text-to-3D Compositional Scene Generation</title>
		<imprint>
			<date type="published" when="2025">2025</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Scenegraphnet: Neural message passing for 3d indoor scene augmentation</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>While</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7384" to="7392" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
