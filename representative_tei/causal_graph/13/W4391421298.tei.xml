<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CORE: Towards Scalable and Efficient Causal Discovery with Reinforcement Learning</title>
				<funder>
					<orgName type="full">Hybrid Intelligence Center</orgName>
				</funder>
				<funder ref="#_sUx3beM">
					<orgName type="full">Dutch Ministry of Education, Culture and Science through the Netherlands Organisation for Scientific Research</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-01-30">30 Jan 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Andreas</forename><surname>Sauter</surname></persName>
							<email>a.sauter@vu.nl</email>
							<affiliation key="aff0">
								<orgName type="institution">Vrije Universiteit Amsterdam Amsterdam</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">NicolÃ²</forename><surname>Botteghi</surname></persName>
							<email>n.botteghi@utwente.nl</email>
							<affiliation key="aff1">
								<orgName type="department">IvI and ILLC</orgName>
								<orgName type="institution">University of Twente Enschede</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Erman</forename><surname>Acar</surname></persName>
							<email>e.acar@uva.nl</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Amsterdam Amsterdam</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Aske</forename><surname>Plaat</surname></persName>
							<email>a.plaat@liacs.leidenuniv.nl</email>
							<affiliation key="aff3">
								<orgName type="institution" key="instit1">LIACS</orgName>
								<orgName type="institution" key="instit2">Leiden University Leiden</orgName>
								<address>
									<country key="NL">The Netherlands</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">CORE: Towards Scalable and Efficient Causal Discovery with Reinforcement Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-01-30">30 Jan 2024</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2401.16974v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-21T20:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Causal Discovery, Reinforcement Learning</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Causal discovery is the challenging task of inferring causal structure from data. Motivated by Pearl's Causal Hierarchy (PCH), which tells us that passive observations alone are not enough to distinguish correlation from causation, there has been a recent push to incorporate interventions into machine learning research. Reinforcement learning provides a convenient framework for such an active approach to learning. This paper presents CORE, a deep reinforcement learning-based approach for causal discovery and intervention planning. CORE learns to sequentially reconstruct causal graphs from data while learning to perform informative interventions. Our results demonstrate that CORE generalizes to unseen graphs and efficiently uncovers causal structures. Furthermore, CORE scales to larger graphs with up to 10 variables and outperforms existing approaches in structure estimation accuracy and sample efficiency. All relevant code and supplementary material can be found at <ref type="url" target="https://github.com/sa-and/CORE">https://github.com/sa-and/CORE</ref>.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION AND RELATED WORK</head><p>Causal discovery (CD) is the challenging task of inferring causal structure from data <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b34">35]</ref>. Traditional approaches to causal discovery consider data from purely observational distributions. These are approaches such as constraint-based ones <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b30">31]</ref>, score-based ones <ref type="bibr" target="#b5">[6]</ref>, and more recently continuous optimization-based ones <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39]</ref>.</p><p>Pearl's Causal Hierarchy (PCH) asserts that distinguishing between mere correlations and genuine causal relationships requires the integration of interventions in general <ref type="bibr" target="#b2">[3]</ref>. As a response to this requirement, there has been a recent push to incorporate interventions into causal discovery research <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b22">23]</ref> including machine learning <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b27">28]</ref>, among others.</p><p>Reinforcement learning (RL) learns an optimal policy for sequential decision problems through interactions <ref type="bibr" target="#b31">[32]</ref>. Therefore, RL is a promising framework for using interventions to investigate causal relationships. In particular, RL plays a dual role in the realm of causal discovery -it can be used not only to recover the causal structure of an environment <ref type="bibr" target="#b39">[40]</ref>, but also to learn causal discovery algorithms <ref type="bibr" target="#b27">[28]</ref>, thus representing a versatile tool for CD.</p><p>In particular, RL has also been used to search the space of causal structures more efficiently based on a fixed dataset <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b39">40]</ref> with the possibility of incorporating prior knowledge <ref type="bibr" target="#b10">[11]</ref>. Similarly, work on RL-related GFlow Nets <ref type="bibr" target="#b3">[4]</ref> has been deployed to generate good estimates of the true causal structure <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b16">17]</ref>. Furthermore, many integrations of RL with causal concepts have been investigated that restrict their CD process to supervised learning <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b33">34]</ref>. In addition to that, RL has also been used to learn policies that choose the best interventions to do for CD <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b32">33]</ref>.</p><p>Although causal discovery has seen substantial progress with these works over the years, leading to a multitude of methodologies, challenges persist in areas such as scalability, generalization, and planning of interventions. In this context, this paper introduces CORE (Causal DiscOvery with REinforcement Learning), a deep-RL-based algorithm designed for the task of learning a CD policy. CORE can learn a policy that sequentially reconstructs causal graphs from both observational and interventional data, while simultaneously performing informative interventions. This dual learning paradigm allows CORE not only to uncover causal structures efficiently, but also to identify interventions that enhance its causal models. The following lists our main contributions:</p><p>â€¢ We formalize the task of learning a CD algorithm as a partially observable Markov decision process (POMDP). â€¢ We propose a dual Q-learning setup to learn intervention design and structure estimation simultaneously. â€¢ We demonstrate that CORE can be successfully applied for causal discovery to previously unseen graphs of sizes of up to 10 variables. In addition to those, we show the importance of jointly learning which interventions to perform and graph generation, and investigate the limitations of our approach regarding the applicability to the real world.</p><p>The most distinctive feature of CORE is that it does not impose a specific algorithm for identifying causal models, but rather attempts to learn it. Among others, this can have positive effects on efficiency and transferability to new problem instances. While MCD <ref type="bibr" target="#b27">[28]</ref> and AVICI <ref type="bibr" target="#b18">[19]</ref> solve the same task, they run into pitfalls that hinder their application to realistic graph sizes or rely on offline data, respectively. We set steps to overcome these pitfalls by imposing additional structure on our policy, more efficient rewards, and learning to actively perform relevant interventions.</p><p>Our results show robust generalization to unseen graphs and the capability to scale to scenarios with up to ten variables, a step forward over the state of the art, and a crucial advancement towards addressing real-world complexities. The subsequent sections delve into the intricacies of CORE's architecture, its training methodologies, and empirical validations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRELIMINARIES AND NOTATION</head><p>In this section, we establish the necessary notation and provide an overview of key concepts and techniques used in the field of causal discovery with interventions and reinforcement learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Causal Models</head><p>Causal relations are often formalized through a structural causal model (SCM) which is a tuple ğ‘€ = (ğ’³ , â„±, ğ’°, ğ’«) with a set of endogenous (random) variables (i.e., relevant variables for the problem) ğ’³ = {ğ‘‹ 1 , . . . , ğ‘‹ğ‘›}, ğ’° = {ğ‘ˆ 1 , . . . , ğ‘ˆğ‘›} a set of exogenous (random) variables (often also called unobservable or noise variables), â„± = {ğ‘“ 1 , . . . , ğ‘“ğ‘›} the set of functions (also called structural equations) whose elements are in the form of ğ‘‹ ğ‘– â† ğ‘“ ğ‘– (ğ‘ƒğ‘(ğ‘‹ ğ‘– ), ğ‘ˆ ğ‘– ) where ğ‘ƒğ‘(ğ‘‹ ğ‘– ) âŠ† ğ’³ {ğ‘‹ ğ‘– } stands for endogenous parent variables of ğ‘‹ ğ‘– , and ğ’« = {ğ‘ƒ 1 , ..., ğ‘ƒğ‘›} the set of pairwise independent probability distributions defined over ğ’° with ğ‘ˆ ğ‘– âˆ¼ ğ‘ƒ ğ‘– .</p><p>Interpreting variables as nodes and the functional dependency between variables as directed edges, every SCM ğ‘€ induces a directed graph structure ğº, which we will call the corresponding causal graph. Directed edges represent direct causation from parent nodes to child nodes, hence absence of edges is as important as present edges. For the sake of simplicity, we shall follow the common assumption that no variable is its own cause i.e., there is no circular functional dependency, hence the induced causal graph is always a directed acyclic graph (DAG). Furthermore, each SCM ğ‘€ induces a joint distribution ğ‘ƒ ğ‘€ (ğ’³ ) over its endogenous variables, whose structural properties inherited from the corresponding induced graph ğº satisfy the Markov condition. That is, each ğ‘‹ ğ‘– is independent of its non-descendants, given its parents ğ‘ƒğ‘(ğ‘‹ ğ‘– ). Along with the independence of the noise variables, this condition implies the following factorization <ref type="bibr" target="#b25">[26]</ref>:</p><formula xml:id="formula_0">ğ‘ƒ ğ‘€ (ğ’³ ) = âˆ ğ‘‹ ğ‘– âˆˆğ’³ ğ‘ƒ(ğ‘‹ ğ‘– â‹ƒï¸€ ğ‘ƒğ‘(ğ‘‹ ğ‘– ))<label>(1)</label></formula><p>We shall refer to this distribution as observational distribution.</p><p>Note that SCMs are generative models, i.e., we can sample values for ğ’³ from them. We can sample the exogenous variables from ğ’« and determine the values of endogenous variables according to their functions in â„±. This procedure effectively corresponds to sampling from the joint distribution over endogenous variables <ref type="bibr" target="#b25">[26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Interventions</head><p>Interventions play a crucial role in causal discovery, allowing us to investigate causal relationships by actively manipulating variables in a system. In general, imposed by Pearl's causal hierarchy <ref type="bibr" target="#b2">[3]</ref>, interventions are necessary to distinguish causation from correlation, and eventually to reason about causal effects.</p><p>Formally, an intervention on a variable ğ‘‹ changes the variable's value to ğ‘¥ (an arbitrary but fixed value), independently of ğ‘‹ 's actual causes. Then ğ‘‹ is called the intervention target. Effectively, at the graph level, intervening on a variable ğ‘‹ , removes all the edges incoming to ğ‘‹ , resulting in ğ‘ƒğ‘(ğ‘‹ ) = âˆ…. This operation is the so-called do-operation (denoted as ğ‘‘ğ‘œ(ğ‘‹ = ğ‘¥)), and allows us to distinguish the causal effect of variable ğ‘‹ on variable(s) ğ‘Œ from the confounding influence of common parents of ğ‘‹ and ğ‘Œ (Figure <ref type="figure" target="#fig_0">1</ref>).</p><p>In an SCM, intervening on a variable ğ‘‹ implies that the corresponding structural equation ğ‘“ ğ‘‹ âˆˆ â„± is replaced by ğ‘‹ â† ğ‘¥, resulting in a modified SCM ğ‘€ â€² . Therefore, an intervention affects the distribution of the intervention target, since:</p><formula xml:id="formula_1">ğ‘ƒ ğ‘€ â€² (ğ‘‹ â‹ƒï¸€ ğ‘ƒğ‘(ğ‘‹ )) = ğ‘ƒ ğ‘€ â€² (ğ‘‹ â‹ƒï¸€âˆ…) = ğ‘ƒ ğ‘€ â€² (ğ‘‹ ) = ğ›¿ğ‘¥ (<label>2</label></formula><formula xml:id="formula_2">)</formula><p>where ğ›¿ğ‘¥ is the probability density function that has all mass on ğ‘¥. Put differently, an intervention replaces the factor associated with the intervened variable. We refer to the resulting joint distribution</p><formula xml:id="formula_3">ğ‘ƒ ğ‘€ (ğ’³ â‹ƒï¸€ ğ‘‘ğ‘œ(ğ‘‹ = ğ‘¥)) = âˆ ğ‘‹ ğ‘– âˆˆğ’³ âˆ–{ğ‘‹ } ğ‘ƒ ğ‘€ (ğ‘‹ ğ‘– â‹ƒï¸€ ğ‘ƒğ‘(ğ‘‹ ğ‘– )) â‹… ğ‘ƒ ğ‘€ â€² (ğ‘‹ = ğ‘¥) (3)</formula><p>as post-interventional distribution. To simplify the notation, we will sometimes use ğ‘ƒ ğ‘€ ğ‘‘ğ‘œ(ğ‘‹ =ğ‘¥ ) (ğ’³ ) or ğ‘ƒ ğ‘€ ğ‘‘ğ‘œ(ğ‘‹ ) (ğ’³ ) to refer to the expression ğ‘ƒ ğ‘€ (ğ’³ â‹ƒï¸€ ğ‘‘ğ‘œ(ğ‘‹ = ğ‘¥)) when the target variable or ğ‘¥ is clear from the context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Reinforcement Learning</head><p>Reinforcement learning (RL) is a general approach to learning through interaction with the world <ref type="bibr" target="#b31">[32]</ref>, especially in sequential decision problems. An RL agent aims to find the sequence of actions that maximize the expected return, i.e., the cumulative (discounted) reward. How the RL agent selects its actions only relies on (possibly indirect) measures of how the world changes after each action is taken.</p><p>POMDP. In reinforcement learning problems, the relationship between an agent and the environment in which the states are not fully observable is often modeled as a Partially Observable Markov Decision Process (POMDP). Formally, a POMDP is a tuple (ğ’®, ğ’œ, ğ’¯ , â„›, Î©, ğ’ª,ğ›¾) where ğ’® is a set of states, ğ’œ is a set of actions, ğ’¯ âˆ¶ ğ’® Ã— ğ’œ Ã— ğ’® â†’ (ï¸€0, 1âŒ‹ï¸€ is a set of transition probabilities between states, â„› âˆ¶ ğ’® Ã— ğ’œ â†’ R is the reward function, Î© is a set of observations, ğ’ª âˆ¶ ğ’® Ã— ğ’œ Ã— Î© â†’ (ï¸€0, 1âŒ‹ï¸€ is the set of conditional observation probabilities, and ğ›¾ âˆˆ (ï¸€0, 1) is the discount factor. RL:. The strategy of actions of an RL agent is called policy. A policy can be either deterministic or stochastic. A deterministic policy ğœ‹ âˆ¶ ğ’® â†’ ğ’œ maps states to an action, whereas a stochastic policy ğœ‹ âˆ¶ ğ’® Ã— ğ’œ â†’ (ï¸€0, 1âŒ‹ï¸€ is characterized by a conditional distribution of actions given states. To maximize the return in the long run, RL agents often estimate the so-called value function ğ‘‰ âˆ¶ ğ’® â†’ R or action value function ğ‘„ âˆ¶ ğ’® Ã— ğ’œ â†’ R. These functions determine the desirability of a state or a state-action pair, respectively. The optimal Q-function ğ‘„ * allows us to derive an optimal policy ğœ‹ * that maximizes the return by greedily choosing the action that maximizes the value of each state, that is, ğœ‹ * (ğ‘ ) = argmax ğ‘ ğ‘„ * (ğ‘ , ğ‘) <ref type="bibr" target="#b31">[32]</ref>.</p><p>Deep Q-Learning: The Q-learning algorithm <ref type="bibr" target="#b36">[37]</ref> estimates the state-action value function ğ‘„ using the temporal difference (TD). In particular, TD learning decomposes the problem of estimating the expected return of a given policy as the sum of the instantaneous reward and the value accumulated by following the optimal policy in the next step:</p><formula xml:id="formula_4">ğ‘„(ğ‘ , ğ‘) = ğ‘Ÿ (ğ‘ , ğ‘) + ğ›¾max ğ‘ â€² ğ‘„(ğ‘  â€² , ğ‘ â€² )<label>(4)</label></formula><p>where ğ‘Ÿ (ğ‘ , ğ‘) is the instantaneous reward of the state-action pair.</p><p>To estimate the future reward, we assume that the agent follows the optimal policy ğœ‹ * (ğ‘ ), i.e., argmax ğ‘ ğ‘„ * (ğ‘ , ğ‘). Especially in the first iterations of the algorithm, the estimate of ğ‘„ does not correspond to the optimal value function ğ‘„ * . However, tabular Qlearning can still converge to the optimal solution <ref type="bibr" target="#b36">[37]</ref>. The deep Q-network (DQN) algorithm <ref type="bibr" target="#b21">[22]</ref> adapts the Q-learning algorithm to non-tabular settings, e.g., continuous state spaces, where the Q-function needs to be approximated via a neural network. DQN utilizes the TD-learning rule to generate a target for the training of the neural network that approximates the Q-value by means of the loss function:</p><formula xml:id="formula_5">â„’(ğœƒ ) = E ğ‘ ,ğ‘,ğ‘Ÿ,ğ‘  â€² (ï¸€(ğ‘Ÿ (ğ‘ , ğ‘) + ğ›¾max ğ‘ â€² ğ‘„(ğ‘  â€² , ğ‘ â€² ; ğœƒ -) -ğ‘„(ğ‘ , ğ‘; ğœƒ )) 2 âŒ‹ï¸€ (5)</formula><p>where ğ‘„(ğ‘ , ğ‘; ğœƒ ) is the Q-function approximated by the neural network of parameters ğœƒ , while ğ‘„(ğ‘ , ğ‘; ğœƒ -) is the so-called target network, used to generate a fixed target and stabilize the training dynamics, and ğ›¾ is the discount factor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">LEARNING A CAUSAL DISCOVERY POLICY WITH INFORMATIVE INTERVENTIONS</head><p>In this section, we present our algorithmic setup for learning causal discovery policies. We consider the classic agent-environment interaction scheme commonly used in RL. The goal is to learn a policy that represents a CD algorithm that uses observational and interventional data to sequentially estimate the true causal structures by performing informative interventions. Such modules can be applied to previously unseen causal structures through a few forward passes of a neural network without retraining, making them a highly efficient tool for causality <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b27">28]</ref>. Following this line of research, we learn a policy that collects a stream of data by intervening on the environment to infer a causal structure estimate. This setup acknowledges the strong influence that informative interventions have, especially when there is a limited budget for interventions. We learn to perform these interventions by rewarding interventions that lead to the generation of a better structural update and limiting the budget for interventions by means of possible steps in an episode.</p><p>One key aspect of learned CD modules is that they need information about the ground truth causal structure only during training. This promises the possibility of (i) training the CD policy on synthetic data where the ground truth can easily be generated, and then (ii) applying it to estimate the causal structure of environments where the ground truth structure is potentially unknown, such as in the real world.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">POMDP Formulation of Causal Discovery Through Interventions</head><p>To conveniently model the causal discovery process where there is partial observability, we will use a POMDP. However, since such a formalization is not entirely obvious, it constitutes our first contribution, which we will present in this section.</p><p>State Space: Our environment is determined by SCMs. <ref type="foot" target="#foot_0">1</ref> Therefore, each state will correspond to an SCM. We shall describe each SCM as the set of functions that determine the endogenous variables. Therefore, having ğ‘› endogenous variables, a state is a set ğ‘  = {ğ‘“ 0 , ..., ğ‘“ ğ‘›-1 } of functions that define the current SCM. Furthermore, each state contains the ground truth observational graph ğº * ğ‘  induced by the observational SCM ğ‘€ ğ‘‘ğ‘œ(âˆ…) .</p><p>Action Space: We model our action space as a multi-discrete space ğ´ = (ï¸€ğ‘› + 1âŒ‹ï¸€ Ã— (ï¸€2ğ‘›(ğ‘› -1) + 1âŒ‹ï¸€ where the notation (ï¸€ğ‘›âŒ‹ï¸€ = {1, 2, . . . , ğ‘›} with ğ‘› being the number of nodes in the graph. The first dimension of the action space represents the endogenous variables of the current SCM and determines the intervention targets. For each variable ğ‘‹ ğ‘– âˆˆ ğ’³ , there is an action ğ‘‘ğ‘œ(ğ‘‹ ğ‘– = ğ‘), where ğ‘ is a predefined constant. In addition, the agent can do nothing and just collect observational data. In total, this dimension of the action space has ğ‘› + 1 elements. The second dimension represents the structural actions. Each action in this space indexes the removal and addition of edges on the currently estimated graph. Additionally, the agent can perform a void structural update which leads to a total  size of 2ğ‘›(ğ‘› -1) + 1 actions in this dimension since we disallow reflective edges. The action space scales quadratically with the number of nodes. To address this problem, we mask the possible actions at every step such that the agent cannot add an already existing edge or remove a non-existing edge. This effectively halves the size of this action space dimension.</p><p>Transition Dynamics: Each episode starts in the observational SCM ğ‘€ ğ‘‘ğ‘œ(âˆ…) where no intervention is performed. An intervention ğ‘‘ğ‘œ(ğ‘‹ ğ‘– ) on this SCM deterministically leads to a new SCM ğ‘€ ğ‘‘ğ‘œ(ğ‘‹ ğ‘– ) where ğ‘“ ğ‘– is replaced by some constant ğ‘. This effectively replaces ğ‘“ ğ‘– in the state. With an intervention ğ‘‘ğ‘œ(ğ‘‹ ğ‘— ), we transition from ğ‘€ ğ‘‘ğ‘œ(ğ‘‹ ğ‘– ) to ğ‘€ ğ‘‘ğ‘œ(ğ‘‹ ğ‘— ) , that is, ğ‘‡ (ğ‘€ ğ‘‘ğ‘œ(ğ‘‹ ğ‘– ) , ğ‘‘ğ‘œ(ğ‘‹ ğ‘— ), ğ‘€ ğ‘‘ğ‘œ(ğ‘‹ ğ‘— ) ) = 1 or, equivalently, ğ‘‡ (ğ‘€ ğ‘‘ğ‘œ(ğ‘‹ ğ‘– ) , ğ‘‘ğ‘œ(ğ‘‹ ğ‘— )) = ğ‘€ ğ‘‘ğ‘œ(ğ‘‹ ğ‘— ) . A minimal example of the transition dynamics of our approach can be seen in Figure <ref type="figure" target="#fig_2">2</ref>.</p><p>Observations: At each step ğ‘¡, the agent collects the value of the endogenous variables {ğ‘¥ 0 , . . . , ğ‘¥ ğ‘›-1 } from the joint distribution ğ‘ƒ ğ‘€ ğ‘‘ğ‘œ(ğ‘‹ ) (ğ’³ ) induced by the current SCM ğ‘€ ğ‘‘ğ‘œ(ğ‘‹ ) . Therefore, the observation is ğ‘œğ‘¡ âˆ¼ ğ‘ƒ ğ‘€ ğ‘‘ğ‘œ(ğ‘‹ ) (ğ’³ ).</p><p>State Representation: The use of a single observation ğ‘œğ‘¡ is not sufficient to determine the best action in POMDPs. Thus, the agent has to build its own state representation â„ğ‘¡ using the history of observations and actions <ref type="bibr" target="#b31">[32]</ref>. We denote the history of observations and actions by â„ğ‘¡ = (ï¸€ğ‘¥ 0 , ğ‘ 0 , ..., ğ‘¥ğ‘¡ , ğ‘ğ‘¡ âŒ‹ï¸€.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reward:</head><p>The structural Hamming distance (SHD) measures the distance between two DAGs by counting the number of different edges. Since our goal is to minimize the distance between the generated and the ground truth observational graphs at every step, we consider the SHD as a natural candidate for our reward function. For a ground truth observational graph ğº * ğ‘  , a graph estimate Äœğ‘¡ , and a graph estimate Äœğ‘¡ â€² in the consecutive step ğ‘¡ â€² , we define the potential-based reward <ref type="bibr" target="#b14">[15]</ref> as ğ‘Ÿ (ğ‘ , ğ‘) = ğ‘†ğ»ğ·(ğº * ğ‘  , Äœğ‘¡ ) -ğ‘†ğ» ğ·(ğº * ğ‘  , Äœğ‘¡ â€² ).</p><p>To simplify, we rewrite our reward function in the following way: Let ğ¸(ğ‘) be the directed edge that is manipulated in action ğ‘. Then, when adding an edge ğ¸(ğ‘), our reward becomes:</p><formula xml:id="formula_6">ğ‘Ÿ (ğ‘ , ğ‘) = )ï¸€ âŒ‰ï¸€ âŒ‰ï¸€ âŒ‹ï¸€ âŒ‰ï¸€ âŒ‰ï¸€ ]ï¸€ 1 if ğ¸(ğ‘) âˆˆ ğº * ğ‘  -1 if ğ¸(ğ‘) â‡‘ âˆˆ ğº * ğ‘ <label>(6)</label></formula><p>When removing an edge ğ¸(ğ‘) our reward becomes -ğ‘Ÿ (ğ‘ , ğ‘). In all other cases (ğ¸(ğ‘) = âˆ…) the reward is 0. This formulation has the computational advantage that only ğ¸(ğ‘) must be compared to the edges of ğº * ğ‘  instead of comparing the entire graph Äœğ‘  â€² . Furthermore, it makes the reward denser and depends only on ğ‘  and ğ‘ instead of relying on the entire history of structural actions that make up the current graph estimate. In Appendix A we demonstrate that this formulation is equivalent to ğ‘†ğ»ğ·(ğº * ğ‘  , Äœğ‘  ) -ğ‘†ğ»ğ·(ğº * ğ‘  , Äœğ‘  â€² ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Data-Generation</head><p>We train our CORE agents using a training set of DAGs. In addition, we have an evaluation set of DAGs that the agent has not seen during training. To ensure that the evaluation set does not include any graphs from the training set, we first create a set of unique DAGs, shuffle it to ensure equal sparsity throughout the list, and then divide it into training and evaluation sets. As per common assumption in machine learning, we assume that having more graphs in the training set will help us to generalize better to the evaluation set.</p><p>Since the space of DAGs grows superexponentially in the number of its nodes, it quickly becomes infeasible to generate all possible graph structures with ğ‘› nodes. For this reason, we generate all possible graphs only for graphs with 3 nodes (for a total of 25 graphs) and graphs with 4 nodes (for a total of 543 graphs). For graphs with more than 4 nodes, we generate subsets of the possible graphs. Similarly to many works in the literature, each graph is generated as an ErdÃ¶s-RÃ©nyi <ref type="bibr" target="#b7">[8]</ref> graph with an edge probability of 0.2. We diversify the training data by generating SCMs based on these graphs by sampling a function ğ‘“ ğ‘– (ğ‘ƒğ‘ ğº (ğ‘‹ ğ‘– )) from a class of possible functions for every node ğ‘‹ ğ‘– in the graph ğº at the beginning of each episode.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Learning Approach</head><p>The CORE agent is based on the DQN algorithm <ref type="bibr" target="#b21">[22]</ref>, but it maintains two multilayer perceptron Q networks simultaneously. One network, ğ‘„ğ‘ ğ‘¡ (â„, ğ‘ğ‘ ğ‘¡ ; Î˜ğ‘ ğ‘¡ ), estimates the Q-values specific to the structural updates, the other ğ‘„ ğ‘–ğ‘› (â„, ğ‘ ğ‘–ğ‘› ; Î˜ ğ‘–ğ‘› ) maintains the values for the interventions. Each of the two networks comes with its own target network, and the loss is computed separately. Note that Q values are determined on the basis of the history of observations, as is often done when applying DQN to POMDP problems <ref type="bibr" target="#b21">[22]</ref>. The networks are identical except for the output layers due to different dimensionalities of the action space. The overall output of the Q-function is the concatenation of the individual Q-values, and our greedy policy picks ğ‘ ğ‘–ğ‘› and ğ‘ğ‘ ğ‘¡ in such a way that the corresponding Q-values are maximized.</p><p>At the procedural level, our algorithm generates a new SCM based on a random sample of the graph data at the beginning of each episode. Furthermore, we start every episode with an empty graph estimate. The inference time for a given SCM is fully determined by the fixed number of steps per episode. This puts a hard upper bound on the number of samples and makes inference highly efficient. Figure <ref type="figure" target="#fig_2">2</ref> gives an overview of our agent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">GENERALIZATION TO UNSEEN STRUCTURES</head><p>In this section, we empirically validate whether our learned policy constitutes a good causal discovery algorithm. To this end, we train our model on a training set of SCMs with known causal structures and evaluate it on SCMs with causal structures that were not seen during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Training Data</head><p>For this experiment, we generate graphs with 3, 4, 5, 8, and 10 variables as described in Section 3.2. We split the generated graphs into training and test sets as follows: We first generate the graphs (25, 543, 16000, 91000, 101000), and then split the final list into train and test sets with splits 19/6, 401/142, 15000/1000, 90000/1000, 100000/1000 for 3, 4, 5, 8 and 10 variables, respectively. We limit the number of test graphs to 1000 since the evaluation would otherwise slow down the training prohibitively. At the beginning of each episode, a graph is sampled from the data set. To generate the SCMs in accordance with Section 3.2, we define a class of linear additive functions. For each node ğ‘‹ ğ‘– in the graph ğº, we sample â‹ƒï¸€ ğ‘ƒğ‘ ğº (ğ‘‹ ğ‘– ) â‹ƒï¸€ weights from ğ‘ˆ ğ‘›ğ‘– ğ‘“ ğ‘œğ‘Ÿğ‘š(0.5, 2.0). The generated function for this node is then ğ‘‹ ğ‘– â† Î£ ğ‘‹ ğ‘— âˆˆğ‘ƒğ‘(ğ‘‹ ğ‘– ) ğ‘¤ ğ‘— â‹…ğ‘¥ ğ‘— for the current values ğ‘¥ ğ‘— of the parents of ğ‘‹ ğ‘– . If ğ‘‹ ğ‘– is a root node, we assign a default value of 0. We use an intervention value of 20 to provide a strong signal about the causal structure w.r.t. to the true causal effect sizes. This value can be considered as a hyperparameter and we discuss its impact further in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Experimental Setup</head><p>We evaluate the generalization capability of CORE w.r.t. the available baselines. While AVICI <ref type="bibr" target="#b18">[19]</ref> learns a graph generator that estimates the causal structure of an offline dataset, CORE operates in a few-shot online data sample regime. Therefore, a meaningful comparison with AVICI is out of reach. Consequently, MCD <ref type="bibr" target="#b27">[28]</ref> is, to the best of our knowledge, the only SOTA method that learns a CD algorithm that actively intervenes. Furthermore, we compare with the random baseline that generates random DAGs, and the empty baseline which represents the empty graph.</p><p>We train both MCD and our approach for the same amount of steps and align all relevant hyperparameters including the neural network sizes. MCD runs into difficulties when scaling up to graphs with more than 4 nodes. Due to such computational infeasibility, we cannot run experiments for MCD on SCMs with 8 or 10 variables. A detailed description of the hyperparameters and the architectures used can be found in Appendix B. We set a maximum compute budget via a timeout of 25 training hours. The precise hardware configuration can be found in Appendix C. We paid special attention to setting comparable episode lengths for both approaches. For the sake of fair comparison, we set the episode length of our approach to half the episode length used in MCD, since our approach can perform interventions and structure updates synchronously, while MCD can only perform them sequentially.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Results</head><p>The results in Table <ref type="table" target="#tab_0">1</ref> are from applying the learned models to three SCMs randomly generated for each graph in the held-out test set. The model that achieved the best performance during training was chosen for each evaluation.</p><p>We can see the favorable performance of CORE compared to MCD and the trivial baselines. For all sizes of graphs tested, we observe that our approach estimates graphs that are closer to the ground-truth structures than the other approaches in less than 34 milliseconds per graph. We generate an average of 0.5, 0.5, 1.3, 2.0, and 5.2 wrong edges in graphs with 3, 4, 5, 8, and 10 nodes, respectively. Even in a set of graphs with 10 variables, our approach adds approximately 2 correct edges out of 90 potential edges while only observing 15 data points. For smaller graphs, the ratio of correctly identified edges is even higher.</p><p>We attribute the improvement over MCD to a variety of aspects. First, addressing the structural actions and the intervention actions with separate networks makes learning the corresponding Q-functions more efficient. This is because two separate networks have a greater degree of freedom to represent structural and intervention actions that are inherently different. Second, representing the reward densely instead of a summary at the end of each episode often improves performance <ref type="bibr" target="#b24">[25]</ref>. Third, instead of learning to integrate observations from the environment via a long short-term memory (LSTM) <ref type="bibr" target="#b12">[13]</ref>, we directly input the history of samples into our policy. And, lastly, by not generating graphs at runtime, but rather having a pre-defined training set, we avoid a significant computational overhead.</p><p>Furthermore, we observe the rather unfavorable performance of MCD compared to the baselines. We partly attribute this to a lack of extensive hyperparameter tuning, since this would likely have random 3.29 Â± 0.97 5.92 Â± 1. <ref type="bibr" target="#b40">41</ref>   been needed to achieve the results in <ref type="bibr" target="#b27">[28]</ref>. For the 4 and 5 variable cases, MCD reached the timeout of 25 hours. Given these results, we conclude that CORE is capable of successfully learning a CD algorithm that can be applied to previously unseen causal structures. Even for these cases, our approach estimates the ground truth graph accurately without having to retrain on the new structure. Furthermore, we show that with CORE's novelties, we are able to scale towards graph sizes of more relevance for real-world applications, while simultaneously increasing training efficiency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Examples</head><p>We present qualitative results on how our learned policy performs on the following two randomly selected example SCMs with unseen causal structures:</p><formula xml:id="formula_7">ğ‘€ 0 ğ‘‘ğ‘œ(âˆ…) = )ï¸€ âŒ‰ï¸€ âŒ‰ï¸€ âŒ‰ï¸€ âŒ‰ï¸€ âŒ‰ï¸€ âŒ‰ï¸€ âŒ‰ï¸€ âŒ‹ï¸€ âŒ‰ï¸€ âŒ‰ï¸€ âŒ‰ï¸€ âŒ‰ï¸€ âŒ‰ï¸€ âŒ‰ï¸€ âŒ‰ï¸€ ]ï¸€ ğ‘‹ 0 â† 0.54 â‹… ğ‘‹ 1 + 0.91 â‹… ğ‘‹ 3 + 0.83 â‹… ğ‘‹ 4 , ğ‘‹ 1 â† 1.52 â‹… ğ‘‹ 2 + 1.84 â‹… ğ‘‹ 3 , ğ‘‹ 2 â† 1.38 â‹… ğ‘‹ 3 , ğ‘‹ 3 â† 0, ğ‘‹ 4 â† 0 (7) ğ‘€ 1 ğ‘‘ğ‘œ(âˆ…) = )ï¸€ âŒ‰ï¸€ âŒ‰ï¸€ âŒ‰ï¸€ âŒ‰ï¸€ âŒ‰ï¸€ âŒ‰ï¸€ âŒ‰ï¸€ âŒ‰ï¸€ âŒ‰ï¸€ âŒ‰ï¸€ âŒ‹ï¸€ âŒ‰ï¸€ âŒ‰ï¸€ âŒ‰ï¸€ âŒ‰ï¸€ âŒ‰ï¸€ âŒ‰ï¸€ âŒ‰ï¸€ âŒ‰ï¸€ âŒ‰ï¸€ âŒ‰ï¸€ ]ï¸€ ğ‘‹ 0 â† 0, ğ‘‹ 1 â† 1.61 â‹… ğ‘‹ 3 , ğ‘‹ 2 â† 0.83 â‹… ğ‘‹ 1 + 1.60 â‹… ğ‘‹ 3 + 1.5 â‹… ğ‘‹ 4 , ğ‘‹ 3 â† 1.39 â‹… ğ‘‹ 0 , ğ‘‹ 4 â† 0.54 â‹… ğ‘‹ 0 ,<label>(8)</label></formula><p>In Figure <ref type="figure" target="#fig_3">3</ref>, we can see that in both cases the agent identifies the underlying causal structure almost correctly, with the exception of the missing edge ğ‘‹ 3 â†’ ğ‘‹ 0 in the first instance. In the second instance, it even recognizes an error and corrects it in Step 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">ON THE IMPORTANCE OF JOINTLY LEARNING AN INTERVENTION POLICY</head><p>As described in Section 3, our method is designed to jointly learn a causal graph generator and an intervention policy. In this section, we show that learning an intervention policy, aimed at performing the interventions that are most informative for CD, helps in learning a CD policy.</p><p>It is worth noting that our agent does not receive any specific reward that represents the quality of the intervention performed in the environment. Instead, the reward function depends only on the structural update of the currently estimated causal structure (see Equation ( <ref type="formula" target="#formula_6">6</ref>)). Since, for the full identification of the causal structure, interventions are generally needed <ref type="bibr" target="#b2">[3]</ref>, our agent has to learn to perform interventions to update the estimate of the causal structure. Therefore, our agent receives good rewards only if it performs interventions that are relevant to discover the current causal structure.</p><p>Although it is clear that interventions, in general, are helpful for CD, we argue that learning an intervention policy by measuring the usefulness for structure identification helps the overall learning process. Especially when the budget for performing interventions is very restrictive, as is the case in many real-world applications, it is crucial to perform the interventions that are most informative about the underlying causal structure <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b32">33]</ref>. Which interventions are the most informative ones depends on the causal structure that is currently being discovered. This further motivates learning the intervention policy jointly with the structure generation policy. In this section, we empirically show that there is in fact a benefit in learning an intervention policy jointly with the CD policy. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Setup</head><p>To show the hypothesized performance gain, we compare the performance of the agent that learns the intervention policy and the structure generation policy jointly, to an agent that learns only the structure generation policy and randomly picks an intervention target at each step. We train the two agents 3 times each on environments with 4 endogenous variables. The graphs, function classes, and hyperparameters remain the same as described in Section 4.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Results</head><p>Figure <ref type="figure" target="#fig_4">4</ref> shows the aggregated results for the two types of agents. We can see that learning the intervention policy jointly with the CD policy results in a significantly better estimation of the ground truth causal structure. Additionally, it decreases the number of learning steps needed to reach a certain level of performance. Lastly, Figure <ref type="figure" target="#fig_4">4</ref> suggests that even with random interventions, our approach performs reasonably well (average SHD of âˆ¼ 2.3). This indicates the robustness of the CORE agent to less informative interventions.</p><p>Overall, we observe that the ability to learn the intervention policy is an integral part of learning a CD policy and that rewarding interventions leading to better structure estimates is sensible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">APPLICABILITY TO THE REAL-WORLD</head><p>Throughout this work, we acknowledge the capability of learning CD algorithms that can be applied to environments with previously unseen causal structures with up to 10 variables. This constitutes a substantial improvement over the SOTA when it comes to the application of learned CD algorithms in the real world, as many problems can be modeled with 10 variables <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b40">41]</ref>. Therefore CORE, reaches graph sizes that are relevant in CD. However, we acknowledge that many applications with up to 5000 variables <ref type="bibr" target="#b19">[20]</ref> are currently out of reach. In this section, we shed light on some of the limitations that this approach currently has with regard to applying it in a real-world scenario.</p><p>Here, we specifically investigate two design aspects that limit real-world applicability, and they are interconnected. First, during training, the functions of the SCMs are sampled from a specific function class, and for some function classes (e.g., non-linear functions), discovering the true causal structure can be harder than for others <ref type="bibr" target="#b9">[10]</ref>. Consequently, as we will show in this section, learning a CD algorithm for these classes of functions is more difficult.</p><p>Second, our approach is tailored to generate graph estimates for the function class on which it was trained. This means that when used for different causal structures, the same function class is expected during inference. Consequently, this can lead to a decrease in performance if the function class is altered. We expect an exception for this for function classes that are either very similar to the training functions or that subsume them. Therefore, when CORE is trained with the intention of being used in a real-world setting, the real-world function class has to be anticipated during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Transferability across Noise and Non-Linearity</head><p>Motivated by these aspects, we show the difficulty in training CD policies on some function classes and test CORE on function classes that it was not trained on. where the lowercase ğ‘¥ ğ‘– represents the current value of the variable ğ‘‹ ğ‘– . Whenever a node is a root node, we set a default value of 0.</p><p>We train two CORE models for various graph sizes for each of the two linear functions, one with an intervention value of 5 and the other with an intervention value of 20. This setup gives us further insight into how the signal-to-noise ratio affects our performance. For the interaction function, we train one model with an intervention value of 5. We then tested all the trained models in all three function classes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2">Results.</head><p>In Table <ref type="table">2</ref>, we show how models that were trained with one function class perform when applied to various function classes with previously unseen causal structures.</p><p>Our first observation is that, as hypothesized, the application of our learned CD algorithm on previously unseen function classes is problematic if the testing function class is very different from the training function class. While applying the linear models to their noisy/non-noisy counterparts still leads to good estimates, applying them to the interaction data mostly fails. This supports our claim that the function class chosen for training must be informative about the function class encountered during testing.</p><p>Looking at the model that was trained on interaction data, we observe two interesting aspects. First, for graphs with 4 and 5 variables, CORE fails to learn a CD policy that generalizes to unseen graphs. We believe that, given the right hyperparameters and a sufficient training budget, we can solve the task for larger graphs,  <ref type="table">2</ref>: We show the performance of trained CORE policies on unseen graphs with various function classes for their corresponding SCMs. Each row describes which function class the model was trained on and what intervention value it uses. Each column describes the function class it was tested on. Empty describes the baseline that generates the empty graph.</p><p>based on the results obtained from smaller graphs. However, these results demonstrate that the performance of a learned CD algorithm depends on the complexity of the SCM function that generates the data. Second, we observe that, for the interaction case, the learned policy can be successfully applied to the linear function. We argue that this is because the interaction data encompasses the linear data as well. This suggests that if the function classes used during training are broad enough, the CD algorithm that is learned will be relatively more applicable to real-world scenarios.</p><p>When comparing linear models with a higher intervention value with those with a lower intervention value, we observe that they tend to perform better on function classes that they successfully learned. We attribute this to a higher signal-to-noise ratio w.r.t. the data-generating process.</p><p>Overall, we can say that learning CD algorithms is limited by the function class that is observed during training. This currently obstructs their application to real-world scenarios, but finding more general functions on which CORE can be trained is a promising research direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Further Limitations</head><p>Apart from aspects related to the function classes on which CORE is trained, we point out the additional limitations of learning a model that is applicable to the real world.</p><p>One of them is the assumption of being able to intervene on any variable with any target value. In real-world scenarios, it might be that some variables cannot be manipulated (imagine changing the outside temperature) or a good target value is unknown during training. Furthermore, CORE-like algorithms might suffer from the presence of unobserved confounders. For both the unknown target value and unobserved confounders, there is hope that augmenting the learning procedure in the future will overcome these limitations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">SUMMARY AND CONCLUSION</head><p>In this paper, we introduce CORE, a deep RL-based approach to tackle the task of causal discovery. CORE learns a policy to sequentially perform informative interventions and generate candidate causal graphs from scratch. Moreover, the learned policy generalizes to previously unseen graphs of up to 10 variables in size. CORE outperforms the current SOTA baseline (i.e., MCD <ref type="bibr" target="#b27">[28]</ref>) both in the number of variables it can deal with and in the accuracy of the estimated structure. Furthermore, it demonstrates that by learning to perform the most informative interventions, highly sample-efficient CD algorithms can be learned (âˆ¼ 15 data samples for 10 variables).</p><p>Such improvement can be attributed to several key design features. One such feature is the imposed additional structure in the policy that separates the networks for interventions and structural updates. However, such separation is not completely isolated. As shown in our ablation study, CORE learns to perform relevant interventions outperforming random interventions. Learning which interventions are relevant is guided solely through a dense reward that assesses the accuracy of the generated graph.</p><p>Moreover, we outlined the real-world applicability of our approach in terms of the number of variables and generalizability across more complex function classes. For the former, while the number of variables that CORE can deal with matches some domains, for some other domains, usual practice is still out of reach. For the latter, it turns out that CORE delivers good estimates when it comes to linear functions, training on noisy functions, and testing on non-noisy counterparts, and vice versa. However, generalizing to more complex classes such as non-linear functions necessitates improvements. Furthermore, we empirically confirmed that training on more complex function classes and testing on simpler classes yields promising estimates. Such an observation can serve as a key idea along the road of applying these methods to real-world problems.</p><p>Future work includes investigating the limitations of our approach in the presence of confounders, soft interventions, and determining the right target value for interventions. In particular, it would be interesting to understand how robust our approach is when it comes to different topologies regarding such non-intervenable variables and unobservable confounders. Another avenue that we plan to address is an extensive study of the transferability of our approach across different function classes, such that learned CD algorithms that autonomously plan interventions can be applied to real-world data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A EQUIVALENCE OF OUR REWARD AND THE DIFFERENCE IN SHDS</head><p>Let ğº * ğ‘  be the observational ground-truth graph of the current SCM ğ‘€ ğ‘– ğ‘‘ğ‘œ(âˆ…) . Furthermore, let Äœğ‘  be the estimated graph at state ğ‘ , Äœğ‘  â€² the estimated graph in the consecutive state and ğ¸(ğ‘) the edge that is manipulated by action ğ‘. We show that if ğ‘ deletes an edge. In the following, we derive this equality. For a simplified notation, we write ğ¸ ğº for the set of edges in ğº. We start by decomposing the difference in its set-theoretic components. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ğ‘†ğ» ğ·(ğº</head><p>Finally, considering the case where the edge was not manipulated (ğ¸(ğ‘) = âˆ…), we add Equations ( <ref type="formula">11</ref>) and ( <ref type="formula" target="#formula_8">12</ref>) and arrive at Equation (9). Case 2; an edge is removed from Äœğ‘  : In this case, the results in Equation ( <ref type="formula">11</ref>) and ( <ref type="formula" target="#formula_8">12</ref>) are multiplied by -1. After again adding the inverted parts, we arrive at Equation <ref type="bibr" target="#b9">(10)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B HYPERPARAMETERS</head><p>Table <ref type="table">3</ref> describes the main hyperparameters that we used throughout this paper. For MCD <ref type="bibr" target="#b27">[28]</ref>, we additionally used the default hyperparameters that were used in the original paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C HARDWARE REQUIREMENTS</head><p>We ran training and evaluation on the DAS-6 compute cluster <ref type="bibr" target="#b1">[2]</ref>. At inference time, CORE generates a graph estimate in about 18-34 milliseconds, depending on the size of the policy network and the episode length.</p><p>For training, CORE took approximately 51min, 1h40min, 2h5min, 23h, 30h to train for 3, 4, 5, 8, 10 variables, respectively. For the 3, 4, and 5 variable case these are the results on a 24 core machine with an RTX4000 GPU, for 8 and 10 variables the results are on a 48 core A100 machine.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A simple graphical illustration of a (hard) intervention. Given the causal graph ğº with endogenous variables ğ’³ = {ğ‘‹, ğ‘Œ, ğ‘ } and the corresponding noise variables ğ’° = {ğ‘ˆ ğ‘‹ , ğ‘ˆ ğ‘Œ , ğ‘ˆ ğ‘ }, intervening on variable ğ‘‹ (i.e., ğ‘‘ğ‘œ(ğ‘‹ = ğ‘¥)) results in modifying ğº into ğº â€² by pruning the incoming edges to node ğ‘‹ and assigning the value ğ‘¥.</figDesc><graphic coords="2,317.96,83.69,247.16,108.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Overview of COREs training setup (right) and a minimal example of the transition dynamics for an SCM with two endogenous variables (left). At each step, the agent picks the intervention/structural actions according to an ğœ–-greedy policy on ğ‘„ ğ‘–ğ‘› and ğ‘„ğ‘ ğ‘¡ respectively. The intervention is applied to the SCM ğ‘€ ğ‘– leading to a post-interventional distribution ğ‘ƒ ğ‘€ ğ‘‘ğ‘œ(.) from which an observation is sampled. The agent receives a reward based on the structure action and the induced graph of ğ‘€ ğ‘– ğ‘‘ğ‘œ(âˆ…) . The observation is added to the history of observations and serves as input to the agent. At the beginning of each episode a new ğ‘€ ğ‘— is drawn from the training set and the observation history is cleared.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Two examples of how the learned CORE policy estimates the causal structure of two unseen SCMs described in Equations (7) and (8). Green elements indicate intervention (do (c = 20)) and structural update (adding an edge) in the current step, respectively. The red arrow indicates the deletion of an edge.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Plot of the average SHD on the test set (lower is better). We present the means over three training runs of CORE with random interventions (blue) and when jointly learning an intervention policy (red) over graphs with 4 variables.</figDesc><graphic coords="7,73.04,83.69,201.73,125.89" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>ğ‘†ğ» ğ·(ğº* ğ‘  , Äœğ‘  ) -ğ‘†ğ»ğ·(ğº * ğ‘  , Äœğ‘  â€² ) = )ï¸€ âŒ‰ï¸€ âŒ‰ï¸€ âŒ‰ï¸€ âŒ‰ï¸€ âŒ‹ï¸€ âŒ‰ï¸€ âŒ‰ï¸€ âŒ‰ï¸€ âŒ‰ï¸€ ]ï¸€ 1 ğ‘– ğ‘“ ğ¸(ğ‘) âˆˆ ğº * ğ‘  -1 ğ‘– ğ‘“ ğ¸(ğ‘) â‡‘ âˆˆ ğº * ğ‘  0 ğ‘– ğ‘“ ğ¸(ğ‘) = âˆ…(9)if ğ‘ adds an edge andğ‘†ğ» ğ·(ğº * ğ‘  , Äœğ‘  ) -ğ‘†ğ»ğ·(ğº * ğ‘  , Äœğ‘  â€² ) = )ï¸€ âŒ‰ï¸€ âŒ‰ï¸€ âŒ‰ï¸€ âŒ‰ï¸€ âŒ‹ï¸€ âŒ‰ï¸€ âŒ‰ï¸€ âŒ‰ï¸€ âŒ‰ï¸€ ]ï¸€ -1 ğ‘– ğ‘“ ğ¸(ğ‘) âˆˆ ğº * ğ‘  1 ğ‘– ğ‘“ ğ¸(ğ‘) â‡‘ âˆˆ ğº * ğ‘  0 ğ‘– ğ‘“ ğ¸(ğ‘) = âˆ…(10)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>8.33 Â± 1.22 11.08 Â± 2.80 14.85 Â± 4.01 empty 1.80 Â± 0.90 3.80 Â± 1.10 6.20 Â± 0.42 5.10 Â± 1.Average SHDs on the test set of SCMs with unseen causal structures.</figDesc><table><row><cell>60</cell><cell>7.0 Â± 2.50</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>6.1.1 Experimental Setup. For the data-generating processes in this section, we test how noise and non-linearities influence the performance and transferability of CORE. Therefore, we use three function classes that define each function class ğ‘“ ğ‘– (ğ‘ƒğ‘(ğ‘‹ ğ‘– ), ğ‘ˆ ğ‘– ) in an SCM as follows:â€¢ linear:ğ‘“ ğ‘– = Î£ ğ‘‹ ğ‘— âˆˆğ‘ƒğ‘(ğ‘‹ ğ‘– ) ğ‘¤ ğ‘— â‹… ğ‘¥ ğ‘— (same as Section 4.1) â€¢ linear + noise: ğ‘“ ğ‘– = Î£ ğ‘‹ ğ‘— âˆˆğ‘ƒğ‘(ğ‘‹ ğ‘– ) ğ‘¤ ğ‘— â‹… ğ‘¥ ğ‘— + ğ‘¢ ğ‘– , where ğ‘¢ ğ‘– âˆ¼ ğ’© (0, 0.5) â€¢ interaction: ğ‘“ ğ‘– = Î£ ğ‘‹ ğ‘— âˆˆğ‘ƒğ‘(ğ‘‹ ğ‘– ) ğ‘¤ ğ‘— â‹… ğ‘¥ ğ‘— + ğ‘¥ ğ‘˜ â‹… ğ‘¥ ğ‘™ , where ğ‘‹ ğ‘— , ğ‘‹ ğ‘˜ âˆˆ ğ‘ƒğ‘(ğ‘‹ ğ‘– )</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>ğ‘†ğ»ğ·(ğº * ğ‘  , Äœğ‘  â€² ) =â‹ƒï¸€ ğ¸ ğº * ğ‘  âˆ– ğ¸ Äœğ‘  â‹ƒï¸€ + â‹ƒï¸€ ğ¸ Äœğ‘  âˆ– ğ¸ ğº * ğ‘  â‹ƒï¸€ -â‹ƒï¸€ ğ¸ ğº * ğ‘  âˆ– ğ¸ Äœğ‘  â€² â‹ƒï¸€ -â‹ƒï¸€ ğ¸ Äœğ‘  â€² âˆ– ğ¸ ğº * ğ‘  â‹ƒï¸€We now distinguish the two cases of adding and deleting, and edge. In these cases, the terms become: Case 1; an edge is added to Äœğ‘  : Thenâ‹ƒï¸€ ğ¸ ğº * ğ‘  âˆ– ğ¸ Äœğ‘  â‹ƒï¸€ -â‹ƒï¸€ ğ¸ ğº * ğ‘  âˆ– ğ¸ Äœğ‘  â€² â‹ƒï¸€= ğ¸ ğº * ğ‘  â‹ƒï¸€ -â‹ƒï¸€ ğ¸ Äœğ‘  â€² âˆ– ğ¸ ğº *</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>)ï¸€ âŒ‰ï¸€ âŒ‰ï¸€ âŒ‹ï¸€ âŒ‰ï¸€ âŒ‰ï¸€ ]ï¸€ 1 ğ‘– ğ‘“ ğ‘’ âˆˆ ğ¸ ğº  *  ğ‘  0 ğ‘– ğ‘“ ğ‘’ â‡‘ ğ‘  âˆˆ ğ¸ ğº  *</cell><cell>(11)</cell></row><row><cell>and</cell><cell>â‹ƒï¸€ ğ¸ Äœğ‘ </cell><cell>âˆ– ğ‘  â‹ƒï¸€=</cell><cell>)ï¸€ âŒ‰ï¸€ âŒ‰ï¸€ 0 âŒ‹ï¸€ âŒ‰ï¸€ âŒ‰ï¸€ ]ï¸€ -1 ğ‘– ğ‘“ ğ‘’ â‡‘ ğ‘– ğ‘“ ğ‘’ âˆˆ ğ¸ ğº  *  ğ‘  âˆˆ ğ¸ ğº  *  ğ‘ </cell></row></table><note><p>* ğ‘  , Äœğ‘  ) -</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Our approach can be applied to any data-generating process that allows for sampling from and intervening on its variables.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>We thank <rs type="person">Frank van Harmelen</rs> for his valuable input throughout this project and the anonymous reviews for helping to improve the final version of this work. This research was partially funded by the <rs type="funder">Hybrid Intelligence Center</rs>, a 10-year programme funded by the <rs type="funder">Dutch Ministry of Education, Culture and Science through the Netherlands Organisation for Scientific Research</rs>, <ref type="url" target="https://hybridintelligence-centre.nl">https://hybridintelligence-centre.nl</ref>, grant number <rs type="grantNumber">024.004.022</rs></p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_sUx3beM">
					<idno type="grant-number">024.004.022</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="table">3</ref><p>: The hyperparameters that we used to obtain our results. These parameters are the same for all experiments we performed in this paper.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Active learning of causal structures with deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Saber</forename><surname>Amir Amirinezhad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matin</forename><surname>Salehkaleybar</surname></persName>
		</author>
		<author>
			<persName><surname>Hashemi</surname></persName>
		</author>
		<idno type="DOI">10.1016/J.NEUNET.2022.06.028</idno>
		<ptr target="https://doi.org/10.1016/J.NEUNET.2022.06.028" />
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">154</biblScope>
			<biblScope unit="page" from="22" to="30" />
			<date type="published" when="2022-10">2022. 10 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A Medium-Scale Distributed System for Computer Science Research: Infrastructure for the Long Term</title>
		<author>
			<persName><forename type="first">Henri</forename><surname>Bal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dick</forename><surname>Epema</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cees</forename><surname>De Laat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rob</forename><surname>Van Nieuwpoort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Romein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Seinstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cees</forename><surname>Snoek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harry</forename><surname>Wijshoff</surname></persName>
		</author>
		<idno type="DOI">10.1109/MC.2016.127</idno>
		<ptr target="https://doi.org/10.1109/MC.2016.127" />
	</analytic>
	<monogr>
		<title level="j">Computer</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="54" to="63" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On Pearl&apos;s Hierarchy and the Foundations of Causal Inference</title>
		<author>
			<persName><forename type="first">Elias</forename><surname>Bareinboim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juan</forename><forename type="middle">D</forename><surname>Correa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duligur</forename><surname>Ibeling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Icard</surname></persName>
		</author>
		<idno type="DOI">10.1145/3501714.3501743</idno>
		<ptr target="https://doi.org/10.1145/3501714.3501743" />
	</analytic>
	<monogr>
		<title level="m">Probabilistic and Causal Inference</title>
		<meeting><address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="507" to="556" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salem</forename><surname>Lahlou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tristan</forename><surname>Deleu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><forename type="middle">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mo</forename><surname>Tiwari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emmanuel</forename><surname>Bengio</surname></persName>
		</author>
		<ptr target="http://jmlr.org/papers/v24/22-0364.html" />
	</analytic>
	<monogr>
		<title level="j">GFlowNet Foundations. Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="1" to="55" />
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Differentiable Causal Discovery from Interventional Data</title>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Brouillard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">SÃ©bastien</forename><surname>Lachapelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Lacoste</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Lacoste-Julien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Drouin</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper_files/paper/2020/file/" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems, H Larochelle, M Ranzato</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>F Balcan</surname></persName>
		</editor>
		<editor>
			<persName><surname>Lin</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="21865" to="21877" />
		</imprint>
	</monogr>
	<note>f8b7aa3a0d349d9562b424160ad18612-Paper.pdf</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Optimal Structure Identification with Greedy Search</title>
		<author>
			<persName><forename type="first">David</forename><surname>Maxwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chickering</forename></persName>
		</author>
		<idno type="DOI">10.1162/153244303321897717</idno>
		<ptr target="https://doi.org/10.1162/153244303321897717" />
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="507" to="554" />
			<date type="published" when="2003-03">2003. 3 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Bayesian structure learning with generative flow networks</title>
		<author>
			<persName><forename type="first">Tristan</forename><surname>Deleu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">AntÃ³nio</forename><surname>GÃ³is</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Emezue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mansi</forename><surname>Rankawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Lacoste-Julien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="https://proceedings.mlr.press/v180/deleu22a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Eighth Conference on Uncertainty in Artificial Intelligence (Proceedings of Machine Learning Research</title>
		<editor>
			<persName><forename type="first">James</forename><surname>Cussens</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</editor>
		<meeting>the Thirty-Eighth Conference on Uncertainty in Artificial Intelligence ( Machine Learning Research</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">180</biblScope>
			<biblScope unit="page" from="518" to="528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On random graphs</title>
		<author>
			<persName><forename type="first">P</forename><surname>ErdÃ³s</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>RÃ©nyi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Publicationes Mathematicae</title>
		<imprint>
			<biblScope unit="page" from="290" to="297" />
			<date type="published" when="1959">1959. 1959</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Factored Adaptation for Non-Stationary Reinforcement Learning</title>
		<author>
			<persName><forename type="first">Fan</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Biwei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sara</forename><surname>Magliacane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">; S</forename><surname>Koyejo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Belgrave</surname></persName>
		</author>
		<author>
			<persName><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><surname>Oh</surname></persName>
		</author>
		<idno>994917177213c55ff438ddf71-Paper-Conference.pdf</idno>
		<ptr target="https://proceedings.neurips.cc/paper_files/paper/2022/file/cf" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="31957" to="31971" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Review of Causal Discovery Methods Based on Graphical Models</title>
		<author>
			<persName><forename type="first">Clark</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Spirtes</surname></persName>
		</author>
		<idno type="DOI">10.3389/fgene.2019.00524</idno>
		<ptr target="https://doi.org/10.3389/fgene.2019.00524" />
	</analytic>
	<monogr>
		<title level="j">Frontiers in Genetics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Kcrl: A prior knowledge based causal discovery framework with reinforcement learning</title>
		<author>
			<persName><forename type="first">Uzma</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Md</forename><surname>Osman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gani</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning for Healthcare Conference</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="691" to="714" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Characterization and Greedy Learning of Interventional Markov Equivalence Classes of Directed Acyclic Graphs Peter B Â¨uhlmann</title>
		<author>
			<persName><forename type="first">Alain</forename><surname>Hauser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Buhlmann@stat</forename><surname>Math</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethz</forename><surname>Ch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="2409" to="2464" />
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Long Short-Term Memory</title>
		<author>
			<persName><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">JÃ¼rgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="DOI">10.1162/neco.1997.9.8.1735</idno>
		<ptr target="https://doi.org/10.1162/neco.1997.9.8.1735" />
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997-11">1997. 11 1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Constraintbased Causal Discovery: Conflict Resolution with Answer Set Programming</title>
		<author>
			<persName><forename type="first">Antti</forename><surname>Hyttinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frederick</forename><surname>Eberhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matti</forename><surname>JÃ¤rvisalo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Uncertainty in Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="340" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Calculus on MDPs: Potential Shaping as a Gradient</title>
		<author>
			<persName><forename type="first">Erik</forename><surname>Jenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Herke</forename><surname>Van Hoof</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Gleave</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2208.09570v2" />
		<imprint>
			<date type="published" when="2022-08">2022. 8 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Causal Discovery for Modular World Models</title>
		<author>
			<persName><forename type="first">Anson</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>SchÃ¶lkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ingmar</forename><surname>Posner</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=VfkjQzdGCH" />
	</analytic>
	<monogr>
		<title level="m">NeurIPS 2022 Workshop on Neuro Causal and Symbolic AI (nCSI)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">GFlowCausal: Generative Flow Networks for Causal Discovery</title>
		<author>
			<persName><forename type="first">Wenqian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinchuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunfeng</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianye</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Pang</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/2210.08185" />
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Efficient Neural Causal Discovery without Acyclicity Constraints</title>
		<author>
			<persName><forename type="first">Phillip</forename><surname>Lippe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taco</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Efstratios</forename><surname>Gavves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Amortized Inference for Causal Structure Learning</title>
		<author>
			<persName><forename type="first">Lars</forename><surname>Lorch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Sussex</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Rothfuss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>SchÃ¶lkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">; S</forename><surname>Koyejo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Belgrave</surname></persName>
		</author>
		<author>
			<persName><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><surname>Oh</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper_files/paper/2022/file/54" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="13104" to="13118" />
		</imprint>
	</monogr>
	<note>f7125dee9b8b3dc798bb9a082b09e2-Paper-Conference.pdf</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Predicting causal effects in large-scale systems from observational data</title>
		<author>
			<persName><forename type="first">H</forename><surname>Marloes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diego</forename><surname>Maathuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Colombo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Kalisch</surname></persName>
		</author>
		<author>
			<persName><surname>BÃ¼hlmann</surname></persName>
		</author>
		<idno type="DOI">10.1038/nmeth0410-247</idno>
		<ptr target="https://doi.org/10.1038/nmeth0410-247" />
	</analytic>
	<monogr>
		<title level="j">Nature Methods</title>
		<imprint>
			<biblScope unit="page" from="247" to="248" />
			<date type="published" when="1947-04-04">2010. 2010 7:4 7, 4 (4 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Causal Discovery and Reinforcement Learning: A Synergistic Integration</title>
		<author>
			<persName><forename type="first">ArquÃ­mides</forename><surname>MÃ©ndez-Molina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eduardo</forename><forename type="middle">F</forename><surname>Morales</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enrique</forename><surname>Sucar</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="https://proceedings.mlr.press/v186/mendez-molina22a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 11th International Conference on Probabilistic Graphical Models (Proceedings of Machine Learning Research</title>
		<editor>
			<persName><forename type="first">Antonio</forename><surname>SalmerÃ³n</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Rafael</forename><surname>Rumi</surname></persName>
		</editor>
		<meeting>The 11th International Conference on Probabilistic Graphical Models ( Machine Learning Research</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">186</biblScope>
			<biblScope unit="page" from="421" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Human-level control through deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joel</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><forename type="middle">K</forename><surname>Fidjeland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Ostrovski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stig</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Beattie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Sadik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Helen</forename><surname>King</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dharshan</forename><surname>Kumaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shane</forename><surname>Legg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Demis</forename><surname>Hassabis</surname></persName>
		</author>
		<idno type="DOI">10.1038/nature14236</idno>
		<ptr target="https://doi.org/10.1038/nature14236" />
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Joint Causal Inference from Multiple Contexts</title>
		<author>
			<persName><forename type="first">Sara</forename><surname>Joris M Mooij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Magliacane</surname></persName>
		</author>
		<author>
			<persName><surname>Claassen</surname></persName>
		</author>
		<ptr target="http://jmlr.org/papers/v21/17-123.html" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1" to="108" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Causal Induction from Visual Observations for Goal Directed Tasks</title>
		<author>
			<persName><forename type="first">Suraj</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Policy invariance under reward transformations: Theory and application to reward shaping</title>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daishi</forename><surname>Harada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stuart</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<biblScope unit="page" from="278" to="287" />
			<date type="published" when="1999">1999. 1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Introduction to Probabilities, Graphs, and Causal Models</title>
		<author>
			<persName><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
		<idno type="DOI">10.1017/CBO9780511803161.003</idno>
		<ptr target="https://doi.org/10.1017/CBO9780511803161.003" />
	</analytic>
	<monogr>
		<title level="m">Causality</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Causal Protein-Signaling Networks Derived from Multiparameter Single-Cell Data</title>
		<author>
			<persName><forename type="first">Karen</forename><surname>Sachs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omar</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dana</forename><surname>Pe'er</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douglas</forename><forename type="middle">A</forename><surname>Lauffenburger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Garry</forename><forename type="middle">P</forename><surname>Nolan</surname></persName>
		</author>
		<idno type="DOI">10.1126/science.1105809</idno>
		<ptr target="https://doi.org/10.1126/science.1105809" />
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">308</biblScope>
			<biblScope unit="page" from="523" to="529" />
			<date type="published" when="2005-04">2005. 4 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A Meta-Reinforcement Learning Algorithm for Causal Discovery</title>
		<author>
			<persName><forename type="first">Andreas W M</forename><surname>Sauter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erman</forename><surname>Acar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Francois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-Lavet</forename></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="https://proceedings.mlr.press/v213/sauter23a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second Conference on Causal Learning and Reasoning (Proceedings of Machine Learning Research</title>
		<editor>
			<persName><forename type="first">Cheng</forename><surname>Zhang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Dominik</forename><surname>Janzing</surname></persName>
		</editor>
		<meeting>the Second Conference on Causal Learning and Reasoning ( Machine Learning Research</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">213</biblScope>
			<biblScope unit="page" from="602" to="619" />
		</imprint>
	</monogr>
	<note>Mihaela van der Schaar</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Learning Neural Causal Models with Active Interventions</title>
		<author>
			<persName><forename type="first">Nino</forename><surname>Scherrer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olexa</forename><surname>Bilaniuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yashas</forename><surname>Annadani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Schwab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>SchÃ¶lkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Michael C Mozer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><forename type="middle">Rosemary</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><surname>Ke</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning Causal Graphs with Small Interventions</title>
		<author>
			<persName><forename type="first">Karthikeyan</forename><surname>Shanmugam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Murat</forename><surname>Kocaoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandros</forename><forename type="middle">G</forename><surname>Dimakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sriram</forename><surname>Vishwanath</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper_files/paper/2015/file/b" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">N</forename><surname>Cortes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Lawrence</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Lee</surname></persName>
		</editor>
		<editor>
			<persName><surname>Sugiyama</surname></persName>
		</editor>
		<editor>
			<persName><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<author>
			<persName><forename type="first">Peter</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clark</forename><forename type="middle">N</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Scheines</surname></persName>
		</author>
		<title level="m">Causation, prediction, and search</title>
		<imprint>
			<publisher>MIT press</publisher>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Reinforcement learning: An introduction</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Interventions, Where and How? Experimental Design for Causal Models at Scale</title>
		<author>
			<persName><forename type="first">Panagiotis</forename><surname>Tigas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yashas</forename><surname>Annadani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Jesson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>SchÃ¶lkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Bauer</surname></persName>
		</author>
		<ptr target="https://github.com/yannadani/cbed" />
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="24130" to="24143" />
			<date type="published" when="2022-12">2022. 12 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Meta learning for causal direction</title>
		<author>
			<persName><forename type="first">Jean-FranÃ§ois</forename><surname>Ton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dino</forename><surname>Sejdinovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenji</forename><surname>Fukumizu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="9897" to="9905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">D&apos;ya Like DAGs? A Survey on Structure Learning and Causal Discovery</title>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">J</forename><surname>Vowels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Necati</forename><surname>Cihan Camgoz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Bowden</surname></persName>
		</author>
		<idno type="DOI">10.1145/3527154</idno>
		<ptr target="https://doi.org/10.1145/3527154" />
	</analytic>
	<monogr>
		<title level="j">Comput. Surveys</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">4 2023</biblScope>
			<biblScope unit="page" from="1" to="36" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Ordering-Based Causal Discovery with Reinforcement Learning</title>
		<author>
			<persName><forename type="first">Xiaoqiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yali</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangjun</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianye</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2105" />
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">J C H</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Watkins</surname></persName>
		</author>
		<author>
			<persName><surname>Dayan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Q-Learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="279" to="292" />
			<date type="published" when="1992">1992. 1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">DAG-GNN: DAG Structure Learning with Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Yue</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tian</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="https://proceedings.mlr.press/v97/yu19a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning (Proceedings of Machine Learning Research</title>
		<editor>
			<persName><forename type="first">Kamalika</forename><surname>Chaudhuri</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</editor>
		<meeting>the 36th International Conference on Machine Learning ( Machine Learning Research</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page" from="7154" to="7163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">DAGs with NO TEARS: Continuous Optimization for Structure Learning</title>
		<author>
			<persName><forename type="first">Xun</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryon</forename><surname>Aragam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pradeep</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<ptr target="https://github.com/xunzheng/notears" />
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Causal Discovery with Reinforcement Learning</title>
		<author>
			<persName><forename type="first">Shengyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ignavier</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitang</forename><surname>Chen</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1906.04477" />
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Matjaz</forename><surname>Zwitter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Milan</forename><surname>Soklic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Breast Cancer. UCI Machine Learning Repository</title>
		<imprint>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
