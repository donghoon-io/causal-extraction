<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LyS at SemEval-2024 Task 3: An Early Prototype for End-to-End Multimodal Emotion Linking as Graph-Based Parsing</title>
				<funder ref="#_WRA36dx">
					<orgName type="full">ERDF</orgName>
				</funder>
				<funder ref="#_4nyCD9R">
					<orgName type="full">European Research Council</orgName>
					<orgName type="abbreviated">ERC</orgName>
				</funder>
				<funder ref="#_3JkPH6p">
					<orgName type="full">Ministry for Digital Transformation and Civil Service and &quot;NextGener-ationEU&quot;/PRTR</orgName>
				</funder>
				<funder>
					<orgName type="full">EU</orgName>
				</funder>
				<funder>
					<orgName type="full">FormaciÃ³n Profesional e Universidades and the Galician universities</orgName>
				</funder>
				<funder ref="#_NSJr56v">
					<orgName type="full">Xunta de Galicia</orgName>
				</funder>
				<funder ref="#_bW8GF7h">
					<orgName type="full">Centro de InvestigaciÃ³n de Galicia &quot;CITIC&quot;</orgName>
				</funder>
				<funder ref="#_T3RD5uZ #_XfJ7HMW">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ana</forename><surname>Ezquerro</surname></persName>
							<email>ana.ezquerro@udc.es</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Universidade da CoruÃ±a</orgName>
								<orgName type="institution" key="instit2">CITIC Departamento de Ciencias de la ComputaciÃ³n y TecnologÃ­as de la InformaciÃ³n Campus de ElviÃ±a s/n</orgName>
								<address>
									<postCode>15071 A</postCode>
									<settlement>CoruÃ±a</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">David</forename><surname>Vilares</surname></persName>
							<email>david.vilares@udc.es</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Universidade da CoruÃ±a</orgName>
								<orgName type="institution" key="instit2">CITIC Departamento de Ciencias de la ComputaciÃ³n y TecnologÃ­as de la InformaciÃ³n Campus de ElviÃ±a s/n</orgName>
								<address>
									<postCode>15071 A</postCode>
									<settlement>CoruÃ±a</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">LyS at SemEval-2024 Task 3: An Early Prototype for End-to-End Multimodal Emotion Linking as Graph-Based Parsing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-21T20:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This paper describes our participation in Se-mEval 2024 Task 3, which focused on Multimodal Emotion Cause Analysis in Conversations. We developed an early prototype for an end-to-end system that uses graph-based methods from dependency parsing to identify causal emotion relations in multi-party conversations. Our model comprises a neural transformerbased encoder for contextualizing multimodal conversation data and a graph-based decoder for generating the adjacency matrix scores of the causal graph. We ranked 7th out of 15 valid and official submissions for Subtask 1, using textual inputs only. We also discuss our participation in Subtask 2 during post-evaluation using multi-modal inputs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>SemEval 2024 Task 3 focused on Multimodal Emotion Cause Analysis in Conversations <ref type="bibr" target="#b31">(Wang et al., 2024)</ref>. Figure <ref type="figure" target="#fig_0">1</ref> shows an example provided by the organizers to illustrate the task. Two subtasks were proposed: Subtask 1, which uses only textual inputs, and Subtask 2, which allows for the consideration of video and audio processing as well.</p><p>The shared task is timely given the recent success of multimodal architectures combining computer vision <ref type="bibr" target="#b27">(Redmon et al., 2016;</ref><ref type="bibr">Wang et al., 2023b)</ref>, natural language processing <ref type="bibr" target="#b7">(Devlin et al., 2019;</ref><ref type="bibr" target="#b3">Beltagy et al., 2020)</ref>, and speech processing <ref type="bibr" target="#b15">(Gong et al., 2021;</ref><ref type="bibr" target="#b26">Radford et al., 2022)</ref> advancements. In the particular context of multimodal emotion analysis, the task builds on top of previous work such as recognizing the triggered emotions as a classification task <ref type="bibr" target="#b0">(Alhuzali and Ananiadou, 2021;</ref><ref type="bibr" target="#b35">Zheng et al., 2023)</ref> or predicting complex causeeffect relations between speakers <ref type="bibr" target="#b33">(Wei et al., 2020;</ref><ref type="bibr" target="#b8">Ding et al., 2020)</ref>. For the particular case of the shared task, the dataset -centered in English -relies on <ref type="bibr">(Wang et al., 2023a)</ref> and provides text, image and audio inputs. The goal of the task consists of predicting (i) the emotion associated to each utterance within the conversation, (ii) the cause-effect relations that trigger the emotions between utterances and (iii) the associated span in the cause utterance.</p><p>We had time and resources only to build a textual model for official participation in Subtask 1. We validated some multimodal baseline approaches using vision and audio inputs, but the computational resources required to fine-tune text and video data were beyond our range, so we participated in Subtask 2 only during post-evaluation. In what follows, we describe our approach. The implementation of our early prototype can be found at <ref type="url" target="https://github.com/anaezquerro/semeval24-task3">https://github.com/anaezquerro/semeval24-task3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contribution</head><p>We propose an end-to-end multimodal prototype based on a large multimodal encoder to contextualize text, image and audio inputs with a graph-based decoder to model the cause-effect relations between triggered emotions within multi-party conversations. The large encoder joins pretrained architectures in text <ref type="bibr" target="#b7">(Devlin et al., 2019)</ref>, image <ref type="bibr" target="#b9">(Dosovitskiy et al., 2021)</ref> and audio <ref type="bibr" target="#b2">(Baevski et al., 2020)</ref> modalities, while the decoder is adapted from the graph-based approaches in semantic parsing <ref type="bibr" target="#b11">(Dozat and Manning, 2018)</ref>. The model is trained end-to-end.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>Multimodal Emotion Cause Analysis A number of datasets collecting multi-party conversations <ref type="bibr" target="#b25">(Poria et al., 2019;</ref><ref type="bibr" target="#b4">Chatterjee et al., 2019;</ref><ref type="bibr" target="#b14">Firdaus et al., 2020)</ref> have been published to train and test multimodal neural architectures. Simpler configurations involve recognizing the speaker emotion at each utterance -this task is commonly known as Emotion Recognition (ER) <ref type="bibr" target="#b25">(Poria et al., 2019)</ref> while others require a deeper level of understanding to model interactions and causal relations between speakers -Emotion-Cause Pair Extraction (ECPE) <ref type="bibr" target="#b34">(Xia and Ding, 2019)</ref>. The most common approaches follow an encoder-decoder neural architecture where the encoder is conformed by multiple modules -one module per input modality (text, image and/or audio) -and produces an inner representation at utterance level; and the decoder accepts the encoder outputs as inputs and returns a suitable output adapted to the specifications of the targeted task. In the context of Multimodal ER, Nguyen et al. ( <ref type="formula">2023</ref>) proposed a GCN-based decoder to capture temporal relations <ref type="bibr" target="#b28">(Schlichtkrull et al., 2017)</ref>, while <ref type="bibr" target="#b12">Dutta and Ganapathy (2024)</ref> used cross-attention to fusion the input modalities and a final classification layer to predict the targeted emotions. Approaches in ECPE require an extra effort to represent and model causal information: <ref type="bibr" target="#b33">Wei et al. (2020)</ref> scored all possible utterance tuples to predict the most probable list of emotion-cause pairs. Other authors, like <ref type="bibr" target="#b5">Chen et al. (2020)</ref> and <ref type="bibr" target="#b13">Fan et al. (2020)</ref>, represented the emotion-cause pairs as a labeled graph between utterances and tried to predict the set of causal edges using a GCN or a transition-based system, respectively. The SemEval 2024 Task 3 joins the recognition and causal extraction tasks and challenges a system able to both model speaker emotions and elicit relations.</p><p>Graph-based decoding For structured prediction tasks, such as dependency parsing, graphbased approaches are a standard for computing output syntactic representations <ref type="bibr" target="#b22">(McDonald, 2006;</ref><ref type="bibr" target="#b21">Martins et al., 2013)</ref>. Particularly, Dozat and Manning (2017) introduced a classifier that computes a head and dependent representation for each token and then uses two biaffine classifiers: one computes a score for each pair of tokens to determine the most likely head, and the other determines the label for each head-dependent token pair. We will also build upon a biaffine graph-based parser: we will frame the task as predicting a dependency graph, where utterances are the nodes and emotions are dependency labels between pairs of utterances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">System Overview</head><p>Our system consists of two modules: a large pretrained encoder and a graph-based decoder (see Figure <ref type="figure">2</ref>). It can add extra input channels into the encoder without requiring any adjustments to the decoder, so the same decoder is used for both tasks, while the encoder is adapted to incorporate textonly (Subtask 1) or multimodal (Subtask 2) inputs.</p><formula xml:id="formula_0">ğ‘ˆ 1 ğ‘ˆ 2 ğ‘ˆ 3 â€¦ ğ‘ˆ ğ‘š u 1 u 2 u 3 â€¦ u ğ‘š ENCODER DECODER ğ‘ˆ 1 ğ‘ˆ 2 ğ‘ˆ 3 â€¦ ğ‘ˆ ğ‘š ğ‘’ 1 ğ‘’ 2 ğ‘’ 3 â€¦ ğ‘’ ğ‘š</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input utterances</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Utterance embeddings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Emotion cause predictions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Post-process</head><p>Figure <ref type="figure">2</ref>: High-level architecture of our system. The encoder takes as input the sequence of m utterances of a given conversation and returns a unique vector representation for each utterance. The decoder uses the utterance embedding matrix to apply the affine attention product in the decoder, obtain the scores of the adjacent matrix and return the predicted sequence of emotions and the cause relations between utterances.</p><p>Let C = (U 1 , ..., U m ) be a conversation of m utterances, where each utterance</p><formula xml:id="formula_1">U i = {W i , s i , Îµ i } is defined by (i) a sequence of words W i = (w (i) 1 , ..., w (i)</formula><p>â„“|w,i| )<ref type="foot" target="#foot_0">foot_0</ref> , (ii) an active speaker s i and (iii) a triggered emotion Îµ i âˆˆ E<ref type="foot" target="#foot_1">foot_1</ref> . The set of cause-pair relations between utterances can be represented as a directed labeled graph G = (U, R) where U = (U 1 , ..., U m ) is the sequence of utterances of the conversation assuming the role of the nodes of the graph and</p><formula xml:id="formula_2">R = {U i Îµ j -â†’ U j , i, j âˆˆ [1, m]}</formula><p>is the set of emotion-cause relations between an arbitrary cause utterance U i and its corresponding effect U j . Thus, the task can be cast as the estimation of the adjacent matrix of G, similarly to syntactic <ref type="bibr" target="#b17">(Ji et al., 2019)</ref> and semantic dependency <ref type="bibr" target="#b11">(Dozat and Manning, 2018)</ref> parsing. Adapting algorithms from parsing to model emotion-cause relations between utterances has also been explored by other authors, such as <ref type="bibr" target="#b13">Fan et al. (2020)</ref>, who instead explored a transition-based strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Textual Extraction</head><p>The first subtask draws from only textual information to predict the adjacent matrix of G with a span that covers the specific words from U i that trigger the emotion Îµ j in the cause relation</p><formula xml:id="formula_3">U i Îµ j -â†’ U j .</formula><p>Textual encoder Figure <ref type="figure">3</ref> illustrates our encoder. Given the sequence of utterances (U 1 , ..., U m ), we encoded with BERT the batched sequence of utterances where each word sequence was preceded by the CLS token <ref type="bibr" target="#b7">(Devlin et al., 2019)</ref>. For each U i , we select the CLS embedding (u i ) from the contextualized embedding matrix W i = (u i , w 1 , ..., w â„“|w,i| ), which is assumed to have information of the whole sentence. The CLS embedding matrix U = (u 1 , ..., u m ) was passed as input to the decoder module and the word embeddings were reserved for the span attention module.</p><formula xml:id="formula_4">B E R T â€¦ ğ‘ˆ1 ğ‘¤1 ğ‘¤ğ‘› â€¦ ğ‘ˆ2 â€¦ ğ‘ˆğ‘š DECODER Word embeddings u 1 â€¦ u 2 u ğ‘š Utterance embeddings SPAN ATTENTION e 1 â€¦ e 2 e ğ‘š Effect embeddings 2 3 4 â€¦ â€¦ Textual batch ğ‘Šğ‘š: ğ‘ ğ‘¤ 1 ğ‘š â‹¯ ğ‘¤ â„“|ğ‘¤,ğ‘š| (ğ‘š) ğ‘Š2: ğ‘ ğ‘¤ 1 2 â‹¯ ğ‘¤ â„“|ğ‘¤,2|<label>(2)</label></formula><p>ğ‘Š1: ğ‘ ğ‘¤ 1 1 â‹¯ ğ‘¤ â„“|ğ‘¤,1|</p><p>(1)</p><p>1</p><p>Figure <ref type="figure">3</ref>: High level representation of the textual encoder. The input ( 1 ) is the matrix of stacked token vectors of each utterance. The last hidden states of BERT are used as word embeddings ( 2 ) and the special CLS tokens are used as utterance embeddings ( 3 ). The effect embeddings ( 4 ) -a partial representation from the decoder -are taken as input to the span module with the contextualized BERT embeddings.</p><p>Graph-based decoder Figure <ref type="figure" target="#fig_1">4</ref> shows the forward-pass of the graph-based decoder from the encoder output of Figure <ref type="figure">3</ref>. To produce an adjacent matrix G of dimensions m Ã— m, where each position (i, j) represents the probability of a causal relation from U i (cause) to U j (effect), the first biaffine module uses a trainable matrix W G âˆˆ R d G Ã—d G and maps U using two feed-forward networks to a cause (C) and an effect (E) representation. By projecting the original BERT embeddings to two different representations, u i âˆ¼ (c i , e i ), the decoder learns different contributions for the same utterance depending on the role. The affine product is</p><formula xml:id="formula_5">u 1 â€¦ u 2 u ğ‘š e 1 â€¦ e 2 e ğ‘š c 1 â€¦ c 2 c ğ‘š E â‹… ğ‘¾ ğº â‹… C âŠ¤ = Graph Scores à´¤ e 1 â€¦ à´¤ e 2 à´¤ e ğ‘š Ò§ c 1 â€¦ Ò§ c 2 Ò§ c ğ‘š à´¤ E â‹… ğ‘¾ ğœ€ â‹… à´¤ C =</formula><p>Emotion Probability defined as</p><formula xml:id="formula_6">G = E â€¢ W â€¢ C âŠ¤ . The second biaffine module uses a trainable tensor W Îµ âˆˆ R d G Ã—|E|Ã—d G</formula><p>to predict the probabilities of triggered emotions between cause-effect utterances.</p><p>Span Attention module To maintain the endto-end prediction while learning the span associated to each relation U i â†’ U j , we created a binary tensor S = (S 1 â€¢ â€¢ â€¢ S m ) of dimensions mÃ—mÃ—max i=1,...,m {â„“|w, i|}<ref type="foot" target="#foot_2">foot_2</ref> to specify if a word w k âˆˆ W i of U i is included in the span that triggers an emotion in U j . To compute each S i , the matrix of word embeddings (W i ) of the utterance U i is passed through a One-Head Attention module (see Figure <ref type="figure" target="#fig_2">5</ref>), where W i acts as the query matrix and E as the key and value matrices, so</p><formula xml:id="formula_7">S i = Î¦(softmax(W i â€¢ E âŠ¤ ) â€¢ E),</formula><p>where Î¦ is a feed-forward network to project the embedding dimension to a unique binary value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Encoding speaker information</head><p>The dataset includes information about the active speakers in each utterance. A first approach to use this information as input would be concatenating the speaker embeddings to the sequence of utterances. However, this might lead to some issues: the model could assume that there is some inner dependency between triggered emotions and the characters in the conversation. This might be true in some cases, but it can also lead to biases, and there is still the challenge of modeling infrequent and unknown characters. To deal with this, we encoded a conversation C with speakers s 1 , ..., s m using relative  <ref type="bibr">et al. (2017)</ref>. The tensor of word embeddings (W 1 â€¢ â€¢ â€¢ W m ) from the encoder (Figure <ref type="figure">3</ref>) and the effect contextualizations (E) from the decoder (Figure <ref type="figure" target="#fig_1">4</ref>) are passed to the attention product using each W i as key and E as query and value matrices.</p><p>positional embeddings. For instance, the sequence (Chandler, Phoebe, Monica, Chandler, Phoebe) in Figure <ref type="figure" target="#fig_0">1</ref> would be encoded as (0, 1, 2, 0, 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Multimodal Extraction</head><p>The second subtask adds a short video representation to each utterance, so</p><formula xml:id="formula_8">U i in a conversation C = (U 1 , ..., U m ) is now a tuple of five differ- ent elements U i = {W i , s i , Îµ i , X i , a i }.</formula><p>The last two added items encode the image and audio: (i)</p><formula xml:id="formula_9">X i = (x (i) 1 , ..., x<label>(i)</label></formula><p>â„“|x,i| ) is the sequence of frames of the input video, where each frame is an image 4 tensor of dimensions h Ã— w Ã— 3 and (ii) a i is the sampled audio signal of arbitrary length.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image encoding</head><p>We relied on a Transformerbased architecture <ref type="bibr" target="#b20">(Ma et al., 2022;</ref><ref type="bibr" target="#b35">Zheng et al., 2023)</ref> to contextualize input images. While recent studies have proposed adaptations of the Vision Transformer and 3-dimensional convolutions that capture temporal correlations between sequences of frames for video classification <ref type="bibr" target="#b1">(Arnab et al., 2021;</ref><ref type="bibr" target="#b24">Ni et al., 2022)</ref>, our experiments were constrained by our resource limitations, preventing us from using these pretrained architectures. Hence, for our multimodal baseline we opted for the the smallest version of the Vision Transformer (ViT) model <ref type="bibr" target="#b9">(Dosovitskiy et al., 2021)</ref> pretrained on the Facial Emotion Recognition dataset <ref type="bibr" target="#b16">(Goodfellow et al., 2013)</ref> 5 to contextualize a small fraction of sampled frames 6 , and incorporated an LSTM-based module to derive a unique image representation for each utterance. From an image batch X i , each image 4 All frames are RGB images, being the majority resolution 720 Ã— 1280.</p><p>5 <ref type="url" target="https://huggingface.co/trpakov/vit-face-expression">https://huggingface.co/trpakov/vit-face-expression</ref>.</p><p>6 For our experiments we used 5 interleaved frames per video, although a lower sampling rate can be considered depending on the computational capabilities.</p><p>x (i) k âˆˆ R hÃ—wÃ—3 was passed to the ViT base model to recover the output of the last hidden layer and introduce it as input to the LSTM module to recover a final representation for U i .</p><p>Audio encoding For our multimodal system we used the hidden contextualizations of the base version of wav2vec 2.0 <ref type="bibr" target="#b2">(Baevski et al., 2020)</ref>  <ref type="foot" target="#foot_3">7</ref> . Given a raw audio (a i ) of an utterance U i , the encoder of wav2vec 2.0 returns a sequence of hidden states that we summarized with an additional trainable LSTM layer to retrieve a unique vector that contents the audio information.</p><p>â€¦ â€¦</p><formula xml:id="formula_10">X 1 : x 1 (1)</formula><p>x 2</p><p>(1) â‹¯ x â„“|ğ‘¥,1|</p><p>(1)</p><formula xml:id="formula_11">X 2 : x 1 (2)</formula><p>x 2</p><p>(2) â‹¯ x â„“|ğ‘¥,2|</p><p>(2) Model fine-tuning The multimodal encoder ( Â§3.2) uses three pretrained architectures to contextualize individual utterances and passes to the decoder the concatenation of the three unimodal representations (Figure <ref type="figure" target="#fig_3">6</ref>). We chose to fine-tune only BERT during training together with the rest of the network. This was based on our empirical observation of superior results when learning from text compared to image and audio data. We entrusted the learning of audiovisual data to the LSTM learnable module within the encoder, presuming an accurate initial contextualization from wav2vec 2.0 and ViT pretrained on FER-2013.</p><formula xml:id="formula_12">X ğ‘š : x 1 (ğ‘š) x 2 (ğ‘š) â‹¯ x â„“|ğ‘¥,ğ‘š| (ğ‘š) â€¦ â€¦ ğ‘Š ğ‘š : ğ‘ ğ‘¤ 1 ğ‘š â‹¯ ğ‘¤ â„“|ğ‘¤,ğ‘š| (ğ‘š) a 1 a 2 â‹¯ a ğ‘š BERT ViT wav2vec b 1 b ğ‘š â€¦ à·¨ X 1 à·¨ X ğ‘š â€¦ à·© A 1 à·© A ğ‘š â€¦ à·© W 1 à·© W ğ‘š â€¦ x 1 x ğ‘š â€¦ a 1 a ğ‘š â€¦ LSTM LSTM ğ‘Š 2 : ğ‘ ğ‘¤ 1 2 â‹¯ ğ‘¤ â„“|ğ‘¤,2| (2) ğ‘Š 1 : ğ‘ ğ‘¤ 1 1 â‹¯ ğ‘¤ â„“|ğ‘¤,1|<label>(</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Post-processing</head><p>Our end-to-end system directly recovers the predicted emotion-cause relations in a single postprocessing step that linearly operates with the output tensors of the decoder. For the first subtask, the decoder returns (i) the adjacent matrix G âˆˆ R mÃ—m , (ii) the labeled adjacent matrix G âˆˆ R mÃ—mÃ—|E| and (iii) the span scores S âˆˆ R mÃ—mÃ—â„“max|w,i| . As <ref type="bibr" target="#b10">Dozat and Manning (2017)</ref>, each arc U i â†’ U j is predicted by thresholding G, and, once the arcs are predicted, the tensor G determines the label (emotion) associated to each arc. Since our formalization ( Â§3.1) associates a given utterance to an unique emotion, we leveraged the scores of G by the cause utterances and return the emotion with highest score. Finally, to produce a continuous span for each score vector s ij , we considered the leftmost and rightmost elements of s ij higher than a fixed threshold. Table <ref type="table">1</ref>: Evaluation of our prototype with different multimodal configurations. Precision (P), recall (R) and F-Score (F) measured the weighted average across the eight emotions of the dataset (superscript w denotes that the measure is weighted) and for the first subtask the span performance is considered with strict correctness (subscript s) or overlapping (subscript p). The symbol â€  remarks the reference metric for each subtask.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Validation The annotated dataset contains 1 375 multi-party conversations with a total of 13 619 utterances <ref type="bibr">(Wang et al., 2023a)</ref>. Although an unbiased estimation of the performance of our system would require validating the trained architecture using all available annotated data, our time and resources limitations prevented us from conducting k-fold cross-validation. Instead, we partitioned a 15% of the annotated dataset as our development set. The specific split used will be available with the accompanying code to replicate our findings.</p><p>Evaluation We use the official metrics<ref type="foot" target="#foot_4">foot_4</ref> : the weighted strict F-Score for the Subtask 1 and the weighted F-Score for the Subtask 2.</p><p>Hyperparameter configuration Our computational limitations prevented us from exhaustively searching the optimal hyperparameters for our system. We conducted some tests varying the pre-trained text encoder<ref type="foot" target="#foot_5">foot_5</ref> , model dimension, gradient descent algorithm and rate and adding or removing the speaker module. We maintained in all experiments some regularization techniques (such as dropout in the hidden layers and gradient norm clipping) to avoid over-fitting. Our final configuration uses AdamW optimizer <ref type="bibr" target="#b19">(Loshchilov and Hutter, 2019)</ref> with learning rate of 10 -6 and is trained during one hundred epochs with early stopping on the validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head><p>Table <ref type="table">1</ref> presents the performance of our system for both subtasks. For the first subtask, we investigated various embedding sizes of the Biaffine decoder while concurrently fine-tuning the largest version of BERT<ref type="foot" target="#foot_6">foot_6</ref> . For the second subtask, we conducted experiments using different types of inputs to evaluate their impact. These included: (i) using only text-based inputs, (ii) adding audio data, (iii) incorporating visual data through frames, and (iv) leveraging all available multimodal inputs together.</p><p>For approaches (i), (ii) and (iii), only BERT was fine-tuned, whereas for approach (iv), all pretrained weights were frozen. These weights solely served to contextualize input information, with the learning process confined to the decoder component.</p><p>Our top-performing model for the first subtask achieved a validation score of 9.75 and ranked in the evaluation set in 7th position among 15 participants with 6.77 points. We observed a slight performance improvement by increasing the hidden dimension of the decoder. Thus, considering the expansion of decoder layers could improve the performance. It is worth noting the significant impact of span prediction on the model performance: the proportional results consistently outperform strict metrics. Removing span prediction while retaining only text inputs results in a notable increase in F-Score (20.43 points for the second subtask), indicating the crucial role of span prediction in model learning. Furthermore, we noticed that there was a consistent delay in the alignment between recall and precision metrics, with precision consistently exceeding recall by more than 5 points across all approaches. This suggests that our system tends to adopt a conservative behavior, avoiding the number of false cause emotion predictions.</p><p>The best validation performance for the second subtask is achieved through the integration of text and audio, yielding a score of 23.36 points in the weighted F-Score. Using image data also improves the text-only baseline, though unexpectedly lags behind the audio model. It is important to note that these two approaches are not directly comparable due to differences in their data inputs: the text and image model only considers a fixed number of sampled frames, suggesting that providing more image data (ideally, the full sequence of frames) could potentially yield a better performance that surpasses the audio-based approach. Unfortunately, we could not fine-tune BERT with the full multimodal encoder, so we were restricted to projecting the multimodal inputs to their respective contextualizations, and relying the trainable weights of the decoder to optimize the full architecture. The results prove the importance of, at least, finetuning the text encoder: the F-Score only reaches 11.25 points, whereas the text finetuned baseline nearly doubles its performance with 20.43 points, highlighting the insufficient context of the original pretrained BERT embeddings to address this task.</p><p>Once the post-evaluation period concluded, we upload an experimental submission of our best multimodal system to the official competition. We obtained 15.32 points in the weighted F-Score, positioning our baseline in the 13th place out of 18 participants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We proposed a graph-based prototype for the analysis emotion-cause analysis in conversations. Given the limited preparation time, we only submitted official results for Subtask 1 (text-only), but also report post-evaluation results for Subtask 2 (multimodal). The task required predicting several aspects of the conversation: (i) the emotion associated with each utterance, (ii) the cause-effect relationships triggering these emotions between utterances, and (iii) the specific span within the cause utterance responsible for the emotion. We achieved 7th place out of 15 valid submissions for Subtask 1, a promising outcome considering the time and resource constraints we had to prepare the task. Yet, our results make us optimistic about exploring future research avenues to enhance our system and study lighter approaches that can perform competitively. As future work, we aim to experiment with smaller and distilled models to encode textual, visual, and audio inputs, enabling us to fine-tune the full model cheaply.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example taken from the official website of the SemEval Task 3https://nustm.github.io/SemEval-2 024_ECAC/. The goal of the task consists of predicting (i) the emotion associated to each utterance within the conversation, (ii) the cause-effect relations that trigger the emotions between utterances and (iii) the associated span in the cause utterance.</figDesc><graphic coords="1,308.69,219.91,213.17,69.28" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Graph-based decoder. The utterance embeddings ( 1 ) are projected to different representations ( 2 , 3 , 5 6 ) using four feed-forward networks to flexibly represent utterance embeddings. The scores of the adjacent matrix and the probability tensor are computed with the affine attention product.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Span Attention module adapted from Vaswaniet al. (2017). The tensor of word embeddings (W 1 â€¢ â€¢ â€¢ W m ) from the encoder (Figure3) and the effect contextualizations (E) from the decoder (Figure4) are passed to the attention product using each W i as key and E as query and value matrices.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Multimodal encoder for Subtask 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>defined as Ui =(Wi, si, Îµi, Xi, ai). Wi Word sequence of Ui as Wi = (w1, ..., w â„“|w,i| ) si Speaker of Ui, where si âˆˆ S Îµi Emotion triggred in Ui, where Îµi âˆˆ E S Set of speakers in the dataset. E Set of annotated emotions. Xi Sequence of frames of Ui as Xi = (x1, ..., x â„“|x,i| ) x (i) k Specific frame of Xi, where x (i) k âˆˆ R hÃ—wÃ—3 . ai Sampled audio signal of Ui, where ai âˆˆ R â„“|a,i| . â„“|w, i| Length of the sequence Wi. â„“|x, i| Lenght of the sequence Xi E Description ui Encoder hidden representation of Ui from BERT, where ui âˆˆ R 1024 . Wi BERT word embeddings of Wi as Wi = (ui, w(i) 1 , ..., w (i) â„“|w,i| ). xi Visual hidden representation for Ui, obtained as xi = LSTM -1 x (ViT(Xi)) âˆˆ R d V . Ã£i Audio hidden representation for Ui, obtained as Ã£i = LSTM -1 a (wav2vec(ai)) âˆˆ R da . Å©i Multimodal representation for Ui as Å©i = (ui|xi|Ã£i). D Description Î¦ Arbitrary feed-forward network. ci Cause embedding for Ui as ci = Î¦c(ui) âˆˆ R d G . ei Effect embedding for Ui as ei = Î¦e(ui) âˆˆ R d G . ci Emotion cause embedding for Ui as ci = Î¦c,Îµ(ui) âˆˆ R d G . ei Emotion effect embedding for Ui as ei = Î¦e,Îµ(ui) âˆˆ R d G . C Matrix of cause embeddings as C = (c1, ..., cm). E Matrix of effect embeddings as E = (e1, ..., em). C Matrix of emotion cause embeddings as C = (c1, ..., cm). E Matrix of emotion cause embeddings as E = (e1, ..., em). W Trainable weights for the first biaffine module, where W âˆˆ R d G Ã—d G WÎµ Trainable weights for the second biaffine module, where WÎµ âˆˆ R d G Ã—|E|Ã—d G .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Symbol notation.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>From now on, we denote as â„“|â€¢, i| the length of the i-th in a sequence â€¢, so â„“|w, i| denotes the length of the Wi. Table</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>summarizes the notation used in this paper.2  The set of emotions are described inWang et al. (2023a).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>Note that each matrix Si has dimensions m Ã— maxi=1,...,m{â„“|w, i|} and is associated to a cause utterance.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_3"><p>https://huggingface.co/facebook/wav2vec2-base-960h.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_4"><p>https://github.com/NUSTM/SemEval-2024_ECAC/tree/main/Cod aLab/evaluation.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_5"><p>We performed some experiments using all the versions of BERT,<ref type="bibr" target="#b7">(Devlin et al., 2019)</ref>, RoBERTa<ref type="bibr" target="#b18">(Liu et al., 2019)</ref> and XLM-RoBERTa<ref type="bibr" target="#b6">(Conneau et al., 2020)</ref> and selected the best-performing textual encoder (BERT-large).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_6"><p>https://huggingface.co/google-bert/bert-large-cased</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This work has received supported by Grant <rs type="grantNumber">GAP (PID2022-139308OA-I00</rs>) funded by <rs type="grantNumber">MCIN/AEI/10.13039/501100011033/</rs> and by <rs type="funder">ERDF</rs>, <rs type="funder">EU</rs>; the <rs type="funder">European Research Council (ERC)</rs>, under the <rs type="programName">Horizon Europe research and innovation programme (SALSA</rs>, grant agreement No <rs type="grantNumber">101100615</rs>); Grant <rs type="projectName">SCANNER-UDC</rs> (<rs type="grantNumber">PID2020-113230RB-C21</rs>) funded by <rs type="grantNumber">MICIU/AEI/10.13039/501100011033</rs>; <rs type="funder">Xunta de Galicia</rs> (<rs type="grantNumber">ED431C 2020/11</rs>); by <rs type="funder">Ministry for Digital Transformation and Civil Service and "NextGener-ationEU"/PRTR</rs> under grant <rs type="grantNumber">TSI-100925-2023-1</rs>; and <rs type="funder">Centro de InvestigaciÃ³n de Galicia "CITIC"</rs>, funded by the <rs type="funder">Xunta de Galicia</rs> through the collaboration agreement between the <rs type="institution">ConsellerÃ­a de Cultura, EducaciÃ³n</rs>, <rs type="funder">FormaciÃ³n Profesional e Universidades and the Galician universities</rs> for the reinforcement of the research centres of the Galician University System (CIGUS).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_T3RD5uZ">
					<idno type="grant-number">GAP (PID2022-139308OA-I00</idno>
				</org>
				<org type="funding" xml:id="_WRA36dx">
					<idno type="grant-number">MCIN/AEI/10.13039/501100011033/</idno>
				</org>
				<org type="funded-project" xml:id="_4nyCD9R">
					<idno type="grant-number">101100615</idno>
					<orgName type="project" subtype="full">SCANNER-UDC</orgName>
					<orgName type="program" subtype="full">Horizon Europe research and innovation programme (SALSA</orgName>
				</org>
				<org type="funding" xml:id="_XfJ7HMW">
					<idno type="grant-number">PID2020-113230RB-C21</idno>
				</org>
				<org type="funding" xml:id="_NSJr56v">
					<idno type="grant-number">MICIU/AEI/10.13039/501100011033</idno>
				</org>
				<org type="funding" xml:id="_3JkPH6p">
					<idno type="grant-number">ED431C 2020/11</idno>
				</org>
				<org type="funding" xml:id="_bW8GF7h">
					<idno type="grant-number">TSI-100925-2023-1</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">SpanEmo: Casting multi-label emotion classification as span-prediction</title>
		<author>
			<persName><forename type="first">Hassan</forename><surname>Alhuzali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sophia</forename><surname>Ananiadou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.eacl-main.135</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference of the European Chapter</title>
		<meeting>the 16th Conference of the European Chapter</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1573" to="1584" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mario</forename><surname>LuÄiÄ‡</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<title level="m">ViViT: A Video Vision Transformer</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations</title>
		<author>
			<persName><forename type="first">Alexei</forename><surname>Baevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henry</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Auli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Longformer: The Long-Document Transformer</title>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">SemEval-2019 task 3: EmoContext contextual emotion detection in text</title>
		<author>
			<persName><forename type="first">Ankush</forename><surname>Chatterjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kedhar</forename><surname>Nath Narahari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meghana</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Puneet</forename><surname>Agrawal</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/S19-2005</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th International Workshop on Semantic Evaluation</title>
		<meeting>the 13th International Workshop on Semantic Evaluation<address><addrLine>Minneapolis, Minnesota, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="39" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">End-to-End Emotion-Cause Pair Extraction with Graph Convolutional Net-work</title>
		<author>
			<persName><forename type="first">Ying</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenjun</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shoushan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caicong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoqiang</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.coling-main.17</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics</title>
		<meeting>the 28th International Conference on Computational Linguistics<address><addrLine>Barcelona, Spain (Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="198" to="207" />
		</imprint>
	</monogr>
	<note>International Committee on Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Alexis</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kartikay</forename><surname>Khandelwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vishrav</forename><surname>Chaudhary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Wenzek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>GuzmÃ¡n</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Unsupervised Cross-lingual Representation Learning at Scale</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">ECPE-2D: Emotion-cause pair extraction based on joint twodimensional representation, interaction and prediction</title>
		<author>
			<persName><forename type="first">Zixiang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfei</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.288</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3161" to="3170" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Beyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Weissenborn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaohua</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mostafa</forename><surname>Dehghani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Minderer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georg</forename><surname>Heigold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Houlsby</surname></persName>
		</author>
		<title level="m">An Image is Worth 16x16 Words: Transformers for Image</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note>Recognition at Scale</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep Biaffine Attention for Neural Dependency Parsing</title>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Dozat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Simpler but more accurate semantic dependency parsing</title>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Dozat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-2077</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="484" to="490" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">HCAM -Hierarchical Cross Attention Model for Multi-modal Emotion Recognition</title>
		<author>
			<persName><forename type="first">Soumya</forename><surname>Dutta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sriram</forename><surname>Ganapathy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Transition-based directed graph construction for emotion-cause pair extraction</title>
		<author>
			<persName><forename type="first">Chuang</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaofa</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiachen</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruifeng</forename><surname>Xu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.342</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3707" to="3717" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">MEISD: A multimodal multi-label emotion, intensity and sentiment dialogue dataset for emotion recognition and sentiment analysis in conversations</title>
		<author>
			<persName><forename type="first">Mauajama</forename><surname>Firdaus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hardik</forename><surname>Chauhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asif</forename><surname>Ekbal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pushpak</forename><surname>Bhattacharyya</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.coling-main.393</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics</title>
		<meeting>the 28th International Conference on Computational Linguistics<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4441" to="4453" />
		</imprint>
	</monogr>
	<note>Online. International Committee on Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">AST: Audio Spectrogram Transformer</title>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-An</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Glass</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><forename type="middle">Luc</forename><surname>Carrier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Hamner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Will</forename><surname>Cukierski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichuan</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Thaler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong-Hyun</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingbo</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chetan</forename><surname>Ramaiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fangxiang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruifan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Athanasakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Shawe-Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Milakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Radu</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marius</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cristian</forename><surname>Grozea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingjing</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Romaszko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhang</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note>Challenges in Representation Learning: A report on three machine learning contests</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Graph-based dependency parsing with graph neural networks</title>
		<author>
			<persName><forename type="first">Tao</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanbin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Man</forename><surname>Lan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1237</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2475" to="2485" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<title level="m">RoBERTa: A Robustly Optimized BERT Pretraining Approach</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Loshchilov</surname></persName>
			<affiliation>
				<orgName type="collaboration">Decoupled Weight Decay Regularization</orgName>
			</affiliation>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Hutter</surname></persName>
			<affiliation>
				<orgName type="collaboration">Decoupled Weight Decay Regularization</orgName>
			</affiliation>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Facial Expression Recognition With Visual Transformers and Attentional Selective Fusion</title>
		<author>
			<persName><forename type="first">Fuyan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shutao</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.1109/taffc.2021.3122146</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1236" to="1248" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Turning on the turbo: Fast third-order nonprojective turbo parsers</title>
		<author>
			<persName><forename type="first">AndrÃ©</forename><surname>Martins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miguel</forename><surname>Almeida</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="617" to="622" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Discriminative training and spanning tree algorithms for dependency parsing</title>
		<author>
			<persName><forename type="first">Ryan</forename><surname>Mcdonald</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
		<respStmt>
			<orgName>University of Pennsylvania</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD Thesis</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Conversation Understanding using Relational Temporal Graph Neural Networks with Auxiliary Cross-Modality Interaction</title>
		<author>
			<persName><forename type="first">Cam</forename><surname>Van</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thi</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tuan</forename><surname>Mai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Son</forename><surname>The</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dang</forename><surname>Kieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duc-Trong</forename><surname>Le</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2023.emnlp-main.937</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2023 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Singapore</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="15154" to="15167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Expanding Language-Image Pretrained Models for General Video Recognition</title>
		<author>
			<persName><forename type="first">Bolin</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Houwen</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minghao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Songyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaofeng</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianlong</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiming</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haibin</forename><surname>Ling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">MELD: A multimodal multi-party dataset for emotion recognition in conversations</title>
		<author>
			<persName><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devamanyu</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Navonil</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gautam</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1050</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="527" to="536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Robust Speech Recognition via Large-Scale Weak Supervision</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><forename type="middle">Wook</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><surname>Brockman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christine</forename><surname>Mcleavey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">You Only Look Once: Unified, Real-Time Object Detection</title>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Santosh</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Farhadi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Modeling Relational Data with Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rianne</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>Attention Is All You Need</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">2023a. Multimodal Emotion-Cause Pair Extraction in Conversations</title>
		<author>
			<persName><forename type="first">Fanfan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zixiang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaoyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfei</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1832" to="1844" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Semeval-2024 task 3: Multimodal emotion cause analysis in conversations</title>
		<author>
			<persName><forename type="first">Fanfan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heqing</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th International Workshop on Semantic Evaluation (SemEval-2024)</title>
		<meeting>the 18th International Workshop on Semantic Evaluation (SemEval-2024)<address><addrLine>Mexico City, Mexico</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="2022" to="2033" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">Wenhai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenhang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiqi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xizhou</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaowei</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lewei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongsheng</forename><surname>Li</surname></persName>
		</author>
		<title level="m">Xiaogang Wang, and Yu Qiao. 2023b. InternImage: Exploring Large-Scale Vision Foundation Models with Deformable Convolutions</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Effective inter-clause modeling for end-to-end emotioncause pair extraction</title>
		<author>
			<persName><forename type="first">Penghui</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiahao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenji</forename><surname>Mao</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.289</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3171" to="3181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Emotion-cause pair extraction: A new task to emotion analysis in texts</title>
		<author>
			<persName><forename type="first">Rui</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zixiang</forename><surname>Ding</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1096</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1003" to="1012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A facial expression-aware multimodal multitask learning framework for emotion recognition in multi-party conversations</title>
		<author>
			<persName><forename type="first">Wenjie</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfei</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shijin</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2023.acl-long.861</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 61st Annual Meeting of the Association for Computational Linguistics<address><addrLine>Toronto, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="15445" to="15459" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
