<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised Structural-Counterfactual Generation under Domain Shift</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2025-09-22">22 Sep 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Krishn</forename><forename type="middle">V</forename><surname>Kher</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Lokesh</forename><surname>Badisa</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Saksham</forename><surname>Mittal</surname></persName>
						</author>
						<author>
							<persName><forename type="first">K</forename><forename type="middle">V D</forename><surname>Sri Harsha</surname></persName>
						</author>
						<author>
							<persName><forename type="first">C</forename><forename type="middle">G</forename><surname>Sowmya</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sakethanath</forename><surname>Jagarlapudi</surname></persName>
						</author>
						<title level="a" type="main">Unsupervised Structural-Counterfactual Generation under Domain Shift</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-09-22">22 Sep 2025</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2502.12013v3[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-21T20:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Motivated by the burgeoning interest in crossdomain learning, we present a novel generative modeling challenge: generating counterfactual samples in a target domain based on factual observations from a source domain. Our approach operates within an unsupervised paradigm devoid of parallel or joint datasets, relying exclusively on distinct observational samples and causal graphs for each domain. This setting presents challenges that surpass those of conventional counterfactual generation. Central to our methodology is the disambiguation of exogenous causes into effect-intrinsic and domain-intrinsic categories. This differentiation facilitates the integration of domain-specific causal graphs into a unified joint causal graph via shared effect-intrinsic exogenous variables. We propose leveraging Neural Causal models within this joint framework to enable accurate counterfactual generation under standard identifiability assumptions. Furthermore, we introduce a novel loss function that effectively segregates effect-intrinsic from domain-intrinsic variables during model training. Given a factual observation, our framework combines the posterior distribution of effect-intrinsic variables from the source domain with the prior distribution of domain-intrinsic variables from the target domain to synthesize the desired counterfactuals, adhering to Pearl's causal hierarchy. Intriguingly, when domain shifts are restricted to alterations in causal mechanisms without accompanying covariate shifts, our training regimen parallels the resolution of a conditional optimal transport problem. Empirical evaluations on a synthetic dataset show that our framework generates counterfactuals in the target domain that very closely resemble the ground truth.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Counterfactual inference and generation are tasks that have diverse applications in a myriad of fields ranging from biol-ogy <ref type="bibr" target="#b25">(Marchezini et al., 2022;</ref><ref type="bibr" target="#b9">Feuerriegel et al., 2024;</ref><ref type="bibr" target="#b35">Qiu et al., 2020)</ref> to computer vision <ref type="bibr" target="#b31">(Pan &amp; Bareinboim, 2024;</ref><ref type="bibr" target="#b48">Yang et al., 2020;</ref><ref type="bibr" target="#b6">De Sousa Ribeiro et al., 2023;</ref><ref type="bibr" target="#b4">Dash et al., 2022)</ref> and natural language processing <ref type="bibr" target="#b15">(Jin et al., 2023;</ref><ref type="bibr" target="#b44">Wu et al., 2023;</ref><ref type="bibr" target="#b12">Hu &amp; Li, 2021;</ref><ref type="bibr">Wang et al., 2024)</ref>, etc. While some applications employ the term "counterfactual" informally <ref type="bibr" target="#b14">(Jeanneret et al., 2024;</ref><ref type="bibr" target="#b19">Lang et al., 2023)</ref>, this work adopts the formal definition of structural counterfactuals as per Pearl's hierarchy of causation <ref type="bibr" target="#b1">(Bareinboim et al., 2022;</ref><ref type="bibr" target="#b33">Pearl, 2013)</ref>. Current research on counterfactual inference typically confines itself to single domains or fixed causal models, neglecting potential domain shifts or alterations in causal mechanisms. Motivated by the growing interest in cross-domain learning, we address scenarios where the causal model experiences a general domain shift. Specifically, we allow both the causal graph and the underlying causal mechanisms to vary between domains, alongside possible covariate shifts <ref type="bibr" target="#b50">(Zhang et al., 2013)</ref>. For simplicity, our focus is on the two-domain case, though our framework readily extends to multiple domains. We consider an unsupervised setting where parallel or joint data across domains is unavailable. Instead, observational data and causal graphs are provided separately for each domain. This approach is more practical, as simultaneous observation of cause-effect relationships before and after a domain shift may be infeasible in practice. However, it introduces significant challenges compared to traditional counterfactual generation. To the best of our knowledge, counterfactual generation under such unsupervised domain shifts is an unexplored problem in the literature.</p><p>To adequately define structural counterfactuals, a joint causal graph that includes all cause &amp; effect variables across both domains is required. We construct this joint graph by distinguishing cause variables into two categories: those intrinsic to the effect variable and those intrinsic to the domain. For instance, in image generation, the effect-intrinsic variable could represent the object's identity, while the domainintrinsic variable might denote the rendering style (e.g., cartoon versus artist-drawn images <ref type="bibr" target="#b16">(Kim et al., 2020)</ref>). We integrate the two causal graphs by fusing their effect-intrinsic variables (both endogenous and exogenous), as illustrated in Figure <ref type="figure" target="#fig_0">1</ref>. Here, subscripts S and T denote the source and target domains, respectively. Exogenous variables are cate-gorized into C (effect-intrinsic) and N (domain-intrinsic), with X representing effect-intrinsic endogenous variables 1 . This fusion of C, X in the joint causal graph 2 ensures that the effects in both domains differ solely by their domainintrinsic variables 3 . Although we assume identical causal graph structures across domains for clarity of presentation, our methodology can accommodate variations in causal graph structures and interconnects between cause variables.</p><p>Consequently, we formally define our novel counterfactual generation task as follows: given a factual observation (x S , y S ) in the source domain, generate counterfactual samples in the target domain, Y T xintv←x |x S ,y S , where x intv is the intervention value for x. Essentially, this involves predicting how the effect Y would appear in the target domain after intervening on the cause x to take the value x intv , given the source observation (x S , y S ). If the causal mechanisms within the joint model are known, counterfactual inference can proceed through Pearl's three-step process <ref type="bibr" target="#b32">(Pearl, 2009)</ref>. In practice, the causal mechanisms of the joint graph are unknown and must be learned. We propose using Neural Causal Models (NCMs) <ref type="bibr" target="#b46">(Xia et al., 2021)</ref>, which are effective for classical counterfactual generation. Training a joint NCM is challenging due to the absence of joint samples. However, since the causal mechanisms remain consistent during sampling, it suffices to learn the causal mechanisms separately for the source and target domains using domain-specific observational data. Accurate identification of exogenous variables is crucial to ensure correct counterfactual inferences. To achieve this, we introduce a novel loss term that facilitates the correct identification of effect-intrinsic variables C. Following the learning of a joint NCM, we model the posterior using a neural push-forward generator trained with a Wasserstein-based loss <ref type="bibr" target="#b10">(Frogner et al., 2015;</ref><ref type="bibr" target="#b0">Arjovsky et al., 2017)</ref>. The three-step counterfactual generation process is then seamlessly implemented using the learned joint NCM and posterior model.</p><p>In scenarios where domain shifts involve only changes in causal mechanisms without covariate shifts, we show that our joint model corresponds with the optimal transport (OT) plan in a conditional OT framework <ref type="bibr" target="#b24">(Manupriya et al., 2024)</ref>. This relationship extends existing works that utilize OT for sample generation under domain shifts <ref type="bibr" target="#b17">(Korotin et al., 2023)</ref>. To validate our methodology for this novel task, we crafted source and target causal mechanisms adhering 1 For simplicity, endogenous domain-intrinsic variables are assumed absent. If present, they remain separate, similar to N .</p><p>2 Although such fusing of graphs is also done in twin networks <ref type="bibr" target="#b41">(Vlontzos et al., 2021)</ref>, both the causal graphs and mechanisms are assumed to be domain-invariant, whereas we allow both to vary. 3 The fused effect-intrinsic variables are chosen to be same as those in the source domain, because the factual is assumed to be on the source side and the counterfactual is expected on the target side. In case of the converse, the fused variables must be chosen as those in the target domain.</p><p>to standard identifiability conditions <ref type="bibr">(Nasr-Esfahany et al., 2023;</ref><ref type="bibr" target="#b28">Nasr-Esfahany &amp; Kiciman, 2023)</ref>. These mechanisms were selected to allow exact computation of the exogenous posterior, thereby precisely determining the counterfactual distribution. Our simulations demonstrate that the proposed methodology correctly estimates the shifted groundtruth counterfactual via a popular two sample test <ref type="bibr" target="#b37">(Schrab et al., 2023)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head><p>We begin by laying out certain pre-requisites in causality theory (we borrow the notation of <ref type="bibr" target="#b46">(Xia et al., 2021)</ref> in this regard) and kernels least squares loss that are pertinent to the contributions of this paper, followed by analyzing some prior work that is arguably most closely related to our contributions in this paper. In general we denote random variables by uppercase letters (W ) and their corresponding values by lowercase letters (w). We denote by D W the domain of W and by P (W = w) the probability of W taking the value w under the probability distribution P (W ). We denote by Ω(W ) the domain of values for a random variable W . Bold font on a semantic letter indicates a set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Preliminaries</head><p>Structural Causal Model An SCM M is defined as a tuple ⟨U, V, F , P (U)⟩, where U is a set of exogenous variables with distribution P (U), V the endogenous variables, and F ≡ {f 1 , . . . , f n } is a set of functions/mechanisms, where each f i :</p><formula xml:id="formula_0">U i × A i → V i , U i ⊂ U, A i ⊂ V \ {V i }.</formula><p>In simple words, V are the observed variables, U are the unobserved variables, and each mechanism (f i ) takes as input some subset of exogenous (U i ) and endogenous variables (A i ) and causes/outputs the corresponding observed variable (V i ). Given a recursive SCM, a directed graph is readily induced, in which the nodes correspond to the endogenous variables V, while the directed edges correspond to the causal mechanism from each variable in A i to V i . It is assumed that this directed graph is acyclic and as such, A i are the parents of V i in this DAG. Every causal diagram is associated with a set of L 1 constraints, which is a set of conditional independences, L 2 constraints, popularly known as the do-calculus rules for interventions and finally, L 3 constraints, rules that the counterfactual distributions satisfy. Every counterfactual distribution induced by the SCM satisfies all three levels of constraints. Using these 3-layered rules, various statistical, interventional, and counterfactual inferences can be performed in a systematic way. Notably, distributions in the lower levels of the causal hierarchy may not satisfy constraints of higher levels. Since in most applications the cause-effect relations are known rather than the SCM itself, one interesting modeling question is, given a causal graph, can we come up with a convenient model that is consistent with all the three levels of constraints induced by the graph? NCMs are one such example of a convenient family of causal models.</p><p>Definition 2.1. NCM, Defn.2 in <ref type="bibr" target="#b45">(Xia &amp; Bareinboim, 2024)</ref> Given</p><formula xml:id="formula_1">a causal diagram G, a G-constrained NCM M θ over V with parameters θ = {θ Vi : V i ∈ V} is an SCM ⟨ U, V, F, P ( U)⟩ such that (1) U = { U C : C ∈ C(G)}, where C(G) is the set of all maximal cliques over bi- directional edges of G; (2) F = { f Vi : V i ∈ V}, where each f Vi is a feedforward neural net parameterized by θ Vi ∈ θ mapping U Vi ∪ A Vi to V i for U Vi = { U C ∈ U : V i ∈ C} and A Vi = A G (V i ); (3) Unif(0, 1) → P ( U ), ∀ U ∈ U.</formula><p>Note that point 3 in Definition 2.1 above critically lies on the fact that there always exists a neural network that can transform Unif(0, 1) to an arbitrary distribution P (cf. Lemma 5 in <ref type="bibr" target="#b46">(Xia et al., 2021)</ref>). To facilitate easier learning of NCMs, we assume N (0, 1) → P ( U ), ∀ U ∈ U in this work. Note that Lemma 5 can be trivially extended to this case. <ref type="bibr" target="#b2">Brockwell, 1991)</ref>. When the kernel embeddings of Y are used instead, it is known as kernels least squares, written as:</p><formula xml:id="formula_2">Kernel Least Squares It is well known that E[Y |X] = argmin f E ∥f (X) -Y ∥ 2 (</formula><formula xml:id="formula_3">E[Φ(Y )|X] = argmin f E ∥f (X) -Φ(Y )∥ 2</formula><p>, where Φ is the canonical feature map corresponding to a kernel. In case the kernel is characteristic <ref type="bibr" target="#b39">(Sriperumbudur et al., 2011)</ref>,</p><formula xml:id="formula_4">Y → E[Φ(Y )|X] is injective and characterizes the distribution of Y .</formula><p>Common examples of characteristic kernels include the radial basis function (RBF) kernel and inverse multiquadric kernel (IMQ) kernel among others. Thus, kernel least-squares loss is well-suited for learning conditional distributions (e.g., see <ref type="bibr" target="#b24">(Manupriya et al., 2024)</ref> which provides empirical and theoretical results in this regard). We espouse the following derivation from <ref type="bibr" target="#b24">(Manupriya et al., 2024)</ref>, which shows how to learn a conditional generator using joint samples, without having to resort to Monte Carlo methods. Let D be a given dataset of samples drawn from the joint distribution of random variables P, Q and let π γ Q|P be a parametrized (by γ) conditional generator that we wish to learn. Accordingly, we wish π γ Q|P (•|p) = s Q|P (•|p), ∀p ∈ Ω(P). Utilizing the injectivity of a characteristic kernel Φ, we can equivalently rewrite the desired condition as</p><formula xml:id="formula_5">Ω(P) E π γ Q|P (•|p) [Φ(Y )] -E s Q|P (•|p) [Φ(Y )] 2 2</formula><p>ds P (p) = 0. The kernels least squares loss inside the above integral is commonly known as the squared Maximum Mean Discrepancy error, aka MMD 2 . For the rest of this paper, we take Φ to be the IMQ kernel defined as k(x, y)</p><formula xml:id="formula_6">= 1 √ ϱ+||x-y|| 2 2 ∀x, y ∈ R d ,</formula><p>where ϱ is a non-negative hyperparameter. This is because we usually observe good results with this kernel. Next, they apply a standard result kernel mean embeddings, <ref type="bibr" target="#b27">(Muandet et al., 2017)</ref>, which states that</p><formula xml:id="formula_7">E[∥G -h(P)∥ 2 ] = E[∥G -E[G|P]∥ 2 ] + E[∥E[G|P] - E[h(P)]∥ 2 ],</formula><p>when G is the kernel mean embedding of δ Q and h(P) the kernel mean embedding of π γ Q|P (•|P). This helps us simplify the integral over the marginal of P in terms of a marginal over the joint distribution of P, Q, since</p><formula xml:id="formula_8">Ω(P) MMD 2 (π γ Q|P (•|p), s Q|P (•|p))ds P (p) + ϑ(s) = Ω(P)×Ω(Q) MMD 2 (π γ Q|P (•|p), δ Q )ds P,Q (p, q)</formula><p>, where ϑ(s) ≥ 0 is purely a function of the dataset and therefore, does not affect the minima of the right hand side of the equation above.</p><p>The left hand side integral can be readily estimated empirically, as</p><formula xml:id="formula_9">1 |D| |D| i=1 1 κ κ qi∼π γ Q|P (•|pi) Φ( q i ) -Φ(q i ) 2 .</formula><p>Training the conditional generator over the dataset D thereby is tantamount to minimizing the previously mentioned empirical estimate with respect to the parameters, γ. This "trick" is repeatedly used in most of our losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Prior Work</head><p>Disentanglement Despite the vast amount of literature in disentanglement particularly in the context of causal representation learning (), there are only a few that are close to the exact setting we assume in this work. <ref type="bibr" target="#b42">(von Kügelgen et al., 2021</ref>) also studied causally disambiguating domain intrinsic exogenous variables ("style") and an effect intrinsic variables ("context"), but they assume the dataset for either domain to be sampled using the same context distribution, whereas we allow for different observed context distributions in the observational data (cf. Figure <ref type="figure" target="#fig_0">1</ref>). <ref type="bibr" target="#b26">(Morioka &amp; Hyvarinen, 2023;</ref><ref type="bibr" target="#b5">Daunhawer et al., 2023)</ref> extended the above work and theoretically analyzed the above setting, known as "multimodal" SCMs. However, they all seem to assume access to paired data across domains that share the same context, which we don't. Additionally, our kernelbased losses critically differ from their contrastive loss based approaches. Lastly, our setting for unpaired observed data is closest to the Out-Of-Variable generalization setting proposed in <ref type="bibr">(Guo et al., 2024</ref>), but we specifically solve (counterfactual) generative tasks in this setting, whereas they restrict themselves to discriminative tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Domain Shifts</head><p>Informally known by various names in the literature such as Domain Adaptation (DA) ( <ref type="bibr" target="#b7">(Farahani et al., 2021;</ref><ref type="bibr">Liu et al., 2022)</ref>), Translation ( <ref type="bibr" target="#b13">(Isola et al., 2017;</ref><ref type="bibr" target="#b20">Lin et al., 2019</ref>)), etc. The underlying idea is to train a model using a samples from one distribution, i.e. the source, and be able to predict objects belonging to another distribution, termed as the target distribution. Whilst a plethora of such works exist in this area, to the best of our knowledge, it appears that a causal angle to this problem has largely been overlooked. <ref type="bibr" target="#b23">(Magliacane et al., 2018)</ref> proposes a causal DAG modeling domain adaptation and incorporates a context variable, as we also do. <ref type="bibr" target="#b40">(Teshima et al., 2020)</ref> on the other hand delves into this approach deeper and attempts to model the DA problem via structural equation models, but relies on the assumption of having the data generative mechanism to be invariant across domains. Our methodology however, admits different priors as well as mechanisms across domains. Lastly, <ref type="bibr" target="#b51">(Zhang et al., 2015)</ref> also examine the DA problem in a simple two-variable setting and demonstrate conditions for transfer of the relevant context across domains by considering the appropriate prior and posterior distributions of the target domain. However, they constrain the causal mechanism to linear structural equations. Furthermore, none of the above works involve answering counterfactual queries in the target domain, nor deal with counterfactual generation in its rigorous form.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Problem Definition and Proposed Methodology</head><p>Consider the joint model presented in Figure <ref type="figure" target="#fig_0">1</ref>. Y S and Y T represent the effect variables in the source and target domains respectively. Each of these is causally influenced by a common set of effect-intrinsic attributes: X (endogenous), and C (exogenous). The exogenous domain-intrinsic variables N S and N T are assumed to be independent of each other and need not have the same distribution. In other words, the effect variable has the same effect-intrinsic causes both before and after the domain shift. The effect variables may hence only differ in their domain-intrinsic causes, which is meaningful. Overall, the model is assumed to be Markovian to avoid non-identifiability issues <ref type="bibr" target="#b28">((Nasr-Esfahany &amp; Kiciman, 2023;</ref><ref type="bibr">Nasr-Esfahany et al., 2023)</ref>). Other such assumptions leading to identifiability are also fine. As noted earlier, the causal mechanism generating the effect variables may change with the domain shift. Thus, we denote Y S ≡ M YS * (X S , C S , N S ) and To motivate this problem, consider the task of person reidentification in surveillance systems, which is crucial for tracking individuals across different camera views in public spaces such as airports or train stations ( <ref type="bibr" target="#b49">(Ye et al., 2022)</ref>).</p><formula xml:id="formula_10">Y T ≡ M YT * (X T , C T , N T ),</formula><p>For the sake of this illustration, we extend this to a case where a suspect needs to be tracked for migration between tourist locations of two different countries with starkly clothing traditions. Imagine that we have two sets of images: one from images of people in region S and the other from those in region D, each of them with their native attires respectively. These images may not overlap, and we do not have paired samples showing the same individual captured in both locations. The task is to generate highly probable images of the suspect in the clothing style native to region D, given the information that the suspect has blended themselves with the local clothing culture. Furthermore, due to the passage of time and local weather conditions in region D, we may assume that certain attributes such as the age, skin color, etc. have changed, that are also provided as additional information. Incorporating this additional information is however, non-trivial. With no paired training data linking the source and target images, one needs to generate a plausible counterfactual version of the person in the source image, conditioned on images representing the person in region S, along with interventions indicating a change in appearance such as age or skin color, and then translate this counterfactual image to the style of clothing adopted in region D.</p><p>We can now elucidate correspondences of different variables of interest in the formal definition of the problem described above to that setting. X S models intervenable attributes such as 'age', 'skin color', etc. and obeys the distribution of such attributes as observed in region S. Analogously, X T models the same in region T. While the attributes semantically allude to the same concepts, the distributions of these variables can differ significantly across regions due to varied reasons such as geography, climate, local culture, etc. C S models latent variables that roughly capture the identity of the person, such as their silhouette, posture, etc. These are not explicitly annotated for in the dataset, yet need to be captured to produce an accurate counterfactually translated image of the suspect. Thus conditioning on the known age/skin color of the person and their image in region S would tantamount to inducing a posterior distribution of silhouettes/postures particularly identifying the suspect.</p><p>Finally, the domain-intrinsic exogenous variables N S , N T may capture other unobserved variables such as the clothing culture in the respective regions etc.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Modeling and Training</head><p>We propose modeling causal mechanisms of interest using NCMs, because of their versatility and competence in executing counterfactually generative tasks <ref type="bibr" target="#b47">(Xia et al., 2023)</ref>. We model the true source and target causal mechanisms M YS * , M YT * using neural networks M YS θS , M YT θT with parameters θ S , θ T respectively. However, we lack explicit supervision for the exogenous variables, C S/T , N S/T . Therefore, we adopt a trick promulgated by <ref type="bibr" target="#b46">(Xia et al., 2021)</ref>, wherein we technically model wherein M YS θS , M YT θT attempt to align with the equivalent (groundtruth) causal models M YS * , M YT * , where C S/T , N S/T ∼ N (0, I d×d ).</p><p>Furthermore, we propose modeling the exogenous variables C S/T , N S/T , using push-forward neural generators, that conjoined with M YS θS , M YT θT , would mimic the structural effect of the exogenous variables on the output causal variables Y S/T .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Specifically, let C</head><formula xml:id="formula_11">S ≡ m CS# ϕ C S (η CS ), C T ≡ m CT# ϕ C T (η CT ),</formula><p>where η CS , η CT are samples from a noise distribution that is easy to sample from, e.g. Gaussian. Similarly, let</p><formula xml:id="formula_12">N S ≡ m NS# ζ N S (η NS ), N T ≡ m NT# ζ N T (η NT ).</formula><p>Note that such careful modeling of the exogenous variables is needed for two reasons: (i) we wish to disambiguate between the effectintrinsic and domain-intrinsic variables (ii) the distribution of C S need not be same as that of C T . Similarly, the distribution of N S need not be same as that of N T . Classical counterfactual models need not resort to such careful modeling and can safely assume</p><formula xml:id="formula_13">m CS/T# ϕ C S/T (η CS/T ), m NS/T# ζ N S/T (η NS/T )</formula><p>are some hidden sub-networks in M YS θS , M YT θT itself. The main reason why training the described joint model is challenging is because it is not practical to assume that the joint samples are available. As mentioned earlier, we make a more practical assumption that cause-effect samples from the source and target models are separately available. Consequently, let (x S 1 , y S 1 ), . . . , (x S m1 , y S m1 ) denote the observational data in the source domain and let (x T 1 , y T 1 ), . . . , (x T m2 , y T m2 ) denote the observational data in the target domain. Our losses significantly differ from <ref type="bibr" target="#b46">(Xia et al., 2021)</ref> in that we mostly train our models via a kernels least squares loss between samples of appropriate distributions whereas they train using negative log-likelihood. Since the causal mechanisms are independent of the sampling procedure, the mechanisms for generating Y S , Y T in the source, target-specific NCMs and in the joint NCM are the same. Hence, the source and target observational data can be used to train the networks in the joint model to be L 1 consistent with the true mechanisms. For instance, on the source side the goal is to learn</p><formula xml:id="formula_14">M YS θS (X S , •, •) such that P M Y S θ S (X S , •, •) = P M Y S * θ S (X S , •, •).</formula><p>To this end, we critically employ the fact that any characteristic kernel Φ induces an injective map over distributions, stated in Section 2.1, as follows:</p><formula xml:id="formula_15">P M Y S θ S (X S , •, •) = P M Y S * (X S , •, •) ⇔ P M Y S θ S (X S = x, •, •) = P M Y S * (X S = x, •, •) , ∀x ∈ Ω(X S ) ⇔ ∀x ∈ Ω(X S ), E YS∼P M Y S θ S (XS=x,•,•) [Φ(Y S )] = E YS∼P M Y S * (XS=x,•,•) [Φ(Y S )].</formula><p>(1) Consequently, we aim to learn a conditional generator</p><formula xml:id="formula_16">M YS θS , using joint samples {(x S i , y S i )} n i=1</formula><p>. This is our preferred loss to train over observational data due to its demonstrated superior performance compared to other traditional losses such as adversarial/KL/Wasserstein losses <ref type="bibr" target="#b24">(Manupriya et al., 2024)</ref>. In particular, we use the MMD 2 loss between the sample kernel means of the conditional distributions</p><formula xml:id="formula_17">P M Y S θ S ( C S , N S |X S = x) and P M Y S * ( C S , N S |X S = x)</formula><p>as follows:</p><formula xml:id="formula_18">ℓ S gen (θ S , ϕ CS , ζ NS ) = 1 n S gen n S gen i=1 Φ(y S i )- 1 q S gen q S gen j=1 Φ M YS θS x S i , m CS# ϕ C S η CS ij , m NS# ζ N S η NS ij 2 2 .</formula><p>(2) Here, n S gen is the number of samples within the training set (or within a mini-batch, if using an optimization algorithm such as mini-batch SGD) and q S gen the number of noise samples η CS/NS ij sampled from N (0, I d×d ) per data point (x S i , y S i ), used to empirically approximate the kernel mean of Y S when in turn sampled from</p><formula xml:id="formula_19">P M Y S θ S ( C S , N S |X S = x).</formula><p>For the sake of completeness, we also note that alternatives from conditional generative adversarial networks based literature may also be viable to learn this conditional generator, as we do not claim the above loss as a novel contribution of our work. A similar loss is also introduced to train the target mechanisms as shown below:</p><formula xml:id="formula_20">ℓ T gen (θ T , ϕ CT , ζ NT ) = 1 n T gen n T gen i=1 Φ(y T i )- 1 q T gen q T gen j=1 Φ M YT θT x T i , m CT# ϕ C T η CT ij , m NT# ζ N T η NT ij 2 2 .</formula><p>(3)</p><p>We now make an important observation: although M YT θT is trained using target domain data, when it is employed in the joint model, the distribution of its inputs is according to the source domain. This is a classical problem of covariate shift and can be easily tackled using state-of-the-art techniques that account for this shift ( <ref type="bibr" target="#b8">(Fatras et al., 2021;</ref><ref type="bibr" target="#b30">Nguyen et al., 2022;</ref><ref type="bibr" target="#b24">Manupriya et al., 2024)</ref>). The essential idea in such methods is to solve an OT problem between the domains and employ the obtained weights while computing the least square loss. This is typically written as a corrective loss term, ℓ cs (θ T , ϕ T , ϕ S , ζ T ) (e.g., objective (6) in ( <ref type="bibr" target="#b3">(Damodaran et al., 2018)</ref>)). Thus, the overall loss for training the mechanisms on the target side is</p><formula xml:id="formula_21">ℓ T gen (θ T , ϕ T , ϕ S , ζ T ) ≡ ℓ T gen (θ T , ϕ T , ζ T ) + ℓ cs (θ T , ϕ T , ϕ S , ζ T ).</formula><p>Finally, the effect-intrinsic, domain-intrinsic variables, C, N , need to be disambiguated correctly. This cannot be done if the training of the networks happen separately using the corresponding losses. For example, if identities of C, N are interchanged, the domain-specific networks still remain the same, with an appropriate permutation of input variables. However, counterfactual inference may catastrophically fail if C, N are interchanged. Hence, we propose to jointly train the NCMs with an additional loss term that helps disambiguate the variables. To this end, we assume a cost function, dis, is available that can perceive effect-intrinsic differences in the variables and ignores the domain-intrinsic differences. For example, in the case of image generation, the cost can be taken as the L pips loss <ref type="bibr" target="#b52">(Zhang et al., 2018)</ref> or as squared Euclidean distance computed with image embeddings from a pre-trained network like AlexNet <ref type="bibr" target="#b18">(Krizhevsky et al., 2012)</ref>, VGG <ref type="bibr" target="#b38">(Simonyan &amp; Zisserman, 2015)</ref>, Swin-T <ref type="bibr" target="#b22">(Liu et al., 2021)</ref>, etc. Such a cost will be high if, say the identity of the object depicted in the images is not the same, and low even if, say the style of rendering of the images is different and vice-versa. Our novel loss term (ℓ tr ) for disambiguating the variables is:</p><formula xml:id="formula_22">ntr i=1 qtr j=1 dis M YS θS x S i , m CS# ϕ C S η CS ij , m NS# ζ N S η NS ij , M YT θT x S i , m CS# ϕ C S η CS ij , m NT# ζ N T η NT ij = ℓ tr • (nq) tr</formula><p>(4) Note that the effect-intrinsic variables are the same in the above in both the mechanisms, whereas the domain-intrinsic variables are different. Hence, for the correct C, N pairs, this term will be low and vice-versa. In particular, if C, N are interchanged, then this term will be high. Thus, this acts as a loss function to correctly disambiguate the exogenous variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Inference</head><p>Given the learned joint NCM, counterfactual generation can be done using the standard three-step procedure: abduction, action, prediction. For the abduction step, we need samples from the posterior p(c S |x S , y S ), where (x S , y S ) is the cause-effect pair observed in the source domain. To this end, we propose modeling the posterior, p(c S , n S |x S , y S ), using a pushforward neural network generator, g ψ (x S , y S , N # )<ref type="foot" target="#foot_0">foot_0</ref> . From the Bayes rule, we have: p(c S , n S |x S , y S )p(x S , y S ) = p(x S )p(c S )p(n S )p(y S |x S , c S , n S ), where the LHS is the factorization corresponding to the source causal graph and the RHS is the factorization involving the posterior to be estimated. Let S θS,ϕS,ζS ≡ Υ, M YS θS (Υ) , be a set of samples from the source NCM, where</p><formula xml:id="formula_23">Υ ≡ x S i , m CS# ϕ C S (η CS ij ), m NS# ζ N S (η NS ij ) and η (C/N ) S ij are independent samples of N (0, I d×d ). Let P ζ N S ,ψ ≡ x S i , g ψ x S i , y S i , η # ij , m NS# ζ N S (η NS ij ), y S i , i.e. samples</formula><p>from the posterior, where η NS ij , η # ij are independent samples from N (0, I d×d ). With this notation, the loss for training g ψ turns out to be:</p><formula xml:id="formula_24">ℓ pos (θ S , ϕ S , ζ S , ψ) ≡ W(S θS,ϕS,ζS , P ζ N S ,ψ ),<label>(5)</label></formula><p>where, W is an appropriate distance between between measures like the Wasserstein metric ( <ref type="bibr" target="#b10">(Frogner et al., 2015)</ref>).</p><p>The training of the inference network can either be done jointly with the NCMs, in which case the optimization problem is:</p><formula xml:id="formula_25">min θS,ϕS,ζS,θT,ϕT,ζT,ψ ℓ S gen + ℓ T gen + ℓ tr + ℓ pos<label>(6)</label></formula><p>or, can be trained separately in which case the optimization problems are:</p><formula xml:id="formula_26">(θ * S , ϕ * S , ζ * S , θ * T , ϕ * T , ζ * T ) ≡ argmin ℓ S gen + ℓ T gen + ℓ tr , ψ * ≡ argmin ψ ℓ pos (θ * S , ϕ * S , γ * S , ψ)<label>(7)</label></formula><p>Once the optimal network parameters are obtained by solving the appropriate optimization problems using solvers like AdamW, counterfactual samples in the target domain can be produced by:</p><p>1. Sampling from posterior using g ψ * (x s , y s , N # ). Different n # samples from the reference Gaussian will produce different samples from the posterior.</p><p>2. Counterfactual samples are obtained using</p><formula xml:id="formula_27">M YT θ * T x intv , g ψ x S , y S , η # ij , m NT# ζ N T η NT ij .</formula><p>Different η #/NT ∼ N (0, I d×d ) samples produce different samples from the counterfactual distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Connection to Conditional Optimal Transport</head><p>Interestingly, in the special case where the distributions of the exogenous variables are the same in the source and the target domains 5 , ( <ref type="formula" target="#formula_26">7</ref> </p><formula xml:id="formula_28">) Θ(x S , η CS )p ϕS (η)p S (x S )d(n CS , x S ) = E X S ,C S E d(Y S , Y ′ T )|X S , C S ,<label>(8)</label></formula><p>where the total expectation is w.r.t. the joint causal model being learned 6 . Also, the source side marginal of this joint conditioned on x S is given by p θS,ϕS (y S |x S , η)p ϕS (η)dη.</p><p>The kernel least squares term in ℓ S gen matches this marginal to the distribution of Y S |x S in the source dataset. Likewise, the ℓ T gen can be understood as a term for matching the target side conditional to Y T |x S . Moreover, if the distributions of the exogenous variables are the same, there is no covariate shift and hence ℓ T gen is same as ℓ T gen in ( <ref type="formula" target="#formula_26">7</ref>). In such a case, (7) becomes same as (6) in <ref type="bibr" target="#b24">(Manupriya et al., 2024)</ref> (apart from the parametrization of the transport plan) and hence can be interpreted as a conditional OT problem between p(y S |x S ) and p(y T |x S ). Thus this connection seems to generalize the idea of employing OT based methods for applications like image-to-image translation <ref type="bibr" target="#b17">(Korotin et al., 2023)</ref>, which is an example of generative modeling under domain shift (without any reference to causality). To summarize, in the special case where the domain shift is purely a conditional shift (change in causal mechanism) without any covariate shift, our model learned is essentially an OT plan corresponding to the conditional OT problem between p(y S |x S ) and p(y T |x S ). Nevertheless, even in this special case, our parametrization of the transport plan (joint) in terms of the effect-intrinsic variable C S is crucial for counterfactual generation.</p><p>5 CS ∼ CT , NS ∼ NT . 6 Θ(x S , η CS ) is the first integral.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Simulations</head><p>We present the details of the synthetic dataset on which we test our methodology. Due to the lack of an existing baseline tailored for this novel task, we introduce the following benchmark dataset that obeys the causal graph depicted in Figure <ref type="figure" target="#fig_0">1</ref>, with structural equations that are described below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dataset</head><p>The SCMs of the source and target domains consist of X, C, N variables that are of dimension d × 1. The corresponding effect variables in both domains i.e., Y are of dimension 2d × 1, while their structural equations read as:</p><formula xml:id="formula_29">Y S = A S e CS /2 (C S ⊙ N S ) + X S ,<label>(9)</label></formula><formula xml:id="formula_30">Y T = (C T ⊙ N 2 T ) + X T A T e CT , (<label>10</label></formula><formula xml:id="formula_31">)</formula><p>where A S/T = B T B + 5I d×d , and B S/T = X S/T (2 XS/T ) T . In this context, the symbol ⊙ denotes a Hadamard product, and 'T' in the superscript refers to the transpose operation over a tensor. Likewise, the notation of the form a X , X b for a, b ∈ R denote exponentiation to the base a, raising to the power of b for each individual element of the vector X ∈ R d , respectively. The priors for the causal variables of the source &amp; target domains are:</p><formula xml:id="formula_32">X S ∼ Uniform(-1, 1), X T ∼ ContinuousBernoulli(0.6), C S ∼ Beta(4, 5), C T ∼ Normal(0, I d×d ),</formula><p>N S ∼ vonMises(0, 4), N T ∼ Normal(0, 0.1 • I d×d ).</p><p>(11) Infact, we pick C S , X T to be an affine transforms of C S , X T respectively, as</p><formula xml:id="formula_33">C S = 1 d×1 -(2• C S ), X T = 1 d×1 -(2• X T ).</formula><p>For the target domain, we explicitly choose different priors so as to establish a strong benchmark that does not utilize spurious correlations across variables and domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1.">COMPUTING SHIFTED COUNTERFACTUALS</head><p>A key property of the proposed dataset is that the sourceside mechanism is invertible, given the values of X S , Y S . This is in line with invertibility assumptions often made in the SCM community <ref type="bibr">(Nasr-Esfahany et al., 2023;</ref><ref type="bibr">Poinsot et al., 2024)</ref> Notice that by design, A S/T is always positive definite and hence invertible, so such an inverse is always defined. Using this C S we can then compute N S = (Y S [d:] -X S )/C S . Thus for each triplet (x intv , x fact , y fact ), there exists a unique counterfactual for that in the source domain. However, this is not the case in the target domain. Thought C T is exactly retrievable given X T , Y T , N T is not, since there are at least two solutions for it given X T , Y T and C T , namely N T = ± (Y T [d:] -X T )/C T . This illustrates that the distribution of shifted counterfactuals is crucially dependent on the target side mechanism. Thus, to generate shifted counterfactual groundtruth samples, we follow the procedure outlined below:</p><p>1. Sample x T intv for the target domain from any distribution with identical support as the prior of X T . For simplicity, we may choose this distribution to be the prior of X T itself.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Sample x S</head><p>fact , c S , n S from the priors of X S , C S , N S respectively. Generate the corresponding y S fact using Equation <ref type="formula" target="#formula_29">9</ref>. Accordingly we randomly select a pair (x S fact , y S fact ). 3. Using the invertibility of Equation <ref type="formula" target="#formula_29">9</ref>, compute c S fact for the chosen (x S fact , y S fact ) pair. 4. Sample n T from the prior of N T and then plug x T intv , c S fact , n T into Equation 10 to generate samples from the distribution of shifted counterfactuals.</p><p>The distribution of shifted counterfactuals considered above is as conditioned on a particular triplet (x T intv , x S fact , y S fact ). Sampling multiple such triplets and generating corresponding shifted counterfactuals then corresponds to generating joint samples of shifted counterfactuals paired with their causal variables of interest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results</head><p>We test our proposed methodology on the above specified dataset with d = 1, as this is sufficient to demonstrate the generation of shifted counterfactuals. When training on this dataset as per Equation <ref type="formula" target="#formula_25">6</ref>, we simply instantiate the following MSE loss for ℓ tr (cf. Equation <ref type="formula">4</ref>):</p><formula xml:id="formula_34">1 mq m i=1 q j=1 M YS θS •, ϕ S , ζ S [0] - 1 2 M YS θT (•, ϕ S , ζ T ) [1] 2 2 .</formula><p>(12) This choice nicely fits in with the intention of disambiguating the latent context from the noise across domains. To see this, notice that the synthetic dataset proposed above bears the property that if X S = X T and C S = C T , then Y S = YT 2 ; thus C S/T satisfies an invariant across domains which N S/T does NOT satisfy, which in turn helps disentangle the role of C S when performing shifted counterfactual inference.</p><p>To evaluate the quality of our shifted counterfactuals, we run a two sample test <ref type="bibr" target="#b37">(Schrab et al., 2023)</ref> between samples from the estimated joint distribution of factuals and shifted counterfactuals, and the corresponding groundtruth distribution. In particular, given a pair (x S i , x intvi ), we randomly sample 100 triplets m CS# are sampled by plugging in x S i in the estimated /groundtruth equations and varying the context and noise from the source domain appropriately. Finally, we average the Agg-MMD two-sample test scores between samples of these distributions over 500 pairs of (x S i , x intvi ) and got a 70% score with 95% confidence, proving the efficacy of our approach<ref type="foot" target="#foot_1">foot_1</ref> .</p><p>We adopt the two-stage training process as explained in Equations 7. This is done to ensure that the posterior network learns to be consistent with the distribution of samples induced from the trained generator network. The hyperparameters for controlling individual loss terms are to be picked via cross-validation. Details on the architectures of the generator, context and noise networks are provided in the Appendix A.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Summary and Conclusions</head><p>In this work, we introduced the novel task of structuralcounterfactual generation under general domain-shift and studied it in the practical setting where no parallel/joint data is available. It turned out that modeling and training are considerably more challenging than in classical counterfactual generation tasks. In terms of modeling, the crucial step was to separately model the effect-intrinsic and domainintrinsic variables, so that the structural-counterfactual is well-defined. In terms of training, disambiguating the effectintrinsic and domain-intrinsic exogenous variables to correctly learn their distributions was the challenging step. From the trained model, the procedure for sampling counterfactuals in the target domain was presented. Connections to the optimal transport problem provided more insight into the proposed methodology.</p><p>In future, we would like to explore vision, biology applications of the novel task studied in this paper. Using the connections to OT and corresponding barycenter problems, we plan to extend structural-counterfactual generation to settings of domain interpolation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Impact Statement</head><p>This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration explaining the creation of the joint causal graph from individual graphs. Shaded nodes indicate observed variables. Y ′ T differs from Y T in the prior for C: they are abducted from the source, target domains respectively.</figDesc><graphic coords="3,320.86,72.04,194.40,92.82" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>. Let Y S [:d], Y S [d:] denote the first and last d elements of Y S ∈ R 2d respectively. Then, given X S , Y S , we can compute C S = A -1 S Y S [:d].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>ij ; M YT * x intvi , c S|(x S i ,y S ij ) , N T . Here y S ij / y S ij</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>following the given source and target causal graphs. We now formally define our novel generation problem: Problem 3.1 (Counterfactual Translation). Let (x, y) be a given sample from the joint distribution P(X S , Y S ) induced by the source causal mechanism M YS * (•, •, •). Let C S|(x,y) denote a random variable obeying the posterior distribution induced over C S , by conditioning on (x, y) in the source mechanism M YS * (•, •, •). Given a value x intv , the task is to generate samples corresponding to the distribution induced by M YT * (x intv , C S|(x,y) , N T ).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>) can be interpreted as a conditional OT problem. Let p θS,ϕS (y S |x S , η CS ), p θT,ϕS (y S |x S , η CS ) denote the distributions induced by the left &amp; right arguments to dist in (7) respectively. Furthermore, let p ϕS (η) denote the distribution induced by m CS# ϕ C S . Let p S (x S ) denote the distribution of X S . With this notation, it is easy to see that ℓ tr is the sample mean corresponding to: p θS,ϕS (y S |x S , η CS )p θT,ϕS (y S |x S , η CS )d(y S , y T</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_0"><p>N # is the pushforward noise.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_1"><p>We will provide code upon acceptance.</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A. Appendix <ref type="bibr">A.1. Training</ref> We use symmetric design for our work i.e. we use same architecture for both source side networks and target side networks. We use 5 hidden layers for f θ . For context network h ϕ , we use single hidden layer. For our noise networks, t γ , we use 3 hidden layers. For every hidden layer, hidden dimension is 128 and every linear layer(except the last layer) is followed by PReLU <ref type="bibr" target="#b11">(He et al., 2015)</ref>. PReLU is initialized with 0.25 for f θ , and with 0.01 for both the context and noise networks. We used skip-connections in the context network and noise network, since we observed better empirical performance using them.</p><p>We train with a dataset size of 25000. We use AdamW optimizer in stage 1 training and Adam in stage 2 training. We use linear increase followed by cosine decrease learning rate scheduler. We set the kernel hyperparameter ϱ to 1.0 throughout.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Wasserstein generative adversarial networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v70/arjovsky17a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Precup</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</editor>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017-08">Aug 2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="6" to="11" />
		</imprint>
	</monogr>
	<note>PMLR</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">On Pearl&apos;s Hierarchy and the Foundations of Causal Inference</title>
		<author>
			<persName><forename type="first">E</forename><surname>Bareinboim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Correa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Ibeling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Icard</surname></persName>
		</author>
		<idno type="DOI">10.1145/3501714.3501743</idno>
		<ptr target="https://doi.org/10.1145/3501714.3501743" />
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>Association for Computing Machinery</publisher>
			<biblScope unit="page" from="507" to="556" />
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
	<note>1 edition</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Time series: Theory and methods</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Brockwell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991">1991</date>
			<publisher>Springer-Verlag</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deepjdot: Deep joint distribution optimal transport for unsupervised domain adaptation</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">B</forename><surname>Damodaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kellenberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Flamary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tuia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Courty</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-01225-0_28</idno>
		<idno>978-3-030-01225-0_28</idno>
		<ptr target="https://doi.org/10.1007/" />
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2018: 15th European Conference</title>
		<meeting><address><addrLine>Munich, Germany; Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2018">September 8-14, 2018. 2018</date>
			<biblScope unit="page" from="467" to="483" />
		</imprint>
	</monogr>
	<note>Proceedings, Part IV</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Evaluating and Mitigating Bias in Image Classifiers: A Causal Perspective Using Counterfactuals</title>
		<author>
			<persName><forename type="first">S</forename><surname>Dash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">N</forename><surname>Balasubramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<idno type="DOI">10.1109/WACV51458.2022.00393</idno>
		<ptr target="https://doi.ieeecomputersociety.org/10.1109/WACV51458.2022.00393" />
	</analytic>
	<monogr>
		<title level="m">2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</title>
		<meeting><address><addrLine>Los Alamitos, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022-01">January 2022</date>
			<biblScope unit="page" from="3879" to="3888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Identifiability results for multimodal contrastive learning</title>
		<author>
			<persName><forename type="first">I</forename><surname>Daunhawer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bizeul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Palumbo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Marx</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Vogt</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=U_2kuqoTcB" />
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">High fidelity image counterfactuals with probabilistic causal models</title>
		<author>
			<persName><forename type="first">De</forename><surname>Sousa Ribeiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Monteiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Pawlowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Glocker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="https://proceedings.mlr.press/v202/de-sousa-ribeiro23a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Krause</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Brunskill</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Engelhardt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Sabato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Scarlett</surname></persName>
		</editor>
		<meeting>the 40th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2023-07">Jul 2023</date>
			<biblScope unit="volume">202</biblScope>
			<biblScope unit="page" from="23" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A brief review of domain adaptation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Farahani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Voghoei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Rasheed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Arabnia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Data Science and Information Engineering</title>
		<editor>
			<persName><forename type="first">R</forename><surname>Stahlbock</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><forename type="middle">M</forename><surname>Weiss</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Abou-Nasr</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C.-Y</forename><surname>Yang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><forename type="middle">R</forename><surname>Arabnia</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Deligiannidis</surname></persName>
		</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="877" to="894" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Unbalanced minibatch optimal transport; applications to domain adaptation</title>
		<author>
			<persName><forename type="first">K</forename><surname>Fatras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Séjourné</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Courty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Flamary</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th International Conference on Machine Learning</title>
		<meeting>the 38th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Causal machine learning for predicting treatment outcomes</title>
		<author>
			<persName><forename type="first">S</forename><surname>Feuerriegel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Frauen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Melnychuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schweisthal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Hess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Curth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kilbertus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Kohane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Van Der Schaar</surname></persName>
		</author>
		<idno type="DOI">10.1038/s41591-024-02902-1</idno>
		<ptr target="http://dx.doi.org/10.1038/s41591-024-02902-1" />
	</analytic>
	<monogr>
		<title level="j">Nature Medicine</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="958" to="968" />
			<date type="published" when="2024-04">April 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Out-ofvariable generalisation for discriminative models</title>
		<author>
			<persName><forename type="first">C</forename><surname>Frogner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Mobahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Araya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">A</forename><surname>Poggio</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=zwMfg9PfPs" />
	</analytic>
	<monogr>
		<title level="m">The Twelfth International Conference on Learning Representations</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Guo</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Wildberger</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015">2015. 2024</date>
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
	<note>Advances in Neural Information Processing Systems</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2015.123</idno>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A causal lens for controllable text generation</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Li</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=kAm9By0R5ME" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Dauphin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Vaughan</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Image-toimage translation with conditional adversarial networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.632</idno>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5967" to="5976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Text-to-image models for counterfactual explanations: a black-box approach</title>
		<author>
			<persName><forename type="first">G</forename><surname>Jeanneret</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jurie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</title>
		<meeting>the IEEE/CVF Winter Conference on Applications of Computer Vision</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="4757" to="4767" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">CLadder: A benchmark to assess causal reasoning capabilities of language models</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Leeb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gresele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Kamal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Blin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">G</forename><surname>Adauto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kleiman-Weiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sachan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=e2wtjx0Yqu" />
	</analytic>
	<monogr>
		<title level="m">Thirty-seventh Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">U-gat-it: Unsupervised generative attentional networks with adaptive layer-instance normalization for image-to-image translation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=BJlZ5ySKPH" />
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Neural optimal transport</title>
		<author>
			<persName><forename type="first">A</forename><surname>Korotin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Selikhanovych</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Burnaev</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=d8CBRlWNkqH" />
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">F</forename><surname>Pereira</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Burges</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">25</biblScope>
		</imprint>
	</monogr>
	<note>d3b9d6b76c8436e924a68c45b-Paper. pdf</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Explaining counterfactual images</title>
		<author>
			<persName><forename type="first">O</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Traynis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="https://rdcu.be/dwVKK" />
	</analytic>
	<monogr>
		<title level="j">Nature Biomedical Engineering</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multi-domain image-to-image translation via relative attributes</title>
		<author>
			<persName><forename type="first">Y.-J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P.-W</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-H</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S.-W</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><surname>Relgan</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV.2019.00601</idno>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5913" to="5921" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep unsupervised domain adaptation: A review of recent advances and perspectives</title>
		<author>
			<persName><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Fakhri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J.-W</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Woo</surname></persName>
		</author>
		<idno type="DOI">10.1561/116.00000192</idno>
	</analytic>
	<monogr>
		<title level="j">APSIPA Transactions on Signal and Information Processing</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Swin transformer: Hierarchical vision transformer using shifted windows</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCV48922.2021.00986</idno>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="9992" to="10002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Domain adaptation by using causal inference to predict invariant conditional distributions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Magliacane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Van Ommen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Claassen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bongers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Versteeg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Mooij</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper_files/paper/2018/file/39" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Grauman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Cesa-Bianchi</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
	<note>e98420b5e98bfbdc8a619bef7b8f61-Paper. pdf</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Consistent optimal transport with empirical conditional measures</title>
		<author>
			<persName><forename type="first">P</forename><surname>Manupriya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Biswas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jagarlapudi</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="https://proceedings.mlr.press/v238/manupriya24a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 27th International Conference on Artificial Intelligence and Statistics</title>
		<title level="s">Proceedings of Machine Learning Research</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Dasgupta</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Mandt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</editor>
		<meeting>The 27th International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2024-05">May 2024</date>
			<biblScope unit="volume">238</biblScope>
			<biblScope unit="page" from="2" to="04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Counterfactual inference with latent variable and its application in mental health care</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">F</forename><surname>Marchezini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Lacerda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">L</forename><surname>Pappa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Meira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Miranda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Romano-Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">M</forename><surname>Diniz</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10618-021-00818-9</idno>
		<ptr target="https://doi.org/10.1007/s10618-021-00818-9" />
	</analytic>
	<monogr>
		<title level="j">Data Min. Knowl. Discov</title>
		<idno type="ISSN">1384- 5810</idno>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="811" to="840" />
			<date type="published" when="2022-03">March 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Connectivity-contrastive learning: Combining causal discovery and representation learning for multimodal data</title>
		<author>
			<persName><forename type="first">H</forename><surname>Morioka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvarinen</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="https://proceedings.mlr.press/v206/morioka23a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 26th International Conference on Artificial Intelligence and Statistics</title>
		<editor>
			<persName><forename type="first">F</forename><surname>Ruiz</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Dy</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J.-W</forename><surname>Van De Meent</surname></persName>
		</editor>
		<meeting>The 26th International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2023-04">Apr 2023</date>
			<biblScope unit="volume">206</biblScope>
			<biblScope unit="page" from="25" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Kernel mean embedding of distributions: A review and beyond</title>
		<author>
			<persName><forename type="first">K</forename><surname>Muandet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fukumizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Sriperumbudur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<idno type="DOI">10.1561/2200000060</idno>
		<ptr target="http://dx.doi.org/10.1561/2200000060" />
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends® in Machine Learning</title>
		<idno type="ISSN">1935-8245</idno>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="1" to="141" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Counterfactual (non-)identifiability of learned structural causal models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nasr-Esfahany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Kiciman</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2301.09031" />
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Counterfactual identifiability of bijective causal models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Nasr-Esfahany</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Alizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Shah</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="https://proceedings.mlr.press/v202/nasr-esfahany23a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International Conference on Machine Learning</title>
		<title level="s">Proceedings of Machine Learning Research</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Krause</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Brunskill</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">B</forename><surname>Engelhardt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Sabato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Scarlett</surname></persName>
		</editor>
		<meeting>the 40th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2023-07">Jul 2023</date>
			<biblScope unit="volume">202</biblScope>
			<biblScope unit="page" from="23" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">On transportation of mini-batches: A hierarchical approach</title>
		<author>
			<persName><forename type="first">K</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">D</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Phung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Ho</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v162/nguyen22d.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 39th International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">K</forename><surname>Chaudhuri</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Szepesvari</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Niu</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Sabato</surname></persName>
		</editor>
		<meeting>the 39th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2022-07">Jul 2022</date>
			<biblScope unit="volume">162</biblScope>
			<biblScope unit="page" from="17" to="23" />
		</imprint>
	</monogr>
	<note>PMLR</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Counterfactual image editing</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bareinboim</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=OXzkw7vFIO" />
	</analytic>
	<monogr>
		<title level="m">Forty-first International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Causality: Models, Reasoning and Inference</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Cambridge University Press</publisher>
			<pubPlace>USA</pubPlace>
		</imprint>
	</monogr>
	<note>2nd edition. ISBN 052189560X</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Structural counterfactuals: A brief introduction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<idno type="DOI">10.1111/cogs.12065</idno>
		<imprint>
			<date type="published" when="2013-08">August 2013</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="977" to="985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning structural causal models through deep generative models: Methods, guarantees, and challenges</title>
		<author>
			<persName><forename type="first">A</forename><surname>Poinsot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Leite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Chesneau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sebag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schoenauer</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2024/907</idno>
		<ptr target="https://doi.org/10.24963/ijcai.2024/907.Sur-veyTrack" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence, IJCAI-24</title>
		<editor>
			<persName><forename type="first">K</forename><surname>Larson</surname></persName>
		</editor>
		<meeting>the Thirty-Third International Joint Conference on Artificial Intelligence, IJCAI-24</meeting>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">2024</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Inferring causal gene regulatory networks from coupled single-cell expression dynamics using scribe</title>
		<author>
			<persName><forename type="first">X</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rahimzamani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Durham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Mcfaline-Figueroa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Saunders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Trapnell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kannan</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cels.2020.02</idno>
		<ptr target="https://doi.org/10.1016/j.cels.2020.02" />
	</analytic>
	<monogr>
		<title level="j">Cell Systems</title>
		<idno type="ISSN">2405- 4712</idno>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="265" to="274" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<ptr target="https://www.sciencedirect.com/science/article/pii/S2405471220300363" />
		<title level="m">URL</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">MMD aggregated two-sample test</title>
		<author>
			<persName><forename type="first">A</forename><surname>Schrab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Guedj</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<ptr target="http://jmlr.org/papers/v24/21-1289.html" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">194</biblScope>
			<biblScope unit="page" from="1" to="81" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1409.1556" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Universality, characteristic kernels and rkhs embedding of measures</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">K</forename><surname>Sriperumbudur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fukumizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Lanckriet</surname></persName>
		</author>
		<ptr target="http://jmlr.org/papers/v12/sriperumbudur11a.html" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">70</biblScope>
			<biblScope unit="page" from="2389" to="2410" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Few-shot domain adaptation by causal mechanism transfer</title>
		<author>
			<persName><forename type="first">T</forename><surname>Teshima</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sugiyama</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning, ICML&apos;20</title>
		<meeting>the 37th International Conference on Machine Learning, ICML&apos;20</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Estimating categorical counterfactuals via deep twin networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vlontzos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kainz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gilligan-Lee</forename></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename></persName>
		</author>
		<ptr target="https://api.semanticscholar.org/CorpusID" />
	</analytic>
	<monogr>
		<title level="j">Nature Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">248986851</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Selfsupervised learning with data augmentations provably isolates content from style</title>
		<author>
			<persName><forename type="first">J</forename><surname>Von Kügelgen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Gresele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Brendel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Besserve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Locatello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th International Conference on Neural Information Processing Systems, NIPS &apos;21</title>
		<meeting>the 35th International Conference on Neural Information Processing Systems, NIPS &apos;21<address><addrLine>Red Hook, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Beyond what if: Advancing counterfactual text generation with structural causal modeling</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Du</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2024/721</idno>
		<ptr target="https://doi.org/10.24963/ijcai.2024/721.MainTrack" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Third International Joint Conference on Artificial Intelligence, IJCAI-24</title>
		<editor>
			<persName><forename type="first">K</forename><surname>Larson</surname></persName>
		</editor>
		<meeting>the Thirty-Third International Joint Conference on Artificial Intelligence, IJCAI-24</meeting>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">2024</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Interpretability at scale: Identifying causal mechanisms in alpaca</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Icard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Potts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goodman</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=nRfClnMhVX" />
	</analytic>
	<monogr>
		<title level="m">Thirty-seventh Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Neural causal abstractions</title>
		<author>
			<persName><forename type="first">K</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bareinboim</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v38i18.30044</idno>
		<ptr target="https://ojs.aaai.org/index.php/AAAI/article/view/30044" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2024-03">Mar. 2024</date>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="20585" to="20595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">The causal-neural connection: Expressiveness, learnability, and inference</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K.-Z</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bareinboim</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=hGmrNwR8qQP" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Dauphin</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Liang</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Vaughan</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Neural causal models for counterfactual identification and estimation</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bareinboim</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=vouQcZS8KfW" />
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Disentangled representation learning via neural structural causal models</title>
		<author>
			<persName><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Causalvae</surname></persName>
		</author>
		<ptr target="https://api.semanticscholar.org/CorpusID" />
	</analytic>
	<monogr>
		<title level="m">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020">2021. 2020</date>
			<biblScope unit="page">220280826</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Deep Learning for Person Re-Identification: A Survey and Outlook</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C H</forename><surname>Hoi</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2021.3054775</idno>
		<ptr target="https://doi.ieeecomputersociety.org/10.1109/TPAMI.2021.3054775" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis &amp; Machine Intelligence</title>
		<idno type="ISSN">1939-3539</idno>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">06</biblScope>
			<biblScope unit="page" from="2872" to="2893" />
			<date type="published" when="2022-06">June 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Domain adaptation under target and conditional shift</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Muandet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on International Conference on Machine Learning</title>
		<meeting>the 30th International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
	<note>ICML&apos;13. III-819-III-827. JMLR.org</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Multisource domain adaptation: A causal view</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schoelkopf</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v29i1.9542</idno>
		<ptr target="https://ojs.aaai.org/index.php/AAAI/article/view/9542" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2015-02">Feb. 2015</date>
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">The unreasonable effectiveness of deep features as a perceptual metric</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
