<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ROBOT POURING: IDENTIFYING CAUSES OF SPILLAGE AND SELECTING ALTERNATIVE ACTION PARAMETERS USING PROBABILISTIC ACTUAL CAUSATION</title>
				<funder>
					<orgName type="full">Causal Human Activity Models for Cognitive Architectures</orgName>
				</funder>
				<funder ref="#_bKraCTy">
					<orgName type="full">German Research Foundation DFG, as part of Collaborative Research Center (Sonderforschungsbereich)</orgName>
				</funder>
				<funder ref="#_vU8GPmD">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2025-06-10">10 Jun 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Jaime</forename><surname>Maldonado</surname></persName>
							<email>jmaldonado@uni-bremen.de</email>
						</author>
						<author>
							<persName><forename type="first">Jonas</forename><surname>Krumme</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Christoph</forename><surname>Zetzsche</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Vanessa</forename><surname>Didelez</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Kerstin</forename><surname>Schill</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Cognitive Neuroinformatics University of Bremen Bremen</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Cognitive Neuroinformatics University of Bremen Bremen</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Department of Biometry and Data Management Leibniz Institute for Prevention Research and Epidemiology -BIPS Bremen</orgName>
								<orgName type="institution">Cognitive Neuroinformatics University of Bremen Bremen</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Cognitive Neuroinformatics University of Bremen Bremen</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">ROBOT POURING: IDENTIFYING CAUSES OF SPILLAGE AND SELECTING ALTERNATIVE ACTION PARAMETERS USING PROBABILISTIC ACTUAL CAUSATION</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-06-10">10 Jun 2025</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2502.09395v3[cs.RO]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-21T20:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>robot pouring</term>
					<term>causality</term>
					<term>probabilistic actual causation</term>
					<term>causal discovery</term>
					<term>action-guiding explanations</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In everyday life, we perform tasks (e.g., cooking or cleaning) that involve a large variety of objects and goals. When confronted with an unexpected or unwanted outcome, we take corrective actions and try again until achieving the desired result. The reasoning performed to identify a cause of the observed outcome and to select an appropriate corrective action is a crucial aspect of human reasoning for successful task execution. Central to this reasoning is the assumption that a factor is responsible for producing the observed outcome. In this paper, we investigate the use of probabilistic actual causation to determine whether a factor is the cause of an observed undesired outcome. Furthermore, we show how the actual causation probabilities can be used to find alternative actions to change the outcome. We apply the probabilistic actual causation analysis to a robot pouring task. When spillage occurs, the analysis indicates whether a task parameter is the cause and how it should be changed to avoid spillage. The analysis requires a causal graph of the task and the corresponding conditional probability distributions. To fulfill these requirements, we perform a complete causal modeling procedure (i.e., task analysis, definition of variables, determination of the causal graph structure, and estimation of conditional probability distributions) using data from a realistic simulation of the robot pouring task, covering a large combinatorial space of task parameters. Based on the results, we discuss the implications of the variables' representation and how the alternative actions suggested by the actual causation analysis would compare to the alternative solutions proposed by a human observer. The practical use of the analysis of probabilistic actual causation to select alternative action parameters is demonstrated.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Pouring the content of a source container into a target container requires planning, perception, and action capabilities. Humans excel at these capabilities and can skillfully pour any material into containers of arbitrary shapes and dimensions. If spillage occurs, we can take corrective actions (e.g., selecting a target container with an appropriate capacity) and try again to pour without spilling. The ability to take corrective actions is a crucial aspect of human reasoning for successful task execution. Implementing similar reasoning capabilities in robotic systems can reduce task failures and enable a robust operation in unstructured environments.</p><p>In the pouring task, possible causes for the undesired outcome of spilling the poured material include the low capacity of the target container or the diameter difference of the containers (imagine pouring from a wide glass into the narrow mouth of a bottle), among others. It is reasonable to assume that corrective actions we take in everyday life target the perceived actual cause of the undesired outcome. The extreme opposite of this behavior would consist of randomly changing action variables and observing the task outcome until finding a suitable solution. For example, if we perceive that the cause of spillage is the rim of the target container being too narrow, the corrective action would consist of selecting a target container with a wider rim. Implementing similar reasoning capabilities on robotic or automatic systems would require 1) a mechanism to identify the (not necessarily unique) actual cause of an observed outcome and 2) a principled way to determine how the causal variable needs to be changed to obtain a different outcome.</p><p>Within the context of causal analysis and modeling methods, the concept of actual cause refers to the conditions under which a particular event is recognized to be responsible for producing an outcome <ref type="bibr">[Pearl, 2009a]</ref>. Definitions of actual causation have been mainly used in practical applications to generate explanations for observed outcomes (see works reviewed in Section 3). It has been argued that explanations obtained from the analysis of actual causation can guide the search for alternative actions aiming to change the observed outcome <ref type="bibr" target="#b1">[Beckers, 2022]</ref>. The practical utility of the analysis of actual causation for the search and selection of alternative actions remains to be explored in real-life applications.</p><p>In this paper, we explore the use of actual causation analysis to select action parameters in a robot pouring task. The aim is to identify the actual cause of spillage and to determine how a task parameter should be changed to pour without spilling. We use the probabilistic actual causation definition <ref type="bibr" target="#b2">[Fenton-Glynn, 2021]</ref> to identify the cause of spillage among the variables involved in the task. In a series of examples, we illustrate how the analysis of actual causation can be used to select alternative task parameters. Section 2 introduces the probabilistic actual causation framework and its potential use for action guidance. In Section 2.1, we propose a procedure to use the actual causation probabilities as a principled criterion to find alternative actions.</p><p>The robot pouring task was implemented in a simulation (described in Section 4.1) using a physics engine for realistic behavior. The simulation enabled us to generate trials covering a large combinatorial space of trial parameters (fullness levels and container properties), which would have been cumbersome to achieve in a physical setup. There are two prerequisites to perform an analysis of probabilistic actual causation: 1) a causal graph of the system and 2) the conditional probability distributions necessary to compute interventional queries (i.e., "do" operations ). The variables used to represent the pouring task as a causal graph are described in Section 4.2. To obtain the graph structure, we used a causal discovery algorithm (Section 4.3). The conditional distributions used to compute the do-operations were estimated using neural networks (Section 4.4). Subsequently, the causal probability and actual causation expressions that result from the graph structure are presented in Section 4.5.</p><p>Considering that causal probability computations cannot tell per se what caused an observed outcome as this may be specific to the given circumstances, in Section 5.1, we analyze the causal probability functions for interventions on each variable to gain insight into their influence on spillage and to emphasize their limitations for action guidance. Subsequently, in Sections 5.2 and 5.3 we demonstrate that the analysis based on actual causation can be used in a principled way to select alternative action parameters. Firstly, in Section 5.2 we present four detailed examples of applying actual causation analysis on spillage trials to select alternative action parameters. The examples illustrate how different alternative actions yield lower or higher probabilities of spillage. Secondly, in Section 5.3 we evaluate the likelihood of finding an alternative value and the pouring success rates obtained when running the spillage trials using alternative values. The evaluation was conducted using a test dataset acquired in simulation. Overall, Sections 5.2 and 5.3 provide empirical evidence of the practical usefulness of the actual causation approach to identify alternative parameters to prevent spillage.</p><p>Finally, in Section 6, we discuss the implications of the variables' representation and how the alternative actions suggested by the actual causation analysis would compare to the alternative solutions proposed by a human observer.</p><p>In summary, our contributions are:</p><p>• We report a complete analysis of probabilistic actual causation on a practical problem with all the necessary steps for its implementation (analysis of the task, variable definition, causal-graph structure, and estimation of causal probabilities using neural networks).</p><p>• We show how to use the explanations obtained with the analysis of probabilistic actual causation for action guidance in a practical use case. By doing this, we go beyond the basic diagnostic (attribution of actual cause) toward action guidance.</p><p>• We evaluate the capability of the actual causation approach to automatically identify alternative parameters of different variables.</p><p>• We evaluate the pouring success rates obtained when the alternative parameters are used to prevent spillage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Probabilistic Actual Causation and Action-Guiding Explanations</head><p>The concept of actual cause refers to the conditions under which a particular event is recognized to be responsible for producing an outcome in a specific scenario or context <ref type="bibr">[Pearl, 2009a]</ref>. The purpose of determining an actual cause is to find which past actions explain an already observed output <ref type="bibr" target="#b1">[Beckers, 2022]</ref>. The definition of actual causation proposed by <ref type="bibr" target="#b3">Halpern and Pearl [2005]</ref> for deterministic scenarios is one of the most prominent definitions in the literature <ref type="bibr" target="#b4">[Borner, 2023]</ref>. <ref type="bibr" target="#b5">Fenton-Glynn [2017</ref><ref type="bibr" target="#b2">, 2021]</ref> proposed an extension of Halpern and Pearl's definition that is apt for probabilistic causal scenarios. The Fenton-Glynn's definition of actual causation is formulated in the framework of probabilistic causal models, where the causal model is a causal Bayesian network represented graphically as a directed acyclic graph (DAG) and the link between each node/variable and its direct causes is modeled probabilistically <ref type="bibr">[Pearl, 2009b]</ref>.</p><p>In the DAG formalism, nodes correspond to variables, and directed edges (i.e., arrows) indicate causal influences.</p><p>Additionally, the do(x) operator represents the operation of setting the value of a variable X to X = x (i.e., the variable X is instantiated to a value x), such that P (y|do(x)) represents the probability of obtaining the outcome Y that would result from setting X to x by means of an intervention. <ref type="bibr" target="#b2">Fenton-Glynn [2021]</ref> proposes the following definition of actual causation<ref type="foot" target="#foot_0">foot_0</ref> : Probabilistic Actual Causation. Within a given causal model, consider a cause X of an outcome Y with a directed path P from X to Y ; let the variables that are not on P be denoted by W, and the set of mediators on P by Z. Given the actually observed values (x, w * , z * ), we say that X taking the value X = x rather than X = x ′ is the actual cause of event Y (or Y = 1) when the following probability raising holds for all subsets Z ′ of Z:</p><formula xml:id="formula_0">P (Y | do(W = w * , X = x, Z ′ = z * )) &gt; P (Y | do(W = w * , X = x ′ )).<label>(1)</label></formula><p>It is important to emphasize that the term actual cause refers to token causal relations, as opposed to type causal relations <ref type="bibr">[Pearl, 2009a</ref><ref type="bibr" target="#b3">, Halpern and Pearl, 2005</ref><ref type="bibr" target="#b5">, Fenton-Glynn, 2017]</ref>. That is, an actual cause refers to a specific scenario, where the causal statements of the definition are regarded as singular, single-event, or token-level <ref type="bibr">[Pearl, 2009a]</ref>. This is evident in the inequality (1), which compares the probabilities of an event or outcome Y given the observed values of the variables in W and Z ′ , which can be seen as the given context.</p><p>In general, the notion of actual causation is considered the key to constructing explanations <ref type="bibr">[Pearl, 2009a]</ref>. <ref type="bibr" target="#b1">Beckers [2022]</ref> has used the term action-guiding explanations for scenarios where the analysis of actual causation aims to find explanations for the outputs produced as the result of performing an action. <ref type="bibr" target="#b1">Beckers [2022]</ref> suggests that actual causes can be used for action guidance because they enable the identification of alternative actions that provide better or worse explanations of an outcome. <ref type="bibr" target="#b1">Beckers [2022]</ref> provides a conceptual analysis<ref type="foot" target="#foot_1">foot_1</ref> of how action-guiding explanations relate to sufficient and counterfactual explanations, two other forms of potentially action-guiding explanations. In Beckers' account, a sufficient explanation indicates the conditions under which an action guarantees a particular output. On the other hand, a counterfactual explanation informs which variables would have had to be different <ref type="bibr">(and in what way)</ref> for the outcome to be different. <ref type="bibr">Beckers [2022, p. 2]</ref> concludes that actual causes stand between sufficient and counterfactual explanations: "an actual cause is a part of a good sufficient explanation for which there exist counterfactual values that would not have made the explanation better."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Using the Actual Causation Inequality to Select Alternative Actions</head><p>In inequality (1),</p><formula xml:id="formula_1">P (Y |do(W = w * , X = x, Z ′ = z *</formula><p>)) provides a reference value to check for actual causation. This reference value takes into account the actual values of the variables. Using a contrastive value X = x ′ on the right side of inequality (1), P (Y |do(W = w * , X = x ′ ) is used to check whether or not probability raising holds, thereby providing a principled criterion to determine whether X taking the value X = x rather than X = x ′ is the actual cause of event Y .</p><p>When the variables are continuous, P (Y |do(W = w * , X = x ′ ) can be plotted as a function of the contrastive value X = x ′ . This is illustrated in Figure <ref type="figure" target="#fig_0">1</ref>. The comparison against the reference probability P (Y |do(W = w * , X = x, Z ′ = z * )) reveals the values x ′ for which probability raising holds (shaded region in Figure <ref type="figure" target="#fig_0">1</ref>).</p><p>Fenton-Glynn's framework of actual causation provides contrastive explanations in a given context <ref type="bibr" target="#b4">[Borner, 2023]</ref>. By definition, inequality (1) demands that the explanation remains valid when holding fixed all variables in W and (the subsets) Z ′ at their actual values; probability raising need not hold for other contexts. In this work, we propose to use the contrastive explanations obtained by applying the actual causation inequality for action guidance. Given that an outcome Y has been observed, the goal is to select an alternative action that will prevent the outcome. Recalling that probability raising entails that X taking the value X = x rather than X = x ′ is the cause of event Y , we propose to select x ′ values as alternative actions. In the case illustrated in Figure <ref type="figure" target="#fig_0">1</ref>, while selecting an X value in the shaded area much smaller than the actual X = x is likely to change the outcome, selecting X close and above the actual value will likely leave the outcome unchanged. We emphasize that the magnitude of probability raising might drastically differ within the range of contrastive values. This has implications for the suitability of different x′ values as alternative actions. Consider the alternative actions X = -5 and X = 0 in the schematic example illustrated in Figure <ref type="figure" target="#fig_0">1</ref>. Based on the probabilities, it can be assumed that X = -5 is a better choice since selecting X = 0 has a higher chance of leaving the outcome unchanged. Under these considerations, the selection of an alternative parameter can be based on a pre-defined probability threshold.</p><p>An alternative parameter can be found automatically with the following steps:</p><p>1. Identify the range of contrastive values where probability raising holds using inequality (1). 2. Within these values, select the subset of values with a probability below a pre-defined probability threshold. 3. From this second subset, select the closest value to the current parameter.</p><p>In the last step we use a distance criterion. However, other application-dependent criteria can be used (e.g., the cost or availability of different alternatives) to select an alternative parameter. The crucial aspect of the automatic search is that the probability threshold is selected such that it becomes very likely to change the outcome.</p><p>In summary, to check for actual causation using inequality (1), one must decide which variable takes the role of X and, given the graph structure, identify the variables for the sets W and Z ′ . Thus, the analysis of actual causation can be applied to different variables. In the robot pouring task, if spillage occurs, the aim is to find alternative parameters to repeat the action. In this work, we use the probabilistic actual causation framework to guide the selection of alternative parameters in a principled way. In Section 5.2, we illustrate in a series of detailed examples the impact of using different probability thresholds (i.e., 0.2 for low spillage probability and and 0.5 for chance-level probability) on the identified alternative parameters. Subsequently, in Section 5.3, we evaluate the pouring success rates obtained using the alternative parameters identified using a 0.1 probability threshold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Related Work</head><p>This section presents robotics applications that use causal methods related to our work (i.e., actual causation, causal Bayesian networks, and causal discovery). We start by presenting applications that use the Halpern-Pearl definition of actual causality <ref type="bibr" target="#b3">[Halpern and Pearl, 2005]</ref>. Subsequently, we review works that use causal discovery to learn the structure of causal Bayesian networks to model robotic tasks in different contexts. Finally, we review the method proposed by <ref type="bibr" target="#b7">Diehl and Ramirez-Amaro [2023]</ref> to predict and prevent failures using causal-based contrastive explanations, highlighting the similarities and differences to our approach. <ref type="bibr" target="#b8">Araujo et al. [2022]</ref> use the actual causation framework by Halpern and Pearl in a human-robot interaction setting where a robot interacts with children with ASD (Autism Spectrum Disorder). The robot plays different interactive games with the children, aiming to improve the children's ability to see the world from the robot's point-of-view. The authors present a tool that uses a causal model of the interactive games and the actual causation framework. This is applied to explain events during the game's course. For example, if the robot cannot see an object involved in the interaction, it explains to the child why it cannot see it (e.g., "I cannot see it because it is too high") <ref type="bibr" target="#b8">[Araujo et al., 2022]</ref>. The actual cause of an event is analyzed using a rule-based system, as opposed to a search over the possible counterfactuals <ref type="bibr" target="#b8">[Araujo et al., 2022]</ref>. The usefulness of the explanations generated by the system was evaluated by asking a group of observers to watch videos of the robot providing explanations in different situations and then rate each explanation. The rating was based on qualitative criteria, e.g., whether the explanation was understandable, sufficiently detailed, or informative about the interaction, among other aspects. <ref type="bibr" target="#b9">Zibaei and Borth [2024]</ref> use the Halpen and Pearl actual causation framework to retrieve explanations of failure events in unmanned aerial vehicles (UAVs). In this context, an actual causation analysis aims to provide actionable explanations, that is, an explanation that indicates which corrective actions can be taken to prevent future failures. The analysis of actual failure causes was applied to different UAV failure scenarios (loss of control, events during take-off and cruise, and equipment problems). The causal models of failure events were constructed using flight logs recorded at run-time containing abstracted events and raw sensor data. In order to diagnose instances of a particular type of failure (e.g., instances of crash events in the logs), the causal graph of the failure and the actual values of the monitored event and sensor data were analyzed using a tool for the automatic checking of the Halpern and Pearl actual causation conditions. The correctness of the diagnoses was evaluated using a manually labeled ground-truth dataset. <ref type="bibr" target="#b10">Chockler et al. [2021]</ref> designed an algorithm to explain the output of neural network-based image classifiers in cases where parts of the classified object are occluded. The algorithm applies concepts of the Halpern and Pearl definition of actual causation to generate explanations. The explanation consists of a subset of image pixels, which is the minimal or approximately minimal subset that allows the neural network to classify the image. It can be tested if a pixel is a cause of the classification by considering a subset of pixels in the image that does not include that pixel. In this case, applying a masking color to any combination of pixels from this subset does not alter the classification output. However, if we apply a masking color to the entire subset along with the individual pixel, this will lead to a change in the classification result. In this way, the algorithm ranks pixels according to their importance for the classification. The authors evaluate the explanations of their system by comparing their results with the outputs of other explanation tools. They compare the size of the explanation (smaller is better) and the size of the intersection of the explanation with the occluding object (smaller is better), getting favorable results for their approach. Additional work on explaining image classifiers was done by <ref type="bibr" target="#b11">Chockler and Halpern [2024]</ref> and has been applied to extend the work of Kommiya <ref type="bibr" target="#b12">Mothilal et al. [2021]</ref>, who used a simplified version of the actual causation framework. The authors show that extending the previous work to the full version of the framework is possible and can benefit future work in explaining image classifiers. However, they suggest that using the full definition might make computing the explanations more complex.</p><p>The reviewed works show that practical applications of the concepts of actual causation have focused on generating explanations, leaving the potential to guide decision-making processes aside. To the best of our knowledge, the probabilistic actual causation definition of <ref type="bibr" target="#b5">Fenton-Glynn [2017</ref><ref type="bibr" target="#b2">, 2021]</ref> has not been used in any practical application.</p><p>Causal methods have been applied to model generic robot tasks such as pushing, pick-and-place, and stacking <ref type="bibr" target="#b13">[Ahmed et al., 2020</ref><ref type="bibr" target="#b14">, Brawer et al., 2020</ref><ref type="bibr" target="#b15">, Huang et al., 2023</ref><ref type="bibr" target="#b7">, Diehl and Ramirez-Amaro, 2023</ref>] and context-specific ones such as human-robot interaction <ref type="bibr" target="#b16">Castri et al. [2022]</ref> and household tasks <ref type="bibr" target="#b17">[Li et al., 2020]</ref>. In the context of tool affordance learning, causal discovery has been used to identify the effect of push and pull actions on an object's final position in a real setup <ref type="bibr" target="#b14">[Brawer et al., 2020]</ref>. To reduce the sim-to-real gap in robot-object trajectories, a custom causal discovery algorithm was used to optimize the simulated physical parameters <ref type="bibr" target="#b15">[Huang et al., 2023]</ref>. Additionally, recent work has used simulations to explore large combinatoric spaces of task parameters and causal methods to model task outcomes <ref type="bibr" target="#b13">[Ahmed et al., 2020</ref><ref type="bibr" target="#b15">, Huang et al., 2023</ref><ref type="bibr" target="#b7">, Diehl and Ramirez-Amaro, 2023]</ref>.</p><p>Similar to our work, causal methods have been applied to find causal-based contrastive explanations for task failures <ref type="bibr" target="#b18">[Diehl and Ramirez-Amaro, 2022]</ref>. In subsequent work, this method has been extended to predict and prevent task failures by finding corrective parameters <ref type="bibr" target="#b7">[Diehl and Ramirez-Amaro, 2023]</ref>. Examples of contrastive explanations are provided for a cube stacking task and for dropping spheres into different containers (bowls, plates, and glasses) <ref type="bibr" target="#b18">[Diehl and Ramirez-Amaro, 2022]</ref>, and the method for prediction and prevention has been applied to a cube stacking task (stacking one cube and stacking three cubes) <ref type="bibr" target="#b7">[Diehl and Ramirez-Amaro, 2023]</ref>. In particular, the method proposed by <ref type="bibr" target="#b7">Diehl and Ramirez-Amaro [2023]</ref> has methodological similarities to our approach. Their method uses a causal Bayesian network to predict errors and probabilities of success to find a corrective action. Simulated data are used to learn the causal Bayesian network's structure and estimate its joint probability distribution. When an action is predicted to fail, a search is conducted in a discretized parameter space to find the parametrization that needs the least interval changes to achieve a successful execution based on the predicted success probability <ref type="bibr" target="#b7">[Diehl and Ramirez-Amaro, 2023]</ref>. For example, searching for close parametrizations in the cube-stacking task means that starting from a robot hand position that will likely fail leads to finding the closest position that will likely succeed <ref type="bibr" target="#b7">[Diehl and Ramirez-Amaro, 2023]</ref>.</p><p>The search criterion is based on contrastive explanations <ref type="bibr" target="#b18">[Diehl and Ramirez-Amaro, 2022]</ref>, which compare the variable parametrization of the failed action and the closest parametrization that exceeds a success probability threshold (ϵ) <ref type="bibr" target="#b7">[Diehl and Ramirez-Amaro, 2023]</ref>. The success probabilities are retrieved from the factorized form of the joint probability distribution. The search is conducted within a tree that contains a complete parametrization of the parent variables of the outcome variable. Since the search is conducted within a tree structure, the time to find an alternative parametrization depends on the success probability threshold, the number of parent variables, and the number of discrete intervals of the variables <ref type="bibr" target="#b7">[Diehl and Ramirez-Amaro, 2023]</ref>.</p><p>In Table <ref type="table" target="#tab_0">1</ref>, we summarize the similarities and differences between the method proposed by Diehl and Ramirez-Amaro [2023] and our approach. Their method automatically finds the corrective parameters, which might involve changing the value of one or more variables. In contrast, our approach is restricted to searching for corrective parameters in a single variable. Their method identifies the closest parametrization that is predicted to succeed. On the other hand, our approach identifies a range of values that explain the outcome according to the probability-raising criterion. Within this range of values, a specific value likely to yield success can be selected using a probability criterion (see explanation in Section 2.1). Similarly to <ref type="bibr" target="#b7">Diehl and Ramirez-Amaro [2023]</ref>, a success probability threshold can be used as a selection criterion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Materials and Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Robot Pouring Simulation</head><p>The robot pouring task was simulated using CoppeliaSim Version 4.8.0 (rev. 0) <ref type="bibr" target="#b19">Rohmer et al. [2013]</ref> with the Open Dynamics Engine (ODE), a physics engine for rigid body dynamics and collision detection. The setup consists of an UR5 robot arm with a parallel jaw gripper, as shown in Figure <ref type="figure" target="#fig_1">2</ref>. In the simulation, the robot poured marbles from a source container into a target container. The marbles were simulated with CoppeliaSim's particle object, which simulates spherical particles using parameters for their diameter (1.5 cm) and density (2829.42 kg/m 3 , empirically determined). In each trial, the source container (capacity = 514.72 cm 3 ) was filled with simulated marbles (Figure <ref type="figure" target="#fig_1">2a</ref>). The characteristics of the pouring movement were fixed for all trials. The robot grasped the source container and brought it to a pouring position relative to the target container's rim (Figure <ref type="figure" target="#fig_1">2b</ref>). In the pouring position, the gripper was rotated with a fixed rotation velocity (1 rad/s) until reaching a fixed angle of -15 • (Figure <ref type="figure" target="#fig_1">2c</ref>). After pouring all the marbles (Figure <ref type="figure" target="#fig_1">2d</ref>), spillage was detected using a force sensor located on the base of the target container. In each trial we manipulated the amount of marbles in the source container and the dimensions of the target container. The dimensions of the source container remained fixed across trials. In each trial, the source container was filled with a random amount of marbles, and a target container with random dimensions (height and rim diameter) was generated. The positioning of the marbles inside the source container was random. Six thousand pouring trials were simulated. The rationale of the random trial parameters and variable definitions are explained in Section 4.2. Details of the random distributions used to sample the trial parameters are provided in Table <ref type="table" target="#tab_1">2</ref>. In each trial, the randomness of the spillage outcome results from the behavior of the particles simulated by the physics engine and its interaction with the characteristics of the target container.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">DAG Variables</head><p>In each trial, the source container was filled to a random fullness level, and a target container with random dimensions was generated. The rationale for the poured amounts and the target container characteristics in each trial aims to capture typical spillage causes. Table <ref type="table" target="#tab_1">2</ref> provides details of the variable definitions and random sampling distributions. The target container can have an equal, smaller, or larger capacity than the source container. The capacity of the target container influences the probability of spillage (spillage is more likely to occur when pouring into a target container of a small capacity). We represent this cause of spillage with the variable relative capacity (RC).</p><p>Aside from the target container's capacity, spillage depends on the complex interplay between the fullness level of the source container and the rim dimensions of the source and target containers. Imagine pouring marbles from a wide glass into a bottle through its narrow mouth. While pouring a single marble is likely to succeed, the amount spilled will increase as the number of marbles increases. We represent the fullness level of the source container with the variable fullness (F U ).</p><p>The target container's rim can be equal, smaller, or larger in diameter than the source container. The relation between rim diameters influences the probability of spillage (spillage is more likely to occur when pouring into a target container with a smaller rim diameter). We represent this cause of spillage with the variable relative diameter (RD). As a result, in each trial, the dimensions of the target container (height and rim diameter) are determined by the RC and RD variables.</p><p>We also consider the fact that spillage occurs when the amount of marbles exceeds the capacity of the target container.</p><p>The source containers' capacity and fullness level determine the poured amount (recall that all the marbles are poured). We represent the difference in volume between the poured amount and the target container's capacity with the variable relative volume (RV ). Finally, we represent the outcome of the pouring trial with the variable spillage (S), a binary variable that indicates whether or not spillage occurred. S is labeled as true irrespective of the number of spilled marbles (i.e., spilling one or twenty marbles yields S = true).  RV represents the volume relation between the poured amount and the target container's capacity, defined as RV = poured volume target capacity . RV &lt; 1 indicates that the poured amount fits into the target container, and RV &gt; 1 indicates that the poured amount exceeds the target container's capacity. spillage S Binary variable indicating whether or not spillage occurred.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Determination of DAG Structure Using Causal Discovery</head><p>In order to perform an analysis of actual causation, we require a DAG of the causes of spillage, where edges represent probabilistic effects between variables. A naive approach to setting the causal structure would be to assume that each variable described in Section 4.2 is connected to the outcome S with an edge. This would constitute a strong assumption in which all variables are direct causes of S, excluding the possibility of indirect effects. In this respect, it is important to recall that the validity of the analysis of actual causation relies on the correctness of the DAG structure. To avoid making naive assumptions about the causal structure, we leverage the availability of simulated pouring trials to determine the structure of the DAG using a causal discovery algorithm.</p><p>In order to determine the structure of the DAG, the variables described in Section 4.2 were processed with the PC algorithm <ref type="bibr" target="#b20">[Spirtes and Glymour, 1991]</ref>, a well-established causal discovery algorithm <ref type="bibr" target="#b21">[Glymour et al., 2019</ref><ref type="bibr" target="#b22">, Nogueira et al., 2022]</ref>. In general, causal discovery algorithms, also known as structure learning algorithms, perform a systematic analysis of many possible causal structures, typically by testing probabilistic independence and dependence between the variables <ref type="bibr" target="#b23">[Malinsky and Danks, 2017</ref><ref type="bibr" target="#b21">, Glymour et al., 2019</ref><ref type="bibr" target="#b22">, Nogueira et al., 2022]</ref>. We used the PC implementation available in Tetrad (version 7.6.5-0)<ref type="foot" target="#foot_2">foot_2</ref> , a software toolbox for causal discovery <ref type="bibr" target="#b24">[Ramsey et al., 2018]</ref>. As a statistical test, we use the Degenerate Gaussian Likelihood Ratio Test (DG-LRT) <ref type="bibr" target="#b25">[Andrews et al., 2019]</ref>, which has shown good discovery performance on datasets containing continuous and discrete variables <ref type="bibr" target="#b25">[Andrews et al., 2019]</ref>. We executed the algorithm on 1000 bootstraps of the data to ensure the stability and reliability of the inferred causal relationships (if the results vary widely over the different bootstrap samples, the output of the algorithm is considered unstable) <ref type="bibr" target="#b21">[Glymour et al., 2019]</ref>. Further details about the PC parameters, bootstrapping results, and modeling assumptions are provided in the supplementary materials. The discovered DAG is shown in Figure <ref type="figure">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Estimation of do-Probabilities Using Neural Autoregressive Density Estimation</head><p>In this paper, we use neural autoregressive density estimators (NADEs) <ref type="bibr" target="#b26">[Garrido et al., 2021]</ref> to estimate the conditional distributions necessary to compute the interventional queries in inequality (1). NADEs are used as universal approximators and are flexible to work with models containing continuous and discrete variables <ref type="bibr" target="#b26">[Garrido et al., 2021]</ref>. <ref type="bibr" target="#b26">Garrido et al. [2021]</ref> demonstrated in a series of examples that NADEs provide a practical modeling architecture to estimate causal quantities from models with linear and non-linear relationships between variables of different distributions (e.g., Normal, Log-normal, and Bernoulli) using the DAG formalism and the do-calculus framework. A DAG model represents the conditional dependencies between the set of J random variables X 1 , • • • , X J . The DAG induces a joint probability distribution P (X) of the variables, which can be factorized into the conditional distributions of each X j conditioned on a function f j of its parents P A(X j ) (the causal Markov condition):</p><formula xml:id="formula_2">P (X) = J j P (X j |f j (P A(X j )))<label>(2)</label></formula><p>A NADE is a generative model that estimates the conditional distributions in equation ( <ref type="formula" target="#formula_2">2</ref>) <ref type="bibr" target="#b26">[Garrido et al., 2021]</ref>. The functions f j are parametrized as independent fully connected feed-forward neural networks. Therefore, there is a neural network for each variable in the DAG. Each neural network takes the parents of the variable of interest P A(X j ) and outputs the parameters of the distribution of X j . For example, the neural network outputs the mean and standard deviation of Gaussian variables. These networks are trained using the negative log-likelihood of equation ( <ref type="formula" target="#formula_2">2</ref>) as the loss function. The individual conditional distributions in equation ( <ref type="formula" target="#formula_2">2</ref>), also called independent causal mechanisms or Markov Kernels, are used to estimate the effects of interventions <ref type="bibr" target="#b26">[Garrido et al., 2021]</ref>. The causal estimates are reliable under the assumption that the DAG structure is correct and that the training data provides enough support to learn the distribution parameters <ref type="bibr" target="#b26">[Garrido et al., 2021]</ref>. Details of the implementation are provided in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Causal Probability Expressions and Actual Causation Inequalities</head><p>The factorized joint probability distribution that results from the DAG structure shown in Figure <ref type="figure">3</ref> is expressed as a product of conditional distributions and independent causal mechanisms:</p><formula xml:id="formula_3">P (RC, F U, RV, RD, S) = P (RC) • P (F U ) • P (RV |RC, F U ) • P (RD) • P (S|F U, RD, RV )<label>(3)</label></formula><p>These conditional distributions and independent causal mechanisms are approximated using NADEs, as described in Section 4.4. The implemented neural networks are shown in Figure <ref type="figure" target="#fig_4">4</ref>. The distribution of the continuous variables RC, F U , RV , and RD is approximated by Gaussian distributions. For these variables, each network takes the parent of the corresponding variable as input and produces two parameters, one for the mean (µ) and one for the standard deviation (σ) of the Gaussian distribution. Following the implementation of <ref type="bibr" target="#b26">Garrido et al. [2021]</ref>, the root nodes (RC, F U , and RD) take a constant value as input, represented as a "1" in Figure <ref type="figure" target="#fig_4">4</ref>. The discrete variable S follows a Bernoulli distribution. Its neural network takes the variables RV , F U , and RD as input and outputs the probability of sampling S = true given the input values. The NADE estimators are used to compute causal (or interventional) probabilities (e.g., P (S|do(RD)) ) and actual causation queries in the form of inequality (1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.1">Causal Probability Expressions</head><p>The DAG indicates that S has direct and indirect causes. The path RC → RV → S shows that RC is an indirect cause of S, and its effect is mediated by RV . The causal relation between F U and S combines a direct path (F U → S) and indirect path (F U → RV → S). Finally, RD is a direct cause of S (path RD → S). Ultimately, the probability of S depends on the interactions between RV , F U , and RD. Additional insight into the factors that yield spillage can be gained by estimating the effects of each variable on S. Causal effects are functions of interventional probabilities P (S|do(X)) over different values of X; the expressions can be obtained using the rules of do-calculus with the causaleffect R package (version 1.3.15) <ref type="bibr" target="#b27">[Tikka and Karvanen, 2017]</ref>. Following the procedure done by Garrido et al. Under the given causal model, the interventional probability of spillage P (S|do(RC)) under a given choice of RC is obtained as:</p><formula xml:id="formula_4">P (S | do(RC)) = RD,F U,RV P (S|F U, RD, RV ) • P (RV |RC, F U ) • P (RD) • P (F U ) dRV dRD dF U<label>(4)</label></formula><p>We approximated P (S|do(RC)) using Monte Carlo integration by sampling from the distributions P (RD), P (F U ), and P (RV |RC, F U ), and propagating forward through the neural network implemented for P (S|F U, RD, RV ).</p><p>The interventional probability P (S|do(F U )) for a given choice of F U is obtained as:</p><formula xml:id="formula_5">P (S | do(F U )) = RC,RV,RD P (S|F U, RD, RV ) • P (RV |RC, F U ) • P (RD) • P (RC) dRC dRD dRV<label>(5)</label></formula><p>Similarly, P (S|do(F U )) was approximated using Monte Carlo integration by sampling from the distributions P (RC), P (RD), and P (RV |RC, F U ), and propagating forward through the neural network implemented for P (S|F U, RD, RV ).</p><p>The expression P (S|do(F U )) in equation ( <ref type="formula" target="#formula_5">5</ref>) combines the effect mediated by RV and the direct effect of F U . In order to assess the direct effect of F U on S we must fix RV . This enables us to assess how the direct effect of F U may differ for different choices of RV . The direct effect can be expressed by varying the values of F U with constant RV in P (S|do(F U, RV )), which is obtained as:</p><formula xml:id="formula_6">P (S | do(F U, RV )) = RD P (S|RD, F U, RV ) • P (RD) dRD<label>(6)</label></formula><p>The probability P (S|do(F U, RV )) was also approximated using Monte Carlo integration by sampling from the distributions P (RD) and propagating forward through the neural network implemented for P (S|F U, RD, RV ).</p><p>Furthermore, P (S|do(RV )) is obtained as:</p><formula xml:id="formula_7">P (S | do(RV )) = RD,F U P (S|RD, F U, RV ) • P (RD) • P (F U ) dF U dRD<label>(7)</label></formula><p>The probability P (S|do(RV )) was approximated using Monte Carlo integration by sampling from the distributions P (RD) and P (F U ), and propagating forward through the neural network implemented for P (S|RD, F U, RV ).</p><p>Finally, P (S|do(RD)) is obtained as:</p><formula xml:id="formula_8">P (S | do(RD)) = RC,F U,RV P (S|F U, RD, RV ) • P (RV |RC, F U ) • P (F U ) • P (RC) dRC dF U dRV (8)</formula><p>The probability P (S|do(RD)) was approximated using Monte Carlo integration by sampling from the distributions P (F U ), P (RC), and P (RV |RC, F U ), and propagating forward through the neural network implemented for P (S|F U, RD, RV ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.2">Inequalities for Analysis of Probabilistic Actual Causation</head><p>For the analysis of actual causation we identify the sets of variables necessary to compute the probabilities compared in inequality ( <ref type="formula" target="#formula_0">1</ref>). In order to analyze whether RD is an actual cause of S, we consider the only path P: RD → S.</p><p>The set of variables W that lie off the path is W = {RC, F U, RV }, and the set Z of variables that lie intermediate between RD and S is Z = ∅. RD taking the value RD = rd rather than RD = rd ′ (in the context of the observed values rc, f u, rv) is an actual cause of S (or S = true) when the following probability raising holds:</p><formula xml:id="formula_9">P (Y | do(W = w * , X = x, Z ′ = z * )) &gt; P (Y | do(W = w * , X = x ′ ))</formula><p>P (S | do(rc, f u, rv, rd)) &gt; P (S | do(rc, f u, rv, rd ′ ))</p><formula xml:id="formula_10">P (S | f u, rv, rd) &gt; P (S | f u, rv, rd ′ )<label>(9)</label></formula><p>The conditional probabilities in inequality (9) were approximated using the neural network for P (S|RV, F U, RD).</p><p>In order to analyze whether F U is an actual cause of S, we consider the path P: F U → RV → S. The set of variables W that lie off the path is W = {RC, RD}, and the set Z of variables that lie intermediate between F U and S on P is Z = {RV }. Recalling that inequality (1) is tested for all the subsets of Z which includes the empty set [Fenton-Glynn, 2021], F U taking the value F U = f u rather than F U = f u ′ is an actual cause of S (or S = true) when the probability raising holds both for Z ′ = RV (inequality ( <ref type="formula" target="#formula_12">10</ref>)) and Z ′ = ∅ (inequality ( <ref type="formula" target="#formula_13">11</ref>)) :</p><formula xml:id="formula_11">P (Y | do(W = w * , X = x, Z ′ = z * )) &gt; P (Y | do(W = w * , X = x ′ ))</formula><p>P (S | do(rc, rd, f u, rv)) &gt; P (S | do(rc, rd, f u ′ ))</p><formula xml:id="formula_12">P (S | rd, f u, rv) &gt; RV P (S | rd, f u ′ , RV ) • P (RV | rc, f u ′ ) dRV<label>(10)</label></formula><formula xml:id="formula_13">P (Y | do(W = w * , X = x, Z ′ = z * )) &gt; P (Y | do(W = w * , X = x ′ )) P (S | do(rc, rd, f u)) &gt; P (S | do(rc, rd, f u ′ )) RV P (S | rd, f u, RV ) • P (RV | rc, f u) dRV &gt; RV P (S | rd, f u ′ , RV ) • P (RV | rc, f u ′ ) dRV<label>(11)</label></formula><p>The integrals in inequality (11) and on the right side of inequality (10) were approximated using Monte Carlo integration by sampling from the distribution P (RV | RC, F U ) and propagating forward through the neural network implemented for P (S | F U, RD, RV ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Causal Probabilities</head><p>The causal probability P (S|do(RC)) was computed using equation (4) over a range of RC values covering low (RC &lt; 1) and large (RC &gt; 1) target container capacities. The estimated probabilities are shown in Figure <ref type="figure" target="#fig_6">5a</ref>. Over the whole range of RC, the probability lies slightly below 0.5. This probability curve reflects the fact that RV mediates the effect of RC, which in turn crucially depends on F U . Without knowing the context of F U , the total effect of RC on S lies around the chance level. Thus, solely knowing or setting a RC value does not provide enough information on causing or preventing spillage.  The probabilities P (S|do(F U )) and P (S|do(F U, RV )) were computed using equations ( <ref type="formula" target="#formula_5">5</ref>) and ( <ref type="formula" target="#formula_6">6</ref>), respectively, over a range of F U values covering low (F U &lt; 0.4), medium (0.4 ≤ F U ≤ 0.6), and high (F U &gt; 0.6) fullness levels of the source container. The estimated probabilities for P (S|do(F U )), shown in Figure <ref type="figure" target="#fig_6">5b</ref>, indicate that low fullness levels yield a low probability (≈ 0.2) of spillage. This probability increases as the fullness level increases, reaching a maximum of 0.6. For the direct effect P (S|do(F U, RV )), we present the probabilities obtained for three levels of RV : RV = 0.25 (the poured amount fits into the target container), RV = 1 (the poured amount equals the target container's capacity), and RV = 1.5 (the poured amount exceeds the target container's capacity). The estimated probabilities are shown in Figure <ref type="figure" target="#fig_6">5c</ref>. It can be observed that the probabilities obtained for RV = 0.25 and RV = 1 are similar. Thus, for RV &lt; 1, increasing F U will have a similar effect on the probability of spillage. In contrast, the results obtained for RV = 1.5 show different non-linear behavior with overall spillage probabilities above the chance level.</p><p>The probability P (S|do(RV )) was computed using equation ( <ref type="formula" target="#formula_7">7</ref>) over a range of RV covering values where the poured amount fits into the target container (RV &lt; 1) and the poured amount exceeds the target container's capacity (RV &gt; 1).</p><p>The estimated probabilities are shown in Figure <ref type="figure" target="#fig_6">5d</ref>. For RV &lt; 1, we obtained spillage probabilities ≈ 0.4. Therefore, without knowing the context of the other variables, when RV &lt; 1, the probability of spillage lies slightly below the chance level. For RV &gt; 1 we observe a moderate probability increase, reaching a maximum of 0.88. By definition, RV &gt; 1 indicates that the poured amount exceeds the target container's capacity. Thus, in practice, a large probability of spillage (≈ 1) for RV ≫ 1, and a step probability increase for RV ≈ 1 would have been expected. This suggests that the NADE for RV is smoothing the estimated probabilities.</p><p>The probability P (S|do(RD)) was computed using equation ( <ref type="formula">8</ref>) over a range of RD values covering target containers of smaller diameter (RD &lt; 1) and target containers of larger diameter (RD &gt; 1). The estimated probabilities are shown in Figure <ref type="figure" target="#fig_6">5e</ref>. For RD &lt; 0.8, we observe a constant large probability of spillage (≈ 0.9). For 0.8 ≤ RD ≤ 1.1, we observe a sharp probability decrease. For RD &gt; 1.1 we observe a low probability of spillage.</p><p>In contrast to the results obtained for P (S|do(F U )) and P (S|do(RV )), P (S|do(RD)) shows distinct ranges where RD yields low and high probability of spillage. Additionally, the range of values where P (S|do(RD)) shows a sharp probability decrease indicates that a slight change in RD can significantly affect the probability of spillage.</p><p>Overall, the causal effects presented in this section exhibit non-linear behavior. Due to the non-linear behavior and the different magnitudes of the spillage probability, using the individual causal probabilities to analyze a trial with particular RC, F U , RV , and RD values does not provide conclusive information. This happens because causal probabilities in the form of P (S|do(X)) do not consider the context (i.e., the actual value) of the other variables. In the following section, we present a set of examples where the actual causation framework is used to determine which variable (at its actual value) is an actual cause of spillage and how the variable should be changed to yield a different outcome.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Actual Causes of Spillage and Alternative Actions</head><p>As explained in Section 2, the actual causation inequality compares a reference probability value against the probabilities obtained from contrastive values to check whether or not probability raising holds. As proposed in Section 2.1, we use the contrastive values where probability raising holds to guide the selection of alternative parameters to avoid spillage.</p><p>In this section, we present the results obtained by using the automatically-selected alternative values to achieve either a chance-level or a low probability of spillage. In the following, we explain the rationale behind the selected spillage trial examples.</p><p>According to the considerations presented in Section 4.2 and the DAG discovered from the training data (Figure <ref type="figure">3</ref>), spillage has three direct causes: RV , F U , and RD. At the beginning of the pouring action, i.e., at the pouring onset, the interaction of F U and RD determines the probability of spillage. Afterward, as the content is poured into the target container, spillage will occur after the poured amount exceeds the container's capacity; that is, spillage caused by RV .</p><p>Recalling that RV is caused by F U and RC, the capacity of the target will be exceeded only if RC &lt; 1. The smaller the RC, the smaller the F U will be necessary to exceed the target container's capacity. Otherwise, when RC &gt; 1, spillage is caused only by F U and RD. Therefore, we exemplify the usage of the actual causation framework for the analysis of spillage trials with 0.5 &lt; RC &lt; 1. For simplicity, we focus the analysis on RD and F U , which have a direct effect on S. We do not consider RC because it doesn't have a direct effect on S (the effect of RC is mediated by RV , which also depends on F U ).</p><p>In each example, we start by presenting the frequency of the outcomes (spillage true or false) over 100 replications using the actual trial parameters. Afterward, we perform the actual causation analysis of the F U and RD variables.</p><p>To compare the suitability of different alternative parameters, we select alternative F U or RD parameters where P (Y |do(W = w * , X = x ′ ) yields a low probability (0.2) and a chance-level (0.5) probability. The alternative parameters are identified automatically following the steps described in Section 2.1. We conduct 100 replications using the alternative parameters and present the frequency of the outcomes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Example 1</head><p>In this example, the source container was filled to a medium level (F U = 0.51). The particles were poured into a target container of smaller capacity (RC = 0.70) and smaller diameter (RD = 0.70). The actual trial parameters are shown in Figure <ref type="figure" target="#fig_7">6a</ref>. The outcomes over 100 replications with the actual parameters, shown in Figure <ref type="figure" target="#fig_7">6b</ref>, indicate a high probability of spillage.</p><p>Figure <ref type="figure" target="#fig_7">6c</ref> shows the reference probability and the probabilities obtained for contrastive RD values obtained from inequality (9). In the area where probability raising holds, the contrastive probabilities show a sharp decrease around RD ≈ 0.8 and a low probability for RD &gt; 0.9. We select the alternative values RD = 0.87 (chance-level probability) and RD = 0.89 (low probability ≈ 0.2), as shown in Figures <ref type="figure" target="#fig_7">6e</ref> and<ref type="figure" target="#fig_7">6g</ref>, respectively. It is important to note that changing RV and keeping RC constant produce a target container of smaller height. The results from 100 replications are shown in Figure <ref type="figure" target="#fig_7">6f</ref> and Figure <ref type="figure" target="#fig_7">6h</ref>. The replications with the alternative parameters show that RD = 0.89 is better suited to avoid spillage. It is also interesting to note that due to the non-linearity of the probabilities, small changes in RD yield a large effect on the probability of spillage.</p><p>Figure <ref type="figure" target="#fig_7">6d</ref> shows the reference probabilities and the probabilities obtained for contrastive FU values obtained from inequalities ( <ref type="formula" target="#formula_12">10</ref>) and ( <ref type="formula" target="#formula_13">11</ref>). The probability curve that results from the contrastive values is nearly horizontal and very close in magnitude to the reference probabilities. The area for which probability raising holds (i.e., the shaded region) thus results from small magnitude differences barely noticeable in Figure <ref type="figure" target="#fig_7">6d</ref>. The contrastive values for which probability raising holds yield a large probability of spillage. Therefore, selecting an alternative F U is unlikely to change the outcome.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Example 2</head><p>In this example, the source container was filled to a medium-high level (F U = 0.64). The particles were poured into a target container of a slightly smaller capacity (RC = 0.96) and slightly larger diameter (RD = 1.10). The actual trial parameters are shown in Figure <ref type="figure">7a</ref>. The outcomes over 100 replications with the actual parameters, shown in Figure <ref type="figure">7b</ref>, indicate a low probability of spillage.</p><p>Figure <ref type="figure">7c</ref> shows the reference probability and the probabilities obtained for contrastive RD values obtained from inequality (9). It can be observed that the contrastive values for which probability raising holds yield a low probability of spillage. Therefore, selecting an alternative RD is unlikely to change the outcome. A similar result is observed from the actual causation analysis of FU. Figure <ref type="figure">7d</ref> indicates that selecting an alternative FU will unlikely change the outcome. In this example, the analysis of actual causation indicates that repeating the execution of the pouring action with the actual parameters is likely to succeed. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.3">Example 3</head><p>In this example, the source container was filled to a high level (F U = 0.77). The particles were poured into a target container of smaller capacity (RC = 0.79) and slightly smaller diameter (RD = 0.98). The actual trial parameters are shown in Figure <ref type="figure" target="#fig_9">8a</ref>. The outcomes over 100 replications with the actual parameters, shown in Figure <ref type="figure" target="#fig_9">8b</ref>, indicate a moderately larger probability of spillage (≈ 0.6).</p><p>Figure <ref type="figure" target="#fig_9">8c</ref> shows the reference probability and the probabilities obtained for contrastive RD values obtained from inequality (9). In the area where probability raising holds, the contrastive probabilities show a sharp decrease around RD ≈ 1.0 and a low probability for RD &gt; 1.1. We select the alternative values RD = 0.99 (chance-level probability) and RD = 1.02 (low probability ≈ 0.2), as shown in Figures <ref type="figure" target="#fig_9">8e</ref> and<ref type="figure" target="#fig_9">8g</ref>, respectively. The results from 100 replications are shown in Figure <ref type="figure" target="#fig_9">8f</ref> and Figure <ref type="figure" target="#fig_9">8h</ref>. The replications with the alternative parameters show that RD = 1.02 is better suited to avoid spillage.</p><p>Figure <ref type="figure" target="#fig_9">8d</ref> shows the reference probabilities and the probabilities obtained for contrastive FU values obtained from inequalities ( <ref type="formula" target="#formula_12">10</ref>) and ( <ref type="formula" target="#formula_13">11</ref>). In the area where probability raising holds, we observe low probabilities for F U &lt; 0.6.</p><p>For F U &gt; 0.6, we observe smooth probability increments. We select the alternative values F U = 0.75 (chance-level probability) and F U = 0.64 (low probability ≈ 0.2), as shown in Figures <ref type="figure" target="#fig_9">8i</ref> and<ref type="figure" target="#fig_9">8k</ref>, respectively. The results from 100 replications are shown in Figure <ref type="figure" target="#fig_9">8j</ref> and Figure <ref type="figure" target="#fig_9">8l</ref>. The replications with the alternative parameters show that F U = 0.64 is better suited to avoid spillage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.4">Example 4</head><p>In this example, the source container was filled to a high level (F U = 0.91). The particles were poured into a target container of smaller capacity (RC = 0.69) and slightly larger diameter (RD = 1.08). The actual trial parameters are shown in Figure <ref type="figure" target="#fig_10">9a</ref>. The outcomes over 100 replications with the actual parameters, shown in Figure <ref type="figure" target="#fig_10">9b</ref>, indicate a large probability of spillage.</p><p>Figure <ref type="figure" target="#fig_10">9c</ref> shows the reference probability and the probabilities obtained for contrastive RD values obtained from inequality (9). It can be observed that the contrastive values for which probability raising holds yield a high probability of spillage. Therefore, selecting an alternative RD is unlikely to change the outcome.</p><p>Figure <ref type="figure" target="#fig_10">9d</ref> shows the reference probabilities and the probabilities obtained for contrastive FU values obtained from inequalities (10) and ( <ref type="formula" target="#formula_13">11</ref>). In the area where probability raising holds, we observe low probabilities for F U &lt; 0.4. For F U &gt; 0.4, we observe smooth probability increments. We select the alternative values F U = 0.76 (chance-level probability) and F U = 0.53 (low probability ≈ 0.2), as shown in Figures <ref type="figure" target="#fig_10">9e</ref> and<ref type="figure" target="#fig_10">9g</ref>, respectively. The results from 100 replications are shown in Figure <ref type="figure" target="#fig_10">9f</ref> and Figure <ref type="figure" target="#fig_10">9h</ref>. The replications with the alternative parameters show that F U = 0.53 is better suited to avoid spillage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Evaluation of alternative actions to prevent spillage</head><p>In this section, we evaluate the capabilities of the analysis of probabilistic actual causation to guide the selection of alternative parameters to prevent spillage. For this evaluation, we generated a test dataset of 3000 pouring trials. The trial parameters of the test dataset were sampled from the same distributions used for the training dataset, as described in Table <ref type="table" target="#tab_1">2</ref>. Spillage occurred in 1216 trials and 1784 trials were successful.</p><p>Following steps described in Section 2.1, we conducted the analysis of probabilistic actual causation on the spillage trials. We identified the range of contrastive values where probability raising holds, and within these values, we selected the subset of values with probability of spillage&lt; 0.1. From this subset, the closest value to the current parameter was selected as alternative parameter. We ran the trial using the alternative parameter and recorded the outcome.</p><p>As explained in Section 2.1, the analysis of actual causation can be applied to different variables, one at a time. When inequality (1) does not hold for any of the contrastive values, the variable cannot be regarded as an actual cause. This indicates that there are no alternative values for the analyzed variable. Even if inequality (1) holds, it can also occur that the probabilities within the range of contrastive values where probability raising holds lay above the chance level or above the desired probability threshold (0.1 in our case). Therefore, it may happen that no alternative values for the variable being analyzed can be identified. To evaluate this aspect, we conducted the analysis of probabilistic actual causation on the RC, F U , and RD variables, and determined the percentage of the spillage trials for which an alternative parameter satisfying the probability threshold could be identified. For RC, an alternative parameter could be identified in 2.7% of the spillage trials, for F U in 59% of the spillage trials, and for RD in 97.9% of the spillage trials.</p><p>The marked differences between variables can be attributed to the causal structure of the task and the magnitude of the causal probabilities presented in Section 5.1. RC has an indirect effect on S through RV . F U also has an indirect effect on S through RV and a direct effect. Comparing the causal probabilities P (S|do(RC)) (Figure <ref type="figure" target="#fig_6">5a</ref>), with P (S|do(F U )) and P (S|do(F U, RV )) (Figures <ref type="figure" target="#fig_6">5b</ref> and<ref type="figure" target="#fig_6">5c</ref>, respectively), it can be observed that the effect of F U on S is stronger than the effect of RC. Under these considerations, finding an alternative RC value is less likely than finding an alternative F U value. In contrast to RC and F U , RD has a direct effect on S, and the P (S|do(RD)) (Figure <ref type="figure" target="#fig_6">5e</ref>) shows a range with low spillage probability, which leads to a higher likelihood of finding an alternative value.</p><p>Next, we evaluate the pouring success rates obtained by running the spillage trials using the alternative RD and F U values, while keeping the other variables unchanged<ref type="foot" target="#foot_3">foot_3</ref> . As explained at the beginning of this section, in each spillage trial we set RD or F U to an alternative value (closest to the current value) predicted to have a spillage probability &lt; 0.1 according to the actual causation analysis. A trial was successful if all the particles were poured into the target container. From 1216 spillage trials, an alternative RD value was identified for 1191 trials. Running these trials with the alternative DD values produced a success rate of 88.7%. From 1216 spillage trials, an alternative F U value was identified for 718 trials. Running these trials with the alternative F U values success rate of 86.9%. The success rates demonstrate the practical value of the actual causation approach in identifying alternative parameters.</p><p>Finally, we evaluate the empirical success rates observed when conducting trial replications using the alternative F U and RD values. For this, we selected a random subset of 100 spillage trials from the test dataset. We ran 10 replications of each trial of the subset using the alternative RD or F U values. As a result, we obtained an empirical success rate for each trial ( number of successf ul replications 10</p><p>). The histogram of the empirical success rates obtained with the alternative RD values is shown in Figure <ref type="figure" target="#fig_12">10a</ref>, and the histogram obtained for the alternative F U values is shown in Figure <ref type="figure" target="#fig_12">10b</ref>. In general, the observed success rates are in line with the probability criterion used to select the alternative values. Nevertheless, lower success rates of 0.6 and 0.7 were also obtained for some trials. Crucially, all the success rates are above the chance level, which provides empirical support for the usefulness of the alternative values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>In Section 5.2, an analysis of actual causation was conducted to determine the actual cause of spillage in a set of selected spillage trials. The analysis of the F U and RD variables was conducted individually. Based on the analysis, alternative F U and RD values were selected. In the examples, we also observed cases in which the analysis indicated that no alternative F U or RD values would significantly reduce the probability of spillage.</p><p>The actual causation probabilities exhibit a non-linear behavior. In particular, a sharp decrease in the spillage probability can be observed in some ranges of RD. Considering the sharpness in the transition from low to high probability of spillage observed in the causal probability P (S|do(RD)) (Figure <ref type="figure" target="#fig_6">5e</ref>) and in the actual causation probabilities in  examples 1, 2, and 3 (Figures <ref type="figure" target="#fig_7">6c, 7c,</ref> and<ref type="figure" target="#fig_9">8c</ref>, respectively), we verified that the sharp transition is not an artifact of the binary representation of the outcome. For this verification, we examined the number of particles spilled and the percentage of spilled particles as a function of RD in the range with sharp probability transitions. Within this range of values, small changes in RD yield significant changes in the number of spilled particles and the spilled percentage. As a result of the sharp probability decrease, when the analysis indicates that a RD is the actual cause of spillage, small changes in its value significantly impact the probability of spillage (cf. examples 1 and 3). The actual causation probabilities of F U also exhibit non-linear behavior, though the ranges where the probability of spillage transitions from low to high show a less pronounced slope (cf. example 4). Nevertheless, there are cases where a small change in F U leads to significant differences in the probability of spillage (cf. example 3).</p><p>The examples show that the probabilities of actual causation provide a principled criterion for comparing alternative actions with respect to the probability of the desired outcome. In the pouring task, small differences between values separate a "bad" from a "good" alternative action due to the non-linear effect of the variables on the outcome. Based on this consideration, it is reasonable to assume that the alternative actions identified using the automatic application of the actual causation analysis might differ from those chosen by a human observer. Consider the target container dimensions from example 3 compared in Figure <ref type="figure" target="#fig_13">11</ref>. A human observer might fail to realize that a slight change in diameter can significantly reduce the chances of spillage due to non-linear interaction between the task variables. Therefore, it can be assumed that a human observer will choose alternative actions based on larger parameter differences than the ones suggested by the analysis of actual causation. For a human observer, a significantly larger diameter difference or a lower fullness level might result from an implicit safety margin in the selection of an alternative action to avoid spillage. Overall, human reasoning about alternative solutions will hardly resemble the analysis of actual causation, as it relies on probabilistic reasoning about the (non-linear) interaction between variables.</p><p>Regarding the usage of actual causation for action guidance, it is important to consider the availability or feasibility of alternative actions in the context of application. In simulation, generating target containers of different dimensions or changing the source container's fullness levels is straightforward. However, the available alternative actions might be limited in the real world. For example, if target containers are available only in two diameter sizes, the selection of an alternative action is reduced to making a forced choice, leaving aside any reasoning about the effect of the diameters in a continuous space on the probability of spillage. Nevertheless, even if the alternatives are limited, an analysis of actual causation can provide the agent performing or monitoring the task with useful information about possible alternative actions.</p><p>It is important to recall that the results obtained from applying the analysis of actual causation depend on the structure of the causal graph and the variable representation. In this work, we opted to represent spillage as a binary variable. However, other representations of the outcome are possible. For example, spillage could be represented as the number of spilled marbles or as a relation, such as S = number of spilled marbles number of marbles in the source container . Defining spillage as a binary variable treats spilling one or many marbles equally. In this sense, the binary representation loses information regarding the severity of spillage. Ideally, the perfect realization of the task entails pouring without spillage. However, whether information about the severity of spillage is necessary depends on the context. For example, while spilling a few snacks at a party would not be a problem, spilling a single particle in a chemistry laboratory might be inadmissible. The previous situational examples emphasize that the context of the application must be considered when determining the definition of the outcome variable(s).</p><p>The representation of the variables has implications for the perceptual capabilities of the agent performing or monitoring the task, be it a human or a robot. The perception of the outcome and the container properties relies on sensory cues, which might consist of visual and force feedback. For example, to determine the number of spilled particles, the agent must be able to perceive and count individual particles. The extent to which this is feasible depends on the context (e.g., counting spilled candies might be way easier than counting spilled rice grains, both for a human or a robot). In this respect, representing spillage as a binary variable has the advantage of being easier to determine, both for a human and a robot, as it requires less perception, reasoning, and action capabilities. Overall, the successful implementation of action guidance based on the analysis of probabilistic actual causation in a real-world application relies on the availability of the information necessary to compute the probabilities in inequality (1). For the pouring task, the agent would need an accurate perception of both containers' dimensions, the source container's fullness level, and whether or not spillage occurred.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>In this paper, we conducted a probabilistic actual causation analysis of a robot pouring task. The modeling based on causal graphs and the estimation of conditional probability distributions using neural networks facilitated a qualitative and quantitative understanding of the influence of various factors on the task's outcome. Throughout a series of examples, we demonstrated that the analysis of actual causation provides a principled approach to check whether a variable is a cause of the outcome and to select alternative actions appropriate to change the observed outcome. Our results show that the analysis of actual causation provides information about the extent to which a variable caused an observed outcome that cannot be retrieved directly from simple causal probabilities. This occurs because the causal probability of a variable on the outcome lacks information about the context of the other variables. In contrast, the definition of probabilistic actual causation considers the context of the variables and their role in the causal structure (i.e., whether the variables are mediators or are outside the causal path). In the pouring task, the analysis of actual causation enabled us to determine whether a variable was a cause of spillage and, based on the assessment of the probabilities of actual causation, the selection of an alternative action parameter.</p><p>The reliability of the analysis of actual causation relies on the correctness of the causal graph structure and the estimated conditional probability distributions. This constitutes a major challenge for implementing an actual causation analysis in real-life tasks as it requires 1) a careful selection of the variables' representation, 2) determining the structure of the causal graph, and 3) estimating conditional probability distributions. The methods described in Section 4 constitute state-of-the art best common practices to obtain reliable and robust causal modeling results. Specifically, we used a realistic simulation of the pouring task to cover an ample combinatorial space of task parameters, which would have been cumbersome to replicate in a real environment. The simulation provided us with a large dataset to learn the causal structure of the task using a causal discovery algorithm with bootstrapping and to estimate its causal probability distributions using neural networks. In addition to the information provided in Section 4, in the supplementary material we discuss further modeling assumptions and we provide empirical support to the correctness of the causal model.</p><p>We demonstrated the practical use of probabilistic actual causation in a robotic task. In addition, the information retrieved from actual causation analysis can be used in the context of human-machine interaction to support human decision-making. Recalling that the actual causation probabilities can be interpreted as the extent to which an alternative action parameter is a "good" or "bad" corrective action, the framework can provide the human operator with additional information and contextual cues, for example, in augmented or virtual reality applications, to support the selection of action parameters.</p><p>In an additional scope of application, the analysis of actual causation can provide an objective baseline to evaluate the human perception of actual causes and the selection of alternative actions when a different outcome is sought. For example, the extent to which the causes perceived by a human observer correspond to the actual causes identified by the probabilistic framework can be investigated. Furthermore, the framework can be used to assess the extent to which human ratings of corrective actions in a continuum from "bad" to "good" correspond to the interpretation of the goodness of an alternative action parameter based on the actual causation probabilities used in this paper.   the stochastic effect of 1) the containers' characteristics (capacities and diameters, expressed as the variables RC and RD) and 2) the poured amount (fullness and volume, expressed as the variables F U and RV ) on the probability of spilling (variable S). The randomness of the outcome S results from the interplay between RC, RD, F U , and RV and the behavior of the particles during the pouring movement. The parameters the physics engine uses (particle size and density) to simulate the particles' behavior have a constant value. Therefore, they are not included as DAG variables.</p><p>Additionally, the pouring movement was executed with constant rotation velocity and angle. Thus, these were not included as DAG variables. Based on these considerations, we are confident that we included all the relevant random variables of the data-generating process of the simulation and assume that there are no latent confounders or other unobserved variables with a causal effect on spillage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Implementation of the neural autoregressive density estimators</head><p>The implementation of the NADE networks is based on the source code provided by <ref type="bibr" target="#b26">Garrido (2021)</ref>, publicly available in <ref type="url" target="https://github.com/Chechgm/causal_effect_estimation_using_nade">https://github.com/Chechgm/causal_effect_estimation_using_nade</ref> (access: 05.12.24). The following parameters were used:</p><p>• Neural network architecture: 2 hidden layers with 16 units each • Activation function: hyperbolic tangent (Tanh)</p><p>• Optimizer: RMSProp • Learning rate: 0.01</p><p>The neural networks were implemented using Pytorch <ref type="bibr" target="#b28">[Paszke et al., 2019]</ref> (Version 1.10.0).</p><p>The hyper-parameters were selected based on the results reported by <ref type="bibr" target="#b26">Garrido et al. [2021]</ref>. In their extensive analysis of the performance of different hyper-parameters (hidden layers, number of units, and learning rates), they conclude that no single combination of hyper-parameters is superior to others. We selected two hidden layers with 16 units. We noticed that increasing the number of layers and units did not reduce the loss or improve the prediction performance of the network. Based on this empirical observation, which is in line with the observations of <ref type="bibr" target="#b26">Garrido et al. [2021]</ref>, we abstained from conducting any further systematic search or comparison of the hyper-parameter space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Empirical support to the correctness of the causal model</head><p>The analysis of probabilistic actual causation relies on the correctness of the causal structure and the estimated causal probabilities. It is important to note that the causal structure of the data-generating process is unknown. Therefore, no ground truth causal graph is available to benchmark the structure and the causal probabilities learned from the training dataset.</p><p>In simulation, we have control over the parameters used in each pouring trial. The actual movement of the particles during the pouring movement, which determines whether or not spillage occurs, depends on the interaction between the trial parameters and the physics simulation. This interaction between the trial parameters and the spillage outcome is unknown. Therefore, the benchmarking of the causal graph can only be conducted on the level of evaluating the spillage predictions against the ground truth. We evaluate the spillage predictions on a test dataset of 3000 pouring trials. The trial parameters of the test dataset were sampled from the same distribution used for the training dataset (see description in Section "4.2 DAG Variables"). For each trial, we compute the causal probability of spillage P (S|do(F U, RC, RD, RV )). If P (S|do(F U, RC, RD, RV )) ≥ 0.5, the outcome prediction is labeled as S = T rue. We compare the predicted outcome with the actual outcome. The prediction results are summarized in the confusion matrix shown in Figure <ref type="figure" target="#fig_17">15</ref>. Among the spillage trials, the causal model predictions yield 90.8% true positives and 9.2% false negatives. On the other hand, the causal model predictions yield 94.5% true negatives and 5.5% false positives among the no-spillage trials. These results provide empirical support that the causal model corresponds with the ground truth data-generating process. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example of actual causation test. The curve corresponds to the right side of inequality (1) as a function of X = x ′ . The horizontal line corresponds to the reference probability value (left side of inequality (1). The shaded area shows the x ′ values for which probability raising holds.</figDesc><graphic coords="4,189.00,72.00,234.03,178.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Course of pouring trial. (a) Source container is filled with simulated marbles and a target container of random dimensions is generated. (b) The source container is transported to the pouring position. (c) The source container is rotated to pour the marbles into the target container. (d) Trial end.</figDesc><graphic coords="7,96.34,72.00,102.95,126.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>target capacity source capacity . Sampled from a truncated Gaussian distribution N trunc. (µ = 1.0, σ = 0.25, min = 0.5, max = 2.0). RC &lt; 1 corresponds to target containers of lower capacity, and RC &gt; 1 to target containers of larger capacity. fullness F U F U expresses the fullness level of the source container as a fraction. Sampled from N trunc. (µ = 0.7, σ = 0.2, min = 0.3, max = 1.0). For example, F U = 0.5 corresponds to a half-full source container. relative diameter RD RD represents the relation between the container rim diameters, defined as RD = target diameter source diameter . Sampled from N trunc. (µ = 1.0, σ = 0.25, min = 0.5, max = 1.5). RD &lt; 1 corresponds to target containers of smaller diameter, and RD &gt; 1 corresponds to target containers of larger diameter. The target container's height and diameter were determined based on the sampled RC and RD. relative volume RV</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure</head><label></label><figDesc>Figure 3: Discovered DAG</figDesc><graphic coords="9,247.50,72.00,117.01,112.31" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Implemented NADEs for the DAG nodes and auxiliary NADE used for causal effect estimation</figDesc><graphic coords="10,72.00,72.00,468.01,148.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>(a) Causal probability of RC on S. (b) Total causal probability of F U on S. (c) Direct causal probabilty of F U on S. (d) Causal probability of RV on S. (e) Causal probability of RD on S.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Causal probabilities of DAG variables.</figDesc><graphic coords="12,164.35,217.65,140.42,107.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Actual causation analysis of example 1. (a) Actual trial parameters of trial with spillage and (b) outcome frequencies over 100 replications. (c) Probabilities of actual causation inequality for RD. (d) Probabilities of actual causation inequality for F U . (e) Alternative RD for 0.5 spillage probability and (f) outcome frequencies over 100 replications. (g) Alternative RD for 0.2 spillage probability and (h) outcome frequencies over 100 replications.</figDesc><graphic coords="14,77.62,225.40,112.32,112.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Figure 7: Actual causation analysis of example 2. (a) Actual trial parameters of trial with spillage and (b) outcome frequencies over 100 replications. (c) Probabilities of actual causation inequality for RD. (d) Probabilities of actual causation inequality for F U .</figDesc><graphic coords="14,307.25,536.08,112.33,97.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Actual causation analysis of example 3. (a) Actual trial parameters of trial with spillage and (b) outcome frequencies obtained over 100 replications. (c) Probabilities of actual causation inequality for RD. (d) Probabilities of actual causation inequality for F U . (e) Alternative RD for 0.5 spillage probability and (f) outcome frequencies over 100 replications. (g) Alternative RD for 0.2 spillage probability and (h) outcome frequencies over 100 replications. (i) Alternative F U for 0.5 spillage probability and (j) outcome frequencies over 100 replications. (k) Alternative F U for 0.2 spillage probability and (l) outcome frequencies over 100 replications.</figDesc><graphic coords="16,77.62,379.95,112.32,112.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Actual causation analysis of example 4. (a) Actual trial parameters of trial with spillage and (b) outcome frequencies over 100 replications. (c) Probabilities of actual causation inequality for RD. (d) Probabilities of actual causation inequality for F U . (e) Alternative F U for 0.5 spillage probability and (f) outcome frequencies over 100 replications. (g) Alternative F U for 0.2 spillage probability and (h) outcome frequencies over 100 replications.</figDesc><graphic coords="17,77.62,225.97,112.32,112.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>(a) Empirical success rates using alternative RD values. (b) Empirical success rates using alternative F U values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Empirical success rates obtained over 10 replications.</figDesc><graphic coords="18,94.16,72.00,210.61,162.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Actual causation probabilities, actual trial parameters and target containers with different RD values.</figDesc><graphic coords="19,72.00,72.00,468.06,265.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: PC and DG parameters specified in Tetrad.</figDesc><graphic coords="23,164.27,114.45,283.46,281.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: Background knowledge specified in Tetrad.</figDesc><graphic coords="23,178.44,511.94,255.12,142.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: Discovered edge type frequencies.</figDesc><graphic coords="24,135.92,72.00,340.18,170.09" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 15 :</head><label>15</label><figDesc>Figure 15: Confusion matrix.</figDesc><graphic coords="25,206.79,205.17,198.43,131.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Comparison of methods</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Similarities</cell><cell></cell><cell></cell><cell>Differences</cell></row><row><cell></cell><cell>Model</cell><cell>Training</cell><cell>Type of</cell><cell>Selection</cell><cell>Input</cell><cell>Parameter</cell><cell>Corrected</cell><cell>Contrasted</cell></row><row><cell></cell><cell></cell><cell>data</cell><cell>expla-</cell><cell>of</cell><cell></cell><cell>space</cell><cell>variables</cell><cell>probabili-</cell></row><row><cell></cell><cell></cell><cell></cell><cell>nation</cell><cell>corrective</cell><cell></cell><cell></cell><cell></cell><cell>ties</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>action</cell><cell></cell><cell></cell><cell></cell></row><row><cell>Diehl</cell><cell>Causal</cell><cell cols="3">Simulation Contrastive Based on a</cell><cell>Current values of all</cell><cell cols="2">Discrete The corrective</cell><cell>Conditional</cell></row><row><cell>and</cell><cell>Bayesian</cell><cell></cell><cell></cell><cell>probability</cell><cell>variables (i.e.,</cell><cell></cell><cell>parametriza-</cell><cell>probabili-</cell></row><row><cell>Ramirez-</cell><cell>network</cell><cell></cell><cell></cell><cell>threshold</cell><cell>parametrization) and</cell><cell></cell><cell>tion might</cell><cell>ties from</cell></row><row><cell>Amaro</cell><cell>(discrete</cell><cell></cell><cell></cell><cell></cell><cell>probability of the</cell><cell></cell><cell>involve</cell><cell>the</cell></row><row><cell>[2023]</cell><cell>variables)</cell><cell></cell><cell></cell><cell></cell><cell>outcome variable</cell><cell></cell><cell>changing one</cell><cell>factorized</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>or more</cell><cell>joint</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>variables</cell><cell>probability</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>distribution</cell></row><row><cell>Ours</cell><cell>Causal</cell><cell cols="3">Simulation Contrastive Based on a</cell><cell>Current values of all</cell><cell cols="2">ContinuousThe corrective</cell><cell>Causal</cell></row><row><cell></cell><cell>Bayesian</cell><cell></cell><cell></cell><cell>probability</cell><cell>variables (i.e.,</cell><cell></cell><cell>parametriza-</cell><cell>probabili-</cell></row><row><cell></cell><cell>network</cell><cell></cell><cell></cell><cell>threshold</cell><cell>parametrization),</cell><cell></cell><cell>tion changes</cell><cell>ties</cell></row><row><cell></cell><cell>(continuous and</cell><cell></cell><cell></cell><cell></cell><cell>causal probability</cell><cell></cell><cell>one variable</cell><cell>(do-</cell></row><row><cell></cell><cell>discrete</cell><cell></cell><cell></cell><cell></cell><cell>(do-operator) of the</cell><cell></cell><cell></cell><cell>operator)</cell></row><row><cell></cell><cell>variables)</cell><cell></cell><cell></cell><cell></cell><cell>outcome variable,</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>and variable to</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>correct</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Variable definitions and sampling of trial parameters</figDesc><table><row><cell>Variable</cell><cell>Acronym Definition and sampling distribution</cell></row><row><cell></cell><cell>RC represents the relation between the containers' capacities, defined as RC =</cell></row><row><cell>relative</cell><cell>RC</cell></row><row><cell>capacity</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>The notation and terminology follow the definition PC1 presented byFenton-Glynn [2021, p. 72]   </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>The conceptual analysis conducted by<ref type="bibr" target="#b1">Beckers [2022]</ref> originally focused on notions of causal explanations in the context of deterministic causal models. Nevertheless, we consider that his conceptual accounts can be applied to probabilistic causal models.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>Publicly available at: https://www.ccd.pitt.edu/tools/ , access: 06.12.24</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>RC was not considered for this evaluation due to the low number of trials for which an alternative value was found.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>The implementation of the neural autoregressive density estimators was possible thanks the collaborative work and support of <rs type="person">Konrad Gadzicki</rs> (<rs type="projectName">EASE -subproject H03 "Discriminative and Generative Human Activity Models for Cognitive Architectures</rs>").</p></div>
			</div>
			<div type="funding">
<div><head>Funding</head><p>The research reported in this paper has been supported by the <rs type="funder">German Research Foundation DFG, as part of Collaborative Research Center (Sonderforschungsbereich)</rs> <rs type="grantNumber">1320 Project-ID 329551904</rs> "<rs type="programName">EASE -Everyday Activity Science and Engineering"</rs>, <rs type="institution">University of Bremen</rs> (<ref type="url" target="http://www.ease-crc.org/">http://www.ease-crc.org/</ref>). The research was conducted in subproject H01 "Sensorimotor and <rs type="funder">Causal Human Activity Models for Cognitive Architectures</rs>."</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_vU8GPmD">
					<orgName type="project" subtype="full">EASE -subproject H03 &quot;Discriminative and Generative Human Activity Models for Cognitive Architectures</orgName>
				</org>
				<org type="funding" xml:id="_bKraCTy">
					<idno type="grant-number">1320 Project-ID 329551904</idno>
					<orgName type="program" subtype="full">EASE -Everyday Activity Science and Engineering&quot;</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conflict of Interest Statement</head><p>The authors declare that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Author Contributions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Material</head><p>7.1 Causal discovery using the PC algorithm -Tetrad setup and bootstrapping results</p><p>Figure <ref type="figure">12</ref> shows the exact parameters used in Tetrad to run the PC algorithm. The specification of background knowledge in the form of tiers is shown in Figure <ref type="figure">13</ref>. Note that within-tier edges are allowed in the background knowledge.</p><p>To validate the causal relationships inferred from the data, we performed a bootstrapping analysis. Conducting a bootstrapping analysis is recommended for causal discovery <ref type="bibr" target="#b23">[Malinsky and</ref><ref type="bibr">Danks, 2017, Glymour et al., 2019]</ref>. We ran the PC algorithm on 1000 bootstraps, producing 1000 different structures. The edge-type frequencies obtained from bootstrapping indicate whether the discovered causal relationships are stable across different bootstrap samples <ref type="bibr" target="#b21">[Glymour et al., 2019]</ref>. In Figure <ref type="figure">14</ref>, we report the frequency of the edge types between variables. For ease of interpretation and comparison, the frequency of edge type is reported as a proportion of the number of bootstraps. Given the different edge types (including "no edge"), we interpret an edge frequency larger than 0.5 as stable. The reported DAG, termed discovered DAG includes the stable edges and no-edges. The PC algorithm discovered direct edges, edges that are definitely direct (DD), and undirected edges (i.e., the data are consistent with X → Y and X ← Y <ref type="bibr" target="#b23">[Malinsky and Danks, 2017]</ref>).</p><p>The reported DAG was constructed from the edges with frequencies larger than 0.5. As shown in Figure <ref type="figure">14</ref>, the discovered DAG results from edge frequencies larger than 0.75. This increases our confidence in the correctness of the discovered causal structure, which is crucial for the analysis of probabilistic actual causation.</p><p>Internally, we also conducted the causal discovery analysis with bootstrapping using Tetrad's GFCI, FGES, GRASP, and BOSS algorithm implementations (for all algorithms, we used the Degenerate Gaussian Likelihood Ratio Test (DG-LRT)). The edge frequencies slightly vary across algorithms, but the discovered structure is the same. For simplicity, we opted to report the PC results since it is a well-established algorithm with extensively studied properties <ref type="bibr" target="#b23">[Malinsky and</ref><ref type="bibr">Danks, 2017, Glymour et al., 2019]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Assumption of no latent variables</head><p>The PC algorithm assumes no latent confounders or unobserved variables. In this respect, we rely on our analysis of the task and the data-generating process to support this assumption.</p><p>Based on the analysis of the task described in Section "4.2 DAG Variables" we assume that the variables used to represent the data-generating process of the simulation capture all the relevant causes of spillage. The variables represent</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The Actual Cause, chapter 10</title>
		<author>
			<persName><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
		<idno type="DOI">10.1017/cbo9780511803161.012</idno>
	</analytic>
	<monogr>
		<title level="m">a. ISBN 9780521749190</title>
		<meeting><address><addrLine>Cambridge</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2009-09">September 2009</date>
			<biblScope unit="page" from="309" to="330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Causal explanations and XAI</title>
		<author>
			<persName><forename type="first">Sander</forename><surname>Beckers</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v177/beckers22a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Conference on Causal Learning and Reasoning</title>
		<editor>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Caroline</forename><surname>Uhler</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</editor>
		<meeting>the First Conference on Causal Learning and Reasoning</meeting>
		<imprint>
			<date type="published" when="2022-04">Apr 2022</date>
			<biblScope unit="volume">177</biblScope>
			<biblScope unit="page" from="11" to="13" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Causation</title>
		<author>
			<persName><forename type="first">Luke</forename><surname>Fenton-Glynn</surname></persName>
		</author>
		<idno type="DOI">10.1017/9781108588300</idno>
		<imprint>
			<date type="published" when="2021-06">June 2021</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Causes and explanations: A structural-model approach. part i: Causes</title>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">Y</forename><surname>Halpern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
		<idno type="DOI">10.1093/bjps/axi147</idno>
	</analytic>
	<monogr>
		<title level="j">The British Journal for the Philosophy of Science</title>
		<idno type="ISSN">1464-3537</idno>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="843" to="887" />
			<date type="published" when="2005-12">December 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Causal explanations -how to generate, identify, and evaluate them</title>
		<author>
			<persName><forename type="first">Jan</forename><surname>Borner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A proposed probabilistic extension of the Halpern and Pearl definition of &apos;actual cause</title>
		<author>
			<persName><forename type="first">Luke</forename><surname>Fenton-Glynn</surname></persName>
		</author>
		<idno type="DOI">10.1093/bjps/axv056</idno>
	</analytic>
	<monogr>
		<title level="j">The British Journal for the Philosophy of Science</title>
		<idno type="ISSN">1464-3537</idno>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1061" to="1124" />
			<date type="published" when="2017-12">December 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Causal Diagrams and the Identification of Causal Effects</title>
		<author>
			<persName><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
		<idno type="DOI">10.1017/cbo9780511803161.005</idno>
	</analytic>
	<monogr>
		<title level="m">b. ISBN 9780521749190</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2009-09">September 2009</date>
			<biblScope unit="page" from="65" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A causal-based approach to explain, predict and prevent failures in robotic tasks</title>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Diehl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karinne</forename><surname>Ramirez-Amaro</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.robot.2023.104376</idno>
	</analytic>
	<monogr>
		<title level="j">Robotics and Autonomous Systems</title>
		<idno type="ISSN">0921-8890</idno>
		<imprint>
			<biblScope unit="volume">162</biblScope>
			<biblScope unit="page">104376</biblScope>
			<date type="published" when="2023-04">April 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Araujo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Holthaus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marina</forename><surname>Sarda Gou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriella</forename><surname>Lakatos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giulia</forename><surname>Galizia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Wood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Robins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Reza</forename><surname>Mousavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Farshid</forename><surname>Amirabdollahian</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-24670-8_9</idno>
	</analytic>
	<monogr>
		<title level="j">Kaspar Causally Explains</title>
		<imprint>
			<biblScope unit="page" from="85" to="99" />
			<date type="published" when="2022">2022</date>
			<publisher>Springer Nature Switzerland</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Building causal models for finding actual causes of unmanned aerial vehicle failures</title>
		<author>
			<persName><forename type="first">Ehsan</forename><surname>Zibaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robin</forename><surname>Borth</surname></persName>
		</author>
		<idno type="DOI">10.3389/frobt.2024.1123762</idno>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Robotics and AI</title>
		<idno type="ISSN">2296-9144</idno>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<date type="published" when="2024-02">February 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Explanations for occluded images</title>
		<author>
			<persName><forename type="first">Hana</forename><surname>Chockler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Kroening</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youcheng</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2021-10">October 2021</date>
			<biblScope unit="page" from="1234" to="1243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Explaining image classifiers</title>
		<author>
			<persName><forename type="first">Hana</forename><surname>Chockler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">Y</forename><surname>Halpern</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2401.13752</idno>
		<imprint>
			<date type="published" when="2024-01">January 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Towards unifying feature attribution and counterfactual explanations: Different means to the same end</title>
		<author>
			<persName><forename type="first">Ramaravind</forename><surname>Kommiya Mothilal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Divyat</forename><surname>Mahajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenhao</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amit</forename><surname>Sharma</surname></persName>
		</author>
		<idno type="DOI">10.1145/3461702.3462597</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society, AIES &apos;21</title>
		<meeting>the 2021 AAAI/ACM Conference on AI, Ethics, and Society, AIES &apos;21</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2021-07">July 2021</date>
			<biblScope unit="page" from="652" to="663" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Ossama</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frederik</forename><surname>Träuble</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Neitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manuel</forename><surname>Wüthrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Bauer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.04296</idno>
		<title level="m">Causalworld: A robotic manipulation benchmark for causal structure and transfer learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A causal approach to tool affordance learning</title>
		<author>
			<persName><forename type="first">Jake</forename><surname>Brawer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meiying</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Scassellati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE/RSJ international conference on intelligent robots and systems (IROS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8394" to="8399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">What went wrong? closing the sim-to-real gap via differentiable causal discovery</title>
		<author>
			<persName><forename type="first">Peide</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xilun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziang</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiqi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengdi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhao</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Francis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingqing</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ding</forename><surname>Zhao</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="https://proceedings.mlr.press/v229/huang23c.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 7th Conference on Robot Learning</title>
		<editor>
			<persName><forename type="first">Jie</forename><surname>Tan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Marc</forename><surname>Toussaint</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Kourosh</forename><surname>Darvish</surname></persName>
		</editor>
		<meeting>The 7th Conference on Robot Learning</meeting>
		<imprint>
			<date type="published" when="2023-11">November 2023</date>
			<biblScope unit="volume">229</biblScope>
			<biblScope unit="page" from="734" to="760" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Causal discovery of dynamic models for predicting human spatial interactions</title>
		<author>
			<persName><forename type="first">Luca</forename><surname>Castri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sariah</forename><surname>Mghames</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Hanheide</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicola</forename><surname>Bellotto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Social Robotics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="154" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Operation mode decision of indoor cleaning robot based on causal reasoning and attribute learning</title>
		<author>
			<persName><forename type="first">Yapeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongbo</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="173376" to="173386" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Why did i fail? a causal-based method to find explanations for robot failures</title>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Diehl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karinne</forename><surname>Ramirez-Amaro</surname></persName>
		</author>
		<idno type="DOI">10.1109/lra.2022.3188889</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Robotics and Automation Letters</title>
		<idno type="ISSN">2377-3774</idno>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="8925" to="8932" />
			<date type="published" when="2022-10">October 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">V-rep: A versatile and scalable robot simulation framework</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Rohmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">N</forename><surname>Surya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><surname>Freese</surname></persName>
		</author>
		<idno type="DOI">10.1109/iros.2013.6696520.www.coppeliarobotics.com</idno>
	</analytic>
	<monogr>
		<title level="j">RSJ International Conference on Intelligent Robots and Systems. IEEE</title>
		<imprint>
			<date type="published" when="2013-11">2013. November 2013</date>
		</imprint>
	</monogr>
	<note>IEEE</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">An algorithm for fast recovery of sparse causal graphs</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clark</forename><surname>Glymour</surname></persName>
		</author>
		<idno type="DOI">10.1177/089443939100900106</idno>
	</analytic>
	<monogr>
		<title level="j">Social Science Computer Review</title>
		<idno type="ISSN">1552-8286</idno>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="62" to="72" />
			<date type="published" when="1991-04">April 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Review of causal discovery methods based on graphical models</title>
		<author>
			<persName><forename type="first">Clark</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Spirtes</surname></persName>
		</author>
		<idno type="DOI">10.3389/fgene.2019.00524</idno>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Genetics</title>
		<idno type="ISSN">1664-8021</idno>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<date type="published" when="2019-06">June 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Methods and tools for causal discovery and causal inference</title>
		<author>
			<persName><forename type="first">Ana</forename><surname>Rita Nogueira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Pugnana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salvatore</forename><surname>Ruggieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dino</forename><surname>Pedreschi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">João</forename><surname>Gama</surname></persName>
		</author>
		<idno type="DOI">10.1002/widm.1449</idno>
	</analytic>
	<monogr>
		<title level="j">WIREs Data Mining and Knowledge Discovery</title>
		<idno type="ISSN">1942-4795</idno>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2022-01">January 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Causal discovery algorithms: A practical guide</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Malinsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Danks</surname></persName>
		</author>
		<idno type="DOI">10.1111/phc3.12470</idno>
	</analytic>
	<monogr>
		<title level="j">Philosophy Compass</title>
		<idno type="ISSN">1747-9991</idno>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2017-11">November 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Tetrad-a toolbox for causal discovery</title>
		<author>
			<persName><forename type="first">Kun</forename><surname>Joseph D Ramsey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madelyn</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruben</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Biwei</forename><surname>Sanchez Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Imme</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Savini</forename><surname>Ebert-Uphoff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elizabeth</forename><forename type="middle">A</forename><surname>Samarasinghe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clark</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName><surname>Glymour</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
	<note>th international workshop on climate informatics</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning high-dimensional directed acyclic graphs with mixed data-types</title>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Ramsey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><forename type="middle">F</forename><surname>Cooper</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v104/andrews19a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning Research</title>
		<meeting>Machine Learning Research</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="page" from="4" to="21" />
		</imprint>
	</monogr>
	<note>PMLR, 05 Aug</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Estimating causal effects with the neural autoregressive density estimator</title>
		<author>
			<persName><forename type="first">Sergio</forename><surname>Garrido</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stanislav</forename><surname>Borysov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeppe</forename><surname>Rich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Pereira</surname></persName>
		</author>
		<idno type="DOI">10.1515/jci-2020-0007</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Causal Inference</title>
		<idno type="ISSN">2193-3685</idno>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="211" to="228" />
			<date type="published" when="2021-01">January 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Identifying causal effects with the R package causaleffect</title>
		<author>
			<persName><forename type="first">Santtu</forename><surname>Tikka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juha</forename><surname>Karvanen</surname></persName>
		</author>
		<idno type="DOI">10.18637/jss.v076.i12</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Statistical Software</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="1" to="30" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Py-Torch: an imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Köpf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zach</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper_files/paper/2019/file/" />
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>Curran Associates Inc</publisher>
			<biblScope unit="page" from="8026" to="8037" />
			<pubPlace>Red Hook, NY, USA</pubPlace>
		</imprint>
	</monogr>
	<note>bdbca288fee7f92f2bfa9f7012727740-Paper.pdf</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
