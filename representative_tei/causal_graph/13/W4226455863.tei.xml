<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">FedDAG: Federated DAG Structure Learning</title>
				<funder ref="#_mPPcP5p">
					<orgName type="full">Australian</orgName>
				</funder>
				<funder>
					<orgName type="full">The University of Melbourne&apos;s Research Computing Services</orgName>
				</funder>
				<funder ref="#_mJ6nsjc">
					<orgName type="full">Petascale Campus Initiative</orgName>
				</funder>
				<funder ref="#_2takRcD">
					<orgName type="full">Major Science and Technology Innovation 2030 &quot;Brain Science and Brain-like Research&quot; key project</orgName>
				</funder>
				<funder ref="#_J5Tpy2q #_dxbJ9dk #_zPTyv3g">
					<orgName type="full">ARC</orgName>
				</funder>
				<funder ref="#_Kc7nNmG">
					<orgName type="full">Australian Research Council</orgName>
				</funder>
				<funder ref="#_KQ6VxUh #_uWbcJ9B #_4DfBWaG">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2023-01-17">17 Jan 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Erdun</forename><surname>Gao</surname></persName>
							<email>erdun.gao@student.unimelb.edu.au</email>
						</author>
						<author>
							<persName><forename type="first">Junjia</forename><surname>Chen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Li</forename><surname>Shen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
							<email>tongliang.liu@sydney.edu.au</email>
						</author>
						<author>
							<persName><forename type="first">Howard</forename><surname>Bondell</surname></persName>
							<email>howard.bondell@unimelb.edu.au</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Mathematics and Statistics</orgName>
								<orgName type="department" key="dep2">Faculty of Electronic and Information Engineering</orgName>
								<orgName type="institution">The University of Melbourne</orgName>
								<address>
									<settlement>Xi&apos;an</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Sydney AI Centre</orgName>
								<orgName type="department" key="dep2">Department of Machine Learning</orgName>
								<orgName type="department" key="dep3">Mohamed bin Zayed</orgName>
								<orgName type="department" key="dep4">School of Mathematics and Statistics</orgName>
								<orgName type="department" key="dep5">School of Mathematics and Statistics</orgName>
								<orgName type="laboratory">TML Lab</orgName>
								<orgName type="institution" key="instit1">Jiaotong University</orgName>
								<orgName type="institution" key="instit2">The University of Sydney</orgName>
								<orgName type="institution" key="instit3">University of Artificial Intelligence Mingming Gong</orgName>
								<orgName type="institution" key="instit4">The University of Melbourne</orgName>
								<orgName type="institution" key="instit5">The University of Melbourne</orgName>
								<address>
									<addrLine>0 25 10 0 10 25 0 25 2.5 0.0 2.5 25 0 25 0 10 25 0 25 0 20 25 0 25</addrLine>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">FedDAG: Federated DAG Structure Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-01-17">17 Jan 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2112.03555v3[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-21T20:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>https:</term>
					<term>openreview. net</term>
					<term>forum? id= MzWgBjZ6Le</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>To date, most directed acyclic graphs (DAGs) structure learning approaches require data to be stored in a central server. However, due to the consideration of privacy protection, data owners gradually refuse to share their personalized raw data to avoid private information leakage, making this task more troublesome by cutting off the first step. Thus, a puzzle arises: how do we discover the underlying DAG structure from decentralized data? In this paper, focusing on the additive noise models (ANMs) assumption of data generation, we take the first step in developing a gradient-based learning framework named FedDAG, which can learn the DAG structure without directly touching the local data and also can naturally handle the data heterogeneity. Our method benefits from a two-level structure of each local model. The first level structure learns the edges and directions of the graph and communicates with the server to get the model information from other clients during the learning procedure, while the second level structure approximates the mechanisms among variables and personally updates on its own data to accommodate the data heterogeneity. Moreover, FedDAG formulates the overall learning task as a continuous optimization problem by taking advantage of an equality acyclicity constraint, which can be solved by gradient descent methods to boost the searching efficiency. Extensive experiments on both synthetic and real-world datasets verify the efficacy of the proposed method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Bayesian Networks (BNs) have become prevalent over the last few decades by leveraging the graph theory and probability theory to model the probabilistic relationships among a set of random variables, which can potentially provide a mechanism for evidential reasoning <ref type="bibr" target="#b49">(Pearl, 1985)</ref>. The success of BNs has contributed to a furry of downstream real-world application problems in econometrics <ref type="bibr" target="#b16">(Heckman, 2008)</ref>, epidemiology <ref type="bibr" target="#b14">(Greenland et al., 1999)</ref>, biological sciences <ref type="bibr" target="#b21">(Imbens &amp; Rubin, 2015)</ref> and social sciences (Marini  <ref type="bibr">(Ng et al., 2022b</ref>) separately trains one model on local data while MCSL (All) trains one model on all data, which is forbidden in FL.</p><p>Each BN is defined by a directed acyclic graph (DAG) and a set of parameters to depict the direction, strength, and shape of the mechanisms between variables <ref type="bibr" target="#b28">(Kitson et al., 2021)</ref>. Over the recent decades, a bunch of methods <ref type="bibr" target="#b62">(Spirtes et al., 2001;</ref><ref type="bibr" target="#b6">Chickering, 2002;</ref><ref type="bibr" target="#b83">Zheng, 2020)</ref> for discovering the DAG structure encoded among concerned events from the observational data have been proposed. In practice, however, finite sample problem bears the brunt of the performance decrease of DAG structure learning methods. Regularly (1) collecting data from various sources and then (2) designing the structure learning algorithm on all collected data can serve as a direct and standard pipeline to alleviate this issue in this field. However, owing to the issue of data privacy, data owners gradually prefer not to share their personalized data<ref type="foot" target="#foot_0">foot_0</ref> with others <ref type="bibr" target="#b24">(Kairouz et al., 2021)</ref>. Naturally, the new predicament, how do we discover the underlying DAG structure from decentralized data? has arisen. In statistical learning problems such as regression and classification, federated learning (FL) has been proposed to learn from locally stored data <ref type="bibr" target="#b37">(McMahan et al., 2017)</ref>. Inspired by the developments in FL, we aim to develop a federated DAG structure learning framework that enables learning the graph structure from the decentralized data. Compared to the traditional FL methods in statistical learning, federated DAG learning, a structural learning task, has the following two main differences:</p><p>‚Ä¢ Learning objective difference. Most of the previous FL researches focus on learning an estimator to estimate the conditional distribution P (Y |X) in supervised learning tasks, e.g., image classification <ref type="bibr" target="#b37">(McMahan et al., 2017;</ref><ref type="bibr" target="#b32">Li et al., 2020)</ref>, sequence tagging <ref type="bibr">(Lin et al., 2022)</ref>, and feature prediction <ref type="bibr" target="#b24">(Kairouz et al., 2021)</ref>. However, DAG structure learning, an unsupervised learning task <ref type="bibr" target="#b13">(Glymour et al., 2019)</ref>, tries to find the underlying graph structure among the concerned variables and the relationship mechanisms estimators to fit with the joint distribution of observations.</p><p>‚Ä¢ Data heterogeneity difference. The setup of data heterogeneity in FL is mainly assumed to be caused by some specific distribution shifts such as label shift (the shift of P (Y )) <ref type="bibr" target="#b34">Lipton et al. (2018)</ref> or covariate shift (the shift of P (X)) <ref type="bibr" target="#b56">Reisizadeh et al. (2020)</ref>, while federated DAG learning handles a generative model, which can allow the data heterogeneity resulted from the joint distribution shift of all variables (Figure <ref type="figure" target="#fig_0">1</ref>(a)). This would bring more challenges than the FL paradigm model design.</p><p>the random noise. M can be leveraged to describe how nature assigns values to variables of interest <ref type="bibr" target="#b50">(Pearl et al., 2016)</ref>. In this paper, we focus on a commonly used model named ANMs. They assume that</p><formula xml:id="formula_0">X i = f i (PA i ) + i , i = 1, 2, ‚Ä¢ ‚Ä¢ ‚Ä¢ , d, (<label>1</label></formula><formula xml:id="formula_1">)</formula><p>where i is independent of variables in PA i and mutually independent with any j for i = j.</p><p>Bayesian Networks (BNs). Let X = (X 1 , X 2 , ‚Ä¢ ‚Ä¢ ‚Ä¢ , X d ) be a vector that includes all variables in X with the index set V := {1, 2, ‚Ä¢ ‚Ä¢ ‚Ä¢ , d} and P (X) with the probability density function p(X) be a marginal distribution induced from M. A DAG G = (V, E) consists of a nodes set V and an edge set E ‚äÜ V<ref type="foot" target="#foot_1">foot_1</ref> . Every SEM M can be associated with a DAG G M , in which each node i corresponds to the variable X i and directed edges point from PA i to X i 2 for i ‚àà [d]<ref type="foot" target="#foot_2">foot_2</ref> . A BN is defined as a pair P (X), G M . Then G M is called the graph structure associated with M and P (X) is Markovian to G M . Throughout the main text, we assume that there is no latent variable<ref type="foot" target="#foot_3">foot_3</ref>  <ref type="bibr" target="#b62">(Spirtes et al., 2001)</ref> and then p(X) can be factorized as</p><formula xml:id="formula_2">p(X) = d i=1 p(X i |X pai ) (2)</formula><p>according to G M <ref type="bibr" target="#b31">(Lauritzen, 1996)</ref>. X pai is the parental vector that includes all variables in PA i . . NOTEARS <ref type="bibr" target="#b84">(Zheng et al., 2018)</ref> formulates a sufficient and necessary condition for B representing a DAG by an equation. The formulation is as follows:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Characterizations of</head><p>Tr</p><formula xml:id="formula_3">[e B ] -d = 0,<label>(3)</label></formula><p>where Tr <ref type="bibr">[‚Ä¢]</ref> means the trace of a given matrix. e (‚Ä¢) , here, is the matrix exponential operation. However, NOTEARS is only designed to solve the linear Gaussian models, which assume that all relationship mechanisms are linear. Therefore, the DAG structure and relationship mechanisms can be modeled together by a weighted matrix. To extend NOTEARS to the non-linear cases, MCSL <ref type="bibr">(Ng et al., 2022b)</ref> proposes to use a mask M , parameterized by a continuous proxy matrix U , to approximate the adjacency matrix B. To enforce the entries of M to approximate the binary form, i.e., 0 or 1, a two-dimensional version of Gumbel-Softmax <ref type="bibr" target="#b22">(Jang et al., 2017)</ref> approach named Gumbel-Sigmoid is designed to reparameterize U and to ensure the differentiability of the model. Then, M can be obtained element-wisely by</p><formula xml:id="formula_4">M ij = 1 1 + exp(-log(U ij + Gumb ij )/œÑ ) , (<label>4</label></formula><formula xml:id="formula_5">)</formula><p>where œÑ is the temperature, Gumb ij = g 1 ij -g 0 ij , g 1 ij and g 0 ij are two independent samples from Gumbel(0, 1). For simplicity but equivalence, g 1 ij and g 0 ij also can be sampled from -log(log(a)) with a ‚àº Uniform(0, 1). See Appendix D in <ref type="bibr">(Ng et al., 2022b)</ref>. MCSL names Eq. ( <ref type="formula" target="#formula_4">4</ref>) as Gumbel-Sigmoid w.r.t. U and temperature œÑ , which is written as g œÑ (U ). Then, the acyclicity constraint can be reformulated as</p><p>Tr[e (gœÑ (U )) ] -d = 0.</p><p>(5)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Problem definition</head><p>Here, we first describe the property of decentralized data and the data distribution shift among different clients if there exists data heterogeneity <ref type="bibr">(Huang et al., 2020b;</ref><ref type="bibr" target="#b39">Mooij et al., 2020;</ref><ref type="bibr" target="#b82">Zhang et al., 2020)</ref>. Then, we define the problem, federated DAG structure learning, considered in this paper.</p><p>Decentralized data and probability distribution set. Let C = {c 1 , c 2 , ‚Ä¢ ‚Ä¢ ‚Ä¢ , c m } be the client set which includes m different clients and s be the only server. The data D c k ‚àà R nc k √ód , in which each observation D c k i for ‚àÄi ‚àà [n c k ] independently sampled from its corresponding probability distribution P c k (X), represent the personalized data owned by the client c k . n c k is the number of observations in D c k . The dataset D = {D c1 , D c2 , ‚Ä¢ ‚Ä¢ ‚Ä¢ , D cm } is called a decentralized dataset and P C (X) = {P c1 (X), P c2 (X), ‚Ä¢ ‚Ä¢ ‚Ä¢ , P cm (X)} is defined as the decentralized probability distribution set.</p><formula xml:id="formula_6">If P c k 1 (X) = P c k 2 (X) for ‚àÄ k 1 , k 2 ‚àà [m]</formula><p>, then D is defined as a homogeneous decentralized dataset throughout this paper. The heterogeneous decentralized dataset is defined by assuming that there exists at least two clients, e.g., c k1 and c k2 , on which the local data are sampled from different distributions, i.e., P c k 1 (X) = P c k 2 (X).</p><p>Assumption 3.1. (Invariant DAG) For ‚àÄc k , P c k (X) ‚àà P C (X) admits the product factorization of Eq. ( <ref type="formula">2</ref>) relative to the same DAG G.</p><formula xml:id="formula_7">Remark 3.2. If P C (X) satisfies Assumption 3.1, then, each P c k (X) ‚àà P C (X) is Markovian relative to G.</formula><p>According to the general definition of mechanisms change in <ref type="bibr" target="#b63">(Tian &amp; Pearl, 2001)</ref>, interventions can be seen as a special case of distribution shifts, where the external influence involves fixing certain variables to some predetermined values. Actually, in general, the external influence may be milder to merely change the conditional probability of certain variables given its causes. In this paper, we restrict our scope by assuming that the distribution shifts across P c k (X) come from the changes of mechanisms in F or distribution shifts of the exogenous variables in E (see Appendix F.1 for detailed discussion). More justifications on Assumption 3.1 are in Appendix F.2.</p><formula xml:id="formula_8">Assumption 3.3. For ‚àÄc k1 , c k2 , if P c k 1 (X) = P c k 2 (X), the distribution shifts are caused by (1) ‚àÉ i ‚àà [d], P c k 1 (X i |X pai ) = P c k 2 (X i |X pai ), i.e., f c k 1 i = f c k 2 i . (2) ‚àÉ i ‚àà [d], P c k 1 ( i ) = P c k 2 ( i ).</formula><p>Federated DAG Structure Learning. Given the decentralized dataset D consisting of data from m clients while the corresponding P C (X) satisfies Assumptions 3.1 and 3.3, federated DAG structure learning aims to identify the underlying DAG G from D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Methodology</head><p>To solve the federated DAG structure learning problem, we formulate a continuous score-based method named FedDAG to learn the DAG structure from decentralized data. Firstly, we define an objective function that guides all models from different clients to federally learn the underlying DAG structure G (or adjacency matrix B), and at the same time also to learn personalized mechanisms for each client. As shown in Figure <ref type="figure" target="#fig_3">2</ref>, for each client c k , the local model consists of a graph learning part and a mechanisms approximation part. The GSL part is parameterized by a matrix U c k ‚àà R d√ód , which would be the same for all clients finally<ref type="foot" target="#foot_4">foot_4</ref> . To make every entry of U c k to approximate the binary entry of the adjacency matrix, a Gumbel-Sigmoid method <ref type="bibr" target="#b22">(Jang et al., 2017;</ref><ref type="bibr">Ng et al., 2022b)</ref>, represented as g œÑ (U c k ), is further leveraged to transform U c k to a differentiable approximation of the adjacency matrix. The mechanisms approximation parts</p><formula xml:id="formula_9">f c k 1 , f c k 2 , ‚Ä¢ ‚Ä¢ ‚Ä¢ , f c k d are</formula><p>parameterized by d sub-networks, each of which has d inputs and one output. In the learning procedure, the GSL parts (specifically U c k ) of participating clients are shared with the server s. Then, the processed information is broadcast to each client for self-updating its matrix. The details of our method are demonstrated in the following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">The overall learning objective</head><p>Now we present the overall learning objective of FedDAG as the following optimization problem:</p><formula xml:id="formula_10">arg max Œ¶,U m k=1 S c k (D c k , Œ¶ c k , U ) subject to g œÑ (U ) ‚àà DAGs ‚áî h(U ) = Tr[e (gœÑ (U )) ] -d = 0, (6)</formula><p>where  learning, selecting a proper score function such as BIC score <ref type="bibr" target="#b59">(Schwarz, 1978)</ref>, generalized score function <ref type="bibr" target="#b18">(Huang et al., 2018)</ref> or equivalently taking the likelihood of P (X) with a penalty function on model parameters <ref type="bibr" target="#b84">(Zheng et al., 2018;</ref><ref type="bibr">Ng et al., 2022b;</ref><ref type="bibr" target="#b85">Zheng et al., 2020;</ref><ref type="bibr" target="#b30">Lachapelle et al., 2020)</ref> can guarantee to identify up the underlying ground-truth graph structure G because G is supposed to have the maximal score over Eq. ( <ref type="formula">6</ref>). Throughout all experiments in this paper, we assume the noise types are Gaussian with equal variance for each local distribution. And the overall score function utilized in this paper is as follows,</p><formula xml:id="formula_11">Œ¶ c k := {Œ¶ c k 1 , Œ¶ c k 2 , ‚Ä¢ ‚Ä¢ ‚Ä¢ , Œ¶ c k d }</formula><formula xml:id="formula_12">ùëã ùëê 1 ùëº ùëê 1 ùëî ùúè (‚ãÖ) ‚ãØ ‚ãØ Client ùëê 1 Client ùëê 2 ‚àö ‚àö ‚àö ùëã ùëê 1 Œ¶ ùëê 1 ùëã ùëê 2 ùëº ùëê 2 ùëî ùúè (‚ãÖ) ‚àö ‚àö ‚àö ùëã ùëê 2 Œ¶ ùëê 2 ùëã ùëê ùëö ùëº ùëê ùëö ùëî ùúè (‚ãÖ) ‚àö ‚àö ‚àö ùëã ùëê ùëö Œ¶ ùëê ùëö Client ùëê ùëö Refine</formula><formula xml:id="formula_13">S c k (D c k , Œ¶ c k , U c k ) = - 1 2n k n k i=1 d j D c k ij -Œ¶ c k j (g œÑ (U c k j,: ) ‚Ä¢ D c k i ) 2 2 -Œª g œÑ (U ) 1 . (7)</formula><p>In our score function, we take the negative Least Squares loss and a sparsity term, which corresponds to the model complexity penalty in the BIC score <ref type="bibr" target="#b59">(Schwarz, 1978)</ref>.<ref type="foot" target="#foot_5">foot_5</ref> However, the global minimum is hard to reach by using the gradient descent method due to the non-convexity of h(U ). More details on discussions of the optimization results can be found in Appendix. C.</p><p>In this paper, instead of directly taking the likelihood of P (X), we leverage the well-known results on the density transformation to model the distribution of P (E), i.e., maximizing the likelihood <ref type="bibr">et al., 2011)</ref>. According to Eq. (1), we have i = X i -f i (PA i ). That is to say, modeling P (E) can be achieved by an auto-regressive model. To get i , the first step is to select the parental set PA i for X i . This can be realized by B[:, i] ‚Ä¢ X, where ‚Ä¢ means the element-wise product. In our paper, for client c k , we predict the noise by #Sub-problem Solving</p><formula xml:id="formula_14">P c k (E|F c k , G) for ‚àÄc k ‚àà C (Mooij</formula><formula xml:id="formula_15">i = X i -Œ¶ i (g œÑ (U )[:, i] ‚Ä¢ X),</formula><formula xml:id="formula_16">7: U t+1 , Œ¶ t+1 ‚Üê SPS(D, C, Œ± t , œÅ t , it in , it f l , r) 8:</formula><p>#Coefficients Updating 9:</p><formula xml:id="formula_17">Œ± t+1 ‚Üê Œ± t + œÅ t E[h(U t+1 )], t ‚Üê t + 1 10: if E[h(U t+1 )] &gt; Œ≥E[h(U t )] then 11: œÅ t+1 = Œ≤œÅ t 12:</formula><p>else 13:</p><formula xml:id="formula_18">œÅ t+1 = œÅ t 14:</formula><p>end if 15: end while</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Federated DAG structure learning</head><p>As suggested in NOTEARS <ref type="bibr" target="#b84">(Zheng et al., 2018)</ref>, the hard-constraint optimization problem in Eq. ( <ref type="formula">6</ref>) can be addressed by an Augmented Lagrangian Method (ALM) to get an approximate solution. Similar to the penalty methods, ALM transforms a constrained optimization problem by a series of unconstrained sub-problems and adds a penalty term to the objective function. ALM also introduces a Lagrangian multiplier term to avoid ill-conditioning by preventing the coefficient of the penalty term from going too large. To solve Eq. ( <ref type="formula">6</ref>), the sub-problem can be written as arg max</p><formula xml:id="formula_19">Œ¶,U m k=1 S c k (D c k , Œ¶ c k , g œÑ (U )) -Œ± t h(U ) - œÅ t 2 h(U ) 2 , (<label>8</label></formula><formula xml:id="formula_20">)</formula><p>where Œ± t and œÅ t are the Lagrangian multiplier and penalty parameter of the t-th sub-problem, respectively. These parameters are updated after the sub-problem is solved. Since neural networks are adopted to fit the mechanisms in our work, there is no closed-form solution for Eq. ( <ref type="formula" target="#formula_19">8</ref>). Therefore, we solve it approximately via ADAM <ref type="bibr" target="#b27">(Kingma &amp; Ba, 2015)</ref>. The method is described in Algorithms 1 and 2. And in Algorithm 1, we share the same coefficients updating strategy as in <ref type="bibr">(Ng et al., 2022b)</ref>.</p><p>Each sub-problem as Eq. ( <ref type="formula" target="#formula_19">8</ref>) is solved mainly by distributing the computation across all local clients. Since data is prevented from sharing among clients and the server, each client owns its personalized model, which is only trained on its personalized data. The server communicates with clients by exchanging the parameters information of models and coordinates the joint learning task. To achieve so, our method alternately updates the server and clients in each communication round.</p><p>Client Update. For each model of client c k , there are two main parts, named GSL part parameterized by U c k and MA part parameterized by Œ¶ c k , respectively. Essentially, the joint objective in Eq. ( <ref type="formula" target="#formula_19">8</ref>) guides the learning process. In the self-updating procedure as described in Algorithm 2, the clients firstly receive the updated penalty coefficients Œ± t and œÅ t and the averaged parameter U new . Then, the renewed learning personalized score of client c k is defined as</p><formula xml:id="formula_21">SP c k = S c k -Œ± t h(U c k ) -œÅt 2 h(U c k ) 2 .</formula><p>it f l times of local gradient-based parameter updates are operated to maximize its personalized score.</p><p>Server Update. After it f l local updates, the server randomly chooses r clients to collect their U s to the set U. Then, U s in U are averaged to get U new . The other operation on the server is updating the Œ± t , œÅ t to Œ± t+1 , œÅ t+1 . The detailed calculating rules are described at lines 8 -14 in Algorithm 1. Then, the new penalty coefficients and parameters are broadcast to all clients. Notice that assuming that data distribution across clients is homogeneous (no distribution shift), Œ¶ c k of the chosen r clients can also be collected and averaged to update the local models of clients in the same way, which is named as All-Shared FedDAG (AS-FedDAG) in this paper. For clarity, we name our general method as Graph-Shared (GS-FedDAG) to distinguish it from Algorithm 2 Sub-Problem Solver (SPS) for FedDAG</p><formula xml:id="formula_22">1: Input: D, C, Parameter-list = {Œ± t , œÅ t , it in , it f l , r} 2: Output: U new , Œ¶ itin 3: Define SP c k = S c k -Œ± t h(U c k ) -œÅt 2 h(U c k ) 2 4: for i in (1, 2, ‚Ä¢ ‚Ä¢ ‚Ä¢ , it in ) do 5:</formula><p>for each client c k do 6:</p><p>#Self-updating 7:</p><formula xml:id="formula_23">U i,c k , Œ¶ i,c k ‚Üê arg max Œ¶ c k ,U c k SP c k 8:</formula><p>end for 9:</p><p>if i (% it f l ) = 0 or i = it in then 10:</p><p>#Aggregating: randomly select r clients and collect their U s into U, then, send U to the server 11:</p><p>U ‚Üê Agg(r, C) </p><formula xml:id="formula_24">U i,c k ‚Üê U new 19:</formula><p>end for 20:</p><p>end if 21: end for AS-FedDAG. It is worth noting that AS-FedDAG can further enhance the performance in the homogeneous case but introduce some additional communication costs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Thresholding</head><p>For continuous optimization, as illustrated in the previous work <ref type="bibr">(Ng et al., 2022b)</ref>, we leverage Gumbel-Sigmoid to approximate the binary mask. That is to say, the exact 0 or 1 is hard to get. The other issue is raised by ALM since the solution of ALM only satisfies the numerical precision of the constraint. This is because we set h tol and it max maximally but not infinite coefficients of penalty terms to formulate the last sub-problem. Therefore, some entries of the output M = Eg œÑ (U ) will be near but not exactly 0 or 1. To alleviate this issue, 1 sparsity is added to the objective function. In our method, since all mask values are in [0, 1], we take the median value 0.5 as the threshold to prune the edges, which follows the same way in our baseline method MCSL <ref type="bibr">(Ng et al., 2022b)</ref>. The iterative thresholding method is also taken to deal with the case that the learned graph is cyclic. This may happen if the number of variables is large (40 variables in our paper). Because, in numerical optimization, the constraint penalty exponentially decreases with the number of variables. To deal with the cyclic graph, we one-by-one cut the edge with the minimum value until the graph is acyclic. Until now, all continuous search methods for DAG learning suffer from these two problems. It is an interesting future direction to be investigated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Convergence analysis</head><p>Let us quickly review our method. For each client c k , the model parameters include Œ¶ c k and U c k . Each client optimizes its parameters on its own data D c k . Like NOTEARS and its following works, our method can reach a stationary point instead of the global maximum (the ground-truth DAG). Then, we separate our discussion into homogeneous and heterogeneous data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Homogeneous data</head><p>For the no distribution shift case, we have</p><formula xml:id="formula_25">Œ¶ c1 = Œ¶ c2 = ‚Ä¢ ‚Ä¢ ‚Ä¢ = Œ¶ cm and U c1 = U c2 = ‚Ä¢ ‚Ä¢ ‚Ä¢ = U cm .</formula><p>Our method named AS-FedDAG (All-Shared FedDAG) sets a central server, which regularly (1) receives all parameters (or U c k for GS-FedDAG), (2) averages these parameters to get Œ¶ new and U new and (3) broadcasts Œ¶ new and U new to all clients during the learning procedures. AS-FedDAG benefits from an advanced technique named FedAvg <ref type="bibr" target="#b37">(McMahan et al., 2017)</ref> for solving the FL problem in the homogeneous data case. FedAvg solves a similar problem by averaging all parameters learned from each client in the learning process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Heterogeneous data</head><p>To solve the overall constraint-based problem, we take ALM to convert the hard constraint to a soft constraint with a series of increasing penalty co-efficiencies. The convergence of ALM for the non-convex problem has been well studied <ref type="bibr" target="#b41">(Nemirovski, 1999)</ref> and presented in NOTEARS <ref type="bibr" target="#b84">(Zheng et al., 2018)</ref>. Thus, we only consider the convergence analysis of our method directly from the inner optimization, i.e., the t-th sub-problem, as follows.</p><p>arg max</p><formula xml:id="formula_26">Œ¶,U m k=1 S c k (D c k , Œ¶ c k , g œÑ (U )) -Œ± t h(U ) - œÅ t 2 h(U ) 2 . (<label>9</label></formula><formula xml:id="formula_27">)</formula><p>Here, for simplification, we just define that ≈úc</p><formula xml:id="formula_28">k (Œ¶ c k , U ) = -S c k (D c k , Œ¶ c k , g œÑ (U )) + Œ± t h(U ) + œÅt 2 h(U ) 2 .</formula><p>Then, the overall optimization problem can be reformulated as follows.</p><p>arg min</p><formula xml:id="formula_29">Œ¶,U ≈ú(U , Œ¶) := m k=1 ≈úc k (U , Œ¶ c k ). (<label>10</label></formula><formula xml:id="formula_30">)</formula><p>Through the following part, we use ‚àá U and ‚àá Œ¶ to represent the gradients of ≈ú(U , Œ¶) w.r.t U and Œ¶ c k , respectively. And, we use ‚àáU and ‚àáŒ¶ to represent the stochastic gradients calculated by a mini-batch of observations w.r.t U and Œ¶ c k , respectively.</p><p>Definition 4.1. (Partial Gradient Diversity). The gradient diversity among all local learning objectives as:</p><formula xml:id="formula_31">m i=1 ‚àá U ≈úc k (U , Œ¶ c k ) -‚àá U ≈ú(U , Œ¶) 2 ‚â§ Œ¥ 2 . (<label>11</label></formula><formula xml:id="formula_32">)</formula><p>Note that the notation of gradient diversity is introduced <ref type="bibr" target="#b76">(Yin et al., 2018;</ref><ref type="bibr" target="#b15">Haddadpour &amp; Mahdavi, 2019)</ref> as a measurement to compute the similarity among gradients updated on different clients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Assumption 4.2. (Smoothness and Lower Bound). The local objective function ≈úc</head><formula xml:id="formula_33">k (‚Ä¢) of the k-th client is differentiable for ‚àÄk ‚àà [m]. Also, ‚àá U ≈úc k (U , Œ¶ c k ) is L U -Lipschitz w.r.t U and L U Œ¶ w.r.t Œ¶ c k , and ‚àá Œ¶ ≈úc k (U , Œ¶ c k ) is L Œ¶ -Lipschitz w.r.t Œ¶ c k and L Œ¶U w.r.t U .</formula><p>We also assume the overall objective function can be bounded by a constant ≈ú * and denote ‚àÜ ≈ú0 = ≈ú(U 0 , Œ¶ 0 ) -≈ú * .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The relative cross-sensitivity of ‚àá</head><formula xml:id="formula_34">U ≈úc k w.r.t Œ¶ c k and ‚àá Œ¶ ≈úc k w.r.t U with the scalar œá := max {L U Œ¶ , L Œ¶U } / L U L Œ¶ . (<label>12</label></formula><formula xml:id="formula_35">) Assumption 4.3. (Bounded Local Variance) For each local data D c k , k ‚àà [m],</formula><p>we can independently sample a batch of data denoted as Œæ ‚äÇ D c k . Then, there exist constant Œ¥ U and Œ¥ Œ¶ such that</p><formula xml:id="formula_36">E ‚àáU ≈úc k (Œ¶ c k , U ) -‚àá U ≈úc k (U , Œ¶ c k ) 2 ‚â§ œÉ 2 U , E ‚àáŒ¶ ≈úc k (Œ¶ c k , U ) -‚àá Œ¶ ≈úc k (U , Œ¶ c k ) 2 ‚â§ œÉ 2 Œ¶ ,</formula><p>The bounded variance assumption is a standard assumption on the stochastic gradients <ref type="bibr" target="#b15">(Haddadpour &amp; Mahdavi, 2019;</ref><ref type="bibr" target="#b54">Pillutla et al., 2022)</ref>.</p><p>Theorem 4.4. (Convergence of <ref type="bibr">GS-FedDAG)</ref>. For GS-FedDAG with all clients involved in the aggregation, for ‚àÄ0 ‚â§ it ‚â§ T -1, under Assumptions 4.2, 4.3 and 4.1, and the learning rate for the U part is set as Œ∑/(L U it in ) and the learning rate for the œÜ part is set as Œ∑/(L œÜ it in ). Then, for Œ∑, depending on the problem parameters, we have</p><formula xml:id="formula_37">1 T T -1 it=0 1 L U E ‚àá U ≈úc k (Œ¶ c k it , U it ) 2 + 1 L Œ¶ E 1 m m i=1 ‚àá U ≈úc k (Œ¶ c k it , U it ) 2 ‚â§ (‚àÜ ≈ú0 œÉ 2 FedDAG,1 ) 1/2 ‚àö T + (‚àÜ ≈ú2 0 œÉ 2 FedDAG,2 ) 1/3 T 2/3 + O( 1 T ). (<label>13</label></formula><formula xml:id="formula_38">)</formula><p>where we define the effective variance terms</p><formula xml:id="formula_39">œÉ 2 FedDAG,1 = 1 + œá 2 œÉ 2 U L U + œÉ 2 Œ¶ L Œ¶ , œÉ 2 FedDAG,2 = 1 + œá 2 Œ¥ 2 L U + œÉ 2 U L U + œÉ 2 Œ¶ L Œ¶ 1 - 1 it in , (<label>14</label></formula><formula xml:id="formula_40">)</formula><p>where it in is the total step of one inner loop used in lines 4 -21 in Algorithm 2.</p><p>From Theorem 4.4, we can see that the gradients</p><formula xml:id="formula_41">‚àá U ≈úc k (Œ¶ c k it , U it ) w.r.t U and ‚àá U ≈úc k (Œ¶ c k it , U it ) w.r</formula><p>.t Œ¶ at the t-th step can be bounded if we choose a proper Œ∑, which affects the learning rates of the model.</p><p>The proof of Theorem 4.4 can be borrowed from the proof of Theorem 2 in <ref type="bibr" target="#b54">(Pillutla et al., 2022)</ref>. Notice that, in our theorem, we have assumed that all clients participate in the aggregation for simplification and the conclusion can be easily extended to the general partial participation case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Privacy and costs discussion</head><p>Privacy issues of FedDAG. The strongest motivation of FL is to avoid personalized raw data leakage. To achieve this, FedDAG proposes to exchange the parameters for modeling the graph. Here, we argue that the information leakage of local data is rather limited. The server, receiving parameters with client index, may infer some data property. However, according to the data generation model (1), the distribution of local data is decided by (1) DAG structure, (2) noise types/strengths, and (3) mechanisms. The gradient information of the shared matrix is decided by (1) the learning objective and (2) model architecture, which are agnostic to the server. Especially for the network part, clients may choose different networks to make the inference more complex. Moreover, suppose the graph structure is also private information for clients. In that case, this problem can be easily solved by selecting a client to serve as the proxy server<ref type="foot" target="#foot_7">foot_7</ref> . The proxy server needs to play two roles, including training its own model and taking on the server's duties. Then, other clients communicate with the proxy server instead of a real server in the communication round. Moreover, the aim of our work, and federated learning in general, is not to provide a full solution to privacy protection. Instead, it is the first step towards this goal, i.e., no data sharing between clients. To further protect privacy, more constraints need to be added to the federated learning framework, such as the prevention of information leakage from gradient sharing, which are studied under the privacy umbrella. To further enhance privacy protection, our method can also include more advanced privacy protection techniques <ref type="bibr">(Wei et al., 2020b)</ref>, which would be an interesting work to be investigated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Communication cost.</head><p>Since FedDAG requires exchanging parameters between the server and clients. Additional communication costs are raised. In our method, however, we argue that GS-FedDAG only brings rather small additional communication pressures. For the case of d variables, a single communication only exchanges a d √ó d matrix twice (sending and receiving). For homogeneous data, which assumes that local data are sampled from the same distribution, one can also transmit the neural network together to further improve the performance since mechanisms are also shared among clients. The trade-off between performances and communication costs can also be controlled by r in Algorithm 2, i.e., enlarging or reducing r. Surprisingly, we find that reducing r does not harm the performance severely (see Table <ref type="table" target="#tab_15">17</ref> in Appendix D for detailed results). Moreover, the partial communication method, which only chooses some clients to exchange training information, is also leveraged to address the issue that not all clients are always online at the same time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Results</head><p>In this section, we study the empirical performances of FedDAG on both synthetic and real-world data. More detailed ablation experiments can also be found in Appendix D.</p><p>Baselines We compare our method with various baselines including some continuous search methods, named NOTEARS <ref type="bibr" target="#b84">(Zheng et al., 2018)</ref>, NOTEARS-MLP (N-S-MLP, for short) <ref type="bibr" target="#b85">(Zheng et al., 2020)</ref>, DAG-GNN <ref type="bibr" target="#b77">(Yu et al., 2019)</ref> and MCSL <ref type="bibr">(Ng et al., 2022b)</ref>, and also two traditional combinatorial search methods named PC <ref type="bibr" target="#b62">(Spirtes et al., 2001)</ref> and GES <ref type="bibr" target="#b6">(Chickering, 2002)</ref>. The comparison results with another method named causal additive models (CAM) <ref type="bibr" target="#b5">(B√ºhlmann et al., 2014)</ref> are put in Appendix D.5. Furthermore, we also include a concurrent work named NOTEARS-ADMM <ref type="bibr">(Ng &amp; Zhang, 2022)</ref>, which also considers learning the Bayesian network in the federated setup. Since NOTEARS-ADMM focuses more on the homogeneous case and linear settings and pays less attention to the nonlinear cases, we only include the results on linear cases of NOTEARS-ADMM in the main paper for fair comparisons. More detailed comparisons are shown in Appendix D.6. Moreover, we also compare our FedDAG with a voting method <ref type="bibr" target="#b40">(Na &amp; Yang, 2010)</ref> in Appendix D.4, which also tries to learn DAG from decentralized data. We provide two training ways for these compared methods. The first way named "All data" is using all data to train only one model, which, however, is not permitted in FedDAG since the ban of data sharing in our setting. For the homogeneous data case, the results on this setting can be an approximate upper bound of our method but unobtainable. The second one named "Separated data" is separately training each siloed model over its personalized data, of which the performances reported are the average results of all clients.</p><p>Metrics. We report two metrics named Structural Hamming Distance (SHD) and True Positive Rate (TPR) averaged over 10 random repetitions to evaluate the discrepancies between estimated DAG and the ground-truth graph G. See more details about SHD, and TPR in Appendix B.3. Notice that PC and GES can only reach the completed partially DAG (CPDAG, or MEC) at most, which shares the same Skeleton with the ground-truth DAG G. When we evaluate SHD, we just ignore the direction of undirected edges learned by PC and GES. That is to say, these two methods can get SHD 0 if they can identify the CPDAG. The implementation details of all methods are given in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Synthetic data</head><p>The synthetic data we consider here is generated from Gaussian ANMs (Model (1)). Two random graph models named Erd≈ës-R√©nyi (ER) and Scale-Free (SF) (detailed definitions are shown in Appendix B.1.) are adopted to generate the graph structure G. And then, for each node V i corresponding to X i in G, we sample a function from the given function sets to simulate f i . Finally, data are generated according to a specific sampling method. In the following experiments, we take 10 clients and each with 600 observations (unless otherwise specified in some ablation studies.) throughout this paper. According to Assumption 3.1, data across all clients share the same DAG structure for both homogeneous and heterogeneous data settings. Due to the space limit, more ablation experiments, such as unevenly distributed observations, varying clients, dense graph, different non-linear functions, and different number of observations, etc., are put in Appendix D. All detailed discussions on the experimental results are in Appendix E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Homogeneous data setting</head><p>Results on linear models. For a fair comparison, here, we also provide the linear version of our method. Since linear data are parameterized with an adjacency matrix, we can directly take the adjacency matrix as our model instead of a GSL part and a MA part. During training, the matrix is communicated and averaged by the server to coordinate the joint learning procedures. NOTEARS-ADMM is also a DAG structure learning method from decentralized data. Different from our averaging strategy to exchange training information among clients, the optimization problem is solved by the alternating direction method of multipliers (ADMM). From Table <ref type="table" target="#tab_2">1</ref>, we find that our method can consistently show its advantage in the linear case. In the ER2 with 10 nodes setting, our AS-FedDAG is even better than NOTEARS with all training data. While it is possible and the detailed explanation can be found in Appendix E. Here, we give a more detailed explanation of why our FedDAG method performs better than the baseline methods. For PC and GES, they can only reach the CPDAG (or MEC) at most, which shares the same skeleton with the ground-truth DAG. When we evaluate the SHD, we just ignore the direction of undirected edges learned by PC and GES. That is to say, these two methods can get SHD 0 if they can identify the true CPDAG. Therefore, the final results are not caused by unfair comparison. For PC, the independence test is leveraged to decode the (conditional) independence from the data distribution. Therefore, the accuracy would be affected by (1) the number of observations and (2) the effectiveness of the non-parametric kernel independence test method. GES leverages greedy search with BIC score. However, the likelihood part of BIC in GES is Linear Gaussian, which is unsuitable for data generated by the Non-linear model. NOTEARS is a linear model but the mechanisms are non-linear. The reason will be the unfitness between data and model. Therefore, the comparisons with GES and NOTEARS on linear homogeneous data are implemented in the Table <ref type="table" target="#tab_2">1</ref>. DAG-GNN is also a non-linear model. However, the non-linear assumption of DAG-GNN is not the same as the data generation model ANMs assumed in our paper. The second reason comes from its mechanisms approximation modules are compulsory to share some parameters. Both NOTEARS-MLP and MCSL have their advantages. Please refer to Tables <ref type="table" target="#tab_14">14</ref> and<ref type="table" target="#tab_2">15</ref>, you will find that NOTEARS-MLP performs better when the non-linear functions are MIM and MLP while MCSL works better on GP and GP-add models.</p><p>Visualization of the learned DAG of FedDAG. We take an example of the AS-FedDAG optimization process on linear Gaussian model with NOTEARS as the baseline method and plot the change of estimated parameters in Fig. <ref type="figure">3</ref> and Fig. <ref type="figure">4</ref>. In this example, the number of nodes is set as 10 and the edges are 10. The data is simulated by ER graph and evenly assign 200 observations on two different clients. In Fig. <ref type="figure">3</ref>, we can see that the learned graph is asymptotically approximating the ground-truth DAG B G , including the existence of edges and their weights. From Fig. <ref type="figure">4</ref>, we can find that with the increase of the penalty coefficients, h loss decreases quickly. For learned graphs on the different clients, we can see that the SHD distance is smaller during the optimization procedures. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Heterogeneous data setting</head><p>As defined in Section 3, the heterogeneous data property of data across clients come from the changes in mechanisms or the shift of noise distributions. To simulate the heterogeneous data, we first generate a graph structure shared by all clients and then decide the types of mechanisms f c k i and noises i for i ‚àà [d] for each client c k . In our experiments, we allow that f c k can be linear or non-linear for each client. If being linear, f c k here is a weighted adjacency matrix with coefficients sampled from Uniform ([-2.0, -0.5] ‚à™ [0.5, 2.0]), with equal probability. If being non-linear, f c k i is independently sampled from GP, GP-add, MLP, or MIM functions <ref type="bibr" target="#b79">(Yuan, 2011)</ref>, randomly. Then, a fixed zero-mean Gaussian noise is set to each client with a randomly sampled variance from {0.8, 1}.</p><p>We can see that the conclusion of experimental results on the heterogeneous data setting is rather similar to that of the homogeneous data. As can be read from Table <ref type="table" target="#tab_4">3</ref>, GS-FedDAG always shows the best performances 0.73 ¬± 0.07 GS-FedDAG 1.9 ¬± 1.6 0.99 ¬± 0.02 2.6 ¬± 1.3 0.93 ¬± 0.07 24.3 ¬± 10.2 0.86 ¬± 0.09 33.9 ¬± 10.9 0.73 ¬± 0.09 across all settings. If taking all data together to train one model using other methods, we can see that data heterogeneity would put great trouble to all compared methods while GS-FedDAG plays pretty well. Here, we also provide the experimental results of AS-FedDAG on this setting. We can find that the model misspecification problem would lead to unsatisfactory results, which motivate us to design the GS-FedDAG. Moreover, GS-FedDAG shows consistently good results with different numbers of observations on each client (see Table <ref type="table" target="#tab_7">16</ref>). NOTEARS takes second place at the setting of 40 nodes because there are some linear data among clients, which is also the reason that GS-FedDAG shows lower SHDs on heterogeneous data in Table <ref type="table" target="#tab_4">3</ref> than Table <ref type="table" target="#tab_3">2</ref>. Compared with non-linear models, NOTEARS easily fits well with even fewer linear data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Real data</head><p>We consider a real public dataset named fMRI Hippocampus <ref type="bibr" target="#b55">(Poldrack et al., 2015)</ref> to discover the underlying relationships among six brain regions. This dataset records signals from six separate brain regions in the resting state of one person in 84 successive days and the anatomical structure provide 7 edges as the ground truth graph (see Figure <ref type="figure" target="#fig_19">10 in (Appendix D</ref>). Herein, we separately select 500 records in each of the 10 days, which can be regarded as different local data. It is worth noting that though this data does not have a real data privacy problem, we can use this dataset to evaluate the learning accuracy of our method. Here, in Table <ref type="table" target="#tab_5">4</ref> we show part of the experimental results while others lie in Table <ref type="table" target="#tab_8">18</ref>). AS-FedDAG shows the best performance on all criteria while GS-FedDAG also performs better than most of the other methods. SHD ‚Üì 9.0 ¬± 0.0 5.0 ¬± 0.0 9.0 ¬± 0.6 8.7 ¬± 1.3 8.0 ¬± 1.9 8.3 ¬± 1.7 6.4 ¬± 0.9 5.0 ¬± 0.0 NNZ 11.0 ¬± 0.0 4.0 ¬± 0.0 12.0 ¬± 0.6 7.6 ¬± 1.3 5.4 ¬± 1.5 9.0 ¬± 1.7 6.8 ¬± 0.6 5.0 ¬± 0.0 TPR ‚Üë 0.43 ¬± 0.00 0.29 ¬± 0.00 0.44 ¬± 0.04 0.26 ¬± 0.11 0.19 ¬± 0.18 0.35 ¬± 0.15 0.27 ¬± 0.12 0.29 ¬± 0.00 FDR ‚Üì 0.73 ¬± 0.00 0.50 ¬± 0.00 0.74 ¬± 0.03 0.76 ¬± 0.10 0.78 ¬± 0.19 0.73 ¬± 0.11 0.72 ¬± 0.11 0.60 ¬± 0.00</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related work</head><p>Two mainstreams, named constraint-based and score-based methods, push the development of DAG structure learning. Constraint-based methods, including SGS and PC <ref type="bibr" target="#b62">(Spirtes et al., 2001)</ref>, take conditional independence constraints induced from the observed distribution to decide the graph skeleton and part of the directions. Another branch of methods (Chickering, 2002) define a score function, which evaluates the fitness between the distribution and graph, and identifies the graph G with the highest score after searching the DAG space. To avoid solving the combinatorial optimization problem, NOTEARS <ref type="bibr" target="#b84">(Zheng et al., 2018)</ref> introduces an equivalent acyclicity constraint and formulates a fully continuous optimization for searching the graph. Following this work, many works leverages this constraint to non-linear case <ref type="bibr" target="#b43">(Ng et al., 2019;</ref><ref type="bibr" target="#b85">Zheng et al., 2020;</ref><ref type="bibr" target="#b30">Lachapelle et al., 2020;</ref><ref type="bibr" target="#b9">Zhu et al., 2020;</ref><ref type="bibr" target="#b68">Wang et al., 2021;</ref><ref type="bibr" target="#b11">Gao et al., 2021;</ref><ref type="bibr">Ng et al., 2022b)</ref>, low-rank graph <ref type="bibr" target="#b9">(Fang et al., 2020)</ref>, interventional data <ref type="bibr" target="#b4">(Brouillard et al., 2020;</ref><ref type="bibr" target="#b26">Ke et al., 2019;</ref><ref type="bibr" target="#b58">Scherrer et al., 2021)</ref>, time-series data <ref type="bibr">(Pamfil et al., 2020)</ref>, incomplete data <ref type="bibr" target="#b10">(Gao et al., 2022;</ref><ref type="bibr" target="#b12">Geffner et al., 2022)</ref> and unmeasured confounding <ref type="bibr" target="#b3">(Bhattacharya et al., 2021)</ref>. GOLEM <ref type="bibr" target="#b44">(Ng et al., 2020)</ref> leverages the full likelihood and soft constraint to solve the optimization problem. <ref type="bibr">Ng et al. (2022a)</ref>, DAG-NoCurl <ref type="bibr" target="#b78">(Yu et al., 2021)</ref> and NOFEARS <ref type="bibr">(Wei et al., 2020a)</ref> focus on the optimization aspect.</p><p>The second line of related work is on the Overlapping Datasets (OD) <ref type="bibr" target="#b8">(Danks et al., 2009;</ref><ref type="bibr" target="#b64">Tillman &amp; Spirtes, 2011;</ref><ref type="bibr" target="#b65">Triantafillou &amp; Tsamardinos, 2015;</ref><ref type="bibr">Huang et al., 2020a)</ref> problem in DAG structure learning. However, OD assumes that each dataset owns observations of partial variables and targets learning the integrated DAG from multiple datasets. In these works, data from different sites need to be collected on a central server.</p><p>The last line is on federated learning <ref type="bibr" target="#b75">(Yang et al., 2019;</ref><ref type="bibr" target="#b24">Kairouz et al., 2021)</ref>, which provides the joint training paradigm to learn from decentralized data while avoiding sharing raw data during the learning process. FedAvg <ref type="bibr" target="#b37">(McMahan et al., 2017)</ref> first formulates and names federated learning. FedProx <ref type="bibr" target="#b32">(Li et al., 2020)</ref> studies the heterogeneous case and provides the convergence analysis results. SCAFFOLD leverages variance reduction by correcting client-shift to enhance training efficiency. Besides these fundamental problems in FL itself, this novel learning way has been widely co-operated with or applied to many real-world tasks such as healthcare <ref type="bibr" target="#b60">(Sheller et al., 2020)</ref>, recommendation system <ref type="bibr" target="#b74">(Yang et al., 2020)</ref>, and smart transport (Samarakoon et al., 2019).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Concurrent work (NOTEARS-ADMM)</head><p>In NOTEARS-ADMM <ref type="bibr">(Ng &amp; Zhang, 2022)</ref>, the authors also consider the same setting for discovering the underlying relations from distributed data owing to privacy and security concerns. The main advantage of our FedDAG over NOTEARS-ADMM is to handle heterogeneous data, which is very common in real applications. Then, NOTEARS-ADMM mainly considers the linear case, which shares the same learning object with our method. Instead of taking an average to share training information, ADMM is taken to make the adjacency matrix close. More detailed experimental comparisons can be found in Appendix D.6, from which we can see that our FedDAG performs better in most settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion and Discussions</head><p>Learning the underlying DAG structure from decentralized data brings considerable challenges to traditional DAG learning methods. In this context, we have introduced one of the first federated DAG structure learning methods called FedDAG, which uses a two-level structure for each local model. During the learning procedure, each client tries to learn an adjacency matrix to approximate the graph structure and neural networks to approximate the mechanisms. The matrix parts of some participating clients are aggregated and processed by the server and then broadcast to each client for updating its personalized matrix. The overall problem is formulated as a continuous optimization problem and solved by gradient descent. Structural identifiability conditions are provided, and extensive experiments on various data sets are to show the effectiveness of our FedDAG.</p><p>The first limitation of our framework is with the no latent variable assumption, which is seldom right in real scenarios. While, as a general framework, the advanced methods <ref type="bibr" target="#b3">(Bhattacharya et al., 2021)</ref>, which can handle the no observed confounder case, can be well incorporated with our method to deal with the federated setup (More details can be seen in Appendix C.3). Another limitation relies on privacy protection. As we said, we focus on the statistical and optimization perspectives of federated DAG structure learning and leave the problem of combining the advanced privacy protection methods <ref type="bibr">(Wei et al., 2020b)</ref> into our framework as a future work. The last limitation is to loose the invariant DAG assumption and allow the causal graph change among different clients, which is more common in the real world.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Structure identifiability</head><p>Besides exploring effective DAG structure learning methods, identifiability conditions of graph structure <ref type="bibr" target="#b62">(Spirtes et al., 2001)</ref> are also important. In general, unique identification of the ground truth DAG is impossible from purely observational data without some specific assumptions. However, accompanying some specific data generation assumptions, the graph can be identified <ref type="bibr" target="#b52">(Peters et al., 2011;</ref><ref type="bibr">Peters &amp; B√ºhlmann, 2014;</ref><ref type="bibr" target="#b81">Zhang &amp; Hyvarinen, 2009;</ref><ref type="bibr" target="#b61">Shimizu et al., 2006;</ref><ref type="bibr" target="#b17">Hoyer et al., 2008)</ref>. We first give the definition of identifiability in the decentralized setting.</p><p>Definition A.1. Consider a decentralized distribution set P C (X) satisfying Assumption 3.1. Then, G is said to be identifiable if P C (X) cannot be induced from any other DAG.</p><p>Condition A.2. (Minimality condition) Given the joint distribution P (X), P (X) is Markovian to a DAG G but not Markovian to any sub-graph of G.</p><p>Condition A.3. (Cond. 19 in <ref type="bibr">(Peters &amp; B√ºhlmann, 2014</ref>)) The triple (f j , P (X i ), P ( j )) does not solve the following differential equation ‚àÄx i ,</p><formula xml:id="formula_42">x j with v (x j -f (x i ))f (x i ) = 0: Œæ = Œæ - ŒΩ f ŒΩ + f f -2ŒΩ f f + ŒΩ f + ŒΩ ŒΩ f f ŒΩ - ŒΩ (f ) 2 f .</formula><p>Here, f := f j and Œæ := log P (X i ), and v := log P ( j ) are the logarithms of the strictly positive densities.</p><p>Definition A.4. (Restricted ANM. Def. 27 in <ref type="bibr">(Peters &amp; B√ºhlmann, 2014)</ref>) Consider an ANM with d variables. This SEM is called restricted ANM if ‚àÄj ‚àà V, i ‚àà PA j and all sets S ‚äÜ V with PA j \{i} ‚äÜ S ‚äÜ PA j \{i, j}, there is an x S with P (x S ) &gt; 0, s.t. the tripe</p><formula xml:id="formula_43">Ô£´ Ô£≠ f j (x PAj \{i} , ‚Ä¢ Xi ), P (X i | X S = x S ) , P ( j ) Ô£∂ Ô£∏ satisfies ConditionA.3.</formula><p>Here, the under-brace indicates the input component of f j for variable X i . In particular, we require the noise variables to have non-vanishing densities and the functions f j to be continuous and three times continuously differentiable.</p><p>Assumption A.5. (Faithfulness) Let P C (X) satisfy Assumption 3.1. At least one distribution P c k (X) ‚àà P C (X) meets Assumption A.6 and the other distributions are faithful to G.</p><p>Assumption A.6. Let a distribution P (X) with X = (X 1 , X 2 , ‚Ä¢ ‚Ä¢ ‚Ä¢ , X d ) be induced from a restricted ANM A.3 with graph G, and P (X) satisfies Minimality condition w.r.t G.</p><p>Proposition A.7. Given P C (X) satisfying Assumption A.5, and then, G can be identified up from P C (X).</p><p>Proof. From Remark 3.2, we have</p><formula xml:id="formula_44">P c k (X) ‚àà P C (X) for ‚àÄc k , is Markov with G. For each c k ‚àà C with P c k (X)</formula><p>does not satisfy Assumption A.6, the Completed Partially DAG (CPDAG) ƒú <ref type="bibr" target="#b49">(Pearl, 2009)</ref>, which represents the CPDAG induced by G, can be identified <ref type="bibr" target="#b62">(Spirtes et al., 2001)</ref>. ( <ref type="formula" target="#formula_0">1</ref>) That also says that these distributions can be induced from any DAG induced from M(G), including G definitely. Notice that skeleton( ƒú) = Skeleton(G) and any X i ‚Üê X j in ƒú is also existed in G. Then, for those c k with with P c k (X) satisfying Assumption A.6, G can be identified.</p><p>(2) That is to say, distributions satisfying Assumption A.6 can only be induced from G. Then, two kinds of graph, ƒú and G, are obtained. Therefore, G can be easily identified.</p><p>With (1) and ( <ref type="formula">2</ref>), P c k (X) ‚àà P C (X) for ‚àÄc k can only be induced by G. Then, G is said to be identifiable Future work is to relax our invariant DAG assumption to the invariant CPDAG assumption, which means that a group of DAGs across different clients share the same conditional independence. For this case, the generalized score functions <ref type="bibr" target="#b18">(Huang et al., 2018)</ref> can be adopted to search for the Markov Equivalence Class. However, it is not straightforward to incorporate this method into our FedDAG framework since the score of this method is motivated by a kernel-based (conditional) independence test rather than penalized likelihood. Moreover, this method does not support a continuous search strategy. It would be interesting to explore the penalized likelihood-based method for this case and incorporate it into our framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Implementations</head><p>The comparing DAG structure learning methods used in this paper all have available implementations, listed below:</p><p>‚Ä¢ MCSL: Codes are available at gCastle <ref type="url" target="https://github.com/huawei-noah/trustworthyAI/tree/master/gcastle">https://github.com/huawei-noah/trustworthyAI/tree/ master/gcastle</ref>. The first author of MCSL added the implementation in this package. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Graph generation</head><p>To simulate DAG for generating observations, we introduce two kinds of graph generation methods named Erd≈ës-R√©nyi (ER) and Scale-Free (SF) graphs. To simulate the ER graph generation, we firstly randomly sample a topological order and by adding directed edges where it is allowed independently with probability p = 2s d 2 -d where s is the number of edges in the resulting DAG. To generate Scale-free (SF) graphs, we first take the Barabasi-Albert model and then add all nodes one by one. From the above descriptions, we can find that the degree distribution of ER graphs follows a Poisson distribution, and the degree of SF graphs follows a power law: few nodes, often called hubs, have a high degree <ref type="bibr" target="#b30">(Lachapelle et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 SEM simulation</head><p>In our experimental setup, for each client, we randomly choose a nonlinear type from the given four functions with equal probability in the heterogeneous data setting. The nonlinear function choice is totally the same as used in NOTEARS-MLP <ref type="bibr" target="#b85">(Zheng et al., 2020)</ref>. The details are as follows.</p><p>We simulate the SEM X j = f j (X paj ) + Z j for all j ‚àà [d] in the topological order induce by G.</p><p>GP: f j is drawn from the Gaussian process with RBF kernel with length-scale one. GP-add: f j (X paj ) = k‚ààpaj f jk (X k ), where each f jk is from GP. MLP: f j is randomly initialized MLP with one hidden layer of size 100 and Sigmoid activation.</p><p>MiM: also named as index model. f j (X paj ) = 3 m=1 h m ( k‚ààpaj Œ∏ jmk X k ), where h 1 = tanh, h 2 = cos, h 3 = sin, and each Œ∏ jmk is uniformly drawn from range [-2, -0.5] ‚à™ [0.5, 2].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Detailed metrics</head><p>SHD is a kind of measurement which is defined to calculate the Hamming distance between two partially directed acyclic graphs (PDAG) by counting the number of edges for which the edge type differs in both PDAGs. In PDAG, there exist four kinds of edges between two nodes: i ‚Üí j, i ‚Üê j, i -j, and i j. SHD just counts the different edges between the two graphs. SHD is defined over the space of PDAGs, so we can, of course, use it to calculate distances in DAG and CPDAG spaces.</p><p>True Positive Rate (TPR) and False Discovery Rate (FDR) are two common metrics in the machine learning community. True positive rate, also referred to as sensitivity or recall, is used to measure the percentage of actual positives which are correctly identified. The FDR is defined as the expected proportion of errors committed by falsely rejecting the null hypothesis. Let T P be true positives (samples correctly classified as positive), F N be false negatives (samples incorrectly classified as negative), F P be false positives (samples incorrectly classified as positive), and T N be true negatives (samples correctly classified as negative). Then, T P R = T P T P +F N and F DR = F P F P +T P .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 Hyper-parameters setting</head><p>In all experiments, there is no extra hyper-parameter to adjust for PC (with Fisher-z test and p-value 0.01) and GES (BIC score). For NOTEARS, NOTEARS-MLP, and DAG-GNN, we use the default hyper-parameters provided in their papers/codes. For MCSL, the hyper-parameters that need to be modified are œÅ init and Œ≤. Specifically, if experimental settings (10 variables and 20 variables) are the same as those in their paper, we just take all the recommended hyper-parameters. For settings not implemented in their paper (40 variables exactly), we have two kinds of implementations. The first one is taking a linear interpolation for choosing the hyper-parameters. The second one is taking the same parameters as ours. We find that the second choice always works better. In our experiment, we report the experimental results in a second way. Notice that CAM pruning is also introduced to improve the performance of MCSL, which however can not guarantee a better result in our settings. For simplicity and fairness, we just take the direct outputs of MCSL.</p><p>Similar to MCSL <ref type="bibr">(Ng et al., 2022b)</ref> and GraN-DAG <ref type="bibr" target="#b30">(Lachapelle et al., 2020)</ref>, we implement several experiments on simulated data with known graph structure to search for the hyper-parameters and then use these hyperparameters for all the simulated experiments. Specifically, we use seeds from 1 to 10 to generate the simulated data to search for the best combination of hyper-parameters while all our experimental results reported in this paper are all conducted using seeds from 2021 to 2030.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5 Hyper-parameters in real-data setting</head><p>Most DAG learning methods have hyper-parameters, more or less, which need to be decided prior to learning. Moreover, NN-based methods are especially sensitive to the selection of hyper-parameters. For instance, Gran-DAG <ref type="bibr" target="#b30">(Lachapelle et al., 2020)</ref> defines a really large hyper-parameters space for searching the optimal combination, which even uses different learning rates for the first sub-problem and the other sub-problems. MCSL and GS-FedDAG are sensitive to the selection of œÅ init and Œ≤ when constructing and solving the sub-problem. As pointed out in <ref type="bibr" target="#b24">(Kairouz et al., 2021)</ref>, NOTEARS focuses more on optimizing the scoring term in the early stage and pays more attention to approximate DAG in the late stage. If NOTEARS cannot find a graph near G in the early stage, then, it would lead to a worse result.</p><p>To alleviate this problem, one may choose to (1) enlarge the learning rate or take more steps when solving the first few sub-problems as Gran-DAG; (2) reduce the value of coefficient œÅ init to let the optimizer pay more attention to the scoring term in the early stages as MCSL. The other trick we find when dealing with real data is increasing 1 . This mostly results from that real data may not fit well with the data generation assumptions in most papers. Therefore, we choose to conduct a grid search to find the best combination of œÅ init , Œ≤, 1 for DAG structure learning on real data.</p><p>In the practice of DAG structure learning, it is impossible to have G to select the hyper-parameters. One common approach is trying multiple hyper-parameter combinations and keeping the one yielding the best score evaluated on a validation set <ref type="bibr" target="#b29">(Koller &amp; Friedman, 2009;</ref><ref type="bibr">Ng et al., 2022b;</ref><ref type="bibr" target="#b30">Lachapelle et al., 2020)</ref>. However, the direct use of this method may not work for some algorithms, such as MCSL, NOTEARS-MLP, and GS-FedDAG. This mainly lies in the similar explanations of the property of the traditional solution of FL. In the late stage of optimization, the optimizer focuses heavily on finding a DAG by enlarging the penalty coefficient œÅ. Then, the learning of relationship mechanisms would be nearly ignored. To address this problem, we first report the DAG directly learned by a combination of hyper-parameters. And then, we replace the parameters part for describing the graph with the learned DAG. Afterward, we just take the score without DAG constraint to optimize the relationship mechanisms approximating part (which may not be the same name in the other algorithms). Finally, the validation set is taken to evaluate the learned model. The final hyper-parameters used on the real dataset in our paper are as follows:</p><p>Table <ref type="table">5</ref>: The hyper-parameters used on real data.</p><p>Parameters œÅ init Œ≤ Œª 1 Values 0.008 2 0.3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.6 Model parameters</head><p>The GSL part in each local model is parameterized by a d √ó d matrix named U and the Gumbel-Sigmoid approach is leveraged for approximating the binary form. Each entry in U is initialized as 0. The temperature œÑ is set to 0.2 for all settings. Then, for the relationship mechanism approximating part, we use 4 dense layers with 16 variables in each hidden layer. All weights in the Network are initialized using the Xavier uniform initialization. The number of parameters used in each method is shown in Table <ref type="table" target="#tab_7">6</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.7 Training parameters</head><p>Our GS-FedDAG and AS-FedDAG reach this point and are implemented with the following hyper-parameters. We take ADAM <ref type="bibr" target="#b27">(Kingma &amp; Ba, 2015)</ref> with learning rate 3 √ó 10 -2 and all the observational data D c k on each client are used for computing the gradient. And the detailed parameters used in Algorithms 1 and 2 are listed in Table <ref type="table">7</ref>.</p><p>Table <ref type="table">7</ref>: The hyper-parameters used on simulated data in this paper.</p><p>Parameters</p><formula xml:id="formula_45">Œ± init h tol it max it inner it f l Œ≥ œÅ max Œª 1</formula><p>Values 0 1 √ó 10 -10 25 1000 200 0.25 1 √ó 10 14 0.01</p><p>Notice that as illustrated in MCSL <ref type="bibr">(Ng et al., 2022b)</ref>, the performance of the algorithm is affected by the initial value of œÅ init and the choice of Œ≤. Since a small initial of œÅ init and Œ≤ would result in a rather long training time. As said in <ref type="bibr" target="#b25">(Kaiser &amp; Sipos, 2021)</ref>, MLE plays an important role in the early stage of training and highly affects the final results. Therefore, carefully picking a proper combination of œÅ init and Œ≤ will lead to a better result. In our method, we tune these two parameters via the same scale of experiment with seeds 1 ‚àº 10. For each variable scale and training type, the parameters are adjusted once and are applied to all other experiments with the same variable scale. We find the combinations of the following parameters in Table <ref type="table" target="#tab_8">8</ref> work well in our method. Our method also adopts a 1 sparsity term on g œÑ (U ), where the sparsity coefficient Œª 1 is chosen as 0.01 for all settings. </p><formula xml:id="formula_46">œÅ init Œ≤ œÅ init Œ≤ œÅ init Œ≤</formula><p>AS-FedDAG 6 √ó 10 -3 10 1 √ó 10 -5 20 1 √ó 10 -11 120 GS-FedDAG 6 √ó 10 -3 10 6 √ó 10 -5 20 1 √ó 10 -11 120 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.8 Sensitivity analysis of hyper-parameters</head><p>Here, we show the sensitivity analysis of it f l , Œ± init , and Œª l1 . From the experimental results in Figure <ref type="figure" target="#fig_5">5</ref>, we find that our method is relatively robust to it f l . That is to say, the it f l can be reduced to alleviate the pressure of communication costs while the performance can be well kept. Œª l1 is the coefficient of l 1 sparsity, which will affect the final results. Because we have no sparsity information of the underlying graph, we set Œª l1 = 0.01 in all settings. When dealing with real data, we recommend the audiences adjust this parameter by using our parameter-tuning method provided in Section B.5. The results of Œ± init are exactly as expected.</p><p>As discussed before, our method tries to maximize the likelihood term of the total loss in the early stages, which is important to find the final ground-truth DAG. If setting a relatively large Œ± init , the early learning stages would be affected. Therefore, we recommend directly taking Œ± init as 0 in all settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Discussions on our method C.1 Novelty and contributions</head><p>Firstly, we acknowledge the contribution of our baseline method MCSL <ref type="bibr">(Ng et al., 2022b)</ref>, which performs well in many settings and helps to guarantee the performance of our proposed method. We also appreciate the excellent baseline method FedAvg <ref type="bibr" target="#b37">(McMahan et al., 2017)</ref>, which provides an efficient federated learning way. Our FedDAG is highly inspired and benefits from these two works. The main contributions, which can be taken by our proposed method, are (1) one of the first works that investigate the practical problem of DAG structure learning in a federated setup and (2) further providing the FedDAG approach that can guarantee the privacy protection by avoiding the raw data leakage and allow the data heterogeneity across the clients. Another concurrent work NOTEARS-ADMM <ref type="bibr">(Ng &amp; Zhang, 2022</ref>) also considers the same problem while our GS-FedDAG can (1) gain better performances in most of the settings, (2) well handle the nonlinear cases, (3) allow heterogeneous data, and (4) provide a quite flexible federated DAG structure learning framework.</p><p>Discussions on the simple averaging Even though averaging is the simplest way to aggregating and exchanging information, we find it is quite an effective way to solve the federated DAG structure learning problem, which is an advantage of our method. Our simple averaging for homogeneous cases can nearly approach the same performance as using all data. For the heterogeneous cases, GS-FedDAG can still obtain satisfactory results. While, as future work, more advanced information aggregation methods <ref type="bibr">(Wang et al., 2020a)</ref> can be well incorporated into our framework to boost the performances further.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Difference with graph neural network (GNN) learning</head><p>Four main reasons make DAG structure learning and GNN two different research lines. (1) Nodes in DAG represent variables, and directed edges describe the single-direction relation between different variables. In GNN, graph talks more about graph-type data, such as social networks, protein networks, and traffic networks.</p><p>(2) Networks in DAG structure learning are leveraged to learn the relationship mechanisms, while networks in GNN are taken to achieve node embedding and feature extraction.</p><p>(3) Learned DAG can be taken for interventional and counterfactual reasoning <ref type="bibr" target="#b28">(Kitson et al., 2021)</ref>. ( <ref type="formula" target="#formula_4">4</ref>) DAG structure learning cares more about identifiability. It is essential to identify the true underlying relationship of the observations precisely.</p><p>In the federated setup, most existing federated GNN <ref type="bibr" target="#b72">(Xie et al., 2021;</ref><ref type="bibr" target="#b35">Liu &amp; Yu, 2022)</ref> methods assume that the underlying graphs are known and localized. What is being learned in the federation is the weight aggregation of the GNN but not its graph. This also leads to the main difference between our federated DAG learning and federated GNN learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 FedDAG as a framework</head><p>In this paper, we restrict our attention to the case that all concerned variables can be well observed. We also only take MCSL <ref type="bibr">(Ng et al., 2022b)</ref> as the baseline method. However, all gradient-based methods can be incorporated into our AS-FedDAG framework to deal with homogeneous data. To deal with the heterogeneous data, we prefer that the baseline methods can separately learn the DAG structure and relationship mechanisms.</p><p>The other baseline methods that can be easily combined into our framework are NOTEARS-MLP <ref type="bibr" target="#b85">(Zheng et al., 2020)</ref> and DAG-GNN <ref type="bibr" target="#b77">(Yu et al., 2019)</ref>. Unfortunately, many works are not in this fashion, such as GraN-DAG <ref type="bibr" target="#b30">(Lachapelle et al., 2020)</ref>, CD-RL <ref type="bibr" target="#b9">(Zhu et al., 2020)</ref>, and their following works.</p><p>Latent variables. In this paper, we carry with no unobserved common confounder assumption. Handling latent confounders is a fundamentally important but challenging problem in the traditional DAG structure learning, not to mention the federated setup. Until now, the theoretical results on the structure identifiability of DAG learning with latent confounder are always too weak to be used in practice since too strict assumptions are taken. In the recent progress of the latent variables research, <ref type="bibr" target="#b3">Bhattacharya et al. (2021)</ref> takes the acyclic directed mixed graphs (ADMGs) to describe the graphs with latent confounders. With different types of restrictions, three classes of proprieties, named Ancestral graph, Acid graph, and Bow-free graph, are given.</p><p>According to different proprieties, different graph constraints are given. For example, trace e D -d + sum(D ‚Ä¢ B) = 0 is set for the Bow-free graph<ref type="foot" target="#foot_9">foot_9</ref> , where D is the adjacency matrix recording the directed edges and B records the double-directed edges. We can directly replace the constraint to incorporate this method in our framework. However, this method can only deal with the linear Gaussian case, which is somewhat limited.</p><p>the 1000 steps, U s are averaged every 200 steps (Yes, the simple average is nothing with acyclicity). When finishing 1000 steps (also the 5-th 200 steps is just finished), a new U new is obtained. Then, Œ± t and œÅ t are updated to Œ± t+1 and œÅ t+1 to formulate the next sub-problem of ALM, which are described in steps 5 ‚àº 9 in Algorithm 1. Then, a new circulation begins. Therefore, we argue that (1) the acyclicity constraint is guaranteed by taking the acyclicity penalty when solving each sub-problem.</p><p>(2) the convergence of U is supported by the convergence analysis of Personalized FedAvg of heterogeneous data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Supplementary experimental details D.1 Uneven distributions</head><p>For federated learning problems in the real world, different clients may own different amounts of observations. To verify the stability of our method, we simulate the setting of uneven distributions in different clients. For each client, the number of observations is randomly chosen from a list [20%, 40%, 60%, 80%] √ó n, where n is the maximal observation. The experimental results are shown in Fig. <ref type="figure" target="#fig_6">6</ref>, from which we can find that our method show relatively stable performance in this setting. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Varying clients</head><p>In this setting, we now consider a fixed number of samples distributed across different clients. We conduct experiments for <ref type="bibr">(2,</ref><ref type="bibr">4,</ref><ref type="bibr">6,</ref><ref type="bibr">8)</ref> clients and show the results in Fig. <ref type="figure" target="#fig_7">7</ref>. With the increase in clients number, our method can show better performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 Dense graphs</head><p>Our method is also implemented on some denser graphs. Experimental results in Table <ref type="table" target="#tab_9">9</ref> and<ref type="table" target="#tab_10">Table 10</ref>. From these experimental results, we can see that our method shows consistently better performance over other methods on the denser graph setting. For the homogeneous case, both AS-FedDAG and GS-FedDAG obtain the nearly low SHD as MCSL trained on all data and far better than all methods trained on separated data. For the heterogeneous case, our GS-FedDAG still shows the best performance. Compared to NOTEARS in 20 variables case, GS-FedDAG shows similar SHD results but a much better TPR result. Therefore, how to reduce the false discovery rate of GS-FedDAG would be an exciting thing.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.4 Comparisons with voting method</head><p>There is another interesting research line <ref type="bibr" target="#b40">(Na &amp; Yang, 2010)</ref>, which also tries to learn DAG from decentralized data. We add a DAG combination method proposed in <ref type="bibr" target="#b40">(Na &amp; Yang, 2010)</ref>, which proposes to vote for each entry of the adjacency matrix to get the final DAG. From the experimental results in Table <ref type="table" target="#tab_11">11</ref>, we can find that For PC and NOTEARS, the combining method seems to contribute little improvement. This is because the reported DAGs local clients are too bad to get a good result. For MCSL, this combing method works well for improving performance. The reason is easy to be inferred from the results. For MCSL, DAGs reported by local clients are of bad SHDs but good TPR, which means that the False Discovery Rates (FDRs) are high.</p><p>In contrast, the combing method can further reduce the FDRs and keep the TPRs still good. Then, SHD can be further reduced. Luckily, our GS-FedDAG still shows the best performances in all settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.5 Comparisons with CAM</head><p>Here, we add one more identifiable baseline named causal additive model (CAM) <ref type="bibr" target="#b5">(B√ºhlmann et al., 2014)</ref>, which also serves as a baseline in MCSL <ref type="bibr">(Ng et al., 2022b)</ref>, GraNDAG <ref type="bibr" target="#b30">(Lachapelle et al., 2020)</ref>, and DAG-GAN <ref type="bibr" target="#b77">(Yu et al., 2019)</ref>. From results in Table <ref type="table" target="#tab_12">12</ref> and 13, we can see that our methods always show an advantage over CAM. CAM also assumes a non-linear ANM for data generation. However, CAM limits the non-linear function to be additive. In normal ANM, X i = f i (X pai ) + i while CAM assumes X i = j‚ààX(pai) f i‚Üêj (X j ) + i , which limits the capacity of its model. From the above experimental results, we can see that our methods show consistent advantages over CAM.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.6 Comparisons with NOTEARS-ADMM</head><p>In this subsection, we give the experimental comparisons with NOTEARS-ADMM in detail to verify the advantage of our averaging strategy is simple but effective. Firstly, we conduct the results on linear models, which are the main part in <ref type="bibr">Ng &amp; Zhang (2022)</ref>. As shown in Fig. <ref type="figure" target="#fig_13">8</ref>, even on linear models, our AS-FedDAG can consistently show its advantage over NOTEARS-ADMM. Then, for the nonlinear models, we consider two different functions named MLP and Gaussian process (GP). The results are presented in Fig. <ref type="figure" target="#fig_18">9</ref>, from which we can see that FedDAG always show better performance over all settings. Since NOTEARS-ADMM can not handle heterogeneous data, we do not give the results on heterogeneous data for fair comparison.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E More discussions on the experimental results</head><p>Why does our method outperform other methods even some baseline methods using all data for training? Let us first discuss the AS-FedDAG (All-Shared FedDAG), which shares all model parameters (both Œ¶ and U ) among all clients. If we set it f l as 1 in AS-FedDAG, AS-FedDAG is totally the same as MCSL using all data for training. For simplicity, we mark all parameters (actually Œ¶ and U ) of client c k together as Œ∏ c k . Let us consider the t-th iteration when all clients receive the average parameters Œ∏ t from the server and update their parameters by Œ∏ t .</p><p>For AS-FedDAG, firstly, we mark the gradients obtained by using the local data of client c k for k ‚àà [m] as g c k t . Then each client c k updates its parameters for one step by Œ∏ c k t = Œ∏ t -lr √ó g c k t , where lr is the learning rate. Afterward, the server collects all parameters and averages them to get Œ∏ t+1 = (the full gradient is just the average of gradients from all samples). We can find that the updated parameters are totally the same. Then if it f l &gt; 1, we average all parameters every it f l iterations. Even though the exact updating procedures are not the same, the expectations of updated parameters are the same. This is why we say that MCSL trained on all data can serve as an approximate upper bound of our method but unobtainable in our paper.      In <ref type="bibr">GS-FedDAG (Graph-Shared FedDAG)</ref> method, only all graphs are averaged. However, this partial information-sharing mechanism also helps on benefiting information from other clients to find a better solution <ref type="bibr" target="#b7">(Collins et al., 2021)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Discussions on Assumptions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.1 Data heterogeneity</head><p>The general heterogeneous data setup should include the distribution shift caused by interventions since interventions on certain variables would also lead to heterogeneous distribution. Previous work <ref type="bibr">(Huang et al., 2020b)</ref> has investigated this case and proposes the CD-NOD algorithm, which enhances the PC method, to learn from heterogeneous data. However, CD-NOD needs to identify some edge directions by capturing the changing information among distributions. That is to say, this method which needs to gather all data and cause the raw data leakage, of course. In our paper, we restrict our attention to the ANMs, which care more about the mechanisms and noise shift among different clients. Moreover, finding the identifiability conditions for learning graphs from the general heterogeneous data (both mechanisms shift and interventional data) in the federated setup is a challenging but important problem, which is left for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2 Invariant DAG assumption</head><p>Firstly, let us skip the homogeneous data setting of FedDAG, which only assumes all SEMs are totally the same but data are generated at different local clients. Then, we mainly talk about the heterogeneous setting that assumes SEMs vary, but DAG is shared among different clients.      Sep data PC 14.1 ¬± 2.4 0.31 ¬± 0.06 11.1 ¬± 3.6 0.48 ¬± 0.14 13.2 ¬± 3.6 0.42 ¬± 0.09 13.5 ¬± 3.2 0.37 ¬± 0.12 GES 12.7 ¬± 2.7 0.37 ¬± 0.09 10.6 ¬± 3.3 0.54 ¬± 0.12 14.6 ¬± 4.6 0.50 ¬± 0.13 12.0 ¬± 2.6 0.48 ¬± 0.08 DAG-GNN 15.7 ¬± 2.3 0.11 ¬± 0.05 11.7 ¬± 3.3 0.37 ¬± 0.12 17.7 ¬± 3.6 0.39 ¬± 0.11 13.0 ¬± 2.0 0.26 ¬± 0.10 NOTEARS 16.5 ¬± 2.0 0.06 ¬± 0.04 12.3 ¬± 3.0 0.33 ¬± 0.12 13.4 ¬± 3.4 0.35 ¬± 0.14 13.3 ¬± 2.3 0.24 ¬± 0.09 N-S-MLP 8.5 ¬± 2.9 0.56 ¬± 0.13 2.8 ¬± 1.5 0.93 ¬± 0.06 6.4 ¬± 1.3 0.81 ¬± 0.11 7.4 ¬± 2.9 0.67 ¬± 0.13 MCSL 7.1 ¬± 3.2 0.83 ¬± 0.08 4.4 ¬± 2.1 0.91 ¬± 0.06 13.4 ¬± 3.9 0.57 ¬± 0.21 6.5 ¬± 3.5 0.84 ¬± 0.07 GS-FedDAG 2.4 ¬± 2.0 0.86 ¬± 0.12 2.1 ¬± 1.4 0.91 ¬± 0.07 11.1 ¬± 3.1 0.57 ¬± 0.20 2.6 ¬± 1.6 0.87 ¬± 0.09 AS-FedDAG 1.8 ¬± 2.0 0.89 ¬± 0.12 1.7 ¬± 1.6 0.91 ¬± 0.08 10.5 ¬± 3.5 0.59 ¬± 0.22 2.4 ¬± 1.6 0.87 ¬± 0.08</p><p>Essentially, an SEM models the physical processes of a system and the generation process behind observations. Intuitively, different SEMs usually describe different systems. Then, naturally, the DAGs may be different. In the case that the deployed systems on different clients are not the same, our method will break down because of the model misspecification. Unfortunately, it is not straightforward to extend our current framework to deal with this case, and we leave it for future work.</p><p>In this paper, we leave the variant causal graphs case aside and focus on the invariant graph case. This can be explained by the fact that a system can have various SEMs at different statuses <ref type="bibr">(Huang et al., 2020b)</ref>. In the real world, some cases can be supported by our assumptions. The first example can be fMRI recordings. As pointed in <ref type="bibr">(Huang et al., 2020b)</ref>, fMRI recordings are usually non-stationary because information flows in the brain may change with stimuli, tasks, and attention of the subject. Our federated setting only has one more assumption that fMRI recordings among different clients cannot be shared. The second example can be causal gene regulatory network inference <ref type="bibr" target="#b47">(Omranian et al., 2016)</ref>. The causal direction among genes, i.e., which gene regulates which gene, is believed to be the same. However, the SEM mechanism could vary in 3.2 ¬± 2.0 0.81 ¬± 0.12 6.7 ¬± 4.8 0.82 ¬± 0.13 2.5 ¬± 2.1 0.97 ¬± 0.04 8.2 ¬± 5.4 0.87 ¬± 0.09 50% 2.9 ¬± 1.8 0.83 ¬± 0.11 5.8 ¬± 4.4 0.85 ¬± 0.12 1.8 ¬± 1.4 0.99 ¬± 0.02 6.3 ¬± 5.1 0.89 ¬± 0.10 80% 2.7 ¬± 1.9 0.84 ¬± 0.12 6.0 ¬± 3.9 0.86 ¬± 0.10 1.8 ¬± 1.3 0.99 ¬± 0.02 5.9 ¬± 4.1 0.90 ¬± 0.08 100% 2.4 ¬± 2.0 0.86 ¬± 0.12 6.2 ¬± 4.0 0.85 ¬± 0.10 1.9 ¬± 1.6 0.99 ¬± 0.02 6.2 ¬± 4.7 0.89 ¬± 0.09 Table <ref type="table" target="#tab_8">18</ref>: Empirical results on fMRI Hippocampus dataset (Part 2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>All data</head><p>Separate data GS-FedDAG AS-FedDAG GES N-S-MLP DAG-GNN GES N-S-MLP DAG-GNN SHD ‚Üì 8.0 ¬± 0.0 9.0 ¬± 0.0 5.4 ¬± 0.5 8.3 ¬± 1.2 11.3 ¬± 1.0 8.2 ¬± 1.9 6.4 ¬± 0.9 5.0 ¬± 0.0 NNZ 11.0 ¬± 0.0 12.0 ¬± 0.0 3.3 ¬± 0.8 8.5 ¬± 1.1 14.4 ¬± 0.8 5.7 ¬± 1.4 6.8 ¬± 0.6 5.0 ¬± 0.0 TPR ‚Üë 0.43 ¬± 0.00 0.43 ¬± 0.00 0.23 ¬± 0.07 0.31 ¬± 0.17 0.44 ¬± 0.10 0.17 ¬± 0.18 0.27 ¬± 0.12 0.29 ¬± 0.00 FDR ‚Üì 0.73 ¬± 0.00 0.75 ¬± 0.00 0.52 ¬± 0.09 0.75 ¬± 0.12 0.78 ¬± 0. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: (a) Visualization of heterogeneous data. Different colors represent data from different sources, while each sub-figure includes the distribution of one fixed dimension of data from all clients. (b) Normalized structural hamming distances (SHDs) (‚Üì) of three methods, where MCSL (Sep) (Ng et al., 2022b) separately trains one model on local data while MCSL (All) trains one model on all data, which is forbidden in FL.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Acyclicity. A DAG G with d nodes can be represented by a binary adjacency matrix B = [B :,1 |B :,2 | ‚Ä¢ ‚Ä¢ ‚Ä¢ |B :,d ] with B :,i ‚àà {0, 1} d for ‚àÄi ‚àà [d]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>represents the MA part of the model on c k . S c k (‚Ä¢) is the scoring function for evaluating the fitness of the local model of client c k and observations D c k . For score-based DAG structure</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: An overview of FedDAG. Each solid-line box includes the local model for each client. For client c k , the GSL part includes a continuous proxy U c k and g œÑ (‚Ä¢), the Gumbel-Sigmoid function, which maps U c k to approximate the binary adjacency matrix. To approximate the mechanisms, the MA part uses Œ¶ c k , including d neural networks. X c k represents observations on c k and Xc k is the predicted data. X c k firstly goes through the GSL part to select the parental variables and then the MA part to get Xc k . The server coordinates the FL procedures by leveraging U among clients.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Figure 3: Visualization of the learned graph during the optimization process. B -n means the learned graph in the n steps. B est is the final estimated DAG. B G is the ground-truth DAG.</figDesc><graphic coords="13,390.66,308.92,54.26,54.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: The sensitivity analysis of hyper-parameters</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Results of uneven distributions on different clients.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Results of performances with varying clients.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>, there is only one Œ∏. If MCSL uses full gradient information, then Œ∏ t+1 = Œ∏ t -lr √ó</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Comparisons with NOTEARS-ADMM on the linear model (IID).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>ER1 with 10 nodes (MLP).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head></head><label></label><figDesc>ER2 with 10 nodes (MLP).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Comparisons with NOTEARS-ADMM on nonlinear models (Homogeneous data).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Anatomical causal-effect relationships of fMRI Hippocampus dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Results on the linear model (Homogeneous data).</figDesc><table><row><cell></cell><cell cols="2">ER2 with 10 nodes</cell><cell cols="2">SF2 with 10 nodes</cell><cell cols="2">ER2 with 20 nodes</cell><cell cols="2">SF2 with 20 nodes</cell></row><row><cell></cell><cell>SHD ‚Üì</cell><cell>TPR ‚Üë</cell><cell>SHD ‚Üì</cell><cell>TPR ‚Üë</cell><cell>SHD ‚Üì</cell><cell>TPR ‚Üë</cell><cell>SHD ‚Üì</cell><cell>TPR ‚Üë</cell></row><row><cell>PC-All</cell><cell>9.0 ¬± 3.9</cell><cell>0.58 ¬± 0.14</cell><cell>4.4 ¬± 1.3</cell><cell>0.76 ¬± 0.07</cell><cell>18.2 ¬± 5.9</cell><cell cols="2">0.59 ¬± 0.12 22.3 ¬± 4.8</cell><cell>0.48 ¬± 0.08</cell></row><row><cell>GES-All</cell><cell cols="2">7.5 ¬± 10.1 0.82 ¬± 0.25</cell><cell>4.1 ¬± 5.6</cell><cell>0.89 ¬± 0.14</cell><cell cols="4">25.2 ¬± 22.1 0.81 ¬± 0.16 22.1 ¬± 11.8 0.74 ¬± 0.15</cell></row><row><cell>NOTEARS-All</cell><cell>1.6 ¬± 1.6</cell><cell>0.93 ¬± 0.06</cell><cell>1.4 ¬± 1.1</cell><cell>0.92 ¬± 0.05</cell><cell>3.0 ¬± 2.7</cell><cell cols="2">0.94 ¬± 0.06 6.9 ¬± 7.0</cell><cell>0.86 ¬± 0.12</cell></row><row><cell>NOTEARS-Sep</cell><cell>3.0 ¬± 2.2</cell><cell>0.85 ¬± 0.08</cell><cell>3.6 ¬± 2.1</cell><cell>0.83 ¬± 0.10</cell><cell>4.1 ¬± 2.4</cell><cell cols="2">0.91 ¬± 0.05 10.2 ¬± 5.9</cell><cell>0.82 ¬± 0.10</cell></row><row><cell cols="2">NOTEARS-ADMM 4.7 ¬± 3.9</cell><cell>0.89 ¬± 0.12</cell><cell>4.4 ¬± 3.0</cell><cell>0.86 ¬± 0.09</cell><cell>7.9 ¬± 5.9</cell><cell cols="2">0.89 ¬± 0.07 10.7 ¬± 5.3</cell><cell>0.82 ¬± 0.08</cell></row><row><cell>AS-FedDAG</cell><cell cols="5">1.3 ¬± 1.5 0.94 ¬± 0.07 1.6 ¬± 1.0 0.91 ¬± 0.06 3.9 ¬± 3.1</cell><cell cols="2">0.91 ¬± 0.06 9.4 ¬± 6.7</cell><cell>0.82 ¬± 0.12</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Results on the nonlinear ANM with GP (Homogeneous data). FedDAG perform better than the other methods in the fashion of separate training. NOTEARS and DAG-GNN, as continuous search methods, obtain unsatisfactory results due to the weak model capacity and improper model assumption. In contrast, the BIC score of GES gets a linear-Gaussian likelihood, which is incapable to deal with non-linear data 8 . With the number of nodes increasing, GS-FedDAG still shows better results than the closely-related baseline method MCSL. However, NOTEAES-MLP can show a comparable result with GS-FedDAG owing to the advantage over MCSL.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">ER2 with 10 nodes</cell><cell cols="2">SF2 with 10 nodes</cell><cell cols="2">ER2 with 40 nodes</cell><cell cols="2">SF2 with 40 nodes</cell></row><row><cell></cell><cell></cell><cell>SHD ‚Üì</cell><cell>TPR ‚Üë</cell><cell>SHD ‚Üì</cell><cell>TPR ‚Üë</cell><cell>SHD ‚Üì</cell><cell>TPR ‚Üë</cell><cell>SHD ‚Üì</cell><cell>TPR ‚Üë</cell></row><row><cell></cell><cell>PC</cell><cell cols="2">15.3 ¬± 2.6 0.37 ¬± 0.10</cell><cell cols="2">14.1 ¬± 4.3 0.44 ¬± 0.20</cell><cell>84.9 ¬± 13.4</cell><cell>0.40 ¬± 0.08</cell><cell>95.0 ¬± 10.4</cell><cell>0.36 ¬± 0.07</cell></row><row><cell>All data</cell><cell>GES NOTEARS N-S-MLP DAG-GNN</cell><cell cols="2">13.0 ¬± 3.9 0.50 ¬± 0.18 16.5 ¬± 2.0 0.05 ¬± 0.04 8.1 ¬± 3.8 0.56 ¬± 0.17 16.2 ¬± 2.1 0.07 ¬± 0.06</cell><cell cols="2">9.6 ¬± 4.4 14.5 ¬± 1.1 0.09 ¬± 0.07 0.71 ¬± 0.17 8.3 ¬± 2.8 0.51 ¬± 0.16 15.2 ¬± 0.8 0.05 ¬± 0.05</cell><cell>59.0 ¬± 9.8 71.2 ¬± 7.2 45.3 ¬± 6.8 73.0 ¬± 7.7</cell><cell>0.53 ¬± 0.08 0.08 ¬± 0.03 0.43 ¬± 0.08 0.06 ¬± 0.03</cell><cell>73.8 ¬± 11.9 70.8 ¬± 2.3 49.2 ¬± 7.7 72.4 ¬± 1.6</cell><cell>0.47 ¬± 0.10 0.07 ¬± 0.03 0.39 ¬± 0.09 0.05 ¬± 0.02</cell></row><row><cell></cell><cell>MCSL</cell><cell>1.9 ¬± 1.5</cell><cell>0.90 ¬± 0.08</cell><cell>1.6 ¬± 1.2</cell><cell>0.91 ¬± 0.07</cell><cell>25.4 ¬± 13.1</cell><cell>0.68 ¬± 0.14</cell><cell>31.6 ¬± 10.0</cell><cell>0.59 ¬± 0.13</cell></row><row><cell></cell><cell>PC</cell><cell cols="2">14.1 ¬± 2.4 0.31 ¬± 0.06</cell><cell cols="2">13.6 ¬± 2.7 0.30 ¬± 0.10</cell><cell>83.8 ¬± 7.4</cell><cell>0.24 ¬± 0.03</cell><cell>86.1 ¬± 4.6</cell><cell>0.23 ¬± 0.04</cell></row><row><cell>Sep data</cell><cell>GES NOTEARS N-S-MLP DAG-GNN</cell><cell cols="2">12.7 ¬± 2.7 0.37 ¬± 0.09 16.5 ¬± 2.0 0.06 ¬± 0.04 8.5 ¬± 2.9 0.56 ¬± 0.13 15.7 ¬± 2.3 0.11 ¬± 0.05</cell><cell cols="2">12.7 ¬± 2.4 0.33 ¬± 0.11 14.6 ¬± 1.0 0.09 ¬± 0.06 8.7 ¬± 2.9 0.53 ¬± 0.16 14.5 ¬± 1.0 0.10 ¬± 0.06</cell><cell>71.0 ¬± 6.7 71.1 ¬± 7.3 51.0 ¬± 6.9 71.5 ¬± 7.5</cell><cell>0.29 ¬± 0.03 0.08 ¬± 0.03 0.41 ¬± 0.06 0.08 ¬± 0.02</cell><cell>73.2 ¬± 4.4 70.7 ¬± 2.0 53.6 ¬± 5.5 70.8 ¬± 1.8</cell><cell>0.29 ¬± 0.05 0.07 ¬± 0.03 0.39 ¬± 0.08 0.07 ¬± 0.02</cell></row><row><cell></cell><cell>MCSL</cell><cell>7.1 ¬± 3.2</cell><cell>0.83 ¬± 0.08</cell><cell>6.9 ¬± 2.8</cell><cell>0.84 ¬± 0.08</cell><cell>77.3 ¬± 19.8</cell><cell>0.64 ¬± 0.11</cell><cell>72.9 ¬± 16.4</cell><cell>0.58 ¬± 0.13</cell></row></table><note><p><p><p><p><p><p><p><p>GS-FedDAG 2.4 ¬± 2.0 0.86 ¬± 0.13 2.7 ¬± 2.2 0.86 ¬± 0.13 36.5 ¬± 12.1 0.65 ¬± 0.15 46.4 ¬± 10.4 0.57 ¬± 0.13 AS-FedDAG 1.8 ¬± 2.0 0.89 ¬± 0.12 2.5 ¬± 2.7 0.85 ¬± 0.15 30.0 ¬± 12.3 0.74 ¬± 0.15 31.5 ¬± 10.0 0.59 ¬± 0.13 Results on the nonlinear model. For the nonlinear setting, all data are generated by an ANM and divided into 10 pieces. Each f i is sampled from a Gaussian Process (GP) with RBF kernel of bandwidth one (See Table</p>14</p>and Table</p>15</p>in Appendix. D for results of other functions.) and noises are sampled from one zero-mean Gaussian distribution with fixed variance. We consider graphs of d nodes and 2d expected edges.</p>Experimental results are reported in Table</p>2</p>with nodes 10 and 40. Since all local data are homogeneous, here, we also provide another effective training method named AS-FedDAG, in which the MA parts are also shared among clients. In all settings, AS-FedDAG shows a better performance than GS-FedDAG because more model information is shared during training. While GS-FedDAG can also show a consistent advantage over other methods. When separately training local models, all models suffer from data scarcity. Therefore, we can observe that both GS-FedDAG and AS-</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Results on ANMs with heterogeneous data.</figDesc><table><row><cell></cell><cell></cell><cell cols="2">ER2 with 10 nodes</cell><cell cols="2">SF2 with 10 nodes</cell><cell cols="2">ER2 with 40 nodes</cell><cell cols="2">SF2 with 40 nodes</cell></row><row><cell></cell><cell></cell><cell>SHD ‚Üì</cell><cell>TPR ‚Üë</cell><cell>SHD ‚Üì</cell><cell>TPR ‚Üë</cell><cell>SHD ‚Üì</cell><cell>TPR ‚Üë</cell><cell>SHD ‚Üì</cell><cell>TPR ‚Üë</cell></row><row><cell></cell><cell>PC</cell><cell cols="2">22.3 ¬± 4.2 0.41 ¬± 0.11</cell><cell cols="2">21.0 ¬± 3.6 0.41 ¬± 0.12</cell><cell cols="2">151.9 ¬± 14.2 0.27 ¬± 0.08</cell><cell>152.5 ¬± 5.4</cell><cell>0.26 ¬± 0.04</cell></row><row><cell>All data</cell><cell>GES NOTEARS N-S-MLP DAG-GNN</cell><cell cols="2">26.4 ¬± 6.2 0.53 ¬± 0.14 20.4 ¬± 4.1 0.49 ¬± 0.14 22.8 ¬± 5.0 0.87 ¬± 0.07 21.2 ¬± 6.0 0.39 ¬± 0.11</cell><cell cols="2">25.4 ¬± 4.6 0.54 ¬± 0.13 18.7 ¬± 3.3 0.45 ¬± 0.11 24.7 ¬± 3.3 0.88 ¬± 0.07 16.6 ¬± 3.0 0.48 ¬± 0.18</cell><cell cols="2">NaN 164.8 ¬± 47.4 0.39 ¬± 0.07 NaN 344.4 ¬± 71.9 0.92 ¬± 0.08 146.6 ¬± 41.6 0.29 ¬± 0.08</cell><cell cols="2">NaN 178.1 ¬± 33.0 0.40 ¬± 0.10 NaN 325.0 ¬± 50.2 0.85 ¬± 0.08 168.2 ¬± 34.2 0.31 ¬± 0.09</cell></row><row><cell></cell><cell>MCSL</cell><cell cols="2">19.4 ¬± 4.4 0.75 ¬± 0.19</cell><cell cols="2">19.0 ¬± 4.0 0.81 ¬± 0.14</cell><cell cols="2">118.6 ¬± 18.1 0.68 ¬± 0.11</cell><cell cols="2">126.9 ¬± 16.5 0.59 ¬± 0.12</cell></row><row><cell></cell><cell>PC</cell><cell cols="2">12.5 ¬± 2.7 0.45 ¬± 0.07</cell><cell cols="2">11.0 ¬± 2.1 0.49 ¬± 0.07</cell><cell>65.7 ¬± 11.0</cell><cell>0.43 ¬± 0.06</cell><cell>73.7 ¬± 5.5</cell><cell>0.36 ¬± 0.05</cell></row><row><cell>Sep data</cell><cell>GES NOTEARS N-S-MLP DAG-GNN</cell><cell cols="2">12.9 ¬± 2.6 0.58 ¬± 0.07 7.6 ¬± 2.6 0.60 ¬± 0.11 5.2 ¬± 1.4 0.80 ¬± 0.05 8.2 ¬± 2.9 0.67 ¬± 0.12</cell><cell cols="2">10.3 ¬± 2.8 0.60 ¬± 0.09 7.6 ¬± 1.8 0.58 ¬± 0.09 6.1 ¬± 1.6 0.76 ¬± 0.05 8.4 ¬± 2.1 0.67 ¬± 0.09</cell><cell>68.2 ¬± 20.8 34.9 ¬± 12.7 46.0 ¬± 10.2 45.7 ¬± 13.5</cell><cell>0.65 ¬± 0.09 0.63 ¬± 0.11 0.73 ¬± 0.08 0.64 ¬± 0.11</cell><cell>77.2 ¬± 13.8 43.4 ¬± 8.4 56.0 ¬± 9.5 52.7 ¬± 8.4</cell><cell>0.60 ¬± 0.07 0.53 ¬± 0.10 0.66 ¬± 0.09 0.60 ¬± 0.11</cell></row><row><cell></cell><cell>MCSL</cell><cell>9.2 ¬± 1.8</cell><cell>0.72 ¬± 0.06</cell><cell>8.9 ¬± 2.0</cell><cell>0.71 ¬± 0.08</cell><cell>76.1 ¬± 13.7</cell><cell>0.53 ¬± 0.09</cell><cell>78.1 ¬± 6.3</cell><cell>0.47 ¬± 0.07</cell></row><row><cell></cell><cell cols="2">AS-FedDAG 3.4 ¬± 1.7</cell><cell>0.97 ¬± 0.04</cell><cell>2.7 ¬± 1.6</cell><cell>0.90 ¬± 0.07</cell><cell>35.9 ¬± 17.0</cell><cell>0.84 ¬± 0.09</cell><cell>41.8 ¬± 12.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Empirical results on fMRI Hippocampus dataset (Part 1).</figDesc><table><row><cell></cell><cell>All data</cell><cell></cell><cell></cell><cell>Separate data</cell><cell>GS-FedDAG AS-FedDAG</cell></row><row><cell>PC</cell><cell>NOTEARS</cell><cell>MCSL</cell><cell>PC</cell><cell>NOTEARS</cell><cell>MCSL</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Model parameters for each client with different nodes.</figDesc><table><row><cell></cell><cell cols="3">10 nodes 20 nodes 40 nodes</cell></row><row><cell>NOTEARS</cell><cell>100</cell><cell>400</cell><cell>1600</cell></row><row><cell cols="2">Mask of MCSL 100</cell><cell>400</cell><cell>1600</cell></row><row><cell>NN of MCSL</cell><cell>9930</cell><cell>19860</cell><cell>39720</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 8 :</head><label>8</label><figDesc>The combinations of œÅ init and Œ≤ on simulated data in our method.</figDesc><table><row><cell>10 nodes</cell><cell>20 nodes</cell><cell>40 nodes</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 9 :</head><label>9</label><figDesc>Results on nonlinear ANM with dense graphs (Homogeneous data).</figDesc><table><row><cell></cell><cell></cell><cell cols="2">ER4 with 10 nodes</cell><cell cols="2">SF4 with 10 nodes</cell><cell cols="2">ER4 with 20 nodes</cell><cell cols="2">SF4 with 20 nodes</cell></row><row><cell></cell><cell></cell><cell>SHD ‚Üì</cell><cell>TPR ‚Üë</cell><cell>SHD ‚Üì</cell><cell>TPR ‚Üë</cell><cell>SHD ‚Üì</cell><cell>TPR ‚Üë</cell><cell>SHD ‚Üì</cell><cell>TPR ‚Üë</cell></row><row><cell>All data</cell><cell>PC NOTEARS MCSL</cell><cell>27.3 ¬± 3.2 34.3 ¬± 1.7 15.5 ¬± 5.9</cell><cell>0.29 ¬± 0.07 0.03 ¬± 0.02 0.57 ¬± 0.15</cell><cell>18.9 ¬± 4.9 22.7 ¬± 1.3 4.5 ¬± 3.1</cell><cell>0.37 ¬± 0.16 0.05 ¬± 0.05 0.83 ¬± 0.11</cell><cell>68.2 ¬± 9.5 71.8 ¬± 7.2 33.8 ¬± 10.4</cell><cell>0.23 ¬± 0.06 0.03 ¬± 0.01 0.55 ¬± 0.11</cell><cell>60.2 ¬± 9.3 62.8 ¬± 0.9 19.8 ¬± 7.5</cell><cell>0.30 ¬± 0.08 0.02 ¬± 0.01 0.69 ¬± 0.11</cell></row><row><cell>Sep data</cell><cell>PC NOTEARS MCSL</cell><cell cols="5">31.5 ¬± 2.1 34.3 ¬± 1.8 15.8 ¬± 3.3 0.61 ¬± 0.09 8.3 ¬± 4.3 0.14 ¬± 0.03 20.4 ¬± 0.58 0.21 ¬± 0.03 0.03 ¬± 0.01 22.7 ¬± 1.0 0.06 ¬± 0.04 0.78 ¬± 0.11 49.3 ¬± 11.8 68.7 ¬± 8.1 70.1 ¬± 6.9</cell><cell cols="3">0.13 ¬± 0.03 0.03 ¬± 0.01 0.63 ¬± 0.10 39.7 ¬± 5.6 60.9 ¬± 2.8 62.3 ¬± 0.56 0.03 ¬± 0.01 0.15 ¬± 0.02 0.73 ¬± 0.07</cell></row><row><cell></cell><cell cols="4">GS-FedDAG 16.9 ¬± 4.9 0.53 ¬± 0.12 5.4 ¬± 3.0</cell><cell>0.78 ¬± 0.12</cell><cell cols="2">35.4 ¬± 10.9 0.53 ¬± 0.11</cell><cell cols="2">20.7 ¬± 5.1 0.69 ¬± 0.08</cell></row><row><cell></cell><cell cols="2">AS-FedDAG 17.4 ¬± 4.8</cell><cell cols="2">0.53 ¬± 0.12 5.5 ¬± 2.8</cell><cell cols="2">0.79 ¬± 0.11 40.7 ¬± 4.8</cell><cell cols="3">0.57 ¬± 0.10 24.1 ¬± 5.8 0.71 ¬± 0.09</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 10 :</head><label>10</label><figDesc>Results on nonlinear ANM with dense graphs (Heterogeneous data).</figDesc><table><row><cell></cell><cell></cell><cell cols="2">ER4 with 10 nodes</cell><cell cols="2">SF4 with 10 nodes</cell><cell cols="2">ER4 with 20 nodes</cell><cell>SF4 with 20 nodes</cell></row><row><cell></cell><cell></cell><cell>SHD ‚Üì</cell><cell>TPR ‚Üë</cell><cell>SHD ‚Üì</cell><cell>TPR ‚Üë</cell><cell>SHD ‚Üì</cell><cell>TPR ‚Üë</cell><cell>SHD ‚Üì</cell><cell>TPR ‚Üë</cell></row><row><cell>Sep data</cell><cell>PC NOTEARS MCSL</cell><cell cols="2">29.3 ¬± 1.3 0.23 ¬± 0.03 20.5 ¬± 2.6 0.45 ¬± 0.08 20.0 ¬± 3.2 0.52 ¬± 0.07</cell><cell cols="2">20.3 ¬± 2.1 0.31 ¬± 0.06 12.2 ¬± 2.9 0.54 ¬± 0.11 13.7 ¬± 2.2 0.65 ¬± 0.07</cell><cell>71.9 ¬± 8.1 43.2 ¬± 7.0 65.1 ¬± 7.7</cell><cell>0.19 ¬± 0.03 0.49 ¬± 0.08 0.33 ¬± 0.05</cell><cell>62.7 ¬± 2.8 39.4 ¬± 6.8 0.47 ¬± 0.10 0.22 ¬± 0.03 59.4 ¬± 5.3 0.31 ¬± 0.05</cell></row><row><cell></cell><cell cols="4">GS-FedDAG 8.5 ¬± 3.7 0.84 ¬± 0.09 4.5 ¬± 2.0</cell><cell cols="4">0.93 ¬± 0.07 40.7 ¬± 14.5 0.74 ¬± 0.07 39.9 ¬± 10.8 0.68 ¬± 0.07</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 11 :</head><label>11</label><figDesc>Comparison with the voting method.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">Homogeneous data (GP)</cell><cell></cell><cell></cell><cell cols="2">Heterogeneous data</cell></row><row><cell></cell><cell></cell><cell cols="2">ER2 with 10 nodes</cell><cell cols="2">ER2 with 20 nodes</cell><cell cols="2">ER2 with 10 nodes</cell><cell>ER2 with 20 nodes</cell></row><row><cell></cell><cell></cell><cell>SHD ‚Üì</cell><cell>TPR ‚Üë</cell><cell>SHD ‚Üì</cell><cell>TPR ‚Üë</cell><cell>SHD ‚Üì</cell><cell>TPR ‚Üë</cell><cell>SHD ‚Üì</cell><cell>TPR ‚Üë</cell></row><row><cell>Sep data</cell><cell>PC NOTEARS MCSL</cell><cell cols="2">14.1 ¬± 2.4 0.31 ¬± 0.06 16.5 ¬± 2.0 0.06 ¬± 0.04 7.1 ¬± 3.2 0.83 ¬± 0.08</cell><cell cols="2">32.7 ¬± 6.5 0.28 ¬± 0.07 31.7 ¬± 6.0 0.11 ¬± 0.04 24.8 ¬± 5.5 0.88 ¬± 0.07</cell><cell cols="2">12.5 ¬± 2.7 0.45 ¬± 0.07 7.6 ¬± 2.6 0.60 ¬± 0.11 9.2 ¬± 1.8 0.72 ¬± 0.06</cell><cell>28.5 ¬± 6.3 15.0 ¬± 3.1 23.3 ¬± 5.8</cell><cell>0.44 ¬± 0.07 0.62 ¬± 0.09 0.56 ¬± 0.08</cell></row><row><cell>Voting</cell><cell>PC NOTEARS MCSL</cell><cell cols="2">13.3 ¬± 3.0 0.27 ¬± 0.11 15.6 ¬± 2.2 0.11 ¬± 0.06 8.0 ¬± 3.1 0.85 ¬± 0.16</cell><cell cols="4">29.7 ¬± 5.9 0.22 ¬± 0.05 32.6 ¬± 6.2 0.09 ¬± 0.05 18.1 ¬± 7.8 0.88 ¬± 0.06 6.9 ¬± 2.2 11.4 ¬± 3.4 0.36 ¬± 0.13 7.8 ¬± 4.0 0.56 ¬± 0.20 0.71 ¬± 0.13</cell><cell>25.5 ¬± 6.8 18.4 ¬± 11.6 0.49 ¬± 0.30 0.29 ¬± 0.13 10.1 ¬± 4.6 0.79 ¬± 0.09</cell></row><row><cell></cell><cell cols="5">GS-FedDAG 2.4 ¬± 2.0 0.86 ¬± 0.12 6.2 ¬± 4.0 0.85 ¬± 0.10</cell><cell cols="3">1.9 ¬± 1.6 0.99 ¬± 0.02 6.2 ¬± 4.7</cell><cell>0.89 ¬± 0.09</cell></row><row><cell></cell><cell cols="6">AS-FedDAG 1.8 ¬± 2.0 0.89 ¬± 0.12 5.0 ¬± 4.2 0.88 ¬± 0.11 NaN</cell><cell>NaN</cell><cell>NaN</cell><cell>NaN</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 12 :</head><label>12</label><figDesc>Comparisons with CAM on nonlinear ANM (Homogeneous data-GP).</figDesc><table><row><cell></cell><cell cols="2">ER2 with 10 nodes</cell><cell cols="2">SF2 with 10 nodes</cell><cell cols="2">ER2 with 20 nodes</cell><cell cols="2">SF2 with 20 nodes</cell></row><row><cell></cell><cell>SHD ‚Üì</cell><cell>TPR ‚Üë</cell><cell>SHD ‚Üì</cell><cell>TPR ‚Üë</cell><cell>SHD ‚Üì</cell><cell>TPR ‚Üë</cell><cell>SHD ‚Üì</cell><cell>TPR ‚Üë</cell></row><row><cell>All data CAM</cell><cell>9.5 ¬± 2.9</cell><cell>0.87 ¬± 0.09</cell><cell>9.1 ¬± 3.1</cell><cell>0.84 ¬± 0.10</cell><cell cols="2">21.4 ¬± 4.7 0.77,¬± 0.08</cell><cell cols="2">26.6 ¬± 6.1 0.75 ¬± 0.07</cell></row><row><cell>Sep data CAM</cell><cell cols="2">11.8 ¬± 2.6 0.40 ¬± 0.10</cell><cell cols="2">11.1 ¬± 1.5 0.38 ¬± 0.11</cell><cell cols="2">24.3 ¬± 5.8 0.40 ¬± 0.07</cell><cell cols="2">26.8 ¬± 2.0 0.36 ¬± 0.06</cell></row><row><cell cols="2">GS-FedDAG 2.4 ¬± 2.0</cell><cell>0.86 ¬± 0.12</cell><cell>2.7 ¬± 2.2</cell><cell cols="2">0.86 ¬± 0.13 6.2 ¬± 4.0</cell><cell>0.85 ¬± 0.10</cell><cell cols="2">14.7 ¬± 7.0 0.80 ¬± 0.11</cell></row></table><note><p>AS-FedDAG 1.8 ¬± 2.0 0.89 ¬± 0.12 2.5 ¬± 2.7 0.85 ¬± 0.15 5.0 ¬± 4.2 0.88 ¬± 0.11 7.8 ¬± 5.5 0.80 ¬± 0.14</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Table 13 :</head><label>13</label><figDesc>Comparisons with CAM on nonlinear ANM (Heterogeneous data).</figDesc><table><row><cell></cell><cell cols="2">ER2 with 10 nodes</cell><cell cols="2">SF2 with 10 nodes</cell><cell cols="2">ER2 with 20 nodes</cell><cell cols="2">SF2 with 20 nodes</cell></row><row><cell></cell><cell>SHD ‚Üì</cell><cell>TPR ‚Üë</cell><cell>SHD ‚Üì</cell><cell>TPR ‚Üë</cell><cell>SHD ‚Üì</cell><cell>TPR ‚Üë</cell><cell>SHD ‚Üì</cell><cell>TPR ‚Üë</cell></row><row><cell>All data CAM</cell><cell cols="2">31.9 ¬± 4.8 0.39 ¬± 0.15</cell><cell cols="2">31.8 ¬± 4.4 0.31 ¬± 0.17</cell><cell cols="2">104.6 ¬± 15.4 0.46 ¬± 0.15</cell><cell cols="2">116.9 ¬± 13.8 0.35 ¬± 0.07</cell></row><row><cell>Sep data CAM</cell><cell cols="2">18.0 ¬± 1.7 0.52 ¬± 0.04</cell><cell cols="2">17.8 ¬± 2.1 0.51 ¬± 0.3</cell><cell>47.5 ¬± 9.2</cell><cell>0.52 ¬± 0.04</cell><cell>53.0 ¬± 6.1</cell><cell>0.50 ¬± 0.03</cell></row><row><cell cols="6">GS-FedDAG 1.9 ¬± 1.6 0.99 ¬± 0.02 2.6 ¬± 1.3 0.93 ¬± 0.07 6.2 ¬± 4.7</cell><cell cols="2">0.89 ¬± 0.09 11.5 ¬± 6.7</cell><cell>0.81 ¬± 0.14</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_14"><head>Table 14 :</head><label>14</label><figDesc>Results on nonlinear ANM with different functions (Homogeneous data, 10 nodes, ER2).</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>GP</cell><cell></cell><cell>MIM</cell><cell></cell><cell>MLP</cell><cell cols="2">GP-add</cell></row><row><cell></cell><cell></cell><cell>SHD ‚Üì</cell><cell>TPR ‚Üë</cell><cell>SHD ‚Üì</cell><cell>TPR ‚Üë</cell><cell>SHD ‚Üì</cell><cell>TPR ‚Üë</cell><cell>SHD ‚Üì</cell><cell>TPR ‚Üë</cell></row><row><cell></cell><cell>PC</cell><cell cols="2">15.3 ¬± 2.6 0.37 ¬± 0.10</cell><cell cols="2">11.0 ¬± 4.9 0.60 ¬± 0.16</cell><cell>11.8 ¬± 4.3</cell><cell>0.61 ¬± 0.14</cell><cell cols="2">14.0 ¬± 4.7 0.49 ¬± 0.16</cell></row><row><cell>All data</cell><cell>GES DAG-GNN NOTEARS N-S-MLP</cell><cell cols="2">13.0 ¬± 3.9 0.50 ¬± 0.18 16.2 ¬± 2.1 0.07 ¬± 0.06 16.5 ¬± 2.0 0.05 ¬± 0.04 8.1 ¬± 3.8 0.56 ¬± 0.17</cell><cell cols="2">9.6 ¬± 4.4 13.7 ¬± 2.4 0.26 ¬± 0.10 0.71 ¬± 0.17 12.1 ¬± 3.2 0.34 ¬± 0.13 1.6 ¬± 1.3 0.95 ¬± 0.06</cell><cell>15.8 ¬± 6.0 18.2 ¬± 3.3 13.3 ¬± 3.4 5.6 ¬± 1.3</cell><cell>0.63 ¬± 0.14 0.36 ¬± 0.12 0.35 ¬± 0.15 0.81 ¬± 0.11</cell><cell cols="2">14.4 ¬± 4.9 0.57 ¬± 0.17 13.3 ¬± 2.3 0.24 ¬± 0.10 13.4 ¬± 2.2 0.23 ¬± 0.09 6.8 ¬± 4.0 0.65 ¬± 0.16</cell></row><row><cell></cell><cell>MCSL</cell><cell>1.9 ¬± 1.5</cell><cell>0.90 ¬± 0.08</cell><cell>0.7 ¬± 1.2</cell><cell>0.97 ¬± 0.06</cell><cell>12.7 ¬± 3.6</cell><cell>0.58 ¬± 0.24</cell><cell>1.9 ¬± 1.7</cell><cell>0.91 ¬± 0.07</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_15"><head>Table 17 :</head><label>17</label><figDesc>Results on randomly selecting models-info of partial clients (heterogeneous data, 20 nodes, ER2). ¬± 2.4 0.78 ¬± 0.14 8.6 ¬± 4.8 0.77 ¬± 0.13 3.8 ¬± 1.4 0.93 ¬± 0.05 8.5 ¬± 5.4 0.89 ¬± 0.07 20%</figDesc><table><row><cell></cell><cell></cell><cell cols="2">Homogeneous data</cell><cell></cell><cell></cell><cell cols="2">Heterogeneous data</cell></row><row><cell></cell><cell cols="2">ER2 with 10 nodes</cell><cell cols="2">ER2 with 20 nodes</cell><cell cols="2">ER2 with 10 nodes</cell><cell cols="2">ER2 with 20 nodes</cell></row><row><cell></cell><cell>SHD ‚Üì</cell><cell>TPR ‚Üë</cell><cell>SHD ‚Üì</cell><cell>TPR ‚Üë</cell><cell>SHD ‚Üì</cell><cell>TPR ‚Üë</cell><cell>SHD ‚Üì</cell><cell>TPR ‚Üë</cell></row><row><cell>10%</cell><cell>3.8</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>r</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>m</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Notice that in this paper, we restrict our scope to define the privacy leakage by sharing the raw data of users.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>In the intact graph structure of ANMs, we just fix directed edges from i to X i and assume the distribution of i . Therefore, in this paper, G is only defined over the endogenous variables.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>For simplicity, we use [d] = {1, 2, ‚Ä¢ ‚Ä¢ ‚Ä¢ , d} to represent the set of all integers from 1 to d.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>This assumption can be relaxed to some restricted cases with latent variables. See Appendix C.3 for details.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>Please notice that GSL parts of different clients may not be the same during the training procedure. So we index them.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>The consistency of BIC score for learning graphs on ANMs is discussed in Appendix C.5.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_6"><p>Published in Transactions on Machine LearningResearch (01/2023)   </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_7"><p>Notice that the DAG structure encoded in the data is not a secret for the data owners (clients).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_8"><p>Please find the ablation experiment with linear data and more discussions of the experimental results in Appendix D.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_9"><p>See more details at Section 4 in<ref type="bibr" target="#b3">(Bhattacharya et al., 2021)</ref> </p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p>LS is supported by the <rs type="funder">Major Science and Technology Innovation 2030 "Brain Science and Brain-like Research" key project</rs> (No. <rs type="grantNumber">2021ZD0201405</rs>). EG is supported by an <rs type="funder">Australian</rs> <rs type="programName">Government Research Training Program (RTP) Scholarship</rs>. This research was supported by <rs type="funder">The University of Melbourne's Research Computing Services</rs> and the <rs type="funder">Petascale Campus Initiative</rs>. This research was undertaken using the LIEF HPC-GPGPU Facility hosted at the <rs type="institution">University of Melbourne</rs>. This Facility was established with the assistance of LIEF Grant <rs type="grantNumber">LE170100200</rs>. TL was partially supported by <rs type="funder">Australian Research Council</rs> Projects <rs type="grantNumber">DP180103424</rs>, <rs type="grantNumber">DE-190101473</rs>, <rs type="grantNumber">IC-190100031</rs>, <rs type="grantNumber">DP-220102121</rs>, and <rs type="grantNumber">FT-220100318</rs>. MG was supported by <rs type="funder">ARC</rs> <rs type="grantNumber">DE210101624</rs>. HB was supported by <rs type="funder">ARC</rs> <rs type="grantNumber">FT190100374</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_2takRcD">
					<idno type="grant-number">2021ZD0201405</idno>
				</org>
				<org type="funding" xml:id="_mPPcP5p">
					<orgName type="program" subtype="full">Government Research Training Program (RTP) Scholarship</orgName>
				</org>
				<org type="funding" xml:id="_mJ6nsjc">
					<idno type="grant-number">LE170100200</idno>
				</org>
				<org type="funding" xml:id="_Kc7nNmG">
					<idno type="grant-number">DP180103424</idno>
				</org>
				<org type="funding" xml:id="_KQ6VxUh">
					<idno type="grant-number">DE-190101473</idno>
				</org>
				<org type="funding" xml:id="_uWbcJ9B">
					<idno type="grant-number">IC-190100031</idno>
				</org>
				<org type="funding" xml:id="_4DfBWaG">
					<idno type="grant-number">DP-220102121</idno>
				</org>
				<org type="funding" xml:id="_J5Tpy2q">
					<idno type="grant-number">FT-220100318</idno>
				</org>
				<org type="funding" xml:id="_dxbJ9dk">
					<idno type="grant-number">DE210101624</idno>
				</org>
				<org type="funding" xml:id="_zPTyv3g">
					<idno type="grant-number">FT190100374</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix Table of Contents</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4 Broader impact statement</head><p>In federated learning, the server and some clients participate in this process. While as we talked about above, the DAG is shared among all clients. FedDAG is motivated by the "data on each client is not enough for identifying up the ground-truth DAG." The graph information is not private for clients. For the server, it depends. In our previous motivations, we only cared about the "raw data leakage" problem but did not consider the privacy of the graph. Some relations can be public in real-world scenarios, such as disease research. For these cases, our method can still work. However, graph structure may sometimes also be private information. This problem can be easily solved by picking one client as the proxy server.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.5 The consistency results by BIC score</head><p>Actually, for linear additive noise models with Gaussian noises, the consistency results for maximizing the BIC score to identify the DAG (Markov Equivalence Class or DAG) have been well established <ref type="bibr" target="#b63">(Tian &amp; Pearl, 2001;</ref><ref type="bibr">Huang et al., 2020a)</ref>. For this case, with the DAG space constraint, the unique maximum of score function</p><p>with BIC score corresponds to the ground-truth DAG. Even for the high-dimensional consistency for linear Gaussian SEM when the model is identifiable <ref type="bibr" target="#b1">(Aragam et al., 2019)</ref>. Since the ground-truth G corresponds to each S c k , the global maximum arg max Œ¶,U</p><p>with DAG constraint can lead to the ground-truth DAG graph. For nonlinear ANMs, however, even many practical methods, e.g., MCSL <ref type="bibr">(Ng et al., 2022b)</ref>, NOTEARS-MLP <ref type="bibr" target="#b85">(Zheng et al., 2020)</ref>, and CD-RL <ref type="bibr" target="#b9">(Zhu et al., 2020)</ref>, have been proposed to solve this problem by maximizing the BIC score, the theoretical results of consistency are still lacking and would be an interesting future work to be investigated. Therefore, our framework based on these methods inherits the theoretical limit for the nonlinear case. From our paper, however, empirical results can still show the method's effectiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.6 Does the global maximum of Eq. (6) correspond to the ground-truth DAG?</head><p>Firstly, for observations of identifiable ANMs on each client, the unique maximum of score function</p><p>with BIC score corresponds to the ground-truth DAG <ref type="bibr" target="#b84">(Zheng et al., 2018;</ref><ref type="bibr">Ng et al., 2022b)</ref>. Even for the high-dimensional consistency of linear Gaussian SEM in the case when the model is identifiable. Since the ground-truth G corresponds to each S c k , the global maximum arg max Œ¶,U</p><p>with DAG constraint can lead to the ground-truth DAG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.7 Can Algorithms 1 and 2 solve Eq. (6)?</head><p>Unfortunately, the global maximum of Eq. ( <ref type="formula">6</ref>) can not be well reached by the gradient-based optimization methods, which is mainly caused by the non-convex property of the acyclicity constraint. Firstly, discovering the ground-truth DAG is an NP-hard problem. Traditional methods like PC and GES search the discrete DAG space to solve this problem, which is relatively time-consuming. Then, NOTEARS introduces an equality constraint (3) to formulate the DAG search problem as a continuous optimization problem, which can be easily solved by the gradient descent methods. However, the trade-off is that this equality constraint is non-convex, which pushes us away from finding the ground-truth DAG (the global minima of ( <ref type="formula">6</ref>)). That is to say, using gradient descent to solve (6) only can reach the local minima of (6). This similar conclusion stands for recent continuous optimization-based CD methods such as GraNDAG <ref type="bibr" target="#b30">(Lachapelle et al., 2020)</ref>, DAG-GNN <ref type="bibr" target="#b77">(Yu et al., 2019)</ref>, and NOTEARS-MLP <ref type="bibr" target="#b85">(Zheng et al., 2020)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.8 Can U finally satisfy the acyclicity constraint in Eq. (3)?</head><p>For simplicity, please do not mind if we explain our method by setting some parameters with specific values. Firstly, following NOTEARS <ref type="bibr" target="#b84">(Zheng et al., 2018)</ref> and MCSL <ref type="bibr">(Ng et al., 2022b)</ref>, we take the Augmented Lagrangian Method (ALM) to covert the constrained optimization problem into a series of sub-problems without the hard constraint but with two penalty terms. For the t-th sub-problem, the specific formulation of Eq. ( <ref type="formula">8</ref>) is related to Œ± t and œÅ t . Œ± t and œÅ t will be updated to Œ± t+1 and œÅ t+1 after solving the t-th sub-problem for 1000 steps (gradient descent step). When dealing with each sub-problem, each client locally updates its personalized model with acyclicity penalty terms, which is indeed for the acyclicity constraint. During  each individual due to individual properties, such as age, gender, etc. Also, the assumption that domain shifts can also come from the distribution shifts of the exogenous variables (noise terms in our paper) has been widely accepted in the machine learning field, such as invariant causal prediction <ref type="bibr" target="#b53">(Peters et al., 2016)</ref>, IRM <ref type="bibr" target="#b2">(Arjovsky et al., 2019)</ref>.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Hipaa regulations: a new era of medical-record privacy?</title>
		<author>
			<persName><forename type="first">J</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName><surname>Annas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">New England Journal of Medicine</title>
		<imprint>
			<biblScope unit="volume">348</biblScope>
			<biblScope unit="page">1486</biblScope>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Globally optimal score-based learning of directed acyclic graphs in high-dimensions</title>
		<author>
			<persName><forename type="first">Bryon</forename><surname>Aragam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arash</forename><surname>Amini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L√©on</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.02893</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">Invariant risk minimization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Differentiable causal discovery under unmeasured confounding</title>
		<author>
			<persName><forename type="first">Rohit</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Nagarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Malinsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Shpitser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Differentiable causal discovery from interventional data</title>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Brouillard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S√©bastien</forename><surname>Lachapelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Lacoste</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Lacoste-Julien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Drouin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Cam: Causal additive models, high-dimensional order search and penalized regression</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>B√ºhlmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Ernest</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="2526" to="2556" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Optimal structure identification with greedy search</title>
		<author>
			<persName><forename type="first">David</forename><surname>Maxwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chickering</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="507" to="554" />
			<date type="published" when="2002-11">Nov. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Exploiting shared representations for personalized federated learning</title>
		<author>
			<persName><forename type="first">Liam</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hamed</forename><surname>Hassani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aryan</forename><surname>Mokhtari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Shakkottai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Integrating locally learned causal structures with overlapping variables</title>
		<author>
			<persName><forename type="first">David</forename><surname>Danks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clark</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Tillman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Low rank directed acyclic graphs and causal structure learning</title>
		<author>
			<persName><forename type="first">Zhuangyan</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiji</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangbo</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.05691</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">MissDAG: Causal discovery in the presence of missing data with continuous additive noise models</title>
		<author>
			<persName><forename type="first">Erdun</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ignavier</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Howard</forename><surname>Bondell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Dag-gan: Causal structure learning with generative adversarial nets</title>
		<author>
			<persName><forename type="first">Yinghua</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu-Tao</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3320" to="3324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Deep end-to-end causal inference</title>
		<author>
			<persName><forename type="first">Tomas</forename><surname>Geffner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Javier</forename><surname>Antoran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbo</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emre</forename><surname>Kiciman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amit</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Angus</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Kukla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Pawlowski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.02195</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Review of causal discovery methods based on graphical models</title>
		<author>
			<persName><forename type="first">Clark</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Spirtes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Genetics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">524</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Causal diagrams for epidemiologic research</title>
		<author>
			<persName><forename type="first">Sander</forename><surname>Greenland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">M</forename><surname>Robins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>Epidemiology</publisher>
			<biblScope unit="page" from="37" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">On the convergence of local descent methods in federated learning</title>
		<author>
			<persName><forename type="first">Farzin</forename><surname>Haddadpour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehrdad</forename><surname>Mahdavi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.14425</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Econometric causality</title>
		<author>
			<persName><forename type="first">J</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName><surname>Heckman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Statistical Review</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="27" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Nonlinear causal discovery with additive noise models</title>
		<author>
			<persName><forename type="first">Patrik</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Joris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Mooij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><surname>Sch√∂lkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Generalized score functions for causal discovery</title>
		<author>
			<persName><forename type="first">Biwei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Sch√∂lkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clark</forename><surname>Glymour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Knowledge Discovery &amp; Data Mining</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Causal discovery from multiple data sets with non-identical variable sets</title>
		<author>
			<persName><forename type="first">Biwei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clark</forename><surname>Glymour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Association for the Advancement of Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">06</biblScope>
			<biblScope unit="page" from="10153" to="10161" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Causal discovery from heterogeneous/nonstationary data</title>
		<author>
			<persName><forename type="first">Biwei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiji</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Ramsey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruben</forename><surname>Sanchez-Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clark</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Sch√∂lkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1" to="53" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Causal inference in statistics, social, and biomedical sciences</title>
		<author>
			<persName><forename type="first">W</forename><surname>Guido</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><forename type="middle">B</forename><surname>Imbens</surname></persName>
		</author>
		<author>
			<persName><surname>Rubin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Categorical reparameterization with gumbel-softmax</title>
		<author>
			<persName><forename type="first">Eric</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shixiang</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Bayesian networks and decision graphs</title>
		<author>
			<persName><forename type="first">V</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">Dyhre</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName><surname>Nielsen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>Springer</publisher>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">Brendan</forename><surname>Peter Kairouz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brendan</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aur√©lien</forename><surname>Avent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Bellet</surname></persName>
		</author>
		<author>
			<persName><surname>Bennis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitin</forename><surname>Arjun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kallista</forename><surname>Bhagoji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Bonawit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rachel</forename><surname>Cormode</surname></persName>
		</author>
		<author>
			<persName><surname>Cummings</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">L</forename><surname>Rafael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hubert</forename><surname>D'oliveira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salim</forename><forename type="middle">El</forename><surname>Eichner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Rouayheb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Evans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adri√†</forename><surname>Garrett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Badih</forename><surname>Gasc√≥n</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phillip</forename><forename type="middle">B</forename><surname>Ghazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Gibbons</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zaid</forename><surname>Gruteser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaoyang</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lie</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhouyuan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Huo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Justin</forename><surname>Hutchinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tara</forename><surname>Jaggi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gauri</forename><surname>Javidi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakub</forename><surname>Khodak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandra</forename><surname>Konecn√Ω</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Farinaz</forename><surname>Korolova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanmi</forename><surname>Koushanfar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tancr√®de</forename><surname>Koyejo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Lepoint</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prateek</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehryar</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ayfer</forename><surname>Nock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rasmus</forename><surname>√ñzg√ºr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Pagh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramesh</forename><surname>Ramage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mariana</forename><surname>Raskar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawn</forename><surname>Raykova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weikang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><forename type="middle">U</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziteng</forename><surname>Stich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ananda</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Florian</forename><surname>Theertha Suresh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Praneeth</forename><surname>Tram√®r</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianyu</forename><surname>Vepakomma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><forename type="middle">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sen</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Now Foundations and Trends</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Unsuitability of notears for causal graph discovery</title>
		<author>
			<persName><forename type="first">Marcus</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maksim</forename><surname>Sipos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.05441</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<author>
			<persName><forename type="first">Nan</forename><surname>Rosemary Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olexa</forename><surname>Bilaniuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Sch√∂lkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><surname>Mozer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.01075</idno>
		<title level="m">Chris Pal, and Yoshua Bengio. Learning neural causal models from unknown interventions</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">Anthony</forename><forename type="middle">C</forename><surname>Neville K Kitson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhigao</forename><surname>Constantinou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kiattikun</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><surname>Chobtham</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.11415</idno>
		<title level="m">A survey of bayesian network structure learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Probabilistic graphical models: principles and techniques</title>
		<author>
			<persName><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nir</forename><surname>Friedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Gradient-based neural dag learning</title>
		<author>
			<persName><forename type="first">S√©bastien</forename><surname>Lachapelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Brouillard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tristan</forename><surname>Deleu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Lacoste-Julien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Graphical models</title>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Lauritzen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
			<publisher>Clarendon Press</publisher>
			<biblScope unit="volume">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Federated optimization in heterogeneous networks</title>
		<author>
			<persName><forename type="first">Tian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anit</forename><surname>Kumar Sahu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maziar</forename><surname>Sanjabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ameet</forename><surname>Talwalkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Virginia</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Machine Learning and Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Fednlp: Benchmarking federated learning methods for natural language processing tasks</title>
		<author>
			<persName><forename type="first">Chaoyang</forename><surname>Bill Yuchen Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihang</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hulin</forename><surname>Ze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yufen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christophe</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Dupuy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mahdi</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Soltanolkotabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salman</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><surname>Avestimehr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the</title>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="157" to="175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Detecting and correcting for label shift with black box predictors</title>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu-Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Smola</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3122" to="3130" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<author>
			<persName><forename type="first">Rui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.07256</idno>
		<title level="m">Federated graph neural networks: Overview, techniques and challenges</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
	<note>Published in Transactions on Machine Learning Research (01/2023</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Margaret</forename><surname>Mooney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marini</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Burton</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Causality in the social sciences. Sociological methodology</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="347" to="409" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Communicationefficient learning of deep networks from decentralized data</title>
		<author>
			<persName><forename type="first">Brendan</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eider</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ramage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seth</forename><surname>Hampson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Blaise</forename><surname>Aguera Y Arcas</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial intelligence and statistics</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">On causal discovery with cyclic additive noise models</title>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Joris M Mooij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Heskes</surname></persName>
		</author>
		<author>
			<persName><surname>Sch√∂lkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Joint causal inference from multiple contexts</title>
		<author>
			<persName><forename type="first">Sara</forename><surname>Joris M Mooij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Magliacane</surname></persName>
		</author>
		<author>
			<persName><surname>Claassen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1" to="108" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Distributed bayesian network structure learning</title>
		<author>
			<persName><forename type="first">Yongchan</forename><surname>Na</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jihoon</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Symposium on Industrial Electronics</title>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="1607" to="1611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Optimization ii: Standard numerical methods for nonlinear continuous optimization</title>
		<author>
			<persName><forename type="first">Arkadi</forename><surname>Nemirovski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Lecture Note</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Towards federated bayesian network structure learning with continuous optimization</title>
		<author>
			<persName><forename type="first">Ignavier</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="8095" to="8111" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">A graph autoencoder approach to causal structure learning</title>
		<author>
			<persName><forename type="first">Ignavier</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuangyan</forename><surname>Fang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1911.07420</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">On the role of sparsity and dag constraints for learning linear dags</title>
		<author>
			<persName><forename type="first">Ignavier</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amiremad</forename><surname>Ghassami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="17943" to="17954" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">On the convergence of continuous constrained optimization for structure learning</title>
		<author>
			<persName><forename type="first">Ignavier</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S√©bastien</forename><surname>Lachapelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><forename type="middle">Rosemary</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Lacoste-Julien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="8176" to="8198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Masked gradientbased causal structure learning</title>
		<author>
			<persName><forename type="first">Ignavier</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhuangyan</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIAM International Conference on Data Mining</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="424" to="432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Gene regulatory network inference using fused lasso on multiple data sets</title>
		<author>
			<persName><forename type="first">Nooshin</forename><surname>Omranian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeanne</forename><surname>Mo Eloundou-Mbebi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernd</forename><surname>Mueller-Roeber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zoran</forename><surname>Nikoloski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific reports</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Dynotears: Structure learning from time-series data</title>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<editor>
			<persName><forename type="first">Roxana</forename><surname>Pamfil</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Nisara</forename><surname>Sriwattanaworachai</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Shaan</forename><surname>Desai</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Philip</forename><surname>Pilgerstorfer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Konstantinos</forename><surname>Georgatzis</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Paul</forename><surname>Beaumont</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Bryon</forename><surname>Aragam</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Bayesian netwcrks: A model cf self-activated memory for evidential reasoning</title>
		<author>
			<persName><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th conference of the Cognitive Science Society</title>
		<meeting>the 7th conference of the Cognitive Science Society<address><addrLine>University of California, Irvine, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Judea Pearl. Causality. Cambridge university press</publisher>
			<date type="published" when="1985">1985. 2009</date>
			<biblScope unit="page" from="15" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Causal inference in statistics: A primer</title>
		<author>
			<persName><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madelyn</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><forename type="middle">P</forename><surname>Jewell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Identifiability of gaussian structural equation models with equal error variances</title>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>B√ºhlmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="219" to="228" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Identifiability of causal graphs using functional models</title>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Joris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Mooij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><surname>Sch√∂lkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Uncertainty in Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Causal inference by using invariant prediction: identification and confidence intervals</title>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>B√ºhlmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolai</forename><surname>Meinshausen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society. Series B (Statistical Methodology)</title>
		<imprint>
			<biblScope unit="page" from="947" to="1012" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Federated learning with partial model personalization</title>
		<author>
			<persName><forename type="first">Krishna</forename><surname>Pillutla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kshitiz</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdel-Rahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Rabbat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maziar</forename><surname>Sanjabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Xiao</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="17716" to="17758" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Long-term neural and physiological phenotyping of a single human</title>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">O</forename><surname>Russell A Poldrack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oluwasanmi</forename><surname>Laumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brenda</forename><surname>Koyejo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashleigh</forename><surname>Gregory</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mei-Yen</forename><surname>Hover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krzysztof</forename><forename type="middle">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Gorgolewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sung</forename><forename type="middle">Jun</forename><surname>Luci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryan</forename><forename type="middle">L</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName><surname>Boyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Communications</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="15" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<title level="m" type="main">Robust federated learning: The case of affine distribution shifts</title>
		<author>
			<persName><forename type="first">Amirhossein</forename><surname>Reisizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Farzan</forename><surname>Farnia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramtin</forename><surname>Pedarsani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Jadbabaie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.08907</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Distributed federated learning for ultra-reliable low-latency vehicular communications</title>
		<author>
			<persName><forename type="first">Mehdi</forename><surname>Sumudu Samarakoon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Walid</forename><surname>Bennis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M√©rouane</forename><surname>Saad</surname></persName>
		</author>
		<author>
			<persName><surname>Debbah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Communications</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1146" to="1159" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<monogr>
		<author>
			<persName><forename type="first">Nino</forename><surname>Scherrer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olexa</forename><surname>Bilaniuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yashas</forename><surname>Annadani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Schwab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Sch√∂lkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Michael C Mozer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><forename type="middle">Rosemary</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><surname>Ke</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2109.02429</idno>
		<title level="m">Learning neural causal models with active interventions</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Estimating the dimension of a model</title>
		<author>
			<persName><forename type="first">Gideon</forename><surname>Schwarz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="page" from="461" to="464" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Federated learning in medicine: facilitating multi-institutional collaborations without sharing patient data</title>
		<author>
			<persName><forename type="first">Micah</forename><forename type="middle">J</forename><surname>Sheller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brandon</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Anthony Reina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sarthak</forename><surname>Pati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aikaterini</forename><surname>Kotrotsou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Milchenko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weilin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rivka</forename><forename type="middle">R</forename><surname>Colen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific Reports</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">A linear non-gaussian acyclic model for causal discovery</title>
		<author>
			<persName><forename type="first">Shohei</forename><surname>Shimizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrik</forename><forename type="middle">O</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aapo</forename><surname>Hyv√§rinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antti</forename><surname>Kerminen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">10</biblScope>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<monogr>
		<title level="m" type="main">Causation, Prediction, and Search</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clark</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Scheines</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>The MIT Press</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Causal discovery from changes</title>
		<author>
			<persName><forename type="first">Jin</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Uncertainty in Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="512" to="521" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Learning equivalence classes of acyclic models with latent and selection variables from multiple datasets with overlapping variables</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>Tillman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Spirtes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Constraint-based causal discovery from multiple interventions over overlapping variable sets</title>
		<author>
			<persName><forename type="first">Sofia</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><surname>Tsamardinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2147" to="2205" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">The eu general data protection regulation (gdpr)</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Voigt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Axel</forename><surname>Von Dem Bussche</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">3152676. 2017</date>
			<publisher>Springer International Publishing</publisher>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="10" to="5555" />
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
	<note>A Practical Guide, 1st Ed</note>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Federated learning with matched averaging</title>
		<author>
			<persName><forename type="first">Hongyi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Yurochkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuekai</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Papailiopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasaman</forename><surname>Khazaeni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Orderingbased causal discovery with reinforcement learning</title>
		<author>
			<persName><forename type="first">Xiaoqiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yali</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liangjun</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianye</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Causal inference for recommender systems</title>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawen</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Charlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Conference on Recommender Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="426" to="431" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Dags with no fears: A closer look at continuous optimization for learning bayesian networks</title>
		<author>
			<persName><forename type="first">Dennis</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tian</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Federated learning with differential privacy: Algorithms and performance analysis</title>
		<author>
			<persName><forename type="first">Kang</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Howard</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Farhad</forename><surname>Farokhi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shi</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><forename type="middle">S</forename><surname>Tony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Quek</surname></persName>
		</author>
		<author>
			<persName><surname>Vincent Poor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Forensics and Security</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="3454" to="3469" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Federated graph classification over non-iid graphs</title>
		<author>
			<persName><forename type="first">Han</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="18839" to="18852" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Causal discovery based on healthcare information</title>
		<author>
			<persName><forename type="first">Jing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gil</forename><surname>Alterovitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lian</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aiguo</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Bioinformatics and Biomedicine</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="71" to="73" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Federated recommendation systems</title>
		<author>
			<persName><forename type="first">Liu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Vincent W Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>Federated Learning</publisher>
			<biblScope unit="page" from="225" to="239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Federated machine learning: Concept and applications</title>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianjian</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongxin</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Intelligent Systems and Technology</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1" to="19" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">Gradient diversity: a key ingredient for scalable distributed learning</title>
		<author>
			<persName><forename type="first">Dong</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashwin</forename><surname>Pananjady</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Papailiopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kannan</forename><surname>Ramchandran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Bartlett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1998" to="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<analytic>
		<title level="a" type="main">DAG-GNN: DAG structure learning with graph neural networks</title>
		<author>
			<persName><forename type="first">Yue</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tian</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Dags with no curl: An efficient DAG structure learning approach</title>
		<author>
			<persName><forename type="first">Yue</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tian</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naiyu</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">On the identifiability of additive index models</title>
		<author>
			<persName><forename type="first">Ming</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistica Sinica</title>
		<imprint>
			<biblScope unit="page" from="1901" to="1911" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<monogr>
		<author>
			<persName><forename type="first">Keli</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcus</forename><surname>Kalander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ignavier</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjian</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lujia</forename><surname>Pan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2111.15155</idno>
		<title level="m">gcastle: A python toolbox for causal discovery</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">On the identifiability of the post-nonlinear causal model</title>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aapo</forename><surname>Hyvarinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Uncertainty in Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Domain adaptation as a problem of inference on graphical models</title>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petar</forename><surname>Stojanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Biwei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingsong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clark</forename><surname>Glymour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<monogr>
		<title level="m" type="main">Learning DAGs with Continuous Optimization</title>
		<author>
			<persName><forename type="first">Xun</forename><surname>Zheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">DAGs with NO TEARS: Continuous Optimization for Structure Learning</title>
		<author>
			<persName><forename type="first">Xun</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryon</forename><surname>Aragam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Pradeep K Ravikumar</surname></persName>
		</author>
		<author>
			<persName><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="page">31</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">Learning sparse nonparametric DAGs</title>
		<author>
			<persName><forename type="first">Xun</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Dan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryon</forename><surname>Aragam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pradeep</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Causal discovery with reinforcement learning</title>
		<author>
			<persName><forename type="first">Shengyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ignavier</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitang</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
