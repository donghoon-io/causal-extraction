<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Causal Fairness-Guided Dataset Reweighting using Neural Networks</title>
				<funder ref="#_WhvGDFB">
					<orgName type="full">European Union</orgName>
				</funder>
				<funder ref="#_JuwrtEG">
					<orgName type="full">Marie Sklodowska-Curie Actions</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2023-11-17">17 Nov 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xuan</forename><surname>Zhao</surname></persName>
							<email>xuan.zhao@schufa.de</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">SCHUFA Holding AG</orgName>
								<orgName type="institution" key="instit2">University of Pisa</orgName>
								<orgName type="institution" key="instit3">Technical University of Munich</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Klaus</forename><surname>Broelemann</surname></persName>
							<email>klaus.broelemann@schufa.de</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">SCHUFA Holding AG</orgName>
								<orgName type="institution" key="instit2">University of Pisa</orgName>
								<orgName type="institution" key="instit3">Technical University of Munich</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Salvatore</forename><surname>Ruggieri</surname></persName>
							<email>salvatore.ruggieri@unipi.it</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">SCHUFA Holding AG</orgName>
								<orgName type="institution" key="instit2">University of Pisa</orgName>
								<orgName type="institution" key="instit3">Technical University of Munich</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Gjergji</forename><surname>Kasneci</surname></persName>
							<email>gjergji.kasneci@tum.de</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">SCHUFA Holding AG</orgName>
								<orgName type="institution" key="instit2">University of Pisa</orgName>
								<orgName type="institution" key="instit3">Technical University of Munich</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Causal Fairness-Guided Dataset Reweighting using Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-11-17">17 Nov 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2311.10512v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-21T20:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Index Terms-causal fairness</term>
					<term>data pre-processing</term>
					<term>adversarial reweighting</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The importance of achieving fairness in machine learning models cannot be overstated. Recent research has pointed out that fairness should be examined from a causal perspective, and several fairness notions based on the on Pearl's causal framework have been proposed. In this paper, we construct a reweighting scheme of datasets to address causal fairness. Our approach aims at mitigating bias by considering the causal relationships among variables and incorporating them into the reweighting process. The proposed method adopts two neural networks, whose structures are intentionally used to reflect the structures of a causal graph and of an interventional graph. The two neural networks can approximate the causal model of the data, and the causal model of interventions. Furthermore, reweighting guided by a discriminator is applied to achieve various fairness notions. Experiments on real-world datasets show that our method can achieve causal fairness on the data while remaining close to the original data for downstream tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Pre-processing data to satisfy fairness requirements is an important research question in machine learning. Models trained on biased data may learn such biases and generalize them, thus leading to discriminatory decisions against socially sensitive groups defined on the grounds of gender, race and age, or other protected grounds <ref type="bibr" target="#b0">[1]</ref>- <ref type="bibr" target="#b4">[5]</ref>. Many methods have been proposed to modify the training data in order to mitigate biases and to achieve specific fairness requirements <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b5">[6]</ref>- <ref type="bibr" target="#b10">[11]</ref>.</p><p>For reliable and effective treatment, particularly in a legal context, discrimination claims usually require demonstrating causal relationships between sensitive attributes and questionable decisions (or predictions), instead of mere associations or correlations. Compared with the fairness notions based on correlation, causality-based fairness notions and methods include additional knowledge of the causal structure of the problem. This knowledge often reveals the mechanism of data generation, which helps comprehend and interpret the influence of sensitive attributes on the output of a decision process. Causal fairness seeks to address the root causes of disparities rather than simply trying to eliminate them in a post-hoc manner.</p><p>We draw upon the ideas and concepts presented in CF-GAN <ref type="bibr" target="#b11">[12]</ref> as the framework for our research. Instead of fair dataset generation in CFGAN, however, we propose a method which reweighs the samples to achieve fairness criteria with the help of two neural networks to reflect the causal and interventional graphs, and a discriminator to guide the reweighting. As the general requirement of modifying datasets is to preserve the data utility as much as possible for the downstream tasks. The intuition of the reweighting scheme is that in a given dataset, there are individuals who are treated 'fairer' in the causal mechanism and by assigning higher weights to these individuals, we could slightly alter the underlying causal mechanism to achieve fairness and do not influence much on the performance of downstream tasks. In this case, hopefully we could mitigate the historical bias. In addition, by analyzing the high/low weights assigned to samples, a reweighting method like ours enables for a highlevel understanding the biases.</p><p>The experiments (Section IV) show that reweighed data outperform generated data in utility. In the taxonomy of preprocessing, in-processing and post-processing methods for bias mitigation <ref type="bibr" target="#b2">[3]</ref>, <ref type="bibr" target="#b12">[13]</ref>- <ref type="bibr" target="#b15">[16]</ref>, our method falls into the category of pre-processing, as we deal with the dataset before it is given in input to the downstream learning algorithm. Thus, our approach is model-agnostic, as any pre-processing method.</p><p>We summarize our contribution as follows: <ref type="bibr" target="#b0">(1)</ref> We formulate a novel and sample-based reweighting method for mitigating different causal bias related to sensitive groups. <ref type="bibr" target="#b1">(2)</ref> We show that by simulating the underlying causal model that reflects the causal relations of the real data, and the causal model after the intervention, with the help of a discriminator, our reweighting approach leads to fair reweighted data. <ref type="bibr" target="#b2">(3)</ref> We provide a thorough evaluation of the proposed technique on benchmark datasets and show the viability of our approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. PRELIMINARY</head><p>Throughout this paper, we consider a structural causal model</p><formula xml:id="formula_0">M = ⟨U, V, F ⟩, that is learned from a dataset D = {(s k , x k , y k )} m k=1 where s k ∈ S = {0, 1}, x k ∈ X ⊆ R d , y k ∈ Y = {0, 1}.</formula><p>1) U denotes exogenous variables that cannot be observed but constitute the background knowledge behind the model. P (U ) is a joint probability distribution of the variables in U .</p><p>2) V denotes endogenous variables that can be observed. In our work, we set V = {S, X, Y }. S represents the sensitive attribute, Y represents the outcome attribute, and X represents all other attributes. Additionally, s + is used to denote S = 1 and s -to denote S = 0.</p><p>3) F denotes the deterministic functions. For each V i ∈ V , there is a corresponding function f Vi that maps from domains of the variables in P a Vi ∪ U Vi to V i , namely V i = f Vi (P a Vi , U Vi ). Here, P a Vi ⊆ V \V i represents the parents of V i , and U Vi also represents the parents (exogenous variables) of V i , U Vi ⊆ U .</p><p>We denote by G the causal graph G associate with M, and assume it is a Directed Acyclic Graph (DAG).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Causal Fairness Criteria</head><p>To understand causal effects in the causal model M, we can use the do-operator <ref type="bibr" target="#b16">[17]</ref>, which represents a physical intervention that sets a variable S ∈ V to a constant value s. By performing an intervention do(S = s), we replace the original function S = f S (P a S , U S ) with S = s. This results in a change in the distribution of all variables that are descendants of S in the causal graph. M s is the interventional causal model and its corresponding graph G s the interventional graph. In G s , edges to S are deleted according to the definition of intervention and S is replaced with constant s. The interventional distribution for Y is denoted by P (Y |do(S = s)). Using the do-operator, we can compare the interventional distributions under different interventions to infer the causal effect of S on Y . In this paper, we focus on the following causal causal fairness notions: a) Total effect: The total effect infers the causal effect of S on Y through all possible causal paths from S to Y . The total effect of the difference of s -to s + on Y is given by T E(s + , s -) = P (Y s + ) -P (Y s -), where P (•) here refers to the interventional distribution probability. Total fairness is satisfied if |T E(s + , s -)| &lt; τ (τ is the fairness threshold). Note that statistical parity is similar to total effect but is fundamentally different. Statistical parity measures the conditional distributions of Y change of the sensitive attribute from s -to s + . b) Path-specific fairness: The path-specific effect is a fine-grained assessment of causal effects, that is, it can evaluate the causal effect transmitted along certain paths. It is used to distinguish among direct discrimination, indirect discrimination, and explainable bias. It infers the causal effect of S on Y through a subset of causal paths from S to Y , which is referred to as the π-specific effect denoting the subset of causal paths as π. The specific effect of a path set π on Y , caused by changing the value of S from s -to s + with reference to s -, is given by the difference of the interventional distributions:</p><formula xml:id="formula_1">SE π (s + , s -) = P (Y s + |π,s -|π ) -P (Y s -),</formula><p>where P (Y s + |π,s -|π ) represents the distribution resulting from intervening do(s + ) only along the paths in π while sis used as a reference through other paths π. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Causal Discovery</head><p>Methods for extracting a causal graph from given data (causal discovery) can be broadly categorized into two constraint-based and score-based methods <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b18">[19]</ref>. Constraint-based methods, such as <ref type="bibr" target="#b19">[20]</ref>- <ref type="bibr" target="#b21">[22]</ref>, utilize conditional independence tests under specific assumptions to determine the Markov equivalence class of causal graphs. Scorebased methods, like <ref type="bibr" target="#b22">[23]</ref>, evaluate candidate graphs using a pre-defined score function and search for the optimal graph within the space of DAGs. Such an approach is formulated as a combinatorial optimization problem:</p><formula xml:id="formula_2">min G Score(G; V ) = L(G; V ) + λR sparse (G), s.t. G ∈ DAG<label>(1)</label></formula><p>In the realm of causal discovery, the problem can be divided into two components, which constrain the score function Score(G; V ) and G ∈ DAG. The score function is comprised of: (1) the goodness-of-fit L(G;</p><formula xml:id="formula_3">V ) = 1 m m k=1 l(v k , F (v k ))</formula><p>is the loss of fitting observation of v k ; F denotes the deterministic functions as defined earlier in Section II (2) the sparsity R sparse (G) which regulates the number of edges in G. λ serves as a hyperparameter that controls the regularization strengths.</p><p>In this work, we assume that the given causal graph G is learned from a score-based causal discovery, so G should have goodness-of-fit and sparsity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Intervention through Controlled Neural Networks</head><p>In CausalGAN <ref type="bibr" target="#b23">[24]</ref>, a noise vector Z is partitioned into {Z V1 , Z V2 , ..., Z V |V | } to mimic the exogenous variables U in the structural causal model M described in Section II. The generator</p><formula xml:id="formula_4">G(Z) contains |V | sub-neural networks {G V1 , G V2 , ..., G V |V | } to generate the values of each node V i in the graph. The input of G Vi is the output of G P a V i combined with Z Vi .</formula><p>Here, G Vi is trying to approximate the corresponding function f Vi (P a Vi , U Vi ) in the causal model M. The adversarial game is played to ensure that the generated observational distribution is not differentiable from the real observational distribution. In the work of CFGAN, two generators are used to simulate the causal model M and the interventional model M s , while two discriminators try to maintain that: (1) the generated data is close to the orginal distribution, and (2) the causal effect is mitigated. In our work, Fig. <ref type="figure">1</ref>. The framework of reweighting: the structure of NN (neural network) F 1 reflects the original causal graph G; the structure of NN F 2 refelects the interventional causal graph Gs; the discriminator D tells if a ŷ estimated by F 2 is from the group S + or the group S -. An adverserial game is played between the reweighting on the data samples and D to reach a situation where D is not capable of differentiating whether y is from S + or S -and a specific causal fairness is reached. The weights of samples are also forwarded to F 1 to make sure that the reweighted empirical data distribution is close to the original data distribution from which the causal graph G is learned.</p><p>we also use a similar design but we do not model the noise Z since our goal is not to generate fairness-aware data, but to reweigh the given data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. A REWEIGHTING APPROACH FOR DIFFERENT CAUSAL FAIRNESS CRITIRIA</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Problem Formulation</head><p>As mentioned in Section II, the notation used in our work is based on the conventional approach. We are given a causal graph G and a dataset D with m i.i.d. samples drawn from P (V ). We assume that G is sufficient to describe the causal relationships between the variables V . In this paper, we build our method on a causal graph of observational data, so we do not specifically model U . The problem we are facing is that from the given causal graph G, S has a causal effect on Y . Our method aims to achieve two objectives: (1) preserve the goodness-of-fit (mentioned in Section II-B) by maintaining the empirical reweighted data distribution close to the original data distribution for utility of the downstream tasks; and (2) ensure that S cannot be used to discriminate when predicting Y based on various causal criteria in the interventional model M s . We treat S and Y as binary variables in this paper. However, this can be easily extended to multi-categorical or numerical cases. Also, we focus on the causal effect of S on Y , but the model can deal with causal effects among multiple variables. We try to reach the following causal fairness notions mentioned in Section II-A, including total fairness <ref type="bibr" target="#b24">[25]</ref>, pathspecific fairness (elimination of indirect discrimination) <ref type="bibr" target="#b3">[4]</ref>, and counterfactual fairness <ref type="bibr" target="#b25">[26]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Reweighting For Causal Fairness</head><p>We propose a reweighting scheme which consists of neural networks (F 1 , F 2 ) and one discriminator (D). Fig. <ref type="figure">1</ref> shows the framework of our method.  As shown in Section III-A, causal fairness notions measures the difference between the interventional distributions. To guarantee these notions, our method adopts two neural networks to approximate the causal relations. One neural network F 1 simulates the causal model M, while the other neural network F 2 approximates the interventional model M s according to which kind of causal effect is measured. F 1 aims to force the reweighted data close to the given causal graph, and F 2 aims to drive the interventional distributions to satisfy the specific notion defined in Section III-A. To represent the connections between the two causal models, the two neural networks share certain structures and parameters, while they differ in sub-neural networks to indicate the intervention (the edges to S in the interventional graph is deleted). Then, our method adopts a discriminator D trying to distinguish the two interventional distributions (reweighted) P (Y s + ) and P (Y s -). Finally, the discriminator and reweighting play an adversarial game to produce weights for individuals in the dataset.</p><p>To better illustrate our design, we divide X into {A, B} and V = {S, A, B, Y } based on the positions of the nodes in the causal graph -variables in A are direct causes of S and variables in B are descendants of S and A.</p><p>1) Reweighting for Total Fairness: The causal graph G is shown in Fig. <ref type="figure" target="#fig_1">2(a)</ref>. We also show the interventional graph G s with the intervention do(S = s) and the edge from A to S is deleted in G s , which is also altered in F 2 . The pair of nodes connected by dashed lines indicate that they share the same function (structures and parameters of the corresponding sub-neural networks) as shown in Fig. <ref type="figure" target="#fig_1">2(b)</ref>. For parallel nodes in the two graphs, the corresponding sub-neural networks are synchronized during the training process.</p><p>We first show our method to achieve total fairness by describing each components of our design. As mentioned in Section II-A , |T E(s + , s -)| &lt; τ must hold for all possible paths from S to Y shown in Fig. <ref type="figure" target="#fig_1">2(a)</ref>. a) Neural Networks F 1 and F 2 : The feed-forward Neural Network F 1 is constructed to correspond with the causal graph G. It consists of |V | -r sub-neural networks (r is the total number of the root nodes in G), with each corresponding to a node in V (expect for the root nodes). Similar to what is described as the design of CFGAN in Section II-C, each subneural network F 1</p><p>Vi is trying to approximate the corresponding function f Vi (P a Vi ) in the causal model M of the given causal graph G. When F 1 is properly trained, the causal model M is learned. Then, F 1  Vi outputs the estimated values of V i , i.e., vi . The other neural network F 2 is constructed to align with the interventional graph G s , where all the incoming edges to S are removed under the intervention do(S = s). The layout of F 2 is analogous to F 1 , but with the exception that the network F 2 S is designated as F 2 S ≡ 1 if s = s + , and F 2 S ≡ 0 if s = s -. To synchronize the two neural networks F 1 and F 2 , they share the identical set of structures and parameters for every corresponding pair of sub-neural networks, i.e., F 1  Vi and F 2</p><p>Vi for each V i except for S. When F 2 is properly trained, the interventional model M s is learned. With M and M s learned, we could manipulate the interventional distributions to reach our goal of causal fairness.</p><p>b) Discriminator: D is used to differentiate between the two interventional distributions ŷs + ∼ P F 2 (Y s + ) and ŷs -∼ P F 2 (Y s -). The aim of the discriminator D to minimize the bias by penalizing differences between both groups. c) Weights: Assuming the to-reach-causal-fairnessimportance of each individual in the given dataset is known, we can assign importance to different individuals in M s to improve causal fairness for any downstream task. w = (w 1 , ..., w m ) is a sample reweighting vector with length m, where w k indicates the importance of the k-th observed sample (s k , x k , y k ). We want to reach a balance of goodness-of-fit to the known causal graph G which is learned from D and reweighting for causal fairness.</p><p>Recall that here we assume that the known causal graph G is learned from a causal discovery which means it achieves goodness-of-fit. We do not want the reweighted data to drift too far from the original causal graph. We use hatted variables to represent the output of the neural networks of the graphs. To reach this objective, we have:</p><formula xml:id="formula_5">S F 1 (G) = min F 1 m i=1 w i l((s i , x i , y i ), (s i , xi , ŷi ))<label>(2)</label></formula><p>where l((s i , x i , y i ), (s i , xi , ŷi )) represents the loss of fitting observation (x i , y i , s i ). In the experiment, we use weighted MSE loss for the continuous variables and weighted cross entropy loss for the categorical variables. The problem then becomes how to learn appropriate the sample reweighting vector w for the objective of causal fairness. We formulate our objective as a minmax problem to reweight with M s :</p><formula xml:id="formula_6">min w max D m k=1 w k (D(ŷ s + k ) -D(ŷ s - k )),<label>(3)</label></formula><p>To avoid information loss by assigning close to zero weights to some samples from the group of S + , we introduce a regularization constraint to the minimization term:</p><formula xml:id="formula_7">m k=1 (w k -1) 2 ⩽ T m<label>(4)</label></formula><p>Thus, by adjusting the value of T , we can balance between similarity and dissimilarity of the weights of samples.</p><p>Samples easily fitted with fairness constraint should contribute more to G s : these are the samples with less difference of discriminator outputs from do(S = s -) to do(S = s + ). We therefore use downweighting on the not-hold-fairness samples, and upweighting on the hold-fairness samples. This could be achieved by assigning weights to samples based on the discriminator D outputs. When the neural networks are properly trained, the discriminator should not be able to tell if the sample is from the group of S + or S -which could achieve total fairness as we describe in Section III-A.</p><p>2) Reweighting for Path-Specific Fairness: The notions of direct and indirect discrimination are connected to effects specific to certain paths. We concentrate on indirect discrimination, even though fulfilling the criterion for direct discrimination is comparable. As mentioned in Section II-A , |SE π (s + , s -)| = |P (Y s + |π C ,s -|π C ) -P (Y s -)| &lt; τ must hold for a path set π C that includes paths passing through certain attributes, shown in Fig. <ref type="figure" target="#fig_8">7</ref>(a) ( in Appendix). F 1 for indirect discrimination is similar to that in Section III-B1. However, the design of F 2 is altered because it needs to adapt to the situation where the intervention is transferred only through π C , shown in Fig. <ref type="figure" target="#fig_8">7</ref>(b) (in Appendix). We examine two possible states for the sub-neural network F 2 S : the reference state and the interventional state. Under the reference state, F 2 S is constantly set to 0. On the other hand, under the interventional state, F 2 S is set to 1 if s = s + , and 0 if s = s -. For other sub-neural networks, there are also two possible values: the reference state and the interventional state, according to the state of F 2 S . If a sub-neural network corresponds to a node that is not present on any path in π C , it only accepts reference states as input and generates reference states as output. However, for any other sub-neural network F 2 Vj that exists on at least one path in π C , it may accept both reference and interventional states as input and generate both types of states as output.</p><p>3) Reweighting for Counterfactual Fairness: In the context of counterfactual fairness, interventions are made based on a subset of variables O = o. Both F 1 and F 2 have similar structures to those in Section III-B2. However, we only use samples in F 2 as interventional samples if they satisfy the condition O = o. This means that the interventional distribution from F 2 is conditioned on O = o as P F 2 (X s , Y s |o). The discriminator D is designed to distinguish between ŷs + |o ∼ P F 2 (Y s + |o) and ŷs -|o ∼ P F 2 (Y s -|o), and aims to reach</p><formula xml:id="formula_8">P F 2 (Y s + |o) = P F 2 (Y s -|o).</formula><p>During training, the value of m should be adjusted based on the number of samples that are involved in the intervention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Training Algorithm</head><p>To train the network F 1 to minimize the loss in Equation <ref type="formula" target="#formula_5">2</ref>, we alternately optimize the network parameters of F 1 and D and learn the weights w by fixing others as known.</p><p>a) Updating parameters of F 1 with fixed w: Fixing w, we update F 1 to minimize the loss in Equation 2 for M steps, using the mini-batch stochastic gradient descent algorithm.</p><p>b) Updating w with fixed F 2 (synchronized with F 1 ): Fixing parameters of F 2 , we control the training data into two groups (S + and S -) for intervention, and learn w in Equation 3. Since Equation 3 is a min-max optimization problem, we can alternately optimize the weights w and the parameters of D of the discriminator by fixing the other one as known. Therefore, we first fix w i = 1 for all i and optimize D to maximize the objective function in Equation 3 using the gradient penalty technique, as in WGAN with Gradient Penalty <ref type="bibr" target="#b26">[27]</ref>. Note that when w i = 1 for all i, Equation 2 is equivalent to the situation when there is no reweighting applied. Then, fixing the discriminator D, we optimize w.</p><p>We denote </p><formula xml:id="formula_9">d k = D(ŷ S + k ) -D(ŷ S - k ) and d = (</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. EXPERIMENTAL EVALUATION</head><p>We conduct experiments on two benchmarks datasets (ADULT <ref type="bibr" target="#b27">[28]</ref> and COMPAS <ref type="bibr" target="#b28">[29]</ref>) to evaluate our reweighting approach and compare it with state-of-the-art methods: FairGAN <ref type="bibr" target="#b29">[30]</ref>, CFGAN <ref type="bibr" target="#b11">[12]</ref> and Causal Inference for Social Discrimination Reasoning (CISD) <ref type="bibr" target="#b30">[31]</ref> for total effect and indirect discrimination (please refer to Appendix (A) for more details about the datasets). CISD <ref type="bibr" target="#b30">[31]</ref> introduces a technique for identifying causal discrimination through the use of propensity score analysis. It consists of mitigating the influence of confounding variables by reweighing samples based on the propensity scores calculated from a logistic regression. The approach, however, is purely statistical with no causal knowledge exploited. We also compare our method with CFGAN and two methods from <ref type="bibr" target="#b25">[26]</ref> (we refer them as CE 1 and CE 3 in our paper) for counterfactual effect. CE 1 only uses on non-descendants of S for classification. CE 3 is similar to CE 1 but presupposes an additive U . The reason we choose these methods is: FairGAN for statistical parity and CFGAN for causal fairness also use adversarial method to mitigate bias, similar to our design; CISD approaches causal fairness with weighting scheme. We then compare the performance of our method with the mentioned methods on total effect, indirect discrimination and counterfactual fairness with 4 different downstream classifiers: decision tree (DT) <ref type="bibr" target="#b31">[32]</ref>, logistic regression (LR) <ref type="bibr" target="#b32">[33]</ref>, support vector machine (SVM) <ref type="bibr" target="#b33">[34]</ref> and random forest (RF) <ref type="bibr" target="#b34">[35]</ref>. We compare the accuracy of the downstream tasks to see if the data preserves good utility, where higher accuracy indicates better utility. For the utility of the downstream task, we also compute the Wasserstein distance between the manipulated data and the original data, where a smaller Wasserstein distance indicates closer the two distributions, and better utility for the downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. The datasets and setup</head><p>Due to page limit, please refer to Appendix for the details of datasets and training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Analysis 1) Total Effect:</head><p>In Table <ref type="table" target="#tab_1">I</ref>, we present the total effect (TE) calculated for the original dataset and the datasets processed using various methods. The original ADULT dataset has a total effect of 0.1854 and COMPAS 0.2389, while applying FairGAN to achieve demographic parity yields almost no total effect. As mentioned in Section II-A, total effect is very similar to demographic parity. However, FairGAN is limited by its focus on statistical fairness, rather than causal fairness, and does not perform well on Wasserstein distance or downstream tasks. It is quite intuitive that if total fairness is met, total fairness should be achieved too on the condition that the causal graph is sufficient. We test it on our two datasets and the result is acceptable. CFGAN produces no total effect, but it performs worse than our method on Wasserstein distance, possibly because reweighted data could manage to stay closer to the original data distribution. Our method also outperforms CISD, which may be due to the use of a neural network instead of logistic regression to calculate weights, allowing for greater flexibility in capturing the dataset.</p><p>A Closer Look at the Weights After ranking the weights of samples in the Adult dataset, we observed that older individuals from Europe or Asia (e.g., Germany and India) tend to have the highest weights, while younger black individuals from Caribbean countries (e.g., Jamaica and Haiti) tend to have lower weights. This suggests that when sex is intervened from female to male, the former group is less influenced by the change, while the latter group is more influenced in terms of income. White, middle-aged individuals born in the US are assigned medium weights. To visualize it, we build a decision tree to classify top 10% individuals with highest weights and bottom 10% individuals with lowest weights using the three root nodes {race, native country, age}, shown in Fig. <ref type="figure">3</ref>.</p><p>2) Indirect Discrimination: To address indirect discrimination (SE), we identify all possible paths except the direct one {S Y } as the path π C and evaluated the results in Table <ref type="table" target="#tab_1">I</ref>. Similar to total effect, FairGAN removes indirect discrimination but at the cost of significant utility loss. In contrast, both CISD and our method can effectively remove indirect discrimination while maintaining better data utility than FairGAN. Although CFGAN and CISD perform similarly using different techniques, our method outperforms both  Fig. <ref type="figure">3</ref>. The visualize of the decision tree trying to classify individuals with low or high weights. we see that age and race are the most important attributes to build the tree. The mapping of label encoder for race is { ′ Amer-Indian-Eskimo ′ : 0, ′ Asian -P ac -Islander ′ : 1, ′ Black ′ : 2, ′ Other ′ : 3, ′ W hite ′ : 4} methods in terms of Wasserstein distance, indicating the best overall utility among these approaches.</p><p>3) Counterfactual Fairness: To evaluate counterfactual effect (CE), we consider the conditions on two variablesrace and native country (binarized) for ADULT, and sex and age (binarized) for COMPAS -resulting in four value combinations. Table <ref type="table" target="#tab_1">II</ref> presents the results for two selections (see Appendix (B6) for more details). We find biases in the original data regarding counterfactual fairness in these two selections. CE 1 is counterfactually fair, but the classifier accuracy is poor because it solely employs non-descendants of the sensitive attributes for outcome attributes. CE 3 cannot achieve counterfactual fairness, probably due to the strong assumptions while introducing U . In contrast, our method performs well on both dimensions due to its flexibility. Although CFGAN performs well in some aspects, our method outperforms it in Wasserstein distance, likely because reweighting better preserves the original distribution than generation methods.</p><p>Summary We find out that in general neural nets-based methods outperform due to the flexibility of neural networks to capture any function, while reweighting outperforms generation. We could see from the experiment results above, imposing strong assumptions on the U and F could cause unwanted problems, and we argue that is why neural nets should be explored more in causal fairness problem settings. Fairness related methods usually formalize the problem as an optimization trade-off between utility and specific fairness objectives. Nevertheless, these discussions are often based on a fixed distribution that does not align with our current situation. We think that an ideal distribution might exist where fairness and utility are in harmony. To include the reweighting scheme into the downstream tasks could be an very interesting future direction to locate this harmonious distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. CONCLUSION, LIMITATION AND FUTURE WORK</head><p>We propose a novel approach for achieving causal fairness by dataset reweighting. Our method considers different causal fairness objectives, such as total fairness, path-specific fairness and counterfactual fairness. It consists of two feed-forward neural networks F 1 and F 2 and a discriminator D. The structures of F 1 and F 2 are designed based on the original causal graph G and interventional graph G s , and the discriminator D is used to ensure causal fairness combined with a reweighting scheme. Our experiments on two datasets show  an individual earns more or less than $50,000 per year. The dataset is imbalanced -the instances made less than $50,000 constitute 25% of the dataset, and the instances made more than $50,000 constitute 75% of the dataset. As for gender, it is also imbalanced. We use age, years of education, capital gain, capital loss, hours-per-week, etc., as continuous features, and education level, gender, etc., as categorical features. We set the batch size at 640 and train 30 epochs for convergence. We set the learning rate η at 0.001 according to the experiment result.</p><p>2) COMPAS Dataset: COMPAS (Correctional Offender Management Profiling for Alternative Sanctions) is a popular commercial algorithm used by judges and parole officers for scoring criminal defendant's likelihood of reoffending (recidivism). The COMPAS dataset includes the processed COMPAS data between 2013-2014. The data cleaning process followed the guidance in the original COMPAS repo. It Contains 6172 observations and 14 features. In our causal graph, we use 7 features. Due to the limited size of COMPAS dataset, it does not perform so well on NN based tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Training Details</head><p>For ADULT and COMPAS datasets, some pre-processing is performed. We normalize the continuous features and use one-hot encoding to deal with the categorical features for the  input of F 1 and F 2 . We use sex and race as the sensitive variable S in ADULT and COMPAS respectively, income and two year recidivism as the outcome variable Y .</p><p>For F 1 and F 2 , we apply fully connected layers. For the discriminator D, we use the same architecture proposed in <ref type="bibr" target="#b26">[27]</ref>. We apply SGD algorithm with a momentum of 0.9 to update F 1 and F 2 . D is updated by the Adam algorithm with a learning rate 0.0001. Following <ref type="bibr" target="#b26">[27]</ref>, we adjust the learning rate η by η = 0.01 (1+10p) -0.75 , where p is the training progress linearly changing from 0 to 1. We update F 1 and F 2 for 2 steps then update D for 1 step. For more details of the experiment (e.g., the split of training and testing datasets, the details of architectures of the neural nets, the estimation of Wasserstein distance), please refer to the Appendix (B). We then evaluate the performance of our method of reweighting to achieve different types of causal fairness and utility.</p><p>Our test are run on an Intel(r) Core(TM) i7-8700 CPU. The networks in the experiments are built based on Pytorch <ref type="bibr" target="#b38">[39]</ref>, the optimization in Equation ( <ref type="formula">5</ref>) is performed with the Python package CVXPY <ref type="bibr" target="#b39">[40]</ref>.</p><p>1) Details of architectures of the feed-forward Neural Networks F 1 and F 2 with sub-neural networks: To simplify our demonstration, we consider a causal graph G with 6 attributes {S, A 1 , A 2 , B 1 , B 2 , Y } as shown in Fig. <ref type="figure" target="#fig_6">6</ref>(a). And Fig. <ref type="figure" target="#fig_6">6(b)</ref> shows the joint neural network of it.</p><p>2) Details of WGAN-GP adaptation for our method: In our design, we adopt the discriminator from WGAN-GP: in   the original work, the discriminator is used to differentiate between the generated and real data while we are trying to differentiate between S + and S -. The difference between orginal GAN and WGAN-GP is that WGAN-GP introduces a gradient penalty term in the training objective to guarantee Wasserstein distance. Wasserstein distance itself has been used a lot in fairness realted topic to help detect or mitigate bias. Note that we choose relatively larger batch size since to approximate Wasserstein distance between two distributions requires relatively larger batch size.</p><p>3) Sensitivity to the Choice of Hyper-Parameters: We conduct an analysis of the sensitivity of our method to the hyper-parameters discussed in Section III, and the results are shown in Fig. <ref type="figure" target="#fig_9">8</ref>. The figures demonstrate that our adversarial reweighting scheme's performance has low sensitivity to hyper-parameter choice when T is above 1. Therefore, we set T at 1.5. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>(a) Causal Graphs G and Gs (b) Neural Networks F 1 and F 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig.2. The Neural Networks F 1 and F 2 on total effect. S is 1 or 0 for the interventional joint distributions P F 2 (s + ) (red path) and P F 2 (s -) (green path), respectively. The pair of nodes connected by dashed lines indicate that they share the same function (structures and parameters of the corresponding sub-neural networks).</figDesc><graphic coords="3,338.29,160.59,198.43,108.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>d 1 , d 2 , ...d m ) T . The optimization problem for w becomes a constrained least squares problem: min w d T w, s.t.w k ⩾ 0, -1) 2 ⩽ T m (5)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. The causal graph of the Adult dataset depicts the indirect path set with blue paths, while the direct path is represented by the green path.</figDesc><graphic coords="8,61.11,50.54,226.77,108.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. The causal graph of the COMPAS dataset depicts the indirect path set with blue paths, while the direct path is represented by the green path</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>(a) Causal graph G (b) Neural Network F 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Fig.6. details of the connection of the neural nets of a given G. In Fig.6(b), each nodes are either input or output of a sub-neural nets or both. Note that we do not show the inner layers here for simplicity.</figDesc><graphic coords="8,48.96,201.35,251.90,168.49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>(a) Causal graph G and interventional graph Gs with the indirect interventional path π C (b) Neural Networks F 1 and F 2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. the Neural Networks F 1 and F 2 based on indrect discrimination. S is 1 or 0 and the intervention is only along π C = {S B Y } for the interventional distributions P F 2 (s + ) (red) and P F 2 (s -) (green) respectively. Compared with Fig. 2, we could see that the intervention is not transferred directly from S to Y ({S Y }) in Fig. 7.</figDesc><graphic coords="9,97.38,361.22,154.22,112.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Sensitivity of total effect on the change of T on ADULT dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Fig. 9 . 6 )</head><label>96</label><figDesc>Fig. 9. Sensitivity of total effect on the change of T on COMPAS dataset.</figDesc><graphic coords="9,360.40,50.54,154.22,109.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>If π contains all direct edge from S to Y , SE π (s + , s -) measures the direct discrimination. If π contains all indirect paths from S to Y that pass through proxy attributes, SE π (s + , s -) evaluates the indirect discrimination. Path-specific fairness is met if |SE π (s + , s -)| &lt; τ . c) Counterfactual fairness: The counterfactual effect of changing S from s -to s + on Y under certain conditions O = o (where O is a subset of observed attributes O ⊆ X) for an individual with features o is given by the difference between the interventional distributions P (Y s + |o) and P (Y s -|o): CE(s + , s -|o) = P (Y s + |o) -P (Y s -|o). Counterfactual fairness is met if |CE(s + , s -|o)| &lt; τ . Any context O = o represents a certain sub-group of the population, specifically, when O = X, it represents specific individual(s).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE I THE</head><label>I</label><figDesc>TOTAL EFFECT (TE) AND INDIRECT DISCRIMINATION (SE) ON ADULT AND COMPAS DATASETS</figDesc><table><row><cell></cell><cell></cell><cell>ADULT</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>COMPAS</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="3">total effect indirect discrimination Wasserstein distance</cell><cell>SVM</cell><cell cols="2">classifier accuracy (%) DT LR</cell><cell>RF</cell><cell></cell><cell cols="3">total effect indirect discrimination Wasserstein distance</cell><cell>SVM</cell><cell cols="2">classifier accuracy (%) DT LR</cell><cell>RF</cell></row><row><cell>original data</cell><cell>0.1854 (0.0301)</cell><cell>0.1773 (0.0489)</cell><cell>0</cell><cell>81.78 (1.45)</cell><cell>81.77 (1.75)</cell><cell>81.70 (1.63)</cell><cell>81.78 (1.76)</cell><cell>orginal data</cell><cell>0.2389 (0.0245)</cell><cell>0.2137 (0.0985)</cell><cell>0</cell><cell>65.24 (2.34)</cell><cell>65.15 (1.46)</cell><cell>65.10 (2.19)</cell><cell>65.27 (1.09)</cell></row><row><cell>Ours (TE) Ours (SE)</cell><cell>0.0017 (0.0009)</cell><cell>0.0012 (0.0007)</cell><cell>0.71 (0.19) 0.69 (0.23)</cell><cell>81.12 (1.72) 81.14 (1.58)</cell><cell>81.20 (1.86) 80.97 (2.01)</cell><cell>81.60 (2.03) 81.65 (1.96)</cell><cell>81.14 (1.05) 81.17 (1.92)</cell><cell>Ours (TE) Ours (SE)</cell><cell>0.0037 (0.0018)</cell><cell>0.0017 (0.0009)</cell><cell>1.21 (0.32) 0.72 (0.35)</cell><cell>65.09 (2.75) 65.11 (1.98)</cell><cell>65.13 (1.76) 65.14 (2.06)</cell><cell>65.06 (2.08) 65.02 (1.12)</cell><cell>65.11 (1.02) 65.09 (1.95)</cell></row><row><cell>FairGAN</cell><cell>0.0021 (0.0007)</cell><cell>0.0148 (0.0075)</cell><cell>5.21 (0.78)</cell><cell>79.88 (1.47)</cell><cell>79.81 (1.89)</cell><cell>80.36 (1.32)</cell><cell>80.82 (1.65)</cell><cell>FairGAN</cell><cell>0.0075 (0.0056)</cell><cell>0.0341 (0.0075)</cell><cell>3.24 (1.45)</cell><cell>64.24 (1.77)</cell><cell>64.15 (2.01)</cell><cell>64.50 (2.75)</cell><cell>64.26 (2.34)</cell></row><row><cell>CFGAN (TE) CFGAN (SE)</cell><cell>0.0106 (0.0008)</cell><cell>0.0034 (0.0012)</cell><cell>1.78 (0.65) 1.89 (0.29)</cell><cell>80.34 (2.56) 80.37 (1.56)</cell><cell>80.15 (1.52) 80.49 (2.05)</cell><cell>80.07 (1.65) 80.04 (1.67)</cell><cell>80.39 (1.32) 80.24 (1.09)</cell><cell>CFGAN (TE) CFGAN (SE)</cell><cell>0.0364 (0.0175)</cell><cell>0.0016 (0.0025)</cell><cell>2.76 (1.65) 2.64 (0.91)</cell><cell>64.59 (2.65) 64.21 (2.45)</cell><cell>65.13 (2.73) 64.25 (1.75)</cell><cell>65.02 (2.03) 64.80 (1.97)</cell><cell>65.01 (2.45) 64.87 (1.54)</cell></row><row><cell>CISD (TE) CISD (SE)</cell><cell>0.0206 (0.0074)</cell><cell>0.0098 (0.0045)</cell><cell>2.57 (0.18) 2.82 (0.23)</cell><cell>80.73 (1.42) 80.75 (1.28)</cell><cell>80.74 (1.75) 80.72 (1.58)</cell><cell>81.15 (1.82) 80.77 (1.96)</cell><cell>81.27 (1.47) 81.32 (1.95)</cell><cell>CISD (TE) CISD (SE)</cell><cell>0.0356 (0.0246)</cell><cell>0.0175 (0.0231)</cell><cell>2.57 (1.61) 2.65 (1.56)</cell><cell>65.04 (1.76) 64.01 (1.56)</cell><cell>65.17 (1.54) 65.02 (1.49)</cell><cell>65.04 (2.47) 64.09 (2.45)</cell><cell>65.05 (1.75) 64.11 (1.32)</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>that the approach improves over state-of-the-art approaches for the considered causal fairness notions achieving minimal loss of utility. Moreover, by analyzing the sample weights assigned by the approach, the user can gain an understanding of the distribution of the biases in the original dataset. Future work involve analyzing the sample weights further, e.g., by using methods from the eXplainable in AI research area. As another relevant research direction, since practitioners often lack sufficient causal graphs when working with a dataset <ref type="bibr" target="#b35">[36]</ref>, an extension of our work could involve causal discovery as an integral part of the approach.</p></div>
<div><head>VI. ACKNOWLEDGEMENT</head><p>This work has received funding from the <rs type="funder">European Union</rs>'s <rs type="programName">Horizon 2020 research and innovation programme</rs> under <rs type="funder">Marie Sklodowska-Curie Actions</rs> (grant agreement number <rs type="grantNumber">860630</rs>) for the project "<rs type="projectName">NoBIAS -Artificial Intelligence without Bias</rs>".</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_WhvGDFB">
					<orgName type="program" subtype="full">Horizon 2020 research and innovation programme</orgName>
				</org>
				<org type="funded-project" xml:id="_JuwrtEG">
					<idno type="grant-number">860630</idno>
					<orgName type="project" subtype="full">NoBIAS -Artificial Intelligence without Bias</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dataset and Training Details</head><p>The causal graph <ref type="bibr" target="#b36">[37]</ref> for ADULT is shown in Fig. <ref type="figure">4</ref>, and for COMPAS <ref type="bibr" target="#b37">[38]</ref> in Fig. <ref type="figure">5</ref>. Note that the causal graphs here are sourced from existing literature.</p><p>1) Adult Dataset: The Adult dataset was drawn from the 1994 United States Census Bureau data. It contains 65,123 samples with 11 variables. It used personal information such as education level and working hours per week to predict whether</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Discrimination-aware data mining</title>
		<author>
			<persName><forename type="first">D</forename><surname>Pedreschi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ruggieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Turini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">KDD</title>
		<imprint>
			<biblScope unit="page" from="560" to="568" />
			<date type="published" when="2008">2008</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Handling conditional discrimination</title>
		<author>
			<persName><forename type="first">I</forename><surname>Zliobaite</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Kamiran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Calders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="992" to="1001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Equality of opportunity in supervised learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Srebro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3315" to="3323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A causal framework for discovering and removing direct and indirect discrimination</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">in IJCAI. ijcai.org</title>
		<imprint>
			<biblScope unit="page" from="3929" to="3935" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Achieving non-discrimination in prediction</title>
	</analytic>
	<monogr>
		<title level="m">IJCAI. ijcai.org</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3097" to="3103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Certifying and removing disparate impact</title>
		<author>
			<persName><forename type="first">M</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Friedler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Moeller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Scheidegger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Venkatasubramanian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">KDD</title>
		<imprint>
			<biblScope unit="page" from="259" to="268" />
			<date type="published" when="2015">2015</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Causal modeling-based discrimination discovery and removal: Criteria, bounds, and algorithms</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Knowl. Data Eng</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2035" to="2050" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Censoring representations with an adversary</title>
		<author>
			<persName><forename type="first">H</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Storkey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR (Poster)</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Controllable invariance through adversarial feature learning</title>
		<author>
			<persName><forename type="first">Q</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">H</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName><surname>Neubig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="585" to="596" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning adversarially fair and transferable representations</title>
		<author>
			<persName><forename type="first">D</forename><surname>Madras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Creager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Pitassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML, ser. Proceedings of Machine Learning Research</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="page" from="3381" to="3390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Mitigating unwanted biases with adversarial learning</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Lemoine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AIES</title>
		<imprint>
			<biblScope unit="page" from="335" to="340" />
			<date type="published" when="2018">2018</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Achieving causal fairness through generative adversarial networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">in IJCAI. ijcai.org</title>
		<imprint>
			<biblScope unit="page" from="1452" to="1458" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Sample selection for fair and robust training</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Roh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Whang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Suh</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="815" to="827" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Optimized pre-processing for discrimination prevention</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">P</forename><surname>Calmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Vinzamuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">N</forename><surname>Ramamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Varshney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3992" to="4001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning optimal and fair decision trees for non-discriminative decision-making</title>
		<author>
			<persName><forename type="first">S</forename><surname>Aghaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Azizi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Vayanos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1418" to="1426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A convex framework for fair regression</title>
		<author>
			<persName><forename type="first">R</forename><surname>Berk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Heidari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jabbari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Joseph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Kearns</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Morgenstern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Neel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roth</surname></persName>
		</author>
		<idno>abs/1706.02409</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<title level="m">Causality: Models, Reasoning and Inference</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Causal discovery and inference: Concepts and recent methodological advances</title>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Informatics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Review of Causal Discovery Methods Based on Graphical Models</title>
		<author>
			<persName><forename type="first">C</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Genetics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Causal inference in the presence of latent variables and selection bias</title>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Meek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Richardson</surname></persName>
		</author>
		<idno>abs/1302.4983</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">An Algorithm for Fast Recovery of Sparse Causal Graphs</title>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Glymour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Social Science Computer Review</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="62" to="72" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning high-dimensional directed acyclic graphs with latent and selection variables</title>
		<author>
			<persName><forename type="first">D</forename><surname>Colombo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Maathuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kalisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="294" to="321" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">D&apos;ya like dags? A survey on structure learning and causal discovery</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Vowels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">C</forename><surname>Camgöz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bowden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">36</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Causal-GAN: Learning causal implicit generative models with adversarial training</title>
		<author>
			<persName><forename type="first">M</forename><surname>Kocaoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Snyder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Dimakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vishwanath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">ICLR (Poster). OpenReview.net</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Fairness in decision-making -the causal explanation formula</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Bareinboim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2037" to="2045" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Counterfactual fairness</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Kusner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Loftus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4066" to="4076" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Improved training of wasserstein GANs</title>
		<author>
			<persName><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5767" to="5777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Scaling up the accuracy of naive-bayes classifiers: A decision-tree hybrid</title>
		<author>
			<persName><forename type="first">R</forename><surname>Kohavi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">KDD</title>
		<imprint>
			<biblScope unit="page" from="202" to="207" />
			<date type="published" when="1996">1996</date>
			<publisher>AAAI Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">How We Analyzed the COMPAS Recidivism Algorithm</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mattu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Angwin</surname></persName>
		</author>
		<author>
			<persName><surname>Kirchner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Surya</surname></persName>
		</author>
		<author>
			<persName><surname>Larson</surname></persName>
		</author>
		<ptr target="https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm" />
	</analytic>
	<monogr>
		<title level="j">ProPublica</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Fairgan: Fairness-aware generative adversarial networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE BigData. IEEE</title>
		<imprint>
			<biblScope unit="page" from="570" to="575" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Causal inference for social discrimination reasoning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Qureshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Kamiran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Karim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ruggieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pedreschi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Intell. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="425" to="437" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Top 10 algorithms in data mining</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Quinlan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Motoda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Mclachlan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Philip</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge and information systems</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="37" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The regression analysis of binary sequences</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Cox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Methodological)</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="215" to="232" />
			<date type="published" when="1958">1958</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Support-vector networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="273" to="297" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The random subspace method for constructing decision forests</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">K</forename><surname>Ho</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="832" to="844" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Causal discovery for fairness</title>
		<author>
			<persName><forename type="first">R</forename><surname>Binkyte-Sadauskiene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Makhlouf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Pinzón</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhioua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Palamidessi</surname></persName>
		</author>
		<idno>abs/2206.06685</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">CFGAN: A generic collaborative filtering framework based on generative adversarial networks</title>
		<author>
			<persName><forename type="first">D</forename><surname>Chae</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CIKM</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="137" to="146" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">fairadapt: Causal reasoning for fair data pre-processing</title>
		<author>
			<persName><forename type="first">D</forename><surname>Plecko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Bennett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Meinshausen</surname></persName>
		</author>
		<idno>abs/2110.10200</idno>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Pytorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">CVXPY: A python-embedded modeling language for convex optimization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Diamond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">P</forename><surname>Boyd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="83" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
