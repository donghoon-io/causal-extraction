<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Robust Emotion Recognition in Context Debiasing</title>
				<funder ref="#_EwY75EJ">
					<orgName type="full">National Key R&amp;D Program of China</orgName>
				</funder>
				<funder ref="#_DHEueeX">
					<orgName type="full">Shanghai Municipal Science and Technology Committee of Shanghai Outstanding Academic Leaders Plan</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-06-02">2 Jun 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Dingkang</forename><surname>Yang</surname></persName>
							<email>dkyang20@fudan.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Academy for Engineering and Technology</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Cognition and Intelligent Technology Laboratory (CIT Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kun</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Academy for Engineering and Technology</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Mingcheng</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Academy for Engineering and Technology</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Cognition and Intelligent Technology Laboratory (CIT Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shunli</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Academy for Engineering and Technology</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Cognition and Intelligent Technology Laboratory (CIT Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shuaibing</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Academy for Engineering and Technology</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Cognition and Intelligent Technology Laboratory (CIT Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lihua</forename><surname>Zhang</surname></persName>
							<email>lihuazhang@fudan.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Academy for Engineering and Technology</orgName>
								<orgName type="institution">Fudan University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Cognition and Intelligent Technology Laboratory (CIT Lab</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Jilin Provincial Key Laboratory of Intelligence Science and Engineering</orgName>
								<address>
									<settlement>Changchun</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department" key="dep1">Engineering Research Center of AI and Robotics</orgName>
								<orgName type="department" key="dep2">Ministry of Education</orgName>
								<address>
									<addrLine>10 0 20 30 40 50 60 70 80 67.26 77</addrLine>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Robust Emotion Recognition in Context Debiasing</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-06-02">2 Jun 2024</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2403.05963v3[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-21T20:31+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Training sample Subject Training Subject branch Prediction Training sample CAER model (Ensemble branches) Prediction Training sample Context branch Prediction Vanilla Training Context Training</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Context-aware emotion recognition (CAER) has recently boosted the practical applications of affective computing techniques in unconstrained environments. Mainstream CAER methods invariably extract ensemble representations from diverse contexts and subject-centred characteristics to perceive the target person's emotional state. Despite advancements, the biggest challenge remains due to context bias interference. The harmful bias forces the models to rely on spurious correlations between background contexts and emotion labels in likelihood estimation, causing severe performance bottlenecks and confounding valuable context priors. In this paper, we propose a counterfactual emotion inference (CLEF) framework to address the above issue. Specifically, we first formulate a generalized causal graph to decouple the causal relationships among the variables in CAER. Following the causal graph, CLEF introduces a non-invasive context branch to capture the adverse direct effect caused by the context bias. During the inference, we eliminate the direct context effect from the total causal effect by comparing factual and counterfactual outcomes, resulting in bias mitigation and robust prediction. As a modelagnostic framework, CLEF can be readily integrated into existing methods, bringing consistent performance gains.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>"Context is the key to understanding, but it can also be the key to misunderstanding."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>-Jonathan Lockwood Huie</head><p>As the spiritual grammar of human life, emotions play an essential role in social communication and intelligent automation <ref type="bibr" target="#b19">[21]</ref>. Accurately recognizing subjects' emotional states from resource-efficient visual content has been ex- § Corresponding author. tensively explored in various fields, including online education <ref type="bibr" target="#b13">[15]</ref>, driving monitoring <ref type="bibr" target="#b56">[59]</ref>, and human-computer interaction <ref type="bibr" target="#b0">[1]</ref>. Conventional works have focused on extracting emotion-related information from subject attributes, such as facial expressions <ref type="bibr" target="#b8">[9]</ref>, body postures <ref type="bibr" target="#b2">[3]</ref>, acoustic behaviors <ref type="bibr" target="#b24">[26]</ref>, or multimodal combinations <ref type="bibr" target="#b20">[22,</ref><ref type="bibr" target="#b29">31,</ref><ref type="bibr" target="#b51">54,</ref><ref type="bibr" target="#b52">55,</ref><ref type="bibr" target="#b54">57,</ref><ref type="bibr" target="#b57">60]</ref>. Despite considerable advances in subject-oriented efforts, their performance suffers from severe bottlenecks in uncontrolled environments. As shown in Figure <ref type="figure" target="#fig_5">1a</ref>, physical representations of subjects in wild-collected images are usually indistinguishable (e.g., ambiguous faces) due to natural occlusions that fail to provide usable emotional signals.</p><p>Inspired by psychological research <ref type="bibr" target="#b1">[2]</ref>, context-aware emotion recognition (CAER) <ref type="bibr" target="#b16">[18]</ref> has been proposed to seek additional affective semantics from situational contexts. The contexts <ref type="bibr" target="#b17">[19]</ref> are typically considered to include out-of-subject factors, such as background objects, place attributes, scene elements, and dynamic interactions of surrounding agents. These rich contextual stimuli promisingly provide complementary emotion clues for accurate recogni- tion. Most existing methods perform emotion inference by extracting ensemble representations from subjects and contexts using sophisticated structures <ref type="bibr" target="#b22">[24,</ref><ref type="bibr" target="#b30">32,</ref><ref type="bibr" target="#b31">33,</ref><ref type="bibr" target="#b50">53,</ref><ref type="bibr" target="#b53">56,</ref><ref type="bibr" target="#b62">65]</ref>or customized mechanisms <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr">14,</ref><ref type="bibr" target="#b23">25,</ref><ref type="bibr" target="#b39">42]</ref>. Nevertheless, a recent study <ref type="bibr" target="#b55">[58]</ref> found that CAER models tend to rely on spurious correlations caused by a context bias rather than beneficial ensemble representations. An intuitive illustration is displayed in Figure <ref type="figure" target="#fig_0">1</ref>. We first randomly choose some training samples on the EMOTIC dataset <ref type="bibr" target="#b17">[19]</ref> and perform unsupervised clustering. From Figure <ref type="figure" target="#fig_5">1a</ref>, samples containing seaside-related contexts form compact feature clusters, confirming the semantic similarity in the feature space. These samples have positive emotion categories, while negative emotions are nonexistent in similar contexts.</p><p>In this case, the model <ref type="bibr" target="#b30">[32]</ref> is easily misled to capture spurious dependencies between context-specific semantics and emotion labels. In the testing phase from Figure <ref type="figure" target="#fig_5">1b</ref>, oriented to the sample with similar contexts but negative emotion categories, the model is confounded by the harmful context bias to infer completely wrong emotional states.</p><p>A straightforward solution is to conduct a randomized controlled trial by collecting images with all emotion annotations in all contexts. This manner is viewed as an approximate intervention for biased training. However, the current CAER debiasing effort <ref type="bibr" target="#b55">[58]</ref> is sub-optimal since the predefined intervention fails to decouple good and bad context semantics. We argue that context semantics consists of the good prior and the bad bias. The toy experiments are performed to verify this insight. Specifically, we train on the EMOTIC dataset separately using the subject branch, the ensemble branches, and the context branch of a CAER baseline <ref type="bibr" target="#b16">[18]</ref> in Figure <ref type="figure" target="#fig_1">2a</ref>. Recognized subjects in samples during context training are masked to capture the direct context effect. Observing the testing results in Figure <ref type="figure" target="#fig_1">2b</ref>, the context prior in ensemble learning as the valuable indirect effect helps the model filter out unnecessary candidates (i.e., removing the "Disapproval" and "Esteem" categories) compared to the subject branch. Conversely, the harmful bias as the direct context effect in the context branch builds a misleading mapping between dim contexts and negative emotions during training, causing biased predictions.</p><p>To disentangle the two effects in context semantics and achieve more appropriate context debiasing, we propose a unified counterfactual emotion inference (CLEF) framework from a causality perspective. CLEF focuses on assisting existing CAER methods to mitigate the context bias and breakthrough performance bottlenecks in a model-agnostic manner rather than beating them. Specifically, we first formulate a generalized causal graph to investigate causal relationships among variables in the CAER task. Along the causal graph, CLEF estimates the direct context effect caused by the harmful bias through a non-intrusive context branch during the training phase. Meanwhile, the valuable indirect effect of the context prior in ensemble representations of subjects and contexts is calculated following the vanilla CAER model. In the inference phase, we subtract the direct context effect from the total causal effect by depicting a counterfactual scenario to exclude bias interference. This scenario is described as follows:</p><p>Counterfactual CAER: What would the prediction be, if the model only sees the confounded context and does not perform inference via vanilla ensemble representations? Intuitively, ensemble representations in the counterfactual outcome are blocked in the no-treatment condition. As such, the model performs biased emotion estimation relying only on spurious correlations caused by the pure context bias, which results similarly to the predictions of the context branch in Figure <ref type="figure" target="#fig_1">2b</ref>. By comparing factual and counterfactual outcomes, CLEF empowers the model to make unbiased predictions using the debiased causal effect. The main contributions are summarized as follows:</p><p>• We are the first to embrace counterfactual thinking to investigate causal effects in the CAER task and reveal that the context bias as the adverse direct causal effect misleads the models to produce spurious prediction shortcuts. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Context-Aware Emotion Recognition. Benefiting from advances in deep learning algorithms <ref type="bibr">[6, 27-29, 46-48, 50-52, 61-63]</ref>, traditional emotion recognition typically infers emotional states from subject-oriented attributes, such as facial expressions <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b21">23]</ref>, body postures <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b56">59]</ref>, and acoustic behaviours <ref type="bibr" target="#b24">[26,</ref><ref type="bibr" target="#b29">31]</ref>. However, these efforts are potentially vulnerable in practical applications since subject characteristics in uncontrolled environments are usually indistinguishable, leading to severe performance deterioration.</p><p>Recently, a pioneering work <ref type="bibr" target="#b16">[18]</ref> inspired by psychological research <ref type="bibr" target="#b1">[2]</ref> has advocated extracting complementary emotional clues from rich contexts, called context-aware emotion recognition (CAER). Kosti et al. <ref type="bibr" target="#b17">[19]</ref> begin by utilizing a two-stream convolutional neural network (CNN) to capture effective semantics from subject-related regions and global contexts of complete images. The implementation is similar to the ensemble branch training in Figure <ref type="figure" target="#fig_1">2a</ref>. After that, most CAER methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr">14,</ref><ref type="bibr" target="#b18">20,</ref><ref type="bibr" target="#b22">24,</ref><ref type="bibr" target="#b23">25,</ref><ref type="bibr" target="#b30">32,</ref><ref type="bibr" target="#b31">33,</ref><ref type="bibr" target="#b50">53,</ref><ref type="bibr" target="#b53">56,</ref><ref type="bibr" target="#b61">64,</ref><ref type="bibr" target="#b62">65]</ref> follow an ensemble learning pattern: i) extracting unimodal/multimodal features from subject attributes; ii) learning emotionally relevant features from created contexts based on different definitions; and iii) producing ensemble representations for emotion predictions via fusion mechanisms. For instance, Yang et al. <ref type="bibr" target="#b53">[56]</ref> discretize the context into scenes, agent dynamics, and agent-object interactions, using customized components to learn complementary contextual information. Despite achievements, they invariably suffer from performance bottlenecks due to spurious correlations caused by the context bias. Causal Inference. Causal inference <ref type="bibr" target="#b11">[12]</ref> is first extensively used in economics <ref type="bibr" target="#b42">[45]</ref> and psychology <ref type="bibr" target="#b9">[10]</ref> as a scientific theory that seeks causal relationships among variables. The investigation of event causality generally follows two directions: intervention and counterfactuals. Intervention <ref type="bibr" target="#b34">[36]</ref> aims to actively manipulate the probability distributions of variables to obtain unbiased estimations or discover confounder effects. Counterfactuals [37] typically utilize distinct treatment conditions to imagine outcomes that are contrary to factual determinations, empowering systems to reason and think like humans. In recent years, several learningbased approaches have attempted to introduce causal inference in diverse fields to pursue desired model effects and exclude the toxicity of spurious shortcuts, including scene graph generation <ref type="bibr" target="#b41">[44]</ref>, visual dialogue <ref type="bibr" target="#b32">[34,</ref><ref type="bibr" target="#b37">40]</ref>, image recognition <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b28">30,</ref><ref type="bibr" target="#b46">49]</ref>, and adversarial learning <ref type="bibr" target="#b14">[16,</ref><ref type="bibr" target="#b15">17]</ref>.</p><p>The CAER debiasing effort <ref type="bibr" target="#b55">[58]</ref> most relevant to our work utilizes a predefined dictionary to approximate interventions and adopts memory-query operations to mitigate the bias dilemma. Nevertheless, the predefined-level intervention fails to capture pure bias effects in the context semantics, causing a sub-optimal solution. Inspired by <ref type="bibr" target="#b32">[34]</ref>, we remove the adverse context effect by empowering models with the debiasing ability of twice-thinking through counterfactual causality, which is fundamentally different in design philosophy and methodology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Preliminaries</head><p>Before starting, we first introduce the concepts and notations related to causal inference to facilitate a better understanding of our framework and philosophy. Causal graph is a highly generalized analytical tool to reveal causal dependencies among variables. It usually follows the structured causal mode <ref type="bibr" target="#b36">[39]</ref> defined as a directed acyclic graph G = {V, E}, where V stands for a set of variables and E implies the corresponding causal effects. A causal graph example with three variables is intuitively displayed in Figure <ref type="figure" target="#fig_2">3a</ref>. Here, we represent a random variable as a capital letter (e.g., P ), and denote its observed value as a lowercase letter (e.g., p). The causality from cause P to effect Q is reflected in two parts: the direct effect follows the causal link P → Q, and the indirect effect follows the link P → M → Q through the mediator variable M . Counterfactual inference endows the models with the ability to depict counterfactual outcomes in factual observations through different treatment conditions <ref type="bibr">[37]</ref>. In the factual outcome, the value of Q would be formalized under the conditions that P is set to p and M is set to m:</p><formula xml:id="formula_0">Q p,m = Q(P = p, M = m), m = M p = M (P = p).<label>(1)</label></formula><p>Counterfactual outcomes can be obtained by exerting distinct treatments on the value of P . As shown in Figure <ref type="figure" target="#fig_2">3b</ref>, when P is set to p * , and the descendant M is changed, we have  According to the causal theory <ref type="bibr" target="#b35">[38]</ref>, The Total Effect (TE) of treatment P = p on Q by comparing the two hypothetical outcomes is formulated as:</p><formula xml:id="formula_1">Q p * ,M p * = Q(P = p * , M p * = M (P = p * )). Sim- ilarly, Q p,</formula><formula xml:id="formula_2">TE = Q p,Mp -Q p * ,M p * .<label>(2)</label></formula><p>TE can be disentangled into the Natural Direct Effect (NDE) and the Total Indirect Effect (TIE) <ref type="bibr" target="#b11">[12]</ref>. NDE reflects the effect of P = p on Q following the direct link P → Q, and excluding the indirect effect along link P → M → Q due to M is set to the value when P had been p * . It reveals the response of Q when P converts from p to p * :</p><formula xml:id="formula_3">NDE = Q p,M p * -Q p * ,M p * .<label>(3)</label></formula><p>In this case, TIE is calculated by directly subtracting NDE from TE, which is employed to measure the unbiased prediction results in our framework:</p><formula xml:id="formula_4">TIE = TE -NDE = Q p,Mp -Q p,M p * .<label>(4)</label></formula><p>4. The proposed CLEF Framework The mediator E is obtained depending on the feature integration mechanisms of different vanilla methods, such as feature concatenation <ref type="bibr" target="#b16">[18]</ref> or attention fusion <ref type="bibr" target="#b18">[20]</ref>. In particular, C provides the valuable context prior along the good causal link C → E → Y , which gives favorable estimations of potential emotional states when the subjects' characteristics are indistinguishable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Counterfactual Inference</head><p>Our design philosophy is to mitigate the interference of the harmful context bias on model predictions by excluding the biased direct effect along the link X → C → Y . Following the notations on causal effects in Section 3, the causality in the factual scenarios is formulated as follows:</p><formula xml:id="formula_5">Y c,e (X) = Y (C = c, E c,s = E(C = c, S = s)|X).<label>(5)</label></formula><p>Y c,e (X) reflects confounded emotion predictions because it suffers from the detrimental direct effect of C, i.e., the pure context bias. To disentangle distinct causal effects in the context semantics, we calculate the Total Effect (TE) of C = c and S = s, which is expressed as follows:</p><formula xml:id="formula_6">TE = Y c,e (X) -Y c * ,e * (X).<label>(6)</label></formula><p>Here, c * and e * represent the non-treatment conditions for observed values of C and E, where c and s leading to e are not given. Immediately, we estimate the Natural Direct Effect (NDE) for the harmful bias in context semantics:</p><formula xml:id="formula_7">NDE = Y c,e * (X) -Y c * ,e * (X).<label>(7)</label></formula><p>Y c,e * (X) describes a counterfactual outcome where C is set to c and E would be imagined to be e * when C had been c * and S had been s * . The causal notation is expressed as:</p><formula xml:id="formula_8">Y c,e * (X) = Y (C = c, E c * ,s * = E(C = c * , S = s * )|X).<label>(8)</label></formula><p>Since the indirect causal effect of ensemble representations E on the link X → C/S → E → Y is blocked, the model can only perform biased predictions by relying on the direct context effect in the link X → C → Y that causes spurious correlations. To exclude the explicitly captured context bias in NDE, we subtract NDE from TE to estimate Total Indirect Effect (TIE):</p><formula xml:id="formula_9">TIE = Y c,e (X) -Y c,e * (X).<label>(9)</label></formula><p>We employ the reliable TIE as the unbiased prediction in the inference phase.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Implementation Instantiation</head><p>Framework Structure. From Figure <ref type="figure" target="#fig_4">4</ref>, CLEF's predictions consist of two parts: the prediction Y c (X) = N C (c|x) of the additional context branch (i.e., X → C → Y ) and Y e (X) = N C,S (c, s|x) of the vanilla CAER model (i.e., X → C/S → E → Y ). The context branch is instantiated as a simple neural network N C (•) (e.g., ResNet <ref type="bibr" target="#b12">[13]</ref>) to receive context images with masked recognized subjects. The masking operation forces the network to focus on pure context semantics for estimating the direct effect. For a given input x, its corresponding context image I x is expressed as:</p><formula xml:id="formula_10">I x = x(i, j) if x(i, j) / ∈ bbox subject , 0 otherwise ,<label>(10)</label></formula><p>where bbox subject means the bounding box of the subject. N C,S (•) denotes any CAER model based on their specific mechanisms to learn ensemble representations e from c and s for prediction. Subsequently, a pragmatic fusion strategy ϕ(•) is introduced to obtain the final score Y c,e (X):</p><formula xml:id="formula_11">Y c,e (X) = ϕ(Y c (X), Y e (X)) = logσ(Y c (X) + Y e (X)),<label>(11)</label></formula><p>where σ is the sigmoid activation. Training Procedure. As a universal framework, we take the multi-class classification task in Figure <ref type="figure" target="#fig_4">4</ref> as an example to adopt the cross-entropy loss CE(•) as the optimization objective. The task-specific losses for Y c,e (X) and Y c,e * (X) are as follows:</p><formula xml:id="formula_12">L task = CE(Y c,e (X), y) + CE(Y c,e * (X), y),<label>(12)</label></formula><p>where y means the ground truth. Since neural models cannot handle no-treatment conditions where the inputs are void, we devise a trainable parameter initialized by the uniform distribution in practice to represent the imagined Y e * (X), which is shared by all samples. The design intuition is that the uniform distribution ensures a safe estimation for NDE, which is justified in subsequent ablation studies. To avoid inappropriate Y e * (X) that potentially causes TIE to be dominated by TE or NDE, we employ the Kullback-Leibler divergence KL(•) to regularize the difference between Y c,e * (X) and Y c,e (X) to estimate Y e * (X):</p><formula xml:id="formula_13">L kl = KL(Y c,e * (X), Y c,e (X)).<label>(13)</label></formula><p>The final loss is expressed as:</p><formula xml:id="formula_14">L f in = (c,s,y)∈D L task + L kl . (<label>14</label></formula><formula xml:id="formula_15">)</formula><p>Inference Procedure. According to Eq. ( <ref type="formula" target="#formula_9">9</ref>), the debiased prediction is performed as follows:</p><formula xml:id="formula_16">TIE = ϕ(Y c (X), Y e (X)) -ϕ(Y c (X), Y e * (X)). (<label>15</label></formula><formula xml:id="formula_17">)</formula><p>5. Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets and Evaluation Metrics</head><p>Experiments are conducted on two large-scale image-based CAER datasets, including EMOTIC <ref type="bibr" target="#b17">[19]</ref> and CAER-S <ref type="bibr" target="#b18">[20]</ref>.</p><p>EMOTIC is the first benchmark to support emotion recognition in real-world contexts, which has from video clips. These images record 7 emotional states of different subjects in various context scenarios from 79 TV shows. The data samples are randomly divided into training, validation, and testing sets in the ratio of 7:1:2. We utilize the standard mean Average Precision (mAP) and classification accuracy to evaluate the results on the EMOTIC and CAER-S datasets, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Model Zoo</head><p>We evaluate the effectiveness of the proposed CLEF using five representative methods, which have completely different network structures and contextual modelling paradigms. Concretely, EMOT-Net <ref type="bibr" target="#b16">[18]</ref> is a two-stream classical CNN model where one stream extracts human features from body regions, and the other captures global context semantics. CAER-Net <ref type="bibr" target="#b18">[20]</ref> extracts subject attributes from faces and uses the images after hiding faces as background contexts. GNN-CNN <ref type="bibr" target="#b62">[65]</ref> utilizes the graph neural network (GNN) to integrate emotion-related objects in contexts and distills subject information with a VGG-16 <ref type="bibr" target="#b40">[43]</ref>. CD-Net <ref type="bibr" target="#b50">[53]</ref> designs a tube-transformer to perform fine-grained interactions from facial, bodily, and contextual features. Emoti-Con <ref type="bibr" target="#b30">[32]</ref> employs attention and depth maps to model context representations. Subject-relevant features are extracted from facial expressions and body postures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Implementation Details</head><p>We use a ResNet-152 <ref type="bibr" target="#b12">[13]</ref> pre-trained on the Places365 <ref type="bibr" target="#b63">[66]</ref> dataset to parameterize the non-invasive context branch in CLEF. The output of the last linear layer is replaced to produce task-specific numbers of neurons for predictions. Rich scene attributes in Places365 provide proper semantics for distilling the context bias. In addition to the annotated EMOTIC, we employ the Faster R-CNN <ref type="bibr" target="#b38">[41]</ref> to detect bounding boxes of recognized subjects in CAER-S.</p><p>Immediately, the context images I x are obtained by masking the target subjects in samples based on the corresponding bounding boxes. For a fair comparison, the five selected CAER methods are reproduced via the PyTorch toolbox <ref type="bibr" target="#b33">[35]</ref> following their reported training settings, including the optimizer, loss function, learning rate strategy, etc. All models are implemented on NVIDIA Tesla V100 GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Comparison with State-of-the-art Methods</head><p>We compare the five CLEF-based methods with existing SOTA models, including HLCR <ref type="bibr" target="#b6">[7]</ref>, TEKG <ref type="bibr" target="#b4">[5]</ref>, RRLA <ref type="bibr" target="#b22">[24]</ref>, VRD [14], SIB-Net <ref type="bibr" target="#b23">[25]</ref>, MCA <ref type="bibr" target="#b53">[56]</ref>, and GRERN <ref type="bibr" target="#b10">[11]</ref>.</p><p>Quantitative Results on the EMOTIC. Table <ref type="table" target="#tab_1">1</ref> shows the Average Precision (AP) of the vanilla methods and their counterparts in the CLEF framework for each emotion category. We have the following critical observations. i) CLEF significantly improves the performance of all models in most categories. For instance, CLEF yields average gains of 8.33% and 6.52% on the AP scores for "Affection" and "Sadness", reflecting positivity and negativity, respectively. ii) Our framework favorably improves several categories heavily confounded by the harmful context bias due to uneven distributions of emotional states across distinct  contexts. For example, the CLEF-based models improve the AP scores for "Engagement" and "Happiness" categories to 90.46%∼97.39% and 72.37%∼87.06%, outperforming the results in the vanilla baselines by large margins. Table <ref type="table" target="#tab_2">2</ref> presents the comparison results with existing models regarding the mean AP (mAP) scores. i) Thanks to CLEF's bias exclusion, the mAP scores of EMOT-Net, CAER-Net, GNN-CNN, CD-Net, and EmotiCon are consistently increased by 3.74%, 3.59%, 4.02%, 3.64%, and 2.77%, respectively. Among them, the most noticeable improvement in GNN-CNN is because the vanilla model more easily captures spurious context-emotion correlations based on fine-grained context element exploration <ref type="bibr" target="#b62">[65]</ref>, leading to the better debiasing effect with CLEF. ii) Compared to SIB-Net and MCA with complex module stacking <ref type="bibr" target="#b53">[56]</ref> and massive parameters <ref type="bibr" target="#b23">[25]</ref>, the CLEF-based EmotiCon achieves the best performance with the mAP score of 38.05% through efficient counterfactual inference. Quantitative Results on the CAER-S. Table <ref type="table">3</ref> provides the evaluation results on the CAER-S dataset. i) Evidently, CLEF consistently improves different baselines by decoupling and excluding the prediction bias of emotional states in the TV show contexts. Concretely, the overall accuracies of EMOT-Net, CAER-Net, GNN-CNN, CD-Net, and EmotiCon are improved by 2.52%, 2.39%, 2.32%, 3.08%, Table <ref type="table">3</ref>. Quantitative results of different models and CLEF-based methods on the CAER-S dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head><p>Accuracy (%)</p><p>Fine-tuned VGGNet <ref type="bibr" target="#b40">[43]</ref> 64.85 Fine-tuned ResNet <ref type="bibr" target="#b12">[13]</ref> 68.46 SIB-Net <ref type="bibr" target="#b23">[25]</ref> 74.56 MCA <ref type="bibr" target="#b53">[56]</ref> 79.57 GRERN <ref type="bibr" target="#b10">[11]</ref> 81.31 RRLA <ref type="bibr" target="#b22">[24]</ref> 84 and 1.97%, respectively. ii) The gains of our framework on the CAER-S are slightly weaker than those on the EMOTIC. A reasonable explanation is that the EMOTIC contains richer context semantics than the CAER-S, such as scene elements and agent dynamics <ref type="bibr" target="#b17">[19]</ref>. As a result, CLEF more accurately estimates the adverse context effect and favorably removes its interference. iii) Also, we find in Figure <ref type="figure" target="#fig_6">5</ref> that the classification accuracies of most emotion categories across the five methods are improved appropriately.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Ablation Studies</head><p>In Table <ref type="table">4</ref>, we select the SOTA CD-Net and EmotiCon to perform thorough ablation studies on both datasets. Necessity of Framework Structure. i) When removing CAER models from CLEF, the significant performance deterioration suggests that the indirect causal effect in ensemble representations provides valuable emotion semantics. ii) When the additional context branch (ACB) is excluded, CLEF degrades to a debiased pattern that is not context-conditional, treated as TE. TE's gains are inferior to TIE's since it reduces the general bias over the whole dataset rather than the specific context bias. iii) Also, we find that the KL( images of ACB is essential for ensuring reliable capture of the context-oriented adverse direct effect. ii) When the ResNet-152 pre-trained on Places365 <ref type="bibr" target="#b63">[66]</ref> is replaced with the one pre-trained on ImageNet <ref type="bibr" target="#b7">[8]</ref> in ACB, the gain drops prove that scene-level semantics are more expressive than object-level semantics in reflecting the context bias. This makes sense since scene attributes usually contain diverse object concepts. iii) Moreover, the improvements from CLEF gradually increase as more advanced pre-training backbones are used, which shows that our framework does not rely on a specific selection of instantiated networks. Effectiveness of No-treatment Assumption. We provide two alternatives regarding the no-treatment condition assumption, where random and average feature embeddings are obtained by the random initialization and the prior distribution of the training set, respectively. The worse-thanbaseline results imply that our uniform distribution assumption ensures a safe estimation of the biased context effect. Debiasing Ability Comparison. A gain comparison between our CLEF and the previous CAER debiasing effort CCIM on both datasets is presented in Table <ref type="table" target="#tab_5">5</ref>. Intuitively, our framework consistently outperforms CCIM <ref type="bibr" target="#b55">[58]</ref> in both methods. The reasonable reason is that CCIM fails to capture the pure context bias due to over-reliance on the predefined context confounders, causing sub-optimal solutions. In contrast, CLEF decouples the good context prior and the bad context effect, enabling robust debiased predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Qualitative Evaluation</head><p>Figure <ref type="figure">6</ref> shows the performance of vanilla CD-Net before and after counterfactual debiasing via CLEF. Intuitively, our framework effectively corrects the misjudgments of the vanilla method for emotional states in diverse contexts. Taking Figure <ref type="figure">6a</ref> as an example, CLEF eliminates spurious correlations between vegetation-related contexts and positive emotions, giving negative categories aligned with ground truths. Moreover, the CLEF-based CD-Net in Figure <ref type="figure">6e</ref> excludes misleading clues about negative emotions provided by dim contexts and achieves an unbiased prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>This paper proposes CLEF, a causal debiasing framework based on counterfactual inference to address the context bias interference in CAER. CLEF reveals that the harmful bias confounds model performance along the direct causal effect via the tailored causal graph, and accomplishes bias mitigation by subtracting the direct context effect from the total causal effect. Extensive experiments prove that CLEF brings favorable improvements to existing models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Illustration of the context bias in the CAER task. GT stands for the Ground Truth. Context-specific semantics easily yield spurious shortcuts with emotion labels during training to confound the model [32], giving erroneous results. Conversely, our CLEF effectively corrects biased predictions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. We conduct toy experiments to show the effects of context semantics. The indirect effect of the good context prior follows ensemble branches, narrowing the emotion candidate space. The bad direct effect follows the context branch, causing pure bias.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. (a) Examples of a causal graph where nodes represent variables and arrows represent causal effects. (b) Examples of counterfactual notations. (c) The proposed CAER causal graph.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>M p * reflects the counterfactual situation where P = p and M is set to the value when P = p * . Causal effects reveal the difference between two corresponding outcomes when the value of the reference variable changes. Let P = p denote the treated condition and P = p * represent the invisible counterfactual condition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. High-level overview of the proposed CLEF framework implementation. In addition to the vanilla CAER model, we introduce an additional context branch in a non-intrusive manner to capture the pure context bias as the direct context effect. By comparing factual and counterfactual outcomes, our framework effectively mitigates the interference of the harmful bias and achieves debiased emotion inference.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>4. 1 .</head><label>1</label><figDesc>Cause-Effect Look at CAERAs shown in Figure3c, there are five variables in the proposed CAER causal graph, including input images X, subject features S, context features C, ensemble representations E, and emotion predictions Y . Note that our causal graph has broad applicability and generality since it follows most CAER modelling paradigms. Link X → C → Y reflects the shortcut between the original inputs X and the model predictions Y through the harmful bias in the context features C. The adverse direct effect of the mediator C is obtained via a non-invasive branch of context modelling, which captures spurious correlations between context-specific semantics and emotion labels. Taking Figure2bas an example, the context branch learns the undesired mapping between dim contexts and negative emotions during training. Link C ← X → S portrays the total context and subject representations extracted from X via the corresponding encoders in vanilla CAER models. Based on design differences in distinct methods, C and S may come from a single feature or an aggregation of multiple sub-features. For instance, S is obtained from global body attributes and joint face-pose information in models<ref type="bibr" target="#b16">[18]</ref> and<ref type="bibr" target="#b30">[32]</ref>, respectively. Link C/S → E → Y captures the indirect causal effect of C and S on the model predictions Y through the ensemble representations E.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Emotion classification accuracy (%) for each category of different CLEF-based methods on the CAER-S dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 .</head><label>1</label><figDesc>Quantitative results of CLEF-based methods for each emotion category on the EMOTIC dataset. We report the average precision of each category to provide comprehensive comparison experiments. The improved results are marked in bold.</figDesc><table><row><cell>23,571 images</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 .</head><label>2</label><figDesc>Quantitative results of different models and CLEF-based methods on the EMOTIC dataset. ↑ represents the improvement of the CLEF-based version over the vanilla method.</figDesc><table><row><cell>Methods</cell><cell>mAP (%)</cell></row><row><cell>HLCR [7]</cell><cell>30.02</cell></row><row><cell>TEKG [5]</cell><cell>31.36</cell></row><row><cell>RRLA [24]</cell><cell>32.41</cell></row><row><cell>VRD [14]</cell><cell>35.16</cell></row><row><cell>SIB-Net [25]</cell><cell>35.41</cell></row><row><cell>MCA [56]</cell><cell>37.73</cell></row><row><cell>EMOT-Net [19]</cell><cell>27.93</cell></row><row><cell>EMOT-Net + CLEF</cell><cell>31.67 (↑ 3.74)</cell></row><row><cell>CAER-Net [20]</cell><cell>23.85</cell></row><row><cell>CAER-Net + CLEF</cell><cell>27.44 (↑ 3.59)</cell></row><row><cell>GNN-CNN [65]</cell><cell>28.16</cell></row><row><cell>GNN-CNN + CLEF</cell><cell>32.18 (↑ 4.02)</cell></row><row><cell>CD-Net [53]</cell><cell>28.87</cell></row><row><cell>CD-Net + CLEF</cell><cell>32.51 (↑ 3.64)</cell></row><row><cell>EmotiCon [32]</cell><cell>35.28</cell></row><row><cell>EmotiCon + CLEF</cell><cell>38.05 (↑ 2.77)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5 .</head><label>5</label><figDesc>Debiasing comparison results of CCIM<ref type="bibr" target="#b55">[58]</ref> and the proposed CLEF on the EMOTIC and CAER-S datasets.</figDesc><table><row><cell>Dataset</cell><cell cols="6">EMOT-Net [19] Vanilla w/ CCIM w/ CLEF Vanilla w/ CCIM w/ CLEF CAER-Net [20]</cell></row><row><cell>EMOTIC</cell><cell>27.93</cell><cell>30.88</cell><cell>31.67</cell><cell>23.85</cell><cell>26.51</cell><cell>27.44</cell></row><row><cell>CAER-S</cell><cell>74.51</cell><cell>75.82</cell><cell>77.03</cell><cell>73.47</cell><cell>74.81</cell><cell>75.86</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements This work is supported in part by the <rs type="funder">National Key R&amp;D Program of China</rs> (<rs type="grantNumber">2021ZD0113503</rs>) and in part by the <rs type="funder">Shanghai Municipal Science and Technology Committee of Shanghai Outstanding Academic Leaders Plan</rs> (No. <rs type="grantNumber">21XD1430300</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_EwY75EJ">
					<idno type="grant-number">2021ZD0113503</idno>
				</org>
				<org type="funding" xml:id="_DHEueeX">
					<idno type="grant-number">21XD1430300</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Table <ref type="table">4</ref>. Ablation study results on the EMOTIC and CAER-S datasets. "ACB" means the additional context branch. "w/" and "w/o" are short for the with and without, respectively.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Humancomputer interaction with detection of speaker emotions using convolution neural networks</title>
		<author>
			<persName><forename type="first">Abeer</forename><surname>Ali Alnuaim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammed</forename><surname>Zakariah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aseel</forename><surname>Alhadlaq</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chitra</forename><surname>Shashidhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Atef</forename><surname>Wesam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hussam</forename><surname>Hatamleh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prashant</forename><surname>Tarazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajnish</forename><surname>Kumar Shukla</surname></persName>
		</author>
		<author>
			<persName><surname>Ratna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Intelligence and Neuroscience</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Context in emotion perception. Current Directions in Psychological</title>
		<author>
			<persName><forename type="first">Lisa</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barrett</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Batja</forename><surname>Mesquita</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Gendron</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Step: Spatial temporal graph convolutional networks for emotion perception from gaits</title>
		<author>
			<persName><forename type="first">Uttaran</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trisha</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohan</forename><surname>Chandra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note>Tanmay Randhavane, Aniket Bera, and Dinesh Manocha</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Krzysztof</forename><surname>Chalupka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frederick</forename><surname>Eberhardt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.2309</idno>
		<title level="m">Visual causal feature learning</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Incorporating structured emotion commonsense knowledge and interpersonal relation into context-aware emotion recognition</title>
		<author>
			<persName><forename type="first">Jing</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kejun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meichen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyan</forename><surname>Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Intelligence</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="4201" to="4217" />
			<date type="published" when="2007">2023. 2, 3, 6, 7</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Miss: A generative pretraining and finetuning approach for med-vqa</title>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dingkang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxuan</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lihua</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2401.05163</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">High-level context representation for emotion recognition in images</title>
		<author>
			<persName><forename type="first">Willams</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lima</forename><surname>Costa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Estefania</forename><surname>Talavera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Silva Figueiredo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veronica</forename><surname>Teichrieb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshop (CVPRW)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshop (CVPRW)</meeting>
		<imprint>
			<date type="published" when="2007">2023. 2, 3, 6, 7</date>
			<biblScope unit="page" from="326" to="334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning associative representation for facial expression recognition</title>
		<author>
			<persName><forename type="first">Yangtao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dingkang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingchen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lihua</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Causal inference and developmental psychology</title>
		<author>
			<persName><forename type="first">Foster</forename><surname>Michael</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Developmental Psychology</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Graph reasoning-based emotion recognition network</title>
		<author>
			<persName><forename type="first">Qinquan</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanxin</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="6488" to="6497" />
			<date type="published" when="2007">2021. 2, 3, 6, 7</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Causal inference in statistics: A primer</title>
		<author>
			<persName><forename type="first">Madelyn</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><forename type="middle">P</forename><surname>Jewell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>John Wiley &amp; Sons</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Context-aware emotion recognition based on visual relationship detection</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">; Manh-Hung</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soo-Hyung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyung-Jeong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guee-Sang</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016">2016. 5, 6, 7, 8 [14</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
	<note>Deep residual learning for image recognition. 2021. 2, 3, 6</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Emotion regulation in education: Conceptual foundations, current applications, and future directions. International Handbook of Emotions in Education</title>
		<author>
			<persName><forename type="first">E</forename><surname>Scott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">J</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName><surname>Gross</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="183" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Structural agnostic model, causal discovery and penalized adversarial learning</title>
		<author>
			<persName><forename type="first">Diviyan</forename><surname>Kalainathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Goudet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isabelle</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michèle</forename><surname>Sebag</surname></persName>
		</author>
		<author>
			<persName><surname>Sam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Causalgan: Learning causal implicit generative models with adversarial training</title>
		<author>
			<persName><forename type="first">Murat</forename><surname>Kocaoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Snyder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandros</forename><forename type="middle">G</forename><surname>Dimakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sriram</forename><surname>Vishwanath</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.02023</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Emotion recognition in context</title>
		<author>
			<persName><forename type="first">Ronak</forename><surname>Kosti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adria</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2006">2017. 1, 2, 3, 4, 6</date>
			<biblScope unit="page" from="1667" to="1675" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Context based emotion recognition using emotic dataset</title>
		<author>
			<persName><forename type="first">Ronak</forename><surname>Kosti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jose</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adria</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2755" to="2766" />
			<date type="published" when="2008">2019. 1, 2, 3, 5, 6, 7, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Context-aware emotion recognition networks</title>
		<author>
			<persName><forename type="first">Jiyoung</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seungryong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunok</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jungin</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kwanghoon</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2008">2019. 3, 4, 5, 6, 7, 8</date>
			<biblScope unit="page" from="10143" to="10152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Text-oriented modality reinforcement network for multimodal sentiment analysis from unaligned multimodal sequences</title>
		<author>
			<persName><forename type="first">Yuxuan</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dingkang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingcheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shunli</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiawei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lihua</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.13205</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Towards robust multimodal sentiment analysis under uncertain signal missing</title>
		<author>
			<persName><forename type="first">Mingcheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dingkang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lihua</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1497" to="1501" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep facial expression recognition: A survey</title>
		<author>
			<persName><forename type="first">Shan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihong</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1195" to="1215" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Human emotion recognition with relational region-level analysis</title>
		<author>
			<persName><forename type="first">Weixin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuan</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunhong</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<date type="published" when="2007">2021. 2, 3, 6, 7</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Sequential interactive biased network for context-aware emotion recognition</title>
		<author>
			<persName><forename type="first">Xinpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojiang</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changxing</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Joint Conference on Biometrics (IJCB)</title>
		<imprint>
			<date type="published" when="2007">2021. 2, 3, 6, 7</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Ctnet: Conversational transformer network for emotion recognition</title>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianhua</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Audio, Speech, and Language Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning appearance-motion normality for video anomaly detection</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengyang</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dingkang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoguang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 IEEE International Conference on Multimedia and Expo (ICME)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Ampnet: Appearance-motion prototype network assisted automatic video anomaly detection system</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bobo</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuzheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dingkang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Industrial Informatics</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Generalized video anomaly event detection: Systematic taxonomy and comparison of deep models</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dingkang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2302.05087</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Discovering causal signals in images</title>
		<author>
			<persName><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Nishihara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6979" to="6987" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">M3er: Multiplicative multimodal emotion recognition using facial, textual, and speech cues</title>
		<author>
			<persName><forename type="first">Trisha</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uttaran</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohan</forename><surname>Chandra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
	<note>Aniket Bera, and Dinesh Manocha</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Emoticon: Context-aware multimodal emotion recognition using frege&apos;s principle</title>
		<author>
			<persName><forename type="first">Trisha</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pooja</forename><surname>Guhan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uttaran</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rohan</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aniket</forename><surname>Bera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinesh</forename><surname>Manocha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2007">2020. 1, 2, 3, 4, 6, 7</date>
			<biblScope unit="page" from="14234" to="14243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multimodal and context-aware emotion perception model with multiplicative fusion</title>
		<author>
			<persName><forename type="first">Trisha</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aniket</forename><surname>Bera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinesh</forename><surname>Manocha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE MultiMedia</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Counterfactual vqa: A causeeffect look at language bias</title>
		<author>
			<persName><forename type="first">Yulei</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaihua</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiwu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian-Sheng</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="12700" to="12710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Causal inference in statistics: An overview</title>
		<author>
			<persName><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics Surveys</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="96" to="146" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Interpretation and identification of causal mediation</title>
		<author>
			<persName><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Methods</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">459</biblScope>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Models, reasoning and inference</title>
		<author>
			<persName><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>CambridgeUniversityPress</publisher>
			<biblScope unit="volume">19</biblScope>
			<pubPlace>Cambridge, UK</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Two causal principles for improving visual dialog</title>
		<author>
			<persName><forename type="first">Jiaxin</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulei</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="10860" to="10869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Context-aware generation-based net for multi-label visual emotion recognition</title>
		<author>
			<persName><forename type="first">Kun</forename><surname>Shulan Ruan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yijun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanqing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weidong</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangyi</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enhong</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE International Conference on Multimedia and Expo (ICME)</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Unbiased scene graph generation from biased training</title>
		<author>
			<persName><forename type="first">Kaihua</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulei</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxin</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3716" to="3725" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Causal inference in economics and marketing</title>
		<author>
			<persName><surname>Hal R Varian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="issue">27</biblScope>
			<biblScope unit="page" from="7310" to="7315" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Tsa-net: Tube self-attention network for action quality assessment</title>
		<author>
			<persName><forename type="first">Shunli</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dingkang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chixiao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lihua</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM International Conference on Multimedia (ACM MM)</title>
		<meeting>the 29th ACM International Conference on Multimedia (ACM MM)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4902" to="4910" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Caspacenet: Counterfactual analysis for 6d pose estimation in space</title>
		<author>
			<persName><forename type="first">Shunli</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuaibing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dingkang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liuzhen</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chixiao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lihua</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="10627" to="10634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Cpr-clip: Multimodal pre-training for composite error recognition in cpr training</title>
		<author>
			<persName><forename type="first">Shunli</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dingkang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lihua</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Visual commonsense r-cnn</title>
		<author>
			<persName><forename type="first">Tan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianqiang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qianru</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="10760" to="10770" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Model robustness meets data privacy: Adversarial robustness distillation without original data</title>
		<author>
			<persName><forename type="first">Yuzheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaoyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dingkang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pinxue</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaixun</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizhe</forename><surname>Qi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2303.11611</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Adversarial contrastive distillation with adaptive denoising</title>
		<author>
			<persName><forename type="first">Yuzheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaoyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dingkang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizhe</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<author>
			<persName><forename type="first">Yuzheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaoyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dingkang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zuhao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunquan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenqiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lizhe</forename><surname>Qi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2307.16601</idno>
		<title level="m">Sampling to distill: Knowledge transfer from open-world data</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Context-dependent emotion recognition</title>
		<author>
			<persName><forename type="first">Zili</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingjie</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Communication and Image Representation</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="page">103679</biblScope>
			<date type="published" when="2008">2022. 2, 3, 6, 7, 8</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Disentangled representation learning for multimodal emotion recognition</title>
		<author>
			<persName><forename type="first">Dingkang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haopeng</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangtao</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lihua</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th ACM International Conference on Multimedia (ACM MM)</title>
		<meeting>the 30th ACM International Conference on Multimedia (ACM MM)</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1642" to="1651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Contextual and cross-modal interaction for multi-modal speech emotion recognition</title>
		<author>
			<persName><forename type="first">Dingkang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lihua</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2022">2093-2097, 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Emotion recognition for multiple context awareness</title>
		<author>
			<persName><forename type="first">Dingkang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shunli</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liuzhen</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingcheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lihua</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference on Computer Vision (ECCV)</title>
		<meeting>the European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<date type="published" when="2007">2022. 2, 3, 6, 7</date>
			<biblScope unit="page" from="144" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Learning modality-specific and -agnostic representations for asynchronous multimodal language sequences</title>
		<author>
			<persName><forename type="first">Dingkang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haopeng</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lihua</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th ACM International Conference on Multimedia (ACM MM)</title>
		<meeting>the 30th ACM International Conference on Multimedia (ACM MM)</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1708" to="1717" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Context deconfounded emotion recognition</title>
		<author>
			<persName><forename type="first">Dingkang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaoyu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuzheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shunli</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingcheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyan</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lihua</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Aide: A vision-driven multi-view, multimodal, multi-tasking dataset for assistive driving perception</title>
		<author>
			<persName><forename type="first">Dingkang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenpeng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shunli</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingcheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuzheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaoyu</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Target and source modality co-reinforcement for emotion understanding from asynchronous multimodal sequences</title>
		<author>
			<persName><forename type="first">Dingkang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Can</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingcheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuzheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lihua</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge-Based Systems</title>
		<imprint>
			<biblScope unit="volume">265</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">110370</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">How2comm: Communication-efficient and collaboration-pragmatic multiagent perception</title>
		<author>
			<persName><forename type="first">Dingkang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuzheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rongbin</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lihua</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-seventh Conference on Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Spatio-temporal domain awareness for multi-agent collaborative perception</title>
		<author>
			<persName><forename type="first">Kun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dingkang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingcheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="23383" to="23392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">What2comm: Towards communication-efficient collaborative perception via feature decoupling</title>
		<author>
			<persName><forename type="first">Kun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dingkang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31th ACM International Conference on Multimedia (ACM MM)</title>
		<meeting>the 31th ACM International Conference on Multimedia (ACM MM)</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="7686" to="7695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">A graph convolutional network for emotion recognition in context</title>
		<author>
			<persName><forename type="first">Hanxin</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinquan</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 Cross Strait Radio Science &amp; Wireless Technology Conference (CSRSWTC)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="3" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Contextaware affective graph reasoning for emotion recognition</title>
		<author>
			<persName><forename type="first">Minghui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yumeng</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huadong</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Multimedia and Expo (ICME)</title>
		<imprint>
			<date type="published" when="2007">2019. 2, 3, 6, 7</date>
			<biblScope unit="page" from="151" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Places: A 10 million image database for scene recognition</title>
		<author>
			<persName><forename type="first">Bolei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Agata</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aude</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1452" to="1464" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
