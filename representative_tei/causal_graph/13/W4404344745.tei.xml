<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LEARNING MIXTURES OF UNKNOWN CAUSAL INTERVENTIONS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-10-31">31 Oct 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Abhinav</forename><surname>Kumar</surname></persName>
							<email>akumar03@mit.edu</email>
						</author>
						<author>
							<persName><forename type="first">Kirankumar</forename><surname>Shiragur</surname></persName>
							<email>kshiragur@microsoft.com</email>
						</author>
						<author>
							<persName><forename type="first">Caroline</forename><surname>Uhler</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="laboratory">LIDS</orgName>
								<orgName type="institution">Massachusetts Institute of Technology Broad Institute of MIT</orgName>
								<address>
									<settlement>Harvard</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="laboratory">LIDS</orgName>
								<orgName type="institution" key="instit1">Microsoft Research</orgName>
								<orgName type="institution" key="instit2">Massachusetts Institute of Technology Broad Institute of MIT</orgName>
								<address>
									<settlement>Harvard</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">LEARNING MIXTURES OF UNKNOWN CAUSAL INTERVENTIONS</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-10-31">31 Oct 2024</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2411.00213v1[stat.ML]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-21T20:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The ability to conduct interventions plays a pivotal role in learning causal relationships among variables, thus facilitating applications across diverse scientific disciplines such as genomics, economics, and machine learning. However, in many instances within these applications, the process of generating interventional data is subject to noise: rather than data being sampled directly from the intended interventional distribution, interventions often yield data sampled from a blend of both intended and unintended interventional distributions. We consider the fundamental challenge of disentangling mixed interventional and observational data within linear Structural Equation Models (SEMs) with Gaussian additive noise without the knowledge of the true causal graph. We demonstrate that conducting interventions, whether do or soft, yields distributions with sufficient diversity and properties conducive to efficiently recovering each component within the mixture. Furthermore, we establish that the sample complexity required to disentangle mixed data inversely correlates with the extent of change induced by an intervention in the equations governing the affected variable values. As a result, the causal graph can be identified up to its interventional Markov Equivalence Class, similar to scenarios where no noise influences the generation of interventional data. We further support our theoretical findings by conducting simulations wherein we perform causal discovery from such mixed data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Interventions are experiments that can help us understand the mechanisms governing complex systems and also modify these systems to achieve desired outcomes <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b20">21]</ref>. For example, in causal discovery, interventions are used to infer causal relationships between variables of interest, which has applications in various fields such as biology <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b11">12]</ref>, economics <ref type="bibr" target="#b7">[8]</ref>, and psychology <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b20">21]</ref>.</p><p>Given their extensive applications, there has been significant research in developing methods and experimental design strategies to conduct interventions under different scenarios <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b23">24]</ref>. Despite significant efforts to develop sophisticated experimental techniques to perform interventions, they often encounter noise <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b29">30]</ref>. For example, the CRISPR technology, extensively used to perform gene perturbations (or interventions), is known to have off-target effects, meaning the interventions do not always occur on the intended genes <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b29">30]</ref>. Consequently, in many applications, performing interventions generates data from a mixture of intended and unintended interventional distributions. Analyzing such mixed data directly can lead to incorrect conclusions, adversely affecting downstream applications. Hence, it is essential to disentangle the mixture and recover the components corresponding to each individual intervention for further use in downstream tasks like causal discovery.</p><p>In our work, we formally address the challenge of disentangling mixtures of unknown interventional and observational distributions within the framework of linear structural equation models (Linear-SEM) with additive Gaussian noise. Given iid samples from a mixture with a fixed number of components as input, we present an efficient algorithm that can learn each individual component. Our results are applicable to both do and more general soft interventions.</p><p>We chose to study our problem in the Linear-SEM with additive Gaussian noise framework for its fundamental importance in the causal discovery literature. Shimizu et al. <ref type="bibr" target="#b24">[25]</ref> showed that observational data is sufficient for learning Learning Mixture of Gaussians. Learning a mixture of Gaussians is a heavily studied problem <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b10">11]</ref>. There exist efficient algorithms both in terms of runtime and sample complexity for a fixed number of components in the mixture <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b1">2]</ref>. Ge et al. <ref type="bibr" target="#b6">[7]</ref> gave an efficient algorithm for the case when the number of components is almost √ n where n is the dimension of the variables. However, this method only guarantees identifiability in the perturbative setting, where the true parameters are randomly perturbed before the samples are generated.</p><p>Causal Discovery with Unknown Interventions. Recent years have seen the development of methods that perform causal discovery with observational and multiple unknown interventional data <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b9">10]</ref>. Squires et al. <ref type="bibr" target="#b25">[26]</ref> takes multiple but segregated datasets from unknown interventional distributions and aims to identify the unknown interventions and learn the underlying causal graph up to its interventional MEC (I-MEC). Jaber et al. <ref type="bibr" target="#b9">[10]</ref> considers the same problem of causal discovery with unknown soft interventions in non-Markovian systems (i.e., latent common cause models). However, they also assume that the interventional and/or observational data is already segregated. To our knowledge, no prior works consider directly learning the causal graph from a mixture of interventions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Notation and Preliminaries</head><p>Notation. We use the upper-case letter X to denote a random variable and lower-case "x" to denote the value taken by the random variable X. Let, the uppercase bold-face letter X denote a set of random variables and the lowercase bold-face letter x denote the corresponding value taken by X. Let the conditional probability function P(X = x|Y = y) be denoted by P(x|y). We use the calligraphic letter S to denote a set and |S| to denote the cardinality of the set S. Let [n] denote the set of natural numbers {1, . . . , n}. For any vector v, we use the notation [v] j to denote it's j th entry and for any matrix M we use [M ] i,j to denote the entry in the i th row and j th column of M . We use R + to denote positive scalars.</p><p>Structural Equation Model (SEM) Following Definition 7.1.1 in Pearl <ref type="bibr" target="#b17">[18]</ref>, let a causal model (or SEM) be defined by a 3-tuple M = ⟨V , U , F⟩, where V = {V 1 , . . . , V n } denotes the set of observed (endogenous) variables and U = {U 1 , . . . , U n } denotes the set of unobserved (exogenous) variables that represent noise, anomalies or assumptions.</p><p>Next, F denotes a set of n functions {f 1 , . . . , f n }, each describing the causal relationships between the random variables having the form:</p><formula xml:id="formula_0">v i = f i (pa(V i ), u i ),</formula><p>where U i ⊆ U and pa(V i ) ⊆ V are such that the associated causal graph (defined next) is acyclic. A causal graph G M<ref type="foot" target="#foot_0">foot_0</ref> is a directed acyclic graph (DAG), where the nodes are the variables V and the edges U with edges pointing from pa(V i ) to V i for all i ∈ [n].</p><p>Linear-SEM (with causal sufficiency) In this work, we study a special class of such causal models (Gaussian Linear-SEMs) where the function class of each f i is restricted to be linear and of the form</p><formula xml:id="formula_1">v i = f i (pa(V i ), u i ) = vj ∈pa(Vi) α ij v j + u i ,</formula><p>where α ij ̸ = 0, ∀V j ∈ pa(V i ). The causal Sufficiency assumption states that U i = {U i }, i.e., U i is the only exogenous variable that causally affects the endogenous variable V i . This is equivalent to the absence of any latent confounder (Chapter 9 in <ref type="bibr" target="#b19">[20]</ref>). In our work, we consider causally-sufficient Linear-SEMs; with a slight abuse of nomeclature, we will call them Linear-SEMs. The functional relationship between the exogenous and endogenous variables is deterministic, and the system's stochasticity comes from a probability distribution over the exogenous noise variables U . Thus, the probability distribution over the exogenous variable P(U ) defines a probability distribution over the endogenous variable P(V ). Without loss of generality, let the nodes {V 1 , . . . , V n } of the underlying causal graph be topologically ordered. Then, we can equivalently write the above set of equations as:</p><formula xml:id="formula_2">v = Av + u =⇒ v = (I -A) -1 u,<label>(1)</label></formula><p>where A, with A ij = α ij ,contains the causal effects between the endogenous variables. Thus, the matrix A, hereafter described as the adjacency matrix, characterizes the causal relationships between the endogenous variables (V ), where A ij ̸ = 0 denotes an edge between the variable V i and V j in G.</p><p>Linear-SEM with additive Gaussian Noise We further specialize the exogenous variable u i (henceforth referred to as noise variable) to be Gaussian with mean µ i and variance σ i , i.e. u i ∼ N (µ i , σ i ). Thus, the joint distribution of the exogenous variables is given by a multivariate Gaussian distribution u ∼ N (µ, D) where [µ] i = µ i and the covariance is given by a diagonal matrix D with [D] ii = σ i . Thus, the endogenous variables also follow a multivariate Gaussian distribution with P(v) = N (m, S), where m ≜ Bµ i S ≜ BDB T and B ≜ (I -A) -1 . Causal discovery aims to identify the unknown adjacency matrix A given observational or other auxiliary data.</p><p>Interventions. Following Definition 7.1.2 from Pearl <ref type="bibr" target="#b17">[18]</ref>, the new causal model describing the interventional distribution, where the variables in a set I are set to a particular value, is given by M I = ⟨U , V, F I ⟩, where</p><formula xml:id="formula_3">F I = {f j : V j / ∈ I} ∪ {f ′ i : V i ∈ I}</formula><p>and the functional relationship of every node V i ∈ I with their parents and corresponding exogenous variable U i is changed from f i to f ′ i . In particular, the functional relationship of node V i is changed to</p><formula xml:id="formula_4">v i = vj ∈pa(Vi) α ij ′ v j + u i ′<label>(2)</label></formula><p>where</p><formula xml:id="formula_5">u i ′ ∼ N (µ i ′ , σ i ′ )</formula><p>. Such interventions are broadly referred to as "soft". Several other kinds of interventions are also defined in the literature, e.g., do, uncertain, soft etc. <ref type="bibr" target="#b2">[3]</ref>. We consider three different types of widely studied specializations of soft interventions in our work:</p><p>(1) shift: the mean of the noise distribution is shifted by a particular value, i.e., µ i ′ = µ i + κ for some κ ∈ R, and everything else remains the same, i.e., σ</p><formula xml:id="formula_6">′ i = σ i and a ij ′ = a ij , ∀j ∈ [n].<label>(2</label></formula><p>) stochastic do (henceforth referred as stochastic): where all the incoming edges from parents are broken, i.e., α ij ′ = 0, and</p><formula xml:id="formula_7">u i ′ ∼ N (µ i ′ , σ i ′ ).</formula><p>(3) do: in addition to breaking all incoming edges, i.e., α ij ′ = 0, we also set the variance of the noise distribution to 0 and the mean to any value of choice, i.e., u i ∼ N (µ i ′ , 0).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Atomic Interventions</head><p>In this work, we consider soft interventions where only one node is intervened at a time, i.e., |I| = 1. Thus after a soft intervention on node V i , the adjacency matrix is modified such that</p><formula xml:id="formula_8">A i ≜ A -e i (a i -a ′ i ) T = A -e i c T i 2 , where c T i ≜ (a i -a ′ i ) T , a T i is the i th row of matrix A and a ′ T</formula><p>i is the new row after intervention such that</p><formula xml:id="formula_9">[a i ] k = 0, ∀k ≥ i</formula><p>, and e i is the unit vector with entry 1 at the i th position and 0 otherwise. Thus, the linear SEM from Eq. 1 is:</p><formula xml:id="formula_10">v i = (I -A i ) -1 u i = (I -A + e i c T i ) -1 u i ,<label>(3)</label></formula><p>where</p><formula xml:id="formula_11">u i ∼ N (µ i , D i ), µ i = µ + γ i e i for some γ i ∈ R, D i = D -δ i e i e T i where δ i ≜ (σ i -σ ′ i</formula><p>), is a diagonal matrix with the i th diagonal entry as σ ′ i and rest is same as D. Thus, the interventional distribution of the endogenous variables is also a multivariate Gaussian distribution, i.e.,</p><formula xml:id="formula_12">P i (V ) = N (m i , S i ) where m i ≜ B i µ i , S i ≜ B i D i B T i and B i ≜ (I -A + e i c T i ) -1 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Problem Formulation and Main Results</head><p>In §4.1, we begin by formulating the problem of learning the mixture of interventions and then state our main result on the identifiability of parameters of the mixture. As a consequence of our identifiability result, under an interventional faithfulness assumption (Squires et al. <ref type="bibr" target="#b25">[26]</ref>), we show in §4.2 that the underlying true causal graph can be identified up to its I-MEC using a mixture of unknown interventions, thereby obtaining the same identifiability results as in the unmixed setting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Learning a Mixture of Interventions</head><p>We begin by formally defining the mixture of interventions over Linear-SEM with additive Gaussian noise and then state the main result of our paper -a mixture of interventions can be uniquely identified under a mild assumption discussed below.</p><p>Definition 4.1 (Mixture of Soft Atomic Interventions). Let M = ⟨V , U , F⟩ be an unknown Gaussian Linear SEM where the distribution of the endogenous variables is given by P(V ) (see §3). Let I = {i 1 , . . . , i K }, hereafter referred to as intervention target set, be a set of unknown soft atomic interventions where each i k generates a new interventional distribution P i (V ). Then, the mixture of soft atomic intervention is defined as:</p><formula xml:id="formula_13">P mix (V ) = i k ∈I π i k P i k (V )<label>(4)</label></formula><p>where π i k ∈ R + , henceforth referred to as mixing weight, is a positive scalar such that</p><formula xml:id="formula_14">i k ∈I π i k = 1, i k ∈ [n] ∪ {0}</formula><p>where n = |V | is the number of endogenous variables. We also allow i k = 0, which denotes the setting when none of the nodes is intervened, i.e., P 0 (V ) ≜ P(V ). Using Eq. 1 and 3, a mixture defined over a linear SEM with additive Gaussian noise is a mixture of Gaussians with parameters</p><formula xml:id="formula_15">θ = {(m i k , S i k , π i k )} i k ∈I where P i k (V ) = N (m i k , S i k ).</formula><p>Having defined a mixture of interventions, we then aim to answer the following questions: (1) Does there exist an algorithm that can uniquely identify the parameters (θ) of the mixture of interventions under an infinite sample limit?</p><p>(2) What is the run time and sample complexity of such an algorithm? It is immediate that if the intervention doesn't change the causal mechanism in any way, then the interventional distribution is equal to the observational distribution, and we would not be able to distinguish between them. This discussion suggests that it is necessary to put an additional constraint on the interventions performed. Below, we formally state the assumption that will ensure this and that it is sufficient for the identifiability of mixture distribution.</p><p>Assumption 4.1 (Effective Intervention). Let P i (V ) = N (m i , S i ) be an interventional distribution after intervening on node v i , where</p><formula xml:id="formula_16">m i = B i µ i = B i (µ + γ k e i ), B i = (I -A + e i c T i ) and c i = (a i -a ′ i ), S k = B i D i B T i and D i = D -δ i e i e T</formula><p>i (see atomic intervention paragraph in §3). Then, at least one of the following holds:</p><formula xml:id="formula_17">γ i ̸ = 0 or ∥c i ∥ ̸ = 0 or δ i ̸ = 0.</formula><p>Now, we are ready to state the main result of our work that will help us answer the above questions. For an exact expression of sample complexity and runtime, see Lemma 5.1.  </p><formula xml:id="formula_18">i k ∈I ∥m i k -mρ(i k ) ∥ 2 + ∥S i k -Ŝρ(i k ) ∥ 2 + |π i k -πρ(i k ) | 2 ≤ ϵ 2</formula><p>for some permutation ρ : {1, 2, . . . , |I|} → {1, 2, . . . , |I|} and arbitrarily small ϵ &gt; 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Causal Discovery with Mixture of Interventions</head><p>Theorem 4.1 helps us separate the mixture of interventions P mix (V ) and provides us with the parameters {(m i , S i )} i∈I of the distribution of all the components in the mixture. However, it does not reveal which nodes were intervened, corresponding to the different components recovered from the mixture. There has been recent progress in performing causal discovery with a disentangled set of unknown interventional distributions <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b15">16]</ref>. Specifically, Squires et al. <ref type="bibr" target="#b25">[26]</ref> proposes an algorithm (UT-IGSP) that greedily searches over the space of permutations to determine the I-MEC and the unknown intervention target of each component. UT-IGSP is consistent, i.e., it will output the correct I-MEC as the sample size goes to infinity. Thus, combining Theorem 4.1 with the UT-IGSP algorithm implies that, as sample size goes to infinity, we can recover the underlying causal graph up to its I-MEC given a mixture of interventions over a Linear-SEM with additive Gaussian noise:</p><p>Corollary 4.1.1 (Mixture-MEC). Given samples from a mixture of interventions P mix (V ) over a linear-SEM with additive Gaussian noise and samples from the observational distribution P(V ), there exists a consistent algorithm that will identify the I-MEC of the underlying causal graph under the I-faithfulness assumption (defined in Squires et al. <ref type="bibr" target="#b25">[26]</ref>) (and restated in §A.1).</p><p>Proof. The proof follows from the identifiability of the parameters of the mixture distribution (Theorem 4.1) and the consistency of UT-IGSP given by Squires et al. <ref type="bibr" target="#b25">[26]</ref>.</p><p>Remark. The I-faithfulness assumption imposes certain restrictions on both observational and interventional distributions. However, as noted by Squires et al. <ref type="bibr" target="#b25">[26]</ref>, in the case of Linear Gaussian distributions, the set of distributions excluded by this assumption is of measure zero. This is because the Linear Gaussian distributions, defined by a matrix A, that do not meet this assumption are subject to multiple polynomial constraints of the form p(A) = 0.</p><p>It is a well-known result that for a random matrix A, the set of matrices that satisfy such polynomial equalities has measure zero <ref type="bibr" target="#b16">[17]</ref>.</p><p>5 Proof Sketch of Theorem 4.1</p><p>Here we provide an overview of the proof of Theorem 4.1. Definition 4.1 tells us that the mixture of interventions defined over a Linear-SEM with additive Gaussian noise is a mixture of Gaussians. Learning mixtures of Gaussian is well-studied in the literature <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b6">7]</ref>. Since most of these approaches require some form of separability between the distribution of components or the parameters of the distributions, in the following lemma we will first show that the covariance matrix and the mean of any interventional or observational distribution taken pairwise is well-separated when Assumption 4.1 holds. In particular, this seperation of parameters will ensure that the Gaussian mixture can be uniquely identified using the results from Belkin and Sinha <ref type="bibr" target="#b1">[2]</ref>.</p><p>Lemma 5.1. [Parameter Separation] Let P 0 (V ) denote the observational distribution of a linear SEM with additive Gaussian noise "M" (see §3) with "n" endogenous variables. For some i, j ∈ [n] ∪ {0}, let P i (V ) = N (m i , S i ) and P j (V ) = N (m j , S j ) be two interventional distributions (observational if one of "i" or "j" =0). Then the separation between covariance S i and S j and mean m i and m j is lower bounded by:</p><formula xml:id="formula_19">∥S i -S j ∥ 2 F + ∥m i -m j ∥ 2 F ≥ f (B, D) ∥c i ∥ 2 + ∥c j ∥ 2 + h(B, D, µ) γ 2 i + γ 2 j + g(B) |δ i | min |δ i |, λ min (D) + |δ j | min |δ j |, λ min (D) ,</formula><p>where after intervention on node k ∈ {i, j}, ∥c k ∥ is the norm of the perturbation (or change) in the k th row of the adjacency matrix, γ k is the perturbation in the mean and |δ k | is the perturbation in the variance of the noise distribution of node k (as defined in the Atomic Intervention paragraph of §3). Also, B = (I -A) -1 , and D and µ are the covariance matrix and mean of the noise distribution in M, respectively. Furthermore, "f ", "g", and "h" are positive valued polynomial functions of B, µ and smallest eigenvalue of D (see the proof in §A.2 for the exact expressions).</p><p>Remark. If one of the distributions is observational, i.e., say i = 0, then the above bound holds with ∥c i ∥ 2 = 0, γ i = 0, and</p><formula xml:id="formula_20">|δ i | = 0.</formula><p>Remark. Different types of interventions will allow us to change certain parameters in the above bound. Let the exogenous noise variables u i ∼ N (µ i , σ i ) when not intervened. Now, if we intervene on node v i , then the new noise variable has distribution</p><formula xml:id="formula_21">u ′ i ∼ N (µ i + γ i , σ ′ i )</formula><p>given as follows for different intervention types: (1) do intervention:</p><formula xml:id="formula_22">∥c i ∥ ̸ = 0 if v i is not a root node, |δ i | ̸ = 0 if σ i ̸ = 0, and γ i ̸ = 0 if the value of node v i is set to any value other than µ i . (2) stochastic intervention: ∥c i ∥ ̸ = 0 if v i is not a root node. (3) shift intervention: γ i ̸ = 0. (<label>4</label></formula><p>) soft intervention is the most general case, and nothing is guaranteed to be non-zero.</p><p>Next, we restate a definition from Belkin and Sinha <ref type="bibr" target="#b1">[2]</ref> that defines the radius of identifiability (R(θ)) of a probability distribution. If R(θ) &gt; 0, this implies that we can uniquely identify the distribution. Definition 5.1. Let P θ , θ ∈ Θ, be a family of probability distributions. For each θ we define the radius of identifiabiity R(θ) as the supremum of the following set:</p><formula xml:id="formula_23">{r &gt; 0 | ∀θ 1 ̸ = θ 2 , (∥θ 1 -θ∥ &lt; r, ∥θ 2 -θ∥ &lt; r) =⇒ (P θ1 ̸ = P θ2 )}.</formula><p>In other words, R(θ) is the largest number, such that the open ball of radius R(θ) around θ intersected with Θ is an identifiable (sub) family of the probability distribution. If no such ball exists, R(θ) = 0.</p><p>Next, we restate a result from <ref type="bibr" target="#b1">[2]</ref> adapted to our setting, which shows that there exists an efficient algorithm for disentangling a mixture of Gaussians as long as the parameters are separated, which will ensure that the radius of identifiability R(θ) &gt; 0.</p><p>Theorem 5.2 (Theorem 3.1 in Belkin and Sinha <ref type="bibr" target="#b1">[2]</ref>). Let P mix (V ) be a mixture of Gaussians with parameters θ = {(m 1 , S 1 , π 1 ), . . . , (m |I| , S |I| , π |I| )} ∈ Θ where Θ is the set of parameters within a ball of radius Q. Then, there exists an algorithm which given ϵ &gt; 0 and 0 &lt; δ &lt; 1 and poly n, max</p><formula xml:id="formula_24">( 1 ϵ , 1 R(Θ) ), 1 δ , Q samples from P mix (V )</formula><p>, with probability greater than (1 -δ), outputs a parameter vector θ = θ = ( m1 , Ŝ1 , π1 ), . . . , ( m|I| , Ŝ|I| , π|I| ) ∈ Θ such that there exists a permutation ρ : {1, . . . , |I|} → {1, . . . , |I|} satisfying:</p><formula xml:id="formula_25">i k ∈I ∥m i k -mρ(i k ) ∥ 2 + ∥S i k -Ŝρ(i k ) ∥ 2 + |π i k -πρ(i k ) | 2 ≤ ϵ 2 ,</formula><p>where the radius of identifiability R(θ) is lower bounded by:</p><formula xml:id="formula_26">R(θ) 2 ≥ min 1 4 min i̸ =j (∥m i -m j ∥ 2 + ∥S i -S j ∥ 2 ), min i π i .</formula><p>Our Theorem 4.1 along with Assumption 4.1 states that for every pair i, j</p><formula xml:id="formula_27">∈ ([n]∪{0}) ⊗2 we have ∥S i -S j ∥ 2 F +∥m i - m j ∥ 2 F &gt; 0.</formula><p>Also, by construction of the mixture of interventions (in Definition 4.1), we have π i &gt; 0, ∀i ∈ [n] ∪ {0}. This implies that the radius of convergence R(θ) &gt; 0 and thus the parameters of the mixture of interventions P(V ) can be identified uniquely given samples from the mixture distribution with sample size inversely proportional to R(θ).</p><p>6 Empirical Results</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Experiment on Simulated Datasets</head><p>Proposition 4.1.1 establishes that given samples from the mixture distribution, one can identify the underlying causal graph up to its I-MEC. To learn the causal graph, we first disentangle the mixture. Theorem 4.1 and 5.2 show that the sample complexity of our mixture disentangling algorithm is inversely proportional to various parameters of the underlying system and the intervention parameters (γ, |δ| and ∥c∥). Our simulation study further validates our theoretical results and characterizes the end-to-end performance of identifying the causal graph with such mixture data and its dependence on the above-mentioned parameters.</p><p>Simulation Setup We consider data generated from a Linear-SEM with additive Gaussian noise, x = (I -A) -1 η (see §3), with n endogenous variables and corresponding exogenous (noise) variables. Here η ∼ N (0, D), where the noise covariance matrix is diagonal with entries D = diag(σ 2 , . . . , σ 2 ) and σ = 1 unless otherwise specified. A is the (lower-triangular) weighted adjacency matrix whose weights are sampled in the range [-1, -0.5] ∪ [0.5, 1] bounded away from 0. Let G * denote the causal graph corresponding to this linear SEM with edge i → j ⇔ A ji &gt; 0. By Algorithm 1: Mixture-UTIGSP input :mixed dataset (D mix ), observational data (D obs ), number of nodes (n), cutoff ratio (τ ) output :Mixture Distribution Parameters (θ), Intervention Targets (I), causal graph ( Ĝ)</p><formula xml:id="formula_28">1. Esstimate θ k ≜ (μ 1 , Ŝ1 ), . . . (μ k , Ŝk ) = GaussianMixtureModel D mix ,k for each possible number of component in the mixture i.e k ∈ [n + 1]. Define Θ ≜ {θ 1 , .</formula><p>. . , θ n+1 } be the set of estimated parameters and L = {l 1 , . . . , l n+1 } be the log-likelihood of the mixture data corresponding to the models with a different number of components. 2. To estimate the number of components in the mixture (k * ) iterate over k = (n + 1) to 2:</p><p>(a) stop where the relative change in the likelihood increases above a cutoff ratio i.</p><formula xml:id="formula_29">e |l k -l k-1 | l k &gt; τ (b) k * = k if the stopping criteria is met otherwise k * = 1.</formula><p>3. I, Ĝ = UT-IGSP D obs , θ k * <ref type="bibr" target="#b25">[26]</ref> return Θ, I, Ĝ sampling from the resulting multivariate Gaussian distribution, we obtain observational data. Next, for each causal graph, we generate separate interventional data by intervening on a given set of nodes in the graph one at a time (atomic interventions), which is again a Gaussian distribution but with different parameters (see Atomic Intervention paragraph in §3). We experiment with two settings:</p><p>(1) all: where we perform an atomic intervention on all nodes in the graph.</p><p>(2) half : where we perform an atomic intervention on a randomly selected half of the nodes. Then, the mixed data is generated by pooling all the individual atomic interventions and observational data with equal proportions into a single dataset. The decision to use equal proportions of samples from all components is solely intended to simplify the design choices for the experiment setup. In our experiment, we vary the total number of samples in the mixed dataset as N ∈ {2 10 , 2 11 , . . . , 2 17 }. In particular, we perform two kinds of atomic interventions: "do" and "stochastic" (see Interventions paragraph in §3 for a formal definition). The initial noise distribution for all the nodes is univariate Gaussian distribution N (0, 1). In our experiments for do interventions, instead of setting the final variance of noise distribution to 0, we set it to a very small value of 10 -9 for numerical stability. Unless otherwise specified, we perform 10 runs for each experimental setting and plot the 0.05 and 0.95 quantiles. See B for additional details on the experimental setup.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method Description and Evaluation Metrics</head><p>Given the mixed data generated from the underlying true causal graph G * , the goal is to estimate the underlying causal graph Ĝ. We break down the task into two steps. First, we disentangle the mixture data and identify the parameters of the individual interventional and/or observational distributions. Our theoretical result (Theorem 4.1) uses <ref type="bibr" target="#b1">[2]</ref> for identifiability of the parameters of a mixture of Gaussians. Since they only show the existence of such an algorithm, we use the standard sklearn python package <ref type="bibr" target="#b18">[19]</ref> that implements an EM algorithm to estimate the parameters of the mixture. Importantly, our experiment doesn't require prior knowledge about the number of components (k) in the mixture. We train separate mixture models varying the number of components. Then we select the optimal number of components using the log-likelihood curve of the fitted Gaussian mixture model using a simple thresholding heuristic (see step 2 in Alg. 1). We leave the exploration of better model selection criteria for future work. For all our simulation experiments, unless otherwise specified, we use a cutoff threshold of 0.07, chosen arbitrarily. In §B, we experiment with different values of this threshold and show that Mixture-UTIGSP is robust to this choice. The intervention targets present in the mixture are still unknown at this step. Next, we provide the estimated mixture parameters to an existing causal discovery algorithm with unknown intervention targets (UT-IGSP <ref type="bibr" target="#b25">[26]</ref>), which internally estimates the unknown intervention targets and outputs one of the possible graphs from the estimated I-MEC. We assume that observational data is given as an input to the UT-IGSP algorithm. The proposed algorithm is provided in Alg. 1. See B for our hyperparameter choice and other experimental details.</p><p>Evaluation Metrics We evaluate the performance of Mixture-UTIGSP (Alg. 1) on three metrics:</p><p>Parameter Estimation Error: This metric measures the least absolute error between the estimated parameters (mean and covariance matrix defining each individual distribution) after the first step of Mixture-UTIGSP matched with the ground truth parameters considering all possible matchings between the components averaged over all runs. See §B.5 for details.</p><p>Average Jaccard Similarity (JS): We measure the average Jaccard similarity between the estimated intervention target and the corresponding ground truth (atomic) intervention target. We use the matching between the estimated and ground truth components found while calculating the parameter estimation error averaged over all runs. See §B.5 for details.   represents the opposite (see Evaluation metric paragraph in §6). In summary, performance improves for both cases as the number of samples increases. However, the graph with more nodes requires a larger sample to perform similarly. For a detailed discussion, see §6.1.</p><p>Average Structural Hamming Distance (SHD): Given the estimated and ground truth graphs, we compute the SHD between the two graphs averaged over all runs.</p><p>Results Fig. <ref type="figure" target="#fig_2">1</ref> shows the performance of Alg. 1 in the two settings, "all" in the first row and "half" in the second row (see Simulation setup above). The first column shows the performance of the first step of Alg. 1 where the mixture parameters are identified. We observe that parameter estimation error decreases as the number of samples increases in both settings. As expected, larger graphs require a larger sample size to perform similarly to smaller-sized graphs within each setting.</p><p>Step 1 of our Alg. 1 only recovers the parameters ({(m i , S i )} k i=1 ) of the components present in the mixture distribution. In step 2 of our Alg. 1, we call UT-IGSP <ref type="bibr" target="#b25">[26]</ref> that identifies the individual intervention targets from the estimated distribution parameters and also returns a causal graph from the estimated I-MEC. Fig. <ref type="figure" target="#fig_2">1b</ref> and<ref type="figure" target="#fig_2">1e</ref> show the average Jaccard Similarity between the ground truth and the estimated intervention targets. The colored lines denote experiments on graphs with different numbers of nodes. The corresponding dotted lines show the oracle performance of UT-IGSP when the separated ground truth mixture distributions were given as input. As expected, in both the settings (Fig. <ref type="figure" target="#fig_2">1b</ref> and<ref type="figure" target="#fig_2">1e</ref>), the oracle version performs much better compared to its non-oracle counterpart for small sample sizes (2 10 to 2 14 ) but performs similarly as sample size increases.</p><p>Finally, in the third column (Fig. <ref type="figure" target="#fig_2">1c</ref> and<ref type="figure" target="#fig_2">1f</ref>), we calculate the SHD between the estimated causal graph Ĝ and the ground truth causal graph G * . The SHD of the graph estimated by Mixture-UTIGSP and the oracle version are similar for different node settings and all sample sizes. This suggests that small errors in the estimation of the parameters of the mixture distribution don't affect the estimation of the underlying causal graph.  <ref type="bibr" target="#b21">[22]</ref>: We evaluate the performance of Mixture-UTIGSP as we vary the cutoff ratio to select the number of component in the mixture. The second column shows the number of estimated components where the actual number of components in the mixture is 6. The third and fourth columns show the Jaccard Similarity of the identified intervention target of Mixture-UTIGSP and oracle versions of the UT-IGSP algorithm. The fourth and last column shows the SHD between the estimated and true causal graphs for both methods respectively. Overall we observe that at a lower cutoff threshold Mixture-UTIGSP is able to perform as well as the oracle UT-IGSP algorithm on all the metrics. See §B.2 for detailed discussion. Additional Experiments: In §B, we provide details on the experimental setup and additional results. In Fig. <ref type="figure">2</ref>, we plot two additional metrics for the simulation experiments. The first metric is the number of estimated components in the mixture and the second metric is the the error in estimation of the mixing coefficient. In Fig. <ref type="figure" target="#fig_3">3</ref>, we study the sensitivity of the cutoff ratio used by Mixture-UTIGSP to select the number of components in the mixture. Next, in Fig. <ref type="figure" target="#fig_4">4</ref>, we evaluate the performance of our Alg. 1 as we vary the density, i.e., the expected number of edges in the graph. In Fig. <ref type="figure">6</ref>, we show how the sparsity of the graph and other intervention parameters like the value of the new mean and variance of the noise distribution after intervention affects the performance of Alg. 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Experiment on Biological Dataset</head><p>We evaluate our method on the Protein Signaling dataset <ref type="bibr" target="#b21">[22]</ref> to demonstrate real-world applicability. The dataset is collected from flow cytometry measurement of 11 phosphorylated proteins and phospholipids and is widely used in causal discovery literature <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b25">26]</ref>. The dataset consists of 5846 measurements with different experimental conditions and perturbations. Following Wang et al. <ref type="bibr" target="#b30">[31]</ref>, we define the subset of the dataset as observational, where only the receptor enzymes were perturbed in the experiment. Next, we select other 5 subsets of the dataset where a signaling protein is also perturbed in addition to the receptor enzyme. The observational dataset consists of 1755 samples, and the 5 interventional datasets have 911, 723, 810, 799, and 848 samples, respectively. The mixed dataset is created by merging all the observational and interventional datasets.</p><p>The total number of nodes in the underlying causal graph is 11. Thus, the maximum number of possible components in the mixture is 12 (11 single-node interventional distribution and one observational). In the mixture dataset described above, we have 6 components (1 observational and 5 interventional). The second column in Table <ref type="table" target="#tab_2">1</ref> shows that Mixture-UTIGSP recovers 4 components close to the ground truth 6 when the cutoff ratio is 0.01 (step 2 of Alg. 1). Next, we give the disentangled dataset from the first step of our algorithm to identify the unknown target. Though the Jaccard similarity of the recovered target is not very high (average of 0.03 shown in the last row of the third column, where the maximum value is 1.0), it is similar to that of the oracle performance of UT-IGSP when the disentangled ground truth mixture distributions were given as input. This shows that it is difficult to identify the correct intervention targets even with correctly disentangled data. Also, the SHD between the recovered graph and the widely accepted ground truth graph for Mixture-UTIGSP (ours) and UT-IGSP (oracle) is very close. Overall, at a lower cutoff ratio, the performance of Mixture-UTIGSP is close to the Oracle UT-IGSP algorithm. Unlike the simulation case (see Fig. <ref type="figure" target="#fig_3">3</ref>), Mixture-UTIGSP's performance is sensitive to the choice of the cutoff ratio on this dataset. In Fig. <ref type="figure" target="#fig_5">5</ref>, we plot the ground truth graph curated by the domain expert alongside the estimated causal graph for visualization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We studied the problem of learning the mixture distribution generated from observational and/or multiple unknown interventional distributions generated from the underlying causal graph. We show that the parameters of the mixture distribution can be uniquely identified under the mild assumption that ensures that any intervention changes the distribution of the observed variables. As a consequence of our identifiability result, under an interventional faithfulness assumption (Squires et al. <ref type="bibr" target="#b25">[26]</ref>), we show that the underlying true causal graph can be identified up to its I-MEC based on a mixture of unknown interventions, thereby obtaining the same identifiability results as in the unmixed setting. Finally, we conduct a simulation study to validate our findings empirically. We demonstrate that as the sample size increases, we obtain parameter estimates of the mixture distribution that are closer to the ground truth and, as a result, we eventually recover the correct underlying causal graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Limitations and Future Work</head><p>Since our work is the first to study the problem of a mixture of causal interventions without assuming knowledge of causal graphs, we have restricted our attention to one particular family of causal models-Linear-SEM with additive Gaussian noise. In the future, it would be interesting to study this problem for a more general family of causal models. Further, our work uses Belkin and Sinha <ref type="bibr" target="#b1">[2]</ref> for identifying the parameters of a mixture of Gaussians, which assumes that the number of components in the mixture is fixed. Recent progress in <ref type="bibr" target="#b6">[7]</ref> gives an efficient algorithm for recovering the parameters when the number of components is almost √ n, where n is the number of variables. However, they only work in perturbative settings, and proving the result for non-perturbative settings is out of the scope of the current paper. Finally, to identify the parameters of the mixture distribution in our empirical study, we use heuristics to estimate the number of components. It would be interesting to explore other methods to automatically select the number of components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Missing Proofs</head><p>A.1 I-faithfulness in UT-IGSP algorithm.</p><p>Here, we restate the assumption from Squires et al. <ref type="bibr" target="#b25">[26]</ref> that is needed for consistency of the UT-IGSP algorithm: Assumption A.1 (I-faithfulness assumption). Let I be a list of intervention targets. The set of distributions {f obs } ∪ {f I } I I is I-faithful with respect to a DAG G if f obs is faithful with respect to G and for any I k ∈ I and disjoint A, C ⊆ [p], we have that</p><formula xml:id="formula_31">(A ⊥ ⊥ ζ k |C ∪ ζ I\{k} ) G I if and only if f k (x A |x C ) = f obs (x A |x C ).</formula><p>A.2 Proof of Lemma 5.1 Lemma 5.1. [Parameter Separation] Let P 0 (V ) denote the observational distribution of a linear SEM with additive Gaussian noise "M" (see §3) with "n" endogenous variables. For some i, j ∈ [n] ∪ {0}, let P i (V ) = N (m i , S i ) and P j (V ) = N (m j , S j ) be two interventional distributions (observational if one of "i" or "j" =0). Then the separation between covariance S i and S j and mean m i and m j is lower bounded by:</p><formula xml:id="formula_32">∥S i -S j ∥ 2 F + ∥m i -m j ∥ 2 F ≥ f (B, D) ∥c i ∥ 2 + ∥c j ∥ 2 + h(B, D, µ) γ 2 i + γ 2 j + g(B) |δ i | min |δ i |, λ min (D) + |δ j | min |δ j |, λ min (D) ,</formula><p>where after intervention on node k ∈ {i, j}, ∥c k ∥ is the norm of the perturbation (or change) in the k th row of the adjacency matrix, γ k is the perturbation in the mean and |δ k | is the perturbation in the variance of the noise distribution of node k (as defined in the Atomic Intervention paragraph of §3). Also, B = (I -A) -1 , and D and µ are the covariance matrix and mean of the noise distribution in M, respectively. Furthermore, "f ", "g", and "h" are positive valued polynomial functions of B, µ and smallest eigenvalue of D (see the proof in §A.2 for the exact expressions).</p><p>Proof. First, we state a lemma that will give us a lower bound on the separation between the covariance matrix of two intervention distributions (or one could be observational). The proof is given in §A.3.</p><p>Lemma A.1 (Minimum Covariance Separation). Let S i and S j be the covariance matrix of the Gaussian distribution corresponding to intervention on node V i and V j in the causal graph. Let, without loss of generality, V i be topologically greater than V j in the causal graph. Let B = (I -A) -1 , D = diag(σ 1 , . . . , σ n ) be the covariance matrix of the noise distribution, let S = BDB T be the covariance matrix of the observed distribution P (V ), and let c i = a i -a ′ i be the soft intervention performed on node V i (see §3 for definitions). Then we have:</p><formula xml:id="formula_33">∥S i -S j ∥ 2 F ≥ f (B, D) ∥c i ∥ 2 + ∥c j ∥ 2 + g(B) |δ i | min |δ i |, λ min (D) + |δ j | min |δ j |, λ min (D) .</formula><p>If one of the covariance matrices is from the observational distribution, i.e., say S j = S then the above lower bounds still holds with ∥c j ∥ = 0 and δ j = 0.</p><p>Using the above Lemma A.1 and substituting the explicit form of f (B, D) from Eq. 19 we obtain:</p><formula xml:id="formula_34">∥S i -S j ∥ 2 F ≥ f (B, D) ∥c i ∥ 2 + ∥c j ∥ 2 + g(B) |δ i | min |δ i |, λ min (D) + |δ j | min |δ j |, λ min (D) ≜ω ≥ f (B, D) 2 ∥c i ∥ 2 + ∥c j ∥ 2 + ω ≜ζ + λ 2 min (D) ∥c i ∥ 2 + ∥c j ∥ 2 8∥B -1 ∥ 4 F .<label>(5)</label></formula><p>Next, we state another lemma that will give us a lower bound on the separation of the mean of two interventional distributions (or one of them could be observational). The proof of the lemma below is given in §A.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma A.2 (Minimum Mean Separation).</head><p>Let m i and m j denote the mean of the Gaussian distribution corresponding to the intervention on node V i and V j in the causal graph. Then we have:</p><formula xml:id="formula_35">∥m i -m j ∥ 2 F ≥                γ 2 i 4∥B -1 ∥ 2 F + γ 2 j 4∥B -1 ∥ 2 F , ψ + i , ψ + j are active γ 2 i 4∥B -1 ∥ 2 F , ψ + i , ψ - j are active and γ 2 j 4∥Bµ∥ 2 ≤ ∥c j ∥ 2 γ 2 j 4∥B -1 ∥ 2 F , ψ - i , ψ + j are active and</formula><formula xml:id="formula_36">γ 2 i 4∥Bµ∥ 2 ≤ ∥c i ∥ 2 0,</formula><p>ψ - i , ψ - j are active and</p><formula xml:id="formula_37">γ 2 i 4∥Bµ∥ 2 ≤ ∥c i ∥ 2 , γ 2 j 4∥Bµ∥ 2 ≤ ∥c j ∥ 2 where ψ + i ≜ |c T i Bµ| &lt; |γi| 2 or |c T i Bµ| &gt; 3|γi| 2 , ψ - i ≜ |γi| 2 ≤ |c T i Bµ| ≤ 3|γi|</formula><p>2 and similarly for ψ + j and ψ - j . If one of the means say m j is from the observational distribution, i.e., m j = m = Bµ, then setting γ j = 0 above will give the appropriate bounds and only case 2 and 4 are applicable. Now, From Case 1 of the above Lemma A.2 (Eq. 42, ψ + i , ψ + j is active) and above equation we have:</p><formula xml:id="formula_38">∥S i -S j ∥ 2 F + ∥m i -m j ∥ 2 F ≥ ζ + λ 2 min (D) ∥c i ∥ 2 + ∥c j ∥ 2 8∥B -1 ∥ 4 F + γ 2 i 4∥B -1 ∥ 2 F + γ 2 j 4∥B -1 ∥ 2 F . (<label>6</label></formula><formula xml:id="formula_39">)</formula><p>From Case 2 of the above Lemma A.2 (Eq. 42, ψ + i , ψ - j is active) and using γ 2 j 4∥Bµ∥ 2 ≤ ∥c j ∥ 2 we have:</p><formula xml:id="formula_40">∥S i -S j ∥ 2 F + ∥m i -m j ∥ 2 F ≥ ζ + λ 2 min (D)∥c i ∥ 2 8∥B -1 ∥ 4 F + γ 2 i 4∥B -1 ∥ 2 F + λ 2 min (D)γ 2 j 32∥B -1 ∥ 4 F ∥Bµ∥ 2 . (<label>7</label></formula><formula xml:id="formula_41">)</formula><p>From Case 3 of the above Lemma A.2 (Eq. 42, ψ - i , ψ + j is active) and using</p><formula xml:id="formula_42">γ 2 i 4∥Bµ∥ 2 ≤ ∥c i ∥ 2</formula><p>we have:</p><formula xml:id="formula_43">∥S i -S j ∥ 2 F + ∥m i -m j ∥ 2 F ≥ ζ + λ 2 min (D)∥c j ∥ 2 8∥B -1 ∥ 4 F + γ 2 j 4∥B -1 ∥ 2 F + λ 2 min (D)γ 2 i 32∥B -1 ∥ 4 F ∥Bµ∥ 2 . (<label>8</label></formula><formula xml:id="formula_44">)</formula><p>Similarly, for Case 4 and using</p><formula xml:id="formula_45">γ 2 i 4∥Bµ∥ 2 ≤ ∥c i ∥ 2 and γ 2 j</formula><p>4∥Bµ∥ 2 ≤ ∥c j ∥ 2 we have:</p><formula xml:id="formula_46">∥S i -S j ∥ 2 F + ∥m i -m j ∥ 2 F ≥ ζ + λ 2 min (D)γ 2 i 32∥B -1 ∥ 4 F ∥Bµ∥ 2 + λ 2 min (D)γ 2 j 32∥B -1 ∥ 4 F ∥Bµ∥ 2 .<label>(9)</label></formula><p>Next, combining Eq. 6, 7, 8, 9 we have:</p><formula xml:id="formula_47">∥S i -S j ∥ 2 F + ∥m i -m j ∥ 2 F ≥ ζ + γ 2 i + γ 2 j max 4∥B -1 ∥ 2 F , 32∥B -1 ∥ 4 F ∥Bµ∥ 2 λ 2 min (D) ≥ f (B, D) 2 ∥c i ∥ 2 + ∥c j ∥ 2 + g(B, D) |δ i | + |δ j | + h(B, D, µ)(γ 2 i + γ 2 j ).<label>(10)</label></formula><p>For the case when one of the distributions is observational, say w.l.o.g. S j = S and m j = m, then the same analysis holds since c j = 0 and δ j = 0 (from Lemma A.1) and only the analyses of case 2 and case 4 are applicable (from Lemma A.2), thereby completing the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Proof of Lemma A.1</head><p>Proof. Without loss of generality, throughout our analysis, we will assume that the endogenous variables V 1 , . . . , V n are topologically ordered based on the underlying causal graph. Thus, the corresponding adjacency matrix A is lower triangular. The value of ∥S i -S j ∥ 2 F will be unaffected by any permutation of the node order in the matrix A as shown next. Let Ã = P AP T be the adjacency matrix when the nodes are permuted by the permutation matrix P where P P T = I. Also, let the corresponding permuted covariance matrix be S = B D BT where B = (I -Ã) -1 = P (I -A) -1 P T = P BP T =⇒ S = P BP T P DP T P B T P T = P BDB T P T = P SP T . Similarly, we have Si = P S i P T and Sj = P S j P T . Now we have:</p><formula xml:id="formula_48">∥ Si -Sj ∥ 2 F = ∥P (S i -S j )P T ∥ 2 F = ∥S i -S j ∥ 2 F ,<label>(11)</label></formula><p>since the permutation matrix only permutes the row and column in the above equation, the Frobenius norm remains the same.</p><p>First, we state a lemma that characterizes the covariance matrix S i of the interventional distribution. The proof of this lemma can be found in §A.4.</p><p>Lemma A.3 (Covariance Matrix Update). Let P i (V ) = N (m i , S i ) be an interventional distribution and let the endogenous nodes V 1 , . . . V n be topologically ordered based on the underlying causal graph, then we have:</p><formula xml:id="formula_49">S i = S -δ i r i r T i , for root node B i DB T i -δ i r i r T i , otherwise where r i = Be i , δ i = σ i -σ ′ i , B i = B -Be i c T i B, B = (I -A) -1</formula><p>, S is the covariance matrix of the observational distribution and D is the covariance matrix of the observational noise distribution (see Atomic Intervention paragraph in §3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Thus from the above Lemma A.3, we have</head><formula xml:id="formula_50">S i = B i DB T i -δ i v i v T i .</formula><p>Substituting the value of B i from the above lemma again we get:</p><formula xml:id="formula_51">S i = BDB T -BDu i v T i -v i u T i DB T + η i v i v T i δ i v i v T i = S -BDB T c i e T i B T -Be i c T i BDB T + η i Be i e T i B T -δ i Be i e T i B T ,<label>(12)</label></formula><p>where</p><formula xml:id="formula_52">u i = B T c i , v i = Be i , η i = u T i Du i = c T i Sc i .</formula><p>Next, multiplying the LHS and RHS of the above equation with B -1 and B -T we get:</p><formula xml:id="formula_53">B -1 (S i -S)B -T = η i e i e T i -DB T c i e T i -e i c T i BD -δ i e i e T i .<label>(13)</label></formula><p>The absolute value of (i, k) th (k ∈ [n]) entry of this matrix is given by:</p><formula xml:id="formula_54">e T i B -1 (S i -S)B -T e k = -D kk c T i Be k , k ̸ = i η i -2D ii$ $ $ $ X 0 c T i Be i -δ i = η i -δ i , k = i<label>(14)</label></formula><p>where in the second case c T i Be i = 0 and D kk = σ k is the k th diagonal entry of the matrix D. Similarly, the (j, k) th entry of this matrix is given by:</p><formula xml:id="formula_55">e T j B -1 (S i -S)B -T e k = 0, k ̸ = i -D jj e T j B T c i k = i<label>(15)</label></formula><p>Similarly, the covariance matrix of node V j and j ̸ = i is given by:</p><formula xml:id="formula_56">S j = S -BDB T c j e T j B T -Be j c T j BDB T + η j Be j e T j B T -δ j Be j e T j B T B -1 (S j -S)B -T = η j e j e T j -DB T c j e T j -e j c T j BD -δ j e j e T j .<label>(16)</label></formula><p>Thus the absolute value of the (i, k) th entry of the above matrix is given by:</p><formula xml:id="formula_57">|e T i B -1 (S j -S)B -T e k | = |D ii e i B T c j (e T j e k )| = 0,<label>(17)</label></formula><p>since we had assumed without loss of generality that V j ≺ V i =⇒ e i B T c j = 0. Similarly, the (j, k) th entry of this matrix is given by:</p><formula xml:id="formula_58">e T j B -1 (S j -S)B -T e k = -D kk c T j Be k , k ̸ = j η j -2D jj e T j B T c j -δ j = η j -δ j k = j<label>(18)</label></formula><p>where in the second case e T j B T c j = 0. Thus, taking the difference of the i th and j th row of both the matrix (from Eq. 14, 15, 17 and 18 and using the fact that c T i Be i = 0, c T j Be j = 0 and c T j Be i = 0 since without loss of generality we have V j ≺ V i ) we obtain the following lower bound:</p><formula xml:id="formula_59">∥B -1 (S i -S j )B -T ∥ 2 F ≥ k (e i B -1 (S i -S j )B -T e k ) 2 + k (e j B -1 (S i -S j )B -T e k ) 2 ≥ n k=1 (D kk c T i Be k ) 2 + (η i -δ i ) 2 + k̸ =i (D kk c T j Be k ) 2 + (D jj e T j B T c i - $ $ $ $ $ X 0 D ii c T j Be i ) 2 + (η j -δ j ) 2 = n k=1 D 2 kk c T i Be k e T k B T c i + (η i -δ i ) 2 + n k=1 D 2 kk c T j Be k e T k B T c j + (η j -δ j ) 2 + (D jj e T j B T c i ) 2 ≥ λ min (D)c T i B n k=1 D kk e k e T k B T c i + (η i -δ i ) 2 + λ min (D)c T j B n k=1 D kk e k e T k B T c j + (η j -δ j ) 2 = λ min (D)c T i BDB T c i + (η i -δ i ) 2 + λ min (D)c T j BDB T c j + (η j -δ j ) 2 = λ min (D)c T i Sc i + (η i -δ i ) 2 + λ min (D)c T j Sc j + (η j -δ j ) 2 = λ min (D)η i + (η i -δ i ) 2 + λ min (D)η j + (η j -δ j ) 2 ≥ λ min (D) (η i + η j ) 4 + |δ i | 4 min |δ i |, λ min (D) + |δ j | 4 min |δ j |, λ min (D)<label>(19)</label></formula><p>after substituting η i = c T i Sc i ≥ λ min (S)∥c i ∥ 2 , η j = c T j Sc j ≥ λ min (S)∥c j ∥ 2 and using Lemma A.4 that gives us a lower bound for the second and third term. Thus, we obtain:</p><formula xml:id="formula_60">∥S i -S j ∥ 2 F ≥ λ min (D) λ min (S)(∥c i ∥ 2 + ∥c j ∥ 2 ) 4∥B -1 ∥ 4 F + |δ i | 4∥B -1 ∥ 4 F min |δ i |, λ min (D) + |δ j | 4∥B -1 ∥ 4 F min |δ j |, λ min (D) ≥ λ 2 min (D) ∥c i ∥ 2 + ∥c j ∥ 2 4∥B -1 ∥ 4 F + |δ i | 4∥B -1 ∥ 4 F min |δ i |, λ min (D) + |δ j | 4∥B -1 ∥ 4 F min |δ j |, λ min (D) ,<label>(20)</label></formula><p>where λ min (S) ≥ λ min (D)λ 2 min (B) = λ min (D) (∵ λ min (ST ) ≥ λ min (S)λ min (T ) and λ min (B) = λ min (B T ) = 1 since it is a lower triangular matrix where all diagonal entries are 1) and ∥ST ∥ F ≤ ∥S∥ F ∥T ∥ F . For the case when one of the covariance matrices is S corresponding to no intervention and the other is S i corresponding to intervention on node V i , then from Eq. 14 and the following similar analysis we have:</p><formula xml:id="formula_61">∥B -1 (S i -S)B -T ∥ 2 F ≥ n k=1 (D kk c T i Be k ) 2 + (η i -δ i ) 2 ≥ λ min (D) η i 4 + |δ i | 4 min |δ i |, λ min (D) ∥S i -S∥ 2 F ≥ λ 2 min (D)∥c i ∥ 2 + |δ i | min |δ i |, λ min (D) 4∥B -1 ∥ 4 F ,<label>(21)</label></formula><p>thereby completing our proof of this lemma.</p><p>Lemma A.4. Given λ ≥ 0 and η ≥ 0, the following inequality holds true:</p><formula xml:id="formula_62">λη + (η -δ) 2 ≥ λη 4 + |δ| 4 min |δ|, λ .<label>(22)</label></formula><p>Proof. Case 1: δ &lt; 0 Let T ≜ λη + (η -δ) 2 , then we have T ≥ λη + δ 2 ≥ λη + δ 2 /4.</p><p>Case 2: δ &gt; 0 and [0 ≤ η &lt; δ/2 or η &gt; 3δ/2] Again in this case T ≥ λη + δ 2 /4.</p><p>Case 3: δ &gt; 0 and δ/2 ≤ η ≤ 3δ/2 In this case, we have T ≥ λη. Also, we have η &gt; δ/2 =⇒ T ≥ λδ/2 which together implies:</p><formula xml:id="formula_63">T ≥ max λη, λ δ 2 ≥ λη 2 + λδ 4 .<label>(23)</label></formula><p>Thus, combining the above three cases we have the following lower bound on the value of T :</p><formula xml:id="formula_64">T ≥ min λη + δ 2 4 , λη 2 + |δ| 4 ≥ λη 4 + min δ 2 4 , λ|δ| 4 = λη 4 + |δ| 4 min |δ|, λ ,<label>(24)</label></formula><p>which completes the proof.</p><p>A.4 Proof of Lemma A.3</p><p>Proof. From Eq. 3, we have</p><formula xml:id="formula_65">S i = B i D i B T i where B i = (I -A+e i c T i ) -1 and D i = D-(σ i -σ ′ i )e i e T i ≜ D-δ i e i e T i . Since e i c T</formula><p>i is a rank-1 update to the (I-A) matrix, using the Sherman-Morrison identity we obtain:</p><formula xml:id="formula_66">B i = (I -A + e i c T i ) -1 = (I -A) -1 - (I -A) -1 e i c T i (I -A) -1 1 + c T i (I -A) -1 e i = B - Be i c T i B 1 + c T i Be i = B -Be i c T i B ≜ B -r i q T i ,<label>(25)</label></formula><p>where r i ≜ Bei di , scalar d i ≜ 1 + c T i Be i = 1 and q i ≜ B T c i . The scalar d i = 1 since q T i e i = c T i Be i = 0 given by the following lemma: Lemma A.5. Let A and B be a lower triangular matrix and c i be a vector such that c i = a i -a ′ i and [c i ] t = 0, ∀t ≥ i (see Atomic Interventions paragraph of §3 for definition), then q T i e i = c T i Be i = 0. Also, if c i ̸ = 0, then q i = B T c i ̸ = 0.</p><p>The proof of the above lemma can be found below after the proof of the current lemma. Next, S i is given by:</p><formula xml:id="formula_67">S i = B i D i B T i = B i (D -δ i e i e T i )B T i = B i DB T i term 1 -δ i B i e i e T i B T i term 2 . (<label>26</label></formula><formula xml:id="formula_68">)</formula><p>If the intervened node V i is one of root nodes of the underlying unknown causal graph, then c i = 0 =⇒ q i = B T c i = 0 =⇒ B i = B (from Eq. 25), and thus:</p><formula xml:id="formula_69">B i DB T i = BDB T = S,<label>(27)</label></formula><p>where S is the covariance matrix of the observational distribution. Otherwise, for non-root nodes, term 1 in the above equation is:</p><formula xml:id="formula_70">B i DB i = (B -r i q T i )D(B -r i q T i ) T = BDB T -BDq i r T i -(BDq i r T i ) T + r i q T i Dq i r T i = S -w i r T i -r i w T i + η i r i r T i (<label>28</label></formula><formula xml:id="formula_71">)</formula><p>where w i = BDq i and scalar η i = q T i Dq i . Next simplifying term 2 in Eq. 26 we get:</p><formula xml:id="formula_72">B i e i (B i e i ) T = Be i -r i q T i e i Be i -r i q T i e i T = Be i e T i B T = r i r T i (<label>29</label></formula><formula xml:id="formula_73">)</formula><p>since q T i e i = c T i Be i = 0 from Lemma A.5 and r i ≜ Be i (from Eq. 25). Thus the covariance matrix of the endogenous variables in the intervened distribution is given by:</p><formula xml:id="formula_74">S i = S -δ i r i r T i , for root node B i DB T i -δ i r i r T i , otherwise<label>(30)</label></formula><p>thereby completing our proof.</p><p>Proof of Lemma A.5. Showing q i ̸ = 0: We are given that B is a lower triangular matrix with non-zero diagonal entries. Also,</p><formula xml:id="formula_75">c i = a i -a ′ i ̸ = 0 and [c i ] t = 0, ∀t ≥ i =⇒ ∃t s.t.</formula><p>[c i ] t ̸ = 0 and let t * be that last index where c i is non-zero. Now lets look at the</p><formula xml:id="formula_76">[B T c i ] t * = c t * • [B T ] t * ,t * ̸ = 0 since B T is a upper triangular matrix and [c i ] t * ̸ = 0.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Showing q T</head><p>i e i = 0: Be i is the i th column of the lower triangular matrix</p><formula xml:id="formula_77">B =⇒ [Be i ] t = 0, ∀t &lt; i =⇒ c T i Be i = 0 since [c i ] t = 0, ∀t ≥ i.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.5 Proof of Lemma A.2</head><p>Proof. Similar to the proof of Lemma A.1, without loss of generality, we will assume that the endogenous variables are topologically ordered based on the underlying causal graph such that the adjacency matrix A is lower triangular. The permutation matrix will only permute the rows of the mean vectors m i and m j ; hence, there will be no effect on the value of ∥m i -m j ∥ 2 F . Now, let the mean of the Gaussian distribution corresponding to intervention on node V i be given by (see Atomic Intervention paragraph in §3 for definition):</p><formula xml:id="formula_78">m i = B i µ i = (B -Be i c T i B)(µ + γ i e i ) =⇒ B -1 m i = (I -e i c T i B)(µ + γ i e i ),<label>(31)</label></formula><p>where</p><formula xml:id="formula_79">B i = (I -A i ) -1 = B -Be i c T i B (from Lemma A.3</formula><p>), µ i is the new mean vector for the noise distribution, µ is the mean vector of the observational noise distribution and γ i e i is the update to the mean of the noise distribution when intervening on node V i . Similarly, the mean corresponding to the intervened distribution on node V j is given by:</p><formula xml:id="formula_80">m j = B j µ j = (B -Be j c T j B)(µ + γ j e j ) =⇒ B -1 m j = (I -e j c T j B)(µ + γ j e j ).<label>(32)</label></formula><p>Looking at the i th entry of the vector B -1 m i , we get:</p><formula xml:id="formula_81">e T i B -1 m i = e T i µ -c T i Bµ + γ i - ¨¨B 0 c T i Be i ,<label>(33)</label></formula><p>and the i th entry of the vector B -1 m j is given by:</p><formula xml:id="formula_82">e T i B -1 m j = e T i µ.<label>(34)</label></formula><p>Similarly, the j th entry of the vector B -1 m i is given by:</p><formula xml:id="formula_83">e T j B -1 m i = e T j µ,<label>(35)</label></formula><p>and the i th entry of the vector B -1 m j is given by: e</p><formula xml:id="formula_84">T j B -1 m j = e T j µ -c T j Bµ + γ j - ¨¨B 0 c T j Be j .<label>(36)</label></formula><p>Thus, the difference in the mean of both distributions can be lower bounded by:</p><formula xml:id="formula_85">∥B -1 (m i -m j )∥ 2 F ≥ (γ i -c T i Bµ) 2 + (γ j -c T j Bµ) 2 . (<label>37</label></formula><formula xml:id="formula_86">)</formula><p>Now based on the value of |γ| i and |c T i Bµ|, we have:</p><formula xml:id="formula_87">(γ i -c T i Bµ) 2 ≥ γ 2 i 4 , ψ + i ≜ |c T i Bµ| &lt; |γi| 2 or |c T i Bµ| &gt; 3|γi| 2 0, ψ - i ≜ |γi| 2 ≤ |c T i Bµ| ≤ 3|γi| 2 ,<label>(38)</label></formula><p>where ψ + i and ψ - i are used to denote two different mutually exclusive different events for ease of exposition later. In the case when ψ - i is true, we have:</p><formula xml:id="formula_88">γ 2 i 4 ≤ (c T i Bµ) 2 ≤ ∥c i ∥ 2 ∥Bµ∥ 2 (Cauchy-Schwarz) γ 2 i 4∥Bµ∥ 2 ≤ ∥c i ∥ 2 .<label>(39)</label></formula><p>Similarly, we have:</p><formula xml:id="formula_89">(γ j -c T j Bµ) 2 ≥ γ 2 j 4 , ψ + j ≜ |c T j Bµ| &lt; |γj | 2 or |c T j Bµ| &gt; 3|γj | 2 0, ψ - j ≜ |γj | 2 ≤ |c T j Bµ| ≤ 3|γj | 2 ,<label>(40)</label></formula><p>and in the event when ψ - j is true, we have:</p><formula xml:id="formula_90">γ 2 j 4 ≤ (c T j Bµ) 2 ≤ ∥c j ∥ 2 ∥Bµ∥ 2 (Cauchy-Schwarz) γ 2 j 4∥Bµ∥ 2 ≤ ∥c j ∥ 2 .<label>(41)</label></formula><p>Combining Eq. 38 and 40 and using ∥B -1 (m i -m j )∥ F ≤ ∥B -1 ∥ F ∥m i -m j ∥ F , we have:</p><formula xml:id="formula_91">∥m i -m j ∥ 2 F ≥              γ 2 i 4∥B -1 ∥ 2 F + γ 2 j 4∥B -1 ∥ 2 F , ψ + i , ψ + j are active γ 2 i 4∥B -1 ∥ 2 F , ψ + i , ψ - j are active, Eq. 41 holds γ 2 j 4∥B -1 ∥ 2 F</formula><p>, ψ - i , ψ + j are active, Eq. 39 holds 0, ψ - i , ψ - j are active, Eq. 39 and 41 holds.</p><p>(42)</p><p>For the case when one of the mean say m j is observational then:</p><formula xml:id="formula_92">m j = m = Bµ =⇒ B -1 m j = µ.<label>(43)</label></formula><p>Thus the i th and j th entry of m j is e T i µ and e T j µ, respectively. Thus using Eq. 33 and 35 we get:</p><formula xml:id="formula_93">∥B -1 (m i -m j )∥ 2 F ≥ (γ i -c T i Bµ) 2 . (<label>44</label></formula><formula xml:id="formula_94">)</formula><p>Then again, following a similar analysis as in Eq. 38 and 39 we have the following cases:</p><formula xml:id="formula_95">∥m i -m j ∥ 2 F ≥ γ 2 i 4∥B -1 ∥ 2 F , ψ + i is active 0, ψ - i is active, Eq. 39 holds,<label>(45)</label></formula><p>which is equivalent to say that only case 2 and 4 are applicable in Eq. 42. This completes the proof.</p><p>Step 2 of Alg. 1: We run the UT-IGSP algorithm in Step 2 of our Alg. 1 with the standard parameter mentioned in the documentation. Specifically, we use α = 10 -3 for both MemoizedCITester and MemoizedInvarianceTester functions used by UT-IGSP.</p><p>Specific to results in Appendix Unless otherwise specified, for all the results in the appendix we run all the experiments for 5 random settings. We ignore the error bars in the appendix for clarity in exposition. Also for all the experiments, the half-intervention setting i.e. where the mixture contains intervention on half of the randomly selected nodes is considered.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Experimental Setup Discussion -Biology Dataset</head><p>Dataset: In the interventional data the signaling protein is also perturbed along with the receptor enzymes. The different perturbed signaling proteins (along with the number of samples corresponding) are: Akt (911), PKC (723), PIP2 (810), Mek (799), PIP3 (848). The observational data contained 1755 samples so overall 5846 samples were used for our experiments. For details see Wang et al. <ref type="bibr" target="#b30">[31]</ref> and Sachs et al. <ref type="bibr" target="#b21">[22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Code</head><p>The source code to all the experiments can be found in the following GitHub repository: <ref type="url" target="https://github.com/BigBang0072/mixture_mec">https://github.com/BigBang0072/mixture_mec</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 Computational Resources</head><p>We use an internal cluster of CPUs to run all our experiments. We run 10 random runs for each of the experimental configurations and report the mean (as points) and 5 th and 95 th quantiles as error bars for all our experiments in the main paper, and we report only mean the mean for the experiments in the appendix to declutter the figures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5 Evaluation Metric</head><p>Parameter Estimation Error Given the mixture distribution, the first step of our Alg. 1 estimates the parameters of every interventional and observational distribution present in the mixture (mixing weight π i , mean m i and covariance matrix S i ). Alg. 1 returns the maximum number of possible components, i.e. k = n + 1, by default. For all our experiments, we used this default value and left searching over the number of components for future work. To calculate the parameter estimation error, we first find the best match of the estimated parameters with the ground truth parameters based on the minimum error between the mean and covariance matrix. More specifically, let k * be the ground truth number of components in the mixture. Then, we iterate over all possible k * sized subsets of the estimated parameters and choose the one with the smallest absolute error sum between the mean and covariance matrix. Formally:</p><formula xml:id="formula_96">Parameter Estimation Error ≜ min k * i=1 |m i -mρ(i) | + |S i -Ŝρ(i) | : ρ ∈ perm([n + 1])<label>(46)</label></formula><p>where perm([n + 1]) represents all possible permutations of the indices [0, 1, . . . , n] and ρ * is the corresponding permutation with the minimum error.</p><p>Average Jaccard Similarity (JS) In step 2 of our Alg. 1, UT-IGSP estimates the unknown interventional targets for each of the individual components disentangled in Step 1. We use the same matching (ρ * ) found in the parameter estimation error step (as mentioned above) to calculate the Jaccard similarity between the estimated and ground truth intervention target for that component. Formally:</p><formula xml:id="formula_97">Avg. Jaccard Similarity ≜ 1 k * k * i=1 JS(t i , tρ(i) ) = 1 k * k * i=1 |t i ∩ tρ * (i) | |t i ∪ tρ * (i) | , (<label>47</label></formula><formula xml:id="formula_98">)</formula><p>where t i is the ground truth intervention target set and tj is the estimated target set. For the case when both t i = ti = ϕ, then JS(t i , ti ) ≜ 1. We consider graphs with 6 nodes in this experiment with half intervention setting. In step 2 of Mixture-UTIGSP, we select the number of components using the log-likelihood curve. We scan the curve starting from the mixture model with the largest number of components to the smallest and stop where the relative change in the likelihood increases above a cutoff ratio (to select the elbow point of the curve). The cutoff ratio in the algorithm is chosen to be an arbitrary number close to zero. Here we compare the performance of Mixture-UTIGSP on all three metrics for the half setting of Fig. <ref type="figure" target="#fig_2">1</ref> as we vary the cutoff ratio. We observe that for the cutoff ratio close to zero i.e. 0.01, 0.15,0.3 the performance remains similar showing that the model selection criteria are robust to the selected cutoff ratio. The number of nodes The column shows different evaluation metrics, i.e., Parameter Estimation Error, Average Jaccard Similarity, and SHD (see Evaluation metric paragraph in §6). In this experiment, we vary the density of the underlying causal graph by keeping the edges in a fully connected graph with a fixed probability, labeled as density in the legend of the above plots (see random graph generation paragraph in §B.1 for details). The maximum possible density is 1, i.e., the probability of keeping an edge is 1, corresponding to a fully connected graph, and the lowest possible density is 0. We observe that as the density of the graph increases, we require more samples to achieve similar performance to less dense graphs on all three metrics. Our Theorem 4.1 shows that the sample complexity required for estimating the parameters of the mixture is proportional to the norm of the adjacency matrix ∥A∥ and as the density of the graph increases ∥A∥ increases. Thus, as the density increases, we require more samples to achieve a similar performance in estimating the parameters of the mixture, as seen in the parameter estimation error plot above.  <ref type="bibr" target="#b30">[31]</ref>. 5b shows the graph estimated by our Mixture-UTIGSP and 5c is the graph estimated by oracle UT-IGSP when they are given the ground truth disentangled mixture. The blue colored arrow in 1b and 1c shows the correctly recovered edges in the domain expert graph. Green shows the edges with the same skeleton in the domain expert graph but in a reversed direction. The red shows the edges that are incorrectly added in the estimated graph. We observe that Mixture-UTIGSP correctly identifies two more edges (PKA-&gt;ERK and PKA-&gt; Akt) as compared to an oracle which could be due to randomness in the UTIGSP algorithm. For this estimation, the best-performing cutoff of 0.01 was selected (see Table <ref type="table" target="#tab_2">1</ref>).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Theorem</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>num intervention = half (↓)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Performance of Alg. 1 as we vary sample size and number of nodes:The first row (a-c) shows the performance when the mixed data contains atomic intervention on all the nodes and observational data. The second row (d-f) shows the performance when the number of atomic interventions (chosen randomly) in mixed data is taken to be half of the number of nodes along with observational data. The column shows different evaluation metrics, i.e., Parameter Estimation Error, Average Jaccard Similarity, and SHD. The symbols (↑) represent higher is better, and (↑) represents the opposite (see Evaluation metric paragraph in §6). In summary, performance improves for both cases as the number of samples increases. However, the graph with more nodes requires a larger sample to perform similarly. For a detailed discussion, see §6.1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure3: Performance of Alg. 1 as we change the cutoff ratio used for automatic component selection: We consider graphs with 6 nodes in this experiment with half intervention setting. In step 2 of Mixture-UTIGSP, we select the number of components using the log-likelihood curve. We scan the curve starting from the mixture model with the largest number of components to the smallest and stop where the relative change in the likelihood increases above a cutoff ratio (to select the elbow point of the curve). The cutoff ratio in the algorithm is chosen to be an arbitrary number close to zero. Here we compare the performance of Mixture-UTIGSP on all three metrics for the half setting of Fig.1as we vary the cutoff ratio. We observe that for the cutoff ratio close to zero i.e. 0.01, 0.15,0.3 the performance remains similar showing that the model selection criteria are robust to the selected cutoff ratio. The number of nodes</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Performance of Alg. 1 as we change the density of the underlying true causal graph: The mixture data contains atomic interventions on all nodes as well as observational data (half setting as described in the results in §6).The column shows different evaluation metrics, i.e., Parameter Estimation Error, Average Jaccard Similarity, and SHD (see Evaluation metric paragraph in §6). In this experiment, we vary the density of the underlying causal graph by keeping the edges in a fully connected graph with a fixed probability, labeled as density in the legend of the above plots (see random graph generation paragraph in §B.1 for details). The maximum possible density is 1, i.e., the probability of keeping an edge is 1, corresponding to a fully connected graph, and the lowest possible density is 0. We observe that as the density of the graph increases, we require more samples to achieve similar performance to less dense graphs on all three metrics. Our Theorem 4.1 shows that the sample complexity required for estimating the parameters of the mixture is proportional to the norm of the adjacency matrix ∥A∥ and as the density of the graph increases ∥A∥ increases. Thus, as the density increases, we require more samples to achieve a similar performance in estimating the parameters of the mixture, as seen in the parameter estimation error plot above.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure5: Ground truth and estimated causal graph for Protein signaling dataset<ref type="bibr" target="#b21">[22]</ref>: Fig 5a is the graph created with the help of domain experts for this problem<ref type="bibr" target="#b30">[31]</ref>. 5b shows the graph estimated by our Mixture-UTIGSP and 5c is the graph estimated by oracle UT-IGSP when they are given the ground truth disentangled mixture. The blue colored arrow in 1b and 1c shows the correctly recovered edges in the domain expert graph. Green shows the edges with the same skeleton in the domain expert graph but in a reversed direction. The red shows the edges that are incorrectly added in the estimated graph. We observe that Mixture-UTIGSP correctly identifies two more edges (PKA-&gt;ERK and PKA-&gt; Akt) as compared to an oracle which could be due to randomness in the UTIGSP algorithm. For this estimation, the best-performing cutoff of 0.01 was selected (see Table1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Performance of Alg. 1 on Protein Signalling Dataset</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>We will drop the subscript M when it is clear from context</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>We have used the subscript "i" notation in Ai to denote the adjacency matrix of the intervened distribution. We will use a similar subscript notation to represent other variables related to intervened distributions</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>For the all setting, the actual number of components corresponding to the system with nodes 4,6 and 8 are 5,7,9 respectively (one intervention on each node + one observational distribution). We observe that Mixture-UTIGSP is able to correctly estimate the number of components even with a small number of samples. Similarly, for half setting, the actual number of components corresponding to the system with nodes 4,6 and 8 are 3,4,5 respectively (intervention on half of node and one observational distribution).</p><p>Even for this case Mixture-UTIGSP is able to correctly estimate the number of components. The second column shows the error in the estimation of the mixing coefficient (π i 's, see Definition 4.1). For both cases, we observe that the error in the estimation of the mixing coefficient goes to zero as the sample size increases. Step 1 of Alg. 1: We use the GaussianMixture class from the scikit-learn Python package to disentangle the components of the mixture. For all our experiments, we use the default tol = 10 -3 used by GaussianMixture to decide on convergence of the underlying EM algorithm. (a) Varying the mean of the noise distribution (|γi|): The initial mean of the noise distribution is 0.0 for all the nodes. Upon intervening on a node to generate the interventional distribution, we change the mean of the noise to a different value. The variance of the noise distribution of the intervened node is kept the same as the initial distribution i.e. 1.0. From Theorem 4.1, we expect that as the new mean increases further from the initial mean = 0.0, the parameter estimation error should be lower for a given sample size and lead to better performance in intervention target estimation and causal discovery. As expected, the setting with the smallest change in the mean of the noise distribution (blue curve) has the worst performance. The case when the new noise mean is 10.0 (orange curve) is unusual where we see an unexpected increase in parameter estimation error. The initial variance of the noise distribution is 1.0 for all nodes, and we change the variance of the new noise distribution upon intervention in this experiment. The mean of the noise distribution of the intervened distribution is kept the same as the initial distribution i.e. 0.0. From Theorem 4.1, we see that sample complexity to recover the parameters of the mixture distribution is inversely proportional to the change in the noise variance |δi|. Thus, we expect that the performance of Alg. 1 should improve as the new noise variance moves away from the initial noise variance 1.0. We can see in the above plot that the performance of the green curve (δi = 0) is worst in terms of the Jaccard similarity and SHD of the recovered graph, validating our expectation. Parameter estimation cannot be directly compared since as the variance increases, the norm of the covariance matrix increases, and thus, the overall error in the estimation error increases. We observe that compared to changing the mean (Fig. <ref type="figure">6a</ref>) increasing the variance gives slightly lower performance gains. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Setup and Additional Empirical Result</head></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Does teacher training affect pupil learning? evidence from matched comparisons in jerusalem public schools</title>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Angrist</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Lavy</surname></persName>
		</author>
		<idno>ucp:jlabec:v:19:y:2001:i:2</idno>
		<ptr target="https://EconPapers.repec.org/RePEc" />
	</analytic>
	<monogr>
		<title level="j">Journal of Labor Economics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="343" to="369" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Polynomial learning of distribution families</title>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaushik</forename><surname>Sinha</surname></persName>
		</author>
		<ptr target="https://api.semanticscholar.org/CorpusID:3089712" />
	</analytic>
	<monogr>
		<title level="m">IEEE 51st Annual Symposium on Foundations of Computer Science</title>
		<imprint>
			<date type="published" when="2010">2010. 2010</date>
			<biblScope unit="page" from="103" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Exact bayesian structure learning from uncertain interventions</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Eaton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="https://proceedings.mlr.press/v2/eaton07a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics</title>
		<editor>
			<persName><forename type="first">Marina</forename><surname>Meila</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Xiaotong</forename><surname>Shen</surname></persName>
		</editor>
		<meeting>the Eleventh International Conference on Artificial Intelligence and Statistics<address><addrLine>San Juan, Puerto Rico</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2007-03">Mar 2007</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="21" to="24" />
		</imprint>
	</monogr>
	<note>of Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Causal discovery and the problem of psychological interventions</title>
		<author>
			<persName><forename type="first">Markus</forename><forename type="middle">I</forename><surname>Eronen</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.newideapsych.2020.100785</idno>
		<ptr target="https://www.sciencedirect.com/science/article/pii/S0732118X19301436" />
	</analytic>
	<monogr>
		<title level="m">New Ideas in Psychology</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page">100785</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Using bayesian networks to analyze expression data</title>
		<author>
			<persName><forename type="first">Nir</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Linial</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iftach</forename><surname>Nachman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dana</forename><surname>Pe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">'</forename><surname>Er</surname></persName>
		</author>
		<idno type="DOI">10.1145/332306.332355</idno>
		<ptr target="https://doi.org/10.1145/332306.332355" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Annual International Conference on Computational Molecular Biology, RECOMB &apos;00</title>
		<meeting>the Fourth Annual International Conference on Computational Molecular Biology, RECOMB &apos;00<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="127" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Highfrequency off-target mutagenesis induced by crispr-cas nucleases in human cells</title>
		<author>
			<persName><forename type="first">Yanfang</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><surname>Foden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cyd</forename><surname>Khayter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgan</forename><surname>Maeder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deepak</forename><surname>Reyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Joung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffry</forename><surname>Sander</surname></persName>
		</author>
		<idno type="DOI">10.1038/nbt.2623</idno>
	</analytic>
	<monogr>
		<title level="j">Nature biotechnology</title>
		<imprint>
			<biblScope unit="volume">06</biblScope>
			<biblScope unit="page">2013</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning mixtures of gaussians in high dimensions</title>
		<author>
			<persName><forename type="first">Rong</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingqing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sham</forename><forename type="middle">M</forename><surname>Kakade</surname></persName>
		</author>
		<idno type="DOI">10.1145/2746539.2746616</idno>
		<ptr target="https://doi.org/10.1145/2746539.2746616" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Forty-Seventh Annual ACM Symposium on Theory of Computing, STOC &apos;15</title>
		<meeting>the Forty-Seventh Annual ACM Symposium on Theory of Computing, STOC &apos;15<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="761" to="770" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Bridging methodologies: Angrist and imbens&apos; contributions to causal identification</title>
		<author>
			<persName><forename type="first">Lucas</forename><surname>Girard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yannick</forename><surname>Guyonvarch</surname></persName>
		</author>
		<ptr target="https://api.semanticscholar.org/CorpusID:267760356" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Identification of mixtures of discrete product distributions in near-optimal sample and time complexity</title>
		<author>
			<persName><forename type="first">L</forename><surname>Spencer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bijan</forename><surname>Jahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuval</forename><surname>Mazaheri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonard</forename><forename type="middle">J</forename><surname>Rabani</surname></persName>
		</author>
		<author>
			<persName><surname>Schulman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Causal discovery from soft interventions with unknown targets: Characterization and learning</title>
		<author>
			<persName><forename type="first">Amin</forename><surname>Jaber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Murat</forename><surname>Kocaoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthikeyan</forename><surname>Shanmugam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elias</forename><surname>Bareinboim</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper_files/paper/2020/file/6" />
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Lin</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="9551" to="9561" />
		</imprint>
	</monogr>
	<note>cd9313ed34ef58bad3fdd504355e72c-Paper.pdf</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning and smoothed analysis</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Adam Tauman Kalai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shang-Hua</forename><surname>Samorodnitsky</surname></persName>
		</author>
		<author>
			<persName><surname>Teng</surname></persName>
		</author>
		<idno type="DOI">10.1109/FOCS.2009.60</idno>
	</analytic>
	<monogr>
		<title level="m">2009 50th Annual IEEE Symposium on Foundations of Computer Science</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="395" to="404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Funding Information: This work was jointly supported by the British Heart Foundation and The Alan Turing Institute</title>
		<author>
			<persName><forename type="first">Jack</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carlo</forename><surname>Berzuini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernard</forename><surname>Keavney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maciej</forename><surname>Tomaszewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Guo</surname></persName>
		</author>
		<idno type="DOI">10.1002/mgg3.2055</idno>
	</analytic>
	<monogr>
		<title level="m">Publisher Copyright: © 2022 The Authors</title>
		<imprint>
			<publisher>Wiley Periodicals LLC</publisher>
			<date type="published" when="2022-10">October 2022. 19/10/34813</date>
			<biblScope unit="volume">10</biblScope>
		</imprint>
	</monogr>
	<note>Molecular Genetics and Genomic Medicine</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Interventionist causal models in psychiatry: repositioning the mind-body problem</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Kendler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Campbell</surname></persName>
		</author>
		<idno type="DOI">10.1017/S0033291708004467</idno>
	</analytic>
	<monogr>
		<title level="j">Psychological Medicine</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="881" to="887" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Disentangling mixtures of unknown causal interventions</title>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Sinha</surname></persName>
		</author>
		<ptr target="https://proceedings.mlr.press/v161/kumar21a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Seventh Conference on Uncertainty in Artificial Intelligence</title>
		<editor>
			<persName><forename type="first">Cassio</forename><surname>De Campos</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Marloes</forename><forename type="middle">H</forename><surname>Maathuis</surname></persName>
		</editor>
		<meeting>the Thirty-Seventh Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2021-07-30">27-30 Jul 2021</date>
			<biblScope unit="volume">161</biblScope>
			<biblScope unit="page" from="2093" to="2102" />
		</imprint>
	</monogr>
	<note>Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Settling the polynomial learnability of mixtures of gaussians</title>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Moitra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Valiant</surname></persName>
		</author>
		<ptr target="https://api.semanticscholar.org/CorpusID" />
	</analytic>
	<monogr>
		<title level="m">IEEE 51st Annual Symposium on Foundations of Computer Science</title>
		<imprint>
			<date type="published" when="2010">2010. 2010</date>
			<biblScope unit="page">3250359</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Joint causal inference from multiple contexts</title>
		<author>
			<persName><forename type="first">M</forename><surname>Joris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sara</forename><surname>Mooij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Magliacane</surname></persName>
		</author>
		<author>
			<persName><surname>Claassen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<idno type="ISSN">1532-4435</idno>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2020-01">jan 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Distinctness of the Eigenvalues of a Quadratic form in a Multivariate Sample</title>
		<author>
			<persName><forename type="first">Masashi</forename><surname>Okamoto</surname></persName>
		</author>
		<idno type="DOI">10.1214/aos/1176342472</idno>
		<ptr target="https://doi.org/10.1214/aos/1176342472" />
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="763" to="765" />
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Causality: Models, Reasoning and Inference</title>
		<author>
			<persName><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Cambridge University Press</publisher>
			<pubPlace>USA</pubPlace>
		</imprint>
	</monogr>
	<note>2nd edition. ISBN 052189560X</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Elements of Causal Inference: Foundations and Learning Algorithms</title>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schlkopf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A comprehensive review of the placebo effect: recent advances and current thought</title>
		<author>
			<persName><forename type="first">Donald</forename><forename type="middle">D</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Damien</forename><forename type="middle">G</forename><surname>Finniss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabrizio</forename><surname>Benedetti</surname></persName>
		</author>
		<ptr target="https://api.semanticscholar.org/CorpusID" />
	</analytic>
	<monogr>
		<title level="j">Annual review of psychology</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page">11316014</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Causal protein-signaling networks derived from multiparameter single-cell data</title>
		<author>
			<persName><forename type="first">Karen</forename><surname>Sachs</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omar</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dana</forename><surname>Pe'er</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douglas</forename><forename type="middle">A</forename><surname>Lauffenburger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Garry</forename><forename type="middle">P</forename><surname>Nolan</surname></persName>
		</author>
		<idno type="DOI">10.1126/science.1105809</idno>
		<ptr target="https://doi.org/10.1126/science.1105809" />
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<idno type="ISSN">0036-8075</idno>
		<imprint>
			<biblScope unit="volume">308</biblScope>
			<biblScope unit="issue">5721</biblScope>
			<biblScope unit="page" from="523" to="529" />
			<date type="published" when="2005-04">April 2005</date>
			<pubPlace>New York, N.Y.</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Causal structure discovery from distributions arising from mixtures of DAGs</title>
		<author>
			<persName><forename type="first">Basil</forename><surname>Saeed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Snigdha</forename><surname>Panigrahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caroline</forename><surname>Uhler</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="https://proceedings.mlr.press/v119/saeed20a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">Hal</forename><surname>Daumé</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Iii</forename></persName>
		</editor>
		<editor>
			<persName><forename type="first">Aarti</forename><surname>Singh</surname></persName>
		</editor>
		<meeting>the 37th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2020-07">Jul 2020</date>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="page" from="13" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning good interventions in causal graphs via covering</title>
		<author>
			<persName><forename type="first">Ayush</forename><surname>Sawarni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Madhavan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siddharth</forename><surname>Barman</surname></persName>
		</author>
		<idno>PMLR</idno>
		<ptr target="https://proceedings.mlr.press/v216/sawarni23a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Ninth Conference on Uncertainty in Artificial Intelligence</title>
		<editor>
			<persName><forename type="first">Robin</forename><forename type="middle">J</forename><surname>Evans</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Ilya</forename><surname>Shpitser</surname></persName>
		</editor>
		<meeting>the Thirty-Ninth Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2023-08-04">31 Jul-04 Aug 2023</date>
			<biblScope unit="volume">216</biblScope>
			<biblScope unit="page" from="1827" to="1836" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A linear non-gaussian acyclic model for causal discovery</title>
		<author>
			<persName><forename type="first">Shohei</forename><surname>Shimizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrik</forename><forename type="middle">O</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aapo</forename><surname>Hyvarinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antti</forename><surname>Kerminen</surname></persName>
		</author>
		<ptr target="http://jmlr.org/papers/v7/shimizu06a.html" />
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">72</biblScope>
			<date type="published" when="2003">2003-2030, 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Permutation-based causal structure learning with unknown intervention targets</title>
		<author>
			<persName><forename type="first">Chandler</forename><surname>Squires</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuhao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caroline</forename><surname>Uhler</surname></persName>
		</author>
		<ptr target="http://proceedings.mlr.press/v124/squires20a.html" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Sixth Conference on Uncertainty in Artificial Intelligence, UAI 2020, virtual online</title>
		<editor>
			<persName><forename type="first">Ryan</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Vibhav</forename><surname>Gogate</surname></persName>
		</editor>
		<meeting>the Thirty-Sixth Conference on Uncertainty in Artificial Intelligence, UAI 2020, virtual online</meeting>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2020">August 3-6, 2020. 2020</date>
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="page" from="1039" to="1048" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Causal discovery with a mixture of dags</title>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">V</forename><surname>Strobl</surname></persName>
		</author>
		<idno type="DOI">10.1007/s10994-022-06159-y</idno>
		<ptr target="https://doi.org/10.1007/s10994-022-06159-y" />
	</analytic>
	<monogr>
		<title level="j">Mach. Learn</title>
		<idno type="ISSN">0885-6125</idno>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="4201" to="4225" />
			<date type="published" when="2022-03">mar 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning mixtures of dag models</title>
		<author>
			<persName><forename type="first">Bo</forename><surname>Thiesson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Meek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Maxwell Chickering</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Heckerman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth Conference on Uncertainty in Artificial Intelligence, UAI&apos;98</title>
		<meeting>the Fourteenth Conference on Uncertainty in Artificial Intelligence, UAI&apos;98<address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="504" to="513" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Separability analysis for causal discovery in mixture of DAGs</title>
		<author>
			<persName><forename type="first">Burak</forename><surname>Varici</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dmitriy</forename><surname>Katz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dennis</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prasanna</forename><surname>Sattigeri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Tajer</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=ALRWXT1RLZ" />
	</analytic>
	<monogr>
		<title level="j">Transactions on Machine Learning Research</title>
		<idno type="ISSN">2835-8856</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Unbiased detection of off-target cleavage by crispr-cas9 and talens using integrase-defective lentiviral vectors</title>
		<author>
			<persName><forename type="first">Xiaoling</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yebo</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiwei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinhui</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingjia</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaojun</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tammy</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">He</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ren-Jang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiing-Kuan</forename><surname>Yee</surname></persName>
		</author>
		<idno type="DOI">10.1038/nbt.3127</idno>
	</analytic>
	<monogr>
		<title level="j">Nature biotechnology</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Permutation-based causal inference algorithms with interventions</title>
		<author>
			<persName><forename type="first">Yuhao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liam</forename><surname>Solus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karren</forename><forename type="middle">Dai</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caroline</forename><surname>Uhler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS&apos;17</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems, NIPS&apos;17<address><addrLine>Red Hook, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5824" to="5833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Active learning for optimal intervention design in causal models</title>
		<author>
			<persName><forename type="first">Jiaqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis</forename><surname>Cammarata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandler</forename><surname>Squires</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Themistoklis</forename><surname>Sapsis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caroline</forename><surname>Uhler</surname></persName>
		</author>
		<idno type="DOI">10.1038/s42256-023-00719-0</idno>
	</analytic>
	<monogr>
		<title level="j">Nature Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">2023</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
