<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Argumentative Causal Discovery</title>
				<funder ref="#_DJZB6m7">
					<orgName type="full">UK Research and Innovation</orgName>
				</funder>
				<funder ref="#_SjTGTJa">
					<orgName type="full">European Research Council</orgName>
					<orgName type="abbreviated">ERC</orgName>
				</funder>
				<funder>
					<orgName type="full">Royal Academy of Engineering</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-08-03">3 Aug 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Fabrizio</forename><surname>Russo</surname></persName>
							<email>fabrizio@imperial.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">Imperial College London</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Anna</forename><surname>Rapberger</surname></persName>
							<email>a.rapberger@imperial.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">Imperial College London</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Francesca</forename><surname>Toni</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">Imperial College London</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Argumentative Causal Discovery</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-08-03">3 Aug 2024</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2405.11250v3[cs.AI]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-21T20:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Causal discovery amounts to unearthing causal relationships amongst features in data. It is a crucial companion to causal inference, necessary to build scientific knowledge without resorting to expensive or impossible randomised control trials. In this paper, we explore how reasoning with symbolic representations can support causal discovery. Specifically, we deploy assumption-based argumentation (ABA), a wellestablished and powerful knowledge representation formalism, in combination with causality theories, to learn graphs which reflect causal dependencies in the data. We prove that our method exhibits desirable properties, notably that, under natural conditions, it can retrieve ground-truth causal graphs. We also conduct experiments with an implementation of our method in answer set programming (ASP) on four datasets from standard benchmarks in causal discovery, showing that our method compares well against established baselines.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Causal Discovery is the process of extracting causal relationships amongst variables in data, represented as graphs. These graphs are crucial for understanding causal effects and perform causal inference <ref type="bibr" target="#b26">(Peters, Janzing, and Schölkopf, 2017;</ref><ref type="bibr" target="#b24">Pearl, 2009;</ref><ref type="bibr" target="#b39">Spirtes, Glymour, and Scheines, 2000)</ref>, e.g. to determine the impact of an action or treatment on an outcome. Causal effects are ideally discovered through interventions or randomised control trials, but these can be expensive, time consuming or outright impossible, e.g. in healthcare, trying to establish whether smoking causes cancer through a randomised control trial would require the study group to take up smoking to measure its (potentially deadly) effect. Hence the need to use observational, as opposed to interventional, data to study causes and effects (Peters, Janzing, and <ref type="bibr" target="#b26">Schölkopf, 2017;</ref><ref type="bibr" target="#b35">Schölkopf et al., 2021)</ref>.</p><p>Prominent approaches to perform causal discovery include constraint-based, score-based and functional causal model-based methods (see e.g. <ref type="bibr" target="#b14">(Glymour, Zhang, and Spirtes, 2019;</ref><ref type="bibr" target="#b43">Vowels, Camgoz, and Bowden, 2022;</ref><ref type="bibr" target="#b44">Zanga, Ozkirimli, and Stella, 2022)</ref> for overviews). These approaches employ statistical methods to retrieve the causal relations between variables. However, statistical methods, even if consistent with infinite data, are prone to errors due to finite data. As a result, the extracted causal relations can deviate from the ground truth and, crucially, also from the observed data. Let us consider an example.</p><p>Example 1.1. We set out to discover the causal relations between rain (r), wet roof terrace (wr), wet street (ws) and watering plants (wp) (on the roof terrace). After collecting sufficient data, we carry out conditional independence tests. These correctly return that r and wp are independent (written r⊥ ⊥ wp); but also find r and wp independent when conditioned on {wr} (written r⊥ ⊥ wp | {wr}) which goes against our intuition: since something must have caused wr, we can infer r when knowing ¬wp and vice versa. That is, r and wp become dependent when conditioning on {wr}.</p><p>Below, we depict the ground truth causal graph (left) and the output of Majority-PC (right), proven to be sound and complete with infinite data <ref type="bibr" target="#b7">(Colombo and Maathuis, 2014)</ref>. A directed edge is interpreted as cause, e.g., wp causes wr; the absence of an edge indicates causal independence; an undirected edge indicates a causal relationship but the direction of the cause and effect relation remains unclear.</p><p>Since the conditional independence test wrongly rendered r and wp independent given {wr}, it is impossible to retrieve the ground truth whilst satisfying all reported causal relations between the variables. In fact, it can happen that no graph exists that faithfully captures the results of the tests.</p><p>To account for the issues observed in the example, researchers have investigated several methods to handle conflicting data; e.g., <ref type="bibr" target="#b9">Corander et al. (2013)</ref> utilised Answer Set Programming (ASP) to learn chordal Markov networks; <ref type="bibr" target="#b17">Hyttinen, Eberhardt, and Järvisalo (2014)</ref> provide an encoding of graphical interventions to compute causal graphs; <ref type="bibr" target="#b31">Rantanen, Hyttinen, and Järvisalo (2020)</ref> use constraint programming. However, argumentative methods, which are ideally suited for conflict resolution, have not received much attention in the context of causal discovery so far. A notable exception is the work by <ref type="bibr" target="#b3">Bromberg and Margaritis (2009)</ref> who employ a form of preference-based argumentation <ref type="bibr" target="#b0">(Amgoud and Cayrol, 2002)</ref>, instantiated with deductive argumentation (see <ref type="bibr" target="#b27">(Philippe Besnard, 2018)</ref> for an overview), to choose a set of tests to use within the PC algorithm <ref type="bibr" target="#b39">(Spirtes, Glymour, and Scheines, 2000)</ref>. Their method is however based on Pearl's graphoid axioms which are incomplete; thus, some inconsistencies between the reported (in)dependences might not be detected by their approach.</p><p>In this paper, we provide a novel argumentative approach to account for inconsistencies in the reported tests and reflect a consistent subset of them into a directed acyclic graph (DAG). In line with the causal discovery literature, we assume faithfulness of the data, i.e., that all the independencies in the data are compatible with some DAG structure <ref type="bibr" target="#b39">(Spirtes, Glymour, and Scheines, 2000)</ref> as well as sufficiency i.e. there are no latent confounders. To handle conflicts in data, we employ assumption-based argumentation (ABA) which is a versatile non-monotonic reasoning formalism <ref type="bibr">( Čyras et al., 2018)</ref> based on assumptions (i.e., defeasible elements) and inference rules. ABA has been studied under numerous semantics, which are criteria to determine the acceptance of assumption sets and their conclusions. A single ABA framework can possess several different extensions, i.e., sets of acceptable assumptions w.r.t. a given semantics, which reflect the different viewpoints that exist within a single framework.</p><p>In Fig. <ref type="figure" target="#fig_1">1</ref> we summarise the workflow of our method. Based on (i) the output of statistical methods as well as potential (ii) domain knowledge provided by experts, we construct an ABA framework whose extensions provide all the DAGs compatible with (i) and (ii). Overall, our contributions in this work are as follows:</p><p>• We formalise causal graphs in the language of ABA (Causal ABA). We use rules to model the d-separation criterion <ref type="bibr" target="#b24">(Pearl, 2009)</ref>, which characterises conditional independence in DAGs; and assumptions to model conditional independence and causal relations.</p><p>• We provide an ASP implementation of our theoretical framework using the independence tests from the Majority-PC algorithm <ref type="bibr" target="#b7">(Colombo and Maathuis, 2014)</ref> as hard or weak constraints, resulting in ABA-PC. We employ weights for fact selection when necessary.</p><p>• We experimentally evaluate our ABA-PC algorithm with four (standard) datasets. Our experiments show that our proposed framework improves on current state-of-the-art baselines in Causal Discovery. In particular, we reconstruct the ground-truth causal DAG better than Majority-PC using the same set of independence relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries</head><p>Graphs are crucial for causal and argumentation theories. A graph G = (V, E) has nodes V and edges</p><formula xml:id="formula_0">E ⊆ V × V; G is directed if either (x, y) ∈ E or (y, x) ∈ E; undirected if (x, y</formula><p>) ∈ E and (y, x) ∈ E; and partially directed otherwise.</p><p>The skeleton of G is the result of replacing all directed edges with undirected ones. x, y ∈ V are adjacent iff (x, y) ∈ E or (y, x) ∈ E. A (x 1 -x n -)path path is a sequence of distinct nodes x 1 . . . x n s.t. for 1 ≤ i &lt; n, x i and x i+1 are adjacent. We omit 'x 1 -x n ' if it is clear from the context. Given a path p = x 1 . . . x n and a node x, we sometimes abuse notation and write x ∈ p to specify that x is contained in p, i.e., there is i</p><formula xml:id="formula_1">≤ n s.t. x = x i . A path x 1 . . . x n is directed if (x i , x i+1 ) ∈ E for all i ≤ n; cyclic if it is directed and x 1 = x n . A directed acyclic graph (DAG) is a directed graph without cycles.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Causal Graphs</head><p>A causal graph represents causal relations between variables <ref type="bibr" target="#b24">(Pearl, 2009;</ref><ref type="bibr" target="#b39">Spirtes, Glymour, and Scheines, 2000)</ref>. In this paper, we focus on causal graphs that admit a DAG structure. A triple (x i , x j , x k ) of variables in a DAG is an Unshielded Triple (UT) if two variables are not adjacent but each is adjacent to the third. An UT (x, y, z) is a v-structure iff (x, y) ∈ E and (z, y) ∈ E; y is a collider (w.r.t. x, z).</p><formula xml:id="formula_2">Definition 2.1. Let G = (V, E) be a DAG. A x-y-path p, x, y ∈ V, x ̸ = y, is Z-active for a set Z ⊆ V \ {x, y} in G iff for each node z ∈ p: if z is a collider in p, then z ∈ Z or there is a descendant z ′ of z s.t. z ′ ∈ Z; otherwise, z / ∈ Z.</formula><p>Two variables x, y ∈ V are independent, conditioned on a set Z ⊆ V \ {x, y}, if fixing the values of the variables in Z does not provide additional information about x or y (resp.). Independence in DAGs is captured by d-separation. Definition 2.2. Let G = (V, E) be a DAG. Two nodes x, y ∈ V are d-connected given Z ⊆ V iff G contains a Z-active x-y-path p. The nodes x, y ∈ V are d-separated given Z iff x, y are not d-connected given Z. Two variables x, y are independent w.r.t. Z in G iff they are d-separated given Z, denoted by x ⊥ ⊥ G y | Z.</p><p>Causal Graphs and Statistics Causal Discovery couples statistical and graphical methods to extract causal graphs from data. The nodes V = {X 1 , . . . , X d } in a causal graph G = (V, E) correspond to random variables (in our running Example 1.1, 'rain' can be a random variable when associated with observed data) and the edges represent causal relationships between them. A joint probability distribution P factorizes according to a DAG</p><formula xml:id="formula_3">G if P (V) = d i=1 P (X i | pa(G, X i ))</formula><p>, where pa(G, X i ) denotes the set of parents of X i in G. A distribution P is Markovian w.r.t. G if it respects the conditional independence relations entailed by G via d-separation. In turn, P is faithful to G if DAG G reflects all conditional independences in P . Different DAGs can imply the same set of conditional independences, in which case they form a Markov Equivalence Class (MEC) <ref type="bibr" target="#b33">(Richardson and Spirtes, 1999)</ref>. DAGs in a MEC present the same adjacencies and v-structures and are uniquely represented by a Completed Partially DAG (CPDAG) <ref type="bibr" target="#b5">(Chickering, 2002)</ref> which is a partially directed graph that has a directed edge if every DAG in the MEC has it, and an undirected edge if both directions appear in the MEC.</p><p>A Conditional Independence Test (CIT), e.g. Fisher's Z <ref type="bibr" target="#b11">(Fisher, 1970)</ref>, HSIC <ref type="bibr" target="#b15">(Gretton et al., 2007)</ref>, or KCI <ref type="bibr" target="#b45">(Zhang et al., 2011)</ref>, is a procedure to measure independence with a known asymptotic distribution under the null hypothesis H 0 of independence. Calculating the test statistic for a dataset allows to estimate the test's observed significance level (pvalue), under H 0 . This is a measure of evidence against H 0 <ref type="bibr" target="#b16">(Hung et al., 1997)</ref>. Under H 0 , p is uniformly distributed in the interval [0, 1], which allows to set a significance level α that represents the pre-experiment Type I error rate (rejecting H 0 when it is true), whose expected value is at most</p><formula xml:id="formula_4">α. A CIT, denoted by I(X i , X j | Z), outputs a p-value. If I(X i , X j | Z) = p ≥ α then X i ⊥ ⊥ X j | Z. Instead, if I(X i , X j | Z) = p &lt; α then we can reject H 0 and declare the variables dependent: X i ̸⊥ ⊥ X j | Z.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Assumption-based Argumentation</head><p>We recall basics of assumption-based argumentation (ABA); for a comprehensive introduction we refer to <ref type="bibr">( Čyras et al., 2018)</ref>. We assume a deductive system (L, R), where L is a formal language, i.e., a set of sentences, and R is a set of rules over L. A rule r ∈ R has the form a 0 ← a 1 , . . . , a n with a i ∈ L, head(r) = a 0 and body(r) = {a 1 , . . . , a n }. Definition 2.3. An ABA framework (ABAF) is a tuple (L, R, A, ), where (L, R) is a deductive system, A ⊆ L a set of assumptions, and : A → L is a function mapping assumptions a ∈ A to sentences L (contrary function).</p><p>A sentence q ∈ L is tree-derivable from S ⊆ A and rules R ⊆ R, denoted by S ⊢ R q, if there is a finite rooted labeled tree T which, intuitively, corresponds to the structure of the derivation: the root of T is labeled with q; the set of labels for the leaves of T is equal to S or S ∪ {⊤}; and for every inner node v of T there is a rule r ∈ R such that v is labelled with head(r), the number of successors of v is |body(r)| and every successor of v is labelled with a distinct a ∈ body(r) or ⊤ if body(r) = ∅. We often drop R and write S ⊢ R q simply as S ⊢ q if it does not cause confusion. Definition 2.4. Let D = (L, R, A, ) be an ABAF. A set S ⊆ A attacks T ⊆ A if there is S ′ ⊆ S, a ∈ T , s.t. S ′ ⊢ a. A set S is conflict-free in an ABAF D (S ∈ cf (D)) if it does not attack itself; S defends T iff it attacks each attacker of T ; S is closed iff S ⊢ a implies a ∈ S; S is admissible (S ∈ ad (D)) if it is conflict-free and defends itself.</p><p>With a little notational abuse we say a set S of assumptions attacks an assumption a if S attacks the singleton {a}; we let S = {a | a ∈ S}.</p><p>An ABAF D is called flat iff each set S of assumptions is closed. We call an ABAF non-flat if it does not belong to the class of flat ABAFs.</p><p>We next recall grounded, complete, preferred, and stable ABA semantics (abbr. gr , co, pr , stb). Definition 2.5. Let D be an ABAF and let S ∈ ad (D).</p><p>• S ∈ co(D) iff S contains every assumption set it defends;</p><formula xml:id="formula_5">• S ∈ gr (D) iff S is ⊆-minimal in co(D); • S ∈ pr (D) iff S is ⊆-maximal in co(D); • S ∈ stb(D) iff S attacks each {x} ⊆ A \ S.</formula><p>Given a semantics σ, we call σ(D) the set of σ-extensions of the ABAF D. We drop 'σ' if it is clear from context.</p><p>Graphical ABA Representation Argumentation frameworks with collective attacks (SETAFs) <ref type="bibr" target="#b22">(Nielsen and Parsons, 2006)</ref> are ideally suited to depict the attack structure between the assumptions in ABAFs as outlined by <ref type="bibr" target="#b19">König, Rapberger, and Ulbricht (2022)</ref>. In brief, a SETAF is a pair (A, R) consisting of a set of arguments A and an attack relation R ⊆ 2 A × A. We can instantiate an ABAF D = (L, A, R, ) as SETAF by setting A = A and R is the induced attack relation: S ⊆ A attacks a ∈ A if S ⊢ a. Example 2.6. Consider an ABAF with assumptions a, b, c, their contraries a = s, b = p, c = q, and rules (p ← a, c) and (s ← c). We can represent the ABAF as SETAF: The graph depicts the attack structure between the assumptions; the collective attack is depicted as a joint arrow.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Capturing Causal Graphs with ABA</head><p>We formalise causal graphs in ABA. We assume a fixed but arbitrary set of variables V with |V| = d. We refrain from explicitly mentioning the language L. Each assumption a below has a distinct contrary a c ; for convenience, we write a instead of a c if it does not cause confusion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Causal ABA</head><p>The class of causal relations we aim to capture are characterised by two factors: acyclicity and d-separation.</p><p>Acyclicity We formalise graph-theoretic properties since our expected outcome, i.e., the resulting extensions, are graphs. Thus, the assumptions in our ABAF are arrows:</p><formula xml:id="formula_6">A arr = {arr xy | x, y ∈ V, x ̸ = y}.</formula><p>Then, we define acyclicity as follows. Definition 3.1. Let D dag = (A dag , R dag , ) where</p><formula xml:id="formula_7">A dag = A arr ∪ {noe xy | x, y ∈ V, x ̸ = y}</formula><p>and R dag contains the following rules: We show that D dag correctly captures the set of all DAGs of fixed size d. The correspondence is true for all (except gr ) argumentation semantics under consideration. Below, we use the assumption arr xy to stand for the arrow (x, y). All proofs of this section are provided in Appendix §A.</p><formula xml:id="formula_8">• a ← b, a ̸ = b,</formula><formula xml:id="formula_9">Proposition 3.3. {(V, S ∩ A arr ) | S ∈ σ(D dag )} = {G | G is a DAG} for σ ∈ {co, pr , stb}.</formula><p>Note that the grounded extension corresponds to the fully disconnected graph G = (V, ∅) since the empty set is complete. Note also that the correspondence between DAGs and the extensions of the ABAF is one-to-many for complete, admissible and conflict-free assumption sets since a single acyclic graph corresponds to several complete extensions. Accepting the absence of an edge between two variables x, y can be realised by accepting noe xy or simply by accepting none of noe xy , arr xy , arr yx in the extension.</p><p>Example 3.4. In the ABAF from Example 3.2, the fully disconnected graph (V, ∅) corresponds to 2 3 complete extensions; i.e, to each subset of {noe xy , noe yz , noe zx }.</p><p>For preferred and stable semantics, the correspondence is one-to-one; the semantics coincide in D dag , as stated below. Lemma 3.5. σ(D dag ) = τ (D dag ) for σ, τ ∈ {pr , stb}. Corollary 3.6. Let σ ∈ {pr , stb}. Each DAG G corresponds to a unique set S ∈ σ(D dag ) and vice versa.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D-separation</head><p>The first step to represent d-separation is to extend our ABAF with independence statements. We do so by assuming independence between variables. We let</p><formula xml:id="formula_10">A ind = {(x⊥ ⊥y | Z) | Z ⊆ V, x, y ∈ V \ Z, x ̸ = y}. The conditional independence x ⊥ ⊥ y | Z is violated if the variables x, y are d-connected, given the conditioning set Z.</formula><p>Intuitively, we want to formalise x⊥ ⊥y | Z if there exists a Z-active path between x, y.</p><p>To capture this, it is convenient to formalise directed paths and we do so by letting R graph contain the following rules: where, intuitively, e xy stands for "edge between x and y."</p><formula xml:id="formula_11">dpath xy ←</formula><p>To formalise d-connectedness in the context of ABA, we introduce collider-trees, which generalise the notion of path by adding branches from collider nodes. Definition 3.7. Let G = (V, E) be a DAG, x, y ∈ V. A x-y-collider-tree t is a sub-graph of G satisfying: (a) t contains an x-y-path p t ; (b) for all u ∈ t, if u / ∈ p t then there is v ∈ t such that v is a collider in p t and u is a descendant of v. A c-t-path from collider node c (of p t ) to a leaf node t, t / ∈ {x, y}, is called a branch of t. For a set of variables Z ⊆ V, we call t Z-active iff p t is active w.r.t. t ∪ (V, ∅).</p><p>In the remainder of the paper, we drop 'x-y' and simply say collider-tree whenever it does not cause confusion. Example 3.8. Consider a causal graph G with five variables x, y, z, u, v as depicted below (left), and some collider-trees, denoted p 1 , p 2 , p 3 , from top to bottom, resp.:</p><formula xml:id="formula_12">x y z u v x u z y x y x y z u v</formula><p>The collider-trees p 1 and p 2 are active w.r.t. ∅; both paths have no collider so they are active w.r.t. every set not intersecting them; p 3 is active for sets containing z, u or v.</p><p>We are now ready to define our causal ABA framework. Definition 3.9. A causal ABAF D ds = (A ds , R ds , ) is characterised by</p><formula xml:id="formula_13">A ds = A dag ∪ A ind , and R ds = R dag ∪ R graph ∪ R act ,</formula><p>where R act contains rules (x⊥ ⊥y | Z ← t) for each Z-active x-y-collider-tree t with x ̸ = y, and Z ⊆ V \ {x, y}.</p><p>Example 3.10. Let us consider again Example 3.2 with V = {x, y, z}. We extend our ABAF with six independence assumptions and add the corresponding contraries. Below, we depict all arguments and attacks for the pair x, y; i.e., all attacks on the new assumptions (x⊥ ⊥y) and (x⊥ ⊥y | {z}). </p><formula xml:id="formula_14">. Let σ ∈ {pr , stb}, S ∈ σ(D ds ), x, y ∈ V, Z ⊆ V \ {x, y}. Then (x⊥ ⊥y | Z) ∈ S iff (x ⊥ ⊥ G y | Z).</formula><p>Note that we cannot guarantee the correspondence for complete semantics, as illustrated next. Example 3.12. In the ABAF from Example 3.10, S = ∅ is complete; indeed, D ds does not contain assumptions that are unattacked. The corresponding graph is G = (V, ∅) (cf. Example 3.4). In G, each pair of variables is independent; however, S does not contain any independence statement.</p><p>The example above shows that the correspondence between independence assumptions and independencies entailed by a DAG via d-separation is not preserved when dropping ⊆-maximality of the extensions. Interestingly, the other direction of Proposition 3.11 still holds for complete semantics; i.e., no incorrect independence statements are included in a complete extension. Proposition 3.13.</p><formula xml:id="formula_15">Let S ∈ co(D ds ), x, y ∈ V, Z ⊆ V \ {x, y}. Then (x⊥ ⊥y | Z) ∈ S implies (x⊥ ⊥ G y | Z).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Integrating Causal Knowledge</head><p>So far, we have introduced an ABAF that faithfully captures conditional independence in causal models. We have shown that an independence statement (x⊥ ⊥y | Z) is contained in an extension S if and only if it is consistent with graph corresponding to S. This, in turn translates to the dependencies of the graph: x and y are dependent given Z iff (x⊥ ⊥y | Z) / ∈ S. Our proposed ABAF can be integrated in any causal discovery pipeline to add formal guarantees that the graph discovered corresponds to the independences in the data. We integrate information from external sources, might they be statistical methods or experts, as facts.<ref type="foot" target="#foot_0">foot_0</ref> </p><p>In the remainder of this section, we write D ∪ {r} = (A, R ∪ {r}, ) for an ABAF D = (A, R, ) and rule r.</p><p>Let us consider again our three-variables example from before (cf. Example 3.10). First, suppose we have learned that x and y are marginally independent. We incorporate this information simply by adding the rule (x⊥ ⊥y ←). This rule ensures that each extension contains (x⊥ ⊥y). Since each extension must be closed, no active path between x and y can be accepted. We can proceed similarly when incorporating specific causal relations (directed edges). Proposition 3.14.</p><formula xml:id="formula_16">Let x, y, a, b ∈ V, Z ⊆ V \ {x, y}, X ⊆ V \ {a, b}, σ ∈ {pr , na, ss, stg, stb}, r ∈ {(x⊥ ⊥y | Z ←), (arr xy ←)}. For each S ∈ σ(D ds ∪ {r}), it holds that (a⊥ ⊥b | X) ∈ S iff (a⊥ ⊥ G b | X).</formula><p>Crucially, we observe that adding external facts comes at a cost: the ABAF is not flat anymore; indeed, the independence and arrow literals might appear in the head of rules.</p><p>Now, what happens if we incorporate test results or causal relation from an external source? Suppose we discovered x and y are marginally dependent. When we add the rule (x⊥ ⊥y ←) we successfully render (x⊥ ⊥y) false; however, we lose the correspondence between the (in)dependence statements and the graph of a given extension: when adding the contrary of (x⊥ ⊥y) nothing (in the framework presented so far) prevents us from accepting one of the arrows arr xy or arr yx . We need to generalise the framework to ensure that our ABAF is sound when adding dependencies as facts to the framework, as discussed next.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Blocked paths</head><p>The ABAF D ds successfully captures that an active path implies dependence. To guarantee soundness, it remains to formalise the other direction: independence between two nodes x, y implies that each path linking them is blocked. For this, we introduce new assumptions</p><formula xml:id="formula_17">A bp = {bp p|Z | p is a x-y-path, Z ⊆ V \ {x, y}} with contraries bp p|Z = ap p|Z .</formula><p>Furthermore, we require two new sets of rules: the first set of rules formalises that the independence between two variables x and y given Z requires that each path between x, y is blocked; the second set specifies when a path is Zactive. Definition 3.15.</p><formula xml:id="formula_18">For x, y ∈ V, x ̸ = y, Z ⊆ V \ {x, y}, we define R ext = R ds ∪ R xyZ with R xyZ containing the rules • x⊥ ⊥y | Z ← bp p1|Z , . . . , bp p k |Z where p 1 , . . . , p k denote</formula><p>all paths between x and y; • ap p|Z ← p t for each Z-active x-y-collider-tree t with underlying x-y-path p t .</p><p>Let us consider the effect of these rules with an example. Example 3.16. Consider again Example 3.10; suppose we observed x ̸⊥ ⊥ y | {z}. We add the independence (x⊥ ⊥y | {z} ←) which prevents us from accepting all bp p|{z} assumptions at the same time (since each extension S is closed, we also accept (x ⊥ ⊥ y | {z}), therefore, this leads to a conflict). Consequently, one of the bp p|{z} assumptions is attacked, i.e., some path between x, y is active.</p><p>It can be checked that the paths p 1 , p 2 , p 3 depicted below are {z}-active:</p><formula xml:id="formula_19">x y x y x y z</formula><p>As visualised in Example 3.10, each of these paths attack (x⊥ ⊥y | {z}). Due to the new rules from Definition 3.15 each path p i also derives ap pi|Z which attacks bp pi|Z . Therefore, each extension S must contain one of these paths.</p><p>We note that it suffices to add rules only for the dependence fact that we want to add. That is, when introducing fact (x⊥ ⊥y | Z ←) it suffices to add the rules from Definition 3.15 for x, y, Z. We define the extended ABAF. Definition 3.17.</p><formula xml:id="formula_20">For x, y ∈ V, Z ⊆ V\{x, y}, the extended causal ABAF D xyZ csl = (A csl , R csl , ) is characterised by A csl = A ds ∪ A bp and R csl = R ext ∪ {x⊥ ⊥y | Z ←}.</formula><p>The ABAF is sound and complete, as stated below.</p><formula xml:id="formula_21">Proposition 3.18. Let x, y, a, b ∈ V, let Z ⊆ V \ {x, y} and X ⊆ V \ {a, b}, let σ ∈ {pr , stb} and let S ∈ σ(D xyZ csl ). It holds that (a⊥ ⊥b | X) ∈ S iff (a⊥ ⊥ G b | X).</formula><p>Together, Propositions 3.14 and 3.18 guarantee that causal knowledge can be integrated in a faithful way. We obtain that this fine-tuned specification allows us to add (in)dependence facts and arrows whilst guaranteeing consistency of the causal ABAF. Independence facts and arrows can be added without further changes to the framework; when adding dependence facts, we require additional rules as specified in Definition 3.15. Below, we denote by </p><formula xml:id="formula_22">D T csl ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Implementation</head><p>In this section, we present an instance of our Causal ABA algorithm which combines our causal ABAF with heuristic approaches to select the independence facts that it can take in input. The workflow of Algorithm 1 is as follows:</p><p>1. The main function of the algorithm is what we name causalaba (line 9 and 12 of Algorithm 1). The causal ABAF instance is determined by the number of nodes in the graph d and a set of facts T. We generate the causal ABAF D T csl presented in §3, using an ASP implementation in clingo <ref type="bibr" target="#b12">(Gebser et al., 2019)</ref>. We then compute the stable extensions of the causal ABAF. Our ASP encoding is detailed in §4.1.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">The main input of</head><formula xml:id="formula_23">for p = I(x, y | Z) ∈ I do 16: s ← |Z| 17: if p &gt; α &amp; x ⊥ ⊥ G y | Z then 18: S G ← S G + S(p, α, s, d) 19: else 20: S G ← S G -S(p, α, s, d) 21: G ← argmax(G ∈ M, S G ) ▷ Select G with max S G return G</formula><p>extension at all. To overcome this problem, we select facts by assigning them appropriate weights (lines 2-12) and use these weights both to optimise (using weak constraints within causalaba) and rank (possibly several) output extensions (lines 10-20). We discuss this in §4.3.</p><p>Our proposed Algorithm 1 is a sound procedure to extract DAGs given a consistent set of independencies.</p><p>Proposition 4.1. Given a set V of variables and a set of (in)dependencies I, compatible with a (set of) MEC(s), Algorithm 1 outputs a DAG consistent with I.</p><p>In this work, we instantiate our Algorithm 1 using the Majority-PC algorithm (MPC) <ref type="bibr" target="#b7">(Colombo and Maathuis, 2014)</ref> to source facts, resulting in the ABA-PC algorithm.</p><p>In the following subsections, we detail our implementation.</p><p>Remark 4.2. The causal ABAF D T csl from Definition 3.17 is potentially non-flat since assumptions can be derived: independence assumptions as well as arr and ap assumptions may appear in the head of rules. Thus, it lies in a broader ABA class, affecting semantical properties known for flat ABAFs; for instance, complete extensions may not always exist <ref type="bibr" target="#b10">( Čyras et al., 2018;</ref><ref type="bibr" target="#b42">Ulbricht et al., 2024)</ref>. As a consequence, standard ABA solvers are not applicable to our case since they typically focus on the class of flat ABAFs. In this work, we therefore propose an Answer Set Programming (ASP) encoding of our causal ABAF under stable semantics. This also allows us to exploit ASP's grounding abilities to obtain causal ABAFs from concise schemata representations (see <ref type="bibr" target="#b28">(Proietti and Toni, 2022)</ref> for the presentation of ABA in terms of schemata). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Encoding Causal ABA in ASP</head><p>Stable ABA semantics and stable semantics for Logic Programs (LP) are closely related <ref type="bibr" target="#b4">(Caminada et al., 2015;</ref><ref type="bibr" target="#b36">Schulz and Toni, 2015)</ref>; crucially, their correspondence has recently been extended to non-flat instances <ref type="bibr" target="#b32">(Rapberger, Ulbricht, and Toni, 2024)</ref>. In standard ABA-LP translations, assumptions are associated with their default negated contraries: an assumption a ∈ A with contrary a c ∈ L corresponds to the default negated literal not a c . These translations, however, consider only the case where the underlying logical language is atomic. To exploit the full power of ASP, we slightly deviate from standard translations, when appropriate, whilst guaranteeing consistency with our model. We also make use of more descriptive contrary names to enable a more intuitive reading.</p><p>For a set of variables V, we express the causal ABAF by 1. encoding DAGs: each answer set corresponds to a DAG;</p><p>2. encoding d-separation: nodes x and y are independent given Z iff x and y are not linked via an active path.</p><p>Following the standard translation, each ABA atom arr xy is translated to not arr xy . Here, we identify "not arr yx " simply with arrow(x,y) and "not noe xy " with edge(x,y). In our encoding, each answer set corresponds to precisely one DAG of size |V| = d for a given set V of variables. We encode acyclicity and further DAG-specific elements as expected; the encoding is given in the Appendix §B.</p><p>To link causality and DAGs we encode the d-separation criterion. To handle sets in ASP, we encode the (k-th) set S ⊆ V with predicates in(k,x). Module Π col in Listing 1 encodes collider and collider descendant (with natural specifications of the arrow and dpath (directed path) predicates). Next, we introduce non-blocking nodes: node v / ∈ {x, y} in an x-y-path is non-blocking, given Z, iff</p><p>• v is a collider (with respect to its neighbours in the path) and either v ∈ Z or a descendant of v is in Z; or • v is not a collider and v / ∈ Z.</p><p>Lines 3-5 in Module Π col in Listing 1 encode these rules. Now, for each pair x, y ∈ V, for each set S \ {x, y}, for each x-y-path p = v 1 . . . v n with x = v 1 and y = v n , we add rules Π ap (p, (v i ) i≤k ) as specified in Listing 2. This</p><p>Listing 3: Module Π bp (x, y, (p i ) i≤k )</p><p>1 indep(x,y,S) ← (not ap(x,y,pi,S)) i≤k , not in(x,S), not in(y,S), set(S).</p><p>guarantees the 'if '-direction: if x and y are connected via a Z-active path then they are dependent. For the 'only if 'direction, we require Module Π bp (x, y, (p i ) i≤k ): for each pair of variables x and y, we add the rule detailed in the listing to ensure that the absence of an active path between x and y implies independence between them; (p i ) i≤k denotes the list of all paths between x and y. The Module Π bp in Listing 3 encodes the blocked path rules defined in Definition 3.15. The indepand dep-predicates take two variables x, y, and a set S of variables as arguments. We note that, in general, the number of paths between two variables can be exponential (up to ⌊(d -2)!e⌋). To lower the number of paths, we make use of the observation that fixing independence facts amounts to removing edges between nodes.<ref type="foot" target="#foot_1">foot_1</ref> When fixing independence facts (a⊥ ⊥b | X ←), we thus consider only the paths in the skeleton that do not contain (a, b).</p><p>As outlined in Proposition 3.18, fixing dependence facts requires only the addition of the blocked path rules corresponding to the fact. That is, adding the fact (a ̸⊥ ⊥ b | X) only requires including the rules Π ap (p, (v i ) i≤k ) and Π bp (x, y, (p i ) i≤k ) to guarantee correctness.</p><p>As discussed in §3, our proposed ABAF returns all the DAGs compatible with some fixed facts, representing relations amongst nodes, may these be conditional/marginal independencies and/or (un)directed causal relations (arrows and edges). In the proposed instantiation of Causal ABA, ABA-PC, we input a set of facts in the form of independence relations and weight them according to their p-value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Sourcing Facts</head><p>A DAG with d nodes is fully characterised by 1 2 d(d-1)2 d-2 independence relations, growing exponentially in the number of nodes. Therefore, it is not computationally efficient to carry out all possible tests, as in <ref type="bibr" target="#b17">(Hyttinen, Eberhardt, and Järvisalo, 2014)</ref>. Several solutions to this problem have been proposed in the Causal Discovery literature, e.g., <ref type="bibr" target="#b39">(Spirtes, Glymour, and Scheines, 2000;</ref><ref type="bibr" target="#b41">Tsamardinos, Brown, and Aliferis, 2006;</ref><ref type="bibr" target="#b7">Colombo and Maathuis, 2014)</ref>. <ref type="bibr" target="#b39">Spirtes, Glymour, and Scheines (2000)</ref>; <ref type="bibr" target="#b41">Tsamardinos, Brown, and Aliferis (2006)</ref>; <ref type="bibr" target="#b7">Colombo and Maathuis (2014)</ref> all use conditional independence tests such as <ref type="bibr" target="#b11">(Fisher, 1970;</ref><ref type="bibr" target="#b45">Zhang et al., 2011;</ref><ref type="bibr" target="#b15">Gretton et al., 2007)</ref>. Other strategies to recover causal graphs from data, referred to as score-based methods, such as <ref type="bibr" target="#b5">(Chickering, 2002;</ref><ref type="bibr" target="#b29">Ramsey et al., 2017)</ref> involve the use of statistical metrics that measure the addedvalue of adding/removing an arrow in terms of fit to the data. Hence they would return arrow weights. In this work, we use the MPC algorithm <ref type="bibr" target="#b7">(Colombo and Maathuis, 2014)</ref>, which provably 3 recovers the underlying CPDAG from data, to source facts. Let us illustrate the input facts we consider through our running example. Based on this erroneous results, MPC yields the graph shown in Example 1.1 (right), deviating from the ground truth. Crucially, the graph does not capture the independence relations listed above. In fact, there is no graph that satisfies the test results because it is not possible that r and wp are independent conditioned on any set, but r and ws, as well as wp and ws, are dependent. The dependencies indicate a path between r and wp, leading to a contradiction.</p><p>Note that Algorithm 1 is flexible to the choice of facts' source, e.g. we could have used the tests performed by <ref type="bibr" target="#b41">(Tsamardinos, Brown, and Aliferis, 2006)</ref> or, with a slight modification, the arrow weights from <ref type="bibr" target="#b29">(Ramsey et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Weighting Facts</head><p>Here we outline our strategy to weight independence tests results, based on their p-value and the size of the conditioning set. We use these weights as weak constraints and to rank facts and extensions. As a result of using stable semantics, wrong tests can render empty extensions if they contradict another (set of) test(s). Our aim is thus to exclude the wrong tests that create inconsistencies and cause our ABAF to output no extension. To this end, we define a simple heuristic to rank p-values from independence tests, given significance level α, but insensitive to whether they fall below or above it. Firstly, we define the following normalising function:</p><formula xml:id="formula_24">γ(p, α) = 2pα -1 iff p &lt; α 2α-p-1 2(α-1)</formula><p>otherwise Below is a plot of the function γ across the p-value interval, for three commonly chosen levels of α.</p><p>pendence information, see <ref type="bibr" target="#b7">(Colombo and Maathuis, 2014)</ref> for detail and formal definitions. Note that the original PC strategy is based on the assumption that there will be no inconsistencies and therefore the algorithm does not test a pair of variables anymore once an independence is found. However, inconsistencies might arise when erroneous results are obtained.</p><p>The output of γ, for a given α and p, follows the intuition that the most uncertainty is around the significance threshold p = α <ref type="bibr" target="#b38">(Sellke, Bayarri, and Berger, 2001;</ref><ref type="bibr" target="#b2">Berger, 2003)</ref>, which we make correspond to the lowest value of γ = 0.5.</p><p>The final strength of the (in)dependence facts is obtained by weighting the output of the normalising function γ by a factor penalising bigger sizes of the conditioning set Z:</p><formula xml:id="formula_25">S(p, α, s, d) = (1 -s) (d -2) γ(p, α)<label>(1)</label></formula><p>where s = |Z| the cardinality of the conditioning set and d = |V|, the cardinality of the set of nodes in the graph. The reason for weighting γ by s and d follows the intuition that the accuracy of independence test lowers as the conditioning set size increases <ref type="bibr" target="#b38">(Sellke, Bayarri, and Berger, 2001)</ref>. We use our final weights S to rank the test carried out by MPC. Then, our strategy is simple: exclude an incremental number of the lowest ranked tests until the returned extension is not empty. Let us illustrate our strategy. We thus start excluding the test with the lowest strength and progressively more until we find a model. In this example, the right DAG is obtained by excluding 9 of the performed tests, including the bottom five of the above list, and keeping the 14 strongest ones.</p><p>Here, we obtain exactly one DAG when excluding 40% of the tests carried out by MPC. If we obtain multiple models, we score each of them as in Algorithm 1, lines 14-19. In addition, we encode the (in)dependence facts as weak constraints, treated as optimisation statements <ref type="bibr" target="#b13">(Gebser, Kaminski, and Schaub, 2011)</ref>, to sort out sub-optimal extensions.</p><p>Our weighting function is similar to the one proposed by <ref type="bibr" target="#b3">Bromberg and Margaritis (2009)</ref>, with two differences: we re-base around 0.5 instead of 1 -α, to allow for more discrimination; and use the conditioning set size irrespective of the test's result, instead of including it only in the case of dependence (trusting that p-values accurately reflect the probability of wrongly rejecting the null hypothesis).</p><p>We emphasize that classical independence tests are asymmetric in nature, and inference of dependence is only possible if there is enough evidence against the null hypothesis (p &lt; α) with an expected Type I error (rejecting independence when it is true) corresponding to α. Conversely, no inference is possible when the p ≥ α, when the null hypothesis cannot be rejected, since p-values are distributed uniformily in [0, 1] under H 0 . As pointed out in <ref type="bibr" target="#b3">(Bromberg and Margaritis, 2009;</ref><ref type="bibr" target="#b17">Hyttinen, Eberhardt, and Järvisalo, 2014)</ref>, using p-values directly as strength is common but neither sound nor consistent. Transforming p-values to probability estimates, e.g. as in <ref type="bibr" target="#b18">(Jabbari et al., 2017;</ref><ref type="bibr" target="#b6">Claassen and Heskes, 2012;</ref><ref type="bibr" target="#b40">Triantafillou, Tsamardinos, and Roumpelaki, 2014)</ref>, would address this point, but out of scope for this work. A possible alternative to weighting and excluding facts might also be the use of less strict ABA semantics, left out of our experiments since not available in the ASP implementation used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Empirical Evaluation</head><p>We evaluate our ABA-PC algorithm on four datasets from the bnlearn repository <ref type="bibr" target="#b37">(Scutari, 2014)</ref>, which hosts commonly used benchmarks in Causal Discovery, some of which based on real published experiments or expert opinions. We use the Asia, Cancer, Earthquake and Survey datasets, which represent problems of decision making in the medical, law and policy domains (see Appendix §C.1 for details). Implementation and computing infrastructure details, including code to reproduce the experiments, are in Appendix §C.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation Metrics and Baselines</head><p>For evaluation, we use a prominent metric in causal discovery: Structural Interventional Distance (SID) <ref type="bibr" target="#b25">(Peters and Bühlmann, 2015)</ref> measures the deviation in the causal effects estimation deriving from a mistake in the estimated graph. SID works as a "downstream task" error rate for the causal inference task, which has causal graphs as a pre-requisite. We calculate SID between the estimated and the true CPDAG and repeat the experiments 50 times per dataset to record confidence intervals. Given that a CPDAG is a mixed graph, SID is calculated for the worst and best scenarios. In order to compare across graphs with different number of edges, we normalise SID (NSID) dividing it by the number of edges in the true DAG. NSID can go above 100% since extra edges could be introduced in the structure. We provide details on the metrics in Appendix §C.3 and results based on additional metrics (SHD, F1 score, precision and recall) in Appendix §C.5.</p><p>We compare ABA-PC to four baselines: a Random sample of graphs of the right dimensions (V,E); Fast Greedy Search (FGS) <ref type="bibr" target="#b29">(Ramsey et al., 2017)</ref> and NOTEARS-MLP <ref type="bibr" target="#b47">(Zheng et al., 2020)</ref> which use, respectively, the Bayesian Information Criterion and Multilayer Perceptrons with a continuous formulation of acyclicity to optimise the graph's fit to the data; and MPC <ref type="bibr" target="#b7">(Colombo and Maathuis, 2014)</ref>. <ref type="foot" target="#foot_3">4</ref> More details are provided in Appendix §C.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>The results of our experiments are in Fig. <ref type="figure" target="#fig_8">2</ref>. Best and worst SID are in the (Low, resp. High) sections for each dataset; the number of edges and nodes in each dataset is given below the x-axis labels. ABA-PC ranks 1 st in the worst case SID (High) for all datasets. It performs significantly (w.r.t. t-tests of difference in means, see §C.6) better than all baselines on three out of four datasets (Cancer, Earhquake and Asia) and is on par with NOTEARS-MLP for the Survey data. Furthermore, ABA-PC performs significantly better than MPC for all datasets. This demonstrates how, with the same underlying information from the data, our proposed method returns more accurate CPDAGs in the worst case scenario. For the best case SID (Low), ABA-PC is significantly better than all baselines for two datasets (Earthquake and Asia). Overall, we observe that ABA-PC performs well on benchmark data compared to a varied selection of baselines from the literature. Scalability In Fig. <ref type="figure" target="#fig_9">3</ref> we show the elapsed time (on a log scale) by the number of nodes. This are the recorded times for the experiments in Fig. <ref type="figure" target="#fig_8">2</ref> with 50 repetitions per dataset. As we can see, ABA-PC is the least efficient method. The main reasons for this are the complexity of both the grounding of logical rules and the calculation of the extensions, which clingo carries out exactly and efficiently, but still constitute a bottleneck. We already identified avenues of future work, discussed next, to address scaling limitations of our implementation, given the promising results shown in Fig. <ref type="figure" target="#fig_8">2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We proposed a novel argumentation-based approach to Causal Discovery, targeting the resolution of inconsistencies in data, and showed that it outperforms existing statisticsbased methods on four (standard) datasets. Our approach uses independence tests and their p-values to narrow down DAGs most fitting to the data, drawn from stable extensions of ABA frameworks. Other methods to identify and resolve inconsistencies in data for causal discovery have been proposed, e.g. by <ref type="bibr" target="#b30">Ramsey, Spirtes, and Zhang (2006)</ref>; <ref type="bibr" target="#b7">Colombo and Maathuis (2014)</ref>, but they focus on marking orientations as ambiguous in the presence of inconsistencies, rather than actually resolving the inconsistencies as we do.</p><p>Our proposed framework allows for the introduction of weighted arrows and edges, on top of independencies, which would allow to integrate, as future work, other data-centric methods like score-based causal discovery algorithms (e.g. <ref type="bibr" target="#b5">(Chickering, 2002;</ref><ref type="bibr" target="#b29">Ramsey et al., 2017;</ref><ref type="bibr" target="#b6">Claassen and Heskes, 2012)</ref>. As for the scalability, we cannot process more than 10 variables at the current state. We are currently working on making the processing of extensions more efficient and on incremental solving to avoid re-grounding when deleting independence facts. Additionally, we would like to extend our approach to deal with latent confounders, in line with <ref type="bibr" target="#b8">(Colombo et al., 2012;</ref><ref type="bibr" target="#b17">Hyttinen, Eberhardt, and Järvisalo, 2014)</ref> and cycles <ref type="bibr" target="#b31">(Rantanen, Hyttinen, and Järvisalo, 2020;</ref><ref type="bibr" target="#b33">Richardson and Spirtes, 1999;</ref><ref type="bibr" target="#b17">Hyttinen, Eberhardt, and Järvisalo, 2014)</ref> and experiment with other argumentation semantics in the literature, making use of a recently developed solver for non-flat ABA <ref type="bibr" target="#b21">(Lehtonen et al., 2024)</ref>. Finally, we plan to explore the explainability capabilities intrinsic in an ABA framework <ref type="bibr">( Čyras et al., 2018)</ref>, which we believe may bring great value to causal discovery in a collaborative human-AI discovery process <ref type="bibr" target="#b34">(Russo and Toni, 2023)</ref>.</p><formula xml:id="formula_26">A Proofs of Section 3 Proposition 3.3. {(V, S ∩ A arr ) | S ∈ σ(D dag )} = {G | G is a DAG} for σ ∈ {co, pr , stb}.</formula><p>Proof. By the well-known relations between the semantics <ref type="bibr" target="#b1">(Baroni, Caminada, and Giacomin, 2018)</ref>, i.e., cf (D) ⊇ ad (D) ⊇ co(D) ⊇ pr (D) ⊇ stb(D) and cf (D) ⊇ stb(D), it suffices to prove the statement for conflict-free and stable semantics. Proof. Since pr (D) ⊇ stb(D) it suffices to show pr (D dag ) ⊆ stb(D dag ). Let S ∈ pr (D dag ). The main observation is that for each pair of variables x, y ∈ V , either noe xy , arr xy or arr yx is contained in S. Towards a contradiction, suppose there exists a pair of variables such that S ∩ {noe xy , arr xy , arr yx } = ∅. It is easy to see that S∪{noe xy } is admissible (noe xy is only attacked by the corresponding arrows and defends itself against these attacks); contradiction to S being a ⊆-maximal admissible set. We obtain pr (D dag ) = stb(D dag ).</p><formula xml:id="formula_27">• (σ = cf ) (⊆) Let S ∈ cf (D dag ).</formula><p>Below, we make use of the following definition. Definition A.1. For a set of assumptions S, we let G(S) = S ∩ A arr denote the graph corresponding to S.</p><formula xml:id="formula_28">Proposition 3.11. Let σ ∈ {pr , stb}, S ∈ σ(D ds ), x, y ∈ V, Z ⊆ V \ {x, y}. Then (x⊥ ⊥y | Z) ∈ S iff (x ⊥ ⊥ G y | Z).</formula><p>Proof. By Lemma 3.5 it suffices to prove the proposition for stable semantics.</p><p>(⇒) Suppose x ⊥ ⊥ y | Z ∈ S. Towards a contradiction, suppose x ̸⊥ ⊥ y | Z in G(S). Then there exists a Z-active path p between x and y in G(S). Then there is a Z-active xy-collider-tree t where p is the connecting path between x and y (i.e., p = p t ). By definition of the graph G(S), each directed arrow in p is contained in S. Hence, S ⊢ x ⊥ ⊥ y | Z can be derived, contradiction to conflict-freeness of S.</p><p>(⇐) For the other direction, suppose x ⊥ ⊥ y | Z in G(S). We show that each set of assumptions T with T ⊢ x ⊥ ⊥ y | Z is attacked. Consider a Z-active x-y-collider-tree t. Let p t denote the path corresponding to t. Note that p t / ∈ S, otherwise, x and y are d-connected given Z. Let (a, b) ∈ p t denote the arrow which is not contained in S. Since S is stable, it holds that {arr ab , arr ba , noe ab } ∩ S = ∅. Therefore, either noe ab ∈ S or arr ba ∈ S. Therefore, p t is attacked by S. Since t was arbitrary, we obtain x ⊥ ⊥ y | Z is defended by S and therefore x ⊥ ⊥ y | Z ∈ S.</p><formula xml:id="formula_29">Proposition 3.13. Let S ∈ co(D ds ), x, y ∈ V, Z ⊆ V \ {x, y}. Then (x⊥ ⊥y | Z) ∈ S implies (x⊥ ⊥ G y | Z). Proof. Suppose x ⊥ ⊥ y | Z ∈ S. Towards a contradiction, suppose x ̸⊥ ⊥ y | Z in G(S).</formula><p>Then there exists a Z-active path p between x and y in G(S). Then there is a Z-active x-y-collider-tree t where p = p t is the connecting path between x and y. By definition of the graph G(S), each directed arrow in p t is contained in S. Hence, S ⊢ x ⊥ ⊥ y | Z can be derived, contradiction to conflict-freeness of S.</p><p>Proposition 3.14. Let x, y, a, b ∈ V, Z ⊆ V \ {x, y}, X ⊆ V \ {a, b}, σ ∈ {pr , na, ss, stg, stb}, r ∈ {(x⊥ ⊥y | Z ←), (arr xy ←)}. For each S ∈ σ(D ds ∪ {r}), it holds that</p><formula xml:id="formula_30">(a⊥ ⊥b | X) ∈ S iff (a⊥ ⊥ G b | X).</formula><p>Proof. The proof is analogous to the proof of Proposition 3.11. The rule r ensures that S contains the assumption</p><formula xml:id="formula_31">x ⊥ ⊥ y | Z resp. arr xy . Proposition 3.18. Let x, y, a, b ∈ V, let Z ⊆ V \ {x, y} and X ⊆ V \ {a, b}, let σ ∈ {pr , stb} and let S ∈ σ(D xyZ csl ). It holds that (a⊥ ⊥b | X) ∈ S iff (a⊥ ⊥ G b | X).</formula><p>Proof. By Lemma 3.5 it suffices to prove the proposition for stable semantics. Let r = x ⊥ ⊥ y | Z ←.</p><p>(⇒) This direction is analogous to the proof of Proposition 3.11.</p><p>(⇐) For the other direction, suppose a ⊥ ⊥ b | X in G(S). We proceed by case distinction.</p><p>Case 1 (a = x, b = y, X = Z) Towards a contradiction, suppose x ⊥ ⊥ y | Z / ∈ S. Since S is closed, there is some x-y-path p such that bp p|Z / ∈ S (otherwise, S ⊢ x ⊥ ⊥ y | Z, contradiction). Therefore, S attacks bp p|Z . This is the case if S derives ap p|Z . By definition of D csl , this is the case if S contains an Z-active x-y-collider-tree t. Consequently, x and y are d-connected given Z, contradiction to x ⊥ ⊥ y | Z in G(S).</p><p>Case 2 (a ̸ = x or b ̸ = y or X ̸ = Z) Analogous to the proof of Proposition 3.11.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B ASP Encodings: Additional Details</head><p>We recall the standard translation from ABA to LP. We assume that the ABA has precisely one contrary for each assumption. Given an ABAF D = (L, A, R, ) and an atom p ∈ L, we let</p><formula xml:id="formula_32">rep(p) = not p, if p ∈ A a, if p = a ∈ A.</formula><p>We extend the operator to ABA rules element-wise: Following the standard ABA to LP translation, each ABA atom arr xy is translated to not arr xy . With a slight deviation from the causal ABAF, we identify "not arr yx " simply with arrow(x,y) and "not noe xy " with edge(x,y). Listing 4 contains our DAG encoding (as expected, var specifies variables). In Line 1, we guess one an arrow or the absence of an arrow; the remaining lines enforce that the graph is acyclic. The code faithfully captures the basis of our causal ABAF: each answer set of Module Π dag in Listing 4 corresponds to precisely one DAG of size |V | for a given set V of variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Experimental Details</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 bnlearn datasets</head><p>For our empirical evaluation (see §5), we used four datasets from the bnlearn repository. <ref type="foot" target="#foot_4">5</ref> This repository is widely used for research in Causal Discovery and hosts a number of commonly used benchmarks, some of which result from real published experiments or from the collection of expert opinions on the causal graph and the conditional probability tables necessary to create a Bayesian Network. Specifically, we use the Asia, Cancer, Earthquake and Survey datasets, as reported in reported in Table <ref type="table" target="#tab_5">1</ref>, which represent problems of decision making in the medical, law and policy domains. The datasets are from the small categories with number of nodes varying from 5 to 8. Details on the number of nodes, edges and density of the DAGs underlying the Bayesian Networks, together with links to a more detailed description on the bnlearn repository can be found in Table <ref type="table" target="#tab_5">1</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Implementation Details</head><p>We provide an implementation of ABA-PC using clingo <ref type="bibr" target="#b12">(Gebser et al., 2019)</ref> version 5.6.2 and python 3.10. The code is available at the following repository: <ref type="url" target="https://github.com/briziorusso/ArgCausalDisco">https://github.com/briziorusso/ArgCausalDisco</ref>.</p><p>In the repository, we also made available the code to reproduce all experiments and we saved all the plots, presented herein and in the main text, in HTML format. <ref type="foot" target="#foot_6">7</ref> Downloading and opening them in a browser allows the inspection of all the numbers behind the plots in an interactive way.</p><p>Hyperparameters We used default parameters for all the methods. For MPC and ABAPC (ours) we used Fisher Z test <ref type="bibr" target="#b11">(Fisher, 1970)</ref>, as implemented in causal-learn, with significance threshold α = 0.05.</p><p>Computing infrastructure Our proposed method, together with MPC and FGS do not benefit from GPU accelleration. All the results were ran on Intel(R) Xeon(R) w5-2455X CPU with 4600 max MHz and 128GB of RAM. The NOTEARS-MLP was ran on NVIDIA(R) GeForce RTX 4090 GPU with 24GB dedicated RAM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Evaluation Metrics</head><p>We evaluated the estimated graphs with five commonly used metrics in causal discovery (as in e.g. <ref type="bibr" target="#b47">(Zheng et al., 2020;</ref><ref type="bibr" target="#b20">Lachapelle et al., 2020;</ref><ref type="bibr" target="#b29">Ramsey et al., 2017;</ref><ref type="bibr" target="#b7">Colombo and Maathuis, 2014</ref> SID was proposed in <ref type="bibr" target="#b25">(Peters and Bühlmann, 2015)</ref> and quantifies the agreement to a causal graph in terms of interventional distributions. It aims at quantifying the incorrect causal inference estimations stemming out of a mistake in the causal graph estimation, akin to a downstream task error on a pre-processing step. SHD is a graphical metric that counts the number of mistakes present in an estimated directed graph compared to a ground truth one. In particular it sums the extra edges, the missing ones and the wrong orientations. The lower the SHD the better. In the formula, Extra (E) is the set of extra edges and Missing (M) are the ones missing from the skeleton of the estimated graph. Reversed (R) are directed edges with incorrect direction. Precision and Recall measure the proportion of correct orientations based on the estimated graph and the true one, respectively. In the formulae, True Positive (TP) is the number of estimated edges with correct direction; False Positive (FP) is an edge which is not in the skeleton of the true graph. True Negative (TN) and False Negative (FN) are edges that are not in the true graph and correctly (resp, incorrectly) removed from the estimated graph. Finally, F1 score is the harmonic mean of precision and recall.</p><p>We carried out the evaluation in the main text on CPDAGs. As discussed in §2, CPDAGs represent Markov Equivalence Classes. MECs are all that can be inferred from a given set of independence relations, since multiple DAGs can entail the same set of independencies. Both our method and NOTEARS-MLP output DAGs rather than CPDAGs (which are the output of the other two baselines used, FGS and MPC). For the methods returning DAGs, we first transform the DAG to a CPDAG (isolating skeleton and v-structures) and then calculate the metrics presented above. Evaluation on DAGs is provided in §C.5 for completeness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4 Baselines</head><p>We used the following four baselines with respective implementations (see Section 5 for context):</p><p>• A Random baseline (RND) as in <ref type="bibr" target="#b20">(Lachapelle et al., 2020)</ref>, by just sampling 10 random graphs with the same number of nodes and edges as the ground truth.  <ref type="bibr" target="#b39">(Spirtes, Glymour, and Scheines, 2000)</ref> that renders it order-independent while mantaining soundness and completeness with infinite data.</p><p>• Fast Greedy equivalence Search<ref type="foot" target="#foot_8">foot_8</ref> (FGS) <ref type="bibr" target="#b29">(Ramsey et al., 2017</ref>) is a score-based Causal Discovery algorithm. It is a fast implementation of GES <ref type="bibr" target="#b5">(Chickering, 2002)</ref> where graphs are evaluated using the Bayesian Information Criterion (BIC) upon addition or deletion of an edge, in a greedy fashion, involving the evaluation of insertion and removal of edges in a forward and backward fashion. Its output is a CPDAG.</p><p>• NOTEARS-MLP<ref type="foot" target="#foot_9">foot_9</ref> (NT) <ref type="bibr" target="#b47">(Zheng et al., 2020</ref>) learns a non-linear SEM via continuous optimisation. Having a Multi-Layer Perceptron (MLP) at its core, this method should adapt to different functional dependencies among the variables. The optimisation is carried out via augmented Lagrangian with a continuous formulation of acyclicity <ref type="bibr" target="#b46">(Zheng et al., 2018)</ref>, outputting a DAG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.5 Additional Results</head><p>Here we provide results that complement the ones in §5 in the main text. Specifically, we evaluate our method based on SHD, F1 score, Precision and Recall (see §C.3 for details).</p><p>Additional Metrics Additionally to the results shown in Fig. <ref type="figure" target="#fig_8">2</ref> in the main text, we evaluate our proposed method with four other metrics commonly used in Causal Discovery: SHD, F1 (Fig. <ref type="figure" target="#fig_13">4</ref>), Precision and Recall (Fig. <ref type="figure" target="#fig_14">5</ref>). Furthermore, we show the average size of the estimated CPDAGs in (Fig. <ref type="figure" target="#fig_15">6</ref>). From Fig. <ref type="figure" target="#fig_13">4</ref>, we can see that, according to Normalised SHD, ABA-PC is the best method for the Earthquake dataset and not significantly different from MPC (ranking first) for the other datasets. In terms of F1 score, ABA-PC is significantly better than all baselines for the Earthquake and Asia datasets, on par with all the others for Cancer and ranking second, after MPC, for the Survey dataset. Precision and Recall provide a breakdown of the F1 score (which is their harmonic mean), hence follows similar patterns. From the estimated graph size (the number of edges in the estimated graph) in Fig. <ref type="figure" target="#fig_15">6</ref> we can observe that ABA-PC is generally in line with the true graph size, apart from the Survey dataset for which some of the edges are missed.</p><p>DAGs Evaluation In the main text, we evaluated our method based on the estimated CPDAG compared to the ground truth one. Here, for completeness, we report results based on DAGs. Indeed, both our method and NOTEARS-MLP have DAGs in output. Note that this evaluation penalises the methods outputting CPDAGs. In transforming CPDAGs to DAGs, we had to only select the directed arrows in order to extract DAGs from the output CPDAGs (see Fig. <ref type="figure" target="#fig_15">6</ref> and 7 for the average size of the estimated CPDAGs and DAGs, respectively). We report the evaluation of DAGs in Fig. <ref type="figure" target="#fig_17">8</ref> (NSID and NSHD) and Fig. <ref type="figure" target="#fig_18">9</ref> (Precision and Recall). The plot of the F1 score is provided in the our repository both as image and interactive files.<ref type="foot" target="#foot_10">foot_10</ref> According to NSHD, we can see that ABA-PC performs significantly better than all baselines on two out of the four datasets (Survey and Survey). For the Earthquake and Asia datasets ABA-PC is on par with MPC and significantly better than the other baselines. According to NSID, ABA-PC is not significantly different than MPC, ranking 1 st for the Earthquake and Asia datasets, and in line with all other baselines for the other two datsets. According to precision and recall, ABAPC is better than MPC for three out of the four datasets when evaluating on recall, while maintaining the same precision, again, three out of four times. For the Asia dataset ABAPC trades some precision for an significantly higher recall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.6 Statistical Tests</head><p>Here we present details of the statistical tests used to measure the significance of the difference in the results presented in Fig. <ref type="figure" target="#fig_8">2</ref> in the main text. In tables 2, 3, 4 and 5 we provide t-statistics and p-values for the Cancer, Earthquake, Survey and Asia datasets, respectively. In each table we present pairwise comparisons of means (shown in brakets together with standard deviations), for the best and worst case SID (High and Low, resp.) presented in Fig. <ref type="figure" target="#fig_8">2</ref> of the main text.       </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method (mean±std)</head><p>t p-value NSID (low) APC (9.8 ± 2.2)vFGS (6.3 ± 1.9) 8.722 0.000*** APC (9.8 ± 2.2)vMPC (3.9 ± 1.8) 14.833 0.000*** APC (9.8 ± 2.2)vNT (9.3 ± 1.6) 1.476 0.144 APC (9.8 ± 2.2)vRND (7.9 ± 3.6)</p><p>3.169 0.002** FGS (6.3 ± 1.9)vMPC (3.9 ± 1.8) 6.503 0.000*** FGS (6.3 ± 1.9)vNT (9.3 ± 1.6) -8.706 0.000*** FGS (6.3 ± 1.9)vRND (7.9 ± 3.6) -2.871 0.005** MPC (3.9 ± 1.8)vNT (9.3 ± 1.6) -16.024 0.000*** MPC (3.9 ± 1.8)vRND (7.9 ± 3.6) -7.078 0.000*** NT (9.3 ± 1.6)vRND (7.9 ± 3.6) 2.401 0.019* NSID (high) APC (11.0 ± 2.1)vFGS (16.3 ± 1.9) -13.231 0.000*** APC (11.0 ± 2.1)vMPC (13.9 ± 1.8) -7.315 0.000*** APC (11.0 ± 2.1)vNT (10.2 ± 0.4) 2.794 0.007** APC (11.0 ± 2.1)vRND (14.0 ± 3.0) -5.801 0.000*** FGS (16.3 ± 1.9)vMPC (13.9 ± 1.8) 6.503 0.000*** FGS (16.3 ± 1.9)vNT (10.2 ± 0.4) 22.465 0.000*** FGS (16.3 ± 1.9)vRND (14.0 ± 3.0) 4.442 0.000*** MPC (13.9 ± 1.8)vNT (10.2 ± 0.4) 14.130 0.000*** MPC (13.9 ± 1.8)vRND (14.0 ± 3.0) -0.321 0.749 NT (10.2 ± 0.4)vRND (14.0 ± 3.0) -8.934 0.000*** </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method (mean±std) t p-value NSID (low)</head><p>APC (0.0 ± 0.0)vFGS (4.6 ± 1.4) -23.742 0.000*** APC (0.0 ± 0.0)vMPC (5.0 ± 0.0) -inf 0.000*** APC (0.0 ± 0.0)vNT (4.6 ± 3.3) -9.741 0.000*** APC (0.0 ± 0.0)vRND (7.9 ± 2.9) -19.347 0.000*** FGS (4.6 ± 1.4)vMPC (5.0 ± 0.0) -2.065 0.044* FGS (4.6 ± 1.4)vNT (4.6 ± 3.3) 0.079 0.937 FGS (4.6 ± 1.4)vRND (7.9 ± 2.9) -7.272 0.000*** MPC (5.0 ± 0.0)vNT (4.6 ± 3.3) 0.940 0.352 MPC (5.0 ± 0.0)vRND (7.9 ± 2.9) -7.071 0.000*** NT (4.6 ± 3.3)vRND (7.9 ± 2.9) -5.351 0.000*** NSID (high) APC (9.3 ± 4.5)vFGS (15.0 ± 0.5) -8.796 0.000*** APC (9.3 ± 4.5)vMPC (15.0 ± 0.0) -8.847 0.000*** APC (9.3 ± 4.5)vNT (13.5 ± 4.0) -4.812 0.000*** APC (9.3 ± 4.5)vRND (13.9 ± 3.5) -5.649 0.000*** FGS (15.0 ± 0.5)vMPC (15.0 ± 0.0) 0.000 1.000 FGS (15.0 ± 0.5)vNT (13.5 ± 4.0)</p><p>2.669 0.010* FGS (15.0 ± 0.5)vRND (13.9 ± 3.5) 2.266 0.028* MPC (15.0 ± 0.0)vNT (13.5 ± 4.0)</p><p>2.689 0.010** MPC (15.0 ± 0.0)vRND (13.9 ± 3.5) 2.289 0.026* NT (13.5 ± 4.0)vRND (13.9 ± 3.5) -0.558 0.578 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method (mean±std)</head><p>t p-value NSID (low) APC (14.2 ± 3.6)vFGS (13.7 ± 3.2) 0.819 0.415 APC (14.2 ± 3.6)vMPC (5.1 ± 3.4) 12.854 0.000*** APC (14.2 ± 3.6)vNT (17.0 ± 0.0) -5.386 0.000*** APC (14.2 ± 3.6)vRND (13.1 ± 4.2)</p><p>1.405 0.163 FGS (13.7 ± 3.2)vMPC (5.1 ± 3.4) 12.940 0.000*** FGS (13.7 ± 3.2)vNT (17.0 ± 0.0) -7.450 0.000*** FGS (13.7 ± 3.2)vRND (13.1 ± 4.  Method (mean±std) t p-value SID (low) APC (11.7 ± 6.8)vFGS (32.3 ± 5.0) -17.164 0.000*** APC (11.7 ± 6.8)vMPC (15.2 ± 5.6) -2.828 0.006** APC (11.7 ± 6.8)vNT (16.5 ± 4.2) -4.243 0.000*** APC (11.7 ± 6.8)vRND (25.0 ± 7.0) -9.622 0.000*** FGS (32.3 ± 5.0)vMPC (15.2 ± 5.6) 15.960 0.000*** FGS (32.3 ± 5.0)vNT (16.5 ± 4.2) 16.895 0.000*** FGS (32.3 ± 5.0)vRND (25.0 ± 7.0) 5.970 0.000*** MPC (15.2 ± 5.6)vNT (16.5 ± 4.2) -1.290 0.200 MPC (15.2 ± 5.6)vRND (25.0 ± 7.0) -7.690 0.000*** NT (16.5 ± 4.2)vRND (25.0 ± 7.0) -7.322 0.000*** SID (high) APC (33.5 ± 7.9)vFGS (41.7 ± 2.8) -6.883 0.000*** APC (33.5 ± 7.9)vMPC (41.0 ± 3.8) -6.046 0.000*** APC (33.5 ± 7.9)vNT (41.1 ± 5.0) -5.739 0.000*** APC (33.5 ± 7.9)vRND (37.2 ± 6.1) -2.633 0.010** FGS (41.7 ± 2.8)vMPC (41.0 ± 3.8)</p><p>1.023 0.309 FGS (41.7 ± 2.8)vNT (41.1 ± 5.0) 0.745 0.458 FGS (41.7 ± 2.8)vRND (37.2 ± 6.1) 4.702 0.000*** MPC (41.0 ± 3.8)vNT (41.1 ± 5.0) -0.091 0.928 MPC (41.0 ± 3.8)vRND (37.2 ± 6.1)</p><p>3.732 0.000*** NT (41.1 ± 5.0)vRND (37.2 ± 6.1)</p><p>3.478 0.001***</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overview of the workflow of our Causal ABA algorithm, which combines statistical methods and expert domain knowledge with non-monotonic reasoning and performs argumentative reasoning to output causal graphs consistent with the reported causal relationships.</figDesc><graphic coords="2,74.16,54.00,463.68,119.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>•</head><label></label><figDesc>{a, c} collectively attacks b as {a, c} ⊢ p; • c attacks a since {c} ⊢ s.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>where T is a set of (in)dependence and arrow facts, the ABAF obtained by the iterative update of the ABAF D ds with D xyZ csl for all dependence facts (x⊥ ⊥y | Z ←) ∈ T . Corollary 3.19. Let T be a set of (in)dependence and arrow facts, σ ∈ {pr , stb}, x, y ∈ V, Z ⊆ V \ {x, y} and S ∈ σ(D T csl ). Then (x⊥ ⊥y | Z) ∈ S iff (x ⊥ ⊥ G y | Z).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>T</head><label></label><figDesc>← T + [(dep(x, y, Z), S(p, α, s, d))] 8: T ← sort(T, S) ▷ Sort elements of T by strength S 9: M = causalaba(d, T) 10: while M = ∅ do 11: T ← T[2 . . . |T|] ▷ Drop fact with lowest S 12: M = causalaba(d, T) 13: for G ∈ M do 14: S G = 0 15:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Module Π ap (p, (v i ) i≤k ) 1 ap(v1,v k ,p,S) ← (arrow(vi,vi+1)) i&lt;k , not in(v1,S), not in(v k ,S), set(S), (nb(vi,vi-1,vi+1,S)) 1&lt;i&lt;k . 2 dep(v1,v k ,S) ← ap(v1,v k ,p,S).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>Example 4.3. We run the MPC algorithm in Example 1.1 which performs 23 out of 24 tests, including the following. r⊥ ⊥wp | {ws} wp⊥ ⊥ws | {r} r⊥ ⊥wp r⊥ ⊥wp | {wr} wp ̸⊥ ⊥ws | {r, wr} r⊥ ⊥wp | {wr, ws} However, only r ⊥ ⊥ wp is correct; the only other independence wp⊥ ⊥ G ws | {r, wr} in G is wrongly classified. All other tests result in dependencies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Example 4.4. Consider again Example 1.1. The results of the independence tests from MPC (using Fisher's Z (1970) and α = 0.05) have the following p-values (we show the same subset of the 23 tests carried out, as in Example 4.3): r ⊥ ⊥ wp p = 0.45 S = 0.71 r ⊥ ⊥ wp | {ws} p = 0.52 S = 0.37 r ⊥ ⊥ wp | {wr} p = 0.33 S = 0.32 wp ⊥ ⊥ ws | {r} p = 0.05 S = 0.25 r ⊥ ⊥ wp | {wr, ws} p = 0.39 S = 0.00 wp ̸⊥ ⊥ ws | {r, wr} p = 0.03 S = 0.00 We apply Eq. 1 to calculate S. Ranking tests by S, as shown above, the right test is the highest scoring one. Fixing all the tests returns no solution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Normalised Structural Interventional Distance for four datasets from the bnlearn repository. Lower is better. Low (resp. High) is the SID for the best (resp. worst) DAG in the estimated CPDAG.</figDesc><graphic coords="9,59.04,53.99,493.91,223.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Mean and Standard Deviation of the elapsed time in log scale by number of nodes averaged over 50 runs.</figDesc><graphic coords="10,57.33,54.00,231.84,81.99" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>By definition, S cannot contain cycles; hence, S corresponds to a DAG. (⊇) Let G = (V, E) be a DAG. Let S = {arr x,y | (x, y) ∈ E}. By acyclicity of G, we obtain that S is conflict-free. • (σ = stb) (⊆) Let S ∈ stb(D dag ). By definition, S cannot contain cycles; hence, S corresponds to a DAG. (⊇) Let G = (V, E) be a DAG. Let S = {arr x,y | (x, y) ∈ E} ∪ {noe xy | (x, y), (y, x) / ∈ E}. Note that S contains exactly one of arr xy , arr yx , noe xy (since G does not contain a bi-directed arrow and S contains noe xy only if x and y are not linked in G). Therefore, S attacks each assumption which is not contained in S. Moreover, S does not contain cycles by acyclicity of G. Therefore, S is conflict-free. Lemma 3.5. σ(D dag ) = τ (D dag ) for σ, τ ∈ {pr , stb}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>rep(r) = rep(head(r)) ← {rep(p) | r ∈ body(r)}. For an LP-ABAF D = (L, R, A, ), we define the associated LP P D = {rep(r) | r ∈ R}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>)): • Structural Intervention Distance (SID) • Structural Hamming Distance (SHD) = E + M + R • Precision = TP/(TP + FP) • Recall = TP/(TP + FN) • F1 = 2×Precision*Recall/(Precision+Recall)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Normalised Structural Hamming Distance (SHD, left y-axis) and F1 score (right y-axis) for the estimated CPDAGs for four datasets in the bnlearn repository. Lower is better for NSHD and higher is better for F1.</figDesc><graphic coords="16,54.00,90.23,503.95,217.85" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Precision (left y-axis) and Recall (right y-axis) for the estimated CPDAGs for four datasets in the bnlearn repository. Higher is better for both metrics.</figDesc><graphic coords="16,54.00,418.40,503.98,217.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Number of edges in the estimated CPDAGs compared to ground truth.</figDesc><graphic coords="17,54.00,98.19,504.01,212.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Number of edges in the estimated DAGs compared to ground truth.</figDesc><graphic coords="17,54.00,427.09,504.00,210.79" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Normalised Structural Hamming Distance (SHD, left y-axis) and Structural Intervention Distance (SID, right y-axis) for the estimated DAGs for four datasets in the bnlearn repository. Lower is better for both metrics.</figDesc><graphic coords="18,54.00,89.41,504.00,219.65" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Precision (left y-axis) and Recall (right y-axis) for the estimated DAGs for four datasets in the bnlearn repository. Higher is better for both metrics.</figDesc><graphic coords="18,54.00,417.74,503.99,218.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head></head><label></label><figDesc>1 ± 3.4)vNT (17.0 ± 0.0) -24.593 0.000*** MPC (5.1 ± 3.4)vRND (13.1 ± 4.2) -10.490 0.000*** NT (17.0 ± 0.0)vRND (13.1 ± 4.2) 6.595 0.000*** NSID (high) APC (16.4 ± 2.8)vFGS (20.7 ± 3.5) -6.764 0.000*** APC (16.4 ± 2.8)vMPC (20.1 ± 4.4) -5.065 0.000*** APC (16.4 ± 2.8)vNT (17.0 ± 0.0) -1.481 0.145 APC (16.4 ± 2.8)vRND (22.4 ± 4.4) -8.092 0.000*** FGS (20.7 ± 3.5)vMPC (20.1 ± 4.4) 0.686 0.494 FGS (20.7 ± 3.5)vNT (17.0 ± 0.0) 7.480 0.000*** FGS (20.7 ± 3.5)vRND (22.4 ± 4.4) -2.151 0.034* MPC (20.1 ± 4.4)vNT (17.0 ± 0.0) 5.060 0.000*** MPC (20.1 ± 4.4)vRND (22.4 ± 4.4) -2.560 0.012* NT (17.0 ± 0.0)vRND (22.4 ± 4.4) -8.633 0.000***</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>to arr xz represent the attack from set {arr yx , arr xz , arr zy } on the assumption arr xz based on the derivation {arr yx , arr xz , arr zy } ⊢ arr xz .</head><label></label><figDesc>a, b ∈ {arr xy , arr yx , noe xy }, x, y ∈ V;• arr xixi+1 ← arr x1x2 , . . . , arr x k-1 x k for each sequence x 1 . . . x k with x 1 = x k , for each 1 ≤ i &lt; k.Intuitively, noe xy stands for "no edge between x and y." Note that we define only one atom noe xy for each pair of variables x, y. The first set of rules enables the choice between noe xy , arr xy and arr yx . The second ensures that no extension contains a cycle. Example 3.2. Consider V = {x, y, z}. The corresponding ABAF D dag contains 9 assumptions: for each pair of variables u, v ∈ V, we have arr uv , arr vu and noe uv . We observe that we have precisely two cyclic sequences of length &gt; 2, namely (from x) c 1 = xyzx and c 2 = xzyx. Both cycles attack each arrow it contains; the attack structure of the ABAF is depicted below.</figDesc><table><row><cell>noe xy</cell><cell>arr yx</cell><cell>arr yz</cell><cell>noe yz</cell></row><row><cell>arr xy</cell><cell></cell><cell>arr zy</cell><cell></cell></row><row><cell></cell><cell>arr xz</cell><cell></cell><cell></cell></row><row><cell></cell><cell>arr zx</cell><cell>noe xz</cell><cell></cell></row><row><cell cols="4">The joint arcs represent collective attacks; e.g., the thick,</cell></row><row><cell cols="2">purple arrows pointing</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>arr xy dpath xz ← dpath xy , arr yz e xy ← arr xy e xy ← arr yx noe xy ← e xy</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>The assumption (x ⊥ ⊥ y) is attacked by arr xy , arr yx and by all x-y-paths with inner node z except for the collider; (x⊥ ⊥y | {z}) is attacked by arr xy , arr yx and the collider {arr xz , arr yz }. Attacks for the other pairs are analogous.The formalization correctly captures independence, as stated in the following proposition. Proposition 3.11</figDesc><table><row><cell cols="2">x⊥ ⊥y</cell><cell>x⊥ ⊥y | {z}</cell><cell></cell></row><row><cell>noe xy</cell><cell>arr yx</cell><cell>arr yz</cell><cell>noe yz</cell></row><row><cell>arr xy</cell><cell></cell><cell>arr zy</cell><cell></cell></row><row><cell></cell><cell>arr xz</cell><cell></cell><cell></cell></row><row><cell></cell><cell>arr zx</cell><cell>noe xz</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head></head><label></label><figDesc>Algorithm 1 is a set of independence facts (I), alongside the significance threshold α and the number of nodes d. We discuss sourcing facts in §4.2.</figDesc><table><row><cell>3:</cell><cell>s ← |Z|</cell></row><row><cell>4:</cell><cell>if p &gt; α then</cell></row><row><cell>5:</cell><cell></cell></row></table><note><p>3. As shown in §3, each stable extension corresponds to a DAG compatible with the fixed set of independence tests. However, statistical methods can return erroneous results, in which case our causal ABAF might output no stable Algorithm 1: Causal ABA (with independence facts) Input: I, α, |V| = d 1: T ← [ ] 2: for p = I(x, y | Z) ∈ I do T ← T + [(indep(x, y, Z), S(p, α, s, d))] 6:</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 1 :</head><label>1</label><figDesc>Details of dataset from bnlearn.</figDesc><table><row><cell cols="4">Dataset Name |N | |E| |E|/|N |</cell></row><row><cell>CANCER</cell><cell>5</cell><cell>4</cell><cell>0.8</cell></row><row><cell>EARTHQUAKE</cell><cell>5</cell><cell>4</cell><cell>0.8</cell></row><row><cell>SURVEY</cell><cell>6</cell><cell>6</cell><cell>1</cell></row><row><cell>ASIA</cell><cell>8</cell><cell>8</cell><cell>1</cell></row><row><cell cols="4">Having downloaded all the .bif files from the repository,</cell></row><row><cell cols="4">we load the Bayesian network and the associated conditional</cell></row><row><cell cols="4">probability tables and sample 5000 observations with 50 dif-</cell></row><row><cell cols="4">ferent seeds to measure variance and confidence intervals. 6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 2 :</head><label>2</label><figDesc>t-tests for difference in means for Cancer dataset. Significance levels: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 3 :</head><label>3</label><figDesc>t-tests for difference in means for Earthquake dataset. Significance levels: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 4 :</head><label>4</label><figDesc>t-tests for difference in means for Survey dataset. Significance levels: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 5 :</head><label>5</label><figDesc>t-tests for difference in means for Asia dataset. Significance levels: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Assuming sufficient accuracy of the data as a first step; later on, we will assign weights to the reported independence statements in our final system to account for statistical errors.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>This observation is key for constraint-based causal discovery algorithms such as PC<ref type="bibr" target="#b39">(Spirtes, Glymour, and Scheines, 2000)</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>under the assumptions of sufficiency (no unmeasured confounders), faithfulness (data represents a DAG) and perfect inde-</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>We would have liked to compare to the method closest to our work, i.e.<ref type="bibr" target="#b3">(Bromberg and Margaritis, 2009)</ref> but unfortunately there is no implementation available.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>https://www.bnlearn.com/bnrepository/</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>We also run all the experiments shown with 2000 samples and resulted in analogous results, hence omitted.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6"><p>https://github.com/briziorusso/ArgCausalDisco/tree/public/ results/figs</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_7"><p>https://github.com/briziorusso/ArgCausalDisco/blob/public/ cd algorithms/PC.py</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_8"><p>https://github.com/bd2kccd/py-causal</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_9"><p>https://github.com/xunzheng/notears</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="11" xml:id="foot_10"><p>https://github.com/briziorusso/ArgCausalDisco/tree/public/ results/figs</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgemets Russo was supported by <rs type="funder">UK Research and Innovation</rs> (grant number <rs type="grantNumber">EP/S023356/1</rs>), in the <rs type="projectName">UKRI Centre for Doctoral Training in Safe and Trusted Artificial Intelligence</rs> (www. safeandtrustedai.org). Rapberger and Toni were funded by the <rs type="funder">ERC</rs> under the <rs type="programName">EU's Horizon 2020 research and innovation programme</rs> (grant number <rs type="grantNumber">101020934</rs>) and Toni also by <rs type="person">J.P. Morgan</rs> and by the <rs type="funder">Royal Academy of Engineering</rs> under the Research Chairs and Senior Research Fellowships scheme.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_DJZB6m7">
					<idno type="grant-number">EP/S023356/1</idno>
					<orgName type="project" subtype="full">UKRI Centre for Doctoral Training in Safe and Trusted Artificial Intelligence</orgName>
				</org>
				<org type="funding" xml:id="_SjTGTJa">
					<idno type="grant-number">101020934</idno>
					<orgName type="program" subtype="full">EU&apos;s Horizon 2020 research and innovation programme</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A reasoning model based on the production of acceptable arguments</title>
		<author>
			<persName><forename type="first">L</forename><surname>Amgoud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cayrol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Mathematics and Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="197" to="215" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Abstract argumentation frameworks and their semantics</title>
		<author>
			<persName><forename type="first">P</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Caminada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Giacomin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Formal Argumentation. College Publications</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="159" to="236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Could fisher, jeffreys and neyman have agreed on testing?</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">O</forename><surname>Berger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistical Science</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="32" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Improving the reliability of causal discovery from small data sets using argumentation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Bromberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Margaritis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="301" to="340" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">On the difference between assumption-based argumentation and abstract argumentation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Caminada</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sá</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F L</forename><surname>Alcântara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Dvorák</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IfCoLog Journal of Logics and their Applications</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="15" to="34" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning equivalence classes of bayesian-network structures</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Chickering</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="445" to="498" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A bayesian approach to constraint based causal inference</title>
		<author>
			<persName><forename type="first">T</forename><surname>Claassen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Heskes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th Conference on Uncertainty in Artificial Intelligence (UAI&apos;12)</title>
		<meeting>the 28th Conference on Uncertainty in Artificial Intelligence (UAI&apos;12)</meeting>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="207" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Orderindependent constraint-based causal structure learning</title>
		<author>
			<persName><forename type="first">D</forename><surname>Colombo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Maathuis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3741" to="3782" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning high-dimensional directed acyclic graphs with latent and selection variables</title>
		<author>
			<persName><forename type="first">D</forename><surname>Colombo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">H</forename><surname>Maathuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kalisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">S</forename><surname>Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="294" to="321" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning chordal markov networks by constraint satisfaction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Corander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Janhunen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Rintanen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Nyman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pensar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th Annual Conference on Neural Information Processing Systems (NeurIPS&apos;13</title>
		<meeting>the 27th Annual Conference on Neural Information Processing Systems (NeurIPS&apos;13</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1349" to="1357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Assumption-based argumentation: Disputes, explanations, preferences</title>
		<author>
			<persName><forename type="first">K</forename><surname>Čyras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Toni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Formal Argumentation</title>
		<imprint>
			<publisher>College Publications. chapter</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="365" to="408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Statistical methods for research workers</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Fisher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Breakthroughs in statistics: Methodology and distribution</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1970">1970</date>
			<biblScope unit="page" from="66" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Multi-shot ASP solving with clingo</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gebser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kaminski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kaufmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schaub</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theory and Practice of Logic Programming</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="27" to="82" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Complex optimization in answer set programming</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gebser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kaminski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Schaub</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Theory and Practice of Logic Programming</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4-5</biblScope>
			<biblScope unit="page" from="821" to="839" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Review of causal discovery methods based on graphical models</title>
		<author>
			<persName><forename type="first">C</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in genetics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">524</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A kernel statistical test of independence</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Fukumizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">H</forename><surname>Teo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st Annual Conference on Neural Information Processing Systems (NeurIPS&apos;07)</title>
		<meeting>the 21st Annual Conference on Neural Information Processing Systems (NeurIPS&apos;07)</meeting>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="585" to="592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The behavior of the p-value when the alternative hypothesis is true</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">M J</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">T</forename><surname>O'neill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kohne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrics</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="11" to="22" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Constraint-based causal discovery: conflict resolution with answer set programming</title>
		<author>
			<persName><forename type="first">A</forename><surname>Hyttinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Eberhardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Järvisalo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th Conference on Uncertainty in Artificial Intelligence (UAI&apos;14)</title>
		<meeting>the 30th Conference on Uncertainty in Artificial Intelligence (UAI&apos;14)</meeting>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="340" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Discovery of causal models that contain latent variables through bayesian scoring of independence constraints</title>
		<author>
			<persName><forename type="first">F</forename><surname>Jabbari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ramsey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cooper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conference of Machine Learning and Knowledge Discovery in Databases (ECML PKDD 2017</title>
		<meeting>the European Conference of Machine Learning and Knowledge Discovery in Databases (ECML PKDD 2017</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="142" to="157" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Just a matter of perspective</title>
		<author>
			<persName><forename type="first">M</forename><surname>König</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rapberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ulbricht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 9th International Conference on Computational Models of Argument (COMMA&apos;22)</title>
		<meeting>the 9th International Conference on Computational Models of Argument (COMMA&apos;22)</meeting>
		<imprint>
			<publisher>IOS Press</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">353</biblScope>
			<biblScope unit="page" from="212" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Gradient-based neural DAG learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Lachapelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Brouillard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Deleu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lacoste-Julien</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Conference on Learning Representations (ICLR&apos;20)</title>
		<meeting>the 8th International Conference on Learning Representations (ICLR&apos;20)</meeting>
		<imprint>
			<publisher>OpenReview</publisher>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>net</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Instantiations and computational aspects of non-flat assumption-based argumentation</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lehtonen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rapberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Toni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ulbricht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Wallner</surname></persName>
		</author>
		<idno>CoRR abs/2404.11431</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">rd International Workshop on Argumentation in Multi-Agent Systems (ArgMAS&apos;06), Revised Selected and Invited Papers</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Nielsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Parsons</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">LNCS</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="54" to="73" />
			<date type="published" when="2006">2006</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
	<note>A generalization of dung&apos;s abstract framework for argumentation: Arguing with sets of attacking arguments</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Graphoids: Graph-based logic for reasoning about relevance relations or When would x tell you more about y if you already know z?</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Paz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986">1986</date>
			<publisher>Probabilistic and Causal Inference</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Causality</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
	<note>2 edition</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Structural intervention distance for evaluating causal graphs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bühlmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="771" to="799" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Elements of causal inference: foundations and learning algorithms</title>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A review of argumentation based on deductive arguments</title>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Besnard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Handbook of Formal Argumentation</title>
		<imprint>
			<publisher>College Publications</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="437" to="484" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Learning assumption-based argumentation frameworks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Proietti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Toni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Inductive Logic Programming</title>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Muggleton</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Tamaddoni-Nezhad</surname></persName>
		</editor>
		<meeting>the 31st International Conference on Inductive Logic Programming</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">13779</biblScope>
			<biblScope unit="page" from="100" to="116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A million variables and more: the fast greedy equivalence search algorithm for learning highdimensional graphical causal models, with an application to functional magnetic resonance images</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ramsey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Sanchez-Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Glymour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of data science and analytics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="121" to="129" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Adjacencyfaithfulness and conservative causal inference</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ramsey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd Conference on Uncertainty in Artificial Intelligence (UAI&apos;06)</title>
		<meeting>the 22nd Conference on Uncertainty in Artificial Intelligence (UAI&apos;06)</meeting>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="401" to="408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Discovering causal graphs with cycles and latent confounders: An exact branch-and-bound approach</title>
		<author>
			<persName><forename type="first">K</forename><surname>Rantanen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hyttinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Järvisalo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Approximate Reasoning</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="page" from="29" to="49" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">On the correspondence of non-flat assumption-based argumentation and logic programming with negation as failure in the head</title>
		<author>
			<persName><forename type="first">A</forename><surname>Rapberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ulbricht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Toni</surname></persName>
		</author>
		<idno>CoRR abs/2405.09415</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Automated Discovery of Linear Feedback Models</title>
		<author>
			<persName><forename type="first">T</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computation, Causation, and Discovery</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Causal discovery and knowledge injection for contestable neural networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Russo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Toni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th European Conference on Artificial Intelligence (ECAI&apos;23)</title>
		<meeting>the 26th European Conference on Artificial Intelligence (ECAI&apos;23)</meeting>
		<imprint>
			<publisher>IOS Press</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">372</biblScope>
			<biblScope unit="page" from="2025" to="2032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Toward causal representation learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Locatello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">R</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">109</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="612" to="634" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Logic programming in assumption-based argumentation revisited -semantics and graphical representation</title>
		<author>
			<persName><forename type="first">C</forename><surname>Schulz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Toni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th AAAI Conference on Artificial Intelligence (AAAI&apos;15)</title>
		<meeting>the 29th AAAI Conference on Artificial Intelligence (AAAI&apos;15)</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1569" to="1575" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Scutari</surname></persName>
		</author>
		<ptr target="http://www.bnlearn.com/bnrepository" />
		<title level="m">Bayesian network repository</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Calibration of ρ values for testing precise null hypotheses</title>
		<author>
			<persName><forename type="first">T</forename><surname>Sellke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bayarri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">O</forename><surname>Berger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The American Statistician</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="62" to="71" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">N</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Scheines</surname></persName>
		</author>
		<title level="m">Causation, prediction, and search</title>
		<imprint>
			<publisher>MIT press</publisher>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning neighborhoods of high confidence in constraint-based causal discovery</title>
		<author>
			<persName><forename type="first">S</forename><surname>Triantafillou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Tsamardinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Roumpelaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th European Workshop on Probabilistic Graphical Models (PGM&apos;14)</title>
		<meeting>the 7th European Workshop on Probabilistic Graphical Models (PGM&apos;14)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="487" to="502" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">The max-min hill-climbing bayesian network structure learning algorithm</title>
		<author>
			<persName><forename type="first">I</forename><surname>Tsamardinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Aliferis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="31" to="78" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Non-flat ABA is an instance of bipolar argumentation</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ulbricht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Potyka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rapberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Toni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 38th AAAI Conference on Artificial Intelligence (AAAI&apos;24)</title>
		<meeting>the 38th AAAI Conference on Artificial Intelligence (AAAI&apos;24)</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="10723" to="10731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">D&apos;ya like dags? a survey on structure learning and causal discovery</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Vowels</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">C</forename><surname>Camgoz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Bowden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="36" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">A survey on causal discovery: Theory and practice</title>
		<author>
			<persName><forename type="first">A</forename><surname>Zanga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ozkirimli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Stella</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Approximate Reasoning</title>
		<imprint>
			<biblScope unit="volume">151</biblScope>
			<biblScope unit="page" from="101" to="129" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Kernel-based conditional independence test and application in causal discovery</title>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th Conference on Uncertainty in Artificial Intelligence (UAI&apos;11)</title>
		<meeting>the 27th Conference on Uncertainty in Artificial Intelligence (UAI&apos;11)</meeting>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="804" to="813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Dags with NO TEARS: continuous optimization for structure learning</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Aragam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Prooceedings of the 31st Annual Conference on Neural Information Processing Systems (NeurIPS&apos;18)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="9492" to="9503" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning sparse nonparametric DAGs</title>
		<author>
			<persName><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Aragam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Ravikumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Xing</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the 23rd International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="3414" to="3425" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
