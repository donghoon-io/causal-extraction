<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Incentivizing Desirable Effort Profiles in Strategic Classification: The Role of Causality and Uncertainty</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2025-02-11">February 11, 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Valia</forename><surname>Efthymiou</surname></persName>
							<email>valia554@mit.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Massachussetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chara</forename><surname>Podimata</surname></persName>
							<email>podimata@mit.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Massachussetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Diptangshu</forename><surname>Sen</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Juba</forename><surname>Ziani</surname></persName>
							<email>jziani3@gatech.edu</email>
							<affiliation key="aff3">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Incentivizing Desirable Effort Profiles in Strategic Classification: The Role of Causality and Uncertainty</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-02-11">February 11, 2025</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2502.06749v1[cs.GT]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-21T20:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We study strategic classification in binary decision-making settings where agents can modify their features in order to improve their classification outcomes. Importantly, our work considers the causal structure across different features, acknowledging that effort in a given feature may affect other features. The main goal of our work is to understand when and how much agent effort is invested towards desirable features, and how this is influenced by the deployed classifier, the causal structure of the agent's features, their ability to modify them, and the information available to the agent about the classifier and the feature causal graph.</p><p>In the complete information case, when agents know the classifier and the causal structure of the problem, we derive conditions ensuring that rational agents focus on features favored by the principal. We show that designing classifiers to induce desirable behavior is generally non-convex, though tractable in special cases. We also extend our analysis to settings where agents have incomplete information about the classifier or the causal graph. While optimal effort selection is again a non-convex problem under general uncertainty, we highlight special cases of partial uncertainty where this selection problem becomes tractable. Our results indicate that uncertainty drives agents to favor features with higher expected importance and lower variance, potentially misaligning with principal preferences. Finally, numerical experiments based on a cardiovascular disease risk study illustrate how to incentivize desirable modifications under uncertainty.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The widespread adoption of automated decision-making systems has brought significant attention to the issue of strategic classification-a machine learning setting where individuals modify their features to secure favorable outcomes. This phenomenon is common in many domains: students enroll in preparatory courses to enhance their chances at college admission; job seekers tailor their resumes to align them with AI-based hiring algorithms, and individuals adjust their financial behaviors to improve credit scores. Some of these modifications reflect genuine efforts to enhance one's qualifications or financial responsibility (e.g., acquiring new skills or consistently paying off loans), while others effectively game the system, e.g., artificially boosting credit scores by opening new credit lines or strategically targeting specific keywords in algorithmic resume screening.</p><p>The distinction between desirable and undesirable modifications is not always clear-cut. While gaming is typically regarded as problematic, even genuine improvements can vary in how desirable they are. For example, in healthcare, encouraging patients to adopt preventive lifestyle changes (such as improved diet and regular exercise) may be preferable to medical interventions like medication or surgery for conditions such as obesity or hyperlipidemia. This highlights the nuanced nature of strategic classification: interventions that lead to real improvements may still not align with preferred or desirable forms of improvement, where desirability is decided by the learner.</p><p>Further, a key challenge in strategic classification is that features are often interdependent. That is, modifications to one feature can have cascading effects on others. For example, increasing the number of credit cards an individual holds will also lower their credit utilization percentage, indirectly influencing their credit-worthiness. Similarly, reducing alcohol consumption or improving dietary habits can mitigate multiple health risks, such as obesity, hyperlipidemia, and cardiovascular diseases. These dependencies are best captured using a causal graph, a framework that has been explored in a limited amount of prior work <ref type="bibr" target="#b21">Kleinberg and Raghavan [2020]</ref>, <ref type="bibr" target="#b26">Miller et al. [2020]</ref>, <ref type="bibr" target="#b32">Shavit et al. [2020]</ref>, <ref type="bibr" target="#b4">Bechavod et al. [2021]</ref> in the specific context of strategic classification.</p><p>Our work builds upon this causal perspective on strategic classification, investigating how agents respond to decision-making systems and, in particular, when their strategic behavior aligns with desirable modifications. We adopt a framework in which a principal (e.g., a decision-maker or machine learning classifier) deploys a model, and agents (or individuals) strategically adjust their features to maximize their probability of receiving a favorable classification outcome.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Summary of Contributions. Our contributions are as follows:</head><p>Our Model: In Section 2, we introduce our model to study incentivizing desirable efforts in the context of causal strategic classification. We build on previous work in two different ways: i) first, we introduce incomplete information to the study of causality in strategic classification, highlighting situations where an agent may not know the classifier, the causal graph, or both; and ii) we introduce a new notion of β-desirability which quantifies the extent to which agents invest effort in features deemed desirable by the principal.</p><p>Complete Information: In Section 3, assuming agents have full knowledge of the classifier and the causal structure, we characterize the optimal effort profiles under various modification cost structures. We establish theoretical conditions guaranteeing investment in desirable effort profiles by rational agents. We also demonstrate that finding classifiers that induce desirable behavior is, in general, a non-convex problem. However, we show that when the principal chooses only one desirable feature to incentivize, the problem of finding good classifiers becomes convex. We also provide a simple convexification heuristic for when the number of desirable features is more than one, ensuring that chosen classifiers do not incentivize more than a certain amount of undesirable feature effort.</p><p>Incomplete Information: We extend our analysis to the setting where agents lack information about either the classifier or the causal graph (or both) in Section 4. There, incomplete information is modeled as agents having Gaussian priors about the classifier and the causal graph. First, we show that in presence of uncertainty over both the classifier and the causal graph, choosing how to invest effort optimally is a non-convex problem for the agent. However, the problem becomes tractable under partial uncertainty, and we provide a semi-closed-form characterization of optimal effort profiles for agents in partial uncertainty settings.</p><p>Case study: Finally, in Section 5, we complement our theoretical insights in the incomplete information setting with numerical experiments, basing our experimental setup on a medical study from previous work that predicts risk of cardiovascular disease (CVDs) in adults. In the process, we provide insights into how to incentivize changes in desirable features under uncertainty.</p><p>Related Work: Strategic classification, a machine learning setting where agents can manipulate or modify their own features to improve their outcomes, has been widely studied in recent years under a range of assumptions. This belongs to a broad class of problems in economics called principalagent problems where agents act strategically in their self-interests which are often misaligned with the principal's interests <ref type="bibr" target="#b14">Grossman and Hart [1992]</ref>, Ross <ref type="bibr">[1973]</ref>, <ref type="bibr" target="#b23">Laffont and Martimort [2009]</ref>, <ref type="bibr" target="#b31">Sappington [1991]</ref>. Early works in strategic classification focused on scenarios where agents manipulate their observable features solely to "game" a published classifier, thereby increasing their chances of a favorable label without genuinely improving underlying attributes (e.g., <ref type="bibr" target="#b15">Hardt et al. [2016]</ref>, <ref type="bibr">Braverman and Garg [2020]</ref>, <ref type="bibr" target="#b9">Dong et al. [2018]</ref>, <ref type="bibr" target="#b34">Zhang et al. [2022]</ref>, <ref type="bibr" target="#b24">Lechner et al. [2023]</ref>, <ref type="bibr" target="#b7">Chen et al. [2020]</ref>, <ref type="bibr" target="#b0">Ahmadi et al. [2021]</ref>, <ref type="bibr" target="#b33">Sundaram et al. [2023]</ref>). Over time, the literature expanded to consider settings where agents make substantive changes to their features (i.e., effectively investing in real improvements) rather than relying on superficial modifications (e.g., <ref type="bibr" target="#b21">Kleinberg and Raghavan [2020]</ref>, <ref type="bibr" target="#b5">Bechavod et al. [2022</ref><ref type="bibr" target="#b4">Bechavod et al. [ , 2021]]</ref>, <ref type="bibr" target="#b32">Shavit et al. [2020]</ref>, <ref type="bibr" target="#b16">Harris et al. [2021]</ref>). In many cases, actual improvement involves investing effort which is not directly observable by the principalthis again has similarities to the notion of moral hazard in insurance markets Pauly <ref type="bibr">[1968]</ref>, Arrow <ref type="bibr">[1968]</ref> and other general settings Arrow <ref type="bibr">[1978]</ref>. There has also been interest in fairness in strategic classification (e.g., <ref type="bibr" target="#b27">Milli et al. [2019]</ref>, <ref type="bibr" target="#b19">Hu et al. [2019]</ref>, <ref type="bibr" target="#b12">Estornell et al. [2023]</ref>) but this line of work is more distantly related to ours.</p><p>A useful tool to model manipulations as opposed to effective improvement is causality. Causal modeling has been extensively studied in decision-making and machine learning, starting with <ref type="bibr" target="#b29">Pearl [2000]</ref>; see <ref type="bibr" target="#b20">Kaddour et al. [2022]</ref> for a recent survey of causal machine learning. In the context of strategic classification, a few recent studies have incorporated causal modeling to account for interdependencies among features <ref type="bibr" target="#b21">Kleinberg and Raghavan [2020]</ref>, <ref type="bibr" target="#b32">Shavit et al. [2020]</ref>, <ref type="bibr" target="#b4">Bechavod et al. [2021]</ref>, <ref type="bibr" target="#b1">Ahmadi et al. [2022]</ref>, <ref type="bibr" target="#b26">Miller et al. [2020]</ref>, <ref type="bibr" target="#b18">Horowitz and Rosenfeld [2023]</ref>. <ref type="bibr" target="#b26">Miller et al. [2020]</ref> highlights that in strategic classification, learning a classifier that incentivizes gaming as opposed to improvement is as hard as learning the underlying causal graph between features. <ref type="bibr" target="#b32">Shavit et al. [2020]</ref> and <ref type="bibr" target="#b4">Bechavod et al. [2021]</ref> both focus on special cases of linear causal graphs, unlike our work that considers general cases of linear graphs. <ref type="bibr" target="#b1">Ahmadi et al. [2022]</ref> explore strategic classification using a different structural framework known as "manipulation graphs," where each agent has a fixed set of costly effort profiles that they may choose from, defining a bipartite graph between initial agent features and induced features after exerting effort-this is, again, a special case of causal graphs. <ref type="bibr" target="#b18">Horowitz and Rosenfeld [2023]</ref>, similarly to us, distinguish among causal, non-causal (or "correlative"), and unobserved features. However, they focus on a different objective of maximizing predictive accuracy, we are interested in incentivizing "desirable" modifications.</p><p>Perhaps closest to our work is the work of <ref type="bibr" target="#b21">Kleinberg and Raghavan [2020]</ref>. Like us, they focus on general causal graphs; however, we highlight several major differences. First, we focus on classification settings, while <ref type="bibr" target="#b21">Kleinberg and Raghavan [2020]</ref> focus on regression and scoring settings. Second, we highlight differences in our agent model, where our agents invest effort to pass the classifier with reasonably high probability, while agents in <ref type="bibr" target="#b21">Kleinberg and Raghavan [2020]</ref> always exert effort to improve their score. Third, we note that our cost model is strictly more general: where <ref type="bibr" target="#b21">Kleinberg and Raghavan [2020]</ref> focuses on linear costs, our work considers general ℓ p -costs. Our results show that this choice of cost is important, noting a sharp distinction in agent behavior between the cases of ℓ 1 -cost and ℓ p costs for p &gt; 1. Finally, unlike <ref type="bibr" target="#b21">Kleinberg and Raghavan [2020]</ref>,</p><p>our study incorporates incomplete information, where agents may not fully understand either the causal graph or the deployed classifier.</p><p>A closely related line of work investigates strategic classification under varying models of information available to agents. In many real-world settings, agents may have incomplete information about the classifier-either because it is too complex, or because the learner's model is proprietary, or the causal relationships governing feature interactions <ref type="bibr" target="#b5">Bechavod et al. [2022]</ref>, <ref type="bibr" target="#b8">Cohen et al. [2024]</ref>, <ref type="bibr" target="#b13">Ghalme et al. [2021]</ref>. Or, agents might misperceive the classifier due to behavioral biases <ref type="bibr" target="#b10">Ebrahimi et al. [2024]</ref>. However, we are not aware of any work studying uncertainty on causal graphs, and to the best of our knowledge, we are the first work in the space of strategic classification to incorporate both causal modeling and incomplete information in strategic classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Model</head><p>We consider a binary classification problem, where there is an interaction between a principal and an agent. The principal, also known as the learner, deploys a machine learning model or classifier. Then, the model assigns a (binary) score or a decision to the agents, based on their features. Finally, agents respond to the deployed classifier, potentially changing their features to obtain better outcomes, at a cost. We provide a detailed description of the model below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Agent Model: Features and Utilities</head><p>Formally, let F ∈ R d be the set of all features and Y = {-1, +1} be the set of labels. Each agent k is defined by a pair (x k , y k ), where x k ∈ F is the agent's feature vector and y k is the agent's true label (i.e., their actual fit or qualification for the classification task at hand).</p><p>Causal modeling of feature interactions. We adopt a causal perspective on strategic classification, where different features can impact each other-i.e., a change in a feature i (e.g., alcohol consumption or diet) that has a causal relationship with feature j (e.g., cholesterol) will also induce a change in feature j.</p><p>The chain of causality between the different features can be captured using a weighted directed graph G = (F, A, w). We will henceforth call graph G the causal graph. We slightly abuse notation here and denote the set of nodes in the graph also by F to indicate that each feature in F corresponds to a node on G. A represents the set of directed edges on G, where an edge from features i to j indicates that i is causal for j. Finally, w : A → R captures the weights of the edges. We make no assumption on the structure of G, other than the fact that it is a directed acyclic graph. <ref type="foot" target="#foot_0">1</ref> We represent all necessary information about the graph using an adjacency matrix A ∈ R d×d : Definition 1 (Adjacency Matrix of a Causal Graph).</p><formula xml:id="formula_0">A ij = 0, if a ij / ∈ A, w(a ij ), if a ij ∈ A.</formula><p>(1)</p><p>Here, if there is an edge a ij ∈ G, then feature i ∈ F causally affects feature j ∈ F; in other words, by changing feature i, an agent is making an implicit change on feature j. Finally, the weight of edge a ij , given by w(a ij ) indicates that if the value of feature i improves by a unit amount, then the value of the downstream feature j will improve by w(a ij )<ref type="foot" target="#foot_1">foot_1</ref> . Importantly, edge weights can be negative-an increase in causal feature i might lead to a decrease in feature j.</p><p>Desirable vs Undesirable Features Importantly, we assume that the set of features F is divided by the principal into two kinds of features: desirable features and undesirable features denoted by sets D and U respectively. Roughly speaking, desirable features are those that the principal wants to incentivize the agent to change; e.g., in the health application of the Introduction, "alcohol consumption" would be a desirable feature that the principal (for example, the agent's primary care physician) would like to see changed<ref type="foot" target="#foot_2">foot_2</ref> . Undesirable features, on the other hand, can be considered as features that we would like to disincentivize agents from modifying directly: e.g., directly intervening to lower an agent's cholesterol level via medication such as statins may be less desirable than promoting lifestyle changes (lower alcohol consumption, improved diet, etc.) that will also lower their cholesterol.</p><p>Remark 1 (Relationship between causality and desirability ). In the traditional causality literature on strategic classification, features on a causal graph are usually classified into causal and non-causal or proxy features. Causal features are those which affect some specified output variable of interest while proxy features are those which do not. A natural question is whether and how desirability of a feature relates with its causality.</p><p>Proxy features are generally seen as undesirable to modify: this is because they do not change the root cause behind an agent's label, hence do not lead to true improvements in said label-this is often referred to as "gaming" the classifier. However, causal features may still be undesirable, even if they do not lead to gaming. For example, in our healthcare example, both diet and cholesterol levels are causal for predicting the risk of cardio-vascular disease-in particular, diet is directly causal for cholesterol levels, and cholesterol levels are directly causal for cardio-vascular disease. Yet, it may be preferable to incentivize sustainable interventions such as a better diet (prevention), rather than resorting to short-term fixes like cholesterol-lowering medications that may have significant side effects (treatment).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Principal -Agent Interaction</head><p>The principal deploys a linear classifier denoted as h 0 ∈ R d from its normal vector. Under this classifier, an agent with feature vector x ∈ R d is assigned a score of s(x) = h ⊤ 0 x. There is a pre-determined threshold τ ∈ R and the classification decision y for said agent is given by:</p><formula xml:id="formula_1">y(x) = 1 [s(x) ≥ τ ] . Agent's Best Response. The agent is assumed to have Gaussian priors Π h := N (µ h , Σ h ) (where µ h ∈ R |F | , Σ h ∈ R |F |×|F |</formula><p>) over the principal's deployed classifier h 0 and Π C := N (µ w , Σ w ) (where</p><formula xml:id="formula_2">µ w ∈ R |A| , Σ w ∈ R |A|×|A| )</formula><p>over the edge weights of the causal graph G<ref type="foot" target="#foot_3">foot_3</ref> . The agent is always assumed to know the topology of the causal graph. We explore two kinds of information structures:</p><p>1. The Complete Information setting where the agent fully knows the classifier h 0 , i.e, µ h = h 0 and Σ h = 0 and the weights of all edges of G, i.e., µ w = w and Σ w = 0. See Section 3 for the complete information setting.</p><p>2. The Incomplete Information setting where i) there is uncertainty over the principal's classifier h 0 , i.e., µ h may differ from h 0 (bias) and Σ h ̸ = 0 (variance), and/or ii) there is uncertainty over the edge weights of the causal graph G, i.e., µ w may differ from w (bias) and Σ w ̸ = 0 (variance). See Section 4 for the incomplete information setting.</p><p>If y(x) = 0, the agent also knows the amount α &gt; 0 by which she fell short of passing the classifier; e.g., in a loan approval setting, an agent may know their current credit score and the threshold credit score that the bank uses to decide who gets approved for a loan. They might, however, not fully understand how said credit score is calculated in the first place. The agent's goal is to obtain a positive classification outcome, i.e. y(x) = 1 ("to pass the classifier"). Therefore, she attempts to change her feature vector x by investing some exogenous effort e ∈ R |F | , which we call the agent's exogenous effort profile. Importantly, this exogenous effort profile, in our model, is exerted directly on features<ref type="foot" target="#foot_4">foot_4</ref> : i.e., e is a vector quantifying how much an agent changes their features directly. However, remember that exerting exogeneous effort on a subset of the features F can also lead to features, particularly those that no effort was exerted on, to change endogenously, due to causality. We call this the induced or endogenous feature change.</p><p>Now, suppose the agent's modified feature vector after investing effort is given by x ′ (e). We define the net change in features due to effort e as: ∆x(e) = x ′ (e) -x, where ∆x(e) can be computed using the structure of the causal graph G. This effort comes at a cost, modeled through a cost function</p><formula xml:id="formula_3">Cost : R d → R ≥0 ,</formula><p>where Cost(e) is the cost incurred to perform effort e. We mainly focus on (weighted) ℓ p -norm cost functions for all p ≥ 1. Formally,</p><formula xml:id="formula_4">Cost(e) =   f ∈F c f |e f | p   1/p , where c f &gt; 0 ∀f ∈ F,<label>(2)</label></formula><p>where c f represents a cost multiplier associated with investing unit exogenous effort into feature f . To express ∆x(e) in terms of the causal graph G, we define the contribution matrix of G, which allows us to quantify how much effort profile e maps to a total change in features, including both exogenous effort and induced feature changes: Definition 2. The contribution matrix C ∈ R d×d associated with causal graph G is:</p><formula xml:id="formula_5">C ii = 1 ∀ i ∈ [d],</formula><p>and</p><formula xml:id="formula_6">C ij = p∈P ij ω(p) ∀ i, j ∈ [d], i ̸ = j,</formula><p>where P ij is the set of all directed paths from node i to node j on G and ω(p) is the weight of path p ∈ P ij with ω(p) = a∈A,a⊂p w(a).</p><p>Note that in causal graphs, a given feature i may affect another feature j directly-in which case there is an edge of non-zero weight from i towards j, but also indirectly, through other features. For example, there may be a path from feature i to feature j through intermediary features i 1 , . . . , i k , where i → i 1 → i 2 → . . . → i k → j<ref type="foot" target="#foot_5">foot_5</ref> . Therefore, investing effort in feature i will lead to modifications of not only feature i, but also all features i 1 , . . . , i k , j that it is directly or indirectly causal for. The contribution matrix C quantifies the impact of any given feature i on any other feature j, even when the features have an indirect causal relationship.</p><p>Observation 1. Given matrix A as defined in Equation (1), the contribution matrix is given by</p><formula xml:id="formula_7">C = |F | k=0 A k ,</formula><p>and therefore can be computed in polynomial time in |F|. Intuitively, A k represents the contribution of all paths of size exactly k; because our graph is acyclic, the longest path must have length at most |F|.</p><p>Proof. This is a well-known result, but we provide a proof in Appendix D for completeness.</p><p>Given the contribution matrix C and exogenous effort profile e, the net change in features ∆x(e) will be given by: ∆x(e) = C ⊤ e.</p><p>(3)</p><p>To simplify notation, we drop the dependence on e and write ∆x when clear from context. We demonstrate how to construct C and how to compute ∆x given an effort profile e through a toy example in Figure <ref type="figure" target="#fig_0">1</ref>.</p><p>The agent chooses their optimal effort profile e ⋆ that ensures that y(x ′ (e ⋆ )) = +1 with probability at least 1 -δ, while incurring the minimum possible cost. We call the effort profile e ⋆ (Π h , Π C ) the agent's best response to priors (Π h , Π C ). Formally:</p><formula xml:id="formula_8">e ⋆ (Π h , Π C ) = arg min e Cost(e) s.t. P h∼Π h ,C∼Π C h ⊤ C ⊤ e ≥ α ≥ 1 -δ.</formula><p>(4)</p><p>Note that the constraint ensures that an agent passes the classifier with probability at least 1 -δ, measured with respect to their prior on the causal graph and on the classifier. In particular, if they exert effort profile e, their features change by C ⊤ e, so their score changes by h ⊤ C ⊤ e, and they have to improve their score by α to make a positive classification outcome. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Incentivizing Effort towards Desirable Features</head><p>We are interested in the properties of the effort profile that the agent exerts as a result of bestresponding to the principal's classifier, and in particular understanding the amount of effort they exert towards desirable features in set D and undesirable features U. The goal is to promote effort towards desirable features and away from undesirable features, i.e. to understand when is it in the agent's best interest to invest more effort into desirable versus undesirable features?.</p><p>To do so, we define a notion of β-desirability, that measures the ratio of investment in features in D vs U: Definition 3 (β-desirable effort profiles). Given 0 &lt; β ≤ 1, an exogenous effort profile e is said to be β-"desirable" if:</p><formula xml:id="formula_9">∥e D ∥ 2 ≥ β∥e∥ 2 ,</formula><p>i.e., the magnitude of effort made towards desirable features is at least a fraction β of the magnitude of total effort.</p><p>From the principal's point of view, incentivizing β-desirable effort profiles is not straightforward since agents are strategic, and may prefer undesirable features if they are low-cost to manipulate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Complete Information Setting</head><p>In the complete information setting, the agent knows precisely the true classifier h 0 deployed by the principal-equivalently, her prior Π h satisfies h = h 0 (the mean belief matches the true classifier) and the covariance is given by Σ h = 0 (there is no uncertainty). She also fully knows the causal graph G-i.e., w = w and Σ w = 0. Therefore, in the complete information case, the deterministic tuple (h 0 , C) is enough to characterize agent beliefs.</p><p>When the agent has no uncertainty about either the classifier or the causal graph, the agent's optimization problem (4) can be written as:</p><formula xml:id="formula_10">e ⋆ (h 0 , C) = arg min e Cost(e) s.t. (Ch 0 ) ⊤ e ≥ α.<label>(5)</label></formula><p>In other words, the agent must find the minimum-cost effort profile that passes the (known) classifier h 0 . We first prove a technical result that helps further simplify the complete information setting. Roughly speaking, the proposition states that for the complete information case, it suffices to only focus on non-negative efforts for all features.</p><p>Proposition 1. For the cost functions defined in (2), we can assume that: (Ch 0 ) f ≥ 0 and that (e) f ≥ 0 ∀f ∈ F, without loss of generality.</p><p>The proof of the proposition can be found in Appendix B.1.</p><p>Computation of the Optimal Effort Profile. Using Proposition 1, we can rewrite optimization problem (5) under the complete information setting as follows:</p><formula xml:id="formula_11">e ⋆ (h 0 , C) = arg min e≥0 Cost(e) s.t. (Ch 0 ) ⊤ e ≥ α,<label>(6)</label></formula><p>where Ch 0 ≥ 0. Program ( <ref type="formula" target="#formula_11">6</ref>) is a convex optimization problem: the objective is convex for our cost functions, and all constraints are linear. As such, this program can be solved efficiently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Characterization of Optimal Effort Profiles</head><p>We now investigate the structural properties of the optimal effort profile e ⋆ for the cost function in Eq. ( <ref type="formula" target="#formula_4">2</ref>), individually for the cases of i) p = 1 and ii) p &gt; 1. The contributions of this section are two-fold: first, we show that the structure of the optimal effort profile largely depends on the cost function that an agent optimizes over. In particular, we prove that in the case of the ℓ 1 cost function, agents only invest effort into one feature when best-responding to the classifier h 0 (Lemma 1). Instead, for general ℓ p costs with p &gt; 1, the agents' effort profile is significantly more diversified and covers any non-trivial dimension i.e., one with contribution (Ch 0 ) f &gt; 0 (Lemma 2). Second, we derive conditions under which effort profiles that put a significant amount of weight on desirable features are incentivized, providing insights as to how to set a classifier h 0 to incentivize effort exertion on desirable features (Theorems 1 and 2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Case 1 (ℓ 1 -norm costs)</head><p>For p = 1, we have the following optimization problem for the agent:</p><formula xml:id="formula_12">min e≥0 c ⊤ e s.t. (Ch 0 ) ⊤ e ≥ α.<label>(7)</label></formula><p>We first show that the case where α ≤ 0 is insignificant and hence we will only focus on α &gt; 0. Indeed, α ≤ 0 indicates that the agent has already passed the classifier (therefore has a non-positive distance from the decision boundary) and hence, they should not invest any effort into modifying features. Formally, Proposition 2. When α ≤ 0, then e ⋆ = 0 which means that the agent does not need to invest any effort to change her features.</p><p>Proof. Note that the objective value is greater than or equal to zero since c f &gt; 0 for all f ∈ F and e ≥ 0. But, since α ≤ 0, e = 0 is feasible to the above problem and it also achieves an objective value of 0. Therefore, e = 0 must be optimal.</p><p>We next focus entirely on the case where α &gt; 0. Our first result is characterizing the structure of the agent's best response.</p><p>Lemma 1. When α &gt; 0, there exists an optimal effort profile for the agent in which she needs to modify exactly one feature to pass the classifier h 0 . The optimal feature to modify f * is the one which offers the best ratio of contribution to cost, i.e.,</p><formula xml:id="formula_13">f ⋆ ∈ arg max f ∈F (Ch 0 ) f c f ,</formula><p>and the optimal amount of effort to be invested into that feature is given by:</p><formula xml:id="formula_14">e f ⋆ = α (Ch 0 ) f ⋆ .</formula><p>The proof of the lemma can be found in Appendix B.2.</p><p>Conditions for β-desirability. The previous lemma provides key insights into the optimal effort profile of an agent. It shows that the agent has to invest effort into a single feature which offers her the best "bang-per-buck". Let I * be the set of all such features, i.e.:</p><formula xml:id="formula_15">I * = f ⋆ : f ⋆ ∈ arg max f ∈F (Ch 0 ) f c f .</formula><p>Therefore, by Definition 3, if I * ∩ U = ∅, then the agent's best response is guaranteed to be a β-desirable effort profile. We now formalize this idea and present the main result of this section:</p><p>Theorem 1. If there exists a desirable feature f ⋆ (f ⋆ ∈ D) such that:</p><formula xml:id="formula_16">max f ∈U (Ch 0 ) f c f &lt; (Ch 0 ) f ⋆ c f ⋆ ,</formula><p>then the agent's best response is always a β-desirable effort profile for any β ∈ (0, 1].</p><p>Proof. The proof follows directly from Lemma 1 and Definition 3.</p><p>3.1.2 Case 2 (ℓ p -norm costs for p &gt; 1)</p><p>For p &gt; 1, the agent is solving the following optimization problem when best-responding:</p><formula xml:id="formula_17">min e≥0   f ∈F c f (e f ) p   1/p s.t. (Ch 0 ) ⊤ e ≥ α.<label>(8)</label></formula><p>Lemma 2. When the cost function is the weighted ℓ p -norm of the effort for p &gt; 1, the optimal effort profile for the agent e ⋆ satisfies:</p><formula xml:id="formula_18">e ⋆ f ∝ (Ch 0 ) f c f 1/(p-1) ∀ f ∈ F.</formula><p>The proof can be found in Appendix B.3.</p><p>Conditions for β-desirability Since we know the structure of the agent's optimal effort profile, we can identify conditions under which the best response is β-desirable.</p><p>Theorem 2. For a ℓ p -norm cost function with p &gt; 1, the agent's best response is always a βdesirable effort profile if:</p><formula xml:id="formula_19">  f ∈D (Ch 0 ) f c f 2/(p-1)   1/2 ≥ β 1 -β 2   f ∈U (Ch 0 ) f c f 2/(p-1)   1/2</formula><p>When c = 1 and p = 2, the condition reduces to:</p><formula xml:id="formula_20">∥(Ch 0 ) D ∥ 2 ≥ β 1 -β 2 ∥(Ch 0 ) U ∥ 2 ,</formula><p>In other words, in the complete information setting and when agents have ℓ 2 cost functions, if the magnitude of the net contribution per unit cost along the desirable features relative to the undesirable features is high enough, then it is always in the agent's best interest to invest more in desirable features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Desirable Classifiers and Where to Find Them</head><p>So far, we have provided conditions which help the principal answer the following problem: "does a given classifier h 0 induce strategic agents to invest only in β-desirable effort profiles?" This means that given a classifier h 0 , we can check that the classifier incentivizes a good profile. However, this does not answer the question of developing algorithms for finding such a profile. We start this section with a negative result: the set of β-desirable classifiers, is, in general, non-convex (Lemma 3)-in particular, there are typically no well-known methods for finding a good effort profile in high dimensions. However, we investigate a simple condition under which the problem becomes convex (Proposition 3), and we also provide a heuristic (using convexification) that ensures that not too much effort is spent on undesirable features (Proposition 4). The benefit of having a convex design space of desirable classifiers is that i) we can find a desirable classifier using standard optimization techniques, and ii) it allows the principal to choose a classifier that simultaneously minimizes (convex) accuracy losses and induces desirable effort profiles.</p><p>The Space of Desirable Classifiers is Non-Convex. Our first main result shows that finding desirable classifiers is a non-convex problem, hence effectively computationally hard in general.</p><p>Lemma 3. There exists an instance of the problem (C, h 0 ) and β &gt; 0, where the space of β-desirable classifiers H is non-convex.</p><p>The proof of the lemma is provided in Appendix B.4. In fact, whenever there is more than one desirable feature, i.e., |D| &gt; 1, H can be shown to be a non-convex set.</p><p>Since the space of desirable classifiers is, in the worst case, non-convex, finding the best classifier in this set (that maximizes some classification accuracy metric) is equivalent to solving a non-convex optimization problem to global optimality, which is typically an NP-hard problem.</p><p>Special Case: When the Learner Focuses on Incentivizing a Single Desirable Feature. We now highlight a special case where the space of β-desirable classifiers is convex, for any β &gt; 0 for a specific range of p values. Namely, we focus on the special case of the principal having a single desirable feature that they wish to target. Note that this assumption is actually aligned with what we expect to see happening in real life: indeed, a principal may define for themselves which feature they want to incentivize, and focus on one feature where they would really like to see improvements, especially if this is a feature that has historically not been properly leveraged. Not only that, but by targeting a single feature, they may lower the agents' cognitive load for best-responding, which is always desirable in practice.</p><p>Proposition 3. Suppose that there is only a single desirable feature, i.e., that |D| = 1. Then for any β &gt; 0, the space of β-desirable classifiers H is convex for any ℓ p -norm cost function with p ∈ [1, 3].</p><p>The proof can be found in Appendix B.5.</p><p>Minimizing Undesirable Features. When |D| &gt; 1, we know that in general, the set H of β-desirable classifiers is not convex, and difficult to optimize over. However, we propose a convexification heuristic (parameterized by γ) where the principal just tries to design a classifier such that "the total contribution of undesirable features is no more than γ":</p><formula xml:id="formula_21">Proposition 4. Let H w(U )≤γ = {h 0 : ∥(Ch 0 ) U ∥ 2/(p-1) ≤ γ}. Then for any γ &gt; 0, H w(U )≤γ is convex for any ℓ p -norm cost function with p ∈ [1, 3].</formula><p>The proof is nearly identical to that of Proposition 3 and is omitted to avoid repetition. This result helps the principal guarantee that they can bound the effort exerted on undesirable features, even if they are not able to guarantee a certain target level of β-desirable effort.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Incomplete Information Setting</head><p>In this section, we switch our attention to the incomplete information setting. Our first set of results provide a characterization of when the optimal effort profile for agents can be efficiently computed. We highlight that under partial uncertainty and for general weighted ℓ p costs, the optimization program of the agent is convex and can be solved efficiently (Lemma ??). We also show a technical result in Lemma 5, capturing when the above convex program is feasible and when it is not. We conclude by providing a negative result in the setting where there is uncertainty over both the classifier and the causal graph (Proposition 5)-in this case, we show that the agent's optimization program is non-convex and hence, hard to solve in the worst case.</p><p>We then aim to characterize what the optimal effort profiles look like. In the ℓ 1 cost case, we show a sharp contrast with the complete information case: namely, an agent may be willing to expend effort across several features, as opposed to a single one in the complete information case (Lemma 6). In the ℓ 2 case, we provide a semi-closed form characterization of the agent's optimal effort profile (Theorem 3). We also highlight how, under some partial information models, it is possible to provide a more interpretable characterization of how the effort per feature depends on E[Ch] and Var(Ch) (Corollary 1): the higher the expected importance (Ch) i of a feature i, the more effort is spent towards it, and the higher the variance of Ch towards feature i, the less effort is spent towards it.</p><p>Recall that for the incomplete information setting, the agent's optimization problem is:</p><formula xml:id="formula_22">e ⋆ (Π h , Π C ) = arg min e</formula><p>Cost(e) s.t.</p><p>(9)</p><formula xml:id="formula_23">P h∼Π h ,C∼Π C (Ch) ⊤ e ≥ α ≥ 1 -δ, δ ∈ (0, 1).</formula><p>Below, we briefly remind the reader of the models of information used in this paper.</p><p>Models of Information. We remind the reader here that there can be two different sources of uncertainty: i) the principal's classifier; and, ii) the edge weights of the causal graph G (the graph topology is assumed to be common knowledge). In particular, we have three following combinations of where uncertainty lies in our problem:</p><p>1. Uncertainty only exists in the principal's classifier, the causal graph is fully known;</p><p>2. Uncertainty only exists in the edge weights of the causal graph, the classifier is fully known;</p><p>3. Uncertainty exists over both the classifier and the causal graph.</p><p>We will henceforth refer to models 1 and 2 as models of partially incomplete information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Optimal Effort Computation</head><p>We start by analyzing the computation of an agent's optimal effort in Models 1 and 2:</p><p>Models 1 and 2: Since uncertainty manifests in the form of Gaussian priors for the agent (as per assumption), it is easy to see that for information models 1 and 2 above, the overall uncertainty is also Gaussian: i.e., Ch is a Gaussian random variable. In that case, we can rewrite the agent's optimization problem as follows:</p><formula xml:id="formula_24">e ⋆ (Π h , Π C ) = arg min Cost(e)<label>(10)</label></formula><formula xml:id="formula_25">s.t. P (Ch)∼N (µ Ch ,Σ Ch ) (Ch) ⊤ e ≥ α ≥ 1 -δ.</formula><p>Our first result is that above problem is a convex optimization problem under Models 1 and 2.</p><p>Lemma 4. Under partially incomplete information (models (1) and (2)) and cost functions given by Eq. (2), the agent's optimization problem to find the optimal effort profile e ⋆ , given by (10), is a convex program for any δ ≤ 1 2 . The proof can be found in Appendix C.1. The last result shows that under limited uncertainty, the agent can still efficiently solve for an effort profile that helps her to pass the classifier with high probability.</p><p>Remark 2. We note however that this optimization problem (10) is not always feasible: To see this, take a look at the worst case when an agent has no information about the problem: i.e., Σ Ch → +∞. Then an agent who is acting blind manipulates in a direction that lowers their true score h ⊤ 0 x with probability exactly 1/2. I.e., with probability at least 1/2, they never pass the classifier, and a probability of 1 -δ with δ small cannot be guaranteed. As δ → 0, the uncertainty intuitively makes it impossible for the agent to guarantee that they will pass the classifier, making the problem also infeasible.</p><p>To further investigate this, we provide a complete characterization of when optimization problem (10) is feasible as a function of δ. The following result highlights the trade-off between the degree of uncertainty in the model and the highest coverage probability (1 -δ) that can be achieved.</p><p>Lemma 5. Suppose that Σ Ch is positive definite. Then the optimization problem (10) is:</p><formula xml:id="formula_26">1. feasible when δ &gt; Φ -1 -∥Σ -1/2</formula><p>Ch µ Ch ∥ 2 , and 2. infeasible otherwise, where Φ -1 (•) indicates the inverse of the standard normal CDF.</p><p>The proof is given in Appendix C.3. Here it is also worth pointing out the main difference with the complete information setting -with complete information, an agent can always pass the classifier by choosing effort correctly, unlike the incomplete information setting where a positive outcome is not guaranteed.</p><p>Model 3: We now show that under model (3) when there is uncertainty over both the classifier and the causal graph, solving for the optimal effort profile is a non-convex problem in general, and classical optimization algorithms cannot be directly used here.</p><p>Proposition 5. Under incomplete information model (3) and cost functions given by Eq. (2), the agent's optimization problem, given by (9), is a non-convex program.</p><p>The proof can be found in Appendix C.2. Since solving non-convex optimization problems to global optimality can be NP-hard in the worst case, the above result shows that Model 3 is likely not computationally tractable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Characterization of Optimal Effort Profiles</head><p>Under partially incomplete information (uncertainty over either the principal's classifier or the edge weights of the causal graph, but not both) with Gaussian priors, we have shown that the agent's optimization problem reduces to the following convex program:</p><formula xml:id="formula_27">e ⋆ (Π h , Π C ) = arg min e Cost(e)<label>(11)</label></formula><formula xml:id="formula_28">s.t. α -µ ⊤ Ch e -p δ • ||Σ 1/2</formula><p>Ch e|| 2 ≤ 0, where (Ch) ∼ N (µ Ch , Σ Ch ), δ ≤ 1 2 and p δ = Φ -1 (δ). Our goal is to gain insights into the agent's optimal effort profile for the cost function class outlined in Eq. ( <ref type="formula" target="#formula_4">2</ref>). While Program (11) can be complex and highly non-convex in the general case, we highlight that we can still obtain structural results for ℓ 1 -costs and a semi-closed form characterization for ℓ 2 -costs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Case 1 (Weighted ℓ 1 -norm costs)</head><p>We have the following optimization problem for the agent:</p><formula xml:id="formula_29">min e f ∈F c f |e f | s.t. α -µ ⊤ Ch e -p δ • ||Σ 1/2</formula><p>Ch e|| 2 ≤ 0.</p><p>Our first result shows that there is a sharp contrast in the structure of the optimal effort profile for ℓ 1 costs between the complete information setting and partially incomplete information setting. Lemma 6. Under partially incomplete information, the optimal effort profile e ⋆ for an agent with weighted ℓ 1 -norm costs requires investment of effort into more than one feature in the worst case.</p><p>The proof can be found in Appendix C.4.</p><formula xml:id="formula_30">4.2.2 Case 2 (ℓ 2 -norm costs)</formula><p>We have the following optimization problem for the agent:</p><formula xml:id="formula_31">min e ||e|| 2 s.t. α -µ ⊤ Ch e -p δ • ||Σ 1/2</formula><p>Ch e|| 2 ≤ 0. Theorem 3. The optimal effort profile e ⋆ for an agent with ℓ 2 -norm cost function under partially incomplete information, is of the following form:</p><formula xml:id="formula_32">e ⋆ = λ * (k 1 I + k 2 Σ Ch ) -1 µ Ch ,</formula><p>where k 1 , k 2 , λ * &gt; 0.</p><p>The full proof can be found in Appendix C.5.</p><p>An interpretable special case. We study a special case of our problem where we can provide an intuitive explanation of how agents decide to exert effort. In particular, we assume the following.</p><p>Assumption 1. Σ Ch is a diagonal matrix. We denote (Σ Ch ) f the f -th diagonal entry, which corresponds to the uncertainty with respect to the total contribution by feature f . We first note that Σ Ch being diagonal arises in very natural settings. One such setting is when i) the uncertainty is on the causal graph G and ii) the causal graph is bipartite: i.e., features are either causal (they affect other features, but cannot be affected themselves) or proxy (they are affected by causal features, but cannot affect any other feature). In this case, causal features only have outgoing edges, while proxy features only have incoming edges. Formally: Proposition 6. Suppose G is a bipartite graph; further, suppose the agent only has uncertainty over the weights of the graph (model 2), i.e. Σ h = 0. Then, Σ Ch is a diagonal matrix.</p><p>The proof can be found in Appendix C.6. We now highlight our result relating the effort spent on feature f to the total expected contribution of that feature, (µ Ch ) f , and the variance of the contribution of said feature, (Σ Ch ) f : Corollary 1. If Σ Ch is a diagonal matrix with entries (Σ Ch ) f corresponding to feature f , then the optimal effort profile e ⋆ has the following form:</p><formula xml:id="formula_33">e ⋆ f = λ * (µ Ch ) f k 1 + k 2 • (Σ Ch ) f ∀ f ∈ F.</formula><p>This result shows that in the optimal effort profile, the agent invests more effort into features that have a higher expected contribution (µ Ch ) f . Further, the denominator highlights that agent may shy away from features they have a lot of uncertainty about: a high value of (Σ Ch ) f leads to a lower effort invested in that feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">β-desirability under Incomplete Information</head><p>We conclude this section with a discussion on how to induce β-desirable effort profiles under incomplete information. As we see throughout Section 4.2, it may be difficult to characterize the agent's optimal effort in closed form under incomplete information, except for some limited cases.</p><p>Here, we focus on providing broad insights here and build on this discussion through numerical experiments in Section 5.</p><p>The Interpretable Special Case: Diagonal Covariance Σ Ch : In this special setting where we have an interpretable form of the agent's optimal effort profile, we can identify conditions that guarantee investment in β-desirable effort profiles by rational agents. We present the following result:</p><p>Corollary 2. Suppose that the covariance matrix of feature importance, given by Σ Ch , is a diagonal matrix. In that setting, if all features have the same overall level of uncertainty and the mean feature importance vector µ Ch satisfies (which follows from Corollary 1):</p><formula xml:id="formula_34">∥ (µ Ch ) D ∥ 2 ≥ β 1 -β 2 ∥ (µ Ch ) U ∥ 2 ,</formula><p>then the best response of a rational agent with ℓ 2 -norm cost is to invest in a β-desirable effort profile.</p><p>The above result should be intuitive-when agents face the same degree of uncertainty about the importance of all features, they choose which features to invest effort in based on the mean importance of the features. Therefore, it makes sense that the higher the total net importance (measured by the ℓ 2 -norm) of the set of desirable features, higher the incentive for agents to invest in desirable effort profiles. Finally, we relate β-desirability to the uncertainty on given features:</p><formula xml:id="formula_35">Corollary 3. e ⋆ f = λ * (µ Ch ) f k 1 +k 2 •(Σ Ch ) f is decreasing in (Σ Ch ) f .</formula><p>In the general case where different features have different levels of uncertainty, if desirable features have a high degree of uncertainty, this pushes agents away from β-desirable effort profiles. On the other hand, more uncertainty on undesirable features is good for β-desirability. This is because having a higher degree of uncertainty (higher variance (Σ Ch ) f ) about the importance of a feature actively discourages agents from investing effort into said feature.</p><p>What does it mean for Σ Ch to not be diagonal? We provide a short discussion of when non-diagonal covariance matrix arise. In particular, non-diagonal covariance matrices Σ Ch arise:</p><p>1. always under Model 1 (i.e. where the causal graph is fully known, but there is uncertainty over the classifier), and 2. under Model 2 when the causal graph G is not bipartite.</p><p>We explore the non-diagonal Σ Ch case in greater detail in Section 5, with experiments on real data that consider more general cases where Σ Ch may not be diagonal, and in particular covering Model 1, when the classifier is not fully known to an agent. Our experiments suggest that many of the same insights about β-desirability hold (even without the assumption that Σ Ch is diagonal).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Numerical Experiments</head><p>Our experimental study focuses on a setting where the learner is trying to reduce a population's risk of cardiovascular disease. To do so, we identify relevant features and build a causal graph based on the recent medical study of <ref type="bibr" target="#b17">Hasani et al. [2024]</ref>. Their study aims to identify the causal links between features such as smoking, diet, or obesity, and whether a patient may develop a cardiovascular disease (CVD). The study is based on an expert survey where several experts were asked to quantify whether specific links between two features as well as links between features and the outcome variable (CVD) are causal or merely correlated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Setup</head><p>Identifying Relevant Features We identify a subset of 8 features used in Hasani et al.</p><p>[2024] that we focus on in our experimental study. In particular, we did not include features that cannot be changed such as age or ethnicity, and only include the features that can be modified by an individual. The 8 features we identified are: alcohol consumption, diet, physical activity, smoking, diabetes mellitus (DM), hyperlypidia (HPL), hypertension (HPT), and obesity. We normalized features to be between [0, 1]<ref type="foot" target="#foot_6">foot_6</ref> . Among these features, and as noted in our introduction, a principal (i.e., a doctor) would like to incentivize people to focus on preventative, lifestyle interventions over medical treatment interventions. Hence, we separate them to desirable and undesirable to modify as follows:</p><p>• Desirable: Alcohol, Diet, Activity, Smoking. Note that desirable means here that these features are desirable to modify, not that, for example, alcohol consumption is desirable.</p><p>• Undesirable: DM, HPL, HPT, Obesity. Note that these features are not "undesirable" per se, but rather less desirable than lifestyle interventions.</p><p>Building the Causal Graph: The study of <ref type="bibr" target="#b17">Hasani et al. [2024]</ref> asked the experts to report the likelihood of causation on a Likert scale from 1 to 7, which is then transformed into "fuzzy score" via the Fuzzy Delphi Method Linstone et al. <ref type="bibr">[1975]</ref>. We denote this score s. A fuzzy score of 0.5 and above indicates that experts at least moderately agree with a relationship being causal, with an increasing score s indicating stronger agreement. A fuzzy score of 0.5 or below indicates that the experts at best disagree with the feature being causal, with the strength of the disagreement increasing as the score goes down. We follow the expert agreement of <ref type="bibr" target="#b17">Hasani et al. [2024]</ref> to build our causal links. Specifically, we identify a link as causal if and only if s &gt; 0.5. Further, since a score of 0.5 denotes that experts are at the boundary of agreeing vs disagreeing on causality, we renormalize our scores to be between 0 and 1: to do so, we apply a linear transformation that maps s = 0.5 to a causal weight of 0, and s = 1 to a causal weight of 1. We obtain the following graph (Figure <ref type="figure" target="#fig_1">2</ref>):</p><p>Generating Prior Beliefs We note that our desirable features are generally harder to observe than our undesirable features. First, DM, HPL, HPT, and Obesity are easy-to-quantify features that are also verifiable by a doctor (e.g., though blood work). On the other hand, lifestyle habits are not only hard to observe, but also often mis-reported to clinicians (i.e., under-reporting alcohol consumption, or lying about smoking to avoid insurance upcharges). Hence, we generate all our priors of h to have both a mean and variance of 0 for all desirable features (i.e., it is fully known that desirable features are not observed by a clinician, and so not used in the clinician's classifier for high risk of CVD), as they are effectively unobservable. We consider four mean beliefs µ h on the vector h, that we denote as follows:</p><p>• DM : There is a weight of 1 on the "DM" feature, and 0 on all others.</p><p>• HPL: There is a weight of 1 on the "HPL" feature, and 0 on all others.</p><p>• HPT : There is a weight of 1 on the "HPT" feature, and 0 on all others.</p><p>• Obesity: There is a weight of 1 on the "Obesity" feature, and 0 on all others.</p><p>We note that all other beliefs that only use undesirable, observable features are a linear combination of the four beliefs above, so our insights extend to general classifiers. Also, we demonstrate later that while the classifier does not put any weight on unobserved/desirable features, agents may still exert effort on them because they affect the observed/undesirable features used in the classifier.</p><p>The variance of each of the desirable features is taken to be 0 (there is complete information that no weight is put on these features in the principal's classifier). Further, we assume that all undesirable features have the same variance, which is parametrized by σ &gt; 0 -thus σ is a measure of the level of incomplete information. Finally, the covariance matrix of the classifier (Σ h ) is taken to be diagonal for simplicity of exposition and interpretation, i.e., individuals' beliefs do not encode correlations between features in the deployed classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Experimental Results</head><p>Note that we are operating under the regime where the agent has uncertainty only over the classifier h and not over the causal graph (Model 1), hence the contribution matrix C is fully known to agents. Agents are assumed to have unweighted ℓ 2 -norm costs. For each of the four classifiers described above, we document the mean contribution of each feature, given by µ Ch = Cµ h and the ℓ 2 norm of the mean contribution over the set of desirable and undesirable features, given by ℓ 2 (D) and ℓ 2 (U) respectively. All values are recorded in Table <ref type="table" target="#tab_0">1</ref>. Also, note that due to our choice of the variance structure on h (given by matrix Σ h = Diag(0, 0, 0, 0, σ 2 , σ 2 , σ 2 , σ 2 ) ), all four classifiers have the same covariance over the contribution vector Ch given by Cov(Ch) = CΣ h C ⊤ .</p><p>Desirable features can be incentivized even if they are never observed. In our four classifiers of choice, observe that no weight has been put on any of the features in set D because they represent features which cannot be directly observed. However, Figures <ref type="figure" target="#fig_2">3</ref> and<ref type="figure">4</ref> demonstrate that agents still choose to invest significant effort into desirable features. This is a direct result of the causal relationship between features. Observe that in the causal graph, the desirable features influence multiple undesirable features simultaneously. This means that an agent obtains a larger improvement in the undesirable features (which actually affect the agent's classification), not by modifying them directly, but by investing effort into desirable features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of total contribution on desirable vs undesirable features:</head><p>From table 1, it is clear that all four classifiers have a higher ℓ 2 -norm on mean contribution over the set of desirable features compared to undesirable features. Naturally, we expect all 4 classifiers to achieve β &gt; 0.5 under most circumstances (except at high levels of uncertainty), i.e., more than half of the total magnitude of effort should be invested into desirable features (as per Corollary 1). Indeed, this is in line with what we observe in Figure <ref type="figure" target="#fig_2">3</ref>. We also note that, as expected, HPT is better than Obesity, which is better than DM, which is better than HPL for incentivizing desirable features: this is consistent with the ordering over ℓ 2 (D) across these four classifiers, again following the insights of Corollary 1.</p><p>Effect of uncertainty level σ. In Figure <ref type="figure" target="#fig_2">3</ref>, we plot how β varies as a function of the uncertainty parameter σ for different classifiers. Higher σ indicates a higher degree of uncertainty about the classifier. As σ increases, all four classifiers degrade in terms of desirability (β). This is intuitive: at higher levels of uncertainty, the contribution of desirable features sees higher variance, as they affect not only themselves but also other features. Undesirable features then become safer to modify.</p><p>Effect of the failure probability δ. At a fixed level of uncertainty σ, as the failure probability δ increases, β improves across all four classifiers (Figure <ref type="figure">4</ref>). This is again expected-higher δ means that the agent is less stringent on the coverage probability requirement and therefore has a much larger space of feasible effort profiles to choose from. Since all features have equal costs, her best response is to invest more in desirable features because they have a higher net contribution which means that she can now "pass" the classifier while incurring a lower cost.</p><p>Trade-offs between σ and δ. The failure probability δ is closely related to the level of uncertainty σ. At a fixed level of uncertainty σ, there is a limit on how low a failure probability δ can be achieved (Figure <ref type="figure">4</ref>). Similarly, in order to achieve a given failure rate δ, there is a maximum amount of uncertainty σ that can be tolerated (Figure <ref type="figure" target="#fig_2">3</ref>); beyond that the problem becomes infeasible. This closely tracks our theoretical findings in Section 4 (Lemma 5). As δ increases (the agent imposes a weaker requirement on the coverage probability), a higher degree of uncertainty can be tolerated. Finally at δ = 0.5, the level of uncertainty becomes irrelevant -this is because at δ = 0.5, the agent wants to pass the classifier with a probability of 1 2 and therefore can afford to make decisions just on the basis on the mean belief classifier µ h . Effect of α. α represents the amount by which the agent is shy of a positive classification outcome. In this case, we see that β has no dependence on α (Figure <ref type="figure">4</ref>). This is an artifact of the ℓ 2 -norm cost function -the agent's optimal effort profile is proportional to α in each feature and therefore β remains unaffected. However, note that α does affect the cost incurred by the agent, the farther she is from the classification boundary (higher α), the higher is the cost incurred.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>In this paper, we adopt a causal perspective to the problem of strategic classification. The principal deploys a linear classifier which classifies agents "positive" or "negative" based on a set of features embedded on a causal graph. Since agents are strategic, they are expected to invest effort "cleverly" to modify their features in the hopes of a positive classification outcome while incurring the minimum possible cost. Therefore, understanding how agents respond when they have different levels of access to information about the deployed classifier and the causal graph, is significant to the principal from the perspective of classifier design. The principal's goal is the following: design a classifier which incentivizes agents to invest in desirable effort profiles.</p><p>The main contributions of our paper are two-fold: i) We characterize the design space of desirable classifiers for the broad class of weighted ℓ p -norm agent cost functions under complete information, while clearly demonstrating computational challenges of finding such classifiers. We also identify special settings and relaxations which can render the design problem computationally tractable.</p><p>ii) We try to understand strategic agent behavior in response to classifiers under incomplete information settings. In particular, we show that under totally incomplete information (uncertainty over both the classifier and the causal graph), finding the agent's best response is computationally difficult. However it becomes tractable under partially incomplete information where there is uncertainty over either the classifier or the causal graph and we provide insights about the structural properties of the agent's best response under this setting. Finally, we use these results to gain some useful insights (through numerical experiments) into the question of how to design desirable classifiers, even under incomplete information.</p><p>There are many avenues of future work. Our model of uncertainty involves agents having Gaussian priors over the classifier or the edge weights of the causal graph or both, under the assumption that the causal graph topology is always fully known. In real life, there may be other forms of uncertainty -for example, when there are a large number of features, it may be unreasonable to assume that agents have complete knowledge about the causal relationships between features. It may also be interesting to explore if there are other kinds of information structures which are more interpretable -for example, instead of priors independently on the classifier and the causal graph, agents have access to an ordering (or partial ordering) on features in terms of their relative importance. This information structure subsumes necessary information from both the classifier and the causal graph but is easier to understand and hence, might be easier to respond to. Our work also has interesting extensions in the domain of fairness. Different populations may have different levels of uncertainty in their priors which might lead to markedly different abilities of each of those groups to respond to the principal's classifier -the downstream fairness in classification of such information asymmetry may be worth exploring. Let e ⋆ be the optimal effort profile for the agent, i.e., the profile that corresponds to the solution of 5. First, note that for any feature f ∈ F, (Ch 0 ) f = 0 implies e ⋆ f = 0. Indeed, if feature f has no contribution towards the classification decision, then no effort should be expended on f in the optimal effort profile. Now, we can focus on features for which (Ch 0 ) f ̸ = 0. When (Ch 0 ) f &lt; 0, we will show that e ⋆ f ≤ 0. Suppose that e ⋆ f &gt; 0. In this case, we can construct a new effort profile e ′ as follows: e ′ f = 0 and e ′ g = e ⋆ g for all g ∈ F, g ̸ = f . It is easy to see that e ′ is still feasible but Cost(e ′ ) &lt; Cost(e ⋆ ) which contradicts the fact that e ⋆ is the optimal solution. Therefore, e ⋆ f ≤ 0. Similarly, we can show that when (Ch 0 ) f &gt; 0, e ⋆ f ≥ 0. The above discussion implies that whenever (Ch 0 ) f ̸ = 0, then (Ch 0 ) f e ⋆ f ≥ 0 -therefore, without loss of generality, it suffices to assume (Ch 0 ) f &gt; 0 and only search over the space e ≥ 0 (because the optimal solution e ⋆ ≥ 0). This concludes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Additional Details for Experimental Section 5 Supplementary Tables</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Proof of Lemma 1</head><p>The optimization problem in (7) (which we will call the primal problem P ) is a linear program whose feasible region is given by the following polyhedron Q = e ∈ R n+k : (Ch 0 ) ⊤ e ≥ α, e ≥ 0 . Our first goal is to argue that the optimal solution is a corner point of Q which requires us to prove the following: i) firstly, Q has at least one corner point, and ii) the optimal solution is bounded which would imply that it must be at a corner point of Q. Note that the polyhedron Q has no line (because it is a subset of the positive orthant) and therefore, it must have at least one corner point 8 . We can now write the dual problem (D) as follows: We know that the dual problem (D) is feasible (π = 0 is feasible to D) which implies that the optimal solution to (P ) cannot be unbounded. Hence, we conclude that there must exist a corner point optimal solution to problem (P ). Now, note that all corner points of Q are obtained by the intersection of the hyperplane (Ch 0 ) ⊤ e ≥ α with the positive axes. So any corner point of Q must be of the form where exactly one entry corresponding to some feature f is positive (i.e., takes value α (Ch 0 ) f ) and all other entries are zero. This implies that there exists an optimal effort profile where the agent needs to modify exactly one feature, proving the first part of the lemma.</p><p>For proving the second part, we will use the complementary slackness conditions on the dual constraints. We already know that there exists an optimal primal solution where there is some feature f ⋆ with e f ⋆ = α (Ch 0 ) f ⋆ and e f ′ = 0 for all f ′ ̸ = f ⋆ . Let π ⋆ be the optimal dual solution. Using complementary slackness, we know that π ⋆ • (Ch 0 ) f ⋆ = c f ⋆ which implies that:</p><formula xml:id="formula_36">π ⋆ = c f ⋆ (Ch 0 ) f ⋆ .</formula><p>Since π ⋆ must also be feasible to (D), we must have:</p><formula xml:id="formula_37">π ⋆ ≤ c f ′ (Ch 0 ) f ′ ∀ f ′ ̸ = f ⋆ , (Ch 0 ) f ′ ̸ = 0,</formula><p>which implies that:</p><formula xml:id="formula_38">f ⋆ ∈ arg max f ∈F (Ch 0 ) f c f .</formula><p>This concludes the proof of the lemma.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Proof of Lemma 2</head><p>We solve the constrained optimization problem using Lagrange multipliers. Define the Lagrangian as follows:</p><formula xml:id="formula_39">L(e, π) =   f ∈F c f (e f ) p   1/p + π -(Ch 0 ) ⊤ e + α ,</formula><p>where π is the Lagrange multiplier associated with the constraint as defined earlier. This gives us the following set of KKT conditions:</p><formula xml:id="formula_40">∇ e L(e, π) = 0, π • (-(Ch 0 ) ⊤ e + α) = 0, π ≥ 0, α -(Ch 0 ) ⊤ e ≤ 0, e ≥ 0.</formula><p>8 for more details on the polyhedral theory related to linear optimization, refer to <ref type="bibr">Bertsimas and Tsitsiklis [1997]</ref> Since our optimization problem is convex, it suffices to find a pair (e ⋆ , π ⋆ ) that satisfies the KKT conditions and we can automatically conclude that e ⋆ is optimal to the primal problem. First, we show that the constraint (Ch 0 ) ⊤ e ≥ α must be active at the optimal solution. We prove this by contradiction. Suppose, if possible that -(Ch 0 ) ⊤ e ⋆ +α &lt; 0. However, this means that we can obtain the optimal solution e ⋆ by solving the primal problem as if it were unconstrained. In that case, it must be that e ⋆ = 0, but observe that e = 0 is not even feasible (and hence cannot be optimal). This implies that the constraint must hold at equality. Therefore, we can solve for e ⋆ and π * by solving the following system:</p><formula xml:id="formula_41">-(Ch 0 ) ⊤ e + α = 0, ∇ e L(e, π) = 0. Now, (∇ e L(e, π)) f = ∂L ∂e f = c f (e f ) p-1   f ∈F c f (e f ) p 1/p   p-1 -π • (Ch 0 ) f .</formula><p>We have already argued that e ⋆ ̸ = 0. Therefore, π ⋆ &gt; 0. This implies that for all features f ∈ F, whenever (Ch 0 ) f &gt; 0, we must have:</p><formula xml:id="formula_42">e ⋆ f ∝ (Ch 0 ) f c f 1/(p-1)</formula><p>, and when (Ch 0 ) f = 0, the condition holds trivially. This concludes the proof of the lemma.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 Proof of Lemma 3</head><p>The set of β-desirable classifiers H is given as follows:</p><formula xml:id="formula_43">H := h 0 ∈ R |F | : e ⋆ (h 0 , C) is β-desirable, Ch 0 ≥ 0 .</formula><p>We define the set Z := {(Ch 0 ) : h 0 ∈ H}. Suppose that C is full row-rank. This implies that H is convex if and only if Z is convex<ref type="foot" target="#foot_7">foot_7</ref> . Therefore, in order to complete the proof, it suffices to show that the transformed set Z is non-convex in the worst case. We now provide instances of problems where Z is non-convex and the agents incur ℓ p -norm cost functions with p = 1 and p &gt; 1. Recall from Theorems 1 and 2 that the set Z is given as follows:</p><formula xml:id="formula_44">Z = z ∈ R |F | ≥0 : max f ∈U z f c f &lt; max f ∈D z f c f (p = 1) Z =      z ∈ R |F | ≥0 :   f ∈D z f c f 2/(p-1)   1/2 ≥ β 1 -β 2   f ∈U z f c f 2/(p-1)   1/2      (p &gt; 1)</formula><p>Weighted ℓ 1 -norm cost function: Consider a setting where there are 4 features with D = {1, 2} and U = {3, 4}. Suppose that the cost vector equals c = 1. In this case,</p><formula xml:id="formula_45">Z = z ∈ R 4 ≥0 : max(z 3 , z 4 ) &lt; max(z 1 , z 2 ) .</formula><p>Now, choose z ′ := (4, 7, 3, 6) and z ′′ := <ref type="bibr">(7,</ref><ref type="bibr">4,</ref><ref type="bibr">3,</ref><ref type="bibr">6)</ref>. Both are clearly points in Z. However, for α = 0.5, αz ′ + (1 -α)z ′′ := (5.5, 5.5, 3, 6) / ∈ Z. Therefore, Z is not a convex set.</p><p>Weighted ℓ p -norm cost function: Consider a setting where there are 3 features with D = {1, 2} and U = {3}. Let p = 2, c = 1 and β = 1 √ 2 . Then Z is given by:</p><formula xml:id="formula_46">Z = z ∈ R 3 ≥0 : z 2 1 + z 2 2 ≥ z 3</formula><p>(0, 1, 1) and (1, 0, 1) are points in Z, but the point halfway between them, given by (0.5, 0.5, 1) is clearly not in Z. Therefore, Z is not a convex set. This concludes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5 Proof of Proposition 3</head><p>We will verify convexity separately for the cases with ℓ 1 -norm and ℓ p -norm (1 &lt; p ≤ 3) cost functions.</p><p>The ℓ 1 -norm case: When the cost function is a weighted ℓ 1 -norm, the set of desirable classifiers is given by</p><formula xml:id="formula_47">H := h 0 ∈ R |F | : Ch 0 ≥ 0, max f ∈U (Ch 0 ) f c f &lt; max f ∈D (Ch 0 ) f c f .</formula><p>Now, suppose |D| = 1 and there is some feature f d ∈ D. Then, we can rewrite H as follows:</p><formula xml:id="formula_48">H := h 0 ∈ R |F | : Ch 0 ≥ 0, max f ∈F \{f d } (Ch 0 ) f c f - (Ch 0 ) f d c f d &lt; 0 .</formula><p>In order to show that H is a convex set, it suffices to show that the function g(h</p><formula xml:id="formula_49">0 ) = max f ∈F \{f d } (Ch 0 ) f c f - (Ch 0 ) f d c f d</formula><p>is a convex function. Function g(•) corresponds to the sum of a maximum of linear functions (which is convex) and a linear function; hence, function g(•) is convex.</p><p>The ℓ p -norm case with p &gt; 1: For ℓ p -norm cost functions with p &gt; 1, set H is given by:</p><formula xml:id="formula_50">H ≜      h 0 ∈ R |F | : Ch 0 ≥ 0,   f ∈D (Ch 0 ) f c f 2/(p-1)   1/2 ≥ β 1 -β 2   f ∈U (Ch 0 ) f c f 2/(p-1)   1/2     </formula><p>Using the fact that |D| = 1, we can rewrite H as follows:</p><formula xml:id="formula_51">H :=      h 0 ∈ R |F | : Ch 0 ≥ 0, (Ch 0 ) f d ≥ K   f ∈U (Ch 0 ) f c f 2/(p-1)   (p-1)/2      where K = c f d β √ 1-β 2 (p-1)</formula><p>&gt; 0. Now in order to complete the proof, we need to show that the function r(h 0 ) is convex, where r(h 0 ) is given by:</p><formula xml:id="formula_52">r(h 0 ) = K   f ∈U (Ch 0 ) f c f 2/(p-1)   (p-1)/2 -(Ch 0 ) f d .</formula><p>When 1 &lt; p ≤ 3, we can rewrite r(h 0 ) as follows: 1) . Note that K∥Bh 0 ∥ 2/(p-1) is a convex function in h 0 since this is a q-norm for q = 2 p-1 ≥ 1. This makes r(h 0 ) a convex function in h 0 (sum of a convex function and a linear function is convex) and concludes the proof.</p><formula xml:id="formula_53">r(h 0 ) = K∥Bh 0 ∥ 2/(p-1) -(Ch 0 ) f d , where B ∈ R (|F |-1)×(|F |-</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Proofs for the Incomplete Information Setting C.1 Proof of Lemma 4</head><p>Since the cost functions defined in Eq. ( <ref type="formula" target="#formula_4">2</ref>) are convex, in order to complete the proof, it suffices to show that the feasible space of the optimization problem in (10), is convex. We have already argued that under incomplete information models (1) and ( <ref type="formula" target="#formula_4">2</ref> </p><formula xml:id="formula_54">P (Ch) ⊤ e ≥ α ≥ 1 -δ ⇐⇒ Φ c α -µ ⊤ Ch e e ⊤ Σ Ch e ≥ 1 -δ ⇐⇒ Φ α -µ ⊤ Ch e e ⊤ Σ Ch e ≤ δ ⇐⇒ α -µ ⊤ Ch e e ⊤ Σ Ch e ≤ p δ (where p δ = Φ -1 (δ)) ⇐⇒ α -µ ⊤ Ch e -p δ • e ⊤ Σ Ch e ≤ 0.</formula><p>When δ = 1 2 , p δ = 0 and the above constraint reduces to a polyhedral constraint, making the problem trivially convex. On the other hand, note that when δ &lt; 1 2 , p δ &lt; 0. Now, since Σ Ch is a covariance matrix, it is always symmetric and positive semidefinite and therefore, Σ</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1/2</head><p>Ch exists (it is also symmetric and positive semidefinite!). In that case, we can express e ⊤ Σ Ch e as follows:</p><formula xml:id="formula_55">e ⊤ Σ Ch e = e ⊤ Σ 1/2 Ch Σ 1/2 Ch e = (Σ 1/2 Ch e) ⊤ (Σ 1/2 Ch e) = ||Σ 1/2 Ch e|| 2 2 = ||Σ 1/2</formula><p>Ch e|| 2 . Now, ||Σ 1/2 e|| 2 is a convex function in e (because all norms are convex functions). Similarly, -p δ • ||Σ </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Proof of Proposition 5</head><p>In order to complete this proof, it suffices to provide a counter-example where the program in ( <ref type="formula">9</ref>) is non-convex. Consider the simplest possible setting where there can be uncertainty in both the classifier and the causal graph. Suppose, there is only one feature, i.e., |F| = 1. Let ω ∼ N (0, 1) be the random variable that captures the uncertainty in the contribution of the feature (encodes uncertainty in the causal graph) and h ∼ N (0, 1) be the random variable that captures the uncertainty in the classifier weight on the feature. Note that ω ⊥ h. We will show that the feasible space given by: P [(ωh)e ≥ α] ≥ 1 -δ is non-convex, which is equivalent to showing that the function f (e) given by:</p><formula xml:id="formula_56">f (e) = P [(ωh)e ≥ α]</formula><p>is not concave. Below in Figure <ref type="figure" target="#fig_9">5</ref>, we plot f (e) as a function of one-dimensional effort e. Since there is no closed-form expression for the distribution of the product of two independent standard normal random variables, we obtain empirical estimates for the probability at each e using Monte-Carlo simulations. Clearly, f (e) is not concave. This concludes the proof of the proposition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.3 Proof of Lemma 5</head><p>Recall that the feasible space of the agent's optimization problem in the partially incomplete information case is given by: α</p><formula xml:id="formula_57">-µ ⊤ Ch e -p δ • ∥Σ 1/2</formula><p>Ch e∥ 2 ≤ 0. This feasible space is empty at a given δ if we have:  Now there are 2 cases: ∇g(e) = 0 has a solution ê: Clearly, ê ̸ = 0 because the gradient is not defined at e = 0. In that case, ê satisfies:</p><formula xml:id="formula_58">α -µ ⊤ Ch e -p δ • ∥Σ 1/2 Ch e∥ 2 &gt; 0 ∀ e ⇐⇒ min</formula><formula xml:id="formula_59">-p δ • Σ Ch ê ∥Σ 1/2 Ch ê∥ 2 = µ Ch .</formula><p>Now, ê must be a global minimizer of g because g(•) is a convex function. We will show that g(ê) = α:</p><formula xml:id="formula_60">g(ê) = α -µ ⊤ Ch ê -p δ • ∥Σ 1/2 Ch ê∥ 2 = α + p δ • ê⊤ Σ Ch ê ∥Σ 1/2 Ch ê∥ 2 -p δ • ∥Σ 1/2</formula><p>Ch ê∥ 2 (using the condition from ∇g(ê) = 0)</p><formula xml:id="formula_61">= α + p δ • ∥Σ 1/2 Ch ê∥ 2 -p δ • ∥Σ 1/2 Ch ê∥ 2 = α.</formula><p>Since α &gt; 0, the problem is always infeasible for this particular value of p δ . Since Σ Ch is positive definite, Σ -1/2 Ch exists, therefore we have:</p><formula xml:id="formula_62">p δ = -∥Σ -1/2 Ch µ Ch ∥ 2 ⇐⇒ δ = Φ -1 -∥Σ -1/2 Ch µ Ch ∥ 2 .</formula><p>∇g(e) = 0 has no solution: This means that either the unique optimal solution is at the point where the gradient does not exist, i.e, e = 0, or the solution is unbounded. The first subcase clearly leads to infeasibility as g(0) = α &gt; 0 while the second sub-case leads to a non-empty feasible region for Problem (10). We will now try to derive conditions on δ which lead to each subcase.</p><p>Suppose that the unique optimal solution is e = 0. This means that for any direction d, g(0 + d) &gt; g(0) or equivalently, Ch µ Ch ∥ 2 . Similarly, if the solution is unbounded, there must exist a direction d ′ at 0 such that:</p><formula xml:id="formula_63">α -µ ⊤ Ch d -p δ • ∥Σ 1/2 Ch d∥ 2 &gt; α ∀ d ⇐⇒ -p δ • ∥Σ</formula><formula xml:id="formula_64">α -µ ⊤ Ch d ′ -p δ • ∥Σ 1/2 Ch d ′ ∥ 2 &lt; α,</formula><p>or equivalently, -p δ • ∥Σ</p><formula xml:id="formula_65">1/2 Ch d ′ ∥ 2 &lt; µ ⊤ Ch d ′ .</formula><p>Using a similar argument as above, we can show that this can happen only when:</p><formula xml:id="formula_66">δ &gt; Φ -1 -∥Σ -1/2</formula><p>Ch µ Ch ∥ 2 . This concludes the proof of the lemma.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.4 Proof of Lemma 6</head><p>In order to complete the proof, it suffices to construct an instance of the problem where the optimal effort profile is not a corner point. Consider a setting where |F| = 2, α &gt; 0 and δ &lt; 1 2 . Suppose, the features are identical in all respects, i.e., (µ Ch ) 1 = (µ Ch ) 2 = μ &gt; 0, Σ Ch = σ 2 0 0 σ 2 and c 1 = c 2 = c. Additionally, suppose that μ &gt; -p δ σ. We first make the following observations:</p><p>• If e ⋆ is not a corner point, it must be symmetric, i.e., e ⋆ 1 = e ⋆ 2 .</p><p>• Since μ &gt; 0 and δ &lt; 1 2 , it must be that e ⋆ ≥ 0 (otherwise, we have infeasibility). Now, there are only two possible corner point solutions: either of the form (e, 0) or (0, e). In order for either of them to be optimal, the constraint must be active at that point. Solving, we obtain: e = α μ + p δ σ</p><p>; and Cost = cα μ + p δ σ .</p><p>However, we will now construct a non-corner point solution (e ′ , e ′ ) where the constraint is active and which produces a strictly better objective value. Solving, we obtain:</p><formula xml:id="formula_67">e ′ = α 2 μ + p δ √ 2 • σ ; and Cost = cα μ + p δ √ 2 • σ ,</formula><p>which is strictly smaller than the earlier cost (since p δ &lt; 0). This concludes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.5 Proof of Theorem 3</head><p>We will use the KKT conditions to obtain the agent's optimal effort profile e ⋆ . The Lagrangian L(•, •) for the above problem is given by:</p><formula xml:id="formula_68">L(e, λ) = ||e|| 2 + λ α -µ ⊤ Ch e -p δ • ||Σ 1/2 Ch e|| 2 ,</formula><p>where λ is the Lagrange multiplier. We can now write the KKT conditions as follows:</p><formula xml:id="formula_69">e ||e|| 2 + λ • -p δ • Σ Ch e ||Σ 1/2</formula><p>Ch e|| 2 -µ Ch = 0,</p><formula xml:id="formula_70">p δ • ||Σ 1/2</formula><p>Ch e|| 2 + µ T Ch e = α, λ &gt; 0.</p><p>Since we have a convex program, it is sufficient to find a pair (e ⋆ , λ * ) satisfying the KKT conditions and we can immediately conclude that e ⋆ is an optimal solution to our original problem. Using the first equality above, we infer that the optimal effort e ⋆ must be of the following form:</p><formula xml:id="formula_71">e ⋆ = λ * (k 1 I + k 2 Σ Ch ) -1 µ,</formula><p>where</p><formula xml:id="formula_72">k 1 = 1 ||e ⋆ || 2 &gt; 0 and k 2 = -λ * p δ ||Σ 1/2 Ch e ⋆ || 2</formula><p>&gt; 0 (so, the inverse exists). In order to obtain the exact expression for e ⋆ , we need to use the other equality condition and solve simultaneously for k 1 , k 2 and λ * . This concludes the proof of the lemma.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.6 Proof of Proposition 6</head><p>Since G is a bipartite graph, the set of nodes (in this case, same as set of features) |F| can be partitioned into two sets F out and F in such that F in ∪ F out = F, F in ∩ F out = ∅ and all arcs in A are directed from F out towards F in .</p><p>Recall that Σ Ch is the covariance matrix of Ch where C ∼ Π C and h ∼ Π h . However, when there is uncertainty only over the edge weights of G, it is clear that Σ Ch = Cov(Ch 0 ). Therefore, in order to show that Σ Ch is a diagonal matrix, it suffices to show that:</p><formula xml:id="formula_73">∀ f 1 , f 2 ∈ F, f 1 ̸ = f 2 , (Ch 0 ) f 1 ⊥ (Ch 0 ) f 2 ,</formula><p>i.e., (Ch 0 ) f 1 and (Ch 0 ) f 2 are independent random variables. Firstly, observe that for any feature f ∈ F in , we must have:</p><p>(Ch 0 ) f = 0. This is because feature f has no outgoing edges (since f ∈ F in ) and therefore, C f,. = 0 ⊤ which implies (Ch 0 ) f = C f,. h 0 = 0. This automatically implies that the covariance of (Ch 0 ) f with any other random variable is also zero. Therefore, we only need to prove that Cov ((Ch 0 ) f 1 , (Ch 0 ) f 2 ) = 0 when f 1 , f 2 both are in F out . Note that:</p><formula xml:id="formula_74">(Ch 0 ) f 1 = f ∈F C f 1 ,f h 0,f and (Ch 0 ) f 2 = f ∈F C f 2 ,f h 0,f .</formula><p>Therefore,</p><formula xml:id="formula_75">Cov ((Ch 0 ) f 1 , (Ch 0 ) f 2 ) = Cov   f ∈F C f 1 ,f h 0,f , f ∈F C f 2 ,f h 0,f   = f ∈F f ′ ∈F (h 0,f • h 0,f ′ ) • Cov(C f 1 ,f , C f 2 ,f ′ )</formula><p>We now argue case by case:</p><p>• f, f ′ ∈ F out : In this case, Cov(C f 1 ,f , C f 2 ,f ′ ) = 0 because there can be no edges from either f 1 or f 2 to f or f ′ since all of them are nodes in F out .</p><p>• f ∈ F out , f ′ ∈ F in : In this case, C f 1 ,f = 0 by the same argument as above. Therefore, the covariance must be 0.</p><p>• f ′ ∈ F out , f ∈ F in : In this case, C f 2 ,f ′ = 0 which makes the covariance 0.</p><p>• f ∈ F in , f ′ ∈ F in : Finally, if both f and f ′ are in F in , there can be edges from f 1 and f 2 towards f and f ′ . But those edges are disjoint and therefore, independent which makes the covariance term 0.</p><p>This concludes the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Supplementary Proofs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.1 Proof of Observation 1</head><p>The key step to complete the proof is to show that A k ij captures the influence exerted by feature i on feature j through a directed path on the graph that is exactly k hops long. We will prove by induction.</p><p>Base case (k = 0): When k = 0, there exists no directed path from feature i to feature j unless i = j. Therefore, all off-diagonal entries are 0. The only entries appear on the diagonal because feature i affects itself with a unit positive multiplier. This gives us the identity matrix in |F| dimensions which is exactly given by A 0 .</p><p>General case: Suppose that the induction hypothesis holds for some k &gt; 1. We will now show that it also holds for k + 1. Note that:</p><formula xml:id="formula_76">A k+1 ij = |F | n=1 A k in • A nj .</formula><p>Since the induction hypothesis is true, A k in captures the influence exerted by feature i on feature n through a directed path exactly k hops long. A nj represents the direct influence exerted by feature n on feature j (in exactly 1 hop). Therefore, the product measures the influence of feature i on feature j exerted on a directed path k + 1 hops long. The sum over all features in F captures all such directed paths from i to j. Thus, our induction hypothesis is also true for k + 1.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Consider a simple causal graph G with |F| = 3. Feature 1 directly affects features 2 and 3 and feature 2 directly affects feature 3. Feature 1 also indirectly affects feature 3 through the path 1 → 2 → 3. The contribution matrix C captures both of these effects.</figDesc><graphic coords="8,107.10,72.00,397.81,134.11" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Causal graph of features which affect the output of interest "Risk of Cardio-vascular disease (CVD)". There are 8 features, all of which are causal. The features at the bottom form the set of desirable features D and those on the top form the set of undesirable features U. The causal links are indicated on the graph. This causal graph has a special structure: it is bipartite. The edge weights are recorded inTable 2 in Appendix A.</figDesc><graphic coords="18,189.00,72.00,234.00,139.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Plot of how β varies with σ at different levels of δ and for different classifiers.</figDesc><graphic coords="20,72.00,72.00,154.44,110.08" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Figure 4: Plots of how β varies with δ for different parameter combinations.</figDesc><graphic coords="21,72.00,190.99,140.41,100.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Features</head><label></label><figDesc>Adjacency matrix A which captures the edge weights of the causal graph in Figure 2 B Proofs for the Complete Information Setting B.1 Proof of Proposition 1</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>), Ch is a multivariate Gaussian, i.e, Ch ∼ N (µ Ch , Σ Ch ) for some µ Ch ∈ R |F | and Σ Ch ∈ R |F |×|F | . This implies, (Ch) ⊤ e ∼ N µ ⊤ Ch e, e ⊤ Σ Ch e . This allows us to rewrite the LHS of the probability constraint as follows: P (Ch) ⊤ e ≥ α = P (Ch) ⊤ e -µ ⊤ Ch e e ⊤ Σ Ch e ≥ α -µ ⊤ Ch e e ⊤ Σ Ch e = P Z ≥ α -µ ⊤ Ch e e ⊤ Σ Ch e (where Z ∼ N (0, 1)) = Φ c α -µ ⊤ Ch e e ⊤ Σ Ch e . Therefore,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Ch e|| 2 is also a convex function because -p δ &gt; 0. The term α -µ ⊤ Ch e is affine in e and therefore, convex by default. Putting everything together, we conclude that α-µ ⊤ Ch e-p δ • e ⊤ Σ Ch e is a convex function in e which makes the constraint:α -µ ⊤Ch e -p δ • e ⊤ Σ Ch e ≤ 0 a convex constraint. This concludes the proof of the proposition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Ch e∥ 2 &gt; 0. Now consider the convex unconstrained optimization problem: min e g(e). Then,∇g(e) = -µ Ch -p δ • Σ Ch e ∥Σ 1/2Ch e∥ 2 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Plot of f (e) with α = 1</figDesc><graphic coords="31,189.00,72.00,234.00,175.50" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>Ch d∥ 2 &gt; µ ⊤ Ch d ∀ d. Note that µ ⊤ Ch d = µ ⊤ Ch ΣCh d∥ 2 where the last inequality follows from the Cauchy-Schwartz inequality. In fact, for d * = Σ -1 Ch µ Ch (which exists since Σ Ch is positive definite and hence, invertible), we have equality. But since -p δ • ∥Σ1/2 Ch d∥ 2 &gt; µ ⊤Ch d for all directions d, it must hold for d * as well, which implies:Ch d * ∥ 2 &lt; -p δ • ∥Σ 1/2 Ch d * ∥ 2 , which means that -p δ &gt; ∥Σ -1/2Ch µ Ch ∥ 2 or equivalently, δ &lt; Φ -1 -∥Σ -1/2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Mean contribution vector µ Ch for the 4 classifiers: DM, HPL, HPT, Obesity</figDesc><table><row><cell cols="11">Classifier Alcohol Diet Activity Smoking DM HPL HPT Obesity ℓ 2 (D) ℓ 2 (U)</cell></row><row><cell>DM</cell><cell>0.1</cell><cell>0.84</cell><cell>0.82</cell><cell>0.52</cell><cell>1</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>1.28</cell><cell>1</cell></row><row><cell>HPL</cell><cell>0.14</cell><cell>0.84</cell><cell>0.82</cell><cell>0.34</cell><cell>0</cell><cell>1</cell><cell>0</cell><cell>0</cell><cell>1.23</cell><cell>1</cell></row><row><cell>HPT</cell><cell>0.62</cell><cell>0.84</cell><cell>0.82</cell><cell>0.86</cell><cell>0</cell><cell>0</cell><cell>1</cell><cell>0</cell><cell>1.58</cell><cell>1</cell></row><row><cell>Obesity</cell><cell>0.64</cell><cell>0.86</cell><cell>0.82</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>0</cell><cell>1</cell><cell>1.35</cell><cell>1</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>This is a standard assumption in the causal strategic classification<ref type="bibr" target="#b26">Miller et al. [2020]</ref>,<ref type="bibr" target="#b21">Kleinberg and Raghavan [2020]</ref> </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Throughout the paper, we assume that causal relationships are linear. This is another common assumption in the literature<ref type="bibr" target="#b21">Kleinberg and Raghavan [2020]</ref>,<ref type="bibr" target="#b32">Shavit et al. [2020]</ref> </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>Here, we would like to see the value of the feature lowered. Note that whether we want to increase or decrease the value of a feature has no bearing on whether it is desirable; only the fact that this is one of the target features we aim to change is.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>Gaussian priors are frequently used to model incomplete information<ref type="bibr" target="#b22">Kong et al. [2020]</ref>,<ref type="bibr" target="#b11">Elzayn and Schutzman [2019]</ref> </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>This is without loss of generality, by a simple revelation principle type of argument</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>x → y indicates that x is directly causal for y.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_6"><p>For simplicity and wlog, we assume that 0 is the least "healthy" value of the feature, and 1 is the "healthiest" value of the feature. For example, for smoking, 1 maps to not smoking; for activity, 1 maps to high amount of weekly physical activity.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_7"><p>This is a standard result in linear algebra; the proof provided in Appendix D for completeness</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Finally, to compute C, we need to sum the influences of directed paths of all lengths starting at node i and ending in node j. Since G is a directed acyclic graph with |F| nodes, the length of the maximum directed path from i to j is at most |F| -1 hops long or conservatively |F| hops long (note that if there are no directed paths of length k from i to j, A k ij = 0. So, it does not hurt to be conservative). This leads to the final expression of C:</p><p>To conclude the proof, we need to argue about the time complexity of computing C, given matrix A. Multiplying 2 matrices of size |F| × |F| takes O(|F| 3 ) time and we need to execute O(|F|) such matrix multiplication steps to compute the different powers of A. Therefore, the overall time complexity is polynomial in |F|.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 Proof of Supporting Result in Lemma 3</head><p>We made the following observation in our proof of Lemma 3:</p><p>We provide a formal proof here. We need to show both directions.</p><p>( =⇒ ) Suppose, set X is convex. We need to show that set Y is convex. Let y 1 , y 2 ∈ Y such that y 1 ̸ = y 2 . Pick any λ ∈ [0, 1]. Then there must exist x 1 , x 2 ∈ X such that y 1 = M x 1 and y 2 = M x 2 . Clearly x 1 ̸ = x 2 . Since X is a convex set, λx 1 + (1 -λ)x 2 ∈ X. This implies,</p><p>( ⇐= ) For the other direction, we assume that set Y is convex and we need to show that set X is convex. Pick any two elements x 1 , x 2 ∈ X, x 1 ̸ = x 2 and any λ ∈ [0, 1]. Let y 1 = M x 1 and y 2 = M x 2 . Clearly, y 1 , y 2 ∈ Y (by definition). Note that y 1 ̸ = y 2 (otherwise, we would have</p><p>The last part follows from noting that λy 1 + (1 -λ)y 2 ∈ Y and since M is full row-rank, the pre-image of λy 1 + (1 -λ)y 2 must be unique. This concludes both directions of the proof.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Avrim Blum, and Keziah Naggita. The strategic perceptron</title>
		<author>
			<persName><forename type="first">Saba</forename><surname>Ahmadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hedyeh</forename><surname>Beyhaghi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM Conference on Economics and Computation</title>
		<meeting>the 22nd ACM Conference on Economics and Computation</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="6" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On classification of strategic agents who can both game and improve</title>
		<author>
			<persName><forename type="first">Saba</forename><surname>Ahmadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hedyeh</forename><surname>Beyhaghi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avrim</forename><surname>Blum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keziah</forename><surname>Naggita</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3rd Symposium on Foundations of Responsible Computing</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The economics of moral hazard: further comment</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kenneth</surname></persName>
		</author>
		<author>
			<persName><surname>Arrow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The American economic review</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="537" to="539" />
			<date type="published" when="1968">1968</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Uncertainty and the welfare economics of medical care</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kenneth</surname></persName>
		</author>
		<author>
			<persName><surname>Arrow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Uncertainty in economics</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="1978">1978</date>
			<biblScope unit="page" from="345" to="375" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Gaming helps! learning from strategic interactions in natural dynamics</title>
		<author>
			<persName><forename type="first">Yahav</forename><surname>Bechavod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katrina</forename><surname>Ligett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juba</forename><surname>Ziani</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1234" to="1242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Information discrepancy in strategic learning</title>
		<author>
			<persName><forename type="first">Yahav</forename><surname>Bechavod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chara</forename><surname>Podimata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juba</forename><surname>Ziani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1691" to="1715" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Introduction to linear optimization. 1997. Mark Braverman and Sumegha Garg. The role of randomness and noise in strategic classification</title>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Bertsimas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Tsitsiklis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">1st Symposium on Foundations of Responsible Computing</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning strategy-aware linear classifiers</title>
		<author>
			<persName><forename type="first">Yiling</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chara</forename><surname>Podimata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="15265" to="15276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Bayesian strategic classification</title>
		<author>
			<persName><forename type="first">Lee</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saeed</forename><surname>Sharifi-Malvajerdi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Stangl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Vakilian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Juba</forename><surname>Ziani</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/2402.08758" />
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Strategic classification from revealed preferences</title>
		<author>
			<persName><forename type="first">Jinshuo</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Schutzman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Waggoner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiwei</forename><forename type="middle">Steven</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 ACM Conference on Economics and Computation</title>
		<meeting>the 2018 ACM Conference on Economics and Computation</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="55" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">The double-edged sword of behavioral responses in strategic classification: Theory and user studies</title>
		<author>
			<persName><forename type="first">Raman</forename><surname>Ebrahimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristen</forename><surname>Vaccaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parinaz</forename><surname>Naghizadeh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.18066</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Price of privacy in the keynesian beauty contest</title>
		<author>
			<persName><forename type="first">Hadi</forename><surname>Elzayn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Schutzman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 ACM Conference on Economics and Computation</title>
		<meeting>the 2019 ACM Conference on Economics and Computation</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="845" to="863" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Group-fair classification with strategic agents</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Estornell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanmay</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yevgeniy</forename><surname>Vorobeychik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency</title>
		<meeting>the 2023 ACM Conference on Fairness, Accountability, and Transparency</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="389" to="399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Strategic classification in the dark</title>
		<author>
			<persName><forename type="first">Ganesh</forename><surname>Ghalme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vineet</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Itay</forename><surname>Eilat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inbal</forename><surname>Talgam-Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nir</forename><surname>Rosenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3672" to="3681" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An analysis of the principal-agent problem</title>
		<author>
			<persName><forename type="first">J</forename><surname>Sanford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oliver</forename><forename type="middle">D</forename><surname>Grossman</surname></persName>
		</author>
		<author>
			<persName><surname>Hart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Foundations of Insurance Economics: Readings in Economics and Finance</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1992">1992</date>
			<biblScope unit="page" from="302" to="340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Strategic classification</title>
		<author>
			<persName><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nimrod</forename><surname>Megiddo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christos</forename><surname>Papadimitriou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mary</forename><surname>Wootters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 ACM Conference on Innovations in Theoretical Computer Science</title>
		<meeting>the 2016 ACM Conference on Innovations in Theoretical Computer Science</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="111" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Stateful strategic regression</title>
		<author>
			<persName><forename type="first">Keegan</forename><surname>Harris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hoda</forename><surname>Heidari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">Z</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="28728" to="28741" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Constructing causal pathways for premature cardiovascular disease mortality using directed acyclic graphs with integrating evidence synthesis and expert knowledge</title>
		<author>
			<persName><forename type="first">Wan</forename><surname>Shakira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rodzlan</forename><surname>Hasani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Imran</forename><surname>Kamarul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><forename type="middle">Wee</forename><surname>Musa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kueh</forename><forename type="middle">Yee</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific Reports</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">28849</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Causal strategic classification: A tale of two shifts</title>
		<author>
			<persName><forename type="first">Guy</forename><surname>Horowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nir</forename><surname>Rosenfeld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="13233" to="13253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The disparate effects of strategic manipulation</title>
		<author>
			<persName><forename type="first">Lily</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicole</forename><surname>Immorlica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><forename type="middle">Wortman</forename><surname>Vaughan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Fairness, Accountability, and Transparency</title>
		<meeting>the Conference on Fairness, Accountability, and Transparency</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="259" to="268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Causal machine learning: A survey and open problems</title>
		<author>
			<persName><forename type="first">Jean</forename><surname>Kaddour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aengus</forename><surname>Lynch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><forename type="middle">J</forename><surname>Kusner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Silva</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2206.15475</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">How do classifiers induce agents to invest effort strategically?</title>
		<author>
			<persName><forename type="first">Jon</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manish</forename><surname>Raghavan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Economics and Computation (TEAC)</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1" to="23" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Information elicitation mechanisms for statistical estimation</title>
		<author>
			<persName><forename type="first">Yuqing</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grant</forename><surname>Schoenebeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Biaoshuai</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fang-Yi</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="2095" to="2102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The theory of incentives: the principal-agent model</title>
		<author>
			<persName><forename type="first">Jean-Jacques</forename><surname>Laffont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Martimort</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The theory of incentives</title>
		<imprint>
			<publisher>Princeton university press</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Strategic classification with unknown user manipulations</title>
		<author>
			<persName><forename type="first">Tosca</forename><surname>Lechner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruth</forename><surname>Urner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shai</forename><surname>Ben-David</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="18714" to="18732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">The delphi method</title>
		<author>
			<persName><forename type="first">Murray</forename><surname>Harold A Linstone</surname></persName>
		</author>
		<author>
			<persName><surname>Turoff</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1975">1975</date>
			<publisher>Addison-Wesley</publisher>
			<pubPlace>Reading, MA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Strategic classification is causal modeling in disguise</title>
		<author>
			<persName><forename type="first">John</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Smitha</forename><surname>Milli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6917" to="6926" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The social cost of strategic classification</title>
		<author>
			<persName><forename type="first">Smitha</forename><surname>Milli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anca</forename><forename type="middle">D</forename><surname>Dragan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conference on Fairness, Accountability, and Transparency</title>
		<meeting>the Conference on Fairness, Accountability, and Transparency</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="230" to="239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">The economics of moral hazard: comment. The american economic review</title>
		<author>
			<persName><forename type="first">Pauly</forename><surname>Mark</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1968">1968</date>
			<biblScope unit="page" from="531" to="537" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Causality: Models, Reasoning, and Inference</title>
		<author>
			<persName><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">The economic theory of agency: The principal&apos;s problem. The American economic review</title>
		<author>
			<persName><forename type="first">Ross</forename><surname>Stephen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1973">1973</date>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="134" to="139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Incentives in principal-agent relationships</title>
		<author>
			<persName><forename type="first">E M</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><surname>Sappington</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of economic Perspectives</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="45" to="66" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Causal strategic linear regression</title>
		<author>
			<persName><forename type="first">Yonadav</forename><surname>Shavit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Edelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Axelrod</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="8676" to="8686" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Pac-learning for strategic classification</title>
		<author>
			<persName><forename type="first">Ravi</forename><surname>Sundaram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anil</forename><surname>Vullikanti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">192</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Fairness interventions as (dis) incentives for strategic manipulation</title>
		<author>
			<persName><forename type="first">Xueru</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><forename type="middle">Mahdi</forename><surname>Khalili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parinaz</forename><surname>Naghizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingyan</forename><surname>Liu</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="26239" to="26264" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
