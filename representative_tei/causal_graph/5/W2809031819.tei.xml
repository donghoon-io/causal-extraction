<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Causal Learning for Partially Observed Stochastic Dynamical Systems</title>
				<funder ref="#_Wj4vGbk">
					<orgName type="full">VILLUM FONDEN</orgName>
				</funder>
				<funder ref="#_StnsgTN">
					<orgName type="full">National Institutes of Health</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Søren</forename><forename type="middle">Wengel</forename><surname>Mogensen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Mathematical Sciences</orgName>
								<orgName type="institution">University of Copenhagen Copenhagen</orgName>
								<address>
									<country key="DK">Denmark</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="department">Department of Mathematical Sciences</orgName>
								<orgName type="institution">University of Copenhagen Copenhagen</orgName>
								<address>
									<country key="DK">Denmark</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Daniel</forename><surname>Malinsky</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Johns Hopkins University Baltimore</orgName>
								<address>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Johns Hopkins University Baltimore</orgName>
								<address>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Niels</forename><forename type="middle">Richard</forename><surname>Hansen</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Mathematical Sciences</orgName>
								<orgName type="institution">University of Copenhagen Copenhagen</orgName>
								<address>
									<country key="DK">Denmark</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="department">Department of Mathematical Sciences</orgName>
								<orgName type="institution">University of Copenhagen Copenhagen</orgName>
								<address>
									<country key="DK">Denmark</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Causal Learning for Partially Observed Stochastic Dynamical Systems</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Many models of dynamical systems have causal interpretations that support reasoning about the consequences of interventions, suitably defined. Furthermore, local independence has been suggested as a useful independence concept for stochastic dynamical systems. There is, however, no well-developed theoretical framework for causal learning based on this notion of independence. We study independence models induced by directed graphs (DGs) and provide abstract graphoid properties that guarantee that an independence model has the global Markov property w.r.t. a DG. We apply these results to Itô diffusions and event processes. For a partially observed system, directed mixed graphs (DMGs) represent the marginalized local independence model, and we develop, under a faithfulness assumption, a sound and complete learning algorithm of the directed mixed equivalence graph (DMEG) as a summary of all Markov equivalent DMGs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Causal learning has been developed extensively using structural causal models and graphical representations of the conditional independence relations that they induce. The Fast Causal Inference (FCI) algorithm and its variations (RFCI, FCI+, ...) can learn a representation of the independence relations induced by a causal model even when the causal system is only partially observed, i.e., the data is "causally insufficient" in the terminology of <ref type="bibr" target="#b29">Spirtes et al. (2000)</ref>. FCI is, however, not directly applicable for learning causal relations among entire processes in a continuous-time dynamical system. The dy-namic evolution of such a system cannot be modeled using a finite number of variables related via a structural causal model, and standard probabilistic independence cannot adequately capture infinitesimal conditional independence relationships between processes since such relationships can be asymmetric. The asymmetry can intuitively be explained by the fact that the present of one process may be independent of the past of another process, or the reverse, or both.</p><p>Local independence was introduced by <ref type="bibr" target="#b26">Schweder (1970)</ref> and is a formalization of how the present of one stochastic process depends on the past of others in a dynamical system. This concept directly lends itself to a causal interpretation as dynamical systems develop as functions of their pasts, see e.g. <ref type="bibr" target="#b0">Aalen (1987)</ref>. <ref type="bibr" target="#b7">Didelez (2000</ref><ref type="bibr">Didelez ( , 2006a</ref><ref type="bibr" target="#b10">Didelez ( , 2008) )</ref> considered graphical representations of local independence models using directed graphs (DGs) and -separation and proved the equivalence of the pairwise and global Markov properties in the case of multivariate counting processes. <ref type="bibr" target="#b19">Nodelman et al. (2002</ref><ref type="bibr" target="#b20">Nodelman et al. ( , 2003) )</ref> and <ref type="bibr" target="#b12">Gunawardana et al. (2011)</ref> also considered learning problems in continuous-time models. In this paper, we extend the theory to a broader class of semimartingales, showing the equivalence of pairwise and global Markov properties in DGs. To represent marginalized local independence models, Mogensen and Hansen (2018) introduced directed mixed graphs (DMGs) with µ-separation. Bidirected edges in DMGs (roughly) correspond to dependencies induced by latent processes, and in this sense DMGs can represent partially observed dynamical systems. In contrast to the "causally sufficient" setting as represented by a DG, multiple DMGs may represent the same set of (marginal) local independence relations; thus we use the characterization of Markov equivalent DMGs by <ref type="bibr" target="#b17">Mogensen and Hansen (2018)</ref> to propose a sound and complete algorithm for selecting a set of DMGs consistent with a given collection of independence relations.</p><p>Proofs omitted from the main text can be found in the supplementary material. The sample paths are from the observational distribution started in the stationary mean as well as under an intervention regime on ↵. For the local independence graph (middle) the color of the edge j ! i indicates if the nonzero entry B ij is positive (red) or negative (blue). The step size h difference quotient at 0 for the semigroup t 7 ! exp(tB) (right) determines the discrete time conditional means for time step h transitions. It does not directly reflect the local independences except in the limit h ! 0, where it converges to the infinitesimal generator B. <ref type="bibr" target="#b6">Danks and Plis (2013)</ref> make a similar point in the case of subsampled time series.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">CAUSAL DYNAMICAL MODELS</head><p>The notion of interventions in a continuous-time model of a dynamical system is not new, and has been investigated thoroughly in the context of control theory. Causal models and interventions for event processes and their relation to graphical independence models have been treated in detail <ref type="bibr" target="#b10">(Didelez, 2008</ref><ref type="bibr" target="#b11">(Didelez, , 2015))</ref>. Relations to structural causal models have been established for ordinary differential equations (ODEs) <ref type="bibr">(Mooij et al., 2013;</ref><ref type="bibr" target="#b24">Rubenstein et al., 2016)</ref>. Notions of causality and interventions have also been treated for general stochastic processes such as stochastic differential equations (SDEs) <ref type="bibr" target="#b1">(Aalen et al., 2012;</ref><ref type="bibr" target="#b5">Commenges and Gégout-Petit, 2009;</ref><ref type="bibr" target="#b27">Sokol and Hansen, 2014)</ref>.</p><p>To motivate and explain the general results of this paper, we introduce the toy linear SDE model in R 5 given by dX</p><formula xml:id="formula_0">t = B(X t A)dt + dW t with A = (1, 2, 3, 4, 5) T , B = 0 B B B @ 1.1 1 1 • • • 1.1 • 2.0 • • • 1.1 • 1 • • 1 1.1 • 1 • • • 1.1 1 C C C A ,<label>(1)</label></formula><p>and (W t ) a five-dimensional standard Brownian motion. The coordinates of this process will be denoted ↵, , , , and ✏. If we assume that this SDE has a causal interpretation, we can obtain predictions under interventions via manipulations of the SDE itself, see e.g. <ref type="bibr" target="#b27">Sokol and Hansen (2014)</ref>. In Figure <ref type="figure" target="#fig_0">1</ref>, for instance, we replace the ↵ coordinate of the SDE by</p><formula xml:id="formula_1">dX ↵ t = 1(X t &gt; 1)dt, X ↵ t X ↵ t = X ↵ t 1(X t  1).</formula><p>The nonzero pattern of the B matrix defines a directed graph which we identify as the local independence graph below, which in turn is related to the local independence model of the SDE. It is a main result of this paper that the local independence model satisfies the global Markov property w.r.t. this graph. Under a faithfulness assumption we can identify (aspects of) the causal system from observational data even when some processes are unobserved.</p><p>It is well known that</p><formula xml:id="formula_2">X t+h X t | X t ⇠ N ((e hB I)(X t A), ⌃(h))</formula><p>with ⌃(h) given in terms of B. Thus a sample of the process at equidistant time points is a vector autoregressive process with correlated errors. We note that e hB I is a dense matrix that will not reveal the local independence graph unless h is sufficiently small, see Figure <ref type="figure" target="#fig_0">1</ref>. The matrix B is, furthermore, a stable matrix, hence there is a stationary solution to the SDE and for h ! 1 we have ⌃(h) ! ⌃, the invariant covariance matrix. We note that ⌃ 1 is also a dense matrix, thus the invariant distribution does not satisfy the global Markov property w.r.t. to any undirected graph but the complete graph.</p><p>In conclusion, the local independence model of the SDE is not encoded directly neither by Markov properties of discrete time samples, nor by Markov properties of the invariant distribution. This is the motivation for our abstract development of local independence models, their relation to continuous-time stochastic processes, and a dedicated learning algorithm.</p><p>Consider some finite set V . An independence model over V is a set of triples hA, B | Ci such that A, B, C ✓ V . We let I denote a generic independence model. Following <ref type="bibr" target="#b7">Didelez (2000</ref><ref type="bibr" target="#b10">Didelez ( , 2008) )</ref> we will consider independence models that are not assumed to be symmetric in A and B. The independence models we consider do however satisfy other properties which allow us to deduce some independences from others. We define the following properties, some of which have previously been described as asymmetric (semi)graphoid properties <ref type="bibr">(Didelez, 2006b</ref><ref type="bibr" target="#b10">(Didelez, , 2008))</ref>. Many of them are analogous to properties in the literature on conditional independence models <ref type="bibr" target="#b13">(Lauritzen, 1996)</ref>, though due to the lack of symmetry, one may define both left and right versions.  Cancellation is related to ordered downward-stability as defined by <ref type="bibr" target="#b25">Sadeghi (2017)</ref> for symmetric independence models over a set with a preorder and studied in relation to separation in acyclic graphs.</p><formula xml:id="formula_3">• Left redundancy: hA, B | Ai 2 I • Left decomposition: hA, B | Ci 2 I, D ✓ A ) hD, B | Ci 2 I • Right decomposition: hA, B | Ci 2 I, D ✓ B ) hA, D | Ci 2 I • Left weak union: hA, B | Ci 2 I, D ✓ A ) hA, B | C [ Di 2 I • Right weak union: hA, B | Ci 2 I, D ✓ B ) hA, B | C [ Di 2 I • Left intersection: hA, B | Ci 2 I, hC, B | Ai 2 I ) hA [ C, B | A \ Ci 2 I • Left composition: hA, B | Ci 2 I, hD, B | Ci 2 I ) hA [ D, B | Ci 2 I • Right composition: hA, B | Ci 2 I, hA, D | Ci 2 I ) hA, B [ D | Ci 2 I • Left weak composition: hA, B | Ci 2 I, D ✓ C ) hA [ D, B | Ci 2 I For disjoint sets A, C, D ✓ V ,</formula><formula xml:id="formula_4">(i) hA, C 1 [ D | C [ Di 2 I (ii) hD, C 2 [ A | C [ Ai 2 I.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">DIRECTED MIXED GRAPHS</head><p>We wish to relate a local independence model, as defined in Section 4, to a graph and therefore we need a notion of graphical separation which allows for asymmetry. Directed mixed graphs along with µ-separation will provide the means for such graphical modeling of local independence. The subsequent definitions follow Mogensen and Hansen ( <ref type="formula">2018</ref>), which we refer to for further details.</p><p>Definition 2 (Directed mixed graph). A directed mixed graph (DMG) is an ordered pair (V, E) where V is a finite set of vertices (also called nodes) and E is a finite set of edges of the types ! and $. A pair of vertices ↵, 2 V may be joined by any subset of {↵ ! , ↵ , ↵ $ }. Note that we allow for loops, i.e., edges ↵ ! ↵ and/or ↵ $ ↵.</p><p>Let G 1 = (V, E 1 ) and G 2 = (V, E 2 ) be DMGs. If</p><formula xml:id="formula_5">E 1 ✓ E 2 , then we write G 1 ✓ G 2 and say that G 2 is a supergraph of G 1 .</formula><p>The complete DMG on V is the DMG which is a supergraph of all other DMGs with vertices V . Throughout this paper, G will denote a DMG with node set V and edge set E. We will also consider directed graphs (DGs) which are DMGs with no bidirected edges. Let ↵, 2 V . We will say that the edge ↵ ! has a head at and a tail at ↵, and that the edge ↵ $ has heads at both ↵ and . When we write e.g. ↵ ! this does not preclude other edges between these nodes. We use ↵ ⇤ ! to denote any edge between ↵ and that has a head at . A letter over an edge, e.g. ↵ e ! , denotes simply that e refers to that specific edge. If the edge ↵ ! is in the graph then we say that ↵ is a parent of and if ↵ $ then we say that ↵ and are siblings. Let pa(↵) (or pa G (↵) to make the graph explicit) denote the set of parents of ↵ in G. Note that due to loops, ↵ can be both a parent and a sibling of itself.</p><p>A walk is an alternating, ordered sequence of nodes and edges along with an orientation of the edge such that each edge is between its two adjacent nodes, h⌫ 1 , e 1 , ⌫ 2 , . . . , e n , ⌫ n+1 i, where ⌫ i 2 V and e j 2 E. We say that the walk is between ⌫ 1 and ⌫ n+1 or from ⌫ 1 to ⌫ n+1 . The ⌫ 1 and ⌫ n+1 are called the endpoint nodes of the walk. A non-endpoint node ⌫ i , i 6 = 1, n + 1, is called a collider if the two adjacent edges on the walk both have heads at the node, and otherwise a noncollider. Note that the endpoint nodes are neither colliders nor non-colliders. A walk is called trivial if it consists of a single node and no edges. A path is a walk where no node is repeated. A path from ↵ to is directed if every edge on the path is directed and points towards . We say that ↵ is an ancestor of a set C ✓ V if there exists a (possibly trivial) directed path from ↵ to 2 C. We let an(C) denote the set of nodes that are ancestors to C. Note that C ✓ an(C). A corresponding factor graph (right) with the three factor nodes 1 , 2 and ¯ , cf. Theorem 14.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">µ-separation</head><p>Definition 3 (µ-connecting walk). A µ-connecting walk from ↵ to given C is a non-trivial walk from ↵ to such that ↵ / 2 C, every non-collider is not in C and every collider is in an(C), and such that the final edge has a head at .</p><formula xml:id="formula_6">Definition 4. Let ↵, 2 V, C ✓ V . We say that is µ-separated from ↵ given C in the graph G if there is no µ-connecting walk from ↵ to in G given C. For general sets, A, B, C ✓ V , we say that B is µ-separated from A given C and write A ? µ B | C if is µ-separated from ↵ given C for every ↵ 2 A and 2 B. We write A ? µ B | C [G]</formula><p>if we wish to make explicit to which graph the statement applies.</p><p>Note that this definition means that B is separated from</p><formula xml:id="formula_7">A given C whenever A ✓ C. We associate an indepen- dence model I(G) with a DMG G by hA, B | Ci 2 I(G) , A ? µ B | C [G].</formula><p>Lemma 5. The independence model I(G) satisfies left and right {decomposition, weak union, composition} and left {redundancy, intersection, weak composition}. Furthermore, hA, B | Ci 2 I(G) whenever B = ;. Lemma 6. I(G) satisfies cancellation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Markov equivalence</head><p>We say that DMGs G</p><formula xml:id="formula_8">1 = (V, E 1 ), G 2 = (V, E 2 ) are Markov equivalent if I(G 1 ) = I(G 2</formula><p>) and this defines an equivalence relation. We let [G] denote the (Markov) equivalence class of G. For DMGs, it does not hold that Markov equivalent graphs have the same adjacencies. Note that the same is true for the directed (cyclic) graphs with no loops considered by <ref type="bibr" target="#b21">Richardson (1996</ref><ref type="bibr" target="#b22">Richardson ( , 1997) )</ref> in another context. We say that a DMG is maximal if it is complete or if no edge can be added without changing the associated Markov equivalence class. Mogensen and Hansen (2018) define for every vertex in a DMG a set of potential parents and potential siblings (both subsets of V ) using the independence model induced by the graph (these definitions are also included in the supplementary material). We let pp(↵, I) denote the set of potential parents of ↵ and ps(↵, I) denote the set of potential siblings of ↵ in the independence model I. If G 1 and G 2 are Markov equivalent we thus have pp(↵, I(G 1 )) = pp(↵, I(G 2 )) and ps(↵, I(G 1 )) = ps(↵, I(G 2 )) for each ↵ 2 V . Given a DMG G and independence model I = I(G), one can construct another DMG N in which ↵ is a parent of if and only if ↵ 2 pp( , I) and ↵ and are siblings if and only if ↵ 2 ps( , I). Mogensen and Hansen (2018) showed that N 2 [G], that it is a supergraph of all elements of [G], and that N is maximal. This allows one to define a directed mixed equivalence graph (DMEG) from the (unique) maximal graph N in the equivalence class to summarize the entire equivalence class. The DMEG is constructed from N by partitioning the edge set into two subsets: one consisting of the edges which are common to all graphs in the Markov equivalence class, and one consisting of edges that are present in some members of the equivalence class but absent in others. One may visualize the DMEG by drawing N and making the edges in the latter set dashed. Note that by collapsing the distinction between dashed and solid edges one may straightforwardly apply µ-separation to a given DMEG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">MARKOV PROPERTIES</head><p>The main result of this section gives conditions on an abstract independence model ensuring equivalence be-tween the pairwise and the global Markov properties w.r.t. a directed graph with µ-separation. In the next section we give examples of classes of processes that fulfill these conditions, extending results in <ref type="bibr" target="#b10">Didelez (2008)</ref> to a broader class of models. We take an axiomatic approach to proving the equivalence in the sense that we describe some abstract properties and use only these to show the equivalence. This is analogous to what Lauritzen and <ref type="bibr" target="#b25">Sadeghi (2017)</ref>   </p><formula xml:id="formula_9">I O = {hA, B | Ci | hA, B | Ci 2 I; A, B, C ✓ O}.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">LOCAL INDEPENDENCE</head><p>This section introduces local independence models and local independence graphs. The main results of the section provide verifiable conditions that ensure that a local independence model satisfies the global Markov property w.r.t. the local independence graph.</p><p>Let X = (X 1 t , . . . , X n t ) for t 2 [0, T ] be a càdlàg stochastic process defined on the probability space (⌦, F, P ). Introduce for A ✓ V = {1, . . . , n} the filtration F A t as the completed and right continuous version of ({X ↵ s , s  t, ↵ 2 A}). Let also = ( 1 t , . . . , n t ) be an integrable càdlàg stochastic process. This -process need not have any specific relation to X a priori, but for the main Theorem 14 the relation is through the compatibility processes defined below. Note that some computations below technically require that E(• | F t ) is computed as the optional projection, cf. Theorem VI.7.1 and Lemma VI.7.8 in <ref type="bibr" target="#b23">Rogers and Williams (2000)</ref>. This is unproblematic, and will not be discussed any further.</p><p>Definition 10. We say that B is -locally independent of A given C if the process</p><formula xml:id="formula_10">t 7 ! E( t | F A[C t ) has an F C t -adapted version for all 2 B. In this case we write A 6 ! B | C.</formula><p>This is slightly different from the definition in <ref type="bibr" target="#b10">Didelez (2008)</ref> in that is not necessarily in the conditioning set. This change in the definition makes it possible for a process to be locally independent from itself given some separating set. We define the local independence model, I(X, ), determined by X and via</p><formula xml:id="formula_11">hA, B | Ci 2 I(X, ) , A 6 ! B | C.</formula><p>When there is no risk of ambiguity we say that B is locally independent of A given C, and we write A 6 ! B | C and I = I(X, ). . Theorem 14 below gives a general factorization condition on the distribution of the stochastic processes that ensures a local independence model to be cancellative. This condition is satisfied for example by event and Itô processes.</p><p>Introduce for C ✓ V and 2 V the shorthand notation</p><formula xml:id="formula_12">C, t = E( t | F C t ). Furthermore, for ↵ 2 A ✓ V let A,↵ t = ↵ t (( A,↵ s ) st , (X ↵ s ) st )</formula><p>denote a càdlàg process that is given in terms of a positive functional ↵ t of the history of the A,↵ -and the X ↵processes up to time t.</p><p>Definition 13. We say that P -factorizes with compatibility processes</p><formula xml:id="formula_13">A,↵ &gt; 0 if for all A ✓ V P = 1 Z A t Y ↵2A A,↵ t • Q A t with Q A t a probability measure on (⌦, F) such that (X ↵ s ) 0st for ↵ 2 A are independent under Q A t .</formula><p>Here, Z A t is a deterministic normalization constant. Theorem 14. The local independence model I(X, ) is cancellative if P -factorizes.</p><p>Proof. Assume that A, { } ✓ V factorize w.r.t. C = C 1 [C 2 . In this proof, (i) and (ii) refer to the factorization properties, see Definition</p><formula xml:id="formula_14">1. Let F = C [ A [ { }. Then by (i) F, t = t (( C[{ }, s ) st , (X s ) st ) = C[{ }, t for 2 C 1 [ { },</formula><p>and by (ii)</p><formula xml:id="formula_15">F, t = t (( C[A, s ) st , , (X s ) st ) = C[A, t for 2 C 2 [ A.</formula><p>It follows that</p><formula xml:id="formula_16">Y 2F F, t = 1 t z }| { Y 2C1[{ } C[{ }, t 2 t z }| { Y 2C2[A C[A, t = 1 t 2 t , cf. Figure 2. Note that 2 t is F C[A t -adapted. Let 2 B. We have hA, B | C [ { }i 2 I, hence with ¯ t = C[{ }, t E( t | F C[A t ) = E(E( t | F C[A[{ } t ) | F C[A t ) = E( ¯ t | F C[A t ) = E Q F t ( ¯ t 1 t 2 t | F C[A t ) E Q F t ( 1 t 2 t | F C[A t ) = E Q F t ( ¯ t 1 t | F C[A t ) E Q F t ( 1 t | F C[A t ) = E Q F t ( ¯ t 1 t | F C t ) E Q F t ( 1 t | F C t ) = C, t</formula><p>where the second last identity follows from X ↵ for ↵ 2</p><formula xml:id="formula_17">A being independent of X for 2 C [ { } under Q F t .</formula><p>We conclude that hA, B | Ci 2 I, and this shows that I is cancellative.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">IT Ô PROCESSES</head><p>For X a multivariate Itô process with X ↵ fulfilling the equation</p><formula xml:id="formula_18">X ↵ t = Z t 0 ↵ s ds + t (↵)W ↵ t</formula><p>with W t a standard Brownian motion ( t (↵) &gt; 0 deterministic) we introduce the compatibility processes</p><formula xml:id="formula_19">A,↵ t = exp Z t 0 A,↵ s 2 s (↵) dX ↵ s 1 2 Z t 0 ✓ A,↵ s s (↵) ◆ 2 ds ! .</formula><p>The following result is a consequence of Theorem 7.3 in <ref type="bibr" target="#b15">Liptser and Shiryayev (1977)</ref> combined with Theorem VI.8.4 in <ref type="bibr" target="#b23">Rogers and Williams (2000)</ref>.</p><formula xml:id="formula_20">Proposition 15. If for all A ✓ V E Y ↵2A ( A,↵ t ) 1 ! = 1<label>(2)</label></formula><p>then P -factorizes.</p><p>It can be shown that the linear SDE introduced earlier satisfies the integrability condition (2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">EVENT PROCESSES</head><p>For X a multivariate counting process with X ↵ having intensity process ↵ we introduce the compatibility processes</p><formula xml:id="formula_21">A,↵ t = exp ✓Z t 0 log( A,↵ s )dX ↵ s Z t 0 A,↵ s ds ◆ .</formula><p>Here A,↵ s = lim r!s A,↵ r denotes the left continuous (and thus predictable) version of the intensity process</p><formula xml:id="formula_22">A,↵ t = E( ↵ t | F A t )</formula><p>. With these compatibility processes, Proposition 15 above holds exactly as formulated for Itô processes, see e.g. <ref type="bibr" target="#b28">Sokol and Hansen (2015)</ref> for details and weak conditions ensuring that (2) holds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">LEARNING ALGORITHMS</head><p>In this section, we assume that we have access to a local independence oracle that can answer whether or not some independence statement is in I. In applications, the oracle would of course be substituted with statistical tests of local independence. The local independence model, I, is assumed to be faithful to some DMG G 0 , i.e. I = I(G 0 ). Meek (2014) described a related algorithm for learning local independence graphs which is, however, not complete when the system of stochastic processes is only partially observed. In the FCI algorithm, which learns an equivalence class of MAGs (Maximal Ancestral Graphs), one can exploit the fact that Markov equivalent graphs have the same adjacencies, so the learning algorithm can first find this so-called skeleton of the graph and then orient the edges by applying a finite set of rules <ref type="bibr" target="#b30">(Zhang, 2008;</ref><ref type="bibr" target="#b2">Ali et al., 2009)</ref>. Since Markov equivalent DMGs may have different adjacencies, we cannot straightforwardly copy the FCI strategy here, and our procedure is more complicated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">A THREE-STEP PROCEDURE</head><p>As described in Section 3.1.2, we know that there exists a unique graph which is Markov equivalent to G 0 and a supergraph of all DMGs in [G 0 ] and we denote this graph by N . In this section we give a learning algorithm exploiting this fact. Having learned the maximal DMG N we can subsequently construct a DMEG to summarize the Markov equivalence class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The characterization of Markov equivalence of DMGs in</head><p>Mogensen and Hansen (2018) implies a learning algorithm to construct N which is Markov equivalent to G 0 . For each pair of nodes ↵, there exists a well-defined list of independence tests such that ↵ ! is in N if and only if all requirements in the list is met by I(G 0 ), analogously for the edge ↵ $ (see conditions (p1)-( <ref type="formula">p4</ref>) and (s1)-(s3) in the supplementary material). This means that we can use these lists of tests to construct a maximal graph N such that I(N ) = I(G 0 ). However such an algorithm would perform many more independence tests than needed and one can reduce the number of independence tests conducted by a kind of preprocessing. Our proposed algorithm starts from the complete DMG input : a local independence oracle for I output: a DMG, G = (V, E) initialize G as the complete DMG, set n = 0, initialize</p><formula xml:id="formula_23">L s = ;, L n = ;; while n  max 2V |pa G ( )| do foreach ↵ ! 2 E do foreach C ✓ pa G ( )\{↵}, |C| = n do if ↵ 6 ! | C then delete ↵ ! and ↵ $ from G; update L s = L s [ {h↵, | Ci}; else update L n = L n [ {h↵, | Ci}; end end end update n = n + 1; end set n = 1; while n  max ↵, 2V |D G (↵, )| do foreach ↵ ! 2 E do foreach C ✓ D G (↵, ), |C| = n do if ↵ 6 ! | C then delete ↵ ! and ↵ $ from G; update L s = L s [ {h↵, | Ci}; else update L n = L n [ {h↵, | Ci}; end end update n = n + 1; end end return G, L s , L n</formula><p>Subalgorithm 1: Separation step and removes edges that are not in G 0 by an FCI-like approach, exploiting properties of DMGs and µ-separation, and then in the end applies the potential parents and potential siblings definitions (see the supplementary material), but only if and when needed.</p><p>In this section we describe three steps (and three subalgorithms): a separation, a pruning, and a potential step, and then we argue that we can construct a sound and complete algorithm by using these steps. For all three steps, we sequentially remove edges starting from the complete DMG on nodes V . We will also along the way update a set of triples L s corresponding to independence statements that we know to be in I and a set of triples L n corresponding to independence statements that we know to not be in I. We keep track of this information as we will reuse some of it to reduce the number of independence tests that we conduct. Figure <ref type="figure">3</ref> illustrates what input : a separability graph, S, a set of known independencies L s output: a DMG initialize G = S;</p><formula xml:id="formula_24">foreach unshielded W -structure in S, w (↵, , ) do if 2 S ↵, such that h↵, | S ↵, i 2 L s then if $ is in G then delete $ from G; end else if ! is in G then delete ! from G; end end end return G</formula><p>Subalgorithm 2: Pruning step each subalgorithm outputs for an example G 0 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">The separation step</head><p>When we have an independence model I over V , we will for ↵, 2 V say that is inseparable from ↵ if there exists no</p><formula xml:id="formula_25">C ✓ V \ {↵} such that h↵, | Ci 2 I. Let u( , I) = { 2 V | is inseparable from in I}.</formula><p>The purpose of the first step is to output a separability graph. The separability graph of an independence model I is the DMG such that the edge ↵ ! is in the DMG if and only if ↵ 2 u( , I) and the edge ↵ $ is in the DMG if and only if ↵ 2 u( , I) and 2 u(↵, I).</p><p>We say that is directedly collider connected to if there exists a non-trivial walk from to such that every non-endpoint node on the walk is a collider and such that the final edge has a head at . As shorthand, we write ⇣ . We define the separator set of from ↵,</p><formula xml:id="formula_26">D G (↵, ) = { 2 an(↵, ) | ⇣ } \ {↵}.</formula><p>If there exists a subset of V \ {↵} that separates from ↵, then this set does <ref type="bibr" target="#b17">(Mogensen and Hansen, 2018)</ref>. This set will play a role analogous to that of the set Possible-D-Sep in the FCI algorithm <ref type="bibr" target="#b29">(Spirtes et al., 2000)</ref>.</p><p>In the first part of Subalgorithm 1, we consider pairs of nodes, ↵, , and test if they can be separated by larger and larger conditioning sets, though only subsets of pa G ( ) \ {↵} in the current G. In the second part, we use all subsets of the current separator set D G (↵, ) to determine separability of each pair of nodes. Note that separability is not symmetric, hence, one needs to determine separability of from ↵ and of ↵ from . The input : a local independence oracle for I, a DMG G = (V, E), a set of known dependencies L n output:</p><formula xml:id="formula_27">a DMG foreach ↵ e ! 2 E do if I(G e) \ L n = ; then if ↵ / 2 pp( , I) then delete ↵ ! in G; end end end foreach ↵ e $ 2 E do if I(G e) \ L n = ; then if ↵ / 2 ps( , I) then delete ↵ $ in G; end end end return G Subalgorithm 3: Potential step</formula><p>candidate separator sets may be chosen in more-or-less efficient ways, but we will not discuss this aspect of the algorithm <ref type="bibr" target="#b4">(Colombo et al., 2012;</ref><ref type="bibr" target="#b3">Claassen et al., 2013)</ref>.</p><p>Lemma 16. Subalgorithm 1 outputs the separability graph of I, S, and furthermore N ✓ S.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">The pruning step</head><p>Let S denote the graph in the output of Subalgorithm 1. One can use some of the information encoded by the graph along with the set L s to further prune the graph. For this purpose, we consider W -structures which are triples of nodes ↵, , such that ↵ 6 = 6 = , and ↵ ! ⇤! . We denote such a triple by w (↵, , ). We will say that a W -structure is unshielded if the edge ↵ ! is not in the graph. For every unshielded W -structure w (↵, , ), there exists exactly one triple h↵, | Ci in L s (output from Subalgorithm 1) and we let S ↵, denote the separating set C. Lemma 17. Subalgorithm 2 outputs a supergraph of N .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">Potential step</head><p>In the final step, we sequentially consider each edge which is still in the graph. If G = (V, E) and e 2 E we let G e denote the DMG (V, E \ {e}). We then check if I(G e) \ L n = ;. If not, we leave this edge in the graph. On the other hand, if the intersection is the empty set, we check if the edge is between a pair of potential parents/siblings using the definition of these sets. That is, in the case of a directed edge we check each of the conditions (p1)-(p4) and in the case of a bidirected edge each of the conditions (s1)-(s3); both sets of conditions are in the supplementary material. Note that if ↵ 2 ps( , I), then also 2 ps(↵, I). Theorem 18. The algorithm defined by first doing the separation step, then the pruning, and finally the potential step outputs N , the maximal element of [G 0 ].</p><p>Using properties of maximal DMGs, Mogensen and Hansen (2018) showed how one can construct the DMEG efficiently. The learning algorithm that is defined by first constructing N and then constructing the DMEG is sound and complete in the sense that if an edge is absent in the DMEG, then it is also absent in any element of [G 0 ] and therefore also in G 0 . If it is present and not dashed in the DMEG, then it is present in all elements of [G 0 ] and therefore also in G 0 . Finally, if it is present and dashed in the DMEG, then there exist G 1 , G 2 2 [G 0 ] such that the edge is present in G 1 and absent in G 2 and therefore it is impossible to determine if the edge is in G 0 using knowledge of I(G 0 ) only.</p><p>One could also skip the potential step to reduce the computational requirements. The resulting DMG is then a supergraph of the true graph. A small simulation study (supplementary material) indicates that one could save quite a number of tests and still get close to the true N .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION AND DISCUSSION</head><p>We have shown that for a given directed graph with µseparation it is possible to specify abstract properties that ensure equivalence of the pairwise and global Markov properties in asymmetric independence models. We have shown that under certain conditions these properties hold in local independence models of Itô diffusions and event processes, extending known results.</p><p>Assuming faithfulness, we have given a sound and complete learning algorithm for the Markov equivalence class of directed mixed graphs representing a marginalized local independence model. Faithfulness is not an innocuous assumption and it remains an open research question how common this property is in different classes of stochastic processes.</p><p>This supplementary material contains proofs that were omitted in the paper. It also contains the potential parent and potential sibling criteria and reports the results of a small simulation study illustrating the cost and the impact of the potential step in the learning algorithm.</p><p>A PROOFS OF LEMMAS 5 AND 6 For left intersection, consider a µ-connecting walk, ! = h⌫ 1 , e 1 , . . . , e n ,</p><formula xml:id="formula_28">⌫ n+1 i from = ⌫ 1 2 A [ C to = ⌫ n+1 2 B given A \ C.</formula><p>This walk is by definition nontrivial. Consider now the shortest possible non-trivial subwalk of ! of the form ! = h⌫ i , e i , . . . , e n , ⌫ n+1 i such that</p><formula xml:id="formula_29">⌫ i 2 (A [ C) \ (A \ C).</formula><p>Such a subwalk always exists and it is µ-connecting either from A to B given C or from C to B given A.</p><p>Lemma 6. I(G) satisfies cancellation. There are two possibilities: either there is an arrowhead into on this subwalk of ! or there is not. In the first case, the subwalk of ! from ↵ into is µ-connecting given C [ { }, i.e., A 6 ? µ | C [ { }. Contradiction. In the second case, we consider a collider " on the subwalk between ↵ and (if there is no collider on the walk, then the directed walk from to ↵ is µ-connecting given C [ A). Either " 2 C 1 , " 2 C 2 , or there is a (non-trivial) directed walk from " to some " 0 that is either in C 1 or C 2 . If " 2 C 1 , there is a µ-connecting subwalk of ! from ↵ to " 2 C 1 given C. Since there are no non-colliders on this walk in { }, it is also µ-connecting given C [{ }. If " 2 C 2 , likewise there is a µ-connecting walk from to C 2 given C [ A (note that there are no non-colliders in A on this walk by choice of ↵). Either way, contradiction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof. The contrapositive of</head><formula xml:id="formula_30">A ? µ B | C [ { } ) A ? µ B | C is A 6 ? µ B | C ) A 6 ? µ B | C [ { }. So we have that A ? µ C 1 [ { } | C [ { }, ? µ C 2 [ A | C [ A,</formula><p>If " 6 2 C, we consider concatenating one of the aforementioned walks to " with the directed path ! 0 from " to " 0 2 C. Either appears on ! 0 or it does not. In the first case, then there is an arrowhead at on ! 0 and so A 6 ? µ | C [ { } as before. In the latter case, there are two subcases to consider: either there is some vertex in A on ! 0 or there is not. If there is, choose ↵ 0 2 A on ! 0 such that there are no vertices in A nearer to " on ! 0 . Then the the walk from to ↵ 0 is µ-connecting given C [ A. If there is no vertex in A on ! 0 , then by concatenating a subwalk of ! to ! 0 we get a µ-connecting walk from ↵ or to " 0 in C 1 or C 2 given C [ { } or C [ A, respectively. In any case, contradiction. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Left decomposition: Assume</head><formula xml:id="formula_31">that A 1 [ A 2 6 ! B | C. We wish to show that A 1 6 ! B | C. E( t | F A1[C t ) = E E( t | F A1[A2[C t ) | {z } =E( B t |F C t ) F A1[C t = E( t | F C t )</formula><p>Left weak union: Simply note that the conditioningalgebra stays the same in the conditional expectation which is assumed to be F C t -adapted and therefore also</p><formula xml:id="formula_32">F C[D t -adapted.</formula><p>Left weak composition: The conditioning -algebra again stays the same in the conditional expectation.</p><p>Right decomposition and right composition follow directly from the coordinate-wise definition of local independence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Left intersection:</head><formula xml:id="formula_33">We note that E( t | F A[C t</formula><p>) by assumption has an F A t -adapted and an F C t -adapted version, thus it has a version, which is adapted w.r.t. the filtration</p><formula xml:id="formula_34">F A t \ F C t = F A\C t .</formula><p>Finally, it is clear that hA, B | Ci 2 I if B = ; as this makes the condition void.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D PROOFS, SECTION 5</head><p>Lemma 16. Subalgorithm 1 outputs the separability graph of I, S, and furthermore N ✓ S.</p><p>Proof. In Subalgorithm 1, we only remove edges ↵ ⇤! when we have found a set C ✓ V \ {↵} that separates from ↵. The DMGs G 0 and N are Markov equivalent and therefore the same separation holds in I(N ). Such an edge would always be µ-connecting from ↵ to given C as ↵ / 2 C and therefore we know it to be absent in N . This means that the output of the algorithm is a supergraph of N .</p><p>The graph G in Subalgorithm 1 is always a supergraph of G 0 and therefore D G0 (↵, ) ✓ D G (↵, ). If there exists a set that separates from ↵ then D G0 (↵, ) does and by the above inclusion we are always sure to test this set. This means that the output is the separability graph.</p><p>Lemma 17. Subalgorithm 2 outputs a supergraph of N .</p><p>Proof. By Lemma 16, N ✓ S. We also know that if there is an edge ↵ ! in S then ↵ 2 u( , I(G 0 )) = u( , I(N )) = u( , I). Assume there is an unshielded W -structure w (↵, , ) in S. The edge between ↵ and in S means that cannot be separated from ↵ in I(N ) and therefore there exists for every C ✓ V \ {↵} a µconnecting walk from ↵ to given C. By definition of µ-connecting walks this has a head at (the final) . The W -structure is unshielded, that is, ↵ ! is not in S. This means that we have previously found a separating set S ↵, , such that h↵, | S ↵, i 2 I(N ) and ↵ / 2 S ↵, . We know that there exists a µ-connecting walk !, from ↵ to given S ↵, in N as ↵ 2 u( , I(N )). If / 2 S ↵, then we can compose ! with the edge ! which gives a µ-connecting walk from ↵ to given S ↵, which is a contradiction, and therefore the edge ! cannot be in N . If 2 S ↵, then we can argue analogously and obtain that $ cannot be in N .</p><p>Theorem 18. The algorithm defined by first doing the separation step, then the pruning, and finally the potential step outputs N , the maximal element of [G 0 ].</p><p>Proof. By Lemma 17, the output after the first two steps is a supergraph of N . In the potential step, an edge ↵ ! is only removed if ↵ is not a potential parent of in I. We know that if the edge is in N then ↵ is a potential parent of in I(N ) = I(G 0 ) = I (Mogensen and Hansen, 2018) and by contraposition of this result it follows that every directed edge removed is not in N . The same argument applies in the case of a bidirected edge and therefore the output is a supergraph of N .</p><p>If we consider some edge ↵ e ! in the output graph, then either ↵ is a potential parent of , in which case e is also in N , or I(G e) \ L n 6 = ;. Assume the latter. We have that G 0 ✓ G, and therefore I(G e) ✓ I(G 0 ) if e is not in G 0 . The above intersection is non-empty and therefore there is some triple which is in both I(G e) and L n , and by I(G e) ✓ I(G 0 ) it is also in I(G 0 ). But by definition L n contains only triples not in I(G 0 ), so this is a contradiction. Therefore, e must be in G 0 and also in N as G 0 ✓ N . One can argue analogously for the bidirected edges. We conclude that the output graph is equal to N , the maximal element of [G 0 ].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E POTENTIAL PARENT/SIBLINGS</head><p>Consider an independence model, I, over V and let ↵, 2 V . The set u( , I) is defined in Subsection 5.1.1. As described in Subsection 5.1 the below definitions define a list of independence tests which one can conduct to directly construct N . This was proven by <ref type="bibr" target="#b17">Mogensen and Hansen (2018)</ref>. However, the list is very large and one can construct N in a more efficient manner. If e.g. |V | = 10, then for each choice of in (s2) we can choose C in 2 8 different ways (omitting sets C containing as such an independence would hold trivially for any independence model satisfying left redundancy and left decomposition). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F SIMULATION STUDY</head><p>We conducted a small simulation study to empirically evaluate the cost and impact of the third step in the learning algorithm, the potential step. This step is computationally expensive as it involves testing the potential parent/siblings conditions, see above.</p><p>We simulated a random DMG on 5 nodes by first drawing p d from a uniform distribution on [0, 1/2] and p b from a uniform distribution on [0, 1/4]. We then generated independent Bernoulli random variates, {b h↵, i }, each with success parameter p d , and one for each ordered pair of nodes, h↵, i. The edge ↵ ! was included if b h↵, i = 1. For each unordered pair of nodes, {↵, }, we did analogously, using p b as success parameter. We discarded graphs for which the maximal Markov equivalent graph had more then 15 edges.</p><p>Simulating 800 random DMGs, we saw that on average the first step required 90 independence tests and removed 26 edges. The second step removed 1.1 edge on average (it does not use any additional independence tests), while the third required an additional 77 independence tests. On average the third step removed 0.8 edge. This simulation is very limited and simple, however, it does indicate that the potential step of the learning algorithm constitutes a substantial part of the computational cost while not removing a lot of edges.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: Simulated sample paths (left) for the linear SDE determined by B in (1). The sample paths are from the observational distribution started in the stationary mean as well as under an intervention regime on ↵. For the local independence graph (middle) the color of the edge j ! i indicates if the nonzero entry B ij is positive (red) or negative (blue). The step size h difference quotient at 0 for the semigroup t 7 ! exp(tB) (right) determines the discrete time conditional means for time step h transitions. It does not directly reflect the local independences except in the limit</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Definition 1 .</head><label>1</label><figDesc>The independence model I satisfies cancellation if hA, B | C [{ }i 2 I implies hA, B | Ci 2 I whenever A and { } factorize w.r.t. C. Such an independence model is called cancellative.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: A DMG G (left) with sets {↵} and { } that factorize w.r.t. C = { 1 , 2 , 3 } such that ↵ ? µ | C [ { }. Any node is µ-separated from either ↵ by C [ { } or by C [ {↵} (middle), and as I(G) is cancellative, ↵ ? µ | C. A corresponding factor graph (right) with the three factor nodes 1 , 2 and ¯ , cf. Theorem 14.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Mogensen and Hansen (2018) give a marginalization algorithm (a.k.a. a "latent projection"), which outputs a marginal DMG, G = (O, F ), from a DG, D = (V, E), such that I(D) O = I(G). If I satisfies the global Markov property w.r.t. D then I(G) = I(D) O ✓ I O . This shows that the marginalized independence model I O then satisfies the global Markov property w.r.t. the DMG G.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :Figure 4 :</head><label>34</label><figDesc>Figure 3: Illustration of the learning algorithm. The DMG G 0 is the underlying graph and we have access to I = I(G 0 ). Subalgorithm 1 outputs S, the separability graph of I(G 0 ). Subalgorithm 2 prunes S and outputs S. Note e.g. the unshielded W -structure ↵ ! ! " in S. The DMG N is the maximal element in [G 0 ]. Note that ! " has been removed by Subalgorithm 3 using the potential parent criteria. The final graph Ñ is the DMEG constructed from N .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Lemma 5 .</head><label>5</label><figDesc>The independence model I(G) satisfies left and right {decomposition, weak union, composition} and left {redundancy, intersection, weak composition}. Furthermore, hA, B | Ci 2 I(G) whenever B = ;. Proof. Left redundancy, left and right decomposition and left and right composition follow directly from the definition of µ-separation. Left and right weak union are also immediate. Left weak composition follows from left redundancy, left decomposition and left composition. It is also clear that hA, B | Ci 2 I(G) if B = ;.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>B</head><label></label><figDesc>| C which is case (i-1). Moreover, either ? µ B | C or ? µ A \ C | C, as otherwise A ? µ B | C would not hold (Lemma 20). ? µ B | C is the above case, so assume that 6 ? µ B | C and ? µ A \ C | C. Using right weak union of I(G), we have A ? µ | C [{ } and ? µ A\C | C [A. Using the induction assumption, we have that hA, | C [ { }i 2 I and h , A \ C | C [ Ai 2 I. We have A ? µ B | C and A ? µ | C and using right composition and right weak union of I(G), we obtain A ? µ B[{ } | C[{ }. Using the induction assumption we have that hA, B | C [ { }i 2 I. Assume to obtain a contradiction that A 6 ? µ | C [ and 6 ? µ | C [ A for some 2 C. We know that A ? µ | C and by using the contrapositive of Lemma 19 this means that A 6 ? µ | C. Similarly, we obtain that 6 ? µ | C. We note that 6 ? µ B | C and by Lemma 20 this means that A 6 ? µ B | C which is a contradiction. Therefore, we have that for each 2 C, either A ? µ | C [ (and therefore also A \ C ? µ | C [ ) or ? µ | C [ A. Using the induction assumption, right composition of I, the cancellation property and left weak composition of I we arrive at the conclusion. Case (ii): If one cannot choose a 2 an(A[B [C) such that / 2 C and 6 = ↵, then an(A [ B [ C) = C [ {↵}. Assume this and furthermore assume that / 2 an(A [ B [ C). We will first argue that A ? µ B | C [ { }. If this was not the case there would be a µ-connecting walk, !, from A to 2 B given C [ { } on which was a collider and furthermore every collider was in C [ { }. Consider now the last occurrence of on this walk, and the subwalk of !, ⇠ . . . ⇠ ✓ ⇠ . . . ! . Let ✓ be the node in an(A [ B [ C)  which is the closest to on the walk. Then there must be a tail at ✓, and this means that ✓ = ↵ as otherwise the walk would be closed. In this case, the subwalk from ↵ to would also be µ-connecting given C which is a contradiction. It also holds that ? µ B | C [ A as every parent of a node in B is in C [ A. Using the induction assumption we have that hA, B | C [ { }i 2 I and h , B | C [ Ai 2 I and using Lemma 21 and left decomposition of I we obtain hA, B | Ci 2 I. C PROOF OF LEMMA 11 Lemma 11. Let I be a local independence model. Then it satisfies left {redundancy, decomposition, weak union, weak composition} and right {decomposition, composi-tion} and furthermore hA, B | Ci 2 I whenever B = ;. If F A t \ F C t = F A\C t holds for all A, C ✓ V and t 2 [0, T ], then left intersection holds. Proof. Left redundancy: We note that F A[C t = F C t from which the result follows.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Definition 23 .</head><label>23</label><figDesc>We say that ↵ and are potential siblings in the independence model I if (s1)-(s3) hold: (s1) 2 u(↵, I) and ↵ 2 u( , I),(s2) for all 2 V , C ✓ V such that 2 C, h , ↵ | Ci 2 I ) h , | Ci 2 I, (s3) for all 2 V , C ✓ V such that ↵ 2 C, h , | Ci 2 I ) h , ↵ | Ci 2 I.Definition 24. We say that ↵ is a potential parent of in the independence model I if (p1)-(p4) hold:(p1) ↵ 2 u( , I), (p2) for all 2 V , C ✓ V such that ↵ / 2 C, h , | Ci ) h , ↵ | Ci, (p3) for all , 2 V , C ✓ V such that ↵ / 2 C, 2 C, h , | Ci ) h , | Ci _ h↵, | Ci, (p4) for all 2 V, C ✓ V , such that ↵ / 2 C, h , | Ci ) h , | C [ {↵}i.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>we say that A and D</figDesc><table><row><cell>factorize w.r.t. C if there exists a partition C = C 1 [ C 2</cell></row><row><cell>such that (i) and (ii) hold:</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>did in the case of symmetric independence models. Definition 7. A DG and an independence model satisfy the pairwise Markov property if for ↵, 2 V ,</figDesc><table><row><cell>↵ / 2 pa( ) ) h↵, | V \ {↵}i 2 I</cell></row><row><cell>A DMG and an independence model satisfy the global</cell></row><row><cell>Markov property if for A, B, C ✓ V ,</cell></row></table><note><p>A ? µ B | C ) hA, B | Ci 2 I. Theorem 8. Assume that I is an independence model that satisfies left {redundancy, intersection, decomposition, weak union, weak composition}, right {decomposition, composition}, is cancellative, and furthermore hA, B | Ci 2 I whenever B = ;. Let D be a DG. Then I satisfies the pairwise Markov property with respect to D if and only if it satisfies the global Markov property with respect to D. To keep consistency with earlier literature, we define the pairwise Markov condition above as the absence of an edge, which does not directly generalize to DMGs. Therefore, we prove the equivalence of pairwise and global Markov only in the class of DGs. The main purpose of DMGs is to represent Markov properties from marginalized DGs as defined below, in which case the global Markov property w.r.t. a DMG is inherited from the DG. Definition 9 (Marginal independence model). Assume that I is an independence model over V . Then the marginal independence model of I over O ✓ V , I O , is the independence model,</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>The local independence model satisfies a number of the properties listed in Section 3. Lemma 11. Let I be a local independence model. Then it satisfies left {redundancy, decomposition, weak union, weak composition} and right {decomposition, composi-tion} and furthermore hA, B | Ci 2 I whenever B = ;.</figDesc><table><row><cell>By Theorem 8 and Lemma 11 a local independence</cell><cell></cell></row><row><cell>model that satisfies left intersection and is cancellative</cell><cell></cell></row><row><cell>satisfies the global Markov property w.r.t. the local in-</cell><cell></cell></row><row><cell>dependence graph. Left intersection holds by Lemma</cell><cell></cell></row><row><cell>11 whenever F A t \ F C t = F A\C t</cell><cell></cell></row><row><cell>If F A t \ F C t</cell><cell>= F A\C</cell></row></table><note><p>t holds for all A, C ✓ V and t 2 [0, T ], then left intersection holds. Definition 12. The local independence graph is the directed graph with node set V = {1, . . . , n} such that ↵ 6 2 pa( ) , ↵ 6 ! | V \{↵}.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>and A 6 ? µ B | C and want to show that A 6 ? µ B | C [ { }. Note that A ? µ | C [ { } by right decomposition.There exists a µ-connecting walk ! from ↵ 2 A to some 2 B given C, and we argue that this walk is alsoµ-connecting given C [ { }. Suppose not, for contradiction. Note that ↵ 6 2 C so ↵ 6 2 C [ { } since by factorization A, C, { } are disjoint. Also every collider on ! is in an(C) so it is in an(C [ { }). Thus if ! is not µ-connecting given C [ { } it must be because there is some non-collider on ! which is not in C but is in C [ { }, i.e.,the non-collider is . Choose now a subwalk of ! between some (possibly different) ↵ 2 A and such that no non-endpoint node of this subwalk is in A [ { }. Again, ↵ / 2 C [ { }. Such a subwalk always exists.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>SWM and NRH were supported by research grant <rs type="grantNumber">13358</rs> from <rs type="funder">VILLUM FONDEN</rs>. DM was supported by research grant <rs type="grantNumber">R01 AI127271-01A1</rs> from the <rs type="funder">National Institutes of Health</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_Wj4vGbk">
					<idno type="grant-number">13358</idno>
				</org>
				<org type="funding" xml:id="_StnsgTN">
					<idno type="grant-number">R01 AI127271-01A1</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary material for "Causal Learning for Partially Observed</head><p>Stochastic Dynamical Systems"</p><p>B PROOF OF THEOREM 8</p><p>In this section, we first prove some lemmas and then use these to prove Theorem 8. Proof. This follows from right composition, right weak union, and right decomposition of µ-separation.</p><p>If there is a walk between ↵ 2 A and such that no noncollider is in C and every collider is in an(C), and there is a µ-connecting walk from to 2 B given C, then there is a µ-connecting walk from A to B given C.</p><p>If ! = h⌫ 1 , e 1 , ⌫ 2 , . . . , e n , ⌫ n+1 i is a walk, then the inverse, ! 1 , is the walk h⌫ n+1 , e n , ⌫ n , . . . , e 1 , ⌫ 1 i.</p><p>Proof. If 2 an(C), then simply compose the walks. Assume / 2 an(C). If 2 an(A) let ⇡ denote the directed path from to ↵ 2 A. We have that there is no node in C on ⇡ and composing ⇡ 1 with the µconnecting walk from to B gives a µ-connecting walk from ↵ 2 A to 2 B given C. If 2 an(B) compose the walk from ↵ to with the directed path from to B (which is µ-connecting given C as / 2 an(C)). Lemma 22. Let D = (V, E) be a DG, and let ↵,</p><p>In the following proofs, we will use ⇠ to denote an arbitrary edge.</p><p>Proof. Assume first that ↵ / 2 pa D ( ), and consider a walk between ↵ and that has a head at , ↵ ⇠ . . . ⇠ ! . We must have that ↵ 6 = and therefore the walk is not µ-connecting given V \ {↵}.</p><p>Assume instead that ↵ ? µ | V \{↵}. The edge ↵ ! would constitute a µ-connecting walk given V \ {↵} and therefore we must have that ↵ / 2 pa D ( ). For the induction step, consider a node / 2 C. Note first that if A ✓ C, then the result once again follows using left redundancy and then left decomposition, and therefore assume that A \ C 6 = ;, and take ↵ 2 A \ C (note that ↵ = is allowed). Assume first that we cannot choose ↵ and such that ↵ 6 = . This means that C = V \ {↵}. Case (i-2): A ? µ | C In this case, we can assume that / 2 A, as otherwise by left decomposition of I(G) we would also have ? µ</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Dynamic modelling and causality</title>
		<author>
			<persName><forename type="first">O</forename><surname>Odd</surname></persName>
		</author>
		<author>
			<persName><surname>Aalen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scandinavian Actuarial Journal</title>
		<imprint>
			<biblScope unit="page" from="177" to="190" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Causality, mediation and time: a dynamic viewpoint</title>
		<author>
			<persName><forename type="first">O</forename><surname>Odd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kjetil</forename><surname>Aalen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><surname>Røysland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bruno</forename><surname>Michael Gran</surname></persName>
		</author>
		<author>
			<persName><surname>Ledergerber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society, Series A</title>
		<imprint>
			<biblScope unit="volume">175</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="831" to="861" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Markov equivalence for ancestral graphs</title>
		<author>
			<persName><forename type="first">Ayesha</forename><forename type="middle">R</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Richardson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Spirtes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">5B</biblScope>
			<biblScope unit="page" from="2808" to="2837" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning sparse causal models is not NP-hard</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Claassen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joris</forename><surname>Mooij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Heskes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the 29th Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="172" to="181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning highdimensional directed acyclic graphs with latent and selection variables</title>
		<author>
			<persName><forename type="first">Diego</forename><surname>Colombo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Marloes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Maathuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Kalisch</surname></persName>
		</author>
		<author>
			<persName><surname>Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="294" to="321" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A general dynamical statistical model with causal interpretation</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Commenges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anne</forename><surname>Gégout-Petit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society. Series B (Statistical Methodology)</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="719" to="736" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning causal structure from undersampled time series</title>
		<author>
			<persName><forename type="first">David</forename><surname>Danks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Plis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">JMLR: Workshop and Conference Proceedings</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Graphical Models for Event History Analysis based on Local Independence</title>
		<author>
			<persName><forename type="first">Vanessa</forename><surname>Didelez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
		<respStmt>
			<orgName>Universität Dortmund</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Graphical models for composable finite Markov processes</title>
		<author>
			<persName><forename type="first">Vanessa</forename><surname>Didelez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scandinavian Journal of Statistics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="169" to="185" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Asymmetric separation for local independence graphs</title>
		<author>
			<persName><forename type="first">Vanessa</forename><surname>Didelez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the 22nd Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Graphical models for marked point processes based on local independence</title>
		<author>
			<persName><forename type="first">Vanessa</forename><surname>Didelez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society, Series B</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="245" to="264" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Causal reasoning for events in continuous time: A decision-theoretic approach</title>
		<author>
			<persName><forename type="first">Vanessa</forename><surname>Didelez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the UAI 2015 Workshop on Advances in Causal Inference</title>
		<meeting>the UAI 2015 Workshop on Advances in Causal Inference</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A model for temporal dependencies in event streams</title>
		<author>
			<persName><forename type="first">Asela</forename><surname>Gunawardana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Meek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Puyang</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 24 (NIPS 2011)</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Lauritzen</surname></persName>
		</author>
		<title level="m">Graphical Models. Oxford: Clarendon</title>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Unifying Markov properties for graphical models</title>
		<author>
			<persName><forename type="first">Steffen</forename><surname>Lauritzen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kayvan</forename><surname>Sadeghi</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1608.05810" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Liptser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Shiryayev</surname></persName>
		</author>
		<title level="m">Statistics of Random Processes I: General Theory</title>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Toward learning graphical and causal process models</title>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Meek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the UAI 2014 Workshop on Causal Inference: Learning and Prediction</title>
		<meeting>the UAI 2014 Workshop on Causal Inference: Learning and Prediction</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Markov equivalence of marginalized local independence graphs</title>
		<author>
			<persName><forename type="first">Søren</forename><surname>Wengel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mogensen</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Niels</forename><forename type="middle">Richard</forename><surname>Hansen</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1802.10163" />
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">From ordinary differential equations to structural causal models: the deterministic case</title>
		<author>
			<persName><forename type="first">M</forename><surname>Joris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Mooij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the 29th Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Continuous time Bayesian networks</title>
		<author>
			<persName><forename type="first">U</forename><surname>Nodelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Shelton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th Conference on Uncertainty in Artifical Intelligence</title>
		<meeting>the 18th Conference on Uncertainty in Artifical Intelligence</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="378" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning continuous time Bayesian networks</title>
		<author>
			<persName><forename type="first">U</forename><surname>Nodelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">R</forename><surname>Shelton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the 19th Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="451" to="458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A discovery algorithm for directed cyclic graphs</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the 12th Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A characterization of Markov equivalence for directed cyclic graphs</title>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Approximate Reasoning</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="107" to="162" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">C G</forename><surname>Rogers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Diffusions, Markov processes, and martingales</title>
		<title level="s">Cambridge Mathematical Library</title>
		<meeting><address><addrLine>Cambridge</addrLine></address></meeting>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2000">2000</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">From deterministic ODEs to dynamic structural causal models</title>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">K</forename><surname>Rubenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Bongers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joris</forename><forename type="middle">M</forename><surname>Mooij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.08028[cs.AI</idno>
		<ptr target="http://arxiv.org/abs/1608.08028" />
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv.org preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Faithfulness of probability distributions and graphs</title>
		<author>
			<persName><forename type="first">Kayvan</forename><surname>Sadeghi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">148</biblScope>
			<biblScope unit="page" from="1" to="29" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Composable Markov processes</title>
		<author>
			<persName><forename type="first">Tore</forename><surname>Schweder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Applied Probability</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="400" to="410" />
			<date type="published" when="1970">1970</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Causal interpretation of stochastic differential equations</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Sokol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niels</forename><forename type="middle">Richard</forename><surname>Hansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electronic Journal of Probability</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">100</biblScope>
			<biblScope unit="page" from="1" to="24" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Exponential martingales and changes of measure for counting processes. Stochastic Analysis and Applications</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Sokol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niels</forename><forename type="middle">Richard</forename><surname>Hansen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="823" to="843" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Causation, Prediction, and Search</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clark</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Scheines</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">On the completeness of orientation rules for causal discovery in the presence of latent confounders and selection bias</title>
		<author>
			<persName><forename type="first">Jiji</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">172</biblScope>
			<biblScope unit="page" from="1873" to="1896" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
