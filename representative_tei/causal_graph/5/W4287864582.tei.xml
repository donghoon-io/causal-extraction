<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Causal Inference with Selectively Deconfounded Data</title>
				<funder>
					<orgName type="full">Salesforce</orgName>
				</funder>
				<funder>
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
				<funder>
					<orgName type="full">Facebook</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Kyra</forename><surname>Gan</surname></persName>
							<email>kyragan@cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<postCode>15213</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Andrew</forename><forename type="middle">A</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<postCode>15213</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zachary</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
							<email>zlipton@cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<postCode>15213</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sridhar</forename><surname>Tayur</surname></persName>
							<email>stayur@cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<postCode>15213</postCode>
									<settlement>Pittsburgh</settlement>
									<region>PA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Causal Inference with Selectively Deconfounded Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Given only data generated by a standard confounding graph with unobserved confounder, the Average Treatment Effect (ATE) is not identifiable. To estimate the ATE, a practitioner must then either (a) collect deconfounded data; (b) run a clinical trial; or (c) elucidate further properties of the causal graph that might render the ATE identifiable. In this paper, we consider the benefit of incorporating a large confounded observational dataset (confounder unobserved ) alongside a small deconfounded observational dataset (confounder revealed ) when estimating the ATE. Our theoretical results suggest that the inclusion of confounded data can significantly reduce the quantity of deconfounded data required to estimate the ATE to within a desired accuracy level. Moreover, in some cases-say, genetics-we could imagine retrospectively selecting samples to deconfound. We demonstrate that by actively selecting these samples based upon the (already observed) treatment and outcome, we can reduce sample complexity further. Our theoretical and empirical results establish that the worst-case relative performance of our approach (vs. a natural benchmark) is bounded while our best-case gains are unbounded. Finally, we demonstrate the benefits of selective deconfounding using a large real-world dataset related to genetic mutation in cancer.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The fundamental problem in causal inference is to estimate causal effects using observational data. This task is particularly motivated by scenarios when experiments are infeasible. While the literature typically addresses a rigid setting in which confounders are either always or never observed, in many applications we might observe confounders for a subset of samples. For example, in healthcare, a particular gene might be suspected to confound the relation between a behavior and a health outcome of interest. Due to the high cost of genetic tests, we might only be able to afford to reveal the value of the genetic confounder for a subset of patients. Note that for a variable such as a genetic mutation, we might observe retrospectively, even after the treatment and outcome have been observed. We call this process of revealing the value of an (initially unobserved) confounder deconfounding, and the samples where treatment, outcome, and confounders are all observed deconfounded data.</p><p>So motivated, this paper addresses the middle ground along the confounded-deconfounded spectrum. Naively, one could estimate the ATE with standard methods using only the deconfounded data. First, we ask: how much can we improve our ATE estimates by incorporating confounded data over approaches that rely on deconfounded data alone? Second, motivated by the setting in which our confounders are genetic traits that might be retrospectively observed for cases with known treatments and outcomes, we introduce the problem of selective deconfounding-allocating a fixed budget for revealing the confounder based upon observed treatments and outcomes. This prompts our second question: what is the optimal policy for selecting data to deconfound? To our knowledge, this is the first paper that focuses on the case where ample (cheaply-acquired) confounded data is available and we can select only few confounded samples to deconfound (expensive).</p><p>We address these questions for a standard confounding graph where the treatment and outcome are binary, and the confounder is categorical. First, we propose a simple method for incorporating confounded data that achieves a constant-factor improvement in ATE estimation error. In short, the inclusion of (infinite) confounded data reduces the number of free parameters to be estimated, improving our estimates of the remaining parameters. Moreover, due to the multiplicative factors in the causal functional, errors in parameter estimates can compound. Thus, our improvements in parameter estimates yield greater benefits in estimating treatment effects. For binary confounders, our numerical results show that on average, over problem instances selected uniformly on the parameter simplex, our method achieves roughly 2.5× improvements in ATE estimation error.</p><p>Next, we show that we can reduce error further by actively choosing which samples to deconfound. Our proposed policy for selecting samples dominates reasonable benchmarks. In the worst case, our method requires no more than 2× as many samples as a natural sampling policy and our best-case gains are unbounded. Moreover, our qualitative analysis characterizes those situations most favorable/unfavorable for our method. We extend our work to the scenario where only a finite amount of confounded samples is present, demonstrating our qualitative insights continue to apply (Appendix C). Additionally, we validate our methods using COSMIC <ref type="bibr" target="#b23">(Tate et al., 2019;</ref><ref type="bibr">Cosmic, 2019)</ref>, a real-world dataset containing cancer types, genetic mutations, and other patient features, showing the practical benefits of our proposed sampling policy. Throughout the paper, we implicitly assume that the confounded data was sampled i.i.d. from the target population of interest (but our policy for selecting data to deconfound need not be).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Causal inference has been studied thoroughly under the ignorability assumption, i.e., no unobserved confounding <ref type="bibr" target="#b14">(Neyman, 1923;</ref><ref type="bibr" target="#b19">Rubin, 1974;</ref><ref type="bibr" target="#b7">Holland, 1986)</ref>. Some approaches for estimating the ATE under ignorability include inverse propensity score weighting <ref type="bibr" target="#b18">(Rosenbaum and Rubin, 1983;</ref><ref type="bibr" target="#b6">Hirano et al., 2003;</ref><ref type="bibr">Mc-Caffrey et al., 2004)</ref>, matching <ref type="bibr" target="#b4">(Dehejia and Wahba, 2002)</ref>, the backdoor adjustment <ref type="bibr" target="#b15">(Pearl, 1995)</ref>, and targeted learning (Van der <ref type="bibr">Laan and Rose, 2011)</ref>. Some related papers look to combine various sources of information, for instance from randomized control trials and observational data to estimate the ATE <ref type="bibr" target="#b22">(Stuart et al., 2011;</ref><ref type="bibr" target="#b5">Hartman et al., 2015)</ref>. Other papers leverage machine learning techniques, such as random forests, for estimating causal effects (Alaa and van der <ref type="bibr" target="#b0">Schaar, 2017;</ref><ref type="bibr">Wager and Athey, 2018)</ref>.</p><p>Some papers investigate ATE estimation with confounded data by leveraging mediators <ref type="bibr" target="#b15">(Pearl, 1995)</ref> and proxies <ref type="bibr" target="#b13">(Miao et al., 2018)</ref>. Others investigate combining confounded observational data with experimental data. <ref type="bibr" target="#b10">Kuroki and Pearl (2014)</ref> identify graphical structures under which causal effect can be identified. <ref type="bibr" target="#b13">Miao et al. (2018)</ref> propose to use two different types of proxies to recover causal effects with one unobserved confounder. <ref type="bibr" target="#b21">Shi et al. (2020)</ref> extend the work by <ref type="bibr" target="#b13">Miao et al. (2018)</ref> to multiple confounders. However, both methods require knowledge of proxy categories a priori and are not robust under misspecification of proxy categories. <ref type="bibr" target="#b11">Louizos et al. (2017)</ref> use variational autoencoders to recover the causal effect under the model where when conditioned on the unobserved confounders, the proxies are independent of treatment and outcome. <ref type="bibr" target="#b15">Pearl (1995)</ref> introduces the front-door adjustment, expressing the causal effect as a functional that concerns only the (possibly confounded) treatment and outcome, and an (unconfounded) mediator that transmits the entire effect.</p><p>In other work, <ref type="bibr" target="#b1">Bareinboim and Pearl (2013)</ref> propose to combine observational and experimental data under distribution shift, learning the treatment effect from the experimental data and transporting it to the confounded observational data to obtain a bias-free estimator for the causal effect. Recently, <ref type="bibr" target="#b8">Kallus et al. (2018)</ref> propose a two-step process to remove hidden confounding by incorporating experimental data. Lastly, few papers provide finite sample guarantees for causal inference. <ref type="bibr" target="#b20">Shalit et al. (2017)</ref> upper bound the estimation error for a family of algorithms that estimate causal effects under the ignorability assumption.</p><p>Unlike most prior work, we (i) address confounded and deconfounded (but not experimental) observational data, (ii) perform finite sample analysis to quantify the relative benefit of additional confounded and deconfounded data towards improving our estimate of the average treatment effect, and (iii) investigate sampleefficient policies for selective deconfounding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods and Theory</head><p>Let T and Y be random variables denoting the treatment and outcome. We restrict these to be binary, viewing T as an indicator of whether a particular treatment has occurred and Y as an indicator of whether the outcome was successful. In this work, we assume the existence of a single (possible) confounder, denoted Z, which can take up to k categorical values (Figure <ref type="figure" target="#fig_0">1</ref>).</p><p>In addition, although we only include one unobserved confounder in our model, because our variables are categorical, (as shown in Section 3.1) this subsumes scenarios with multiple categorical confounders. Following Pearl's nomenclature <ref type="bibr" target="#b16">(Pearl, 2000)</ref>, let</p><formula xml:id="formula_0">P (Y = y|do(T = t)) := z∈[k] P Y |T,Z (y|t, z)P Z (z).</formula><p>Our goal is to estimate the ATE, which can be expressed via the back-door adjustment in terms of the joint distribution P Y,T,Z on (Y, T, Z), as:</p><formula xml:id="formula_1">ATE := P (Y = 1|do(T = 1)) -P (Y = 1|do(T = 0)), = z∈[k] P Y |T,Z (1|1, z) -P Y |T,Z (1|0, z) P Z (z).<label>(1)</label></formula><p>Our key contribution is to analyze and empirically validate methods for estimating the ATE from both confounded and deconfounded observations. In our setup, the confounded data contains n i.i.d. samples from the joint distribution P Y,T (marginalized over the hidden confounder Z), and the deconfounded data contains m i.i.d. samples from the full joint distribution P Y,T,Z . Thus, the confounded and deconfounded data are (y, t) and (y, t, z) tuples, respectively. Recall that here deconfounding means selecting a confounded data point (y, t) and revealing the value of its confounder z. There are two ways that we can obtain m deconfounded data, one through collecting m deconfounded data directly without using the confounded data, and the other through revealing the value of the confounder for m confounded data points. Note that given this graph, we cannot exactly calculate the ATE unless we intervene or make further assumptions on the structure of the causal graph. Recall that such interventions or graph structures may not be available (e.g., in the case of genetic mutation). Furthermore, when deconfounded data is scarce and confounded data is comparatively plentiful, we hope to improve our ATE estimates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Generalizability of Our Model</head><p>First, we note that the use of categorical (even binary) data is well-established in both theory <ref type="bibr" target="#b1">(Bareinboim and Pearl, 2013)</ref> and application <ref type="bibr" target="#b9">(Knudson, 2001;</ref><ref type="bibr" target="#b17">Rayner et al., 2016)</ref>, and not merely a simplifying proxy for continuous data. Next, we show that our model subsumes scenarios with multiple categorical confounders. First, absent additional distributional assumptions, our model captures multiple unobserved confounders by simple concatenation (since we impose no limit on the number of classes) without loss. Now, one could make additional assumptions (indeed, a high-dimensional setting might necessitate such assumptions) that could render alternative algorithms applicable. However, there exist many applications where (a) the confounder is of moderate dimension; and (b) a practitioner would be dubious of any additional assumption <ref type="bibr" target="#b2">(Bates et al., 2020)</ref>. Second, although in this scenario we implicitly assume that the set of confounders is either never observed or entirely observed, this is also without loss so long as the costs of revealing each confounders are the same (e.g. the genetic example). Intuitively, because we do not impose any independence assumption on the set of confounder, revealing all confounders offers maximal information on the joint distribution of the confounders.</p><p>We formalize this statement in Appendix A.1. While for simplicity we focus only on the setting in which our confounder can be retroactively observed, as we show in Appendix A.2 our model can be applied straightforwardly to handle a set of additional pretreatment covariates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Infinite Confounded Data</head><p>In this subsection, we address the setting where we have an infinite amount of confounded data (n = ∞), i.e., the marginal distribution P Y,T is known exactly.</p><p>We leave the analysis on finite confounded data to Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deconfounded Data Alone</head><p>We begin with the baseline approach of using only the deconfounded data. Let p z yt = P Y,T,Z (y, t, z), and let pz yt be empirical estimate of p z yt from the deconfounded data using the Maximum Likelihood Estimator (MLE). Let ATE be the estimated average treatment effect calculated by plugging pz yt 's into Equation (1). In the following theorem, we show a quantity of deconfounded samples m which is sufficient to estimate the ATE to within a desired level of accuracy under the estimation process described above. Let C = 12.5k 2 ln(8k/δ) -2 throughout.</p><p>Theorem 1. (Upper Bound) Using deconfounded data alone, P ATE -ATE ≥ &lt; δ is satisfied if the deconfounded sample size m is at least</p><formula xml:id="formula_2">m base := max t,z C y p z yt -2 = max t,z 1 P T,Z (t, z) 2 C.</formula><p>The proof of Theorem 1 (Appendix B.2) relies on an additive decomposition of ATE estimation error in terms of the estimation errors on the p z yt 's, along with concentration via Hoeffding's inequality. We will contrast Theorem 1 with counterpart methods that use confounded data.</p><p>Incorporating Confounded Data Estimating the ATE requires estimating the entire distribution P Y,T,Z . To assess the utility of confounded data, we decompose P Y,T,Z into two components: (i) the confounded distribution P Y,T ; and (ii) the conditional distributions P Z|Y,T . Given infinite confounded data, the confounded distribution P Y,T is known exactly, reducing the number of free parameters in P Y,T,Z by three. The deconfounded data can then be used exclusively to estimate the conditional distributions P Z|Y,T . To ease notation, let a yt = P Y,T (y, t), and let q z yt = P Z=z|Y,T (y, t). Moreover, let a:= (a 00 , a 01 , a 10 , a 11 ), and let q denote the vector that contains q z yt for all values of Y, T and Z.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Hardness of The Problem</head><p>We first show that for particular choices of the conditional distributions, this estimation problem can be arbitrarily hard for any confounded distribution a. In particular, we show that for every fixed confounded distribution encoded by a and for any finite amount of deconfounded data m, there exist two conditional distributions encoded by q's such that we cannot distinguish these two distributions with high probability while their corresponding ATE values are constant away from each other. Let ATE a (q) denote the value of the ATE when evaluated under the distributions a and q. Then, we have Proposition 1. For every a, there exists some , δ such that for any fixed number of deconfounded samples m, we can always construct a pair of q's, say q 1 and q 2 , such that no algorithm can distinguish these two conditional distributions with probability more than 1δ, and their corresponding ATE values are away:</p><formula xml:id="formula_3">|ATE a (q 1 ) -ATE a (q 2 )| ≥ .</formula><p>Here, is a function of the confounded distribution a, and the values of q 1 and q 2 depend on the variable δ and the number of deconfounded samples, m. The proof of Proposition 1 (Appendix B.3) relies on constructing a pair of q 1 and q 2 such that the value of |ATE a (q 1 ) -ATE a (q 2 )| is constant for all confounded distribution a where the entries of a are strictly positive. In particular, this happens when the entries of the conditional distribution q approach to 0.</p><p>Unless otherwise mentioned, in the rest of the paper, we assume that each entry of the conditional distribution, q z yt , is bounded within the interval [β, 1 -β], for some small positive constant β. We first provide a lower bound on the sample complexity needed for any algorithm and any confounded distribution: Theorem 2. (Lower Bound) For any estimator and sample selection policy, the number of deconfounded samples m needed to achieve P ATE -ATE ≥ &lt; δ is at least Ω( -2 log(δ -1 )).</p><p>The proof of Theorem 2 (Appendix B.4) proceeds by construction. When comparing this lower bound with the upper bounds that we will present later, we observe that our sample complexities are tight up to a constant.</p><p>In the rest of the section, we first derive an upper bound of the sample complexity of a natural policy that is analogous to passive sampling (Theorem 3). We then derive the worst-case upper bound over all possible conditional distributions, P Z|Y,T , in Corollary 1. Next, we propose two additional sampling policies, one of which enjoys an instance independent guarantee over the worst-case conditional distribution in P Z|Y,T . We compare these sampling policies by investigating their sample complexity upper bounds (Theorem 5), worstcase upper bounds (Corollary 2), and lower bounds (Theorem 6). Table <ref type="table">1</ref> summarizes our worst-case upper bound and lower bound results. In addition, we derive a worst-case sample complexity guarantee of our proposed sampling policy in Theorem 4.</p><p>Let qz yt be the empirical estimate of q z yt from the confounded data using the MLE (where m confounded data were deconfounded randomly). Then, we will always calculate ATE by plugging the a yt 's and qz yt 's into Equation (1). The following theorem upper bounds the sample complexity for this estimator (later, we refer to this sampling policy as the natural selection policy):</p><formula xml:id="formula_4">Theorem 3. (Upper Bound) When incorporating (in- finite) confounded data, P (| ATE -ATE| ≥ ) &lt; δ is satisfied if the number of deconfounded samples m is at least m nsp := max t,z C y a yt y a yt q z yt 2 = max t,z P T (t) P T,Z (t, z) 2 C. (2)</formula><p>The proof of Theorem 3 is included in Appendix B.5. Notably, m nsp is less than m base for any problem instance, highlighting the value of confounded data.</p><p>In addition, when q z yt ∈ [β, 1 -β], the maximum of Equation (2) over q is obtained at q z yt = β for some t, z. Let M nsp be the worst-case m nsp over all possible values of q. Since min max t 1/( y a yt ) is achieved when y a yt = 1/2, max t 1/( y a yt ) ≥ 2. Thus, Corollary 1. (Worst-Case Upper Bound Guarantee)</p><formula xml:id="formula_5">M nsp := max q m nsp = max t C β 2 y a yt ≥ 2C β 2 .</formula><p>Sample Selection Policies One important consequence of our procedure for estimating the ATE is that the four conditional distributions are estimated separately: the deconfounded data is partitioned into four groups, one for each (y, t) ∈ {0, 1} 2 , and the empirical measures qz yt 's are then calculated separately. This means that the procedure does not rely on the fact that the deconfounded data is drawn from the exact distribution P Y,T,Z , and in particular, the draws might as well have been made directly from the conditional</p><formula xml:id="formula_6">w M NSP β -2 C 1 max t a1t( y a yt ) 2 ( y ayt) 2 , a0t( y a yt ) 2 ( y ayt) 2 β -2 C max t 1 y ayt USP 4β -2 C 1 max t a 2 1t ( y a yt ) 2 ( y ayt) 2 , a 2 0t ( y a yt ) 2 ( y ayt) 2 4β -2 C max t y a 2 yt ( y ayt) 2 OWSP 2β -2 C 1 max t a1t( y a yt ) 2 y ayt , a0t( y a yt ) 2 y ayt 2β -2 C</formula><p>Table <ref type="table">1</ref>: Comparison between the instance-specific lower bound (w) and the worst-case upper bound (M ).</p><p>distributions P Z|Y,T . Suppose now that we can draw directly from these conditional distributions. This situation may arise when the confounder is fixed (like a genetic trait) and can be observed retrospectively. We now ask, given a budget for selective deconfounding samples, how should we allocate our samples among the four groups ((y, t) ∈ {0, 1} 2 )?</p><p>Let x = (x 00 , x 01 , x 10 , x 11 ) denote a selection policy with x yt indicating the proportion of samples allocated to group (y, t), and yt x yt = 1. We consider the following three non-adaptive selection policies:</p><p>1. Natural (NSP): x yt = a yt = P Y,T (y, t)-this is similar to drawing from P Y,T,Z , and is analogous to passive sampling.</p><p>2. Uniform (USP): x yt = 1/4. Splits samples evenly across all four conditional distributions.</p><p>3. Outcome-weighted (OWSP): x yt = ayt 2 y ayt = P Y |T (y|t)/2. Splits samples evenly across treatment groups (T = 0 vs. 1), and within each treatment group, choosing the number of samples to be proportional to the outcome (Y = 0 vs. 1).</p><p>While the particular form of OWSP appears to be the least intuitive, later we show it was in fact the unique policy that provides an instance independent guarantee when considering the worst-case q's.</p><p>For some fixed and δ, let µ nsp be the minimum number of samples needed to achieve P (| ATE -ATE| ≥ ) &lt; δ under the natural selection policy over all estimators. We similarly define µ usp and µ owsp . Then Theorem 3 provides an upper bound on µ nsp by studying the upper bound of a specific estimator. Before we provide an upper bound of the sample complexity of µ usp and µ owsp , we first establish that µ nsp may be significantly worse than µ owsp , but µ owsp is never much worse.</p><p>Theorem 4. For any fixed ∈ [0, 0.5 -2β(1 -β)] and any fixed δ &lt; 1, there exist distributions where µ owsp /µ nsp is arbitrarily close to zero. In addition, for any estimator and every distribution, µ owsp /µ nsp ≤ 2.</p><p>The proof of Theorem 4 (Appendix B.6) proceeds by construction. Note that the upper bound of in Theorem 4 is not necessary the maximum achievable . Instead it provides a range where Theorem 4 holds. Next, we provide the upper bounds of µ usp and µ owsp by analyzing our algorithm (analogous to Theorems 1 and 3): Theorem 5. (Upper Bound) Under the uniform selection policy, with (infinite) confounded data incorporated,</p><formula xml:id="formula_7">P (| ATE -ATE| ≥ ) &lt; δ is satisfied if µ usp is at least m usp := max t,z C y 4a 2 yt y a yt q z yt 2 = max t,z 4 y P Y,T (y, t) 2 P T,Z (t, z) 2 C.</formula><p>Similarly, for the outcome-weighted selection policy:</p><formula xml:id="formula_8">m owsp := max t,z 2C y a yt 2 y a yt q z yt 2 = max t,z 2 P Z|T (z|t) 2 C.</formula><p>The proofs of Theorems 3 and 5 (Appendix B.5), which differ from that of Theorem 1, require a modification to Hoeffding's inequality (Appendix, Lemma 4), which we derive to bound the sample complexity of the weighted sum of two independent random variables. Theorem 5 points to some additional advantages of OWSP. First, OWSP has the nice property that the sufficient number of samples, m owsp , does not depend on P Y,T . Second, a comparison of the quantities m usp and m owsp suggests that USP is strictly dominated by OWSP, since 4a</p><formula xml:id="formula_9">2 0t + 4a 2 1t -2(a 0t + a 1t ) 2 = 2(a 0t -a 1t ) 2 ≥ 0.</formula><p>We might hope for a similar result by comparing m owsp with m nsp from comparing Theorems 3 and 5, but neither strictly dominates the other. Instead, recall that Theorem 4 shows that µ nsp may be significantly worse than µ owsp , but µ owsp is never much worse. Similar to Corollary 1, we now derive an equivalent corollary for Theorem 5 where we consider the worst case over q's.</p><p>Let M usp and M owsp be the maximum values of m usp and m owsp , respectively, over all possible values of q. Corollary 2. (Worst-Case Upper Bound Guarantee) First, note that M owsp is independent of the confounded distribution a. Furthermore, from the proof of Theorem 5, we observe that OWSP is the unique policy that makes this upper bound independent of a. When comparing Corollaries 1 and 2, we observe that OWSP always dominates NSP when taking the worst case over q's. Lastly, we provide the lower bounds of µ nsp , µ usp , and µ owsp that are analogous to Theorem 2: Theorem 6. (Lower Bound) For every a, there exists a q such that µ nsp is at least</p><formula xml:id="formula_10">M usp = max t 4C y a 2 yt β 2 ( y a yt ) 2 ; M owsp = max t 2C β 2 ≤ M nsp .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Number of Deconfounded Samples</head><formula xml:id="formula_11">w nsp := C 1 β 2 max t a 1t ( y a y t) 2 ( y a yt ) 2 , a 0t ( y a y t) 2</formula><p>( y a yt ) 2 ;</p><p>similarly for uniform selection policy:</p><formula xml:id="formula_12">w usp := C 1 β 2 max t 4 a 2 1t ( y a y t) 2 ( y a yt ) 2 , 4 a 2 0t ( y a y t) 2 ( y a yt ) 2 ;</formula><p>similarly for outcome-weighted sample selection policy:</p><formula xml:id="formula_13">w owsp := C 1 β 2 max t 2 a 1t ( y a y t) 2 y a yt , 2 a 0t ( y a y t) 2 y a yt , where t = 1 -t and C 1 ∝ (kβ -1) 2 ln(δ -1 ) -2 .</formula><p>The proof (Appendix B.7) proceeds by construction. Table <ref type="table">1</ref> summarizes our worst-case upper bounds and instance-specific lower bounds. When comparing the constants C and C 1 , we observe that the upper bounds and lower bounds match in k, , and δ, demonstrating the relative tightness of our analysis.</p><p>We have shown the advantages of OWSP given an infinite amount of confounded data. However, in practice, the confounded data is finite. In Appendix C, we analyze the sample complexity upper bound of our algorithm under finite confounded data. One new issue that arises with finite confounded data is that a sampling policy may not be feasible because there are not enough confounded samples to deconfound. In our experiments, when this happens, we approximate the target sampling policy as closely as is feasible (see Appendix E).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Since the upper bounds that we derived in Section 3 are not necessarily tight, we first perform synthetic experiments to assess the tightness of our bounds.</p><p>For the purpose of illustration, we focus on binary confounders Z throughout this section, and denote q yt = P Z=1|Y,T (y, t). We first compare the sampling policies in synthetic experiments on randomly chosen distributions P Y,T,Z , measuring both the average and worst-case performance of each sampling policy. We then measure the effect of having finite (vs. infinite) confounded data. Finally, we test the performance of OWSP on real-world data taken from a genetic database, COSMIC, that includes genetic mutations of cancer patients <ref type="bibr" target="#b23">(Tate et al., 2019;</ref><ref type="bibr">Cosmic, 2019)</ref>. Because this is (to our knowledge) the first paper to investigate the problem of selective deconfounding, the methods in described Section 2 are not directly comparable to ours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Infinite Confounded Data</head><p>Assuming access to infinite confounded data, we experimentally evaluate all four sampling methods for estimating the ATE: using deconfounded data alone, and using confounded data that has been selected according to NSP, USP, and OWSP. Let a :=  worst: a = (0.9, 0.02, 0.01, 0.07) and q = (0.9, 0.7, 0.01, 0.3). Top (middle) USP performs the worst: a = (0.79, 0.01, 0.02, 0.18) and q = (0.5, 0.01, 0.05, 0.5). Top (right) OWSP performs the worst: a = (0.5, 0.01, 0.19, 0.3) and q = (0.05, 0.5, 0.055, 0.4). Bottom: the same a's but averaged over 500 q's drawn uniformly from [0, 1] 4 .</p><p>(a 00 , a 01 , a 10 , a 11 ) , and q := (q 00 , q 01 , q 10 , q 11 ) , encoding the confounded and conditional distributions, respectively. We evaluate the performance of four methods in terms of the absolute error, | ATE -ATE|. Because the variance of our estimators cannot be analyzed in closed form, we report the variance of the absolute error averaged over different instances in terms of the error bar in the figures.</p><p>Randomly Generated Instances We first evaluate the four methods over a randomly selected set of distributions. Figure <ref type="figure" target="#fig_1">2</ref> was generated by averaging over 13,000 instances, each with the distribution P Y,T,Z drawn uniformly from the unit 7-Simplex. Every instance consists of 100 replications, each with a random draw of 1,200 deconfounded samples. The absolute error is measured as a function of the number of deconfounded samples in steps of 100 samples. Figure <ref type="figure" target="#fig_1">2</ref> (left) compares the use of deconfounded data along with the incorporation of confounded data selected naturally (as in the comparison of Theorems 1 and 3). It shows that incorporating confounded data yields a significant improvement in estimation error. For example, achieving an absolute error of 0.02 using deconfounded data alone requires more than 1,200 samples on average, while by incorporating confounded data, only 300 samples are required. We observe that the variance of our estimator has decreased dramatically by incorporating infinite confounded data. Having established the value of confounded data, Figure <ref type="figure" target="#fig_1">2</ref> (middle) compares the three selection policies. We find that OWSP outperforms both NSP and USP in terms of both the absolute error and the variance when averaged over joint distributions.</p><p>To compare the performance of our sampling policies on an instance level, we provide two scatter plots in Figure <ref type="figure" target="#fig_1">2</ref> (right), each containing the 13,000 instances in the left figures and averaged over 100 replications.</p><p>The number of deconfounded samples is fixed at 1,200. We observe that OWSP outperforms NSP and USP in the majority of instances.</p><p>Worst-Case Instances We evaluate the performance of the three selection policies on joint distributions chosen adversarially against each in Figure <ref type="figure" target="#fig_2">3</ref>. The three sub-figures (the columns) correspond to instances where NSP, USP, and OWSP perform the worst, respectively, from the left to the right. Each sub-figure is further subdivided: the top contains results for the single adversarial example while the bottom is averaged over 500 q's sampled uniformly from [0, 1] 4 . The absolute error is averaged over 10,000 replications in the left figures and over 500 in the right. In all cases, we draw 500 deconfounded samples and measure the absolute error in steps of 50 samples. Corollary 4. We observe that when the distribution of a is heavily skewed towards (Y = 0, T = 0), OWSP and USP significantly outperform NSP. Figure <ref type="figure" target="#fig_2">3</ref> (middle) shows that USP can underperform NSP, but when averaged over all possible values of q, USP performs better than NSP. Figure <ref type="figure" target="#fig_2">3</ref> (right) shows that OWSP can underperform NSP and USP, but, when compared with the left and middle column, the performance of OWSP is close to that of NSP and USP. When averaged over all possible values of q, OWSP outperforms both. Moreover, OWSP's variance is the lowest across all scenarios. Appendix D provides examples in which each of these joint distributions could appear.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Finite Confounded Data</head><p>Given only n confounded data, we test the performance of the OWSP against NSP and USP. In Figure <ref type="figure" target="#fig_3">4</ref>, the absolute error is measured as a function of the number of confounded samples in step sizes that increment in the log scale from 100 to 10,000 while fixing the number of deconfounded samples to 100. Since when we only have 100 confounded samples, the three sampling policies are identical, the error curves corresponding to NSP, USP and OWSP start at the same point on the top left corner in Figure <ref type="figure" target="#fig_3">4</ref> (left). We observe that as the number of confounded samples increases, OWSP quickly outperforms NSP and USP on average, and the gaps between OWSP and the other two selection policies widen. Since we fix the number of deconfounded samples to be 100, 1) all three sampling policies are equivalent when there are only 100 confounded samples in the dataset (i.e., we need to deconfound all 100 confounded samples in all cases), and 2) the average absolute errors of the three selection policies do not converge to 0 in Figure <ref type="figure" target="#fig_3">4</ref>. Figure <ref type="figure" target="#fig_3">4</ref> (middle and right) compare the performance of OWSP with that of the NSP and USP, respectively, on an instance level. We observe that OWSP dominates NSP and USP in the majority of instances by both the absolute error and variance. Note that if we fix the number of confounded samples and increase the number of deconfounded samples (with m ≤ n), we observe that OWSP dominates USP and NSP when the number of deconfounded samples is small. The gap shrinks as the number of deconfounded samples increases. When at m = n, all three methods are equivalent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Real-World Experiments: COSMIC</head><p>Data Previously, we chose the underlying distribution P Y,T,Z uniformly from the unit 7-Simplex. However, real-world problems of interest may not be uniformly distributed. To illustrate the practicality of our methods, we consider a real-world dataset, picking three variables to be the outcome, treatment, and confounder, and artificially hiding the confounder for some samples. Finally, we evaluate our proposed sampling methods under the assumption that we have access to infinitely many confounded samples. The Catalogue Of Somatic Mutations In Cancer (COSMIC) is a public database of DNA sequences of tumor samples. It consists of targeted gene-screening panels aggregated and manually curated over 25,000 peer reviewed papers. We focus on the variables: primary cancer site and gene. Specifically, for 1,350,015 cancer patients, we observe their cancer types, and for a subset of genes, whether or not a mutation was observed in each gene.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Causal Models</head><p>In our experiments, we designate cancer type as the outcome, a particular mutation as the treatment, and another mutation as the confounder-this setup seems reasonable because it is well known that multiple genetic mutations are correlated with individual cancer types <ref type="bibr" target="#b9">(Knudson, 2001)</ref>, and that mutations can cause both cancer itself and other mutations. As a concrete example, mutations in the genes that code RNA polymerases (responsible for ensuring the accuracy of replicating RNA) are found to increase the likelihood of both other mutations and certain cancer types <ref type="bibr" target="#b17">(Rayner et al., 2016)</ref>. The setting where the treatment mutation and cancer outcome are observed and the confounding mutation is unobserved is plausible because it is common that the majority of patients only have a subset of genes sequenced (e.g. from a commercial panel). The top 6 most commonly mutated genes were selected as treatment and confounder candidates. For each combination of a cancer type and two of these genes, we removed patients for whom these genes was not sequenced, and kept all pairs that had at least 40 patients in each of the four treatment-outcome groups (to ensure our deconfounding policies would have enough samples to deconfound). This procedure gave us 275 unique combinations of a cancer (outcome), mutation (treatment), and another mutation (confounder). Since on average, each {cancer, mutation, mutation} tuple contains around 125,883 patients, we took the estimated empirical distribution as the data-generating distribution and applied the ATE formula described in Section 3 to obtain the true ATE. To model the unobserved confounder, we hid the confounding mutation parameter, only revealing it to a sampling policy when it requested a deconfounded sample. We compared the use of deconfounded data along with the incorporation of confounded data under the three sampling selection polices: NSP, USP, and OWSP.</p><p>Results Figure <ref type="figure">5</ref> (left) was generated with these 275 instances each repeated for 10,000 replications. The absolute error is measured as a function of the number of deconfounded samples in step sizes of 15. First, similar to Figure <ref type="figure" target="#fig_1">2</ref>, we observe that incorporating confounded data reduces both the absolute estimation error and the variance of the estimator by a large margin. Note the improvement of OWSP over NSP is larger in this case as compared to that seen in Figure <ref type="figure" target="#fig_1">2</ref>. Furthermore, when the number of deconfounded samples is small, OWSP outperforms USP. Note that Figure <ref type="figure">5</ref> (left) does not start with 0 because absent any deconfounded data, the estimated ATE is the same for all sampling policies. In Figure <ref type="figure">5</ref> (middle, right), we fix the number of deconfounded samples to be 45 and compare the performance of OWSP against that of NSP and USP, respectively. Both figures contain the 275 instances in the left figure, averaged over 10,000 replications. We observe that under this setup, OWSP dominates NSP in all instances, and outperforms USP in the majority of instances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>We propose the problem of causal inference with selectively deconfounded data. This problem is particularly motivated by the scenarios where interventions on treatment is not available. We theoretically analyze the upper bounds and lower bounds on the amount of deconfounded data required under each sample selection policy. In addition, we theoretically demonstrate that the best-case gain of our proposed policy OWSP is unbounded when compared with NSP while the worst-case relative performance is bounded. We point to several promising directions for potential future research. First, we are currently extending our analysis to the adaptive case using ideas from active learning and combinatorial optimization. Second, we plan to extend our results to more general causal problems, including linear and semi-parametric causal models. Finally, we may extend the idea of selective revelation of information beyond confounders to incorporate mediators and proxies. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A The Generalization of Our Models</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Multiple Confounders</head><p>In this section, we show that because we do not impose any independence assumption on the set of confounder, revealing the values of all confounders offers maximal information on the joint distribution of the confounders. In particular, we will illustrate through the case where we have two binary confounders. The extension to multiple categorical confounders is straight forward.</p><p>In the case where we have two binary confounders Z 1 and Z 2 , we can express the ATE as follows:</p><formula xml:id="formula_14">ATE = z1,z2 P Y |T,Z1,Z2 (1|1, z 1 , z 2 ) -P Y |T,Z1,Z2 (1|0, z 1 , z 2 ) P Z1,Z2 (z 1 , z 2 ).</formula><p>With an infinite amount of confounded data, we are provided with the joint distribution P Y,T (y, t). Thus, it remains to estimate the conditional distributions P Z1,Z2|Y,T . In our paper, we consider only the non-adaptive policies, i.e., the number of samples to deconfound in each group (y, t) is fixed a priori. In the case where the costs of revealing the values of Z 1 and Z 2 are the same and we do not have any prior knowledge on the distributions of Z 1 and Z 2 , the variables Z 1 and Z 2 becomes exchangeable. In the case where the sample selection policies are completely non-adaptive (which is the case that we consider in this paper), by the symmetry of the variables Z 1 and Z 2 , we have that sampling from the joint distribution of Z 1 and Z 2 yields the maximum expected information on the value of the ATE. (Note that if the confounders take categorical values of different sizes and we allow adaptive policies, then we might be able to reduce the total cost of deconfounding to estimate the ATE to within a desired accuracy level.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Pretreatment Covariates</head><p>In the case where we have known pretreatment covariates X, our model can be applied in estimating the individual treatment effect where we make the common ignorability assumption on the pretreatment covariates X and the confounder Z: given pretreatment covariates X and the confounder Z, the values of outcome variable, Y = 0 and Y = 1, are independent of treatment assignment. In this case, the distributions P Y,T (y, t) and P X (x) are known and the Individual Treatment Effect (ITE):</p><formula xml:id="formula_15">ITE = z,x P Y |T,Z,X (1|1, z, x) -P Y |T,Z,X (1|0, z, x) P Z,X (z, x) = z,x P Y |T,Z,X (1|1, z, x) -P Y |T,Z,X (1|0, z, x) P Z|X (z|x)P X (x).<label>(3)</label></formula><p>Note that in Equation (3) the only distributions we need to estimate are the conditional distributions P Z|Y,T,X .</p><p>The values of P Y |T,Z,X and P Z|X can be calculated from P Z|Y,T,X by first conditioning the confounded distributions P Y,T on the values of the pretreatment covariates X, i.e., we first subsample all confounded (outcome, treatment) pairs for a fixed value of X, X = x, and then within each subsample, estimate the conditional distributions P Z|Y,T,X by applying our methods. To obtain ITE, we weight the estimates we obtain from all subsamples by P X (x).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Proofs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Review of Classical Results in Concentration Inequalities</head><p>Before embarking on our proofs, we state some classic results that we will use frequently. The following concentration inequalities are part of a family of results collectively referred to as Hoeffding's inequality (e.g., see Vershynin ( <ref type="formula">2018</ref>)).</p><p>Lemma 1 (Hoeffding's Lemma). Let X be any real-valued random variable with expected value E[X] = 0, such that a ≤ X ≤ b almost surely. Then, for all λ ∈ R, E [exp(λX)] ≤ exp λ 2 (b-a) 2</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>8</head><p>.</p><p>Theorem 7 (Hoeffding's inequality for general bounded r.v.s). Let X 1 , ..., X N be independent random variables such that X i ∈ [m i , M i ], ∀i. Then, for t &gt; 0, we have</p><formula xml:id="formula_16">P N i=1 (X i -E[X i ]) ≥ t ≤ 2 exp - 2t 2 N i=1 (Mi-mi) 2 .</formula><p>To begin, recall the notation introduced in Section 3: we model the binary-valued treatment, the binary-valued outcome, and the categorical confounder as the random variables T ∈ {0, 1}, Y ∈ {0, 1}, and Z ∈ {1, . . . , k}, respectively. The underlying joint distribution of these three random variables is represented as P Y,T,Z (•, •, •). To save on space for terms that are used frequently, we define the following shorthand notation:</p><p>p z yt = P Y,T,Z (y, t, z), a yt = P Y,T (y, t), q z yt = P Z|Y,T (z|y, t).</p><p>These terms appear frequently because, to estimate the entire joint distribution on Y, T, Z (the p z yt 's), it suffices to estimate the joint distribution on Y, T (the a yt 's), along with the conditional distribution of Z on Y, T (the q z yt 's):</p><formula xml:id="formula_17">p z yt = a yt q z yt .</formula><p>Finally, let pz yt , âz yt , and qz yt be the empirical estimates of p z yt , a z yt , and q z yt , respectively, using the MLE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Proof of Theorem 1</head><p>Theorem 1. (Upper Bound) Using deconfounded data alone, P ATE -ATE ≥ &lt; δ is satisfied if the deconfounded sample size m is at least</p><formula xml:id="formula_18">m base := max t,z C y p z yt -2 = max t,z 1 P T,Z (t, z) 2 C.</formula><p>Proof of Theorem 1. This proof proceeds as follows: first, we prove a sufficient (deterministic) condition, on the errors of our estimates of p z yt 's, under which | ATE -ATE| is small. Second, we show that the errors of our estimates of p z yt 's are indeed small with high probability.</p><p>Step 1: First, we can write the ATE in terms of the p z yt 's as follows:</p><formula xml:id="formula_19">ATE = z P Y |T,Z (1|1, z) -P Y |T,Z (1|0, z) P Z (z) = z       p z 11 y p z y1 - p z 10 y p z y0    y,t p z yt    .</formula><p>In order for the ATE to be well-defined, we assume y p z yt ∈ (0, 1) for all t, z throughout. We can then decompose | ATE -ATE|: </p><formula xml:id="formula_20">| ATE -ATE| = z      </formula><p>Step 2: To bound the above terms, we first derive Lemma 2 for bounding the error of the product of two estimates in terms of their two individual errors:</p><p>Lemma 2. For any u, û ∈ [-1, 1], and v, v ∈ [0, 1], suppose there exists , θ ∈ (0, 1) such that all of the following conditions hold:</p><formula xml:id="formula_22">1. |u -û| ≤ (1 -θ) 2. |v -v| ≤ θ 3. u + ≤ 1 4. v + ≤ 1 5. ≤ min(u, v) Then, |uv -ûv| ≤ . Proof of Lemma 2. Since |u -û| ≤ (1 -θ) , we have û ∈ [u -(1 -θ) , u + (1 -θ) ], and similarly, from |v -v| ≤ θ , we have v ∈ [v -θ , v + θ ]. Thus, |uv -ûv| ≤ max (|uv -(u + (1 -θ) )(v + θ )|, |uv -(u -(1 -θ) )(v -θ )|) (because v, v ≥ 0) = max( θu + (1 -θ)v + (1 -θ)θ 2 , θu + (1 -θ)v -(1 -θ)θ 2 ) = θu + (1 -θ)v + (1 -θ)θ 2 (because (1 -θ)θ 2 &gt; 0) ≤ |θ(u + ) + (1 -θ)v | (because θ 2 &gt; (1 -θ)θ 2 ) ≤ (because u + ∈ [-1, 1], and v ≤ 1).</formula><p>We can apply Lemma 2 directly to the terms in (4) by setting  <ref type="formula" target="#formula_21">4</ref>) holds if, for some θ ∈ (0, 1), we have</p><formula xml:id="formula_23">|v z -vz | &lt; θ k and |u z -ûz | &lt; 1 -θ k .</formula><p>While we can apply standard concentration results to the |v z -vz | terms, the |u z -ûz | terms will need to be further decomposed: (5)</p><p>Step 3: To bound these terms, we derive Lemma 3. Recall that p z 1t + p z 0t , pz 1t + pz 0t ∈ (0, 1). Lemma 3. For any w + s, ŵ + ŝ ∈ (0, 1), if |w + s -ŵ -ŝ| ≤ (w + s) and |w -ŵ| ≤ (w + s) , then</p><formula xml:id="formula_24">w w + s - ŵ ŵ + ŝ ≤ 2 .</formula><p>Proof of Lemma 3. First, since |w + s -ŵ -ŝ| ≤ (w + s) , we have that</p><formula xml:id="formula_25">w + s ŵ + ŝ -1 ≤ w + s ŵ + ŝ ,</formula><p>or equivalently,</p><formula xml:id="formula_26">1 - w + s ŵ + ŝ ≤ w + s ŵ + ŝ ≤ 1 + w + s ŵ + ŝ .</formula><p>We can apply this inequality and rearrange terms as follows to conclude the proof:</p><formula xml:id="formula_27">w w + s - ŵ ŵ + ŝ = 1 w + s w - ŵ w + s ŵ + ŝ ≤ 1 w + s max w -ŵ 1 - w + s ŵ + ŝ , w -ŵ 1 + w + s ŵ + ŝ = 1 w + s max w -ŵ + w + s ŵ + ŝ ŵ , w -ŵ - w + s ŵ + ŝ ŵ = max w - ŵ w + s + ŵ ŵ + ŝ , w - ŵ w + s - ŵ ŵ + ŝ ≤ w - ŵ w + s + ŵ ŵ + ŝ ≤ w + s w + s + ŵ ŵ + ŝ ≤ 2 .</formula><p>The second to last inequality follows from the assumption that |w -ŵ| ≤ (w + s) .</p><p>Lemma 3 implies that ( <ref type="formula">5</ref>) is satisfied if</p><formula xml:id="formula_28">|p z 1t -pz 1t | &lt; ( y p z yt )(1 -θ) 4k and |p z 1t + p z 0t -pz 1t -pz 0t | &lt; ( y p z yt )(1 -θ) 4k .</formula><p>Step 4: We've shown above that | ATE -ATE| ≤ is satisfied when</p><formula xml:id="formula_29">|v z -vz | &lt; θ k , |p z 1t -pz 1t | &lt; ( y p z yt )(1 -θ) 4k ,</formula><p>and</p><formula xml:id="formula_30">|p z 1t + p z 0t -pz 1t -pz 0t | &lt; ( y p z yt )(1 -θ) 4k , ∀t, z. Note that if ∀t, |p z 1t + p z 0t -pz 1t -pz 0t | = y p z yt -y pz yt &lt; ( y p z yt )(1-θ) 4k then |v z -vz | = y,t p z yt - y,t pz yt ≤ t y p z yt - y pz yt &lt; ( y,t p z yt )(1 -θ) 4k ≤ (1 -θ) 4k .</formula><p>Thus, to remove the first constraint</p><formula xml:id="formula_31">|v z -vz | &lt; θ k , we set θ k = (1 -θ) 4k ,</formula><p>and obtain θ = 1 5 .</p><p>Step 5: To summarize so far, Lemmas 2 and 3 allow us to upper bound the error of our estimated ATE in terms of upper bounds on the error of our estimates of its constituent terms:</p><formula xml:id="formula_32">P | ATE -ATE| &lt; ≥ P t,z |p z 1t -pz 1t | &lt; y p z yt 5k t,z |p z 1t + p z 0t -pz 1t -pz 0t | &lt; y p z yt 5k ,</formula><p>or equivalently,</p><formula xml:id="formula_33">P | ATE -ATE| ≥ ≤ P t,z |p z 1t -pz 1t | ≥ y p z yt 5k t,z |p z 1t + p z 0t -pz 1t -pz 0t | ≥ y p z yt 5k .</formula><p>Applying a union bound, we have</p><formula xml:id="formula_34">P | ATE -ATE| ≥ ≤ t,z P |p z 1t -pz 1t | ≥ y p z yt 5k + P |p z 1t + p z 0t -pz 1t -pz 0t | ≥ y p z yt 5k . (<label>6</label></formula><formula xml:id="formula_35">)</formula><p>Step 6: Finally, we can apply Hoeffding's inequality (Theorem 7) to obtain the upper bound for the inequality above. Let X z yt be the random variable that maps the event (Y = y, T = t, Z = z) → {0, 1}. Then, X z yt is a Bernoulli random variable with parameter p z yt . Let m denote the total number of deconfounded samples that we have. Since pyt is estimated through the MLE, we have pz</p><formula xml:id="formula_36">yt = m i=1 X z yt m</formula><p>. Applying Theorem 7, we obtain:</p><formula xml:id="formula_37">P m i=1 X z yt m -p z yt ≥ y p z yt 5k ≤ 2 exp   -2m y p z yt 2 2 25k 2    , and<label>(7)</label></formula><formula xml:id="formula_38">P m i=1 X z 1t + X z 0t m -p z 1t -p z 0t ≥ y p z yt 5k ≤ 2 exp   -2m y p z yt 2 2 25k 2    .<label>(8)</label></formula><p>Combining ( <ref type="formula" target="#formula_34">6</ref>), ( <ref type="formula" target="#formula_37">7</ref>), and (8), we have</p><formula xml:id="formula_39">P | ATE -ATE| ≥ ≤ t,z P |p z 1t -pz 1t | ≥ y p z yt 5k + P |p z 1t + p z 0t -pz 1t -pz 0t | ≥ y p z yt 5k ≤ 4k max t,z   2 exp   -2m y p z yt 2 2 25k 2       = 8k max t,z exp   -2m y p z yt 2 2 25k 2    ≤ δ,</formula><p>where the second line follows from the fact that, since t is binary, there are 4k terms in total. Solving the above equation, we conclude that P (| ATE -ATE| ≥ ) &lt; δ is satisfied when the sample size m is at least</p><formula xml:id="formula_40">m ≥ 12.5k 2 ln( 8k δ ) 2 max t,z 1 y p z yt 2 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.3 Proof of Proposition 1</head><p>Proposition 1. For every a, there exists some , δ such that for any fixed number of deconfounded samples m, we can always construct a pair of q's, say q 1 and q 2 , such that no algorithm can distinguish these two conditional distributions with probability more than 1 -δ, and their corresponding ATE values are away:</p><formula xml:id="formula_41">|ATE a (q 1 ) -ATE a (q 2 )| ≥ .</formula><p>Proof of Proposition 1. It suffices to show for the case where confounder takes binary value. The extension to categorical confounder is straightforward as illustrated in the proof of Theorem 6 in Appendix B.7. Let q yt = P (Z = 1|Y = y, T = t). To show that Proposition 1 is true, it is sufficient to show that there exist a positive constant c (that depends on a) such that for all fixed a, there exists a pair of q and q such that ATE a (q) -ATE a (q ) &gt; c, with q and q close in distribution. We proceed by construction. For fixed a, consider the following q pairs: q = (q 00 , 0, q 10 , γ) and q = (q 00 , γ, q 10 , 0). Then, we have ATE a (q) = (a 00 q 00 + a 10 q 10 + a 11 γ)</p><formula xml:id="formula_42">+ a 11 (1 -γ) a 11 (1 -γ) + a 01</formula><p>(1 -a 00 q 00 -a 10 q 10 -a 11 γ)a 10 q 10 a 10 q 10 + a 00 q 00 (a 00 q 00 + a 10 q 10 + a 11 γ) -a 10 (1 -q 10 ) a 10 (1 -q 10 ) + a 00 (1 -q 00 ) (1 -a 00 q 00 -a 10 q 10 -a 11 γ),</p><p>and similarly, we have</p><formula xml:id="formula_43">ATE a (q ) = a 11 a 11 + a 01 (1 -γ)</formula><p>(1 -a 00 q 00 -a 01 γ -a 10 q 10 ) -a 10 q 10 a 10 q 10 + a 00 q 00 (a 00 q 00 + a 01 γ + a 10 q 10 ) -a 10 (1 -q 10 ) a 10 (1 -q 10 ) + a 00 (1 -q 00 ) (1 -a 00 q 00 -a 01 γ -a 10 q 10 ).</p><p>In particular, lim γ→0 ATE a (q) -ATE a (q ) = a 00 q 00 + a 10 q 10 ≤ a 00 + a 10 ,</p><p>where we can choose q 00 and q 10 to be 1.</p><p>On the other hand, we can show that the number of samples needed to distinguish q from q is at least Ω(1/γ): since q and q are the same in two of the entries and symmetric on the rest two, to distinguish q and q is to distinguish a Bernoulli random variable with parameter 0 (denoting this variable B 0 ) from a Bernoulli random variable with parameter γ (denoting this random variable B γ ). Let f be any estimator of the Bernoulli random variable, and x i , ..., x m be the sequence of m observations. Then we have</p><formula xml:id="formula_45">|E X∼B m 0 [f ] -E X∼B m γ [f ]| ≤ B m 0 -B m γ 1 ≤ 2(ln 2)KL(B m 0 B m γ ) ≤ 2 (ln 2)γm,</formula><p>where the last inequality is because when given m samples,</p><formula xml:id="formula_46">KL(B m 0 B m γ ) ≤ (2γ ln 2 + (1 -2γ) ln 1-2γ 1-γ )m ≤ 2γm.</formula><p>On the other hand, any hypothesis test that takes n samples and distinguishes between H 0 : X 1 , ..., X n ∼ P 0 and H 1 : X 1 , ..., X n ∼ P 1 has probability of error lower bounded by max(P 0 (1), P 1(0)) ≥ 1 4 e -nKL(P0 P1) , where P 0 (1) indicates the probability that we identify class H 0 while the true class is H 1 . Since P 0 (1) + P 1 (0) ≤ δ, by contradiction, we can show that m ∼ Ω(ln(δ -1 )γ -1 ).</p><p>Note that this lower bound on m can be arbitrarily large by choosing γ to be sufficiently small. However their ATE values stay constant away as observed in Equation ( <ref type="formula" target="#formula_44">9</ref>). Thus, for every fixed confounded distribution encoded by a and fixed number of deconfounded samples m, we can always construct a pair of conditional distributions encoded by q and q such that their corresponding ATEs are constant away while the probability that we correctly identify the true conditional distribution from q and q is less than 1 -δ. In particular, = c = a 00 + a 10 in the above example. (Here, we implicitly assume that a 00 + a 10 is strictly greater than zero, i.e., a 00 + a 10 &gt; 0.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.4 Proof of Theorem 2</head><p>Theorem 2. (Lower Bound) For any estimator and sample selection policy, the number of deconfounded samples m needed to achieve P ATE -ATE ≥ &lt; δ is at least Ω( -2 log(δ -1 )).</p><p>Proof of Theorem 2. Again, it suffices to show for the case where the confounder is binary. The extension to categorical confounder is straightforward as illustrated in the proof of Theorem 6 in Appendix B.7. Let q yt = P (Z = 1|Y = y, T = t). We will proceed by construction. Consider q = (q 00 , q 01 , β, β + γ) and q = (q 00 , q 01 , β + γ, β), for some small γ. Then ATE a (q) = a 11 (β + γ) a 11 (β + γ) + a 01 q 01 (a 00 q 00 + a 01 q 01 + a 10 β + a</p><formula xml:id="formula_47">11 (β + γ)) + a 11 (1 -β -γ) a 11 (1 -β -γ) + a 01 (1 -q 01 )</formula><p>(1 -a 00 q 00 -a 01 q 01 -a 10 β -a 11 (β + γ)) -a 10 β a 10 β + a 00 q 00 (a 00 q 00 + a 01 q 01 + a 10 β + a 11 (β + γ))-</p><formula xml:id="formula_48">a 10 (1 -β) a 10 (1 -β) + a 00 (1 -q 00 )</formula><p>(1 -a 00 q 00 -a 01 q 01 -a 10 β -a 11 (β + γ)),</p><p>and similarly, we have ATE a (q ) = a 11 β a 11 β + a 01 q 01 (a 00 q 00 + a 01 q 01 + a 10 (β + γ)</p><formula xml:id="formula_49">+ a 11 β) + a 11 (1 -β) a 11 (1 -β) + a 01 (1 -q 01 )</formula><p>(1 -a 00 q 00a 01 q 01 -a 10 (β + γ) -a 11 β) -a 10 (β + γ) a 10 (β + γ) + a 00 q 00 (a 00 q 00 + a 01 q 01 + a 10 (β + γ) + a 11 β)-</p><formula xml:id="formula_50">a 10 (1 -β -γ) a 10 (1 -β -γ) + a 00 (1 -q 00 )</formula><p>(1 -a 00 q 00 -a 01 q 01 -a 10 (β + γ) -a 11 β).</p><p>Ignoring the γ in the denominator, we have that ATE a (q) -ATE a (q ) = ( a 11 a 11 β + a 01 q 01 + a 10 a 10 β + a 00 q 00</p><p>)(a 00 q 00 + a 01 q 01 + a 10 β + a 11 β)γ</p><formula xml:id="formula_51">-( a 11 a 11 (1 -β) + a 01 (1 -q 01 ) + a 10 a 10 (1 -β) + a 00 (1 -q 00 )</formula><p>)(1 -a 00 q 00 -a 01 q 01 -a 10 β -a 11 β)γ</p><formula xml:id="formula_52">+ a 2 11 -a 11 a 10 a 11 β + a 01 q 01 βγ - a 2 11 -a 11 a 10 a 11 (1 -β) + a 01 (1 -q 01 ) (1 -β)γ + a 2</formula><p>10 -a 11 a 10 a 10 β + a 00 q 00 βγ -a 2 10 -a 11 a 10 a 10 (1 -β) + a 00 (1</p><formula xml:id="formula_53">-q 00 ) (1 -β)γ + a 2 11 a 11 β + a 01 q 01 γ 2 + a 2 11 a 11 (1 -β) + a 01 (1 -q 01 ) γ 2 + a 2 10</formula><p>a 10 β + a 00 q 00 γ 2 + a 2 10 a 10 (1 -β) + a 00 (1 -q 00 ) γ 2 (10)</p><p>Similar to the proof above, let B 1 denote the Bernoulli random variable with parameter β, and let B 2 denote the Bernoulli random variable with parameter β + γ. Then, given m deconfounded samples, we have</p><formula xml:id="formula_54">KL(B m 1 B m 2 ) ≤ mβ ln( β β+γ ) + m(1 -β) ln( 1-β 1-β-γ ) ≤ m ln(1 + γ 1-β-γ ) ≤ m( γ 1-β-γ - γ 2 2(1-β-γ) 2 ). Thus, we have m ∼ Ω( ln(δ -1 ) γ 2</formula><p>). From Equation (10), we observe that = ATE a (q) -ATE a (q ) ∼ Ω(γ). Combining above, we have m ∼ Ω( ln(δ -1 ) 2 ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.5 Proof of Theorems 3 and 5</head><p>Theorem 3. (Upper Bound) When incorporating (infinite) confounded data, P (| ATE -ATE| ≥ ) &lt; δ is satisfied if the number of deconfounded samples m is at least</p><formula xml:id="formula_55">m nsp := max t,z C y a yt y a yt q z yt 2 = max t,z P T (t) P T,Z (t, z) 2 C.</formula><p>(2)</p><p>Theorem 5. (Upper Bound) Under the uniform selection policy, with (infinite) confounded data incorporated,</p><formula xml:id="formula_56">P (| ATE -ATE| ≥ ) &lt; δ is satisfied if µ usp is at least m usp := max t,z C y 4a 2 yt y a yt q z yt 2 = max t,z 4 y P Y,T (y, t) 2 P T,Z (t, z) 2 C.</formula><p>Similarly, for the outcome-weighted selection policy:</p><formula xml:id="formula_57">m owsp := max t,z 2C y a yt 2 y a yt q z yt 2 = max t,z 2 P Z|T (z|t) 2 C.</formula><p>Proof of Theorems 3 and 5. In these theorems, we derive the concentration of the ATE assuming infinite confounded data, and parametrize p z yt by p z yt = a yt q z yt . Since under infinite confounded data, a yt 's are known, and thus we only need to estimate the q z yt 's. The key difference between Theorem 5 and Theorem 1 is that now we define the random variables X z yt to map the event (Z = z|Y = y, T = t) to {0, 1}. Thus, X z yt is distributed according to Bernoulli(q z yt ). Thus, to decompose |a 1t q z 1t + a 0t q z 0t -a 1t qz 1t -a 0t qz 0t |, we first show the following lemma:</p><p>Lemma 4. Let X 1 , ..., X x1m and Y 1 , ..., Y x2m be independent random variables in [0,1]. Then for any t &gt; 0, we have</p><formula xml:id="formula_58">P α x1m i=1 X i -E [X i ] x 1 m + β x2m j=1 Y j -E [Y j ] x 2 m ≥ αt + βk ≤ 2 exp   - 2m(αt + βk) 2 α 2 x1 + β 2 x2   .</formula><p>Proof of Lemma 4. First observe that</p><formula xml:id="formula_59">P α x1m i=1 X i -E [X i ] x 1 m + β x2m j=1 Y j -E [Y j ] x 2 m ≥ αt + βk = P α x 1 x1m i=1 (X i -E [X i ]) + β x 2 x2m j=1 (Y j -E [Y j ]) ≥ mαt + mβk . Now, let Z i = α x1 X i if i ∈ [1, x 1 m],</formula><p>and</p><formula xml:id="formula_60">Z i = β x2 Y i if i ∈ [x 1 m + 1, (x 1 + x 2 )m].</formula><p>Then applying Theorem 7, we have</p><formula xml:id="formula_61">P   (x1+x2)m i=1 (Z i -E[Z i ]) ≥ mαt + mβk   ≤ 2 exp - 2m 2 (αt + βk) 2 (x1+x2)m i=1 (M i -m i ) 2 = 2 exp - 2m(αt + βk) 2 α 2 x1 + β 2 x2 .</formula><p>As defined in Section 3, let x yt denote the percentage data we sample from the group yt.</p><p>Recall that from the proof of Theorem 1, we have</p><formula xml:id="formula_62">P | ATE -ATE| ≥ ≤ t,z P |p z 1t -pz 1t | ≥ y p z yt 5k + P |p z 1t + p z 0t -pz 1t -pz 0t | ≥ y p z yt 5k = t,z P |a 1t q z 1t -a 1t qz 1t | ≥ y a yt q z yt 5k + P |a 1t q z 1t + a 0t q z 0t -a 1t qz 1t -a 0t qz 0t | ≥ y a yt q z yt 5k = t,z P |q z 1t -qz 1t | ≥ y a yt q z yt 5ka 1t + P |a 1t q z 1t + a 0t q z 0t -a 1t qz 1t -a 0t qz 0t | ≥ y a yt q z yt 5k ≤ 4k max t,z   2 exp   -2x1tm y a yt q z yt 2 2 25k 2 a 2 1t    , 2 exp   -2m y a yt q z yt 2 2 25k 2 y a 2 yt xyt       ≤ δ,</formula><p>where the second to last line follows from applying Lemma 4 to the second half of the line above it. Theorem 4. For any fixed ∈ [0, 0.5 -2β(1 -β)] and any fixed δ &lt; 1, there exist distributions where µ owsp /µ nsp is arbitrarily close to zero. In addition, for any estimator and every distribution, µ owsp /µ nsp ≤ 2.</p><p>Proof of Theorem 4. We proceed by construction. For simplicity, we illustrate the correctness of Theorem 4 for binary confounders. The extension to the multi-valued confounder is straightforward and will be demonstrated in the proof of Theorem 6.</p><p>Consider the following example: a 01 = a 10 = a 11 = η, a 00 = 1 -3η, and consider the following pair of q's: q = (β, β, β, cβ) and q = (β, β, β, β), where c ≤ 1-β β is some constant. Here, one of the q and q represents the true ATE, and the other represents the estimated ATE using the best estimator. Without loss of generality, we assume that we have already identified three components of the true conditional distribution. (In general, we can always construct an instance by modifying the values of a 01 and a 10 so that the majority error is induced by estimation error on q 11 .) Then, we have ATE a (q</p><formula xml:id="formula_63">) = cβ 1+c + (1-cβ)(1-β) 2-cβ-β -η 1-2η , and ATE a (q ) = 1 2 -η 1-2η . Thus, ∆ATE := |ATE a (q) -ATE a (q )|: ∆ATE = 1 2 - cβ c + 1 - (1 -cβ)(1 -β) 2 -cβ -β .</formula><p>Note that when c = 1-β β , ∆ATE = 0.5 -2β(1 -β) ≈ 0.5. Thus, for any ∈ [0, 0.5 -2β(1 -β)], there exists some c such that = ∆ATE. Then, for any δ, let µ denote the minimum expected number of samples that we need to distinguish q from q under the best estimator. Then under NSP, the minimum number of samples that we need under the best estimator equals to µ nsp := µ/η, and under OWSP, the minimum number of samples that we need under the best estimator equals to µ oswp = 4µ. (Note that x yt = ( 1-3η 2(1-2η) , 1 4 , η 2(1-2η) , 1 4 ) under OWSP in this example.) Thus, µ owsp /µ nsp = 4η. Since in this example, η is at most 1/4, µ owsp /µ nsp ≤ 1 and can be arbitrarily close to 0 as η → 0. (Intuitively, the first statement is true because when t a 0t t a 1t and a 00 ≈ a 01 , it is equally important to estimate q z 0t 's and q z 1t 's according to the ATE expression. However, under this setup, the number of samples allocated to groups (0, t)'s decreases as a 0,t 's approach to 0 under NSP, while under OWSP, half of the deconfounded samples are always dedicated to estimate the q z 0t 's.)</p><p>Next, we show the last sentence in Theorem 4 is true. For any fixed , δ &lt; 1, let µ nsp be the minimum expected number of samples needed to achieve P (| ATE -ATE| ≥ ) &lt; δ under natural selection policy for the best estimator, then when w owsp := 2µ nsp max t y a yt also achieves P (| ATE -ATE| ≥ ) &lt; δ under the outcomeweighted selection policy. The reason is that when using w owsp number of deconfounded samples, the number of deconfounded data allocated to each yt group is at least as much as those under the natural selection policy. Thus, we have µ owsp ≤ w owsp ≤ 2µ nsp , where the last inequality is because max t y a yt &lt; 1.</p><p>B.7 Proof of Theorem 6 Theorem 6. (Lower Bound) For every a, there exists a q such that µ nsp is at least</p><formula xml:id="formula_64">w nsp := C 1 β 2 max t a 1t ( y a y t) 2</formula><p>( y a yt ) 2 , a 0t ( y a y t) 2</p><p>( y a yt ) 2 ;</p><p>similarly for uniform selection policy:</p><formula xml:id="formula_65">w usp := C 1 β 2 max t 4 a 2 1t ( y a y t) 2 ( y a yt ) 2 , 4</formula><p>a 2 0t ( y a y t) 2 ( y a yt ) 2 ;</p><p>similarly for outcome-weighted sample selection policy:</p><formula xml:id="formula_66">w owsp := C 1 β 2 max t 2 a 1t ( y a y t) 2 y a yt , 2 a 0t ( y a y t) 2 y a yt</formula><p>, where t = 1 -t and C 1 ∝ (kβ -1) 2 ln(δ -1 ) -2 .</p><p>Proof. Consider q = (q z 00 , q z 01 , q z 10 , q z 11 ) where q 1 01 = β, q 1 11 = β + γ, and q z 11 = q z 01 -γ/(k -1) for z = 2, ..., k, with z q z 01 = z q z 11 = 1. We assume that q z 11 , q z 01 ∈ [β, 1 -β] for some suitable β and γ for all values of Z. Similarly, we consider the q where the entries of q z 01 and q z 11 are flipped, i.e., q = (q z 00 , q z 11 , q z 10 , q z 01 ), for some small γ, where the q z yt 's are defined above. Then, ATE a (q) = z a 11 q z 11 y a y1 q z y1 -a 10 q z 10 y a y0 q z y0 y,t a yt q z yt = a 11 (β + γ) a 11 (β + γ) + a 01 β (a 00 q 1 00 + a 01 β + a 10 q 1 10 + a 11 (β + γ)) -a 10 q 1 10 a 10 q 1 10 + a 00 q 1 00 (a 00 q 1 00 + a 01 β + a 10 q 1 10 +</p><formula xml:id="formula_67">a 11 (β + γ)) + k z=2 a 11 q z 01 -γ k-1</formula><p>a 11 q z 01 -γ k-1 + a 01 q z 01 a 00 q z 00 + a 01 q z 01 + a 10 q z 10 + a 11 q z 01 -</p><formula xml:id="formula_68">γ k -1 - k z=2</formula><p>a 10 q z 10 a 10 q z 10 + a 00 q z 00 a 00 q z 00 + a 01 q z 01 + a 10 q z 10 + a 11 q z 01 -</p><formula xml:id="formula_69">γ k -1 ,</formula><p>and similarly, we have ATE a (q ) = a 11 β a 11 β + a 01 (β + γ) (a 00 q 1 00 + a 01 (β + γ) + a 10 q 1 10 + a 11 β) -a 10 q 1 10 a 10 q 1 10 + a 00 q 1 00 (a 00 q 1 00 + a 01 (β + γ)+ a 10 q 1 10 + a 11 β) + k z=2 a 11 q z 01 a 11 q z 01 + a 01 q z 01 -γ k-1 a 00 q z 00 + a 01 q z 01 -γ k -1 + a 10 q z 10 + a 11 q z 01k z=2 a 10 q z 10 a 10 q z 10 + a 00 q z 00 a 00 q z 00 + a 01 q z 01 -γ k -1 + a 10 q z 10 + a 11 q z</p><p>Ignoring the γ in the denominator, we have that ATE a (q) -ATE a (q ) ≈ a 11 a 11 β + a 01 β (a 00 q 1 00 + a 01 β + a 10 q 1 10 + a 11 β)γ + a 10 q 1 10 (a 01 -a 11 ) a 10 q 1 10 + a 00 q 1 00 γ k z=2 a 11 /k -1 a 11 q z 01 + a 01 q z 01 (a 00 q z 00 + a 10 q z 10 + (a 01 + a 11 )q z 10 ) γ -k z=2 a 10 q z 10 (a 01 -a 11 ) a 10 q z 10 + a 00 q z 00 1</p><formula xml:id="formula_70">k -1 γ + a 2 11 a 11 β + a 01 β γ 2 + k z=2 a 2 11 a 11 q z 01 + a 01 q z 01 γ 2 (k -1) 2 =</formula><p>a 11 a 11 β + a 01 β (a 00 q 1 00 + a 10 q 1 10 )γ + a 10 q 1 10 (a 01 -a 11 ) a 10 q 1 10 + a 00 q 1 00 γ -</p><formula xml:id="formula_71">a 11 k -1 k z=2</formula><p>a 00 q z 00 + a 10 q z 10 a 11 q z 01 + a 01 q z 01 γ</p><formula xml:id="formula_72">- 1 k -1 k z=2 a 10 q z 10 (a 01 -a 11 ) a 10 q z 10 + a 00 q z 00 γ + a 2 11 a 11 β + a 01 β γ 2 + k z=2 a 2 11 a 11 q z 01 + a 01 q z 01 γ 2 (k -1) 2<label>(11)</label></formula><p>Since the second order terms in γ is dominated by the first order terms in γ, thus to find the highest lower bound for sample complexity in this instance is to find the largest coefficient in front of γ.</p><p>Assuming that β k and kβ &lt; 1, then the maximum of Equation ( <ref type="formula" target="#formula_72">11</ref>) is achieved when q z 00 = q z 10 = β , q 1 00 = q 1 10 = 1 -kβ, and q z 01 = (1 -β)/(k -1), and the coefficient in front of γ is</p><formula xml:id="formula_73">a 11 a 11 + a 01 (a 00 + a 10 )( 1 β - k -β 1 -β ) ≈ a 11 a 11 + a 01 (a 00 + a 10 ) 1 β -k .</formula><p>Similar to the proof of Theorem 2, we have m ∼ Ω( ln(δ -1 ) γ 2</p><p>). From Equation (10), we observe that = ATE a (q) -ATE a (q ) ∼ Ω(γ). Combining above, we have m ∼ Ω( ln(δ -1 )</p><p>2</p><p>). In the case above, ≈ a11 a11+a01 (a 00 + a 10 ) 1 β γ, thus, the number of deconfounded samples needed is approximately m ∝ ln(δ -1 )a 2 11 (a 00 + a 10 ) 2 2 (a 11 + a 01 ) 2</p><formula xml:id="formula_74">1 β -k 2 . Let C 1 ∝ (kβ -1) 2 ln(δ -1 ) -2 . Then m ∼ Ω C1 β 2 a 2 11 (a00+a 2 10 ) (a11+a01) 2 .</formula><p>If we flip the values of q z 01 and q z 11 with the values of q z 00 and q z 10 in both q and q , then we have m ∼ C1 β 2 a 2 10 (a01+a11) 2 (a10+a00) 2 . Note that this is because that the estimation error on ATE and 1 -ATE is symmetric. In addition, under natural selection policy, we need at least m a11 samples; uniform selection policy, we need at least 4m deconfounded samples; under outcome-weighted selection policy, we need at least 2 a11+a01 a11 m deconfounded samples. Combining all of the above, we obtained Theorem 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Finite Confounded Data</head><p>In this case, deconfounding reveals the value of Z for one (initially confounded) sample, and thus we gain no additional information about P Y,T . Thus, these n confounded data provide us with an estimate of the confounded distribution, PY,T (y, t), which we denote âyt , and thus provide us an estimated OWSP. Similarly, we estimate âyt using the MLE from the confounded data. To check the robustness of OWSP, we extend our analysis to handle finite confounded data. With x yt defined as in Section 3.2, we can derive a theorem analogous to Theorems 1-5: </p><formula xml:id="formula_75">(q z yt ) 2 n = min y,t,z   P T,Z (t, z) 2 1 xytm + (q z yt ) 2 n   ≥ 4C.<label>(12)</label></formula><p>The proof of Theorem 8 (Appendix C.1) requires a bound we derive (Appendix, Lemma 5) for the product of two independent random variables. A few results follow from Theorem 8. First, a quick calculation shows that when m is held constant, P (|ATE -ATE| ≥ ) remains positive as n → ∞. This means that for a certain combinations of , δ, n, there does not necessarily exist a sufficiently large m s.t. P (|ATE -ATE| ≥ ) ≤ δ can be satisfied. However, when there exists such an m, then m ≥ max y,t,z</p><p>x</p><formula xml:id="formula_76">-1 yt P T,Z (t, z) 2 4C - (q z yt ) 2 n -1</formula><p>.</p><p>Although Theorem 8 does not recover Theorems 3 and 5 exactly when n → ∞,<ref type="foot" target="#foot_0">foot_0</ref> it provides us with insights into relative performance of our sampling policies. Theorem 8 implies that when n (q z yt )<ref type="foot" target="#foot_1">foot_1</ref> x yt m ∀y, t, the majority of the estimation error comes from not deconfounding enough data. This is because when the number of confounded data that we have is more than Ω(m), the error on the ATE in Equation ( <ref type="formula" target="#formula_75">12</ref>) is dominated by fact that we have not deconfounded enough data. To put it another way, for a given m, having n = Ω(m) confounded samples is sufficient. </p><formula xml:id="formula_77">(q z yt ) 2 n = min y,t,z   P T,Z (t, z) 2 1 xytm + (q z yt ) 2 n   ≥ 4C. (<label>12</label></formula><formula xml:id="formula_78">)</formula><p>Proof of Theorem 8. In this theorem, we derive the concentration for the ATE under finite confounded data. The difference between Theorem 5 and Theorem 8 is that now we need to estimate a yt in addition to q z yt . Thus, to decompose |a yt q z yt -âyt qz yt |, we first derive Lemma 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1.1 Lemma 5</head><p>Lemma 5 (Sample complexity for two independent r.v.s with two independent sampling processes). Let X 1 , ..., X n and Y 1 , ..., Y m be two sequences of Bernoulli random variables independently drawn from distribution p 1 and p 2 , respectively. Let S</p><formula xml:id="formula_79">X = n i=1 X i , S Y = m i=1 Y i . Then, P S X S Y -E [S X ] E [S Y ] ≥ nmt ≤ 2 exp -2t 2 1 m + p 2 2 n .</formula><p>Proof of Lemma 5. The proof follows the proof of Hoeffding's inequality:</p><formula xml:id="formula_80">P S X S Y -E[S X ]E[S Y ] ≥ nmt = P exp(aS X S Y -aE[S X ]E[S Y ])) ≥ exp(anmt) (13) ≤ exp(-anmt)E [exp(aS X S Y -aE[S X ]E[S Y ]))] , (because of Markov's inequality) (14) = exp(-anmt)E [exp(aS X (S Y -E[S Y ]) + aE[S Y ](S X -E[S X ])] ≤ exp(-anmt)E [exp(a max(S X )(S Y -E[S Y ]) + aE[S Y ](S X -E[S X ]))] (because S X ≥ 0) (15) = exp(-anmt)E [exp(an(S Y -E[S Y ]) + aE[S Y ](S X -E[S X ]))] = exp (-anmt) E [exp (an(S Y -E[S Y ]))] E [exp(aE[S Y ](S X -E[S X ]))] (becauseX |= Y ) (16) = exp(-anmt) m i=1 n j=1 E [exp(an(Y i -E[Y i ]))] E [exp(aE[S Y ](X j -E[X j ]))] ≤ exp(-anmt) m i=1 exp a 2 8 n 2 n j=1 exp a 2 8 E[S Y ] 2 (17) = exp -anmt + a 2 8 mn 2 + a 2 8 nm 2 p 2 2 (because the minimum is achieved at a = 4t n + mp 2 2 ) (18) ≤ exp - 2mnt 2 n + mp 2 2 = exp - 2t 2 1 m + p 2 2 n . Line (17) is because Y i -E[Y i ] ∈ {-E[Y i ], 1 -E[Y i ]), and thus n(Y i -E(Y i )) ∈ [-nE[Y i ], n(1 -E[Y i ])]. Furthermore, E[S Y ](X i -E[X i ]) ∈ (-E[X]E[S Y ], (1 -E[X])E[S Y ]).</formula><p>Finally, applying Hoeffding's Lemma (Lemma 1), we obtain line ( <ref type="formula">17</ref>).</p><p>Now we are ready to prove Theorem 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1.2 Proof of Theorem 8</head><p>In this theorem, we assume that the number of confounded data is finite. Thus, instead of a yt , we have estimates of them, namely âyt . Let n yt denote the number of samples in the confounded data such that (Y = y, T = t).</p><p>Let m z yt be the number of samples in the deconfounded data such that (Y = y, T = t, Z = z). Furthermore, let n = y,t n yt , m = y,t,z m z yt . Then, under our setup, we estimate a yt and q z yt as follows: |a 1t q z 1t + a 0t q z 0t -â1t qz 1t -â0t qz 0t | &lt; y a yt q z yt 5k .</p><p>Notice that |a 1t q z 1t + a 0t q z 0t -â1t qz 1t -â0t qz 0t | &lt; y aytq z yt 5k is satisfied when both |a 1t q z 1t -â1t qz 1t | &lt; y a yt q z yt 10k , and |a 0t q z 0t -â0t qz 0t | &lt; y a yt q z yt 10k .</p><p>We have: </p><formula xml:id="formula_81">P | ATE -</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Corresponding Stories</head><p>In this section, we will provide an example for each selection method such that this particular sampling performs the worst when compared with the other two methods. For the purpose of illustration, we consider binary confounder throughout this section. To ease notation, let q yt denote q 1 yt .</p><p>A Scenario in Which NSP Performs the Worst A drug repositioning start-up discovered that drug T can potentially cure a disease γ. which has no known drug cure and goes away without treatments once a while. Since drug T is commonly used to treat another disease η, the majority patients who has disease γ do not receive any treatment. Among the ones who received drug T , the start-up discovered that the health outcomes of the majority of patients have improved. The start-up proposes to bring drug T to an observational study to verify whether drug T could treat disease γ while not controlling for patient's treatment adherence levels. As in most cases, patient's treatment adherence levels could influence doctors' decision of whether to prescribe drug T and whether the treatment for disease γ will be successful. Translating this scenario into our notations, we have a 01 = 1 , a 10 = 2 , a 11 = 3 , and a 00 = 1 -3 i=1 i , say a = (0.9, 0.02, 0.01, 0.07). Now, imagine in the clinical trial, the patients are given a drug case containing drug T such that the drug case automatically records the frequency that the patient takes the drug. Somehow we know a priori that the patients who do not have health improvement have on average poor treatment adherence, e.g., q 00 = 0.9, q 01 = 0.7; furthermore, those who have health improvement on average have good treatment adherence, e.g., q 10 = 0.01, q 11 = 0.3. Deconfounding according to NSP, i.e., x = (a 00 , a 01 , a 10 , a 11 ), in this case, will select most samples from the group (Y = 0, T = 0). Since the ATE depends on the estimation that relies on both T = 0, and T = 1, one would expect that NSP and OWSP will outperform NSP. The left column in Figure <ref type="figure" target="#fig_2">3</ref> confirms this hypothesis.</p><p>A Scenario in Which USP Performs the Worst A group biostatisticians discovered that mutations on gene T is likely to cause cancer Y in patients with a particular type of heart disease. In particular, they discovered that among the those heart disease patients, 79% of patients have neither mutation on T nor cancer Y ; 18% patients have both mutation on T and cancer Y . In other words, a 00 = 0.79, a 11 = 0.18. Furthermore, we have a 01 = 0.01, a 10 = 0.02. This group of biostatisticians want to run a small experiment to confirm whether gene T causes cancer Y . In particular, they are interested in knowing whether those patients also have mutations on gene Z, which is also suspected by the same group of biostatisticians to cause cancer Y . Somehow, we know a priori that q 00 = 0.5, q 01 = 0.01, q 10 = 0.05, q 11 = 0.5. From the calculation of the ATE, it is not difficult to observe that the error on the ATE is dominated by the estimation errors on q 00 , q 11 . Thus, we should sample more from the groups (Y = 0, T = 0) and (Y = 1, T = 1).</p><p>A Scenario in Which OWSP Performs the Worst A team wants to reposition drug T to cure diabetes. Drug T has been used to treat a common comorbid condition of diabetes that appears in 31% of the diabetic patient population. Among those patients who receive drug T , about 97% has improved health, that is a 01 = 0.01 and a 11 = 0.3. Among the patients who have never received drug T , about 70% have no health improvement, that is a 00 = 0.5, and a 10 = 0.19. Let q 00 = 0.05, q 01 = 0.5, q 10 = 0.055, and q 11 = 0.4. In the ATE, it is easy to observe that a11q11 a11q11+a01q01 and a11(1-q11) a11(1-q11)+a01(1-q01) are both dominated by 1 regardless of the estimates of q 11 and q 01 . In this case, USP outperforms OWSP and NSP when the sample size is larger than 200. On the other hand, the bottom figure in the third column of Figure <ref type="figure" target="#fig_2">3</ref> shows that, when averaged over all possible values of q, OWSP performs the best.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Approximate Sampling Policies Under Finite Confounded Data</head><p>To deconfound according to NSP with finite confounded data is to deconfound the first m confounded data. For USP, we split the samples to the 4 groups as evenly as possible. That is, we max out the bottleneck group/groups and distribute the excess data as evenly as possible among the remaining groups.</p><p>For OWSP, we have x yt = âyt y âyt , and when implementing OWSP, we will first ensure that the deconfounded samples are split as evenly as possible across treatment groups, and then within the each group, we split the samples close as possible to the outcome ratio.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Causal graph with treatment T , outcome Y , and selectively observed confounder Z</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: Performance of the four policies over 13,000 distributions P Y,T,Z , given infinite confounded data. Left and Middle: averaged error over 13,000 distributions for varying numbers of deconfounded samples. Right: error comparison (each point is a single distribution averaged over 100 replications) for 1,200 deconfounded samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure3: Comparison of selection policies for adversarially chosen instances. Top (left) NSP performs the worst: a = (0.9, 0.02, 0.01, 0.07) and q = (0.9, 0.7, 0.01, 0.3). Top (middle) USP performs the worst: a = (0.79, 0.01, 0.02, 0.18) and q = (0.5, 0.01, 0.05, 0.5). Top (right) OWSP performs the worst: a = (0.5, 0.01, 0.19, 0.3) and q = (0.05, 0.5, 0.055, 0.4). Bottom: the same a's but averaged over 500 q's drawn uniformly from [0, 1] 4 .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>FigureFigure 4 :</head><label>4</label><figDesc>Figure 4: Experiment on finite confounded data over 13,000 distributions P Y,T,Z , each averaged over 100 replications. The number of deconfounded samples is fixed at 100. Left: averaged over the 13,000 distributions. Middle and Right: error comparison at 681 confounded samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>order to upper bound ATE -ATE by some , it suffices to show that</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>u z , ûz ∈ [-1, 1], and v z , vz ∈ [0, 1].Lemma 2 implies that the upper bound in (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Theorem 8. (Upper Bound) Given n confounded and m deconfounded samples, with n ≥ m, P (|ATE -ATE| ≥ ) ≤ δ is satisfied when min</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>C. 1</head><label>1</label><figDesc>Proof of Theorem 8 Theorem 8. (Upper Bound) Given n confounded and m deconfounded samples, with n ≥ m, P (|ATE -ATE| ≥ ) ≤ δ is satisfied when min</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Performance of the four sampling policies on the COSMIC dataset assuming infinite confounded data. 275 unique (cancer, mutation, mutation) combinations were extracted. Left: averaged over 275 instances, and each averaged over 10,000 replications. Middle and Right: error comparison at 45 deconfounded samples.</figDesc><table><row><cell>Average Absolute Error</cell><cell>0.01 0.02 0.03 0.04</cell><cell>Deconf only Conf + NSP Conf + USP Conf + OWSP</cell><cell>Average Error of OWSP</cell><cell>0.200 0.025 0.050 0.075 0.100 0.125 0.150 0.175</cell><cell></cell><cell></cell><cell>Average Error of OWSP</cell><cell>0.06 0.01 0.02 0.03 0.04 0.05</cell></row><row><cell></cell><cell>0.00</cell><cell>Number of Deconfounded Samples</cell><cell></cell><cell>0.000</cell><cell>0.00</cell><cell>0.05 Average Error of NSP 0.10 0.15</cell><cell>0.20</cell><cell>0.00</cell><cell>Average Error of USP 0.00 0.01 0.02 0.03 0.04 0.05 0.06</cell></row><row><cell cols="3">Figure 5:</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>Mark J Van der Laan and Sherri Rose. Targeted learning: causal inference for observational and experimental data. Springer Science &amp; Business Media, 2011.</figDesc><table><row><cell>Roman Vershynin. High-dimensional probability: An</cell></row><row><cell>introduction with applications in data science, vol-</cell></row><row><cell>ume 47. Cambridge university press, 2018.</cell></row><row><cell>Stefan Wager and Susan Athey. Estimation and infer-</cell></row><row><cell>ence of heterogeneous treatment effects using random</cell></row><row><cell>forests. Journal of the American Statistical Associa-</cell></row><row><cell>tion, 113(523):1228-1242, 2018.</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>We could apply Lemma</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>(Appendix B)  to obtain a bound that recovers Theorems</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>and 5 exactly as n → ∞. However, this method does not give us sufficient insights into the comparative performance of our sampling policies.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We wish to thank <rs type="person">Sivaraman Balakrishnan</rs> and <rs type="person">Uri Shalit</rs> for their valuable feedback. We would also like to thank <rs type="person">Amazon AI</rs>, <rs type="funder">Facebook</rs>, <rs type="funder">Salesforce</rs>, the <rs type="funder">NSF</rs>, <rs type="institution">UPMC</rs>, the <rs type="institution">PwC Center, and DARPA</rs> for their support of our research.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Bayesian inference of individualized treatment effects using multi-task gaussian processes</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alaa</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Mihaela</forename><surname>Van Der Schaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3424" to="3432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A general algorithm for deciding transportability of experimental results</title>
		<author>
			<persName><forename type="first">Elias</forename><surname>Bareinboim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Causal Inference</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="107" to="134" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Bates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matteo</forename><surname>Sesia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chiara</forename><surname>Sabatti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emmanuel</forename><surname>Candes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.09644</idno>
		<title level="m">Causal inference in genetic trio studies</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Cosmic -catalogue of somatic mutations in cancer</title>
		<ptr target="https://cancer.sanger.ac.uk/" />
		<imprint>
			<date type="published" when="2019-09">Sep 2019</date>
		</imprint>
	</monogr>
	<note>Cosmic</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Propensity scorematching methods for nonexperimental causal studies</title>
		<author>
			<persName><forename type="first">H</forename><surname>Rajeev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sadek</forename><surname>Dehejia</surname></persName>
		</author>
		<author>
			<persName><surname>Wahba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Review of Economics and Statistics</title>
		<imprint>
			<biblScope unit="volume">84</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="151" to="161" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">From sample average treatment effect to population average treatment effect on the treated: combining experimental with observational studies to estimate population treatment effects</title>
		<author>
			<persName><forename type="first">Erin</forename><surname>Hartman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Grieve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roland</forename><surname>Ramsahai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jasjeet</forename><forename type="middle">S</forename><surname>Sekhon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series A (Statistics in Society)</title>
		<imprint>
			<biblScope unit="volume">178</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="757" to="778" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Efficient estimation of average treatment effects using the estimated propensity score</title>
		<author>
			<persName><forename type="first">Keisuke</forename><surname>Hirano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guido</forename><forename type="middle">W</forename><surname>Imbens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geert</forename><surname>Ridder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econometrica</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1161" to="1189" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Journal of the American statistical Association</title>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">W</forename><surname>Holland</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1986">1986</date>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page" from="945" to="960" />
		</imprint>
	</monogr>
	<note>Statistics and causal inference</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Removing hidden confounding by experimental grounding</title>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Kallus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aahlad</forename><surname>Manas Puli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uri</forename><surname>Shalit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="10888" to="10897" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Two genetic hits (more or less) to cancer</title>
		<author>
			<persName><surname>Alfred G Knudson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Cancer</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="157" to="162" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Measurement bias and effect restoration in causal inference</title>
		<author>
			<persName><forename type="first">Manabu</forename><surname>Kuroki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="423" to="437" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Causal effect inference with deep latent-variable models</title>
		<author>
			<persName><forename type="first">Christos</forename><surname>Louizos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uri</forename><surname>Shalit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Joris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Mooij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6446" to="6456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Propensity score estimation with boosted regression for evaluating causal effects in observational studies</title>
		<author>
			<persName><forename type="first">Greg</forename><surname>Daniel F Mccaffrey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">R</forename><surname>Ridgeway</surname></persName>
		</author>
		<author>
			<persName><surname>Morral</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Methods</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">403</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Identifying causal effects with proxy variables of an unmeasured confounder</title>
		<author>
			<persName><forename type="first">Wang</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric J Tchetgen</forename><surname>Tchetgen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="987" to="993" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Sur les applications de la théorie des probabilités aux experiences agricoles: Essai des principes</title>
		<author>
			<persName><forename type="first">Jersey</forename><surname>Neyman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Roczniki Nauk Rolniczych</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1" to="51" />
			<date type="published" when="1923">1923</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Causal diagrams for empirical research</title>
		<author>
			<persName><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="669" to="688" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Causality: models, reasoning and inference</title>
		<author>
			<persName><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>Springer</publisher>
			<biblScope unit="volume">29</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A panoply of errors: polymerase proofreading domain mutations in cancer</title>
		<author>
			<persName><forename type="first">Emily</forename><surname>Rayner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inge</forename><forename type="middle">C</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claire</forename><surname>Palles</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">E</forename><surname>Kearsey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tjalling</forename><surname>Bosse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Tomlinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">N</forename><surname>Church</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature Reviews Cancer</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The central role of the propensity score in observational studies for causal effects</title>
		<author>
			<persName><forename type="first">R</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><forename type="middle">B</forename><surname>Rosenbaum</surname></persName>
		</author>
		<author>
			<persName><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="41" to="55" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Estimating causal effects of treatments in randomized and nonrandomized studies</title>
		<author>
			<persName><surname>Donald B Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Educational Psychology</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">688</biblScope>
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Estimating individual treatment effect: generalization bounds and algorithms</title>
		<author>
			<persName><forename type="first">Uri</forename><surname>Shalit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Fredrik D Johansson</surname></persName>
		</author>
		<author>
			<persName><surname>Sontag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3076" to="3085" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Multiply robust causal inference with double-negative control adjustment for categorical unmeasured confounding</title>
		<author>
			<persName><forename type="first">Xu</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wang</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jennifer</forename><forename type="middle">C</forename><surname>Nelson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric J Tchetgen</forename><surname>Tchetgen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Statistical Methodology)</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="521" to="540" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The use of propensity scores to assess the generalizability of results from randomized trials</title>
		<author>
			<persName><forename type="first">Elizabeth</forename><forename type="middle">A</forename><surname>Stuart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Catherine</forename><forename type="middle">P</forename><surname>Stephen R Cole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">J</forename><surname>Bradshaw</surname></persName>
		</author>
		<author>
			<persName><surname>Leaf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series A (Statistics in Society)</title>
		<imprint>
			<biblScope unit="volume">174</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="369" to="386" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Cosmic: the catalogue of somatic mutations in cancer</title>
		<author>
			<persName><forename type="first">Sally</forename><surname>John G Tate</surname></persName>
		</author>
		<author>
			<persName><surname>Bamford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Harry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zbyslaw</forename><surname>Jubb</surname></persName>
		</author>
		<author>
			<persName><surname>Sondka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nidhi</forename><surname>David M Beare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harry</forename><surname>Bindal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charlotte</forename><forename type="middle">G</forename><surname>Boutselakis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Celestino</forename><surname>Cole</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elisabeth</forename><surname>Creatore</surname></persName>
		</author>
		<author>
			<persName><surname>Dawson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nucleic Acids Research</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">D1</biblScope>
			<biblScope unit="page" from="941" to="D947" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
