<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Graph Infomax Adversarial Learning for Treatment Effect Estimation with Networked Observational Data</title>
				<funder ref="#_MZN9aQZ">
					<orgName type="full">U.S. Army Research Office Award</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2021-06-05">5 Jun 2021</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhixuan</forename><surname>Chu</surname></persName>
							<email>zhixuan.chu@uga.edu</email>
						</author>
						<author>
							<persName><forename type="first">Stephen</forename><forename type="middle">L</forename><surname>Rathbun</surname></persName>
							<email>rathbun@uga.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">University of Georgia Athens</orgName>
								<address>
									<country key="GE">Georgia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Sheng Li</orgName>
								<orgName type="institution">University of Georgia Athens</orgName>
								<address>
									<country key="GE">Georgia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">University of Georgia Athens</orgName>
								<address>
									<country key="GE">Georgia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Graph Infomax Adversarial Learning for Treatment Effect Estimation with Networked Observational Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2021-06-05">5 Jun 2021</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2106.02881v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Treatment effect estimation from observational data is a critical research topic across many domains. The foremost challenge in treatment effect estimation is how to capture hidden confounders. Recently, the growing availability of networked observational data offers a new opportunity to deal with the issue of hidden confounders. Unlike networked data in traditional graph learning tasks, such as node classification and link detection, the networked data under the causal inference problem has its particularity, i.e., imbalanced network structure. In this paper, we propose a Graph Infomax Adversarial Learning (GIAL) model for treatment effect estimation, which makes full use of the network structure to capture more information by recognizing the imbalance in network structure. We evaluate the performance of our GIAL model on two benchmark datasets, and the results demonstrate superiority over the state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>A further understanding of causality beyond observational data is critical across many domains including statistics, computer science, education, public policy, economics, and health care. Although randomized controlled trials (RCT) are usually considered as the gold standard for causal inference, estimating causal effects from observational data has received growing attention owing to the increasing availability of data and the low costs compared to RCT.</p><p>When estimating treatment effects from observational data, we face two major issues, i.e., missing counterfactual outcomes and treatment selection bias. The foremost challenge for solving these two issues is the existence of confounders, which are the variables that affect both treatment assignment and outcome. Unlike RCT, treatments are typically not assigned at random in observational data. Due to the confounders, subjects would have a preference for a certain treatment option, which leads to a bias of the distribution for the confounders among different treatment options. This phenomenon exacerbates the difficulty of counterfactual outcome estimation. For most of existing methods <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref>, the strong ignorability assumption is the most important prerequisite. It assumes given covariates, the treatment assignment is independent of the potential outcomes and for any value of covariates, treatment assignment is not deterministic. Strong ignorability is also known as the no unmeasured confounders assumption. This assumption requires that all the confounders be observed and sufficient to characterize the treatment assignment mechanism. Moreover, strong Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD <ref type="bibr">'21)</ref>, August 14-18, 2021, Virtual Event, Singapore. ignorability is a sufficient condition for the individual treatment effect (ITE) function to be identifiable <ref type="bibr" target="#b10">[11]</ref>.</p><p>However, due to the fact that identifying all of the confounders is impossible in practice, the strong ignorability assumption is usually untenable. By leveraging big data, it becomes possible to find a proxy for the hidden confounders. Network information, which serves as an efficient structured representation of non-regular data, is ubiquitous in the real world. Advanced by the powerful representation capabilities of various graph neural networks, networked data has recently received increasing attention <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22]</ref>. Besides, it can be used to help recognize the patterns of hidden confounders. A network deconfounder <ref type="bibr" target="#b5">[6]</ref> is proposed to recognize hidden confounders by combining the graph convolutional networks <ref type="bibr" target="#b13">[14]</ref> and counterfactual regression <ref type="bibr" target="#b18">[19]</ref>.</p><p>The networked observational data consists of two components, node features and network structures. Due to the confounding bias in causal inference problem, the imbalance not only exists in distributions of feature variables in treatment and control groups but also in network structures. For example, in social networks, the links are more likely to appear among more similar people, so the subjects are more likely to follow other subjects in the same group as shown in Fig. <ref type="figure" target="#fig_0">1</ref>, which will aggravate the imbalance in the representation space learned by graph neural networks. Fig. <ref type="figure">2</ref> shows the existence of imbalanced network structures in the benchmarks of causal inference with networked data (BlogCatalog and Flickr). Unlike the networked data in traditional graph learning tasks, such as node classification and link detection, the networked data under the causal inference problem has its particularity, i.e., imbalanced network structure. For most existing work on networked observational data, they did not consider this peculiarity of graph structure under causal inference settings. Directly applying graph neural networks designed for traditional graph learning tasks cannot capture all of the information from imbalanced networked data.</p><p>To fully exploit the information in the networked data with the imbalanced network structure, we propose a Graph Infomax Adversarial Learning method (GIAL) to estimate the treatment effects from networked observational data. In our model, structure mutual information is maximized to help graph neural networks to extract Figure <ref type="figure">2</ref>: Under the assumption that each node has the same possibility to be connected with another node regardless of node's treatment assignment, for ğ‘› nodes, there should be ğ‘› 2 4 -ğ‘› 2 homogeneous edges (that link the nodes in the same group, i.e., treatment-treatment or control-control) and ğ‘› 2 4 heterogeneous edges (that link the nodes in different groups, i.e., treatmentcontrol). The number of heterogeneous edges should be greater than that of homogeneous edges. However, in the benchmarks of causal inference with networked data (BlogCatalog and Flickr), the homogeneous edges are consistently greater than heterogeneous edges for both datasets. Besides, as the selection bias increases, the difference between homogeneous and heterogeneous edges gets larger. This result totally agrees with our expectation that, in the causal inference problem, the network structure is imbalanced. The relationship is more likely to appear among people who are in the same group. a representation space, which best represents observed and hidden confounders from the networked data with the imbalanced structure. Also, adversarial learning is applied to balance the learned representation distributions of treatment and control groups and to generate the potential outcomes for each unit across two groups. Overall, GIAL can make full use of network structure to recognize patterns of hidden confounders, which has been validated by extensive experiments on benchmark datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">BACKGROUND</head><p>Suppose that the observational data contain ğ‘› units and each unit received one of two or more treatments. Let ğ‘¡ ğ‘– denote the treatment assignment for unit ğ‘–; ğ‘– = 1, ..., ğ‘›. For binary treatments, ğ‘¡ ğ‘– = 1 is for the treatment group, and ğ‘¡ ğ‘– = 0 for the control group. The outcome for unit ğ‘– is denoted by ğ‘Œ ğ‘– ğ‘¡ when treatment ğ‘¡ is applied to unit ğ‘–; that is, ğ‘Œ ğ‘– 1 is the potential outcome of unit ğ‘– in the treatment group and ğ‘Œ ğ‘– 0 is the potential outcome of unit ğ‘– in the control group. For observational data, only one of the potential outcomes is observed according to the actual treatment assignment of unit ğ‘–. The observed outcome is called the factual outcome, and the remaining unobserved potential outcomes are called counterfactual outcomes. Let ğ‘‹ âˆˆ R ğ‘‘ denote all observed variables of a unit.</p><p>Let G(V, E) denote an undirected graph, where V represents ğ‘› nodes in G and E is a set of edges between nodes. According to the adjacency relationships in E, the corresponding adjacent matrix ğ´ âˆˆ R ğ‘›Ã—ğ‘› of the graph G can be defined as follows. If (ğ‘£ ğ‘– , ğ‘£ ğ‘— ) âˆˆ E, ğ´ ğ‘– ğ‘— = 1, otherwise ğ´ ğ‘– ğ‘— = 0. When edges have different weights, ğ´ ğ‘– ğ‘— can be assigned to a real value.</p><p>In this paper, we explore the observational data as networks. In particular, the graph G is the networked observational data. Every node in V is one unit in observational data, an edge in V describes the relationship between a pair of units, and adjacent matrix ğ´ represents the whole network structure. Therefore, the observational data can be denoted as ({ğ‘¥ ğ‘– , ğ‘¡ ğ‘– , ğ‘¦ ğ‘– } ğ‘› ğ‘–=1 , ğ´). We follow the potential outcome framework for estimating treatment effects <ref type="bibr" target="#b17">[18]</ref>. The individual treatment effect (ITE) for unit ğ‘– is the difference between the potential treated and control outcomes, which is defined as: ITE ğ‘– = ğ‘Œ ğ‘– 1 -ğ‘Œ ğ‘– 0 , (ğ‘– = 1, ..., ğ‘›). The average treatment effect (ATE) is the difference between the mean potential treated and control outcomes, which is defined as ATE = 1 ğ‘› ğ‘› ğ‘–=1 (ğ‘Œ ğ‘– 1 -ğ‘Œ ğ‘– 0 ), (ğ‘– = 1, ..., ğ‘›). The success of the potential outcome framework is based on the strong ignorability assumption, which ensures that the treatment effect can be identified <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b24">25]</ref>. Assumption 2.1. Strong Ignorability: Given covariates ğ‘‹ , treatment assignment ğ‘‡ is independent of the potential outcomes, i.e., (ğ‘Œ 1 , ğ‘Œ 0 ) âŠ¥ âŠ¥ ğ‘‡ |ğ‘‹ and for any value of ğ‘‹ , treatment assignment is not deterministic, i.e.,ğ‘ƒ (ğ‘‡ = ğ‘¡ |ğ‘‹ = ğ‘¥) &gt; 0, for all ğ‘¡ and ğ‘¥.</p><p>In our model, we relax the strong ignorability and allow the existence of hidden confounders. We aim to use network structure information to recognize the hidden confounders and then estimate treatment effects based on the learned confounder representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">THE PROPOSED FRAMEWORK 3.1 Motivation</head><p>The foremost challenge of causal inference from observational data is how to recognize hidden confounders. Recently, leveraging the powerful representation capabilities of various graph neural networks, network structures can be utilized to help recognize the patterns of hidden confounders in networked observational data.</p><p>Due to the particularity of the causal inference problem, the networked data in causal inference is different from that in traditional graph learning tasks such as node classification and link detection. As network information is incorporated into the model, we face a new imbalance issue,i.e., imbalance of network structure in addition to the imbalance of observed covariate distributions. A link has a larger probability of appearing between two more similar people. It implies that one unit is more likely to be connected to other units in the same group. Therefore, directly applying traditional graph learning methods to learn the representation of networked data could not fully exploit the useful information for causal inference.</p><p>It is essential to design a new method that can capture the representation of hidden confounders implied from the imbalanced network structure and observed confounders that exist in the covariates simultaneously. To solve this problem, we propose the Graph Infomax Adversarial Learning method (GIAL) to estimate the treatment effects from the networked observational data, which can recognize patterns of hidden confounders from imbalanced network structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model Architecture</head><p>As shown in Fig. <ref type="figure" target="#fig_1">3</ref>, our GIAL consists of four main components, i.e., confounder representation learning, structure mutual information maximization, potential outcome generator, and counterfactual outcome discriminator. Firstly, we utilize the graph neural network and structure mutual information to learn the representations of hidden confounders and observed confounders, by mapping the feature covariates and network structure simultaneously into a representation space. Then the potential outcome generator is applied to infer the potential outcomes of units across treatment and control groups based on the learned representation space and treatment assignment. At the same time, the counterfactual outcome discriminator is incorporated to remove the imbalance in the learned representations of treatment and control groups, and thus it improves the prediction accuracy of potential outcomes inferred in the outcome generator by playing a minimax game. In the following, we present the details of each component.</p><p>Confounder Representation Learning. Based on the graph G(V, E), our goal is to learn the representation of confounders by a function ğ‘” : ğ‘‹ Ã— ğ´ â†’ ğ‘…, ğ‘… âˆˆ R ğ‘‘ , which is parameterized by a graph neural network. To better capture information resided in the networked data, we separately adopt two powerful graph neural network methods, i.e., the graph convolutional network (GCN) <ref type="bibr" target="#b13">[14]</ref> and graph attention network layers (GAT) <ref type="bibr" target="#b20">[21]</ref>, to learn the representation space. For these two models, their effectiveness of the learned representations has been verified in various graph learning tasks. The major difference between GCN and GAT is how the information from the one-hop neighborhood is aggregated. For GCN, a graph convolution operation is used to produce the normalized sum of the node features of neighbors. GAT introduces the attention mechanism to better quantify the importance of each edge. Here, we want to find out which model is better to unravel patterns of hidden confounders from the networked data with imbalanced covariate and imbalanced network structure.</p><p>For the graph convolutional network (GCN) model, the representation learning function ğ‘” : ğ‘‹ Ã— ğ´ â†’ ğ‘… is parameterized with the following layer-wise propagation rule:</p><formula xml:id="formula_0">ğ‘Ÿ (ğ‘™+1) = ğœ ( D-1 2 Ãƒ D-1 2 ğ‘Ÿ (ğ‘™) ğ‘Š (ğ‘™) ),<label>(1)</label></formula><p>where Ãƒ = ğ´ + ğ¼ ğ‘› is the adjacency matrix of graph G(V, E) with inserted self-loops, i.e., the identity matrix ğ¼ ğ‘› . D is its corresponding degree matrix, i.e., Dğ‘–ğ‘– = ğ‘— Ãƒğ‘– ğ‘— and ğ‘Š (ğ‘™) is a layer-specific trainable weight matrix. ğœ (â€¢) denotes an activation function and here we apply the parametric ReLU (PReLU) function <ref type="bibr" target="#b6">[7]</ref>. A number of GCN layers can be stacked to approximate the function ğ‘” : ğ‘‹ Ã— ğ´ â†’ ğ‘….</p><p>For the graph attention network (GAT) model, the representation of confounder for the ğ‘–-th node is a function of its covariates and receptive field. Here, we define the ğ‘–-th node ğ‘£ ğ‘– and its one-hop neighbor nodes as the receptive field N (ğ‘£ ğ‘– ). The representation learning function ğ‘” : ğ‘‹ Ã—ğ´ â†’ ğ‘… is parameterized with the following equation:</p><formula xml:id="formula_1">ğ‘Ÿ (ğ‘™+1) ğ‘– = ğœ âˆ‘ï¸ ğ‘— âˆˆN (ğ‘–) ğ›¼ (ğ‘™) ğ‘– ğ‘— ğ‘Š (ğ‘™) ğ‘Ÿ (ğ‘™) ğ‘— ,<label>(2)</label></formula><p>where ğ‘Š (ğ‘™) is the learnable weight matrix and</p><formula xml:id="formula_2">ğ‘Š (ğ‘™) ğ‘Ÿ (ğ‘™)</formula><p>ğ‘— is a linear transformation of the lower layer representation ğ‘Ÿ (ğ‘™) ğ‘— . ğœ (â€¢) is the activation function for nonlinearity. In Eq. ( <ref type="formula" target="#formula_1">2</ref>), the representation of the ğ‘–-th node and its neighbors are aggregated together, scaled by the normalized attention scores ğ›¼</p><formula xml:id="formula_3">(ğ‘™) ğ‘– ğ‘— . ğ›¼ (ğ‘™) ğ‘– ğ‘— = exp(LeakyReLU(ğ‘ (ğ‘™) ğ‘‡ (ğ‘Š (ğ‘™) ğ‘Ÿ (ğ‘™) ğ‘– ||ğ‘Š (ğ‘™) ğ‘Ÿ (ğ‘™) ğ‘— ))) ğ‘˜ âˆˆN (ğ‘–) exp(LeakyReLU(ğ‘ (ğ‘™) ğ‘‡ (ğ‘Š (ğ‘™) ğ‘Ÿ (ğ‘™) ğ‘– ||ğ‘Š (ğ‘™) ğ‘Ÿ (ğ‘™) ğ‘˜ ))) ,<label>(3)</label></formula><p>where softmax is used to normalize the attention scores on each node's incoming edges. The pair-wise attention score between two neighbors is calculated by LeakyReLU(ğ‘ (ğ‘™) ğ‘‡ (ğ‘Š (ğ‘™) </p><formula xml:id="formula_4">ğ‘Ÿ (ğ‘™) ğ‘– ||ğ‘Š (ğ‘™) ğ‘Ÿ (ğ‘™) ğ‘— )).</formula><p>Here, it first concatenates the linear transformation of the lower layer representations for two nodes, i.e., ğ‘Š (ğ‘™) ğ‘Ÿ</p><formula xml:id="formula_5">(ğ‘™) ğ‘– ||ğ‘Š (ğ‘™) ğ‘Ÿ (ğ‘™)</formula><p>ğ‘— , where || denotes concatenation, and then it takes a dot product of itself and a learnable weight vector ğ‘ (ğ‘™) . Finally, the LeakyReLU function is applied.</p><p>To stabilize the learning process, a multi-head attention mechanism is employed. We compute multiple different attention maps and finally aggregate all the learned representations. In particular, ğ¾ independent attention mechanisms execute the transformation of Eq. ( <ref type="formula" target="#formula_1">2</ref>), and then their outputs are merged in two ways:</p><formula xml:id="formula_6">concatenation : ğ‘Ÿ (ğ‘™+1) ğ‘– = || ğ¾ ğ‘˜=1 ğœ âˆ‘ï¸ ğ‘— âˆˆN (ğ‘–) ğ›¼ ğ‘˜ ğ‘– ğ‘— ğ‘Š ğ‘˜ ğ‘Ÿ (ğ‘™) ğ‘—<label>(4)</label></formula><p>or average : â„</p><formula xml:id="formula_7">(ğ‘™+1) ğ‘– = ğœ 1 ğ¾ ğ¾ âˆ‘ï¸ ğ‘˜=1 âˆ‘ï¸ ğ‘— âˆˆN (ğ‘–) ğ›¼ ğ‘˜ ğ‘– ğ‘— ğ‘Š ğ‘˜ â„ (ğ‘™) ğ‘—<label>(5)</label></formula><p>When performing the multi-head attention on the final layer of the network, concatenation is no longer sensible. Thus, we use the concatenation for intermediary layers and the average for the final layer. An arbitrary number of GAT layers can be stacked to approximate the function ğ‘” : ğ‘‹ Ã— ğ´ â†’ ğ‘….</p><p>Structure Mutual Information Maximization. Inspired by a recent successful unsupervised graph learning method <ref type="bibr" target="#b21">[22]</ref>, we maximize structure mutual information to capture the imbalanced graph structure with respect to treatment and control nodes in the networked observational data. We aim to learn representations that Here, our purpose is to learn a representation vector, which can capture the entire graph structure encoded by the graph structure summary vector ğ‘  and also reflect the abnormal imbalance in the graph structure. Therefore, we aim at maximizing the mutual information between the learned representation vector ğ‘Ÿ ğ‘– and the structure summary vector ğ‘ .</p><p>Mutual information is a fundamental quantity for measuring the relationship between random variables. For example, the dependence of two variables ğ‘Š and ğ‘ is quantified by mutual information as <ref type="bibr" target="#b1">[2]</ref>:</p><formula xml:id="formula_8">ğ¼ (ğ‘Š ; ğ‘ ) = âˆ« WÃ—Z log ğ‘‘P ğ‘Š ğ‘ ğ‘‘P ğ‘Š âŠ— P ğ‘ ğ‘‘P ğ‘Š ğ‘ ,<label>(6)</label></formula><p>where P ğ‘Š ğ‘ is the joint probability distribution, and P ğ‘Š = âˆ« W ğ‘‘P ğ‘Š ğ‘ and P ğ‘ = âˆ« Z ğ‘‘P ğ‘Š ğ‘ are the marginals. However, mutual information has historically been difficult to compute. From the viewpoint of Shannon information theory, mutual information can be estimated as Kullback-Leibler divergence:</p><formula xml:id="formula_9">ğ¼ (ğ‘Š ; ğ‘ ) = ğ» (ğ‘Š ) -ğ» (ğ‘Š |ğ‘ ) = ğ· ğ¾ğ¿ (P ğ‘Š ğ‘ ||P ğ‘Š âŠ— P ğ‘ ). (7)</formula><p>Actually, in our model, it is unnecessary to use the exact KLbased formulation of MI, as we only want to maximize the mutual information between representation vector ğ‘Ÿ ğ‘– and structure summary vector ğ‘ . A simple and stable alternative based on the Jensen-Shannon divergence (JSD) can be utilized. Thus, we follow the intuitions from deep infomax <ref type="bibr" target="#b8">[9]</ref> and deep graph infomax <ref type="bibr" target="#b21">[22]</ref> to maximize the mutual information.</p><p>To act as an agent for maximizing the mutual information, one discriminator ğ‘‘ : ğ‘… Ã— ğ‘† â†’ ğ‘ƒ, ğ‘ƒ âˆˆ R is employed. The discriminator is formulated by a simple bilinear scoring function with nonlinear activation: ğ‘‘ (ğ‘Ÿ ğ‘– , ğ‘ ) = ğœ (ğ‘Ÿ ğ‘– ğ‘‡ ğ‘Š ğ‘ ), which estimates the probability of the ğ‘–-th node representation contained within the graph structure summary ğ‘ . ğ‘Š is a learnable scoring matrix.</p><p>To implement the discriminator, we also need to create the negative samples compared with original samples and then use the discriminator to distinguish which one is from positive samples (original networked data) and which one is from the negative samples (created fake networked data), such that the original graph structure information could be correctly captured. The choice of the negative sampling procedure will govern the specific kinds of structural information that is desirable to be captured <ref type="bibr" target="#b21">[22]</ref>. Here, we focus on the imbalance between the edges that link nodes in the same group and those that link nodes in the different groups, i.e., treatment unit to treatment unit, treatment unit to control unit, and control unit to control unit. Therefore, our discriminator is designed to force the representations to capture this imbalanced structure by creating negative samples where the original adjacency matrix ğ´ is preserved, whereas the negative samples X are obtained by the row-wise shuffling of ğ‘‹ . That is, the created fake networked data consists of the same nodes as the original graph, but they are located in different places in the same structure. Thus, the nodes at both ends of the edges may change the treatment choices, e.g., from treatment to control, from control to treatment, or remain unchanged. Then we also conduct the confounder representation learning for the created fake networked data ( X, ğ´) to get the rğ‘– . With the proposed discriminator, we could have ğ‘‘ (ğ‘Ÿ ğ‘– , ğ‘ ) and ğ‘‘ ( rğ‘– , ğ‘ ), which indicate the probabilities of containing the representations of the ğ‘–-th positive sample and negative sample in the graph structure summary, respectively.</p><p>We optimize the discriminator to maximize mutual information between ğ‘Ÿ ğ‘– and ğ‘  based on the Jensen Shannon divergence via a noise-contrastive type objective with a standard binary crossentropy (BCE) loss <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b21">22]</ref>:</p><formula xml:id="formula_10">L ğ‘š = 1 2ğ‘› ğ‘› âˆ‘ï¸ ğ‘–=1 E (ğ‘‹ ,ğ´) [log ğ‘‘ (ğ‘Ÿ ğ‘– , ğ‘ )] + ğ‘› âˆ‘ï¸ ğ‘—=1 E ( X ,ğ´) [log (1 -ğ‘‘ ( rğ‘– , ğ‘ ))] .</formula><p>(8) Potential Outcome Generator. So far, we have learned the representation space of confounders from networked data with the imbalanced network structure and imbalanced covariates. The function Î¨ : ğ‘… Ã—ğ‘‡ â†’ ğ‘Œ maps the representation of hidden confounders and observed confounders as well as a treatment to the corresponding potential outcome, which is parameterized by a feed-forward deep neural network with multiple hidden layers and non-linear activation functions. The function Î¨ : ğ‘… Ã— ğ‘‡ â†’ ğ‘Œ uses representations and treatment options as inputs to predict potential outcomes.</p><p>The output of Î¨ estimates potential outcomes across treatment and control groups, including the estimated factual outcome Å·ğ‘“ and the estimated counterfactual outcomes Å·ğ‘ğ‘“ . The factual outcomes ğ‘¦ ğ‘“ are used to minimize the loss of prediction Å·ğ‘“ . We aim to minimize the mean squared error in predicting factual outcomes:</p><formula xml:id="formula_11">L Î¨ = 1 ğ‘› ğ‘ âˆ‘ï¸ ğ‘–=1 ( Å·ğ‘“ ğ‘– -ğ‘¦ ğ‘“ ğ‘– ) 2 ,<label>(9)</label></formula><p>where Å·ğ‘– = Î¨(ğ‘Ÿ ğ‘– , ğ‘¡ ğ‘– ) denotes the inferred observed outcome of unit ğ‘– corresponding to the factual treatment ğ‘¡ ğ‘– .</p><p>Counterfactual Outcome Discriminator. The counterfactual outcome discriminator is intended to remove the imbalance of confounder representations between treatment and control groups, and thus it could improve the prediction accuracy of potential outcomes inferred by the outcome generator. We define the counterfactual outcome discriminator as Î¦ : ğ‘… Ã— ğ‘‡ Ã— (ğ‘Œ ğ‘“ or Å¶ğ‘ğ‘“ ) â†’ ğ‘ƒ, where ğ‘ƒ is the discriminator's judgement, i.e., probability that this outcome for unit ğ‘– given ğ‘… and ğ‘‡ is factual outcome. ğ‘ƒ is defined as: ğ‘ƒ = ğ‘ƒ (judges ğ‘¦ ğ‘“ as factual|ğ‘¥, ğ‘¡) if ğ‘¡ is factual treatment choice ğ‘ƒ (judges Å·ğ‘ğ‘“ as factual|ğ‘¥, ğ‘¡) if ğ‘¡ is not factual treatment choice.</p><p>(10) To improve the accuracy of prediction and avoid risk of losing the influence of treatment ğ‘¡ and potential outcomes (ğ‘¦ ğ‘“ or Å·ğ‘ğ‘“ ) due to high dimensional representation vector, we adopt separate head networks for treatment and control groups <ref type="bibr" target="#b18">[19]</ref>. Besides, to improve the influence of (ğ‘¦ ğ‘“ , Å·ğ‘ğ‘“ ) in the discriminator, we add (ğ‘¦ ğ‘“ or Å·ğ‘ğ‘“ ) into each layer of the neural network, repetitively.</p><p>The discriminator deals with a binary classification task, which assigns one label (i.e., factual outcome or counterfactual outcome) to the vector concatenating the representation vector ğ‘Ÿ and potential outcome (ğ‘¦ ğ‘“ or Å·ğ‘ğ‘“ ) under the treatment head network and control head network, respectively. Thus, the loss of discrimination is measured by the cross-entropy with truth probability, where ğ‘ƒ truth = 1 if ğ‘¦ ğ‘“ is input, and ğ‘ƒ truth = 0 if Å·ğ‘ğ‘“ is input. In each iteration of training, we make sure to input the same number of units in the treatment and control groups to ensure that there exist the same number of factual outcomes as counterfactual outcomes in each head network to overcome the imbalanced classification. The inputs of discriminator are generated by the outcome generator Î¨(ğ‘…,ğ‘‡ ), and then the cross entropy loss of the counterfactual outcome discriminator is defined as:</p><formula xml:id="formula_12">L Î¦,Î¨ = - 1 2ğ‘› 1 âˆ‘ï¸ ğ‘¡ =0 ğ‘› âˆ‘ï¸ ğ‘–=1 (ğ‘ truth ğ‘¡ğ‘– log(ğ‘ ğ‘¡ğ‘– ) + (1 -ğ‘ truth ğ‘¡ğ‘– ) log(1 -ğ‘ ğ‘¡ğ‘– )),<label>(11)</label></formula><p>where ğ‘ truth ğ‘¡ğ‘– is the indicator that this input outcome for unit ğ‘– under treatment option ğ‘¡ is the observed factual outcome or inferred outcome from generator module, i.e., ğ‘ truth ğ‘¡ğ‘– equals 1 or 0, separately. ğ‘ƒ ğ‘¡ğ‘– is the probability judged by discriminator that how likely this input outcome for unit ğ‘– under treatment option ğ‘¡ is a factual outcome.</p><p>Thus far, we have introduced the outcome generator to estimate potential outcomes for each unit across treatment and control groups, and the discriminator to determine if the potential outcome is factual, given a unit's confounder representation under treatment or control group. In the initial iterations of the model training, the outcome generator may generate potential outcomes that are very different from factual outcomes as determined by the discriminator. As the model is trained further, the discriminator may no longer be able to distinguish the generated counterfactual outcome and the factual outcome. At this point, we have attained all potential outcomes for each unit under treatment and control groups. For the training procedure of optimizing the outcome generator and discriminator, the minimax game is adopted. Putting all of the above together, the objective function of our Graph Infomax Adversarial Learning (GIAL) method is:</p><formula xml:id="formula_13">min Î¨ max Î¦,ğ‘š (L Î¨ + ğ›¼ L ğ‘š -ğ›½L Î¦,Î¨ ),<label>(12)</label></formula><p>where ğ›¼ and ğ›½ are the hyper-parameters controlling the trade-off among the outcome generator, mutual information, and discriminator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Overview of GIAL</head><p>The proposed Graph Infomax Adversarial Learning method (GIAL) can estimate the treatment effects from networked observational data, which utilizes the graph neural network (GCN or GAT) and structure mutual information to learn the representations of hidden confounders and observed confounders, by mapping the feature covariates and network structure simultaneously into a representation space. Adversarial learning is also employed to mitigate the representation imbalance between treatment and control groups and to predict the counterfactual outcomes. After obtaining the counterfactual outcomes, GIAL can estimate the treatment effects. We summarize the procedures of GIAL as follows:</p><p>(1) Create the negative samples ( X, ğ´) by the row-wise shuffling of ğ‘‹ and keeping the original adjacency matrix ğ´. ğ‘ƒ to remove imbalance of confounder representations between treatment and control group. ( <ref type="formula" target="#formula_11">9</ref>) Here, Steps 6, 7, and 8 in the procedure are jointly trained together by optimizing minimax rule Eq. ( <ref type="formula" target="#formula_13">12</ref>) about L ğ‘š , L Î¨ , and L Î¦,Î¨ to update parameters in ğ‘”, ğ‘“ , ğ‘‘, Î¦, and Î¨.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In this section, we conduct experiments on two semi-synthetic networked datasets, including the BlogCatalog and Flickr, to evaluate the following aspects: (1) Our proposed method can improve treatment effect estimation with respect to average treatment effect and individualized treatment effect compared to the state-of-the-art methods. ( <ref type="formula" target="#formula_1">2</ref>) The structure mutual information can help representations capture more hidden confounder information, and thus increase the predictive accuracy for counterfactual outcomes. ( <ref type="formula" target="#formula_3">3</ref>)</p><p>The proposed method is robust to the hyperparameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>BlogCatalog. BlogCatalog is a social blog directory that manages the bloggers and their blogs. In this dataset, each unit is a blogger and each edge represents the social relationship between two bloggers. The features are bag-of-words representations of keywords in bloggers' descriptions. We follow the assumptions and procedures of synthesizing the outcomes and treatment assignments in <ref type="bibr" target="#b5">[6]</ref>. In this semi-synthetic networked dataset, the outcomes are the opinions of readers on each blogger and the treatment options are mobile devices or desktops on which blogs are read more. If the blogger's blogs are read more on mobile devices, the blogger is in the treatment group; if they are read more on desktops, the blogger is in the control group. We also assume that the topics of bloggers with the social relationship can causally affect their treatment assignment and readers' opinions on them. To model readers' preference on reading some topics from mobile devices and others from desktops, one LDA topic model <ref type="bibr" target="#b5">[6]</ref> is trained. Three settings of datasets are created with ğ‘˜ = 0.5, 1, and 2 that represent the magnitude of the confounding bias in the dataset. ğ‘˜ = 0 means the treatment assignment is random and there is no selection bias, and greater ğ‘˜ means larger selection bias.</p><p>Flickr. Flickr is a popular photo-sharing and hosting service, and it supports an active community where people can share each other's photos. In the Flickr dataset, each unit is a user and each edge represents the social relationship between two users. The features of each user represent a list of tags of interest. The same settings and simulation procedures as BlogCatalog dataset are adopted here. Table <ref type="table" target="#tab_0">1</ref> presents an overview of these two datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baseline Methods</head><p>We compare the proposed GIAL with the following baseline methods. Network Deconfounder (ND) <ref type="bibr" target="#b5">[6]</ref> utilizes the GCN and integral probability metric to learn balanced representations to recognize patterns of hidden confounders from the network dataset.</p><p>Counterfactual Regression (CFRNET) <ref type="bibr" target="#b18">[19]</ref> maps the original features into a balanced representation space by minimizing integral probability metric between treatment and control representation spaces. Treatment-agnostic Representation Networks (TARNet) <ref type="bibr" target="#b18">[19]</ref> is a variant of counterfactual regression without balance regularization. Causal Effect Variational Autoencoder (CEVAE) <ref type="bibr" target="#b16">[17]</ref> is based on Variational Autoencoder (VAE), which simultaneously estimates the unknown latent space summarizing the confounders and the causal effect. Causal Forests (CF) <ref type="bibr" target="#b22">[23]</ref> is a nonparametric forest-based method for estimating heterogeneous treatment effects by extending Breiman's random forest algorithm. Bayesian Additive Regression Trees (BART) <ref type="bibr" target="#b2">[3]</ref> is a nonparametric Bayesian regression model, which uses dimensionally adaptive random basis elements. Before estimating the treatment effects from these two networked datasets, we provide the descriptive data analysis to demonstrate the existence of network structural imbalance in the networked data for causal inference problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Descriptive Data Analysis</head><p>According to graph theory, in the complete graph which is a simple undirected graph where every pair of distinct nodes is connected by a unique edge, there are ğ‘›Ã—(ğ‘›-1) 2 edges for ğ‘› nodes. We assume that the ğ‘› nodes are evenly divided into treatment group and control group with the same ğ‘› 2 nodes in each group, and also each node has the same possibility to have an edge (relationship) edges. Now the edges in this graph are put into two categories: (a) the homogeneous group including the edges that link the nodes in the same group (treatment-treatment or control-control); (b) the heterogeneous group including the edges that link the nodes in different groups (treatment-control). Under the assumption that each node has the same possibility to be connected with another node regardless of the node's treatment assignment, we can find that in the homogeneous group, there are ğ‘› 2 4 -ğ‘› 2 edges and in the heterogeneous group, there are ğ‘› 2 4 edges. The number of edges in the heterogeneous group should be greater than that in the homogeneous group. For example, as shown in Fig. <ref type="figure" target="#fig_3">4</ref>, there is one complete graph with 6 nodes including 3 treatment nodes and 3 control nodes. The heterogeneous group has 9 edges, while the homogeneous group has 6 edges.</p><p>We separately calculate the average numbers of homogeneous edges and heterogeneous edges for the BlogCatalog datasets and Flickr datasets, then report them in Table <ref type="table" target="#tab_1">2</ref>. We can observe that the homogeneous edges are consistently greater than the heterogeneous edges for both datasets with different ğ‘˜. This result totally agrees with our expectation that, in the causal inference problem, the network structure is imbalanced. Therefore, the relationship is more likely to appear among people who are in the same group. This is the major difference between traditional graph learning tasks and the causal inference task on networked data, which is also the motivation of our proposed model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Experimental Settings</head><p>In the following experiments, we randomly sample 60% and 20% of the units as the training set and validation set, and use the remaining 20% units to form the test set. For each dataset with a different imbalance ğ‘˜, the simulation procedures are repeated 10 times and we report the average mean.</p><p>GIAL. By using different graph neural networks to learn the representation space from the networked dataset, the proposed GIAL method has two variants denoted as GIAL GCN and GIAL GAT , which adopt the original implementation of graph convolutional network <ref type="bibr" target="#b13">[14]</ref> and graph attention network (GAT) <ref type="bibr" target="#b21">[22]</ref>, respectively. Besides, a squared ğ‘™ 2 norm regularization with hyperparameter 10 -4 is added into our model to mitigate the overfitting issue. The hyperparameters of our method are chosen based on performance on the validation dataset, and the searching range is shown in Table <ref type="table" target="#tab_4">5</ref>. The Adam SGD optimizer <ref type="bibr" target="#b12">[13]</ref> is used to train the final objective function Eq. ( <ref type="formula" target="#formula_13">12</ref>) with an initial learning rate of 0.001 and an early stopping strategy with patience of 100 epochs.</p><p>Baseline Methods. BART, CF, CEVAE, TARNet, and CFRNET are not originally designed for the networked observational data, so they cannot directly utilize the network information. To be fair, we concatenate the corresponding row of adjacency matrix to the original features, but this strategy cannot effectively improve the performance of baselines due to the curse of dimensionality. Besides, we adopt their default hyperparameter settings <ref type="bibr" target="#b5">[6]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Results</head><p>For the BlogCatalog and Flickr datasets, we adopt two commonly used evaluation metrics to evaluate the performance of our method and baselines. The first one is the error of ATE estimation, which is defined as ğœ– ATE = |ATE -ATE|, where ATE is the true value and ATE is an estimated ATE. The second one is the error of expected precision in estimation of heterogeneous effect (PEHE) <ref type="bibr" target="#b7">[8]</ref>, which is defined as 2 , where ITE ğ‘– is the true ITE for unit ğ‘– and ITE ğ‘– is an estimated ITE for unit ğ‘–.</p><formula xml:id="formula_14">ğœ– PEHE = 1 ğ‘› ğ‘› ğ‘–=1 (ITE ğ‘– -ITE ğ‘– )</formula><p>Table <ref type="table" target="#tab_2">3</ref> shows the performance of our method and baseline methods on the BlogCatalog and Flickr datasets over 10 realizations. We report the average results of</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>âˆš</head><p>ğœ– PEHE and ğœ– ATE on the test sets. GIAL GCN achieves the best performance with respect to</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>âˆš</head><p>ğœ– PEHE and ğœ– ATE in all cases of both datasets. Although the GIAL GAT also has obvious improvements compared to baseline methods, it is outperformed by GIAL GCN . GCN demonstrates clear superiority over GAT when recognizing patterns of hidden confounders from imbalanced network structure. Because ğ‘˜ = 0.5, 1, and 2 is used to represent the magnitude of the confounding bias in both datasets, results show that GIAL consistently outperforms the baseline methods under different levels of divergence, and our method is robust to a high level of confounding bias. Compared to baseline methods (e.g., CFRNET) only relying on observed confounders but without utilizing the network information, our model is capable of recognizing the patterns of hidden confounders from the network structure. Compared to baseline methods with learning network information (e.g., ND), our model has significant performance advantages, which demonstrates our model can capture more information from an imbalanced network structure. The reason is that our method maximizes the structure mutual information, instead of directly adopting the graph learning method without considering the specificity of networked data in the causal inference problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Model Evaluation</head><p>Experimental results on both datasets show that GIAL obtains a more accurate estimation of the ATE and ITE than the state-of-theart methods. We further evaluate the performance of GIAL from two perspectives, including the effectiveness of each component, and its robustness to hyper-parameters.</p><p>We perform two ablation studies of GIAL GCN on both datasets. The first one is GIAL (w/o SMI) where the structure mutual information maximizing module is removed. We directly adopt graph neural networks to learn the representation space without considering the structural imbalance of networked data. The second ablation study is GIAL (w/o CD) where the counterfactual outcome discriminator is removed and there is not any restriction on the As shown in Table <ref type="table" target="#tab_3">4</ref>, the performance becomes poor after removing either the structure mutual information or counterfactual outcome discriminator, compared to the original GIAL. More specifically, after removing the structure mutual information,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>âˆš</head><p>ğœ– PEHE and ğœ– ATE increase dramatically and have similar performance to other baseline methods. Besides, as the bias (ğ‘˜) increases, the difference between the performance of GIAL (w/o CD) and the original GIAL increases further. Therefore, the structure mutual information and counterfactual outcome discriminator are essential components of our model.</p><p>Next, we explore the model's sensitivity to the most important parameters ğ›¼ and ğ›½, which control the ability to capture the graph structure and handle the confounding bias when estimating the potential outcomes. We show the results of</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>âˆš</head><p>ğœ– PEHE and ğœ– ATE on BlogCatalog dataset with different ğ‘˜ in Fig. <ref type="figure" target="#fig_4">5</ref>. We observe that the performance is stable over a large parameter range. It confirms the effectiveness and robustness of structure mutual information and counterfactual outcome discriminator in GIAL, which is consistent with our ablation studies, i.e., GIAL (w/o SMI) and GIAL (w/o CD).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RELATED WORK</head><p>The related work is presented along with two directions: learning causal effects from observational data and graph neural networks.</p><p>Various causal effect estimation methods for observational data have sprung up. For most existing methods, the strong ignorability assumption is the most important prerequisite. However, this assumption might be untenable in practice. A series of methods have been proposed to relax the strong ignorability assumption. A latent variable is inferred as a substitute for unobserved confounders <ref type="bibr" target="#b23">[24]</ref>. Variational Autoencoder has been used to infer the relationships between the observed confounders based on the assumption joint distribution of the latent confounders and the observed confounders can be approximately recovered solely from the observations <ref type="bibr" target="#b16">[17]</ref>. Recently, some work aims to relax the strong ignorability assumption via network knowledge, where the network connecting the units is a proxy is for the unobserved confounding. The network deconfounder <ref type="bibr" target="#b5">[6]</ref> learns representations of confounders from network data by adopting the graph convolutional networks. Another work utilizes graph attention networks to learn representations and mitigates confounding bias by representation balancing and treatment prediction, simultaneously <ref type="bibr" target="#b4">[5]</ref>. Causal network embedding (CNE) <ref type="bibr" target="#b19">[20]</ref> is proposed to learn node embeddings from network data to represent confounders by reducing the causal estimation problem to a semi-supervised prediction of both the treatments and outcomes. For the existing methods about networked data, they do not dig deeply on what is the essential difference between the networked data under the causal inference problem and the networked data for traditional graph learning tasks such as node classification, link detection, etc. This is the reason why we propose this GIAL model, instead of directly adopting the GCN or GAT to learn the representation from the networked data.</p><p>Graph learning is increasingly becoming fascinating as more and more real-world data can be modeled as networked data. Graph convolutional network <ref type="bibr" target="#b13">[14]</ref> is an effective approach for semi-supervised learning on networked data, via a localized first-order approximation of spectral graph convolutions. Graph attention network (GAT) <ref type="bibr" target="#b20">[21]</ref> is an attention-based architecture leveraging masked self-attentional layers where nodes are able to attend over their neighborhoods' features. Deep graph infomax (DGI) <ref type="bibr" target="#b21">[22]</ref> is one approach for learning node representations within networked data in an unsupervised manner, which relies on maximizing mutual information between patch representations and high-level summaries of graphs. In our model, we extend the idea in DGI originally aimed for unsupervised learning to representation learning under the causal inference setting. Utilizing the structure mutual information can help representations capture the imbalanced structure that is specific to the causal inference problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper, we propose the Graph Infomax Adversarial Learning method (GIAL) to capture the hidden confounders and estimate the treatment effects from networked observational data. GIAL makes full use of the network structure to capture more information by recognizing the imbalance in the network structure. Our work clarifies the greatest particularity of networked data under the causal inference problem compared with traditional graph learning tasks, that is, the structural imbalance due to confounding bias between treatment and control groups. Extensive experiments show the effectiveness and advantages of the proposed GIAL method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example of the imbalance of network structure.</figDesc><graphic coords="1,361.30,184.03,151.32,70.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Framework of our Graph Infomax Adversarial Learning method (GIAL). Graph neural networks and structure mutual information are utilized to learn the representations of hidden confounders and observed confounders. Then the potential outcome generator is applied to infer the potential outcomes of units across treatment and control groups based on the learned representation space and treatment assignment. At the same time, the counterfactual outcome discriminator is incorporated to remove the imbalance in the learned representations of treatment and control groups.</figDesc><graphic coords="4,103.12,83.69,403.51,186.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>( 2 )( 4 )</head><label>24</label><figDesc>Learn the representation space ğ‘… for the positive samples (ğ‘‹, ğ´) by function ğ‘” : ğ‘‹ Ã—ğ´ â†’ ğ‘… by a graph neural network. (3) Learn the representation space R for the negative samples ( X, ğ´) by function ğ‘” : X Ã— ğ´ â†’ R by the same graph neural network as Step 2. Utilize a structure summary function ğ‘“ : ğ‘… ğ‘›Ã—ğ‘‘ â†’ ğ‘† to summarize the learned representation into a graph-level structure representation, i.e., ğ‘  = ğ‘“ (ğ‘”(ğ‘‹, ğ´)). (5) Employ a discriminator ğ‘‘ : ğ‘… Ã— ğ‘† â†’ ğ‘ƒ to obtain ğ‘‘ (ğ‘Ÿ ğ‘– , ğ‘ ) and ğ‘‘ ( rğ‘– , ğ‘ ), which are the probabilities that the representations of ğ‘–-th positive and negative samples are contained within the original graph structure summary ğ‘ . (6) Utilize functions ğ‘”, ğ‘“ and ğ‘‘ to maximize mutual information between ğ‘… and ğ‘†. (7) Use potential outcome generator Î¨ : ğ‘… Ã— ğ‘‡ â†’ ğ‘Œ to estimate the potential outcomes. (8) Apply counterfactual discriminator Î¦ : ğ‘…Ã—ğ‘‡ Ã—(ğ‘Œ ğ‘“ or Å¶ğ‘ğ‘“ ) â†’</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Example of complete graph. The solid line represents heterogeneous edge and the dashed line means homogeneous edge.</figDesc><graphic coords="6,386.52,464.81,100.88,86.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Sensitivity analysis for ğ›¼ and ğ›½ of structure mutual information and counterfactual outcome discriminator.</figDesc><graphic coords="9,53.80,83.69,504.40,121.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="2,90.51,83.69,428.74,144.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Properties of BlogCatalog and Flickr datasets.</figDesc><table><row><cell>Datasets</cell><cell cols="2">BlogCatalog Flickr</cell></row><row><cell>Nodes</cell><cell>5,196</cell><cell>7,575</cell></row><row><cell>Features</cell><cell>8,189</cell><cell>12,047</cell></row><row><cell>Edges</cell><cell>171,743</cell><cell>239,738</cell></row><row><cell cols="2">Treatments 2</cell><cell>2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Summary of homogeneous edges and heterogeneous edges for the BlogCatalog datasets and Flickr datasets.</figDesc><table><row><cell>Dataset</cell><cell cols="4">k Homogeneous Heterogeneous</cell></row><row><cell></cell><cell cols="2">0.5 94524.5</cell><cell cols="2">77218.5</cell></row><row><cell cols="2">BlogCatalog 1</cell><cell>101102.8</cell><cell cols="2">70640.2</cell></row><row><cell></cell><cell>2</cell><cell>116031.8</cell><cell cols="2">55711.2</cell></row><row><cell></cell><cell cols="2">0.5 124320.9</cell><cell cols="2">115417.1</cell></row><row><cell>Flickr</cell><cell>1</cell><cell>130978.5</cell><cell cols="2">108759.5</cell></row><row><cell></cell><cell>2</cell><cell>141957.3</cell><cell cols="2">97780.7</cell></row><row><cell cols="5">with another node regardless of the node's treatment assignment.</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>ğ‘›Ã—(ğ‘›-1)</cell></row><row><cell cols="4">Then, this graph is still a complete graph with</cell><cell>2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Performance comparison on BlogCatalog and Flickr datasets with different ğ‘˜ âˆˆ 0.5, 1, 2. We present the mean value of âˆš ğœ– PEHE and ğœ– ATE on the test sets. Results of baseline methods on the same datasets are reported in<ref type="bibr" target="#b5">[6]</ref>.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell cols="2">BlogCatalog</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Flickr</cell></row><row><cell></cell><cell cols="2">k=0.5</cell><cell>k=1</cell><cell></cell><cell>k=2</cell><cell></cell><cell cols="2">k=0.5</cell><cell>k=1</cell><cell>k=2</cell></row><row><cell></cell><cell>âˆš</cell><cell></cell><cell>âˆš</cell><cell></cell><cell>âˆš</cell><cell></cell><cell>âˆš</cell><cell></cell><cell>âˆš</cell><cell>âˆš</cell></row><row><cell>Method</cell><cell cols="2">ğœ– PEHE ğœ– ATE</cell><cell cols="2">ğœ– PEHE ğœ– ATE</cell><cell cols="2">ğœ– PEHE ğœ– ATE</cell><cell cols="2">ğœ– PEHE ğœ– ATE</cell><cell cols="2">ğœ– PEHE ğœ– ATE</cell><cell>ğœ– PEHE ğœ– ATE</cell></row><row><cell>BART [3]</cell><cell>4.808</cell><cell cols="2">2.680 5.770</cell><cell cols="2">2.278 11.608</cell><cell>6.418</cell><cell>4.907</cell><cell cols="2">2.323 9.517</cell><cell>6.548</cell><cell>13.155</cell><cell>9.643</cell></row><row><cell>CF [23]</cell><cell>7.456</cell><cell cols="2">1.261 7.805</cell><cell cols="2">1.763 19.271</cell><cell>4.050</cell><cell>8.104</cell><cell cols="2">1.359 14.636</cell><cell>3.545</cell><cell>26.702</cell><cell>4.324</cell></row><row><cell>CEVAE [17]</cell><cell>7.481</cell><cell cols="2">1.279 10.387</cell><cell cols="2">1.998 24.215</cell><cell>5.566</cell><cell>12.099</cell><cell cols="2">1.732 22.496</cell><cell>4.415</cell><cell>42.985</cell><cell>5.393</cell></row><row><cell>TARNet [19]</cell><cell>11.570</cell><cell cols="2">4.228 13.561</cell><cell cols="2">8.170 34.420</cell><cell cols="2">13.122 14.329</cell><cell cols="2">3.389 28.466</cell><cell>5.978</cell><cell>55.066</cell><cell>13.105</cell></row><row><cell cols="2">CFRNET MMD [19] 11.536</cell><cell cols="2">4.127 12.332</cell><cell cols="2">5.345 34.654</cell><cell cols="2">13.785 13.539</cell><cell cols="2">3.350 27.679</cell><cell>5.416</cell><cell>53.863</cell><cell>12.115</cell></row><row><cell cols="2">CFRNET Wass [19] 10.904</cell><cell cols="2">4.257 11.644</cell><cell cols="2">5.107 34.848</cell><cell cols="2">13.053 13.846</cell><cell cols="2">3.507 27.514</cell><cell>5.192</cell><cell>53.454</cell><cell>13.269</cell></row><row><cell>ND [6]</cell><cell>4.532</cell><cell cols="2">0.979 4.597</cell><cell cols="2">0.984 9.532</cell><cell>2.130</cell><cell>4.286</cell><cell cols="2">0.805 5.789</cell><cell>1.359</cell><cell>9.817</cell><cell>2.700</cell></row><row><cell>GIAL GAT (Ours)</cell><cell>4.215</cell><cell cols="2">0.912 4.258</cell><cell cols="2">0.937 9.119</cell><cell>1.982</cell><cell>4.015</cell><cell cols="2">0.773 5.432</cell><cell>1.2312 9.428</cell><cell>2.586</cell></row><row><cell>GIAL GCN (Ours)</cell><cell>4.023</cell><cell cols="2">0.841 4.091</cell><cell cols="2">0.883 8.927</cell><cell cols="2">1.780 3.938</cell><cell cols="2">0.682 5.317</cell><cell>1.194 9.275</cell><cell>2.245</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Summary of results in ablation studies.</figDesc><table><row><cell></cell><cell cols="2">k=0.5</cell><cell>k=1</cell><cell></cell><cell>k=2</cell></row><row><cell></cell><cell>âˆš</cell><cell></cell><cell>âˆš</cell><cell></cell><cell>âˆš</cell></row><row><cell></cell><cell cols="2">ğœ– PEHE ğœ– ATE</cell><cell cols="2">ğœ– PEHE ğœ– ATE</cell><cell>ğœ– PEHE ğœ– ATE</cell></row><row><cell>BlogCatalog</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>GIAL</cell><cell>4.023</cell><cell cols="2">0.841 4.091</cell><cell cols="2">0.883 8.927</cell><cell>1.780</cell></row><row><cell cols="2">GIAL (w/o SMI) 4.422</cell><cell cols="2">0.982 4.481</cell><cell cols="2">0.981 9.315</cell><cell>2.142</cell></row><row><cell cols="2">GIAL (w/o CD) 4.482</cell><cell cols="2">0.987 4.951</cell><cell cols="2">1.023 13.598</cell><cell>3.215</cell></row><row><cell>Flickr</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>GIAL</cell><cell>3.938</cell><cell cols="2">0.682 5.317</cell><cell cols="2">1.194 9.275</cell><cell>2.245</cell></row><row><cell cols="2">GIAL (w/o SMI) 4.158</cell><cell cols="2">0.792 5.694</cell><cell cols="2">1.375 9.673</cell><cell>2.661</cell></row><row><cell cols="2">GIAL (w/o CD) 4.284</cell><cell cols="2">0.812 6.127</cell><cell cols="2">1.435 11.524</cell><cell>3.564</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Hyperparameters and ranges.</figDesc><table><row><cell>Hyperparameter</cell><cell>Range</cell></row><row><cell>ğ›¼, ğ›½</cell><cell>0, 10 -4 ,10 -3 ,10 -2 ,10 -1</cell></row><row><cell cols="2">Dim. of confounder representation 50, 100, 150, 200</cell></row><row><cell>No. of GCN and GAT layers</cell><cell>1, 2, 3</cell></row><row><cell>No. of attention heads in GAT</cell><cell>1, 2, 3, 4</cell></row><row><cell>No. of outcome generator layer</cell><cell>1, 2, 3, 4</cell></row><row><cell cols="2">divergence between the representation distributions of treatment</cell></row><row><cell>and control groups.</cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>We would like to thank the anonymous reviewers for their insightful comments. This research is supported in part by the <rs type="funder">U.S. Army Research Office Award</rs> under Grant Number <rs type="grantNumber">W911NF-21-1-0109</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_MZN9aQZ">
					<idno type="grant-number">W911NF-21-1-0109</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Bayesian inference of individualized treatment effects using multi-task gaussian processes</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alaa</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Mihaela</forename><surname>Van Der Schaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3424" to="3432" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Mutual information neural estimation</title>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Ishmael Belghazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aristide</forename><surname>Baratin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sai</forename><surname>Rajeshwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="531" to="540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">BART: Bayesian additive regression trees</title>
		<author>
			<persName><forename type="first">Edward</forename><forename type="middle">I</forename><surname>Hugh A Chipman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">E</forename><surname>George</surname></persName>
		</author>
		<author>
			<persName><surname>Mcculloch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Applied Statistics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="266" to="298" />
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Matching in Selective and Balanced Representation Space for Treatment Effects Estimation</title>
		<author>
			<persName><forename type="first">Zhixuan</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">L</forename><surname>Rathbun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.06828</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">IGNITE: A Minimax Game Toward Learning Individual Treatment Effects from Networked Observational Data</title>
		<author>
			<persName><forename type="first">Ruocheng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jundong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>SelÃ§uk Candan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrienne</forename><surname>Raglin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<author>
			<persName><forename type="first">Ruocheng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jundong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.03485</idno>
		<title level="m">Learning individual treatment effects from networked observational data</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName><forename type="first">Kaiming</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyu</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings</title>
		<meeting>null</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
	<note>Shaoqing Ren, and Jian Sun</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Bayesian nonparametric modeling for causal inference</title>
		<author>
			<persName><forename type="first">L</forename><surname>Jennifer</surname></persName>
		</author>
		<author>
			<persName><surname>Hill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational and Graphical Statistics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="217" to="240" />
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Devon</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Fedorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samuel</forename><surname>Lavoie-Marchildon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karan</forename><surname>Grewal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Phil</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Trischler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1808.06670</idno>
		<title level="m">Learning deep representations by mutual information estimation and maximization</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Causal inference in statistics, social, and biomedical sciences</title>
		<author>
			<persName><forename type="first">W</forename><surname>Guido</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><forename type="middle">B</forename><surname>Imbens</surname></persName>
		</author>
		<author>
			<persName><surname>Rubin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Recent developments in the econometrics of program evaluation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Guido</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><forename type="middle">M</forename><surname>Imbens</surname></persName>
		</author>
		<author>
			<persName><surname>Wooldridge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of economic literature</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="5" to="86" />
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">CensNet: Convolution with Edge-Node Switching in Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengsheng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2656" to="2662" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Matching on balanced nonlinear representations for treatment effects estimation</title>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="929" to="939" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Matching via Dimensionality Reduction for Estimation of Treatment Effects in Digital Marketing Campaigns</title>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikos</forename><surname>Vlassis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaya</forename><surname>Kawale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3768" to="3774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Causal effect inference with deep latent-variable models</title>
		<author>
			<persName><forename type="first">Christos</forename><surname>Louizos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uri</forename><surname>Shalit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Joris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Mooij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6446" to="6456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Estimating causal effects of treatments in randomized and nonrandomized studies</title>
		<author>
			<persName><surname>Donald B Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of educational Psychology</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page">688</biblScope>
			<date type="published" when="1974">1974. 1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Estimating individual treatment effect: generalization bounds and algorithms</title>
		<author>
			<persName><forename type="first">Uri</forename><surname>Shalit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Fredrik D Johansson</surname></persName>
		</author>
		<author>
			<persName><surname>Sontag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="3076" to="3085" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Using embeddings to correct for unobserved confounding in networks</title>
		<author>
			<persName><forename type="first">Victor</forename><surname>Veitch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="13792" to="13802" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Petar</forename><surname>VeliÄkoviÄ‡</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
	<note type="report_type">Graph attention networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Fedus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>LiÃ²</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devon</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><surname>Hjelm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Deep Graph Infomax.. In ICLR</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>Poster</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Estimation and inference of heterogeneous treatment effects using random forests</title>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Wager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susan</forename><surname>Athey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Amer. Statist. Assoc</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="page" from="1228" to="1242" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The blessings of multiple causes</title>
		<author>
			<persName><forename type="first">Yixin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Amer. Statist. Assoc</title>
		<imprint>
			<biblScope unit="volume">114</biblScope>
			<biblScope unit="page" from="1574" to="1596" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Liuyi</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhixuan</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidong</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.02770</idno>
		<title level="m">A Survey on Causal Inference</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Representation learning for treatment effect estimation from observational data</title>
		<author>
			<persName><forename type="first">Liuyi</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengdi</forename><surname>Huai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2633" to="2643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">On the estimation of treatment effect with text covariates</title>
		<author>
			<persName><forename type="first">Liuyi</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongfei</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidong</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 28th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4106" to="4113" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
