<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CAUSAL-AWARE GRAPH NEURAL ARCHITECTURE SEARCH UNDER DISTRIBUTION SHIFTS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-12-30">30 Dec 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Peiwen</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
							<email>xin_wang@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zeyang</forename><surname>Zhang</surname></persName>
							<email>zy-zhang20@mails.tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yijian</forename><surname>Qin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
							<email>zwzhang@tsinghua.edu.cn</email>
						</author>
						<author>
							<persName><forename type="first">Jialong</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Alibaba Cloud</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yang</forename><surname>Li</surname></persName>
							<email>yangli@sz.tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
							<email>wwzhu@tsinghua.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">CAUSAL-AWARE GRAPH NEURAL ARCHITECTURE SEARCH UNDER DISTRIBUTION SHIFTS</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-12-30">30 Dec 2024</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2405.16489v2[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph neural architecture search (Graph NAS) has emerged as a promising approach for autonomously designing graph neural network architectures by leveraging the correlations between graphs and architectures. However, the existing methods fail to generalize under distribution shifts that are ubiquitous in real-world graph scenarios, mainly because the graph-architecture correlations they exploit might be spurious and varying across distributions. In this paper, we propose to handle the distribution shifts in the graph architecture search process by discovering and exploiting the causal relationship between graphs and architectures to search for the optimal architectures that can generalize under distribution shifts. The problem remains unexplored with the following critical challenges: 1) how to discover the causal graph-architecture relationship that has stable predictive abilities across distributions, 2) how to handle distribution shifts with the discovered causal graph-architecture relationship to search the generalized graph architectures. To address these challenges, we propose a novel approach, Causal-aware Graph Neural Architecture Search (CARNAS), which is able to capture the causal graph-architecture relationship during the architecture search process and discover the generalized graph architecture under distribution shifts. Specifically, we propose Disentangled Causal Subgraph Identification to capture the causal subgraphs that have stable prediction abilities across distributions. Then, we propose Graph Embedding Intervention to intervene on causal subgraphs within the latent space, ensuring that these subgraphs encapsulate essential features for prediction while excluding non-causal elements. Additionally, we propose Invariant Architecture Customization to reinforce the causal invariant nature of the causal subgraphs, which are utilized to tailor generalized graph architectures. Extensive experiments on synthetic and real-world datasets demonstrate that our proposed CARNAS achieves advanced out-of-distribution generalization ability by discovering the causal relationship between graphs and architectures during the search process.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Graph neural architecture search (Graph NAS), aiming at automating the designs of GNN architectures for different graphs, has shown great success by exploiting the correlations between graphs and architectures. Present approaches <ref type="bibr" target="#b10">(Gao et al., 2021;</ref><ref type="bibr" target="#b24">Li et al., 2020;</ref><ref type="bibr" target="#b30">Liu et al., 2018)</ref> leverage a rich search space filled with GNN operations and employ strategies like reinforcement learning and continuous optimization algorithms to pinpoint an optimal architecture for specific datasets, aiming to decode the natural correlations between graph data and their ideal architectures. Based on the independently and identically distributed (I.I.D) assumption on training and testing data, existing methods assume the graph-architecture correlations are stable across graph distributions.</p><p>In this paper, we study the problem of graph neural architecture search under distribution shifts by capturing the causal relationship between graphs and architectures to search for the optimal graph architectures that can generalize under distribution shifts. The problem is highly non-trivial with the following challenges:</p><p>• How to discover the causal graph-architecture relationship that has stable predictive abilities across distributions?</p><p>• How to handle distribution shifts with the discovered causal graph-architecture relationship to search the generalized graph architectures?</p><p>To address these challenges, we propose the Causal-aware Graph NAS (CARNAS), which is able to capture the causal relationship, stable to distribution shifts, between graphs and architectures, and thus handle the distribution shifts in the graph architecture search process. Specifically, we design a Disentangled Causal Subgraph Identification module, which employs disentangled GNN layers to obtain node and edge representations, then further derive causal subgraphs based on the importance of each edge. This module enhances the generalization by deeply exploring graph features as well as latent information with disentangled GNNs, thereby enabling a more precise extraction of causal subgraphs, carriers of causally relevant information, for each graph instance. Following this, our Graph Embedding Intervention module employs another shared GNN to encode the derived causal subgraphs and non-causal subgraphs in the same latent space, where we perform interventions on causal subgraphs with non-causal subgraphs. Additionally, we ensure the causal subgraphs involve principal features by engaging the supervised classification loss of causal subgraphs into the training objective. We further introduce the Invariant Architecture Customization module, which addresses distribution shifts not only by constructing architectures for each graph with their causal subgraph but also by integrating a regularizer on simulated architectures corresponding to those intervention graphs, aiming to reinforce the causal invariant nature of causal subgraphs derived in module 1. We remark that the classification loss for causal subgraphs in module 2 and the regularizer on architectures for intervention graphs in module 3 help with ensuring the causality between causal subgraphs and the customized architecture for a graph instance. Moreover, by incorporating them into the training and search process, we make the Graph NAS model intrinsically interpretable to some degree. Empirical validation across both synthetic and real-world datasets underscores the remarkable out-of-distribution generalization capabilities of CARNAS over existing baselines. Detailed ablation studies further verify our designs. The contributions of this paper are summarized as follows:</p><p>• We are the first to study graph neural architecture search under distribution shifts from the causal perspective, by proposing the causal-aware graph neural architecture search (CARNAS), that integrates causal inference into graph neural architecture search, to the best of our knowledge.</p><p>• We propose three modules: disentangled causal subgraph identification, graph embedding intervention, and invariant architecture customization, offering a nuanced strategy for extracting and utilizing causal relationships between graph data and architecture, which is stable under distribution shifts, thereby enhancing the model's capability of out-of-distribution generalization.</p><p>• Extensive experiments on both synthetic and real-world datasets confirm that CARNAS significantly outperforms existing baselines, showcasing its efficacy in improving graph classification accuracy across diverse datasets, and validating the superior out-of-distribution generalization capabilities of our proposed CARNAS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRELIMINARY</head><p>2.1 GRAPH NAS UNDER DISTRIBUTION SHIFTS Denote G and Y as the graph and label space. We consider a training graph dataset G tr = {(G i , Y i )} Ntr i=1 and a testing graph dataset</p><formula xml:id="formula_0">G te = {(G i , Y i )} Nte i=1</formula><p>, where G i ∈ G, Y i ∈ Y, N tr and N te represent the number of graph instances in training set and testing set, respectively. The generalization of graph classification under distribution shifts can be formed as:</p><p>Problem 1 We aim to find the optimal prediction model F * (•) : G -→ Y that performs well on G te when there is a distribution shift between training and testing data, i.e. P (G tr ) ̸ = P (G te ):</p><formula xml:id="formula_1">F * (•) = arg min F E (G,Y )∼P (Gte) [ℓ(F (G), Y ) | G tr ] ,<label>(1)</label></formula><p>where ℓ(•, •) : Y × Y -→ R is a loss function.</p><p>Graph NAS methods search the optimal GNN architecture A * from the search space A, and form the complete model F together with the learnable parameters ω. Unlike most existing works using a fixed GNN architecture for all graphs, <ref type="bibr" target="#b39">(Qin et al., 2022)</ref> is the first to customize a GNN architecture for each graph, supposing that the architecture only depends on the graph. We follow the idea and inspect deeper concerning the graph neural architecture search process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">CAUSAL VIEW OF THE GRAPH NAS PROCESS</head><p>Causal approaches are largely adopted when dealing with out-of-distribution (OOD) generalization by capturing the stable causal structures or patterns in input data that influence the results <ref type="bibr">(Li et al., 2022b)</ref>. While in normal graph neural network cases, previous work that studies the problem from a causal perspective mainly considers the causality between graph data and labels <ref type="bibr">(Li et al., 2022c;</ref><ref type="bibr">Wu et al., 2021a)</ref>.</p><p>Causal analysis in Graph NAS. Based on the known that different GNN architectures suit different graphs <ref type="bibr" target="#b6">(Corso et al., 2020;</ref><ref type="bibr" target="#b58">Xu et al., 2020)</ref> and inspired by <ref type="bibr">(Wu et al., 2021b)</ref>, we analyze the potential relationships between graph instance G, causal subgraph G c , non-causal subgraph G s and optimal architecture A * for G in the graph neural architecture search process as below:</p><p>• G c → G ← G s indicates that two disjoint parts, causal subgraph G c and non-causal subgraph G s , together form the input graph G. • G c → A * represents our assumption that there exists the causal subgraph which solely determines the optimal architecture A * for input graph G. Taking the Spurious-Motif dataset <ref type="bibr" target="#b61">(Ying et al., 2019)</ref> as an example, <ref type="bibr" target="#b39">(Qin et al., 2022)</ref> discovers that different shapes of graph elements prefer different architectures. • G c G s means that there are potential probabilistic dependencies between G c and G s <ref type="bibr" target="#b38">(Pearl et al., 2000;</ref><ref type="bibr">2016)</ref>, which can make up spurious correlations between non-causal subgraph G s and the optimal architecture A * .</p><p>Intervention. Inspired by the ideology of invariant learning <ref type="bibr" target="#b1">(Arjovsky et al., 2019;</ref><ref type="bibr" target="#b23">Krueger et al., 2021;</ref><ref type="bibr" target="#b4">Chang et al., 2020)</ref>, that forms different environments to abstract the invariant features, we do interventions on causal subgraph G c by adding different spurious (non-causal) subgraphs to it, and therefore simulate different environments for a graph instance G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">PROBLEM FORMALIZATION</head><p>Based on the above analysis, we propose to search for a causal-aware GNN architecture for each input graph. To be specific, we target to guide the search for the optimal architecture A * by identifying the causal subgraph G c in the Graph NAS process. Therefore, Problem 1 is transformed into the following concrete task as in Problem 2. Problem 2 We systematize model</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Graph Embedding Intervention</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Disentangled Causal Subgraph Identification</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Invariant</head><formula xml:id="formula_2">F : G - → Y into three modules, i.e. F = f C • f A • f Y , in which f C (G) = G c : G - → G c abstracts the causal subgraph G c from input graph G, where causal subgraph space G c is a subset of G, f A (G c ) = A : G c - → A customizes the GNN architecture A for causal subgraph G c , and f Y (G, A) = Ŷ : G × A -</formula><p>→ Y outputs the prediction Ŷ . Further, we derive the following objective function:</p><formula xml:id="formula_3">min f C ,f A ,f Y σL pred + (1 -σ)L causal ,<label>(2)</label></formula><formula xml:id="formula_4">L pred = Ntr i=1 ℓ F f C (Gi),f A (Gci),f Y (Gi,Ai) (G i ) , Y i ,<label>(3)</label></formula><p>where L pred guarantees the final prediction performance of the whole model, L causal is a regularizer for causal constraints and σ is the hyper-parameter to adjust the optimization of those two parts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">METHOD</head><p>We present our proposed method in this section based on the above causal view. Firstly, we present the disentangled causal subgraph identification module to obtain the causal subgraph for searching optimal architecture in Section 3.1. Then, we propose the intervention module in Section 3.2, to help with finding the invariant subgraph that is causally correlated with the optimal architectures, making the NAS model intrinsically interpretable to some degree. In Section 3.3, we introduce the simulated customization module which aims to deal with distribution shift by customizing for each graph and simulating the situation when the causal subgraph is affected by different spurious parts. Finally, we show the total invariant learning and optimization procedure in Section 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">DISENTANGLED CAUSAL SUBGRAPH IDENTIFICATION</head><p>This module utilizes disentangled GNN layers to capture different latent factors of the graph structure and further split the input graph instance G into two subgraphs: causal subgraph G c and non-Preprint.</p><p>causal subgraph G s . Specifically, considering an input graph G = (V, E), its adjacency matrix is D ∈ {0, 1} |V|×|V| , where D i,j = 1 denotes that there exists an edge between node V i and node V j , while D i,j = 0 otherwise. Since optimizing a discrete binary matrix M ∈ {0, 1}</p><p>|V|×|V| is unpractical due to the enormous number of subgraph candidates <ref type="bibr" target="#b61">(Ying et al., 2019)</ref>, and learning M separately for each input graph fails in generalizing to unseen test graphs <ref type="bibr" target="#b32">(Luo et al., 2020)</ref>, we adopt shared learnable disentangled GNN layers to comprehensively unveil the latent graph structural features and better abstract causal subgraphs. Firstly, we denote Q as the number of latent features taken into account, and learn Q-chunk node representations by Q GNNs:</p><formula xml:id="formula_5">Z (l) = ∥ Q q=1 GNN 0 Z (l-1) q , D ,<label>(4)</label></formula><p>where Z l q is the q-th chunk of the node representation at l-th layer, D is the adjacency matrix, and ∥ denotes concatenation. Then, we generate the edge importance scores S E ∈ R |E|×1 with an MLP:</p><formula xml:id="formula_6">S E = MLP Z (L) row , Z (L) col ,<label>(5)</label></formula><p>where Z (L) ∈ R |V|×d is the node representations after L layers of disentangled GNN, and</p><formula xml:id="formula_7">Z (L) row , Z (L) col</formula><p>are the subsets of Z (L) containing the representations of row nodes and column nodes of edges E respectively. After that, we attain the causal and non-causal subgraphs by picking out the important edges through S E :</p><formula xml:id="formula_8">E c = Top t (S E ), E s = E -E c ,<label>(6)</label></formula><p>where E c and E s denotes the edge sets of G c and G s , respectively, and Top t (•) selects the top t-percentage of edges with the largest edge score values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">GRAPH EMBEDDING INTERVENTION</head><p>After obtaining the causal subgraph G c and non-causal subgraph G s of an input graph G, we use another shared GNN 1 to encode those subgraphs so as to do interventions in the same latent space:</p><formula xml:id="formula_9">Z c = GNN 1 (G c ) , Z s = GNN 1 (G s ) .<label>(7)</label></formula><p>Moreover, a readout layer is placed to aggregate node-level representations into graph-level representations:</p><formula xml:id="formula_10">H c = READOUT (Z c ) , H s = READOUT (Z s ) .<label>(8)</label></formula><p>Supervised classification for causal subgraphs. We claim that the causal subgraph G c inferred in Section 3.1 for finding the optimal GNN architecture is supposed to contain the main characteristic of graph G's structure as well as capture the essential part for the final graph classification predicting task. Hence, we employ a classifier on H c to construct a supervised classification loss:</p><formula xml:id="formula_11">L cpred = Ntr i=1 ℓ Ŷci , Y i , Ŷci = Φ (H ci ) , (<label>9</label></formula><formula xml:id="formula_12">)</formula><p>where Φ is a classifier, Ŷci is the prediction of graph G i 's causal subgraph G ci and Y i is the ground truth label of G i .</p><p>Interventions by non-causal subgraphs. Based on subgraphs' embedding H c and H s , we formulate the intervened embedding H v in the latent space. Specifically, we collect all the representations of non-causal subgraphs</p><formula xml:id="formula_13">{H si }, i ∈ [1, N tr ], corresponding to each input graph {G i }, i ∈ [1, N tr ],</formula><p>in the current batch, and randomly sample N s of them as the candidates {H sj }, j ∈ [1, N s ] to do intervention with. As for a causal subgraph G c with representation H c , we define the representation under an intervention as:</p><formula xml:id="formula_14">do(S = G sj ) : H vj = (1 -µ) • H c + µ • H sj , j ∈ [1, N s ],<label>(10)</label></formula><p>in which µ ∈ (0, 1) is the hyper-parameter to control the intensity of an intervention.</p><p>Preprint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">INVARIANT ARCHITECTURE CUSTOMIZATION</head><p>After obtaining graph representations H c and H vj , j ∈ [1, N s ], we introduce the method to construct a specific GNN architecture from a graph representation on the basis of differentiable NAS <ref type="bibr" target="#b30">(Liu et al., 2018)</ref>.</p><p>Architecture customization. To begin with, we denote the space of operator candidates as O and the number of architecture layers as K. Then, the ultimate architecture A can be represented as a super-network:</p><formula xml:id="formula_15">g k (x) = |O| u=1 α k u o u (x), k ∈ [1, K],<label>(11)</label></formula><p>where x is the input to layer k, o u (•) is the operator from O, α k u is the mixture coefficient of operator o u (•) in layer k, and g k (x) is the output of layer k. Thereat, an architecture A can be represented as a matrix A ∈ R K×|O| , in which A k,u = α k u . We learn these coefficients from graph representation H via trainable prototype vectors</p><formula xml:id="formula_16">op k u (u ∈ [1, |O|], k ∈ [1, K]</formula><p>), of operators:</p><formula xml:id="formula_17">α k u = exp op k u T H |O| u ′ =1 exp op k u ′ T H . (<label>12</label></formula><formula xml:id="formula_18">)</formula><p>In addition, the regularizer for operator prototype vectors:</p><formula xml:id="formula_19">L op = k u,u ′ ∈[1,|O|],u̸ =u ′ cos(op k u , op k u ′ ),<label>(13)</label></formula><p>where cos(•, •) is the cosine distance between two vectors, is engaged to avoid the mode collapse, following the exploration in <ref type="bibr" target="#b39">(Qin et al., 2022)</ref>.</p><p>Architectures from causal subgraph and intervention graphs. So far we form the mapping of f A : G → A in Problem 2. As for an input graph G, we get its optimal architecture A c with the matrix A c based on its causal subgraph's representation H c through equation ( <ref type="formula" target="#formula_17">12</ref>), while for each intervention graph we have A vj based on H vj , j ∈ [1, N s ] similarly.</p><p>The customized architecture A c is used to produce the ultimate prediction of input graph G by f Y : G × A → Y in Problem 2, and we formulate the main classification loss as:</p><formula xml:id="formula_20">L pred = Ntr i=1 ℓ( Ŷi , Y i ), Ŷi = f Y (G i , A ci ).<label>(14)</label></formula><p>Furthermore, we regard each A vj , j ∈ [1, N s ] as an outcome when causal subgraph G c is in a specific environment (treating the intervened part, i.e. non-causal subgraphs, as different environments). Therefore, the following variance regularizer is proposed as a causal constraint to compel the inferred causal subgraph G c to have the steady ability to solely determine the optimal architecture for input graph instance G:</p><formula xml:id="formula_21">L arch = 1 N tr Ntr i=1 1 T • Var i • 1, Var i = var A vij , j ∈ [1, N s ],<label>(15)</label></formula><p>where var(•) calculates the variance of a set of matrix, 1 T • Var i • 1 represents the summation of elements in matrix Var i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">OPTIMIZATION FRAMEWORK</head><p>Up to now, we have introduced</p><formula xml:id="formula_22">f C : G - → G c in section 3.1, f A : G c - → A in section 3.2 and 3.3, f Y : G × A -→ Y in section 3.3,</formula><p>and whereby deal with Problem 2. To be specific, the overall objective function in equation ( <ref type="formula" target="#formula_3">2</ref>) is as below:</p><formula xml:id="formula_23">L all = σL pred + (1 -σ)L causal , L causal = L cpred + θ 1 L arch + θ 2 L op ,<label>(16)</label></formula><p>where θ 1 , θ 2 and σ are hyper-parameters. Additionally, we adopt a linearly growing σ p corresponding to the epoch number p as:</p><formula xml:id="formula_24">σ p = σ min + (p -1) σ max -σ min P , p ∈ [1, P ], (<label>17</label></formula><formula xml:id="formula_25">)</formula><p>where P is the maximum number of epochs. In this way, we can dynamically adjust the training key point in each epoch by focusing more on the causal-aware part (i.e. identifying suitable causal subgraph and learning vectors of operators) in the early stages and focusing more on the performance of the customized super-network in the later stages. We show the dynamic training process and how σ p improve the training and convergence efficiency in Appendix E.3. The overall framework and optimization procedure of the proposed CARNAS are summarized in Figure <ref type="figure" target="#fig_0">1</ref> and Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>In this section, we present the comprehensive results of our experiments on both synthetic and real-world datasets to validate the effectiveness of our approach. We also conduct a series of ablation studies to thoroughly examine the contribution of the components within our framework. More analysis on experimental results, training efficiency, complexity and sensitivity are in Appendix E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">EXPERIMENT SETTING</head><p>Setting. To ensure reliability and reproducibility, we execute each experiment ten times using distinct random seeds and present the average results along with their standard deviations. We do not employ validation dataset for conducting architecture search. The configuration and use of datasets in our method align with those in other GNN methods, ensuring fairness across all approaches. Baselines. We compare our model with 12 baselines from the following two different categories:</p><p>• Manually design GNNs. We incorporate widely recognized architectures GCN (Kipf and Welling, 2022), GAT <ref type="bibr" target="#b47">(Velickovic et al., 2017)</ref>, GIN <ref type="bibr" target="#b57">(Xu et al., 2018)</ref>, SAGE <ref type="bibr" target="#b14">(Hamilton et al., 2017)</ref>, and GraphConv <ref type="bibr" target="#b35">(Morris et al., 2019)</ref>, and MLP into our search space as candidate operators as well as baseline methods in our experiments. Apart from that, we include two recent advancements: ASAP <ref type="bibr" target="#b40">(Ranjan et al., 2020)</ref> and DIR <ref type="bibr">(Wu et al., 2021b)</ref>, which is specifically proposed for out-ofdistribution generalization. • Graph Neural Architecture Search. For classic NAS, we compare with DARTS <ref type="bibr" target="#b30">(Liu et al., 2018)</ref>, a differentiable architecture search method, and random search. For graph NAS, we explore a reinforcement learning-based method GNAS <ref type="bibr" target="#b10">(Gao et al., 2021)</ref>, and PAS <ref type="bibr" target="#b49">(Wei et al., 2021)</ref> that is specially designed for graph classification tasks. Additionally, we compare two state-of-the-art graph NAS methods that are specially designed for non-i.i.d. graph datasets, including GRACES <ref type="bibr" target="#b39">(Qin et al., 2022)</ref> and DCGAS <ref type="bibr" target="#b59">(Yao et al., 2024)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">ON SYNTHETIC DATASETS</head><p>Datasets. The synthetic dataset, Spurious-Motif <ref type="bibr" target="#b39">(Qin et al., 2022;</ref><ref type="bibr">Wu et al., 2021b;</ref><ref type="bibr" target="#b61">Ying et al., 2019)</ref>, encompasses 18,000 graphs, each uniquely formed by combining a base shape (denoted as Preprint.</p><p>Tree, Ladder, or Wheel with S = 0, 1, 2) with a motif shape (represented as Cycle, House, or Crane with C = 0, 1, 2). Notably, the classification of each graph relies solely on its motif shape, despite the base shape typically being larger. This dataset is particularly designed to study the effect of distribution shifts, with a distinct bias introduced solely on the training set through the probability distribution</p><formula xml:id="formula_26">P (S) = b × I(S = C) + 1-b 2 × I(S ̸ = C)</formula><p>, where b modulates the correlation between base and motif shapes, thereby inducing a deliberate shift between the training set and testing set, where all base and motif shapes are independent with equal probabilities. We choose b = 0.7/0.8/0.9, enabling us to explore our model's performance under various significant distributional variations. The effectiveness of our approach is measured using accuracy as the evaluation metric on this dataset.</p><p>Results. Table <ref type="table" target="#tab_0">1</ref> presents the experimental results on three synthetic datasets, revealing that our model significantly outperforms all baseline models across different scenarios.</p><p>Specifically, we observe that the performance of all GNN models is particularly poor, suggesting their sensitivity to spurious correlations and their inability to adapt to distribution shifts. However, DIR <ref type="bibr">(Wu et al., 2021b)</ref>, designed specifically for non-I.I.D. datasets and focusing on discovering invariant rationale to enhance generalizability, shows pretty well performance compared to most of the other GNN models. This reflects the feasibility of employing causal learning to tackle generalization issues.</p><p>Moreover, NAS methods generally yield slightly better outcomes than manually designed GNNs in most scenarios, emphasizing the significance of automating architecture by learning the correlations between input graph data and architecture to search for the optimal GNN architecture. Notably, methods specifically designed for non-I.I.D. datasets, such as GRACES <ref type="bibr" target="#b39">(Qin et al., 2022)</ref>, DCGAS <ref type="bibr" target="#b59">(Yao et al., 2024)</ref>, and our CARNAS, exhibit significantly less susceptibility to distribution shifts compared to NAS methods intended for I.I.D. data. Among these, our approach consistently achieves the best performance across datasets with various degrees of shifts, demonstrating the effectiveness of our method in enhancing Graph NAS performance, especially in terms of out-of-distribution generalization, which is attained by effectively capturing causal invariant subgraphs to guide the architecture search process, and filtering out spurious correlations meanwhile. Datasets. The real-world datasets OGBG-Mol*, including Ogbgmolhiv, Ogbg-molbace, and Ogbgmolsider <ref type="bibr" target="#b16">(Hu et al., 2020;</ref><ref type="bibr" target="#b56">Wu et al., 2018)</ref>, feature 41127, 1513, and 1427 molecule graphs, respectively, aimed at molecular property prediction. The division of the datasets is based on scaffold values, designed to segregate molecules according to their structural frameworks, thus introducing a significant challenge to the prediction of graph properties. The predictive performance of our approach across these diverse molecular structures and properties is measured using ROC-AUC as the evaluation metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">ON REAL-WORLD DATASETS</head><p>Results. Results from real-world datasets are detailed in Table <ref type="table" target="#tab_1">2</ref>, where our CARNAS model once again surpasses all baselines across three distinct datasets, showcasing its ability to handle complex distribution shifts under various conditions.</p><p>For manually designed GNNs, the optimal model varies across different datasets: GIN achieves the best performance on Ogbg-molhiv, GCN excels on Ogbg-molsider, and GraphConv leads on Ogbg-molbace. This diversity in performance confirms a crucial hypothesis in our work, that different GNN models are predisposed to perform well on graphs featuring distinct characteristics.</p><p>In the realm of NAS models, we observe that DARTS and PAS, proposed for I.I.D. datasets, perform comparably to manually crafted GNNs, whereas GRACES, DCGAS and our CARNAS, specifically designed for non-I.I.D. datasets outshine other baselines. Our approach reaches the top performance across all datasets, with a particularly remarkable breakthrough on Ogbg-molsider, highlighting our method's superior capability in adapting to and excelling within diverse data environments. Besides, the average and standard deviations of the best-performed baseline on each dataset are denoted as the dark and light thick dash lines respectively.</p><p>Results. From Figure <ref type="figure">2</ref>, we have the following observations. First of all, our proposed CARNAS outperforms all the variants as well as the best-performed baseline on all datasets, demonstrating the effectiveness of each component of our proposed method. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">RELATED WORK</head><p>Neural Architecture Search (NAS) automates creating optimal neural networks using RL-based <ref type="bibr" target="#b74">(Zoph and Le, 2016)</ref>, evolutionary <ref type="bibr" target="#b41">(Real et al., 2017)</ref>, and gradient-based methods <ref type="bibr" target="#b30">(Liu et al., 2018)</ref>. GraphNAS <ref type="bibr" target="#b10">(Gao et al., 2021)</ref> inspired studies on GNN architectures for graph classification in various datasets <ref type="bibr" target="#b39">(Qin et al., 2022;</ref><ref type="bibr" target="#b59">Yao et al., 2024)</ref>. Real-world data differences between training and testing sets impact GNN performance <ref type="bibr" target="#b44">(Shen et al., 2021;</ref><ref type="bibr">Li et al., 2022b;</ref><ref type="bibr">Zhang et al., 2023a;</ref><ref type="bibr">2022c)</ref>. Studies <ref type="bibr">(Li et al., 2022c;</ref><ref type="bibr" target="#b8">Fan et al., 2022)</ref> identify invariant subgraphs to mitigate this, usually with fixed GNN encoders. Our method automates designing generalized graph architectures by discovering causal relationships. Causal learning explores variable interconnections <ref type="bibr" target="#b37">(Pearl et al., 2016)</ref>, enhancing deep learning <ref type="bibr" target="#b63">(Zhang et al., 2020)</ref>. In graphs, it includes interventions on noncausal components <ref type="bibr">(Wu et al., 2022c)</ref>, causal and bias subgraph decomposition <ref type="bibr" target="#b8">(Fan et al., 2022)</ref>, and ensuring out-of-distribution generalization <ref type="bibr" target="#b5">(Chen et al., 2022;</ref><ref type="bibr">Zhang et al., 2023b;</ref><ref type="bibr">c)</ref>. These methods use fixed GNN architectures, while we address distribution shifts by discovering causal relationships between graphs and architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper, we address distribution shifts in graph neural architecture search (Graph NAS) from a causal perspective. Existing methods struggle with distribution shifts between training and testing sets due to spurious correlations. To mitigate this, we introduce Causal-aware Graph Neural Architecture Search (CARNAS). Our approach identifies stable causal structures and their relationships with architectures. We propose three key modules: Disentangled Causal Subgraph Identification, Graph Embedding Intervention, and Invariant Architecture Customization. These modules leverage causal relationships to search for generalized graph neural architectures. Experiments on synthetic and realworld datasets show that CARNAS achieves superior out-of-distribution generalization, highlighting the importance of causal awareness in Graph NAS.</p><p>Preprint.</p><p>A NOTATION </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B ALGORITHM</head><p>The overall framework and optimization procedure of the proposed CARNAS are summarized in Figure <ref type="figure" target="#fig_0">1</ref> and Algorithm 1, respectively. Algorithm 1 The overall algorithm of CARNAS Require: Training Dataset G tr , Hyper-parameters t in Eq. ( <ref type="formula" target="#formula_8">6</ref>), µ in Eq. ( <ref type="formula" target="#formula_14">10</ref>), θ 1 , θ 2 in Eq. ( <ref type="formula" target="#formula_23">16</ref>) 1: Initialize all trainable parameters 2: for p = 1, . . . , P do 3:</p><p>Set σ p as Eq. ( <ref type="formula" target="#formula_24">17</ref>) 4:</p><p>Derive causal and non-causal subgraphs as Eq. (4) (5) (6) 5:</p><p>Calculate graph representations of causal and non-causal subgraphs as Eq. ( <ref type="formula" target="#formula_9">7</ref> Calculate L op using Eq. ( <ref type="formula" target="#formula_19">13</ref>) 13: Calculate L pred using Eq. (11) (14) 14:</p><p>Calculate L arch using Eq. ( <ref type="formula" target="#formula_21">15</ref>)) 15:</p><p>Calculate the overall loss L all using Eq. ( <ref type="formula" target="#formula_23">16</ref>) 16:</p><p>Update parameters using gradient descends 17: end for</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C THEORETICAL ANALYSIS</head><p>In this section, in order to more rigorously establish our method, we provide a theoretical analysis about the problem of identifying and leveraging causal graph-architecture relationship to find the optimal architecture. Preprint.</p><p>To begin with, since causal relationships are, by definition, invariant across environments, we make the below assumption on our causal invariant subgraph generator f C (G) = G c : G -→ G c , following previous literature on invariant learning <ref type="bibr" target="#b42">(Rojas-Carulla et al., 2018;</ref><ref type="bibr">Li et al., 2022c)</ref>.</p><p>Assumption 1 There exists an optimal causal invariant subgraph generator f C (G) satisfying: Invariance assumption indicating that the subgraph generator f C (G) is capable of generating invariant subgraphs across different environments e, e ′ ∈ supp(E), where E is a random variable of all environments. This ensures that the conditional distribution P (A * |f C (G)) remains consistent and unaffected by the environment. Sufficiency assumption demonstrates that the subgraph generated by f C (G) has sufficient expressive power to enable prediction of the optimal architecture A * . This is achieved through f A (•), customizing the GNN architecture from a graph, while the added random noise ϵ is independent of the graph G.</p><p>Then, how can we get the optimal causal invariant subgraph generator? Following previous work <ref type="bibr">(Li et al., 2022c)</ref>, we can prove that it can be obtain through maximizing I(A * ; f C (G)), i.e. the mutual information between optimal architecture and the generated subgraph.</p><p>Theorem 1 (Optimal Generator of Causal Subgraphs) A generator f C (G) is the optimal generator that satisfies Assumption 1 if and only if it is the maximal causal subgraph generator, i.e.,</p><formula xml:id="formula_27">f * C = arg max f C ∈F E I(A * ; f C (G)),<label>(18)</label></formula><p>where F E is the subgraph generator set with related to the random vector of all environments, and I(•; •) is the mutual information between the optimal architecture A * and the generated causal subgraph.</p><p>Proof. Let fC = arg max f C ∈F E I(A * ; f C (G)). From the invariance property in Assumption 1, it follows that f * C ∈ F E . To prove the theorem, we show that: I(A * ; fC (G)) ≤ I(A * ; f * C (G)), which implies fC = f * C . Using the functional representation lemma <ref type="bibr" target="#b7">(El Gamal and Kim, 2011)</ref>, any random variable X 2 can be expressed as a function of another random variable X 1 and an independent random variable X 3 . Applying this to f * C (G) and fC (G), there exists a</p><formula xml:id="formula_28">f ′ C (G) such that f ′ C (G) ⊥ f * C (G) and fC (G) = γ(f * C (G), f ′ C (G))</formula><p>, where γ(•) is a deterministic function. Then, the mutual information can be decomposed as follows:</p><formula xml:id="formula_29">I(A * ; fC (G)) = I(A * ; γ(f * C (G), f ′ C (G))) ≤ I(A * ; f * C (G), f ′ C (G)) = I(f A (f * C (G)); f * C (G), f ′ C (G)) = I(f A (f * C (G)); f * C (G)) = I(A * ; f * C (G)),<label>(19)</label></formula><p>which completes the proof.</p><p>Since maximizing I(A * ; f C (G)) is difficult, we transform it into anther way which will be introduced in later passage.</p><p>Next, we show the theorem that guide us to optimize the model, represented as Q, that can construct optimal architecture A * for graph instance G under distribution shifts. The prediction is based on the causal subgraph G * c , which delineates the causal graph-architecture relationship. We denote the conditional distribution modeled by Q as q(A * |G * c ).</p><p>Theorem 2 Let f * C be the optimal causal invariant subgraph generator from Assumption 1, and let</p><formula xml:id="formula_30">G * c = f * C (G) and G * s = G \ G * c .</formula><p>Then, we can get the optimal model Q under distribution shifts by minimizing the following objective:</p><formula xml:id="formula_31">min E log p(A * |G * c ) q(A * |G * c ) + I(G * s ; A * |G * c ).<label>(20)</label></formula><p>Preprint.</p><p>Here, I(G * s ; A * |G * c ) quantifies the spurious correlation between G * s and A * , which the model need to ignore, and the first term ensures that q(A * |G * c ) closely matches p(A * |G * c ).</p><p>Proof.</p><p>From the sufficiency assumption of f * C in Assumption 1, we know that:</p><formula xml:id="formula_32">A * = f A (f C (G)) + ϵ</formula><p>, where ϵ ⊥ G. This implies that A * is conditionally independent of G * s (i.e., the non-causal subgraph) given G * c . Therefore, the full graph G = (G * c , G * s ) satisfies:</p><formula xml:id="formula_33">P (A * |G) = P (A * |G * c ).<label>(21)</label></formula><p>Additionally, by the invariance property, for any e, e ′ ∈ supp(E), the conditional distribution of A * given G * c remains invariant across environments: P e (A * |G * c ) = P e ′ (A * |G * c ). This invariance guarantees that Q will generalize well under distribution shifts caused by changes in the environment, when q(A * |G * c ) approximates the stable p(A * |G * c ). To approximate p(A * |G * c ) with q(A * |G * c ), we minimize the negative conditional log-likelihood of the observed data:</p><formula xml:id="formula_34">-ℓ = - n i=1 log q(A * i |G * ci ).<label>(22)</label></formula><p>Expanding this objective using G = (G * c , G * s ), we rewrite it as:</p><formula xml:id="formula_35">-ℓ = n i=1 log p(A * i |G * ci ) q(A * i |G * ci ) + n i=1 log p(A * i |G i ) p(A * i |G * ci ) - n i=1 log p(A * i |G i ) (23) = E log p(A * |G * c ) q(A * |G * c ) + E log p(A * |G) p(A * |G * c ) -E [log p(A * |G)] .<label>(24)</label></formula><p>The third term is irreducible constant inherent in the dataset, so we omit it when optimizing. Then, we decompose G into (G * c , G * s ) and rewrite the second term as:</p><formula xml:id="formula_36">E log p(A * |G) p(A * |G * c ) = E log p(A * |G * c , G * s ) p(A * |G * c ) (25) = n i=1 p(G * ci , G * si , A * i ) log p(A * i |G * ci , G * si ) p(A * i |G * ci ) (26) = n i=1 p(G * ci , G * si , A * i ) log p(A * i |G * ci , G * si )p(G * si |G * ci ) p(A * i |G * ci )p(G * si |G * ci ) (27) = n i=1 p(G * ci , G * si , A * i ) log p(G * si , A * i |G * ci ) p(A * i |G * ci )p(G * si |G * ci ) (28) = I(G * s ; A * |G * c ).<label>(29)</label></formula><p>Thus, the final objective to optimize q(A * |G * c ) is:</p><formula xml:id="formula_37">min E log p(A * |G * c ) q(A * |G * c ) + I(G * s ; A * |G * c ),<label>(30)</label></formula><p>where the second term, I(G * s ; A * |G * c ), measures the residual spurious correlation between G * s and A * given G * c . This concludes the proof.</p><p>However, this objective is challenging to optimize directly in practice. To address this, we analyze each term intuitively and explain how our method is derived from the theorem. (Equation <ref type="formula" target="#formula_20">14</ref>), which measures the loss between the ground-truth label and the prediction from the learned optimal architecture A * /A c . This surrogate loss guides q(A * |G * c ) to approximate p(A * |G * c ), as A * is inherently tied to the optimal predictive performance on final task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The first term, E log</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The second term I(G *</head><p>s ; A * |G * c ) represents the conditional mutual information between the optimal architecture A * and the spurious subgraph G * s , given the causal subgraph G * c . Minimizing this term encourages the model to reduce its reliance on the spurious subgraph G * s when predicting the optimal architecture, given G * c . This motivates the use of L arch in Equation <ref type="formula" target="#formula_21">15</ref>, which measures the variance of simulated architectures corresponding to intervention graphs formed by combining the causal subgraph with different spurious subgraphs. By reducing this variance, the model is encouraged to rely solely on the causal subgraph G * c for determining the optimal architecture, ensuring that the causal subgraph has a stable and consistent predictive capability across varying spurious components in input graph G. Then, we prove that</p><formula xml:id="formula_38">I(G * s ; A * |G * c ) = I(G; A * ) -I(G * c ; A * ): Proof.</formula><p>By the chain rule of mutual information, we have</p><formula xml:id="formula_39">I(G; A * ) = I(G * c , G * s ; A * ) = I(G * c ; A * ) + I(G * s ; A * |G * c ),<label>(31)</label></formula><p>where</p><formula xml:id="formula_40">G = (G * c , G * s ).</formula><p>Rearranging the equation, we obtain</p><formula xml:id="formula_41">I(G * s ; A * |G * c ) = I(G; A * ) -I(G * c ; A * ). (<label>32</label></formula><formula xml:id="formula_42">)</formula><p>Thus, minimizing I(G * s ; A * |G * c ) in turn encourages maximizing I(A * ; f C (G)), which proved to lead to optimizing the causal subgraph generator in Theorem 1.</p><p>Therefore, we propose to jointly optimize causal graph-architecture relationship and architecture search by offering an end-to-end training strategy for extracting and utilizing causal relationships between graph data and architecture, which is stable under distribution shifts, during the architecture search process, thereby enhancing the model's capability of OOD generalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D REPRODUCIBILITY DETAILS D.1 DEFINITION OF SEARCH SPACE</head><p>The number of layers in our model is predetermined before training, and the type of operator for each layer can be selected from our defined operator search space O. We incorporate widely recognized architectures GCN, GAT, GIN, SAGE, GraphConv, and MLP into our search space as candidate operators in our experiments. This allows for the combination of various sub-architectures within a single model, such as using GCN in the first layer and GAT in the second layer. Furthermore, we consistently use standard global mean pooling at the end of the GNN architecture to generate a global embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.2 DATASETS DETAILS</head><p>We utilize synthetic SPMotif datasets, which are characterized by three distinct degrees of distribution shifts, and three different real-world datasets, each with varied components, following previous works <ref type="bibr" target="#b39">(Qin et al., 2022;</ref><ref type="bibr" target="#b59">Yao et al., 2024;</ref><ref type="bibr">Wu et al., 2021b)</ref>. Based on the statistics of each dataset as shown in Table <ref type="table" target="#tab_4">4</ref>, we conducted a comprehensive comparison across various scales and graph sizes. This approach has empirically validated the scalability of our model. Detailed description for real-world datasets The real-world datasets are 3 molecular property prediction datasets in OGB <ref type="bibr" target="#b16">(Hu et al., 2020)</ref>, and are adopted from the MoleculeNet <ref type="bibr" target="#b56">(Wu et al., 2018)</ref>. Each graph represents a molecule, where nodes are atoms, and edges are chemical bonds.</p><p>• The HIV dataset was introduced by the Drug Therapeutics Program (DTP) AIDS Antiviral Screen, which tested the ability to inhibit HIV replication for over 40000 compounds. Screening results were evaluated and placed into 2 categories: inactive (confirmed inactive CI) and active (confirmed active CA and confirmed moderately active CM).</p><p>• The Side Effect Resource (SIDER) is a database of marketed drugs and adverse drug reactions (ADR). The version of the SIDER dataset in DeepChem has grouped drug side-effects into 27 system organ classes following MedDRA classifications measured for 1427 approved drugs (following previous usage).</p><p>• The BACE dataset provides quantitative (IC 50 ) and qualitative (binary label) binding results for a set of inhibitors of human β-secretase 1 (BACE-1). It merged a collection of 1522 compounds with their 2D structures and binary labels in MoleculeNet, built as a classification task.</p><p>The division of the datasets is based on scaffold values, designed to segregate molecules according to their structural frameworks, thus introducing a significant challenge to the prediction of graph properties.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D.3 DETAILED HYPER-PARAMETER SETTINGS</head><p>We fix the number of latent features Q = 4 in Eq. ( <ref type="formula" target="#formula_5">4</ref>), number of intervention candidates N s as batch size in Eq. ( <ref type="formula" target="#formula_14">10</ref>), σ min = 0.1, σ max = 0.7, P = 100 in Eq. ( <ref type="formula" target="#formula_24">17</ref>), and the tuned hyper-parameters for each dataset are as in Table <ref type="table">5</ref>. Table <ref type="table">5</ref>: Hyper-parameter settings Dataset t in Eq. ( <ref type="formula" target="#formula_8">6</ref>) µ in Eq. ( <ref type="formula" target="#formula_14">10</ref>) θ 1 in Eq. ( <ref type="formula" target="#formula_23">16</ref>) θ 2 in Eq. ( <ref type="formula" target="#formula_23">16</ref>) SPMotif-0.7/0.8/0.9 We compare the following ablated variants of our model in Section 4.4:</p><p>• 'CARNAS w/o L arch ' removes L arch from the overall loss in Eq. ( <ref type="formula" target="#formula_23">16</ref>). In this way, the contribution of the graph embedding intervention module together with the invariant architecture customization module to improve generalization performance by restricting the causally invariant nature for constructing architectures of the causal subgraph is removed.</p><p>• 'CARNAS w/o L cpred ' removes L cpred , thereby relieving the supervised restriction on causal subgraphs for encapsulating sufficient graph features, which is contributed by disentangled causal subgraph identification module together with the graph embedding intervention module to enhance the learning of causal subgraphs.</p><p>• 'CARNAS w/o L arch &amp; L cpred ' further removes both of them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E DEEPER ANALYSIS E.1 SUPPLEMENTARY ANALYSIS OF THE EXPERIMENTAL RESULTS</head><p>Sythetic datasets. We notice that the performance of CARNAS is way better than DIR <ref type="bibr">(Wu et al., 2021b)</ref>, which also introduces causality in their method, on synthetic datasets. We provide an explanation as follows: Our approach differs from and enhances upon DIR in several key points. Firstly, unlike DIR, which uses normal GNN layers for embedding nodes and edges to derive a causal subgraph, we employ disentangled GNN. This allows for more effective capture of latent features when extracting causal subgraphs. Secondly, while DIR focuses on the causal relationship between a graph instance and its label, our study delves into the causal relationship between a graph instance and its optimal architecture, subsequently using this architecture to predict the label. Additionally, we incorporate NAS method, introducing an invariant architecture customization module, which considers the impact of architecture on performance. Based on these advancements, our method may outperform DIR. Real-world datasets. We also notice that our methods improves a lot on the performance for the second real-world dataset SIDER. We further conduct an ablation study on SIDER to confirm that each proposed component contributes to its performance, as present in Figure <ref type="figure" target="#fig_5">3</ref>. The model 'w/o Larch' shows a slight decrease in performance, while 'w/o Lcpred' exhibits a substantial decline. This indicates that both restricting the invariance of the influence of the causal subgraph on the architecture via Larch, and ensuring that the causal subgraph retains crucial information from the input graph via Lcpred, are vital for achieving high performance on SIDER, especially the latter which empirically proves to be exceptionally effective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2 COMPLEXITY ANALYSIS</head><p>In this section, we analyze the complexity of our proposed method in terms of its computational time and the quantity of parameters that require optimization. Let's denote by |V | the number of nodes in a graph, by |E| the number of edges, by |O| the size of search space, and by d the dimension of hidden representations within a traditional graph neural network (GNN) framework. In our approach, d 0 represents the dimension of the hidden representations within the identification network GNN 0 , d 1 represents the dimension of the hidden representations within the shared graph encoder GNN 1 , and d s denotes the dimension within the tailored super-network. Notably, d 0 encapsulates the combined dimension of Q chunks, meaning the dimension per chunk is d 0 /Q.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2.1 TIME COMPLEXITY ANALYSIS</head><p>For most message-passing GNNs, the computational time complexity is traditionally O(|E|d+|V |d 2 ). Following this framework, the GNN 0 in our model exhibits a time complexity of O(|E|d 0 + |V |d 2 0 ), and the GNN 1 in our model exhibits a time complexity of O(|E|d 1 + |V |d 2 1 ). The most computationally intensive operation in the invariant architecture customization module, which involves the computation of L op , leads to a time complexity of O(|O| 2 d 1 ). The time complexity attributed to the customized super-network is O(|O|(|E|d s + |V |d 2 s )). Consequently, the aggregate time complexity of our method can be summarized as</p><formula xml:id="formula_43">O(|E|(d 0 + d 1 + |O|d s ) + |V |(d 2 0 + d 2 1 + |O|d 2 s ) + |O| 2 d 1 ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.2.2 PARAMETER COMPLEXITY ANALYSIS</head><p>A typical message-passing GNN has a parameter complexity of O(d 2 ). In our architecture, the disentangled causal subgraph identification network GNN 0 possesses O(d 2 0 ) parameters, the shared GNN encoder GNN 1 possesses O(d 2 1 ), the invariant architecture customization module contains O(|O|d 1 ) parameters and the customized super-network is characterized by O(|O|d 2 s ) parameters. Therefore, the total parameter complexity in our framework is expressed as</p><formula xml:id="formula_44">O(d 2 0 + d 2 1 + |O|d 1 + |O|d 2 s ).</formula><p>The analyses underscore that the proposed method scales linearly with the number of nodes and edges in the graph and maintains a constant number of learnable parameters, aligning it with the efficiency of prior GNN and graph NAS methodologies. Moreover, given that |O| typically represents a modest constant (for example, |O| = 6 in our search space) and that d 0 and d 1 is generally much less than d s , the computational and parameter complexities are predominantly influenced by d s . To ensure equitable comparisons with existing GNN baselines, we calibrate d s within our model such that the  For a deeper understanding of our model training process, and further remark the impact of the dynamic σ p in Eq.( <ref type="formula" target="#formula_24">17</ref>), we conduct experiments and compare the training process in the following settings:</p><p>• 'with Dynamic σ' means we use the dynamic σ p in Eq.( <ref type="formula" target="#formula_24">17</ref>) to adjust the training key point in each epoch.</p><p>• 'w/o Dynamic σ' means we fix the σ in Eq.( <ref type="formula" target="#formula_23">16</ref>) as a constant value σmax+σmin 2 .</p><p>According to Figure <ref type="figure" target="#fig_7">4</ref>, our method can converge rapidly in 10 epochs. Figure <ref type="figure" target="#fig_7">4</ref> also obviously reflects that after 10 epochs the validation loss with dynamic σ keeps declining and its accuracy continuously rising. However, in the setting without dynamic σ, the validation loss may rise again, and accuracy cannot continue to improve.</p><p>These results verify our aim to adopt this σ p to elevate the efficiency of model training in the way of dynamically adjusting the training key point in each epoch by focusing more on the causal-aware part (i.e. identifying suitable causal subgraph and learning vectors of operators) in the early stages and focusing more on the performance of the customized super-network in the later stages. We also empirically confirm that our method is not complex to train in Appendix E.4.</p><p>Furthermore, we report both the training loss and validation loss for the two components (Lcausal, representing the causal-aware part, and Lpred, representing the customized super-network optimization as defined in Equation <ref type="formula" target="#formula_23">16</ref>) with and without the dynamic σ schedule in Figure <ref type="figure" target="#fig_9">5</ref>.  For the training loss, L pred decreases more steadily and reaches a lower value with less fluctuation under the dynamic schedule. In terms of validation loss, L pred with the dynamic schedule decreases significantly in later stages, whereas without it, L pred struggles to converge. Additionally, L causal without the dynamic schedule exhibits a slight initial increase before decreasing, whereas with the dynamic schedule, it decreases smoothly from the outset. These results indicate that the dynamic schedule effectively adjusts the training focus during each epoch. It emphasizes the causal-aware part (i.e., identifying suitable causal subgraphs and learning operator vectors) in the early stages and shifts focus to the customized super-network performance in later stages.</p><p>Preprint.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E.4 TRAINING EFFICIENCY</head><p>To further illustrate the efficiency of CARNAS, we provide a direct comparison with the bestperformed NAS baseline, DCGAS, based on the total runtime for 100 epochs. As shown in Table <ref type="table" target="#tab_6">6</ref>, CARNAS consistently requires less time across different datasets while achieving superior best performance, demonstrating its enhanced efficiency and effectiveness.  We empirically observe that our model is insensitive to most hyper-parameters, which remain fixed throughout our experiments. Consequently, the number of parameters requiring tuning in practice is relatively small. t, µ, θ 1 and θ 2 have shown more sensitivity, prompting us to focus our tuning efforts on these 4 hyper-parameters.</p><p>Therefore, we conduct sensitivity analysis (on BACE) for the 4 important hyper-parameters, as shown Figure <ref type="figure" target="#fig_10">6</ref>. The value selection for these parameters were deliberately varied evenly within a defined range to assess sensitivity thoroughly. The specific hyper-parameter settings used for the CARNAS reported in Table <ref type="table" target="#tab_1">2</ref> are more finely tuned and demonstrate superior performance to the also finely tuned other baselines. The sensitivity allows for potential performance improvements through careful parameter tuning, and our results in sensitivity analysis outperform most baseline methods, indicating a degree of stability and robustness in response to these hyper-parameters.</p><p>Mention that, the best performance of the fine-tuned DCGAS may exceed the performance of our method without fine-tuning sometimes. This is because, DCGAS addresses challenge of out-ofdistribution generalization through data augmentation, generating a sufficient quantity of graphs for training. In contrast, CARNAS focuses on capturing and utilizing causal and stable subparts to guide the architecture search process. The methodological differences and the resulting disparity in the volume of data used could also contribute to the performance variations observed.</p><p>Limitation. Although the training time and search efficiency of our method is comparable to most of the Graph NAS methods, we admit that it is less efficient than standard GNNs. At the same time, in order to obtain the best performance for a certain application scenario, our method does need to fine-tune four sensitive hyper-parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F MORE COMPARISON WITH OOD GNN</head><p>In our initial experiment, we compared our model with two non-NAS-based graph OOD methods, ASAP and DIR. We expanded our evaluation to include 13 well-known non-NAS-based graph OOD methods, providing a comprehensive comparison. The results, presented in Table <ref type="table">7</ref>, demonstrate that CARNAS not only performs well among NAS-based methods but also significantly outperforms non-NAS graph OOD methods. This superior performance is attributed to CARNAS's ability to  We observe that different motif shapes indeed prefer different architectures, e.g., graphs with cycle prefer GAT in the third layer, while this operator is seldomly chosen in neither layer of the other two types of graphs; the operator distributions are similar for graphs with cycle and house in the first layer, but differ in other layers. To be specific, Motif-Cycle is characterized by a closed-loop structure where each node is connected to two neighbors, displaying both symmetry and periodicity.</p><p>For graphs with this motif, CARNAS identifies SAGE-GCN-GAT as the most suitable architecture. Motif-House, on the other hand, features a combination of triangular and quadrilateral structures, introducing a certain level of hierarchy and asymmetry. For graphs with this shape, CARNAS determines that GIN-MLP-GCN is the optimal configuration. Lastly, Motif-Crane presents more complex cross-connections between nodes compared to the previous two motifs, and CARNAS optimally configures graphs with it with a GIN-SAGE-GCN architecture.</p><p>By effectively integrating various operations and customizing specific architectures for different causal subparts (motifs) with diverse features, our NAS-based CARNAS can further improve the OOD generalization.</p><p>To better illustrate the learned graph-architecture relationship, we also visualize the causal subgraphs for each dataset in our case study in Figure <ref type="figure" target="#fig_12">8</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H RELATED WORK H.1 GRAPH NEURAL ARCHITECTURE SEARCH</head><p>In the rapidly evolving domain of automatic machine learning, Neural Architecture Search (NAS) represents a groundbreaking shift towards automating the discovery of optimal neural network architectures. This shift is significant, moving away from the traditional approach that heavily relies on manual expertise to craft models. NAS stands out by its capacity to autonomously identify architectures that are finely tuned for specific tasks, demonstrating superior performance over manually engineered counterparts. The exploration of NAS has led to the development of diverse strategies, including reinforcement learning (RL)-based approaches <ref type="bibr" target="#b74">(Zoph and Le, 2016;</ref><ref type="bibr" target="#b17">Jaafra et al., 2019)</ref>, evolutionary algorithms-based techniques <ref type="bibr" target="#b41">(Real et al., 2017;</ref><ref type="bibr" target="#b31">Liu et al., 2021)</ref>, and methods that leverage gradient information <ref type="bibr" target="#b30">(Liu et al., 2018;</ref><ref type="bibr" target="#b60">Ye et al., 2022)</ref>. Among these, graph neural architecture search has garnered considerable attention.</p><p>The pioneering work of GraphNAS <ref type="bibr" target="#b10">(Gao et al., 2021)</ref> introduced the use of RL for navigating the search space of graph neural network (GNN) architectures, incorporating successful designs from the GNN literature such as GCN, GAT, etc. This initiative has sparked a wave of research <ref type="bibr" target="#b10">(Gao et al., 2021;</ref><ref type="bibr" target="#b49">Wei et al., 2021;</ref><ref type="bibr" target="#b39">Qin et al., 2022;</ref><ref type="bibr" target="#b3">Cai et al., 2021;</ref><ref type="bibr" target="#b11">Guan et al., 2021;</ref><ref type="bibr">Zhang et al., 2023d;</ref><ref type="bibr" target="#b12">Guan et al., 2022)</ref>, leading to the discovery of innovative and effective architectures. Recent years have seen a broadening of focus within Graph NAS towards tackling graph classification tasks, which are particularly relevant for datasets comprised of graphs, such as those found in protein molecule studies. This research area has been enriched by investigations into graph classification on datasets that are either independently identically distributed <ref type="bibr" target="#b49">(Wei et al., 2021)</ref> or non-independently identically distributed, with GRACES <ref type="bibr" target="#b39">(Qin et al., 2022)</ref> and DCGAS <ref type="bibr" target="#b59">(Yao et al., 2024)</ref> being notable examples of the latter. Through these efforts, the field of NAS continues to expand its impact, offering tailored solutions across a wide range of applications and datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.2 GRAPH OUT-OF-DISTRIBUTION GENERALIZATION</head><p>In the realm of machine learning, a pervasive assumption posits the existence of identical distributions between training and testing data. However, real-world scenarios frequently challenge this assumption with inevitable shifts in distribution, presenting significant hurdles to model performance in out-ofdistribution (OOD) scenarios <ref type="bibr" target="#b44">(Shen et al., 2021;</ref><ref type="bibr">Zhang et al., 2023a;</ref><ref type="bibr">2022c)</ref>. The drastic deterioration in performance becomes evident when models lack robust OOD generalization capabilities, a concern particularly pertinent in the domain of Graph Neural Networks (GNNs), which have gained prominence within the graph community <ref type="bibr">(Li et al., 2022b)</ref>. Several noteworthy studies <ref type="bibr">(Wu et al., 2022b;</ref><ref type="bibr">a;</ref><ref type="bibr">Li et al., 2022c;</ref><ref type="bibr" target="#b8">Fan et al., 2022;</ref><ref type="bibr" target="#b45">Sui et al., 2022;</ref><ref type="bibr" target="#b29">Liu et al., 2022;</ref><ref type="bibr" target="#b46">Sui et al., 2023)</ref> have tackled this challenge by focusing on identifying environment-invariant subgraphs to mitigate distribution shifts. These approaches typically rely on pre-defined or dynamically generated environment labels from various training scenarios to discern variant information and facilitate the learning of invariant subgraphs. <ref type="bibr" target="#b50">(Wu et al., 2024;</ref><ref type="bibr" target="#b66">Zhang et al., 2024)</ref> have divided recent literature that solve the graph OOD generalization problem, into three categories: 1) Graph augmentation methods <ref type="bibr" target="#b73">(Zhao et al., 2021;</ref><ref type="bibr" target="#b43">Shen et al., 2023;</ref><ref type="bibr" target="#b9">Feng et al., 2020;</ref><ref type="bibr" target="#b62">You et al., 2020)</ref> enhance OOD generalization by increasing the quantity and diversity of training data through systematic graph modifications.</p><p>2) The second type of methods <ref type="bibr" target="#b33">(Ma et al., 2019;</ref><ref type="bibr">Li et al., 2022a)</ref> develop new graph models to learn OOD-generalized representations.</p><p>3)The third type of methods <ref type="bibr" target="#b67">(Zhang et al., 2021;</ref><ref type="bibr" target="#b15">Hu et al., 2019)</ref> enhance OOD generalization through tailored training schemes with specific objectives and constraints. There are various datasets and benchmarks <ref type="bibr" target="#b16">(Hu et al., 2020;</ref><ref type="bibr" target="#b36">Morris et al., 2020;</ref><ref type="bibr" target="#b13">Gui et al., 2022;</ref><ref type="bibr" target="#b19">Ji et al., 2022;</ref><ref type="bibr">2023)</ref> help for assessing generalizability and adaptability. Moreover, the existing methods usually adopt a fixed GNN encoder in the whole optimization process, neglecting the role of graph architectures in out-of-distribution generalization. In this paper, we focus on automating the design of Preprint.</p><p>generalized graph architectures by discovering causal relationships between graphs and architectures, and thus handle distribution shifts on graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H.3 CAUSAL LEARNING ON GRAPHS</head><p>The field of causal learning investigates the intricate connections between variables <ref type="bibr" target="#b37">(Pearl et al., 2016;</ref><ref type="bibr">Pearl, 2009)</ref>, offering profound insights that have significantly enhanced deep learning methodologies.</p><p>Leveraging causal relationships, numerous techniques have made remarkable strides across diverse computer vision applications <ref type="bibr" target="#b63">(Zhang et al., 2020;</ref><ref type="bibr" target="#b34">Mitrovic et al., 2020)</ref>. Additionally, recent research has delved into the realm of graphs <ref type="bibr">(Zhang et al., 2023b;</ref><ref type="bibr">c)</ref>. For instance, <ref type="bibr">Wu et al. (2022c)</ref> implements interventions on non-causal components to generate representations, facilitating the discovery of underlying graph rationales. Fan et al. ( <ref type="formula">2022</ref>) decomposes graphs into causal and bias subgraphs, mitigating dataset biases. <ref type="bibr">Li et al. (2022d)</ref> introduces invariance into self-supervised learning, preserving stable semantic information. <ref type="bibr" target="#b5">(Chen et al., 2022)</ref> ensures out-of-distribution generalization by capturing graph invariance. <ref type="bibr" target="#b18">(Jaber et al., 2020)</ref> tackled the challenge of learning causal graphs involving latent variables, which are derived from a mixture of observational and interventional distributions with unknown interventional objectives. To mitigate this issue, the study proposed an approach leveraging a Ψ-Markov property. <ref type="bibr" target="#b0">Addanki et al. (2020)</ref> introduced a randomized algorithm, featuring p-colliders, for recovering the complete causal graph while minimizing intervention costs. Additionally, <ref type="bibr" target="#b2">Brouillard et al. (2020)</ref> presented an adaptable method for causality detection, which notably benefits from various types of interventional data and incorporates sophisticated neural architectures such as normalizing flows, operating under continuous constraints. However, these methods adopt a fixed GNN architecture in the optimization process, neglecting the role of architectures in causal learning on graphs. In contrast, in this paper, we focus on handling distribution shifts in the graph architecture search process from the causal perspective by discovering the causal relationship between graphs and architectures. There are also works <ref type="bibr" target="#b48">(Wang et al., 2023;</ref><ref type="bibr" target="#b22">Kong et al., 2022;</ref><ref type="bibr">Zhang et al., 2022a;</ref><ref type="bibr">b)</ref> employ GNN for causality learning.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The framework of our proposed method CARNAS. As for an input graph G, the disentangled causal subgraph identification module abstracts its causal subgraph G c with disentangled GNN layers. Then, in the graph embedding intervention module, we conduct several interventions on G c with non-causal subgraphs in latent space and obtain L cpred from the embedding of G c in the meanwhile. After that, the invariant architecture customization module aims to deal with distribution shift by customizing architecture from G c to attain Ŷ , L pred , and form L arch , L op to further constrain the causal invariant property of G c . Blue lines present the prediction approach and grey lines show other processes in the training stage. Additionally, green lines denote the updating process.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Figure 2: Results of ablation studies on synthetic datasets, where 'w/o L arch ' removes L arch from the overall loss in Eq. (16), 'w/o L cpred ' removes L cpred , and 'w/o L arch &amp; L cpred ' removes both of them. The error bars report the standard deviations.Besides, the average and standard deviations of the best-performed baseline on each dataset are denoted as the dark and light thick dash lines respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>N s non-causal subgraphs as candidates 8: for causal subgraph G c of graph G in G tr do 9: Do interventions on G c in latent space as Eq. (10) 10: Calculate architecture matrix A c and {A vj } from causal subgraph and their intervention graphs as Eq.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>a.</head><label></label><figDesc>Invariance property: For all e, e ′ ∈ supp(E), P e (A * |f C (G)) = P e ′ (A * |f C (G)). b. Sufficiency property: A * = f A (f C (G)) + ϵ, where f A (•) customizes the GNN architecture from a graph, ϵ ⊥ G (indicating statistical independence), and ϵ is random noise.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>p(A * |G * c ) q(A * |G * c ) , ensures that the model accurately approximates the true conditional distribution p(A * |G * c ) based on the causal subgraph G * c . Since the optimal architecture A * is defined as the one achieving the best predictive performance on label Y , we indirectly optimize the first term E log p(A * |G * c ) q(A * |G * c ) by focusing on label's prediction performance. Specifically, we minimize L pred Preprint.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Results of ablation studies on SIDER, where 'w/o L arch ' removes L arch from the overall loss in Eq. (16), 'w/o L cpred ' removes L cpred , and 'w/o L arch &amp; L cpred ' removes both of them. The error bars report the standard deviations. Besides, the average and standard deviations of the best-performed baseline on each dataset are denoted as the dark and light thick dash lines respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Training process of synthetic datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Changes of the two parts of loss.For the training loss, L pred decreases more steadily and reaches a lower value with less fluctuation under the dynamic schedule. In terms of validation loss, L pred with the dynamic schedule decreases significantly in later stages, whereas without it, L pred struggles to converge. Additionally, L causal without the dynamic schedule exhibits a slight initial increase before decreasing, whereas with the dynamic schedule, it decreases smoothly from the outset. These results indicate that the dynamic schedule effectively adjusts the training focus during each epoch. It emphasizes the causal-aware part (i.e., identifying suitable causal subgraphs and learning operator vectors) in the early stages and shifts focus to the customized super-network performance in later stages.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 6 :</head><label>6</label><figDesc>Figure6: Hyper-parameters sensitivity analysis. The area shows the average ROC-AUC and standard deviations. The green, yellow, grey dashed lines represent the average performance corresponding to the fine-tuned hyper-parameters of CARNAS, best performed baseline DCGAS, 2nd best performed baseline GRACES, respectively. We empirically observe that our model is insensitive to most hyper-parameters, which remain fixed throughout our experiments. Consequently, the number of parameters requiring tuning in practice is relatively small. t, µ, θ 1 and θ 2 have shown more sensitivity, prompting us to focus our tuning efforts on these 4 hyper-parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Comparison of operation probabilities for graphs with different motif shapes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Visualization of edge importance for forming causal subgraphs in SP-Motif Dataset. Structures with deeper colors mean higher importance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table><row><cell>Method</cell><cell>b = 0.7</cell><cell>b = 0.8</cell><cell>b = 0.9</cell></row><row><cell>GCN</cell><cell>48.39 ±1.69</cell><cell>41.55 ±3.88</cell><cell>39.13 ±1.76</cell></row><row><cell>GAT</cell><cell>50.75 ±4.89</cell><cell>42.48 ±2.46</cell><cell>40.10 ±5.19</cell></row><row><cell>GIN</cell><cell>36.83 ±5.49</cell><cell>34.83 ±3.10</cell><cell>37.45 ±3.59</cell></row><row><cell>SAGE</cell><cell>46.66 ±2.51</cell><cell>44.50 ±5.79</cell><cell>44.79 ±4.83</cell></row><row><cell cols="2">GraphConv 47.29 ±1.95</cell><cell>44.67 ±5.88</cell><cell>44.82 ±4.84</cell></row><row><cell>MLP</cell><cell>48.27 ±1.27</cell><cell>46.73 ±3.48</cell><cell>46.41 ±2.34</cell></row><row><cell>ASAP</cell><cell cols="3">54.07 ±13.85 48.32 ±12.72 43.52 ±8.41</cell></row><row><cell>DIR</cell><cell>50.08 ±3.46</cell><cell>48.22 ±6.27</cell><cell>43.11 ±5.43</cell></row><row><cell>Random</cell><cell>45.92 ±4.29</cell><cell>51.72 ±5.38</cell><cell>45.89 ±5.09</cell></row><row><cell>DARTS</cell><cell>50.63 ±8.90</cell><cell>45.41 ±7.71</cell><cell>44.44 ±4.42</cell></row><row><cell>GNAS</cell><cell cols="3">55.18 ±18.62 51.64 ±19.22 37.56 ±5.43</cell></row><row><cell>PAS</cell><cell>52.15 ±4.35</cell><cell>43.12 ±5.95</cell><cell>39.84 ±1.67</cell></row><row><cell>GRACES</cell><cell cols="3">65.72 ±17.47 59.57 ±17.37 50.94 ±8.14</cell></row><row><cell>DCGAS</cell><cell>87.68 ±6.12</cell><cell cols="2">75.45 ±17.40 61.42 ±16.26</cell></row><row><cell>CARNAS</cell><cell>94.41 ±4.58</cell><cell cols="2">88.04 ±13.77 87.15 ±11.85</cell></row></table><note><p>The test accuracy of all methods on synthetic dataset Spurious-Motif. Values after ± denote the standard deviations. The best results overall are in bold and the best results of baselines in each category are underlined separately.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>The test ROC-AUC of all methods on real-world datasets OGBG-Mol*. Values after ± denote the standard deviations. The best results overall are in bold and the best results of baselines in each category are underlined separately.</figDesc><table><row><cell>Method</cell><cell>HIV</cell><cell>SIDER</cell><cell>BACE</cell></row><row><cell>GCN</cell><cell cols="2">75.99 ±1.19 59.84 ±1.54</cell><cell>68.93 ±6.95</cell></row><row><cell>GAT</cell><cell cols="2">76.80 ±0.58 57.40 ±2.01</cell><cell>75.34 ±2.36</cell></row><row><cell>GIN</cell><cell cols="2">77.07 ±1.49 57.57 ±1.56</cell><cell>73.46 ±5.24</cell></row><row><cell>SAGE</cell><cell cols="2">75.58 ±1.40 56.36 ±1.32</cell><cell>74.85 ±2.74</cell></row><row><cell cols="3">GraphConv 74.46 ±0.86 56.09 ±1.06</cell><cell>78.87 ±1.74</cell></row><row><cell>MLP</cell><cell cols="2">70.88 ±0.83 58.16 ±1.41</cell><cell>71.60 ±2.30</cell></row><row><cell>ASAP</cell><cell cols="2">73.81 ±1.17 55.77 ±1.18</cell><cell>71.55 ±2.74</cell></row><row><cell>DIR</cell><cell cols="2">77.05 ±0.57 57.34 ±0.36</cell><cell>76.03 ±2.20</cell></row><row><cell>DARTS</cell><cell cols="2">74.04 ±1.75 60.64 ±1.37</cell><cell>76.71 ±1.83</cell></row><row><cell>PAS</cell><cell cols="2">71.19 ±2.28 59.31 ±1.48</cell><cell>76.59 ±1.87</cell></row><row><cell>GRACES</cell><cell cols="2">77.31 ±1.00 61.85 ±2.58</cell><cell>79.46 ±3.04</cell></row><row><cell>DCGAS</cell><cell cols="2">78.04 ±0.71 63.46 ±1.42</cell><cell>81.31 ±1.94</cell></row><row><cell>CARNAS</cell><cell cols="3">78.33 ±0.64 83.36 ±0.62 81.73 ±2.92</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Important Notation Edge set of the causal subgraph and the non-causal subgraph H c , H s Graph-level representation of causal subgraph and non-causal subgraph Ŷci Prediction of graph G i 's causal subgraph G ci H v Mixture coefficient of operator o u (•) in layer k op k</figDesc><table><row><cell cols="2">Notation Meaning</cell></row><row><cell>G, Y</cell><cell>Graph space and Label space</cell></row><row><cell>G tr , G te</cell><cell>Training graph dataset, Testing graph dataset</cell></row><row><cell>G i , Y i</cell><cell>Graph instance i, Label of graph instance i</cell></row><row><cell>G c , G s</cell><cell>Causal subgraph and Non-causal subgraph</cell></row><row><cell>D</cell><cell>Adjacency matrix of graph G</cell></row><row><cell>Z</cell><cell>Node representations</cell></row><row><cell>S E</cell><cell>Edge importance scores</cell></row><row><cell>E c , E s</cell><cell></cell></row><row><cell></cell><cell>Intervened graph representation</cell></row><row><cell>O, o u (•)</cell><cell>Space of operator candidates, operator from O</cell></row><row><cell>K</cell><cell>Number of architecture layers</cell></row><row><cell>A</cell><cell>Search space of GNN architectures</cell></row><row><cell>A, A</cell><cell>An architecture represented as a super-network; matrix of architecture A</cell></row><row><cell>g k (x)</cell><cell>Output of layer k in the architecture.</cell></row><row><cell>α k u</cell><cell></cell></row></table><note><p><p>u</p>Trainable prototype vectors of operators</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Statistics for different datasets.</figDesc><table><row><cell></cell><cell cols="3">Graphs Avg. Nodes Avg. Edges</cell></row><row><cell>ogbg-molhiv</cell><cell>41127</cell><cell>25.5</cell><cell>27.5</cell></row><row><cell>ogbg-molsider</cell><cell>1427</cell><cell>33.6</cell><cell>35.4</cell></row><row><cell>ogbg-molbace</cell><cell>1513</cell><cell>34.1</cell><cell>36.9</cell></row><row><cell cols="2">SPMotif-0.7/0.8/0.9 18000</cell><cell>26.1</cell><cell>36.3</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 6 :</head><label>6</label><figDesc>Comparison of runtime</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Method</cell><cell cols="3">SPMotif</cell><cell cols="2">HIV</cell><cell cols="5">BACE SIDER</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">DCGAS</cell><cell cols="9">104 min 270 min 12 min 11 min</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="9">CARNAS 76 min 220 min 8 min</cell><cell></cell><cell>8 min</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="10">E.5 HYPER-PARAMETERS SENSITIVITY</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell>82</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>82</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>82</cell><cell></cell><cell></cell></row><row><cell></cell><cell>80</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>82</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>80</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>80</cell><cell></cell><cell></cell></row><row><cell>ROC-AUC (%)</cell><cell>74 76 78</cell><cell></cell><cell></cell><cell></cell><cell>ROC-AUC (%)</cell><cell>76 78 80</cell><cell></cell><cell></cell><cell></cell><cell>ROC-AUC (%)</cell><cell>72 74 76 78</cell><cell></cell><cell></cell><cell></cell><cell>ROC-AUC (%)</cell><cell>74 76 78</cell><cell></cell><cell></cell></row><row><cell></cell><cell>72</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>70</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>72</cell><cell></cell><cell></cell></row><row><cell></cell><cell>0.1</cell><cell>0.4</cell><cell>1</cell><cell>0.7</cell><cell>1.0</cell><cell>0.001</cell><cell>0.005</cell><cell>2</cell><cell>0.01</cell><cell>0.02</cell><cell>0.4</cell><cell>0.5</cell><cell>t</cell><cell>0.6</cell><cell>0.7</cell><cell>0.4</cell><cell>0.5</cell><cell>0.6</cell><cell>0.7</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 9 :</head><label>9</label><figDesc>Performance Comparison on Graph-SST2. All performances are reported under 100 epochs, except for those annotated. Time(Mins), Mem.(MiB).</figDesc><table><row><cell>Method</cell><cell>Acc</cell><cell cols="3">Time Mem. Method</cell><cell>Acc</cell><cell cols="2">Time Mem.</cell></row><row><cell>Coral</cell><cell>77.28 ± 1.98</cell><cell>62</cell><cell>4820</cell><cell>DANN</cell><cell>77.96 ± 3.50</cell><cell>38</cell><cell>4679</cell></row><row><cell>DIR</cell><cell>67.90 ± 10.08</cell><cell>171</cell><cell>4891</cell><cell>DIR (200 epochs)</cell><cell>79.19 ± 1.85</cell><cell>326</cell><cell>4891</cell></row><row><cell>GIL</cell><cell>75.53 ± 6.01</cell><cell>418</cell><cell>5628</cell><cell>GIL (200 epochs)</cell><cell>78.67 ± 1.48</cell><cell>816</cell><cell>5629</cell></row><row><cell>GSAT</cell><cell>78.79 ± 1.85</cell><cell>36</cell><cell>4734</cell><cell>Mixup</cell><cell>78.76 ± 2.00</cell><cell>31</cell><cell>4682</cell></row><row><cell>ERM</cell><cell>75.99 ± 3.25</cell><cell>29</cell><cell>4667</cell><cell>GroupDRO</cell><cell>76.97 ± 3.49</cell><cell>28</cell><cell>4695</cell></row><row><cell>IRM</cell><cell>78.12 ± 1.73</cell><cell>70</cell><cell>1389</cell><cell>VREx</cell><cell>79.62 ± 1.26</cell><cell>27</cell><cell>4692</cell></row><row><cell>CIGA</cell><cell>65.62 ± 7.87</cell><cell>157</cell><cell>4683</cell><cell cols="2">CIGA (200 epochs) 79.98 ± 1.61</cell><cell>306</cell><cell>4683</cell></row><row><cell>CARNAS</cell><cell>80.58 ± 1.72</cell><cell>199</cell><cell>2736</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Preprint. Regarding time and memory costs, Table <ref type="table">8</ref> and<ref type="table">9</ref> show that CARNAS is competitive with non-NASbased graph OOD methods, as we search the architecture and learn its weights simultaneously. The time and memory efficiency of CARNAS make it a practical choice. Thus, we experimentally verify that the proposed CARNAS does make sense, for addressing the graph OOD problem by diving into the NAS process from causal perspective. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G CASE STUDY</head><p>For graphs with different motif shapes (causal subparts), we present the learned operation probabilities for each layer (in expectation) in Figure <ref type="figure">7</ref>. The values that are notably higher than others for each layer are highlighted in bold, and the most preferred operators for each layer are listed in the last row.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Efficient intervention design for causal discovery with latents</title>
		<author>
			<persName><forename type="first">Raghavendra</forename><surname>Addanki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiva</forename><surname>Kasiviswanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mcgregor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cameron</forename><surname>Musco</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="63" to="73" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.02893</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">Invariant risk minimization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Differentiable causal discovery from interventional data</title>
		<author>
			<persName><forename type="first">Philippe</forename><surname>Brouillard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sébastien</forename><surname>Lachapelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Lacoste</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Lacoste-Julien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Drouin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="21865" to="21877" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Rethinking graph neural architecture search from message-passing</title>
		<author>
			<persName><forename type="first">Shaofei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jincan</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Beichen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng-Jun</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qingming</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="6657" to="6666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Invariant rationalization</title>
		<author>
			<persName><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1448" to="1458" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning causally invariant representations for out-of-distribution generalization on graphs</title>
		<author>
			<persName><forename type="first">Yongqiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonggang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yatao</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Kaili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Binghui</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="22131" to="22148" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Principal neighbourhood aggregation for graph nets</title>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Cavalleri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominique</forename><surname>Beaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="13260" to="13271" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Abbas</forename><surname>El</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gamal</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Young-Han</forename><surname>Kim</surname></persName>
		</author>
		<title level="m">Network information theory</title>
		<imprint>
			<publisher>Cambridge university press</publisher>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Debiasing graph neural networks via learning disentangled causal substructure</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Shaohua Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanhu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Graph random neural networks for semi-supervised learning on graphs</title>
		<author>
			<persName><forename type="first">Wenzheng</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huanbo</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qian</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evgeny</forename><surname>Kharlamov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="22092" to="22103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Graph neural architecture search</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chuan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International joint conference on artificial intelligence. International Joint Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Autoattend: Automated attention representation search</title>
		<author>
			<persName><forename type="first">Chaoyu</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3864" to="3874" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Large-scale graph neural architecture search</title>
		<author>
			<persName><forename type="first">Chaoyu</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="7968" to="7981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Good: A graph out-of-distribution benchmark</title>
		<author>
			<persName><forename type="first">Shurui</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiner</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Limei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="2059" to="2073" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.12265</idno>
		<title level="m">Strategies for pre-training graph neural networks</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Open graph benchmark: Datasets for machine learning on graphs</title>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyu</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michele</forename><surname>Catasta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="22118" to="22133" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Reinforcement learning for neural architecture search: A review</title>
		<author>
			<persName><forename type="first">Yesmina</forename><surname>Jaafra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><forename type="middle">Luc</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aline</forename><surname>Deruyver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><surname>Saber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naceur</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image and Vision Computing</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="page" from="57" to="66" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Causal discovery from soft interventions with unknown targets: Characterization and learning</title>
		<author>
			<persName><forename type="first">Amin</forename><surname>Jaber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Murat</forename><surname>Kocaoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthikeyan</forename><surname>Shanmugam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elias</forename><surname>Bareinboim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="9551" to="9561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Drugood: Out-of-distribution (ood) dataset curator and benchmark for ai-aided drug discovery-a focus on affinity prediction problems with noise annotations</title>
		<author>
			<persName><forename type="first">Yuanfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingzhe</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long-Kai</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lanqing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ding</forename><surname>Xue</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.09637</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Drugood: Out-of-distribution dataset curator and benchmark for ai-aided drug discovery-a focus on affinity prediction problems with noise annotations</title>
		<author>
			<persName><forename type="first">Yuanfeng</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxiang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bingzhe</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lanqing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Long-Kai</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ding</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="8023" to="8031" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Causal graph convolutional neural network for emotion recognition</title>
		<author>
			<persName><forename type="first">Wanzeng</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Menghang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuanyu</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cognitive and Developmental Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Out-of-distribution generalization via risk extrapolation (rex)</title>
		<author>
			<persName><forename type="first">David</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joern-Henrik</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amy</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Binas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dinghuai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Remi</forename><surname>Le Priol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="5815" to="5826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Sgas: Sequential greedy architecture search</title>
		<author>
			<persName><forename type="first">Guohao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guocheng</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Itzel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Delgadillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernard</forename><surname>Thabet</surname></persName>
		</author>
		<author>
			<persName><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1620" to="1630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Ood-gnn: Out-of-distribution generalized graph neural network</title>
		<author>
			<persName><forename type="first">Haoyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Out-of-distribution generalization on graphs: A survey</title>
		<author>
			<persName><forename type="first">Haoyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.07987</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning invariant graph representations for out-ofdistribution generalization</title>
		<author>
			<persName><forename type="first">Haoyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="11828" to="11841" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Xiangnan He, and Tat-Seng Chua. Let invariant rationale discovery inspire graph contrastive learning</title>
		<author>
			<persName><forename type="first">Sihang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">An</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingxin</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="13052" to="13065" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Graph rationalization with environment-based augmentations</title>
		<author>
			<persName><forename type="first">Gang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tengfei</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1069" to="1078" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Darts: Differentiable architecture search</title>
		<author>
			<persName><forename type="first">Hanxiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A survey on evolutionary neural architecture search</title>
		<author>
			<persName><forename type="first">Yuqiao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanan</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengjie</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kay</forename><surname>Gary G Yen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tan</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Parameterized explainer for graph neural network</title>
		<author>
			<persName><forename type="first">Dongsheng</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongkuan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenchao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<date type="published" when="2020">19620-19631, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Disentangled graph convolutional networks</title>
		<author>
			<persName><forename type="first">Jianxin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4212" to="4221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Representation learning via invariant causal mechanisms</title>
		<author>
			<persName><forename type="first">Jovana</forename><surname>Mitrovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jacob</forename><forename type="middle">C</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Holger Buesing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Weisfeiler and leman go neural: Higher-order graph neural networks</title>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Ritzert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Eric Lenssen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Rattan</surname></persName>
		</author>
		<author>
			<persName><surname>Grohe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4602" to="4609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Tudataset: A collection of benchmark datasets for learning with graphs</title>
		<author>
			<persName><forename type="middle">Christopher</forename><surname>Preprint</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nils</forename><forename type="middle">M</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franka</forename><surname>Kriege</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristian</forename><surname>Bause</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petra</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marion</forename><surname>Mutzel</surname></persName>
		</author>
		<author>
			<persName><surname>Neumann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2007.08663</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Causal inference in statistics: A primer</title>
		<author>
			<persName><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madelyn</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><forename type="middle">P</forename><surname>Jewell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Models, reasoning and inference</title>
		<author>
			<persName><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>CambridgeUniversityPress</publisher>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page">3</biblScope>
			<pubPlace>Cambridge, UK</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Graph neural architecture search under distribution shifts</title>
		<author>
			<persName><forename type="first">Yijian</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengtao</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="18083" to="18095" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Asap: Adaptive structure aware pooling for learning hierarchical graph representations</title>
		<author>
			<persName><forename type="first">Ekagra</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumya</forename><surname>Sanyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Partha</forename><surname>Talukdar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="5470" to="5477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Large-scale evolution of image classifiers</title>
		<author>
			<persName><forename type="first">Esteban</forename><surname>Real</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherry</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Selle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saurabh</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutaka</forename><surname>Leon Suematsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexey</forename><surname>Quoc V Le</surname></persName>
		</author>
		<author>
			<persName><surname>Kurakin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2902" to="2911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Invariant models for causal transfer learning</title>
		<author>
			<persName><forename type="first">Mateo</forename><surname>Rojas-Carulla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Peters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">36</biblScope>
			<biblScope unit="page" from="1" to="34" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Neighbor contrastive learning on learnable graph augmentation</title>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dewang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurence</forename><forename type="middle">T</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="9782" to="9791" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Towards out-ofdistribution generalization: A survey</title>
		<author>
			<persName><forename type="first">Zheyan</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiashuo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingxuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renzhe</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2108.13624</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Xiangnan He, and Tat-Seng Chua. Causal attention for interpretable and generalizable graph classification</title>
		<author>
			<persName><forename type="first">Yongduo</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiancan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 28th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Unleashing the power of graph data augmentation on covariate distribution shift</title>
		<author>
			<persName><forename type="first">Yongduo</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qitian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiancan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Longfei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-seventh Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Velickovic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">stat</title>
		<imprint>
			<biblScope unit="volume">1050</biblScope>
			<biblScope unit="issue">20</biblScope>
			<biblScope unit="page" from="10" to="48550" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Causal-trivial attention graph neural network for fault diagnosis of complex industrial processes</title>
		<author>
			<persName><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruonan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><forename type="middle">X</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qinghua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zengxiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongkuan</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Industrial Informatics</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Pooling architecture search for graph classification</title>
		<author>
			<persName><forename type="first">Lanning</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quanming</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiqiang</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management</title>
		<meeting>the 30th ACM International Conference on Information &amp; Knowledge Management</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2091" to="2100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Graph learning under distribution shifts: A comprehensive survey on domain adaptation, out-of-distribution, and continual learning</title>
		<author>
			<persName><forename type="first">Man</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiong</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingquan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2402.16374</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Handling distribution shifts on graphs: An invariance perspective</title>
		<author>
			<persName><forename type="first">Qitian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hengrui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wipf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Handling distribution shifts on graphs: An invariance perspective</title>
		<author>
			<persName><forename type="first">Qitian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hengrui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wipf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<monogr>
		<title level="m" type="main">Xiangnan He, and Tat seng Chua. Discovering invariant rationales for graph neural networks</title>
		<author>
			<persName><forename type="first">Ying-Xin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">An</forename><surname>Zhang</surname></persName>
		</author>
		<imprint/>
	</monogr>
	<note>In ICLR, 2022b</note>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Xiangnan He, and Tat-Seng Chua. Discovering invariant rationales for graph neural networks</title>
		<author>
			<persName><forename type="first">Yingxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">An</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Xiangnan He, and Tat-Seng Chua. Discovering invariant rationales for graph neural networks</title>
		<author>
			<persName><forename type="first">Yingxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">An</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Moleculenet: a benchmark for molecular machine learning</title>
		<author>
			<persName><forename type="middle">Zhenqin</forename><surname>Preprint</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bharath</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evan</forename><forename type="middle">N</forename><surname>Ramsundar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Feinberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caleb</forename><surname>Gomes</surname></persName>
		</author>
		<author>
			<persName><surname>Geniesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Aneesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karl</forename><surname>Pappu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Leswing</surname></persName>
		</author>
		<author>
			<persName><surname>Pande</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chemical science</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="513" to="530" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">How neural networks extrapolate: From feedforward to graph neural networks</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mozhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingling</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Shaolei Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ken-Ichi</forename><surname>Kawarabayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<title level="m" type="main">Data-augmented curriculum graph neural architecture search under distribution shifts</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yijian</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Mei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Jiayuan Fan, and Wanli Ouyang. beta-darts: Beta-decay regularization for differentiable architecture search</title>
		<author>
			<persName><forename type="first">Baopu</forename><surname>Peng Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yikang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="10864" to="10873" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Gnnexplainer: Generating explanations for graph neural networks</title>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dylan</forename><surname>Bourgeois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Graph contrastive learning with augmentations</title>
		<author>
			<persName><forename type="first">Yuning</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianlong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongduo</forename><surname>Sui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="5812" to="5823" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Xian-Sheng Hua, and Qianru Sun. Causal intervention for weaklysupervised semantic segmentation</title>
		<author>
			<persName><forename type="first">Dong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinhui</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="page" from="655" to="666" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Granger-causality-based multi-frequency band eeg graph feature extraction and fusion for emotion recognition</title>
		<author>
			<persName><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guijun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Brain Sciences</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">1649</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Improved graph convolutional neural networks based on granger causality analysis for eeg emotion recognition</title>
		<author>
			<persName><forename type="first">Jing</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueying</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 International Conference on Computer Engineering and Artificial Intelligence (ICCEAI)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="684" to="688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<monogr>
		<title level="m" type="main">A survey of deep graph learning under distribution shifts: from graph out-of-distribution generalization to adaptation</title>
		<author>
			<persName><forename type="first">Kexin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Song</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weili</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jundong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaize</forename><surname>Ding</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2410.19265</idno>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b67">
	<monogr>
		<title level="m" type="main">Stable prediction on graphs with agnostic distribution shift</title>
		<author>
			<persName><forename type="first">Shengyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Kuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiezhong</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongfei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2110.03865</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Dynamic graph neural networks under spatio-temporal distribution shift</title>
		<author>
			<persName><forename type="first">Zeyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Out-of-distribution generalized dynamic graph neural network for human albumin prediction</title>
		<author>
			<persName><forename type="first">Zeyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingwang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Teng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ning</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xueling</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Medical Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<monogr>
		<title level="m" type="main">Out-of-distribution generalized dynamic graph neural network with disentangled intervention and invariance promotion</title>
		<author>
			<persName><forename type="first">Zeyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2311.14255</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Spectral invariant learning for dynamic graphs under distribution shifts</title>
		<author>
			<persName><forename type="first">Zeyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhou</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weigao</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Dynamic heterogeneous graph attention neural architecture search</title>
		<author>
			<persName><forename type="first">Zeyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yijian</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhou Qin</surname></persName>
		</author>
		<author>
			<persName><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Seventh AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Data augmentation for graph neural networks</title>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yozen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonardo</forename><surname>Neves</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oliver</forename><surname>Woodford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Neil</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the aaai conference on artificial intelligence</title>
		<meeting>the aaai conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="11015" to="11023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Neural architecture search with reinforcement learning</title>
		<author>
			<persName><forename type="first">Barret</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
