<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Causal Intervention for Abstractive Related Work Generation</title>
				<funder>
					<orgName type="full">Fundamental Research Funds for the Central Universities</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jiachang</forename><surname>Liu</surname></persName>
							<email>jc_liu@bit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Beijing Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qi</forename><surname>Zhang</surname></persName>
							<email>zhangqi_cs@tongji.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="institution">Tongji University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">DeepBlue Academy of Science</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chongyang</forename><surname>Shi</surname></persName>
							<email>cy_shi@bit.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Beijing Institute of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Usman</forename><surname>Naseem</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">James Cook University</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Shoujin</forename><surname>Wang</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">University of Technology</orgName>
								<address>
									<settlement>Sydney</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><surname>Liang Hu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Tongji University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">DeepBlue Academy of Science</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ivor</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
							<affiliation key="aff5">
								<orgName type="department">Agency for Science, Technology and Research</orgName>
								<orgName type="laboratory">CFAR</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
							<affiliation key="aff6">
								<orgName type="department">IHPC</orgName>
								<orgName type="institution">Agency for Science, Technology and Research</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Causal Intervention for Abstractive Related Work Generation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:32+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstractive related work generation has attracted increasing attention in generating coherent related work that helps readers grasp the current research. However, most existing models ignore the inherent causality during related work generation, leading to spurious correlations which downgrade the models' generation quality and generalizability. In this study, we argue that causal intervention can address such limitations and improve the quality and coherence of generated related work. To this end, we propose a novel Causal Intervention Module for Related Work Generation (CaM) to effectively capture causalities in the generation process. Specifically, we first model the relations among the sentence order, document (reference) correlations, and transitional content in related work generation using a causal graph. Then, to implement causal interventions and mitigate the negative impact of spurious correlations, we use do-calculus to derive ordinary conditional probabilities and identify causal effects through CaM. Finally, we subtly fuse CaM with Transformer to obtain an end-to-end related work generation framework. Extensive experiments on two real-world datasets show that CaM can effectively promote the model to learn causal relations and thus produce related work of higher quality and coherence.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A comprehensive related work usually covers abundant reference papers, which costs authors plenty of time in reading and summarization and even forces authors to pursue ever-updating advanced work <ref type="bibr" target="#b12">(Hu and Wan, 2014)</ref>. Fortunately, the task of related work generation emerged and attracted increasing attention from the community of text summarization and content analysis in recent years <ref type="bibr" target="#b7">(Chen et al., 2021</ref><ref type="bibr" target="#b6">(Chen et al., , 2022))</ref>. Related work generation can be considered as a variant of the multi-document summarization task <ref type="bibr" target="#b17">(Li and Ouyang, 2022)</ref>. Distinct from multi-document summarization, related work generation entails comparison after the summarization of a set of references and needs to sort out the similarities and differences between these references <ref type="bibr" target="#b0">(Agarwal et al., 2011)</ref>.</p><p>Recently, various abstractive text generation methods have been proposed to generate related work based on the abstracts of references. For example, <ref type="bibr">Xing et al. (2020a)</ref> used the context of citation and the abstract of each cited paper as the input to generate related work. <ref type="bibr" target="#b10">Ge et al. (2021)</ref> encoded the citation network and used it as external knowledge to generate related work. <ref type="bibr" target="#b6">Chen et al. (2022)</ref> proposed a target-aware related work generator that captures the relations between reference papers and the target paper through a target-centered attention mechanism. Equipped with well-designed encoding strategies, external knowledge, or novel training techniques, these studies have made promising progress in generating coherent related work.</p><p>However, those models are inclined to explore and exploit spurious correlations, such as highfrequency word/phrase patterns, writing habits, or presentation skills, to build superficial shortcuts between reference papers and the related work of the target paper. Such spurious correlations may harm the quality of the generated related work, especially when distribution shift exists between the testing set and training set. This is because spurious correlations are different from genuine causal relations. They often do not intrinsically contribute to the related work generation and easily cause the robustness problem and impair the models' generalizability <ref type="bibr" target="#b2">(Arjovsky et al., 2019)</ref>.</p><p>Figure <ref type="figure" target="#fig_0">1</ref> illustrates the difference between causality and spurious correlation. The phrases "for example" and "later" are often used to bridge two sentences in related work. Their usage may be attributed to writers' presentation habits about organizing sentence orders or the reference document relations corresponding to the sentences. Ideally, a related work generation model is expected to learn the reference relation and distinguish it from the writing habits. However, previous generation models easily capture the superficial habitual sentence organization (a spurious correlation) instead of learning complex causal reference relations, especially when the habitual patterns frequently occur in the training set. In this case, the transitional phrases generated mainly based on writing habits are likely to be unsuitable and subsequently affect the content generation of related work during testing when the training and testing sets are not distributed uniformly.</p><p>Fortunately, causal intervention can effectively remove spurious correlations and focus on causal relations by intervening in the learning process. It not only observes the impact of the sentence order and document relation on generating transitional content, but also probes the impact of each possible order on the whole generation of related work, thereby removing the spurious correlations <ref type="bibr">(Pearl, 2009a)</ref>. Accordingly, causal intervention serving as an effective solution allows causal relations to exert a greater impact and instruct the model to produce the correct content.</p><p>Accordingly, to address the aforementioned gaps in existing work for related work generation, we propose a novel Causal Intervention Module for Related Work Generation (CaM), which effectively removes spurious correlations by performing the causal intervention. Specifically, we first model the relations among sentence order, document relation, and transitional content in related work generation and identify the confounder that raises spurious correlations (see Figure <ref type="figure" target="#fig_2">2</ref>). Then, we implement causal intervention that consists of three compo-nents: 1) Primitive Intervention cuts off the connection that induces spurious correlations in the causal graph by leveraging do-calculus and backdoor criterion <ref type="bibr">(Pearl, 2009a)</ref>, 2) Context-aware Remapping smoothens the distribution of intervened embeddings and injects contextual information, and 3) Optimal Intensity Learning learns the best intensity of overall intervention by controlling the output from different parts. Finally, we strategically fuse CaM with Transformer <ref type="bibr" target="#b27">(Vaswani et al., 2017)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Problem Formulation</head><p>Given a set of reference papers D = {r 1 , ..., r |D| }, we assume the ground truth related work Y = (w 1 , w 2 , ..., w M ), where r i = (w i 1 , w i 2 , ..., w i |r i | ) denotes a single cited paper, w i j is the j-th word in r i , and w j is the j-th word in related work Y . Generally, the related work generation task can be formulated as generating a related work section Ŷ = ( ŵ1 , ŵ2 , ..., ŵ M ) based on the reference input D and minimizing the difference between Y and Ŷ . Considering that the abstract section is usually well-drafted to provide a concise paper summarization <ref type="bibr" target="#b12">(Hu and Wan, 2014)</ref>, we use the abstract section to represent each reference paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>We first analyze the causalities in related work generation, identify the confounder that raises spurious correlations, and use a causal graph to model these relations. Then, we implement CaM to enhance the quality of related work through causal intervention. Finally, we describe how CaM, as an intervention module, is integrated with Transformer to intervene in the entire generation process. The overall structure of our model is shown in Figure <ref type="figure" target="#fig_3">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Causal Modeling for Related Work Generation</head><p>We believe that three aspects play significant roles in related work generation for better depicting the relations between different references, namely, sentence order c, document relation x, and transitional content y (illustrated in Figure <ref type="figure" target="#fig_2">2</ref>). In many cases, sentence order is independent of the specified content and directly establishes relations with transitional content. For example, we tend to use "firstly" at the beginning and "finally" at the end while composing a paragraph, regardless of what exactly is in between. This relation corresponds to path c → y, and it should be preserved as a writing experience or habit. Meanwhile, there is a lot of transitional content that portrays the relations between referred papers based on the actual content, at this time, models need to analyze and use these relations. The corresponding path is x → y. Though ideally, sentence order and document relation can instruct the generation of transitional content based on practical writing needs, deep learning models are usually unable to trade off the influence of these two aspects correctly but prioritize sentence order. This can be attributed to the fact that sentence order information is easily accessible and learnable. In Figure <ref type="figure" target="#fig_2">2</ref>, such relation corresponds to c → x → y. In this case, sentence order c is the confounder that raises a spurious correlation with transitional content y. Although performing well on the training set, once a data distribution shift exists between the test set and training set where the test set focuses more on document relations, the transitional content instructed by sentence order can be quite unreliable. To mitigate the impact of the spurious correlation, we need to cut off the path c → x, enabling the model to generate transitional content based on the correct and reliable causality of both c → y and x → y.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Causal Intervention Module for Related Work Generation</head><p>The proposed CaM contains three parts as shown in Figure <ref type="figure" target="#fig_3">3</ref>: Primitive Intervention performs causal intervention and preliminarily removes the spurious correlations between sentence order and transitional content.Context-aware Remapping captures and fuses contextual information, facilitating the smoothing of the intervened embeddings. Optimal Intensity Learning learns the best intensity of holistic causal intervention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Primitive Intervention</head><p>Based on the causal graph G shown in Figure <ref type="figure" target="#fig_2">2</ref>, we first perform the following derivation using docalculus and backdoor criterion.</p><formula xml:id="formula_0">p(y|do(x)) = c p(y|do(x), c)p(c|do(x)) = c p(y|x, c)p(c|do(x)) = c p(y|x, c)p(c)<label>(1)</label></formula><p>In short, the do-calculus is a mathematical representation of an intervention, and the backdoor criterion can help identify the causal effect of x on y <ref type="bibr">(Pearl, 2009b)</ref>. As a result, by taking into consideration the effect of each possible value of sentence order c on transitional content y, c stops affecting document relation x when using x to estimate y, which means path c → x is cut off (see the arrow-pointed graph in Figure <ref type="figure" target="#fig_2">2</ref>). Next, we will explain how to estimate separately p(y|x, c) and p(c) using deep learning models and finally obtain p(y|do(x)).</p><p>Let E ori ∈ R M ×d denote the input embeddings corresponding to M -sized related work and E itv ∈ R M ×d denote the output embeddings of Primitive Intervention. We first integrate the sentence order information into the input embeddings:</p><formula xml:id="formula_1">e odr(j) i = Linear(e ori i ⊕ o j ), e ori i ∈ E ori (2)</formula><p>where j = 1, ..., s, s is the total number of sentences in the generated related work and e odr(j) i</p><p>denotes the order-enhanced embedding for the i-th word. We take o j = (lg (j + 1), • • • , lg (j + 1)) with the same dimension as e ori . The linear layer (i.e., Linear) further projects the concatenated embedding to e odr with the same dimension as e ori . Accordingly, we have the estimation of p(y|x, c) := e odr . Then, we feed the subsequence E itv 1:i-1 to a feed-forward network to predict the sentence position probability of the current decoding word:</p><formula xml:id="formula_2">h i = Softmax(FFN(ReLU( i-1 E itv 1:i-1 )))<label>(3)</label></formula><p>where each h j i ∈ h i denotes the probability. Thus, we estimate the sentence position probability of each decoding word p(c) := h. After obtaining the estimation of p(y|x, c) and p(c), the final embedding with primitive causal intervention is achieved:</p><formula xml:id="formula_3">e itv i = s j=1 e odr(j) i × h j i , h j i ∈ h i (4)</formula><p>where e odr(j) i × h j i multiplying sentence order probability with order-enhanced embeddings is exactly p(y|x, c)p(c) in Equation <ref type="formula" target="#formula_0">1</ref>. Since most transitions are rendered by start words, our CaM intervenes only with these words, namely part of e itv ∈ E itv being equal to e ori ∈ E ori . For simplicity, we still use E itv in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Context-aware Remapping</head><p>Two problems may exist in Primitive Intervention: 1) The lack of learnable parameters may lead to the intervened embeddings and the original ones being apart and obstructs the subsequent decoding process. 2) Intervention in individual words may damage the context along with the order-enhanced embedding. To solve the two problems, we propose a Context-aware Remapping mechanism. First, we scan E itv with a context window of fixed size n w :</p><formula xml:id="formula_4">B i = WIN(E itv ) = E itv i:i+nw-1 (5)</formula><p>where WIN(•) returns a consecutive subsequence of E itv at length n w . Then, we follow the process of Multi-head Attention Mechanism <ref type="bibr" target="#b27">(Vaswani et al., 2017)</ref> to update the embeddings in B i :</p><formula xml:id="formula_5">B rmp i = MultiHead(B i , B i , B i ) = (e rmp i , ..., e rmp i+nw-1 )<label>(6)</label></formula><p>Even though all embeddings in B i are updated, we only keep the renewed e rmp i+(nw/2) ∈ B rmp i as the output, and leave the rest unchanged. Since WIN(•) scans the entire sequence step by step, every embedding will have the chance to update. The output is denoted as E rmp ∈ R M ×d .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Optimal Intensity Learning</head><p>There is no guarantee that causal intervention with maximum (unaltered) intensity will improve model performance, especially when combined with pretrained models <ref type="bibr" target="#b5">(Brown et al., 2020;</ref><ref type="bibr" target="#b15">Lewis et al., 2020)</ref>, as the intervention may conflict with the pre-training strategies. To guarantee performance, we propose the Optimal Intensity Learning.</p><p>By applying Primitive Intervention and Contextaware Remapping, we have three types of embeddings, E ori ,E itv , and E rmp . To figure out their respective importance to the final output, we derive the output intensity corresponding to each of them: e opm = c ori e ori + c itv e itv + c rmp e rmp (11)</p><formula xml:id="formula_6">g ori = σ(W ori • e ori )<label>(7)</label></formula><formula xml:id="formula_7">g itv = σ(W itv • e ori )<label>(8)</label></formula><formula xml:id="formula_8">g rmp = σ(W rmp • e ori ) (9) c ori , c itv , c rmp = f s ([g ori , g itv , g rmp ])<label>(10)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Fusing CaM with Transformer</head><p>To derive an end-to-end causal generation model and ensure that the intervened information can be propagated, we choose to integrate CaM with Transformer <ref type="bibr" target="#b27">(Vaswani et al., 2017)</ref>. However, unlike the RNN-based models that generate words recurrently <ref type="bibr" target="#b22">(Nallapati et al., 2016)</ref>, the attention mechanism computes the embeddings of all words in parallel, while the intervention is performed on the sentence start words.</p><p>To tackle this challenge, we perform vocabulary mapping on word embeddings before intervention and compare the result with sentence start token [CLS] to obtain M ask:</p><formula xml:id="formula_9">I = argmax[Linear vocab (E ori )]<label>(12)</label></formula><p>M ask = δ(I, ID CLS )</p><p>I contains the vocabulary index of each word. δ(•) compares the values of the two parameters, and returns 1 if the same, 0 otherwise. M ask indicates whether the word is a sentence start word. Therefore, E opm can be calculated as:</p><formula xml:id="formula_11">E opm = E opm ⊙M ask+E ori ⊙(∼ M ask) (14)</formula><p>The ⊙ operation multiplies each embedding with the corresponding {0, 1} values, and ∼ denotes the inverse operation. Note that we omit M ask for conciseness in Section 3.2.3. M ask helps restore the non-sentence-start word embeddings and preserve the intervened sentence-start ones. As illustrated in Figure <ref type="figure" target="#fig_3">3</ref>, we put CaM between the Transformer layers in the decoder. The analysis of the amount and location settings will be discussed in detail in Section 4.6. The model is trained to minimize the cross-entropy loss between the predicted Ŷ and the ground-truth Y , v is the vocabulary index for w i ∈ Y :</p><formula xml:id="formula_12">L = -M i log p v i ( Ŷ )<label>(15)</label></formula><p>4 Experiments</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>Following the settings in <ref type="bibr" target="#b7">Chen et al. (2021</ref><ref type="bibr" target="#b6">Chen et al. ( , 2022))</ref>, we adopt two publicly available datasets derived from the scholar corpora S2ORC <ref type="bibr">(Lo et al., 2020)</ref> and Delve <ref type="bibr" target="#b1">(Akujuobi and Zhang, 2017)</ref> respectively to evaluate our proposed method in related work generation. S2ORC consists of scientific papers from multiple domains, and Delve focuses on the computer domain. The datasets are summarized in Table <ref type="table" target="#tab_0">1</ref>, where the corresponding ratios of the training/validation/test pairs are detailed<ref type="foot" target="#foot_0">foot_0</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Settings</head><p>In our experiments, we incorporate CaM into the Transformer decoder (see Figure <ref type="figure" target="#fig_3">3</ref>) and evaluate our model using the resultant encoder-decoder architecture. We utilize pre-trained weights from BERT <ref type="bibr" target="#b8">(Devlin et al., 2019)</ref> for both the encoder and decoder of the architecture, as described in <ref type="bibr" target="#b26">Rothe et al. (2020)</ref> <ref type="foot" target="#foot_1">foot_1</ref> . Also, when CaM is removed from the decoder in the following experiments, the remaining Transformer model we evaluate still employs pre-trained weights.</p><p>In the Transformer architecture we use, the dimension of word embedding is 768, both the number of attention heads and hidden layers in the encoder and decoder are 12, and the intermediate size is 3072. We implement our model with PyTorch on NVIDIA 3080Ti GPU. The maximum reference paper number is set to 5, i.e., |D| = 5. We select the first 440/|D| words in each reference paper abstract and concatenate them to obtain the model input sequence. The total number of sentences in target related work is set to 6, i.e., s = 6. We use beam search for decoding, with a beam size of 4 and a maximum decoding step of 200. We use SGD as the optimizer with a learning rate 1e -3. We use ROUGE-1, ROUGE-2, and ROUGE-L on F1 as the metrics <ref type="bibr" target="#b18">(Lin, 2004;</ref><ref type="bibr" target="#b13">Jiang et al., 2022)</ref>.  TAG <ref type="bibr" target="#b6">(Chen et al., 2022)</ref>: It takes the paper that related work belongs to as the target and employs a target-centered attention mechanism to generate related work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Overall Performance</head><p>It can be found in Table <ref type="table" target="#tab_1">2</ref> that abstractive models have attracted more attention in recent years and usually outperform extractive ones. Among the generative models, pretrained model T5 and BART achieve promising results in our task without additional design. Meanwhile, Longformer, which is good at handling long text input, also achieves favorable results. However, the performance of these models is limited by the complexity of the academic content in the dataset.</p><p>Our proposed CaM achieves the best performance on both datasets. Due to fusing CaM with Transformer, its large scale ensures that our model can still effectively capture document relations without additional modeling. Accordingly, CaM enables the model to obviate the impact of spurious correlations through causal intervention and promotes the model to learn more robust causalities to achieve the best performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study</head><p>To analyze the contribution of the different components of CaM, we separately control the use of Primitive Intervention (PI), Context-aware Remapping (RMP) and Optimal Intensity Learning (OPT). Figure <ref type="figure">4</ref> and Figure <ref type="figure" target="#fig_4">5</ref> show the performance comparison between different variants of CaM.</p><p>First, we observe that Transformer already guarantees a desirable base performance. When only PI is used, the model generally shows a slight performance drop. PI+RMP outperforms RMP, showing the necessity of the PI and the effectiveness of RMP. PI+RMP+OPT achieves optimal results, indicating that OPT can effectively exploit the information across different representations. We evaluate the quality of related work generated by the CaM, RRG, and Transformer from three perspectives (informativeness, coherence, and succinctness) by randomly selecting forty samples from S2ORC and rating the generated results by 15 PhD students. In the QA task, three PhD students posed three questions for each sample, ensuring that the answers existed in ground truth. Participants need to answer these questions after reading the generated text and we use accuracy as the metric. As table 3 shows, our method achieves the best in informativeness, coherence, and the QA task. However, succinctness is slightly lower than RRG, probably due to the output length limit.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Human</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Fusing Strategy Comparison</head><p>In our setting, the Transformer decoder consists of 12 layers, so there are multiple locations to fuse a different number of CaMs. For each scenario, CaMs are placed evenly among the Transformer decoder layers, and one will always be placed at the end of the entire model. The results of all cases are shown in Figure <ref type="figure" target="#fig_5">6</ref>. It can be observed that the model performs best when the number of CaM is 4 both on S2ORC and Delve. With a small number of CaMs, the model may underperform the benchmark model and fail to achieve optimal performance due to the lack of sufficient continuous intervention. If there are too many CaMs, the distance between different CaMs will be too short, leaving an insufficient learning process for the entire fused model, and this might cause the CaMs to bring the noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Robustness Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.1">Testing with Reordered Samples</head><p>We randomly select 50 samples (15 from S2ORC and 35 from Delve) and manually rearrange the order of the cited papers and the order of their corresponding sentences in each sample.Transitional content in related work is also removed since the reordering damages the original logical relations.</p><p>Figure <ref type="figure" target="#fig_6">7</ref> shows that CaM has better performance no matter whether the samples have been reordered or not. Regarding the reordered samples, the per-2154 formance of Transformer decreases on all three metrics, but CaM only decreases on ROUGE-1 and ROUGE-2 at a much lower rate. Particularly, compared to Transformer, CaM makes improvement on ROUGE-L when tested with reordered samples. The result indicates that CaM is able to tackle the noise disturbance caused by reordering, and the generated content maintains better coherence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.2">Testing with Migrated Test Set</head><p>We train the models on Delve and test them on S2ORC, which is a challenging task and significant for robustness analysis. As expected, the performances of all models drop, but we can still obtain credible conclusions. Since CaM outper-Transformer initially, simply comparing the ROUGE scores after migrating the test set is not informative. To this end, we use Relative Outperformance Rate (ROR) for evaluation:</p><formula xml:id="formula_13">ROR = (S CaM -S TF )/S TF<label>(16)</label></formula><p>S CaM and S TF are the ROUGE scores of CaM and Transformer, respectively. ROR computes the advantage of CaM over Transformer. Figure <ref type="figure" target="#fig_7">8</ref> reports that CaM outperforms Transformer regardless of migrating from Delve to S2ORC for testing. In addition, comparing the change of ROR, we observe that although migration brings performance drop, CaM not only maintains its advantage over Transformer but also enlarges it. The above two experiments demonstrate that the CaM effectively learns causalities to improve model robustness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8">Causality Visualization</head><p>To visualize how causal intervention works in the generation process, we compare the related work generated by Transformer and CaM with a case study. Specifically, we map their cross attention corresponding to "however" and "the" to the input content using different color shades (Figure <ref type="figure" target="#fig_9">10</ref>) to explore what information these two words rely on. More details of the above two experiments can be found in Appendix B.</p><p>We picked out the words that "however" and "the" focused on the most and analyzed the implications of these words in the context of the input. The results are shown in Figure <ref type="figure" target="#fig_8">9</ref>. It can be found that the words highlighted by CaM have their respective effects in the cited papers. When generating "however", the model aggregates this information, comparing the relations between the documents and producing the correct result. However, there is no obvious connection between the words focused on by Transformer, hence there is no clear decision process after combining the information, and the generated word "the" is simply a result obtained from learned experience and preference. Through causality visualization, it can be observed very concretely how CaM improves model performance by conducting causal intervention.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper, we propose a Causal Intervention Module for Related Work Generation (CaM) to capture causalities in related work generation. We first model the relations in related work generation using a causal graph. The proposed CaM implements causal intervention and enables the model to capture causality. We subtly fuse CaM with Transformer to obtain an end-to-end model to integrate the intervened information throughout the generation process. Extensive experiments show the superiority of CaM and demonstrate our method's effectiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations</head><p>Although extensive experiments have demonstrated that CaM can effectively improve the performance of the generation model, as mentioned above, since the intervention occurs on the sentence start words, it is inconclusive that CaM can bring improvement if the generation of sentence start words is inaccurate. That is, if it is to be combined with small-scale models without any pre-trained knowledge, then the effectiveness of the model might not be ensured. This will also be a direction of improvement for our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Related Work Generation</head><p>The related work generation task can be viewed as a variant of the multi-document summarization task, and its methods can be categorized as extractive or abstractive. Most of the early studies use extractive methods. The work of Hoang and Kan ( <ref type="formula">2010</ref>) is one of the first attempts. They propose a heuristic approach to generate general and specific content separately given a topic tree. <ref type="bibr">Wang et al. (2020)</ref> train the model to extract cited text spans through a specific training set and use a greedy algorithm to select the most suitable candidate sentences to compose related work. Most recent studies focus on abstractive approaches. <ref type="bibr">Xing et al. (2020b)</ref> use the citation context and the abstract of the cited papers together as inputs to generate citation text. Chen et al. ( <ref type="formula">2021</ref>) construct a relation graph of the cited papers during the encoding process and update them iteratively. The relation graph is used as an auxiliary information for decoding. The most recent work is done by <ref type="bibr" target="#b6">Chen et al. (2022)</ref>, in which they take the paper that related work belongs to as the target and employ a target-centered attention mechanism to generate informative related work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Causal Intervention</head><p>In recent years, causality theory has attracted increasing attention in various domains. In the field of recommendation system, <ref type="bibr">Wang et al. (2022a)</ref> use the causal graph to model multi-scenario recommendation and solve the problem of existing systems that may introduce unnecessary information from other scenarios. <ref type="bibr">Wang et al. (2022b)</ref> propose a framework for sequential recommendation that can perceive data biases by reweighing training data and using inverse propensity scores(Austin, 2011). In the field of natural language processing, <ref type="bibr" target="#b9">Feng et al. (2021)</ref> introduce counterfactual reasoning into the sentiment analysis task and leverage the knowledge of both factual and counterfactual samples. <ref type="bibr" target="#b30">Wang and Culotta (2020)</ref> propose a method for identifying spurious correlations in the text classification task. The method extracts the words with the highest relevance to the category and uses an estimator to determine whether the correlation is a spurious correlation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Experiment Result for Causal Visualization</head><p>In this section, we will give an extra analysis of the experiments introduced in Section 4.8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Generated Related Work Comparison</head><p>From Table <ref type="table" target="#tab_3">4</ref>, we can notice that CaM generates enriched content and its meaning is closer to ground truth compared to Transformer. Crucially, when pointing out the problems of previous approaches and presenting the new ones(sentence marked in green), CaM correctly generates "however" at the beginning of the sentence and the entire sentence has a more accurate expression, making the transitions more seamless. But Transformer only generates a very high-frequency word "the" at the same position. It can be perceived that in this process Transformer is not making effective decisions, but simply generating with preference and experience.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Visualization Result Analysis on Full Text</head><p>Figure <ref type="figure" target="#fig_9">10</ref> visualizes the cross attention of words "however" and "the" in CaM and Transformer. Different cited papers are split with vertical lines. The deeper blue color denotes the higher attention received by the input source word. Judging from the overall coloring situation, we can find that in CaM, there is more deep blue text, as well as more light-colored text. This means the information that "however" focuses on is more targeted and more important, and CaM is capable to produce correct content by accurately capturing document relations and avoid distractions from the confounder. In the result of Transformer, both light and deep blue text become less visible, and the coverage of normal blue increases greatly, indicating that "the" focuses on a wider range of information but lacks emphasis. It indicates that the decision process in Transformer is unclear and ineffective. Detailed analysis of the exact words they focus on and the decision process of the models is presented in Section 4.8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground Truth</head><p>Many dimension reduction techniques are proposed based on the vector forms, which are generally divided into two parts, linear and nonlinear. The classical methods of principal component analysis and multidimensional scaling are linear, since the outputs returned by these methods are related to the input patterns by a simple linear transformation. However, when the input patterns lie on or near a low dimensional sub of the input space, that is the structure of the data set may be highly nonlinear, then linear methods are bound to fail. As the research for manifold learning, several graph-based nonlinear methods have been proposed, such as locally linear em.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Transformer</head><p>Reduction methods have been proposed on the dimensional space, such are divided into two categories: linear and nonlinear. The first method are the component analysis, the dimensional of the methods are linear to the kernel data. The data of the input dimensional space are not linear to the large dimensional space. The data space dimensional of the data be the nonlinear, and are not used. The graph-based nonlinear methods have been proposed. Including as the linear kernel, and entropy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CaM</head><p>Reduction methods have been proposed on the kernel space, such are divided into two categories: linear and nonlinear. The first approach component analysis are linear and dimensional analysis are based the kernel of the methods. Data of the input are not represented to the low dimensional space. However, the data are not on a low dimensional space. The data space is more nonlinear, and the methods can not be used. The graph-based nonlinear methods have been proposed. Including as the linear entropy.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An illustration of the effect difference between causality (solid arrows) and spurious correlations (dashed arrows) in related work generation.</figDesc><graphic coords="2,70.86,70.85,218.27,140.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>to deliver an end-to-end causal related work generation model. Our main contributions are as follows: • To the best of our knowledge, this work is the first attempt to introduce causality theory into the related work generation task. • We propose a novel Causal Intervention Module for Related Work Generation (CaM) which utilizes causal intervention to mitigate the impact of spurious correlations. CaM is subtly fused with Transformer to derive an end-to-end causal related work generation model, enabling the propagation of intervened information. • Extensive experiments on two real-world benchmark datasets demonstrate that our proposed model can generate related works of high quality and verify the effectiveness and rationality of bringing causality theory into the related work generation task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Causal graph G for related work generation. By applying do-calculus, path c → x is cut off and the impact of spurious correlation c → x → y is mitigated.</figDesc><graphic coords="3,70.86,70.85,218.28,65.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The structure of CaM fused with the Transformer in the decoder. CaM consists of three parts: Primitive Intervention, Context-aware Remapping and Optimal Intensity Learning.</figDesc><graphic coords="4,85.37,70.85,421.81,211.95" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 4: Ablation results on S2ORC.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Performance analysis on the number of CaMs fused with Transformer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Comparison between Transformer and CaM on original and reordered samples.</figDesc><graphic coords="7,315.69,234.41,196.45,104.89" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: The result of migrating test set from Delve to S2ORC (trained on Delve).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Visualization of the generating process within CaM and Transformer.</figDesc><graphic coords="8,315.69,70.85,196.45,169.27" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Raw visualization result from CaM on the word "however" and Transformer on the word "the".</figDesc><graphic coords="12,70.86,474.86,453.55,208.57" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Statistics of the datasets where σ(•) is the sigmoid function, f s (•) is the softmax function. Combining c ori , c itv , c rmp , we can obtain the optimal intervention intensity and the final word embedding set E opm = (e opm</figDesc><table><row><cell></cell><cell>Statistic</cell><cell>S2ORC</cell><cell>Delve</cell></row><row><cell></cell><cell cols="3">Pairs # source # words/sent(doc) # 1079/45 126k/5k/5k 72k/3k/3k 5.02 3.69 626/26 words/sent(sum) # 148/6.69 181/7.88 vocab size # 377,431 190,381</cell></row><row><cell>1</cell><cell cols="3">, ..., e opm M ) with causal intervention:</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>ROUGE scores comparison between our CaM and the baselines.</figDesc><table><row><cell>Model</cell><cell cols="6">S2ORC ROUGE-1 ROUGE-2 ROUGE-L ROUGE-1 ROUGE-2 ROUGE-L Delve</cell></row><row><cell>TextRank BertSumEXT MGSum-ext</cell><cell>22.36 24.62 24.10</cell><cell>2.65 3.62 3.19</cell><cell>19.73 21.88 20.87</cell><cell>25.25 28.43 27.85</cell><cell>3.04 3.98 3.95</cell><cell>22.14 24.71 24.28</cell></row><row><cell>TransformerABS BertSumABS MGSum-abs GS T5-base BART-base longformer RRG NG-Abs TAG CaM (ours)</cell><cell>21.65 23.63 23.94 23.92 23.20 23.36 26.00 25.46 25.06 25.04 26.65</cell><cell>3.64 4.17 4.58 4.51 4.01 4.13 4.96 4.93 5.18 5.68 5.40</cell><cell>20.43 21.69 21.57 22.05 21.41 21.08 23.20 22.97 22.33 23.02 24.62</cell><cell>26.89 28.02 28.13 28.27 26.38 26.96 28.05 29.10 27.49 27.82 29.31</cell><cell>3.92 3.50 4.12 4.36 5.69 5.33 5.20 4.94 5.93 6.16 6.17</cell><cell>23.64 24.74 24.95 25.08 24.35 24.42 25.65 26.29 24.56 25.50 26.61</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Human evaluation result</figDesc><table><row><cell>Evaluation</cell><cell></cell><cell></cell></row><row><cell>inf</cell><cell>coh suc</cell><cell>QA</cell></row><row><cell cols="3">CaM RRG Transformer 2.11 1.97 1.92 38.3 2.21 2.38 2.01 41.6 2.07 2.10 2.05 34.1</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Related work generated by CaM and Transformer. Analysis of the bolded words is in Section 4.8.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://github.com/iriscxy/relatedworkgeneration</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://huggingface.co/docs/transformers/main/en/model_ doc/encoder-decoder#transformers.EncoderDecoderModel</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work is supported by the <rs type="funder">Fundamental Research Funds for the Central Universities</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">SciSumm: A multidocument summarization system for scientific articles</title>
		<author>
			<persName><forename type="first">Nitin</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ravi</forename><surname>Shankar Reddy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kiran</forename><surname>Gvr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carolyn</forename><forename type="middle">Penstein</forename><surname>Rosé</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL-HLT 2011 System Demonstrations</title>
		<meeting>the ACL-HLT 2011 System Demonstrations<address><addrLine>Portland, Oregon</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="115" to="120" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Delve: A dataset-driven scholarly search and analysis system</title>
		<author>
			<persName><forename type="first">Uchenna</forename><surname>Akujuobi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangliang</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1145/3166054.3166059</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="36" to="46" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.1907.02893</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>Invariant risk minimization</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">An introduction to propensity score methods for reducing the effects of confounding in observational studies</title>
		<author>
			<persName><surname>Peter C Austin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multivariate Behav Res</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="399" to="424" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Iz</forename><surname>Beltagy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">E</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arman</forename><surname>Cohan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.05150</idno>
		<title level="m">Longformer: The long-document transformer</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Alec Radford, Ilya Sutskever, and Dario Amodei</title>
		<author>
			<persName><forename type="first">Tom</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Mann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nick</forename><surname>Ryder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Melanie</forename><surname>Subbiah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jared</forename><forename type="middle">D</forename><surname>Kaplan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pranav</forename><surname>Shyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Girish</forename><surname>Sastry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amanda</forename><surname>Askell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sandhini</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ariel</forename><surname>Herbert-Voss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gretchen</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Henighan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clemens</forename><surname>Winter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Hesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Sigler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mateusz</forename><surname>Litwin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Scott</forename><surname>Gray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Chess</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jack</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Berner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Mccandlish</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1877" to="1901" />
		</imprint>
	</monogr>
	<note>Language models are few-shot learners</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Target-aware abstractive related work generation with contrastive learning</title>
		<author>
			<persName><forename type="first">Xiuying</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hind</forename><surname>Alamro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingzhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangliang</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1145/3477495.3532065</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;22</title>
		<meeting>the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR &apos;22<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="373" to="383" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Capturing relations between scientific papers: An abstractive model for related work section generation</title>
		<author>
			<persName><forename type="first">Xiuying</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hind</forename><surname>Alamro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingzhe</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shen</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangliang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.473</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="6068" to="6077" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Empowering language understanding with counterfactual reasoning</title>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jizhi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangnan</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tat-Seng</forename><surname>Chua</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.findings-acl.196</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</title>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2226" to="2236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">BACO: A background knowledge-and content-based framework for citing sentence generation</title>
		<author>
			<persName><forename type="first">Yubin</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ly</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaofeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinsong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ante</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jana</forename><surname>Diesner</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.116</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1466" to="1478" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Towards automated related work summarization</title>
		<author>
			<persName><forename type="first">Cong</forename><surname>Duy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vu</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min-Yen</forename><surname>Kan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Coling 2010 Organizing Committee</title>
		<meeting><address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="427" to="435" />
		</imprint>
	</monogr>
	<note>Coling 2010: Posters</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Automatic generation of related work sections in scientific papers: An optimization approach</title>
		<author>
			<persName><forename type="first">Yue</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)<address><addrLine>Doha, Qatar</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An ion exchange mechanism inspired story ending generator for different characters</title>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chongyang</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaiying</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shoujin</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-031-26390-3_32</idno>
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Knowledge Discovery in Databases -European Conference, ECML PKDD 2022</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<meeting><address><addrLine>Grenoble, France</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022-09-19">2022. September 19-23, 2022</date>
			<biblScope unit="volume">13714</biblScope>
			<biblScope unit="page">2156</biblScope>
		</imprint>
	</monogr>
	<note>Proceedings, Part II</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multi-granularity interaction network for extractive and abstractive multi-document summarization</title>
		<author>
			<persName><forename type="first">Jin</forename><surname>Hanqi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianming</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.556</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6244" to="6254" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.703</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7871" to="7880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Leveraging graph to improve abstractive multi-document summarization</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyan</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiachen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junping</forename><surname>Du</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.555</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6232" to="6243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Automatic related work generation: A meta study</title>
		<author>
			<persName><forename type="first">Xiangci</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jessica</forename><surname>Ouyang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">ROUGE: A package for automatic evaluation of summaries</title>
		<author>
			<persName><forename type="first">Chin-Yew</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Text Summarization Branches Out</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="74" to="81" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Text summarization with pretrained encoders</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1387</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3730" to="3740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Rodney Kinney, and Daniel Weld. 2020. S2ORC: The semantic scholar open research corpus</title>
		<author>
			<persName><forename type="first">Kyle</forename><surname>Lo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lucy</forename><forename type="middle">Lu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Neumann</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.447</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="4969" to="4983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">TextRank: Bringing order into text</title>
		<author>
			<persName><forename type="first">Rada</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Tarau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2004 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="404" to="411" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Abstractive text summarization using sequence-tosequence rnns and beyond</title>
		<author>
			<persName><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.1602.06023</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>Cicero Nogueira dos santos, Caglar Gulcehre, and Bing Xiang</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Causal inference in statistics: An overview</title>
		<author>
			<persName><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
		<idno type="DOI">10.1214/09-SS057</idno>
	</analytic>
	<monogr>
		<title level="j">Statistics Surveys</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="96" to="146" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<author>
			<persName><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
		<title level="m">Causality: Models, Reasoning and Inference</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Exploring the limits of transfer learning with a unified text-to</title>
		<author>
			<persName><forename type="first">Colin</forename><surname>Raffel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sharan</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Matena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanqi</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>text transformer</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Leveraging pre-trained checkpoints for sequence generation tasks</title>
		<author>
			<persName><forename type="first">Sascha</forename><surname>Rothe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shashi</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00313</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="264" to="280" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ł Ukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Toc-rwg: Explore the combination of topic model and citation information for automatic related work generation</title>
		<author>
			<persName><forename type="first">Pancheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shasha</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jintao</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.1109/ACCESS.2019.2959056</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="13043" to="13055" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Causalint: Causal inspired intervention for multi-scenario recommendation</title>
		<author>
			<persName><forename type="first">Yichao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huifeng</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiwen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhirong</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhicheng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongkun</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiwei</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenhua</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruiming</forename><surname>Tang</surname></persName>
		</author>
		<idno type="DOI">10.1145/3534678.3539221</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD &apos;22</title>
		<meeting>the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD &apos;22<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="4090" to="4099" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Identifying spurious correlations for robust text classification</title>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aron</forename><surname>Culotta</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.findings-emnlp.308</idno>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2020</title>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Unbiased sequential recommendation with latent confounders. WWW &apos;22, page 2195-2204</title>
		<author>
			<persName><forename type="first">Zhenlei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiqi</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhipeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
		<idno type="DOI">10.1145/3485447.3512092</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
			<publisher>Association for Computing Machinery</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Automatic generation of citation texts in scholarly papers: A pilot study</title>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaosheng</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.550</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6181" to="6190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Automatic generation of citation texts in scholarly papers: A pilot study</title>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaosheng</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojun</forename><surname>Wan</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.550</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6181" to="6190" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Differentiable n-gram objective on abstractive summarization. Expert Systems with Applications</title>
		<author>
			<persName><forename type="first">Yunqi</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuebing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanyuan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingjin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wensheng</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.eswa.2022.119367</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="volume">215</biblScope>
			<biblScope unit="page">119367</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
