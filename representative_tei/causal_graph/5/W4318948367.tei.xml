<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Synthesizing explainable counterfactual policies for algorithmic recourse with program synthesis</title>
				<funder ref="#_GJSgZGW">
					<orgName type="full">EU</orgName>
				</funder>
				<funder ref="#_M4Uqwu4">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2023-02-02">2 February 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Giovanni</forename><surname>De Toni</surname></persName>
							<email>giovanni.detoni@unitn.it</email>
							<idno type="ORCID">0000-0002-8387-9983</idno>
							<affiliation key="aff0">
								<orgName type="institution">Fondazione Bruno Kessler</orgName>
								<address>
									<addrLine>Via Sommarive 18</addrLine>
									<postCode>38123</postCode>
									<settlement>Trento</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Information Engineering and Computer Science</orgName>
								<orgName type="institution">University of Trento</orgName>
								<address>
									<addrLine>Via Sommarive 9</addrLine>
									<postCode>38123</postCode>
									<settlement>Trento</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Bruno</forename><surname>Lepri</surname></persName>
							<email>lepri@fbk.eu</email>
							<affiliation key="aff0">
								<orgName type="institution">Fondazione Bruno Kessler</orgName>
								<address>
									<addrLine>Via Sommarive 18</addrLine>
									<postCode>38123</postCode>
									<settlement>Trento</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Andrea</forename><surname>Passerini</surname></persName>
							<email>andrea.passerini@unitn.it</email>
							<affiliation key="aff1">
								<orgName type="department">Department of Information Engineering and Computer Science</orgName>
								<orgName type="institution">University of Trento</orgName>
								<address>
									<addrLine>Via Sommarive 9</addrLine>
									<postCode>38123</postCode>
									<settlement>Trento</settlement>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alireza</forename><surname>Tamaddoni-Nezhad</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Alan</forename><surname>Bundy</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Luc</forename><surname>De Raedt</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Artur</forename><surname>D'</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Avila</forename><surname>Garcez</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sebastijan</forename><surname>Dumančić</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Cèsar</forename><surname>Ferri</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Pascal</forename><surname>Hitzler</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Nikos</forename><surname>Katzouris</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Denis</forename><surname>Mareschal</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Stephen</forename><surname>Muggleton</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ute</forename><surname>Schmid</surname></persName>
						</author>
						<title level="a" type="main">Synthesizing explainable counterfactual policies for algorithmic recourse with program synthesis</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-02-02">2 February 2023</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1007/s10994-022-06293-7</idno>
					<note type="submission">Received: 1 May 2022 / Revised: 21 November 2022 / Accepted: 7 December 2022 /</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:28+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Algorithmic recourse</term>
					<term>Counterfactuals examples</term>
					<term>Explainable AI</term>
					<term>Machine learning</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Being able to provide counterfactual interventions-sequences of actions we would have had to take for a desirable outcome to happen-is essential to explain how to change an unfavourable decision by a black-box machine learning model (e.g., being denied a loan request). Existing solutions have mainly focused on generating feasible interventions without providing explanations of their rationale. Moreover, they need to solve a separate optimization problem for each user. In this paper, we take a different approach and learn a program that outputs a sequence of explainable counterfactual actions given a user description and a causal graph. We leverage program synthesis techniques, reinforcement learning coupled with Monte Carlo Tree Search for efficient exploration, and rule learning to extract explanations for each recommended action. An experimental evaluation on synthetic and real-world datasets shows how our approach, FARE (eFficient counterfActual REcourse), generates effective interventions by making orders of magnitude fewer queries to the blackbox classifier with respect to existing solutions, with the additional benefit of complementing them with interpretable explanations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Counterfactual explanations are very powerful tools to explain the decision process of machine learning models <ref type="bibr" target="#b10">(Wachter et al., 2017;</ref><ref type="bibr">Karimi et al., 2020)</ref>. They give us the intuition of what could have happened if the state of the world was different (e.g., if you had taken the umbrella, you would not have gotten soaked). Researchers have developed many methods that can generate counterfactual explanations given a trained model <ref type="bibr" target="#b10">(Wachter et al., 2017;</ref><ref type="bibr" target="#b16">Dandl et al., 2020;</ref><ref type="bibr" target="#b27">Mothilal et al., 2020;</ref><ref type="bibr">Karimi et al., 2020;</ref><ref type="bibr" target="#b20">Guidotti et al., 2018;</ref><ref type="bibr" target="#b8">Stepin et al., 2021)</ref>. However, these methods do not provide any actionable information about which steps are required to obtain the given counterfactual. Thus, most of these methods do not enable algorithmic recourse. Algorithmic recourse describes the ability to provide "explanations and recommendations to individuals who are unfavourably treated by automated decision-making systems" <ref type="bibr" target="#b24">(Karimi et al., 2021)</ref>. For instance, algorithmic recourse can answer questions such as: what actions does a user have to perform to be granted a loan? Recently, providing feasible algorithmic recourse has also become a legal necessity <ref type="bibr" target="#b9">(Voigt &amp; Bussche, 2017)</ref>. Some research works address this problem by developing ways to generate counterfactual interventions <ref type="bibr" target="#b24">(Karimi et al., 2021)</ref>, i.e., sequences of actions that, if followed, can overturn a decision made by a machine learning model, thus guaranteeing recourse. While being quite successful, these methods have several limitations. First, they are purely optimization methods that must be rerun from scratch for each new user. As a consequence, this requirement prevents their use for real-time intervention generation. Second, they are expensive in terms of queries to the black-box classifier and computing time. Last but not least, they fail to explain their recommendations (e.g., why does the model suggest getting a better degree rather than changing jobs?). On the contrary, explainability has been pointed out as a major requirement for methods generating counterfactual interventions <ref type="bibr" target="#b12">(Barocas et al., 2020)</ref>.</p><p>In this paper, we cast the problem of providing explainable counterfactual interventions as a program synthesis task <ref type="bibr" target="#b17">(De Toni et al., 2021;</ref><ref type="bibr" target="#b5">Pierrot et al., 2019;</ref><ref type="bibr" target="#b13">Bunel et al., 2018;</ref><ref type="bibr" target="#b11">Balog et al., 2017)</ref>: we want to generate a "program" that provides all the steps needed to overturn a bad decision made by a machine learning model. We propose a novel reinforcement learning (RL) method coupled with a discrete search procedure, Monte Carlo Tree Search <ref type="bibr" target="#b14">(Coulom, 2006)</ref>, to generate counterfactual interventions in an efficient data-driven manner. We call it FARE (eFficient counterfActual REcourse). As done by <ref type="bibr" target="#b28">Naumann and Ntoutsi (2021)</ref>, we assume a causal model encoding relationships between user features and consequences of potential interventions. We also provide a solution to distil an explainable deterministic program from the learned policy in the form of an automaton (E-FARE, Explainable and eFficient counterfActual REcourse). Figure <ref type="figure" target="#fig_0">1</ref> provides an overview of the architecture and the learning strategy and an example of an explainable intervention generated by the extracted automaton. Our approach addresses the three main limitations characterizing existing solutions:</p><p>• It learns a general policy that can be used to generate interventions for multiple users, rather than running separate user-specific optimizations. • By extracting a program from the learned policy, it can complement the intervention with explanations motivating each action from contextual information. Furthermore, the program can be executed in real-time without accessing the black-box classifier.</p><p>Our experimental results on synthetic and real-world datasets confirm the advantages of the proposed solution over existing alternatives in terms of generality, scalability and interpretability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Counterfactual explanations are versatile techniques to provide post-hoc interpretability of black-box machine learning models <ref type="bibr" target="#b10">(Wachter et al., 2017;</ref><ref type="bibr" target="#b16">Dandl et al., 2020;</ref><ref type="bibr" target="#b27">Mothilal et al., 2020;</ref><ref type="bibr">Karimi et al., 2020;</ref><ref type="bibr" target="#b20">Guidotti et al., 2018;</ref><ref type="bibr" target="#b8">Stepin et al., 2021)</ref>. They are modelagnostic, which means that they can be applied to trained models without performance loss. Compared to other global methods <ref type="bibr" target="#b19">(Greenwell et al., 2018;</ref><ref type="bibr" target="#b0">Apley &amp; Zhu, 2020)</ref>, they provide instead local explanations. Namely, they underline only the relevant factors impacting a decision for a given initial target instance. They are also human-friendly and present many characteristics of what it is considered to be a good explanation <ref type="bibr" target="#b3">(Miller, 2019)</ref>. Therefore, they are suitable candidates to provide explanations to end-users since they are both highly-informative and localized. Recent research has shown how to generate counterfactual interventions for algorithmic recourse via various techniques <ref type="bibr">(Karimi et al., 2020)</ref>, such as probabilistic models <ref type="bibr">(Karimi et al., 2020</ref><ref type="bibr">), integer programming (Ustun et al., 2019;</ref><ref type="bibr" target="#b21">Kanamori et al., 2020)</ref>, reinforcement learning <ref type="bibr" target="#b34">(Yonadav &amp; Moses, 2019)</ref>, program synthesis <ref type="bibr" target="#b29">(Ramakrishnan et al., 2020)</ref>, and genetic algorithms <ref type="bibr" target="#b28">(Naumann &amp; Ntoutsi, 2021)</ref>. Researchers also developed solutions tied to a specific class of machine learning models, such as linear models <ref type="bibr" target="#b32">(Tolomei et al., 2017)</ref> or Additive Tree Models <ref type="bibr" target="#b15">(Cui et al., 2015)</ref>. Methods with (approximated) convergence guarantees on the optimal counterfactual policies have also been proposed <ref type="bibr" target="#b33">(Tsirtsis &amp; Rodriguez, 2020)</ref>. However, most of these methods ignore the causal relationships between user features (Tsirtsis &amp; . Once found, the reward received upon making the action is used to improve the MCTS estimates, and correct traces (i.e., those leading to the desired outcome change) are saved in a replay buffer. 2. Training step. The buffer is used to sample a subset of correct traces to be used to train the RL agent to mimic the behaviour of MCTS. 3. Explainable intervention. Example of an explainable intervention generated by the automaton extracted from the learned agent. Actions are in black, while explanations for each action are in red <ref type="bibr" target="#b33">Rodriguez, 2020;</ref><ref type="bibr">Ustun et al., 2019;</ref><ref type="bibr" target="#b34">Yonadav &amp; Moses, 2019;</ref><ref type="bibr" target="#b29">Ramakrishnan et al., 2020)</ref>. Without assuming an underlying causal graph, the proposed interventions become permutation invariant. For example, given an intervention consisting of three actions [A, B, C], any intervention that is a permutation of the actions will have the same total cost. More importantly, it has been recently shown that optimal algorithmic recourse is impossible to achieve without a causal model of the interactions between the features <ref type="bibr">(Karimi et al., 2020)</ref>. The work by <ref type="bibr">Karimi et al. (2020)</ref> provides algorithmic recourse following a probabilistic causal model but optimizes for subpopulation-based interventions instead of personalizing for a single user. CSCF <ref type="bibr" target="#b28">(Naumann &amp; Ntoutsi, 2021)</ref> is the only model-agnostic method capable of producing consequence-aware sequential interventions by exploiting causal relationships between features represented by a causal graph. However, CSCF is still purely an (evolutionary-based) optimization method, so it has to be run from scratch for each new user. Furthermore, the approach is opaque with respect to the reasons behind a suggested intervention. In this work, we show how our approach improves over CSCF in terms of generality, efficiency and interpretability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem setting</head><p>The state of a user is represented as a vector of attributes s ∈ S (e.g., age, sex, monthly income, job). A black-box classifier h ∶ S → {True, False} predicts an outcome given a user state, with True being favourable to the user and False being unfavourable. The setting can be easily extended to multiclass classification by either grouping outcomes in favourable and unfavourable ones or learning separate programs converting from one class to the other. A counterfactual intervention I is a sequence of actions. Each action is represented as a tuple, (f , x) ∈ A , composed by a function, f, and its argument, x ∈ X f (e.g., (change_ income, 500)). When an action is performed for a certain user, it modifies their state by altering one of their attributes according to its argument. A library F contains all the pos- sible functions which can be called. This library and the corresponding DSL (Domain Specific Language) are typically defined as a-priori by experts to prevent changes to protected attributes (e.g., age, sex, etc.). Examples of such DSLs can be found in "Appendix B". Moreover, each function possesses pre-conditions in the form of Boolean predicates over its arguments which describe the conditions that a user state must meet in order for a function to be called. The end of an intervention I is always specified by the STOP action. We also define a cost function, C ∶ A × S → ℝ which mimics the effort made by a given user to perform an action given the current state. The cost is computed by looking at a causal graph G (Pearl, 2009), where the nodes of the graph are the user's features. This assumption encodes the concept of consequences and it ensures a notion of order for the intervention's actions. For example, it might be easier to get first a degree and then a better salary rather than doing the opposite. The causal graph is problem-specific, and we can estimate it using domain knowledge or a domain expert. If we have observational data, we can also try to learn a candidate G using automated methods (Tian &amp; Pearl, 2001; Spirtes &amp;  Zhang, 2016), although inferring the "true" causal graph without interventions is not trivial. We use the former method for the evaluation by manually crafting the causal graphs. Figure <ref type="figure" target="#fig_1">2</ref> shows an example of a causal graph G and of the corresponding costs. Our goal is to train an agent that, given a user with an unfavourable outcome, generates counterfactual interventions that overturn it. Given a black-box classifier h, a user s 0 for whom the pre- diction by h is unfavourable (i.e., h(s 0 ) = False ), a causal graph G and a set of possible actions A (implicitly represented by the functions in F and their arguments in X ), we want to generate a sequence I * , that, if applied to s 0 , produces a new state, s * = I(s 0 ) , such that h(s * ) = True . This sequence must be actionable, which means that the user has to be able to perform those actions, and minimize the user's cost. More formally:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model architecture</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Overall structure</head><p>Figure <ref type="figure" target="#fig_0">1</ref> shows the complete FARE model architecture. It is composed of a binary encoder and an RL agent coupled with the Monte Carlo Tree Search procedure. The binary encoder converts the user's features into a binary representation. The conversion is done by onehot-encoding the categorical features and discretizing the numerical features into ranges. In the following sections, we will use s t to directly indicate the user's state binary version. Given a state s t , the RL agent generates candidate policies, f and x , for the function and argument generation respectively. MCTS uses these policies as priors for its exploration of the action space and extracts the best next action (f , x) * t+1 . The action is then applied to the environment. The procedure ends when the STOP action is chosen (i.e., the intervention was successful) or when the maximum intervention length is reached, in which case the result is marked as a failure. During training, the reward is used to improve the MCTS estimates of the policies. Moreover, correct traces (i.e., traces of interventions leading to the desired outcome change) are stored in a replay buffer, and a sample of traces from the buffer is used to refine the RL agent.</p><p>(1) </p><formula xml:id="formula_0">I * = min I T ∑ t=0 C(a t , s t ) s.t. I = {a t } T t=0 a t ∈ A ∀t s t = I t-1 (s t-1 ) ∀t &gt; 0 h(I(s 0 )) ≠ h(s 0 )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">RL agent structure</head><p>The agent structure is inspired by previous program synthesis works <ref type="bibr" target="#b17">(De Toni et al., 2021;</ref><ref type="bibr" target="#b5">Pierrot et al., 2019)</ref>. It is composed by 5 components: a state encoder, g enc , an LSTM con- troller, g lstm , a function network g f , an argument network g x and a value network g V . See Fig. <ref type="figure">3</ref> for an overview. We use simple feedforward networks to implement g f , g x and g V .</p><p>g enc encodes the user's state in a latent representation which is fed to the controller, g lstm . The controller, g lstm learns an implicit representation of the program to generate the inter- ventions. The function and argument networks are then used to extract the corresponding policies, f and x , by taking as input the hidden state h t from g lstm . g V represents the value function V and it outputs the expected reward from the state s t . Here, we omit the state s t when defining the policies and the value function output, since s t is already embedded into the h t representation. In our settings, we try to learn a single program, which we call INTERVENE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Policy</head><p>A policy is a distribution over the available actions (i.e., functions and their arguments) such that ∑ N i=0 (i) = 1 . Our agent produces two policies: f on the function space, and x on the argument space. The next action, (f , x) t+1 , is chosen by taking the argmax over the policies:</p><p>Each program starts by calling the program INTERVENE, and it ends when the action STOP is called.</p><p>(2)</p><formula xml:id="formula_1">g enc (s t ) =e t g lstm (e t , h t-1 ) = h t (3) g f (h t ) = f g x (h t ) = x g V (h t ) = v t f t+1 = argmax f ∈F f (f ) x t+1 = argmax x∈X f t+1</formula><p>x (x|f t+1 ) Fig. <ref type="figure">3</ref> Agent architecture. Given the user's state s t , it outputs a function policy, f , an argument policy x and an estimate of the expected reward from the state v t . These outputs are used to select the next best action (f , x) t+1 1 3</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Reward</head><p>Once we have applied the intervention I, given the black-box classifier h, the reward, r, is computed as:</p><p>where is a regularization coefficient and T is the length of the intervention. The T penalizes longer interventions in favour of shorter ones. Minimizing the intervention length is related to minimizing the sparsity, which indicates how many features we have changed to obtain a successful counterfactual <ref type="bibr" target="#b10">(Wachter et al., 2017)</ref>. Sparsity is regarded as an important quality for counterfactual examples and algorithmic recourse <ref type="bibr" target="#b3">(Miller, 2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Monte Carlo tree search</head><p>Monte Carlo Tree Search (MCTS) is a discrete heuristic search procedure that can successfully solve combinatorial optimization problems with large action spaces <ref type="bibr" target="#b7">(Silver et al., 2018</ref><ref type="bibr" target="#b6">(Silver et al., , 2016))</ref>. MCTS explores the most promising nodes by expanding the search space based on a random sampling of the possible actions. In our setting, each tree node represents the user's state at a time t, and each arc represents a possible action determining a transition to a new state. MCTS searches for the correct sequence of interventions that minimize the user effort and changes the prediction of the blackbox model. We use the agent policies, f and x , as a prior to explore the program space. Then, the newly found sequence of interventions is used to train the RL agent. To select the next node, we maximize the UCT criterion <ref type="bibr" target="#b26">(Kocsis &amp; Szepesvári, 2006)</ref>:</p><p>Here Q(s, (f, x)) returns the expected reward by taking action (f, x). U(s, (f, x)) is a term that trades-off exploration and exploitation, and it is based on how many times we visited node s in the tree. L(s, (f, x)) is a scoring term which is defined as follows:</p><p>where l cost = C(a, s) ∈ ℝ represents the effort needed to perform the a = (f , x) ∈ A action, and l count ∈ ℝ penalizes interventions that call multiple times the same function f. MCTS uses the simulation results to return an improved version of the agent policies mcts f and mcts x . We can also specify the depth of the search tree as a hyperparameter to balance the computational load requested by the procedure.</p><p>From the found intervention, we build an intervention trace, which is a sequence of tuples that stores, for each time step t: the input state, the output state, the reward, the hidden state of the controller and the improved policies. The traces are stored in the replay buffer, to be used to train the RL agent.</p><formula xml:id="formula_2">(4) r = T R ∈ (0, 1), R = 1 h(I(s)) ≠ h(s) 0 otherwise (5) (f , x) t+1 = argmax f ∈F,x∈X f Q(s, (f , x)) + U(s, (f , x)) + L(s, (f , x)) (6) L(s, (f , x)) = e -(l cost ((f ,x),s)+l count (f ))</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Training the agent</head><p>The agent has to learn to replicate the interventions provided by MCTS at each step t. Given the replay buffer, we sample a batch of intervention traces and we minimize the cross-entropy L between the MCTS policies and the agent policies for each time step t:</p><p>where represents the agent's parameters and V is the value function evaluation computed by the agent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Generate interventions through RL</head><p>When training the agent, we learn a general policy that can be used to provide interventions for many different users. The inference procedure is similar to the one used for training. Given an initial state s, MCTS explores the tree search space using as "prior" the learnt policies x and f coming from the agent. The policies x and f give MCTS a hint of which node to select at each step. Once MCTS finds the minimal cost trace that achieves recourse, we return it to the user. In principle, we can also use only x and f to obtain a viable intervention (e.g., by deterministically taking the action with highest probability each time). However, keeping the search component (MCTS) with a small exploration budget outperforms the RL agent alone. See Table <ref type="table" target="#tab_2">2</ref> in Sect. 4 for the comparison between the agent-only model and the agent augmented with MCTS.</p><p>Learning a general policy to provide interventions is a powerful feature. However, the policy is encoded in the latent states of the agent, thus making it impossible for us to understand it. We want to be able to extract from the trained model an explainable version of this policy, which can then be used to explain why the model suggested a given intervention. Namely, besides providing to the users a sequence of actions, we want to show also the reason behind each suggested action. The intuition to achieve this is the following: given a set of successful interventions generated by the agent, we can distill a synthetic automaton, or program, (E-FARE) which condense the policy in a graph-like structure which we can traverse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">Explainable intervention program</head><p>We now show how we can build a deterministic program given the agent. Figure <ref type="figure" target="#fig_2">4</ref> shows the complete procedure and an example of the produced trace. First, we sample M intervention traces from the trained agent and extract a sequence of {(s i , (f , x) i )} T i=0 for each trace. Then, we construct an automaton graph, P , in the following way:</p><p>1. Given the function library F , we create a node for each function f available. We also add a starting node called INTERVENE and a "sink" node called STOP; 2. We connect each node by unrolling the sampled traces. Starting from INTERVENE, we treat each action (f , x) t as a transition. We label the transition with (f, x) and we connect the current node to the one representing the function f;</p><formula xml:id="formula_3">(7) argmin ∑ batch (V -r) 2 -( mcts f ) T log( f ) -( mcts x ) T log( x )</formula><p>1 3</p><p>3. Lastly, for each node f, we store a collection of outgoing state-action pairs (s i , (f , x) i ) .</p><p>Namely, we store all the states s and the corresponding outward transitions which were decided by the model while at the node f; 4. For each node, f ∈ P , we train a decision tree on the tuples (s i , (f , x) i ) stored in the node to predict the transition (f , x) i given a user's state s i .</p><p>The decision trees are trained only once by using the collection of traces sampled from the trained agent. The agent is frozen at this step, and it is not trained further. At this point, we perform Step 1 to 3 of Fig. <ref type="figure" target="#fig_2">4</ref>. The pseudocode of the entire procedure is available in the "Appendix A".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7">Generate explainable interventions</head><p>The intervention generation is done by traversing the graph P , starting from the node INTER- VENE, until we reach the STOP node or we reach the maximum intervention length. In the last case, the program is marked as a failure. Given the node f ∈ P and given the state s t , we use the decision tree of that node to predict the next transition (f � , x � ) . Moreover, we can extract from the decision tree interpretable rules which tell us why the next action was chosen. A rule is a boolean proposition on the user's features such as (income &gt; 5000 ∧ education = bachelor) . Then, we follow (f � , x � ) , which is an arc going from f to the next node f ′ , and we apply the action to s t to get s t+1 . Again, the program is "fixed" at inference time, and it is not trained further. See Step 4 of Fig. <ref type="figure" target="#fig_2">4</ref> for an example of the inference procedure and of the produced explainable trace. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>Our experimental evaluation aims at answering the following research questions: (1) Does our method provide better performances than the competitors in terms of the validity of the algorithmic recourse? (2) Does our approach allow us to complement interventions with action-by-action explanations in most cases? (3) Does our method minimize the interaction with the black-box classifier to provide interventions?</p><p>The code and the dataset of the experiments are available on Github to ensure reproducibility. <ref type="foot" target="#foot_0">1</ref> The software exploit parallelization through mpi4python <ref type="bibr" target="#b1">(Dalcin &amp; Fang, 2021)</ref> to improve inference and training time. We compared the performance of our algorithm with CSCF <ref type="bibr" target="#b28">(Naumann &amp; Ntoutsi, 2021)</ref>, to the best of our knowledge the only existing model-agnostic approach that can generate consequence-aware interventions following a causal graph. However, note that earlier solutions still perform user-specific optimization, so that our results in terms of generality, interpretability and cost (number of queries to the black-box classifier and computational cost) carry over to these alternatives. For the sake of a fair comparison, we built our own parallelized version of the CSCF model based on the original code. We developed the project to make it easily extendable and reusable by the research community. The experiments were performed using a Linux distribution on an Intel(R) Xeon(R) CPU E5-2660 2.20GHz with 8 cores and 100 GB of RAM (only 4 cores were used).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset and black-box classifiers</head><p>Table <ref type="table" target="#tab_1">1</ref> shows a brief description of the datasets. They all represent binary (favourable/ unfavourable) classification problems. The two real world datasets, German Credit (german) and Adult Score (adult) <ref type="bibr" target="#b18">(Dua &amp; Graff, 2017)</ref>, are taken from the relevant literature. Given that in these datasets a couple of actions is usually sufficient to overturn the outcome of the black-box classifier, we also developed two synthetic datasets, syn and syn_long, where longer interventions are required, so as to evaluate the models in more challenging scenarios. The datasets are made of both categorical and numerical features (e.g., monthly income, job type, etc.). Each dataset was randomly split into 80% train and 20% test. For each dataset, we manually define a causal graph, G , by looking at the features available. For the synthetic datasets, we sampled instances directly from the causal graph. See Fig. <ref type="figure" target="#fig_7">10</ref> in the Appendix for an example of these graphs. The black-box classifier for german and adult was obtained by training a 5-layers MLP with ReLu activations. The trained classifiers are reasonably accurate ( ∼ 0.9 test-set accuracy for german, ∼ 0.8 for adult). The synthetic datasets (syn and syn_long) do not require any training since we directly use our manually defined decision function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Models</head><p>We evaluate four different models: FARE, the agent coupled with MCTS ( M FARE ), E-FARE, the explainable deterministic program distilled from the agent ( M E-FARE ), and two versions of CSCF, one ( M cscf ) with a large budget of generation, n, and population size, p, ( n = 50, p = 200 ) and one ( M small cscf ) with a smaller budget ( n = 25, p = 100 ). For M FARE , we set the MCTS exploration depth to 7 for all the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation</head><p>The left plot in Fig. <ref type="figure" target="#fig_3">5</ref> shows the average validity of the different models, namely the fraction of instances for which a model manages to generate a successful intervention <ref type="bibr" target="#b10">(Wachter et al., 2017)</ref>. We can see how M FARE outperforms or is on-par with the M cscf and M small cscf models on both the real-world and synthetic datasets. The performance difference is more evident in the synthetic datasets because the evolutionary algorithm struggles to generate interventions that require more than a couple of actions. The validity loss incurred in distilling M FARE into a program ( M E-FARE ) is rather limited. This implies that we are able to provide interventions with explanations for 94% (german), 66% (adult), 99% (syn) and 87% (syn_long) of the test users.<ref type="foot" target="#foot_1">foot_1</ref> Moreover, M E-FARE generates similar interventions to M FARE . The sequence similarity between their respective interventions for the same user The main reason for the validity gains of our model is the ability to generate long interventions, something evolutionary-based algorithms struggle with. This effect can be clearly seen from the middle plot of Fig. <ref type="figure" target="#fig_3">5</ref>. Both M cscf and M small cscf rarely generate interventions with more than two actions, while our approach can easily generate interventions with up to five actions. A drawback of this ability is that intervention costs are, on average, higher (right plot of Fig. <ref type="figure" target="#fig_3">5</ref>). On the one hand, this is due to the fact that our model is capable of finding interventions for more complex instances, while M cscf and M small cscf fail. Indeed, if we compute lengths and costs on the subset of instances for which all models find a successful intervention, the difference between the approaches is less pronounced. See Fig. <ref type="figure" target="#fig_4">6</ref> for the evaluation. On the other hand, there is a clear trade-off between solving a new optimization problem from scratch for each new user, and learning a general model that, once trained, can generate interventions for new users in real-time and without accessing the black-box classifier.</p><p>We also conducted a quantitative analysis of the quality of the explanations generated using M E-FARE . We measured the average number of boolean clauses in the rule of a given suggested action. Our explanations need to be concise, thus involving a limited number of features, to be easily understandable. The literature defines seven as the maximum acceptable number of concepts in an explanation <ref type="bibr" target="#b3">(Miller, 2019</ref><ref type="bibr" target="#b2">(Miller, , 1956))</ref>. We have an average of 3 for the syn, syn_long and german datasets, while we have an average of 6.5 clauses for the adult dataset. Indeed, the M E-FARE model can generate compact explanations as boolean predicates. The adult dataset requires a more complex recourse policy. Therefore the decision rules of the automaton are more complex, thus involving longer boolean predicates. See Fig. <ref type="figure">7</ref> for examples of interventions coupled with rule-based explanations.</p><p>Figure <ref type="figure">8</ref> reports the average number of queries to the black-box classifier. Our approach requires far fewer queries than M cscf (note that the plot is in logscale), and If we restrict the comparison to the subset of instances for which all models manage to generate a successful intervention, the difference in costs between methods shrinks substantially (top left vs bottom left). The same behaviour applies to the intervention length (top right vs bottom right) even substantially less than M small cscf (that is anyhow not competitive in terms of validity). Furthermore, most queries are made for training the agent ( M FARE (train) ), which is only done once for all users. Once the model is trained, generating interventions for a single user requires around two orders of magnitude fewer queries than the competitors. Note that MCTS is crucial to allow the RL agent to learn a successful policy with a low budget of queries. Indeed, training an RL agent without the support of MCTS fails to converge in the given budget (between 50 and 100 iterations), leading to a completely useless policy. By efficiently searching the space of interventions, MCTS manages to quickly correct inaccurate initial policies, allowing the agent to learn high quality policies with a limited query budget. MCTS is also critical during inference, since it increases the validity of the results. Given a trained agent, the validity drops if we perform inference without the MCTS components. See Table <ref type="table" target="#tab_2">2</ref> for the evaluation. Using the automaton to generate interventions does not require to query the blackbox classifier. This characteristic can substantially increase the usability of the system, as M E-FARE can be employed directly by the user even if they have no access to the classifier. Computationally speaking, the advantage of a two-step phase is also quite dramatic. M cscf takes an average of ∼ 693 s for each user to provide a solution (the same order of magnitude of training a model for all users with M FARE ), while M FARE infer- ence time is under 1s, allowing real-time interaction with the user.</p><p>Additionally, Fig. <ref type="figure" target="#fig_6">9</ref> shows how it is possible to improve the performances of M E-FARE by just sampling more traces from the trained agent ( M FARE ). We can see how the validity increases in the adult, syn and syn_long datasets. We also notice that using a larger budget to train M E-FARE produces longer explainable rules by keeping the length and cost of the generated interventions almost constant. The total number of queries to the black-box classifier will also slightly increase.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This work improves the state-of-the-art on algorithmic recourse by providing a method, FARE (eFficient counterfActual REcourse), that can generate effective and interpretable counterfactual interventions in real-time. Our experimental evaluation confirms the advantages of our solution with respect to alternative consequence-aware approaches in terms of validity, interpretability and number of queries to the black-box classifier. Our work unlocks many new research directions, which could be explored to solve some of its limitations. First, following previous work on causal-aware intervention generation, we use manually-crafted causal graphs and action costs. Learning them from the available data directly, minimizing the human intervention, would allow applying the approach in settings where this information is not available or unreliable. Second, we showed how our method learns a general program by optimizing over multiple users. It would be interesting to investigate additional RL methods to optimize the interventions globally and locally to provide more personalized sequences to the users. Such methods could be coupled with interactive approaches eliciting preferences and constraints directly from the user, thus maximizing the chance to generate the most appropriate intervention for a given user. numerical arguments simply add their argument to the current value of the target feature. The program STOP does not accept any argument and it signals only the end of the intervention without changing the features. The DSLs for the synthetic_long experiment is similarly defined and is omitted for brevity (Fig. <ref type="figure" target="#fig_7">10</ref>).  Here, the graphs encode the assumption that we know the factors influencing the target features (Risk and Loan). However, in practice, we cannot know which features the decision function of the black-box model is using for inference permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit <ref type="url" target="http://creat">http:// creat</ref> iveco mmons. org/ licen ses/ by/4. 0/.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 1 .</head><label>1</label><figDesc>Fig. 1 1. Model architecture. Given the state s t representing the features of the user, the agent generates candidate intervention policies f and x for functions and arguments, respectively (an action is a function-argument pair). MCTS uses these policies as a prior, and it extracts the best next action (f , x) * t+1</figDesc><graphic coords="3,50.71,57.72,337.94,108.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2</head><label>2</label><figDesc>Fig. 2 Examples of interventions on a causal graph. A A causal graph and a set of candidate actions. B Examples of interventions together with their costs. Note that the green line ( ∑ C = 15 ) has a lower cost than the red line ( ∑ C = 28 ) thanks to a better ordering of the actions making up the intervention (Color figure online)</figDesc><graphic coords="5,49.57,57.72,340.22,84.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4</head><label>4</label><figDesc>Fig.4Procedure to generate the explainable program from intervention traces. 1. For all f ∈ F , we add a new node. 2. Given the samples traces, we add the transitions, and we store (s i , (f i , x i )) in each node. 3. We train a decision tree for each node to predict the next action (consistently with the sampled traces). 4. We execute the program on the new instance at prediction time, using the decision trees to decide the next action at each node. We extract a Boolean rule explaining it from the corresponding decision tree for each action. On the right, an example of generated intervention. The actions (f, x) are black, while the explanations are red (Color figure online)</figDesc><graphic coords="9,49.57,57.72,340.22,110.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5</head><label>5</label><figDesc>Fig. 5 Experimental results. (Left) validity (fraction of successful interventions); (Middle) Average length of a successful intervention; (Right) Average cost of a successful intervention. Results are averaged over 100 test examples</figDesc><graphic coords="11,49.57,57.84,340.22,119.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6</head><label>6</label><figDesc>Fig.6Evaluation considering only the instances for which all the models provide a successful intervention. If we restrict the comparison to the subset of instances for which all models manage to generate a successful intervention, the difference in costs between methods shrinks substantially (top left vs bottom left). The same behaviour applies to the intervention length (top right vs bottom right)</figDesc><graphic coords="12,92.11,57.84,255.14,179.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 Fig. 8</head><label>78</label><figDesc>Fig. 7 Example of Interventions with rule-based explanations. We show here two additional examples of successful interventions (syn and german datasets) combined with boolean predicates explaining why we suggested the given action. The black text indicates the action (f , x) t , while the red text indicates the decision rule (Color figure online)</figDesc><graphic coords="13,92.11,57.72,255.14,185.54" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 9</head><label>9</label><figDesc>Fig. 9 Validity of M E-FARE when varying the training budget. We show the effect on increasing the sampling budget (from 100 to 700 traces) when training the M E-FARE model</figDesc><graphic coords="14,49.63,207.93,340.10,184.46" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 10</head><label>10</label><figDesc>Fig. 10 Causal Graphs. Depiction of the causal graphs used in the experiments for the syn and german dataset. The bold nodes indicate the variables we want to predict. Here, the graphs encode the assumption that we know the factors influencing the target features (Risk and Loan). However, in practice, we cannot know which features the decision function of the black-box model is using for inference</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1</head><label>1</label><figDesc>Description of the datasets|D| is the size of the dataset. |s| the number of features for an instance. |B(s)| shows how many binary features the agent sees after the conversion with the binary converter. |F| is the size of the agent program</figDesc><table><row><cell>Dataset</cell><cell>|D|</cell><cell>h(s) = 1</cell><cell>h(s) = 0</cell><cell>|s|</cell><cell>|B(s)|</cell><cell>|F|</cell></row><row><cell>german</cell><cell>1002</cell><cell>301</cell><cell>701</cell><cell>10</cell><cell>44</cell><cell>7</cell></row><row><cell>adult</cell><cell>48,845</cell><cell>11,691</cell><cell>37,154</cell><cell>15</cell><cell>125</cell><cell>6</cell></row><row><cell>syn</cell><cell>10,004</cell><cell>5002</cell><cell>5002</cell><cell>10</cell><cell>40</cell><cell>6</cell></row><row><cell>syn_long</cell><cell>10,004</cell><cell>5002</cell><cell>5002</cell><cell>14</cell><cell>64</cell><cell>10</cell></row><row><cell cols="7">library. h(s) indicates the number of favourable (1) and unfavourable</cell></row><row><cell>(0) samples</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2</head><label>2</label><figDesc>Ablation study In order to evaluate the contribution of MCTS in finding successful interventions, we also evaluated a trained FARE model which only uses the RL agent, M agent . The agent predicts the next action using its own policy without leveraging MCTS to refine the choice. Results indicate that RL alone is incapable of finding successful interventions and the validity drops. Bold values indicate the best results</figDesc><table><row><cell>Dataset</cell><cell>M FARE</cell><cell>M agent</cell></row><row><cell>german</cell><cell>1.00</cell><cell>0.00</cell></row><row><cell>adult</cell><cell>0.93</cell><cell>0.00</cell></row><row><cell>syn</cell><cell>0.98</cell><cell>0.59</cell></row><row><cell>syn_long</cell><cell>0.92</cell><cell>0.00</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3</head><label>3</label><figDesc>Example of the DSL for the german experiment</figDesc><table><row><cell>Program</cell><cell cols="2">Argument Type Argument</cell></row><row><cell>CHANGE_SAVINGS</cell><cell>Categorical</cell><cell>unknown, little, moderate, rich, quite_rich</cell></row><row><cell>CHANGE_JOB</cell><cell>Categorical</cell><cell>unskilled_non_resident, unskilled_resident, skilled, highly_</cell></row><row><cell></cell><cell></cell><cell>skilled</cell></row><row><cell>CHANGE_CREDIT</cell><cell>Numerical</cell><cell>100, 1000, 2000, 5000</cell></row><row><cell>CHANGE_HOUSING</cell><cell>Categorical</cell><cell>free, rent, own</cell></row><row><cell cols="2">CHANGE_DURATION Numerical</cell><cell>10, 20, 30</cell></row><row><cell>CHANGE_PURPOSE</cell><cell>Categorical</cell><cell>business, car, domestic_appliances, education, furniture/</cell></row><row><cell></cell><cell></cell><cell>equipment, radio/TV, repairs, vacation/others</cell></row><row><cell>STOP</cell><cell>-</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4</head><label>4</label><figDesc>Example of the DSL for the synthetic experiment</figDesc><table><row><cell>Program</cell><cell>Argument Type</cell><cell>Argument</cell></row><row><cell>CHANGE_EDUCATION</cell><cell>Categorical</cell><cell>none,secondary school diploma, bachelor, master, phd</cell></row><row><cell>CHANGE_JOB</cell><cell>Categorical</cell><cell>unemployed, worker, office worker, manager, ceo</cell></row><row><cell>CHANGE_INCOME</cell><cell>Numerical</cell><cell>5000, 10000, 20000, 30000, 40000, 50000</cell></row><row><cell>CHANGE_HOUSE</cell><cell>Categorical</cell><cell>none, rent, own</cell></row><row><cell>CHANGE_RELATION</cell><cell>Categorical</cell><cell>single, married, divorced, widow/er</cell></row><row><cell>STOP</cell><cell>-</cell><cell>-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 5</head><label>5</label><figDesc>Example of the DSL for the adult Without-pay, Self-emp-not-inc, Self-emp-inc, Private, Local-gov, State-gov, Federal-gov, ? CHANGE_EDUCATION Categorical Preschool, 1st-4th, 5th-6th, 7th-8th, 9th, 10th, 11th, 12th, HS-grad, Some-college, Bachelors, Masters, Doctorate, Assoc-acdm, Assoc-voc, Prof-school</figDesc><table><row><cell></cell><cell>Argument</cell><cell>Never-worked,</cell></row><row><cell>experiment</cell><cell>Argument Type</cell><cell>Categorical</cell></row><row><cell></cell><cell>Program</cell><cell>CHANGE_WORKCLASS</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https:// github. com/ unitn-sml/ syn-inter venti ons-algor ithmic-recou rse.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Note that the validity loss observed on adult is due to the limited sampling budget we allocated for M E-FARE (250 traces for all datasets). Adapting this budget to the feature space size (considerably larger for adult) can help boost the performance, at the cost of generating longer explanations.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_2"><p>Ethical ImpactThe research field of algorithmic recourse aims at improving fairness, by providing unfairly treated users with tools to overturn unfavourable outcomes. By providing realtime, explainable interventions, our work makes a step further in making these tools widely accessible. As for other approaches providing counterfactual interventions, our model could in principle be adapted by malicious users to "hack" a fair system. Research on adversarial training can help in mitigating this risk.</p></note>
		</body>
		<back>

			<div type="funding">
<div><p>Funding This research was partially supported by <rs type="projectName">TAILOR</rs>, a project funded by <rs type="funder">EU</rs> <rs type="programName">Horizon 2020 research and innovation programme under GA No 952215</rs>. The work of <rs type="person">Giovanni De Toni</rs> was partially supported by the project <rs type="projectName">AI@Trento (FBK-Unitn</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_GJSgZGW">
					<orgName type="project" subtype="full">TAILOR</orgName>
					<orgName type="program" subtype="full">Horizon 2020 research and innovation programme under GA No 952215</orgName>
				</org>
				<org type="funded-project" xml:id="_M4Uqwu4">
					<orgName type="project" subtype="full">AI@Trento (FBK-Unitn</orgName>
				</org>
			</listOrg>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Availability of data and materials</head><p>The datasets used in the experimental evaluation are freely available at <ref type="url" target="https://github">https:// github</ref>. com/ unitn-sml/ syn-inter venti ons-algor ithmic-recou rse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Code availability</head><p>The code is freely available at <ref type="url" target="https://github">https:// github</ref>. com/ unitn-sml/ syn-inter venti ons-algor ithmicrecou rse.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A: Program distillation pseudocode</head><p>We present here the pseudocode of two algorithms. Algorithm 1 shows how to distill the synthetic program from the agent and it refers to Step 3 of Fig. <ref type="figure">4</ref>. Algorithm 1 shows how the distilled program is applied at inference time to a new user and it refers to Step 4 of Fig. <ref type="figure">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B: Domain specific languages (DSL) and causal graphs</head><p>We now show the Domain Specific Languages (DSL) used for the german (Table <ref type="table">3</ref>), synthetic (Table <ref type="table">4</ref>) and adult (Table <ref type="table">5</ref>) experiments. For each setting, we show the functions available, the argument type they accept and an exhaustive list of the potential arguments. Each program operates on a single feature. The name of the program suggests the feature it operates on (e.g., CHANGE_JOB operate on the feature job). The programs which accept Author contributions GDT designed the method, conducted the data collection process, built the experimental infrastructure and performed the relevant experiments. BL and AP contributed to the design of the method, provided supervision and resources. All authors contributed to the writing of the manuscript.</p><p>Publisher's Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Visualizing the effects of predictor variables in black box supervised learning models</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">W</forename><surname>Apley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Statistical Methodology)</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1059" to="1086" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">mpi4py: Status update after 12 years of development</title>
		<author>
			<persName><forename type="first">L</forename><surname>Dalcin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<idno type="DOI">10.1109/MCSE.2021.3083216</idno>
		<ptr target="https://doi.org/10.1109/MCSE.2021" />
	</analytic>
	<monogr>
		<title level="j">Computing in Science Engineering</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">16</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The magical number seven, plus or minus two: Some limits on our capacity for processing information</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Review</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">81</biblScope>
			<date type="published" when="1956">1956</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Explanation in artificial intelligence: Insights from the social sciences</title>
		<author>
			<persName><forename type="first">T</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">267</biblScope>
			<biblScope unit="page" from="1" to="38" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Causality</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning compositional neural programs with recursive tree search and planning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Pierrot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ligner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Sigaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Perrin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Laterre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NeurIPS</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="14673" to="14683" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Mastering the game of go with deep neural networks and tree search</title>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Van Den Driessche</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">529</biblScope>
			<biblScope unit="page" from="484" to="503" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A general reinforcement learning algorithm that masters chess, shogi, and go through self-play</title>
		<author>
			<persName><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<idno type="DOI">10.1126/science.aar6404</idno>
		<ptr target="https://doi.org/10.1126/science.aar" />
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">362</biblScope>
			<biblScope unit="issue">6419</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A survey of contrastive and counterfactual explanation generation methods for explainable artificial intelligence</title>
		<author>
			<persName><forename type="first">I</forename><surname>Stepin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Alonso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C M</forename><surname>Pereira-Fariña</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="11974" to="12001" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">The EU general data protection regulation (GDPR): A practical guide</title>
		<author>
			<persName><forename type="first">P</forename><surname>Voigt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bussche</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
	<note>st ed.</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Counterfactual explanations without opening the black box: Automated decisions and the GDPR</title>
		<author>
			<persName><forename type="first">S</forename><surname>Wachter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Mittelstadt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Harvard Journal of Law and Technology</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page">841</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">DeepCoder: Learning to write programs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Balog</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Gaunt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Tarlow</surname></persName>
		</author>
		<ptr target="https://openreview.net/pdf" />
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>id= rkE3y 85ee</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">The hidden assumptions behind counterfactual explanations and principal reasons</title>
		<author>
			<persName><forename type="first">S</forename><surname>Barocas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Selbst</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Raghavan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>In FAT*</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Leveraging grammar and reinforcement learning for neural program synthesis</title>
		<author>
			<persName><forename type="first">R</forename><surname>Bunel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=H1Xw62kRZ" />
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Efficient selectivity and backup operators in Monte-Carlo tree search</title>
		<author>
			<persName><forename type="first">R</forename><surname>Coulom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings computers and games</title>
		<meeting>computers and games</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Optimal action extraction for random forests and boosted trees</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining</title>
		<meeting>the 21th ACM SIGKDD international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="179" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Multi-objective counterfactual explanations</title>
		<author>
			<persName><forename type="first">S</forename><surname>Dandl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Molnar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Bischl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">PPSN</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="448" to="469" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning compositional programs with arguments and sampling</title>
		<author>
			<persName><forename type="first">G</forename><surname>De Toni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Erculiani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Passerini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AIPLANS</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">UCI machine learning repository</title>
		<author>
			<persName><forename type="first">D</forename><surname>Dua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Graff</surname></persName>
		</author>
		<ptr target="http://archive.ics.uci.edu/ml" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Greenwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">C</forename><surname>Boehmke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Mccarthy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.04755</idno>
		<title level="m">A simple and effective model-based variable importance measure</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Local rule-based explanations of black box decision systems</title>
		<author>
			<persName><forename type="first">R</forename><surname>Guidotti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Monreale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ruggieri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Pedreschi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Turini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Giannotti</surname></persName>
		</author>
		<idno>arxiv: 1805. 10820</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dace: Distribution-aware counterfactual explanation by mixed-integer linear optimization</title>
		<author>
			<persName><forename type="first">K</forename><surname>Kanamori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Takagi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kobayashi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Arimura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2855" to="2862" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Model-agnostic counterfactual explanations for consequential decisions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Karimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Barthe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Balle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Valera</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="895" to="905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Karimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Barthe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Valera</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.04050</idno>
		<title level="m">A survey of algorithmic recourse: Definitions, formulations, solutions, and prospects</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Algorithmic recourse: from counterfactual explanations to interventions</title>
		<author>
			<persName><forename type="first">A</forename><surname>Karimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Valera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FaccT</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="353" to="362" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Algorithmic recourse under imperfect causal knowledge: A probabilistic approach</title>
		<author>
			<persName><forename type="first">A</forename><surname>Karimi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Von Kügelgen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Valera</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>In NeurIPS. https:// proce edings. neuri ps. cc/ paper/ 2020/ file/ 02a3c 7fb3f 48928 8ae69 42498 498db 20-Paper. pdf</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Bandit based Monte-Carlo planning</title>
		<author>
			<persName><forename type="first">L</forename><surname>Kocsis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szepesvári</surname></persName>
		</author>
		<idno type="DOI">10.1007/11871842_29</idno>
		<ptr target="https://doi.org/10.1007/11871842_29" />
	</analytic>
	<monogr>
		<title level="m">ECML</title>
		<meeting><address><addrLine>Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="282" to="293" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Explaining machine learning classifiers through diverse counterfactual explanations</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">K</forename><surname>Mothilal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Tan</surname></persName>
		</author>
		<editor>FAT*</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="607" to="617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Consequence-aware sequential counterfactual generation</title>
		<author>
			<persName><forename type="first">P</forename><surname>Naumann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Ntoutsi</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-86520-7_42</idno>
		<idno>978-3-030-86520-7_ 42</idno>
		<ptr target="https://doi.org/10.1007/" />
	</analytic>
	<monogr>
		<title level="m">ECMLP-KDD</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Synthesizing action sequences for modifying model decisions</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ramakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Albarghouthi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In AAAI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="5462" to="5469" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Causal discovery and inference: concepts and recent methodological advances</title>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Applied Informatics</title>
		<imprint>
			<publisher>SpringerOpen</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="28" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Causal discovery from changes</title>
		<author>
			<persName><forename type="first">J</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the seventeenth conference on uncertainty in artificial intelligence</title>
		<meeting>the seventeenth conference on uncertainty in artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="512" to="521" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Interpretable predictions of tree-based ensembles via actionable feature tweaking</title>
		<author>
			<persName><forename type="first">G</forename><surname>Tolomei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Silvestri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Haines</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lalmas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining</title>
		<meeting>the 23rd ACM SIGKDD international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="465" to="474" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Decisions, counterfactual explanations and strategic behavior</title>
		<author>
			<persName><forename type="first">S</forename><surname>Tsirtsis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rodriguez</surname></persName>
		</author>
		<ptr target="https://proceedings.neurips.cc/paper/2020/hash/c" />
	</analytic>
	<monogr>
		<title level="m">Neu-rIPS</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Ustun</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Spangher</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2019">2020. 2019</date>
			<biblScope unit="page" from="10" to="19" />
		</imprint>
	</monogr>
	<note>FAT*</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Extracting incentives from black-box decisions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Yonadav</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">S</forename><surname>Moses</surname></persName>
		</author>
		<idno>arxiv: 1910. 05664</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
