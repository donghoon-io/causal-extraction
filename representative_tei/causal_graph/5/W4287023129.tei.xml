<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Temporal Graph Network Embedding with Causal Anonymous Walks Representations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Ilya</forename><surname>Makarov</surname></persName>
							<email>iamakarov@hse.ru</email>
						</author>
						<author>
							<persName><forename type="first">Andrey</forename><surname>Savchenko</surname></persName>
							<email>avsavchenko@hse.ru</email>
						</author>
						<author>
							<persName><forename type="first">Arseny</forename><surname>Korovko</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Leonid</forename><surname>Sherstyuk</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Nikita</forename><surname>Severin</surname></persName>
							<email>severin.nn@phystech.edu</email>
						</author>
						<author>
							<persName><forename type="first">Aleksandr</forename><surname>Mikheev</surname></persName>
							<email>agmikheev@sberbank.ru</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">HSE University Moscow</orgName>
								<address>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">University of Ljubljana Ljubljana</orgName>
								<address>
									<country key="SI">Slovenia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">HSE University Moscow</orgName>
								<address>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Moscow Institute of Physics and Technology Moscow</orgName>
								<address>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff4">
								<orgName type="department">Sber AI Lab Moscow</orgName>
								<orgName type="institution">HSE University Moscow</orgName>
								<address>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff5">
								<orgName type="department" key="dep1">Dmitrii Babaev</orgName>
								<orgName type="department" key="dep2">Sber AI Lab Moscow</orgName>
								<address>
									<country key="RU">Russia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Temporal Graph Network Embedding with Causal Anonymous Walks Representations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:18+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Temporal networks</term>
					<term>dynamic networks</term>
					<term>temporal network embedding</term>
					<term>temporal random walks</term>
					<term>temporal graph attention</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Many tasks in graph machine learning, such as link prediction and node classification, are typically solved by using representation learning, in which each node or edge in the network is encoded via an embedding. Though there exists a lot of network embeddings for static graphs, the task becomes much more complicated when the dynamic (i.e. temporal) network is analyzed. In this paper, we propose a novel approach for dynamic network representation learning based on Temporal Graph Network by using a highly custom message generating function by extracting Causal Anonymous Walks. For evaluation, we provide a benchmark pipeline for the evaluation of temporal network embeddings. This work provides the first comprehensive comparison framework for temporal network representation learning in every available setting for graph machine learning problems involving node classification and link prediction. The proposed model outperforms state-of-the-art baseline models. The work also justifies the difference between them based on evaluation in various transductive/inductive edge/node classification tasks. In addition, we show the applicability and superior performance of our model in the real-world downstream graph machine learning task provided by one of the top European banks, involving credit scoring based on transaction data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>It is crucial for banks to be able to predict possible future interactions between companies: knowing that one company will be a client of another one gives an opportunity to offer financial and other siervices. It is also important to have comprehensive and meaningful information about each client. If this knowledge is expressed as client embeddings, then the problem of their compactness and expressiveness emerges. Banks own large datasets of financial interactions between their clients, which can be used for training and testing models solving link prediction.</p><p>Graph structures that describe dependencies between data are nowadays widely used to improve effectiveness of machine learning models trained on streaming data. In order to use conventional machine learning frameworks, it is necessary to develop a vector representation of the graph (such as network embeddings) by combining attributes from nodes (labels, text, etc.) and edges (i.e. weights, labels, timing) and taking into account the dynamic graph structure appearing in real-world problems. Existing graph embedding models have been actively studied in recent years in attempts to apply deep learning methods for network representation learning. Hundreds of graph embeddings models have been developed in computer vision, text processing, recommendation systems, and interdisciplinary research in biology and genetics <ref type="bibr" target="#b18">[19]</ref>. All approaches are united by a common problem statement, which is to learn a model for the selected type of networks and important graph statistics. This will make it possible to apply standard machine learning frameworks and at the same time generalize attribute and structural information from the data.</p><p>Recently, modern machine learning methods have been proposed for processing networks and solving various prediction tasks on the arXiv:2108.08754v2 [cs.LG] 24 Aug 2021 local level. <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b41">42]</ref>. Examples include node classification and link prediction, for nodes both seen and unseen during the process of model training. In practice, they represent important problems to be solved on large dynamic network data, for e.g., transaction data between companies and bank clients can be used to predict future transactions; user search history on the Web can be used to generate contextual advertising instances; disease transmission data can be used to predict epidemics dynamics.</p><p>Although most of the previous works on graph representation learning mainly focus on static graphs (with a fixed set of nodes and edges), there are many real-world applications in which graphs evolve over time, like social networks or sales data. One particularly common sub-type of graphs used to represent such structures is a temporal graph. It is a graph in which each edge has a time index, indicating a moment in time when the interaction, represented by an edge, occurred.</p><p>There are various problems when switching from static to dynamic networks <ref type="bibr" target="#b45">[46]</ref>, among which computational complexity and variance of connectivity patterns over time. In addition, for practical applications, one needs to have models suitable for inference in inductive settings which enable proper prediction on the fly with rear overall model retraining for large network data <ref type="bibr" target="#b27">[28]</ref>.</p><p>In this work, we describe a novel network embedding which combines the best elements of the efficient Temporal Graph Network embedding (TGN) <ref type="bibr" target="#b11">[12]</ref> with fast computed historic node embeddings and highly precise Causal Anonymous Walks (CAW) <ref type="bibr" target="#b60">[61]</ref> with attention over temporal edge dynamics. To properly evaluate the proposed model, we present an experimental framework for temporal network embedding evaluation in downstream graph machine learning tasks, allowing integration of various temporal network embeddings and different temporal network data under a unified evaluation framework shown in Fig. <ref type="figure" target="#fig_0">1</ref>.</p><p>Our main contributions with this work consist of the following:</p><p>(1) Novel temporal network embedding model achieving stateof-the-art results in various temporal graph machine learning tasks; (2) Standardized temporal network embedding evaluation framework and comparison of state-of-the-art models under common training setting, providing new insights and clarification of real-world performance compared to reported in the original research articles.</p><p>In addition, we prove the effectiveness of the proposed pipeline and its sub-modules via extensive ablation study and provide the industrial application of the proposed approach, involving the transaction data of a major European bank. We showed that feature enrichment of temporal attention over temporal edge random walks improves both quantitative and qualitative results in the real-world application of a banking graph machine learning task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">PRELIMINARIES &amp; PROBLEM STATEMENT</head><p>In order to proceed with the problem statement, we describe basic concepts used throughout the text following notations by <ref type="bibr" target="#b27">[28]</ref>. We use G(V, E) to denote a static graph, where V is the set of its vertices, and E is the set of edges of the graph, and A is an adjacency matrix of that graph. Dynamic graph in general is a graph, which structure and node/edge attributes change over time.</p><p>Generally, events may contain an updated state of the node, but in our experiments we consider all node features to be static, and represent them as X -|ğ‘‰ | Ã— ğ‘˜ matrix, where ğ‘˜ is the node feature dimensionality.</p><p>In this work, we will use dynamic graph and temporal graph as interchangeable terms. We outline two possible kinds of dynamic graph representations below. There are two standard views on temporal network representation as a data structure:</p><p>â€¢ Discrete-time dynamic graph (DTDG) or snapshot-like graph is a sequence of snapshots from a dynamic graph, sampled at regularly-spaced times. Formally, we define DTDG as a set {G where ğ‘’ğ‘£ğ‘’ğ‘›ğ‘¡_ğ‘ğ‘œğ‘œğ‘Ÿğ‘‘ğ‘–ğ‘›ğ‘ğ‘¡ğ‘’ğ‘  are ordered pairs of nodes, between which the event has occurred, and ğ‘’ğ‘£ğ‘’ğ‘›ğ‘¡_ğ‘‘ğ‘ğ‘¡ğ‘ is any additional data on the event.</p><p>Event stream may be represented as concatenation of index matrix containing transactions vectors ğ‘’ ğ‘– ğ‘— (ğ‘¡) = (ğ‘£ ğ‘– , ğ‘£ ğ‘— , ğ‘¡) with source and target node IDs, timestamp, and temporal edges' features.</p><p>We will refer to both CTDG and DTDG as dynamic graphs, although we will focus more on CTDGs as a natural representation of transactions in banking networks, which appear in non-uniform timestamps and represent a real-world streaming structured data, rather than discretized snapshot representation DTDG.</p><p>Finally, in our study, we consider such temporal graph machine learning problems as node classification and link prediction, both in transductive and inductive settings:</p><p>â€¢ Transductive edge prediction evaluates whether a transaction between two priorly known nodes occurred at a given time; â€¢ Inductive edge prediction predicts a transaction between known and unknown nodes at a given time; â€¢ Transductive node classification determines the dynamic label of a priorly known node; â€¢ Inductive node classification determines the dynamic label of a priorly unknown node.</p><p>All the mentioned above problems on the chosen datasets can be formulated as binary classification (with effortless extension to a multi-class case), which is evaluated via AUC-ROC (area under curve for receiver operating characteristic) and AP (average precision) quality metrics measuring performance for the classification at various error threshold settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RELATED WORK</head><p>In this section, we overview state-of-the-art methods of constructing network embeddings for static and dynamic networks using taxonomies suggested in <ref type="bibr" target="#b18">[19]</ref> and <ref type="bibr" target="#b10">[11]</ref>. We focus on dynamic network embedding as an evolutionary process of graph formation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Static graph embedding methods</head><p>When constructing network embeddings via solving the optimization problem, researchers usually focus on three main concepts: matrix factorization, node sequence methods and methods based on deep learning. We consider the snapshot method, in which the current snapshot of a temporal network is taken and missing links are predicted based on the available graph information.</p><p>Factorization techniques can be applied to different graph representations and optimized for different objectives, such as direct decomposition of the graph adjacency matrix <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b48">49]</ref> or approximating proximity matrix <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b53">54]</ref>. Despite factorizations being widely used in recommendation systems, these models have high computational complexity and are difficult to extend for inductive learning.</p><p>Inspired by word2vec <ref type="bibr" target="#b52">[53]</ref>, sequence-based methods aim to preserve local node neighborhoods based on random walks. The two most prominent examples of models in this class are DeepWalk <ref type="bibr" target="#b5">[6]</ref> and Node2vec <ref type="bibr" target="#b0">[1]</ref>. Anonymous graphs walks have been proposed in <ref type="bibr" target="#b19">[20]</ref>. However, their adaptations to temporal random walks have limited applications, since they require retraining after adding new edges.</p><p>Recently, advances in geometric deep learning led to the creation of graph neural networks combining the best out of both fully connected and convolutional neural network architectures. Most of them use the method of neighborhood information aggregation from graph convolution network (GCN) <ref type="bibr" target="#b38">[39]</ref> and extend it with classical deep learning architectures such as recurrent neural network (RNN) <ref type="bibr" target="#b24">[25]</ref>, attention mechanism <ref type="bibr" target="#b41">[42]</ref>, generative adversarial network (GAN) <ref type="bibr" target="#b34">[35]</ref>, and graph transformers <ref type="bibr" target="#b62">[63]</ref>.</p><p>Recent studies show that a combination of deep learning models with semi-supervised techniques gives state-of-the-art results in terms of scalability, speed and quality in downstream tasks <ref type="bibr" target="#b18">[19]</ref>. However, static models are limited by the necessity to retrain the model with each significant change of graph structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Dynamic graph embedding methods</head><p>Methods for dynamic graphs are often extensions of those for static ones, with an additional focus on the temporal dimension and update schemes <ref type="bibr" target="#b2">[3]</ref>. All these techniques can be categorized according to which model of graph evolution representation is chosen: Continuous-Time Dynamic Graphs (CTDG) or Discrete-Time Dynamic Graphs (DTDG).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">DTDG-focused methods.</head><p>Most of the early work on dynamic graph learning focuses exclusively on discrete-time dynamic graphs. Such models encode snapshots individually to create an array of embeddings or aggregate the snapshots in order to use a static method on them <ref type="bibr" target="#b54">[55]</ref>. All DTDG models can be divided into several categories, according to the approach they use for dealing with the temporal aspect of a dynamic graph.</p><p>Single-snapshot models. Static models are used on the graph snapshots to make predictions for the next one ( <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b46">47]</ref>). Another way of implementing this methodology, called TI-GCN (Time Interval Graph Convolutional Networks) via residual architectures was proposed in <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b61">62]</ref>. Besides single snapshots, these works use information from networks formation <ref type="bibr" target="#b43">[44]</ref> represented by edge differences of several snapshots.</p><p>Multi-snapshot models. In <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b51">52]</ref>, authors learn structural information of each snapshot by separate models. Authors of <ref type="bibr" target="#b35">[36]</ref> compute individual sets of random walks for each snapshot in Node2Vec fashion and learn final node embeddings jointly, while in <ref type="bibr" target="#b51">[52]</ref> autoencoders for each snapshot were trained in a consistent way to preserve similarity between consequent graph updates.</p><p>RNN-based models. In contrast to previous methods, models in this category aim to capture sequential temporal dependencies mostly by feeding the output node embeddings or graph structure of each snapshot into RNN. Thus, GCN is combined with long shortterm memory (LSTM) in <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b59">60]</ref> or gated recurrent units (GRU) in <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b56">57]</ref>. Following these ideas, authors of <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b47">48]</ref> add residual connections to propagate topological information between neighboring snapshots. Recently, some works <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b26">27]</ref> have proposed to use GANs in combination with RNNs. On the other hand, EvolveGCN <ref type="bibr" target="#b2">[3]</ref> argues that directly modeling dynamics of the node representation will hamper the model performance on graphs with dynamic node sets. Instead of treating node features as the input to RNN, it feeds the weights of the GCN into the RNN.</p><p>Temporal graph attention. Inspired by advances in natural language processing (NLP), models in this class leverage attentionmechanism to capture temporal information. In <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b25">26]</ref>, authors follow the RNN module by an attention mechanism to take into account the contribution of its hidden states or leverage self-attention mechanism without RNN stage <ref type="bibr" target="#b3">[4]</ref>.</p><p>Convolutional models. Although previous models can capture sequential dependencies, in practice, most of them use a limited number of historical snapshots. So, convolutions are used to propagate information between snapshots in <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b64">65]</ref>.</p><p>Despite promising results, most of the models struggle from two disadvantages. First, methods lose the order of edge formation and reflect only partial information on network formation <ref type="bibr" target="#b61">[62]</ref>. Second, computing static representations on each snapshot is inefficient in terms of memory usage on large graphs <ref type="bibr" target="#b63">[64]</ref> and can not be used in practical applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">CTDG-focused methods. Continuous-time dynamic graphs</head><p>require different approaches as it becomes computationally difficult to work with the entirety of such graphs after each interaction <ref type="bibr" target="#b42">[43]</ref>. Below we provide a more general classification of CTDG-focused methods, comparing with DTDG-focused ones based on approaches used for learning evolution patterns.</p><p>Temporal random-walks models. The approach implies including the time dependency directly in a sequence of nodes generated by random walks. Such methods create a corpus of walks over time (so-called "temporal random walks") with respect to the order of nodes/edges appearance in the graph. Based on this idea, authors of <ref type="bibr" target="#b49">[50]</ref> leverage a custom attention mechanism to learn the importance between a node and its temporal random-walk-based neighbors. Using this work, <ref type="bibr" target="#b16">[17]</ref> proposes several methods to select the subsequent nodes connected to a starting one. A promising method for link prediction task was proposed by the authors of <ref type="bibr" target="#b60">[61]</ref> who have developed Causal Anonymous Walks (CAWs), constructed from temporal random walks. CAWs adopt a novel anonymization strategy that replaces node identities with the hitting counts of the nodes based on a set of sampled walks to keep the method inductive.</p><p>The model outperforms previous methods in both inductive and transductive settings.</p><p>Local neighborhood models. When interactions happen (node or edge adding and removal), models in this class update embeddings of the relevant nodes by aggregating information from their new neighborhoods. In <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b63">64]</ref>, authors utilize a GCN-based aggregation scheme and propagate changes to higher-order neighbors of interacting nodes. To cope with information asymmetry, the authors of <ref type="bibr" target="#b15">[16]</ref> propose to determine the priority of the nodes that receive the latest interaction information. Models from <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b50">51]</ref> embed dynamic network for recommendation systems in similar ways. Several recent works <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b57">58]</ref> consider interactions between nodes as stochastic processes with probabilities depending on the network statistics.</p><p>Memory-based models. The core idea of this class of temporal network embeddings lies in extending existing models by using special memory modules storing a history of interactions for each node. Methods vary from updating LSTM <ref type="bibr" target="#b58">[59]</ref> to using augmented matrices of interactions <ref type="bibr" target="#b65">[66]</ref>. APAN (Asynchronous Propagation Attention Network) <ref type="bibr" target="#b55">[56]</ref> aims to store detailed information about k-hop neighborhood interactions of each node in so-called "mailboxes". The recently developed temporal graph network <ref type="bibr" target="#b11">[12]</ref> proposes a flexible framework, which consists of several independent modules and generalizes other recent CTDGs-focused models such as <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b50">51]</ref>. Combining the advantages of JODIE <ref type="bibr" target="#b50">[51]</ref> and temporal graph attention (TGAT) <ref type="bibr" target="#b9">[10]</ref>, TGN introduces the node-wise memory into the temporal aggregate phase of TGAT, showing stateof-the-art for industrial tasks results.</p><p>Because of the potential of CTDG-based models <ref type="bibr" target="#b21">[22]</ref> and necessity to apply the model to transaction data, we focus on developing a model in this class, while keeping in mind efficient DTDG models <ref type="bibr" target="#b61">[62]</ref>. In what follows, we describe the idea of improving existing state-of-the-art CTDG models by properly fusing memory-based, neighborhood and interaction information under unified framework, thus combining multiple best practices of CTDG methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">PROPOSED APPROACH</head><p>As stated above in our literature survey, the best-known model for temporal graph prediction tasks, namely, TGN <ref type="bibr" target="#b11">[12]</ref>, relies primarily on propagating information ("message") through edges and generates node embeddings from occurring edges in a straightforward manner. At the same time, it was noticed that the CAW <ref type="bibr" target="#b60">[61]</ref> shows excellent performance in link prediction tasks by learning edge representations. Unfortunately, the latter model cannot be directly applied to the extraction of node embeddings, and, consecutively, downstream graph machine learning tasks, such as node classification.</p><p>Hence, in this paper, we propose a novel network embedding taking the best out of the TGN framework and the CAW edge encodings (Fig. <ref type="figure" target="#fig_1">2</ref>) to improve the quality of edge and node classification. Our contribution is to leverage the highly informative edge embedding generated in CAW. This will allow us to refine various functions of the TGN framework, including generating messages and embeddings, as well as aggregating memory. Below, we describe our model by listing and describing all modules in a consecutive fashion. In the following descriptions and equations, LSTM refers to the Long Short-Term Memory type RNN, GRU is the Gated Recurrent Unit, "attn" and "self-attn" is attention and self-attention, respectively. A walk on a graph, performed backwards in time on a temporal graph (time-inverse walk) with length ğ‘€ is denoted by</p><formula xml:id="formula_0">ğ‘Š = ((ğ‘¤ 0 , ğ‘¡ 0 ), (ğ‘¤ 1 , ğ‘¡ 1 ), .., (ğ‘¤ ğ‘€ , ğ‘¡ ğ‘€ )), ğ‘¡ 0 &gt; ğ‘¡ 1 &gt; &gt; ğ‘¡ ğ‘€ , (ğ‘¤ ğ‘š-1 , ğ‘¤ ğ‘– , ğ‘¡ ğ‘š ) âˆˆ E, ğ‘š âˆˆ {1, 2, ..., ğ‘€ }<label>(1)</label></formula><p>where ğ‘¤ ğ‘š âˆˆ V is the node entered at step ğ‘š, ğ‘¡ ğ‘š is the time of step ğ‘š, (ğ‘¤ ğ‘š , ğ‘¡ ğ‘š ) is the ğ‘š-th node-time pair. In future, we will refer to the elements of the ğ‘š-th step as ğ‘Š</p><p>ğ‘š = ğ‘¤ ğ‘š and ğ‘Š</p><p>ğ‘š = ğ‘¡ ğ‘š . The ğ‘¡-timed ğ‘˜-hop neighborhood of ğ‘–-th node is a set of nodes which can be reached from ğ‘– in ğ‘˜ steps if one walks on the edges existing prior to time ğ‘¡, without regard for their direction:</p><formula xml:id="formula_3">ğœ‚ ğ‘˜ ğ‘– = {ğ‘¤ ğ‘— : âˆƒğ‘Š = ((ğ‘¤ 0 , ğ‘¡ 0 ), ..., (ğ‘¤ ğ‘— , ğ‘¡ ğ‘— )), ğ‘¤ 0 = ğ‘–, âˆ€ğ‘™ : ğ‘¡ ğ‘™ &lt; ğ‘¡, (ğ‘¤ ğ‘-1 , ğ‘¤ ğ‘ , ğ‘¡ ğ‘ ) âˆˆ E âˆ¨ (ğ‘¤ ğ‘ , ğ‘¤ ğ‘-1 , ğ‘¡ ğ‘-1 ) âˆˆ ğ¸, |ğ‘Š | â‰¤ ğ‘˜ + 1} (2)</formula><p>4.1.1 Neighborhood Edge Feature (NEF) Generator). This trainable module and its integration in the following modules are intended to improve the predictive capabilities of the original TGN framework, which is our core contribution. It generates highly informative feature representations of pairs of nodes for any given moment in time. For a pair of nodes ğ‘– and ğ‘—, the output of this module as ğ‘ ğ¸ğ¹ ğ‘– ğ‘— (ğ‘¡) is computed as follows.</p><p>At first, we sample an equal number ğ¾ &gt; 1 of the time-inverse walks for both nodes in order to capture information about the edge neighborhoods. All sampled walks have identical lengths (1-2 steps typically). A constant decay hyperparameter, which regulates how strongly the sampling process prioritizes more recent connections, is used during sampling of all walks.</p><p>The walk sets ğ‘† ğ‘– and ğ‘† ğ‘— are generated for edge ğ‘– ğ‘— between nodes ğ‘– and ğ‘—. ğ‘† ğ‘– consists of sampled time-inverse walks ğ‘Š ğ‘˜ , ğ‘˜ âˆˆ {1, ..., ğ¾ }. Each edge for each walk is sampled with the probabilities proportional to ğ‘’ğ‘¥ğ‘ (ğ›¼ â–³ğ‘¡), where ğ›¼ is the decay parameter, and â–³ğ‘¡ is the time difference between edges. The next step makes the sampled walks anonymous in order to limit extracted information to the edge neighborhood alone. Each node from ğ‘† ğ‘– is replaced by a pair of vectors that encode a positional frequency of the node in each corresponding position of walks in ğ‘† ğ‘– and ğ‘† ğ‘— . The encoding vector of positional frequencies relative to the walks of ğ‘– for node ğ‘¤ is defined as:</p><formula xml:id="formula_4">ğ‘”(ğ‘¤, ğ‘† ğ‘– ) = {|{ğ‘Š |ğ‘Š âˆˆ ğ‘† ğ‘– , ğ‘¤ = ğ‘Š (1)</formula><p>ğ‘š , ğ‘š âˆˆ {1, ..., ğ‘€ }}|}.</p><p>(</p><formula xml:id="formula_5">)<label>3</label></formula><p>This equation simply specifies that the node index ğ‘¤ is encoded as a vector, so that each its ğ‘š-th component is equal to the number of times where ğ‘¤ is the ğ‘š-th node of some walk in ğ‘† ğ‘– .</p><p>The anonymization of walks <ref type="bibr" target="#b60">[61]</ref> is achieved by using the defined function to transform node indices in walks. Each position in any walk of ğ‘† ğ‘– containing ğ‘¤ is replaced by</p><formula xml:id="formula_6">ğ¼ ğ‘– ğ‘— (ğ‘¤) = {ğ‘”(ğ‘¤, ğ‘† ğ‘– ), ğ‘”(ğ‘¤, ğ‘† ğ‘— )}.<label>(4)</label></formula><p>Similarly, any value in walks of ğ‘† ğ‘— filled by ğ‘¤ is replaced with ğ¼ ğ‘—ğ‘– (ğ‘¤). In the remaining part of this section we write ğ¼ = ğ¼ ğ‘– ğ‘— , assuming the known orientation of the edge.</p><p>The remaining steps simply attempt to transform the "anonymous walk" representation of the node pair to a more compressed and usable state. Each obtained representation of each position of each walk, i.e. pairs of {ğ‘”(ğ‘¤, ğ‘† ğ‘— ), ğ‘”(ğ‘¤, ğ‘† ğ‘– )}, are fed through separate instances of the same two-layered multi-layered perceptron (MLP) and sum-pooled:</p><formula xml:id="formula_7">ğ‘“ 1 (ğ¼ (ğ‘¤)) = MLP(ğ‘”(ğ‘¤, ğ‘† ğ‘– )) + MLP(ğ‘”(ğ‘¤, ğ‘† ğ‘— ))<label>(5)</label></formula><p>Then, this representation is concatenated with the time-difference encoding and node or edge features of the corresponding step:</p><formula xml:id="formula_8">â„(ğ¼ (ğ‘¤)) = concat(ğ‘“ 1 (ğ¼ (ğ‘¤)), ğ‘“ 2 (â–³ğ‘¡), ğ‘‹ )<label>(6)</label></formula><p>where ğ‘“ 2 (â–³ğ‘¡) is time Fourier features, and ğ‘‹ is a concatenation of all relevant node and edge attributes of the corresponding walk step. Finally, each walk with encoded positions is passed to an RNN (usually, Bi-LSTM):</p><formula xml:id="formula_9">enc(ğ‘Š ) = Bi-LSTM({â„(ğ¼ (ğ‘¤ ğ‘š )), ğ‘š âˆˆ {1, ..., ğ‘€ }}),<label>(7)</label></formula><p>where ğ‘¤ ğ‘š being ğ‘š-th position of walk ğ‘Š . The encoded walks are aggregated across all walks for the node pair ğ‘– ğ‘—:</p><formula xml:id="formula_10">ğ‘ ğ¸ğ¹ ğ‘– ğ‘— (ğ‘¡) = 1 |ğ‘† ğ‘– âˆª ğ‘† ğ‘— | âˆ‘ï¸ ğ‘Š âˆˆ (ğ‘† ğ‘– âˆªğ‘† ğ‘— ) agg(enc(ğ‘Š )),<label>(8)</label></formula><p>where |ğ‘† ğ‘– âˆª ğ‘† ğ‘— | is amount of walks in a set ğ‘† ğ‘– âˆª ğ‘† ğ‘— , and agg is either self-attention module or identity for mean pooling aggregation.</p><p>4.1.2 Message store. In order to apply the gradient descent, a memory of a node should be updated after it is passed as a training instance. Let's say an event associated with the ğ‘–-th node has occurred, e.g. an edge involving ğ‘– has been passed for training in the current batch. Then, all information about the batch transactions involving node ğ‘– will be recorded in the message store after the batch inference, replacing the existing records for ğ‘–. This information is used and updated during the processing of the next batch involving ğ‘–.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Message</head><p>Generator. When the model processes a batch containing a node, all transaction information about edges associated with this node is pulled. For each transaction between ğ‘– and some other node ğ‘— at a time ğ‘¡, a message for a node ğ‘– is computed as a concatenation of the current memory vectors of the nodes, edge features, time-related features, and neighborhood edge features of the corresponding edge:</p><p>ğ‘š ğ‘– (ğ‘¡) = ğ‘ğ‘œğ‘›ğ‘ğ‘ğ‘¡ (ğ‘  ğ‘– (ğ‘¡ -), ğ‘  ğ‘— (ğ‘¡ -), ğ‘¡ -ğ‘¡ -, ğ‘’ ğ‘– ğ‘— (ğ‘¡), ğ‘ ğ¸ğ¹ ğ‘– ğ‘— (ğ‘¡)), <ref type="bibr" target="#b8">(9)</ref> where ğ‘– ğ‘— is the index of a transaction between nodes ğ‘– and ğ‘—, ğ‘  ğ‘– (ğ‘¡ -) is a memory state of node ğ‘– at time ğ‘¡ - ğ‘– of last memory update for node ğ‘–, and ğ‘ ğ¸ğ¹ ğ‘– ğ‘— (ğ‘¡) is a neighborhood edge feature vector for edge ğ‘– ğ‘—. The usage of NEF features here is our novel idea aimed to include more information about the type of the update message. The generated messages will update memory states of the batch nodes. Note that, aside from the basic and non-trainable concatenation, other choices for the message function, like MLPs are possible.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.4">Message Aggregator.</head><p>To perform a memory update on a node at time ğ‘¡, its message representation is obtained by aggregating all currently stored messages timestamps ğ‘¡ 1 &lt; ğ‘¡ 2 &lt; ... &lt; ğ‘¡ which are related to this node:</p><formula xml:id="formula_11">ğ‘š ğ‘– (ğ‘¡) = aggr(ğ‘š ğ‘– (ğ‘¡ 1 ), ğ‘š ğ‘– (ğ‘¡ 2 ), ..., ğ‘š ğ‘– (ğ‘¡)).<label>(10)</label></formula><p>Here aggregation function ğ‘ğ‘”ğ‘”ğ‘Ÿ can be computed as a mean of generated messages. It is still possible to use the most recent message ğ‘š ğ‘– (ğ‘¡) as a value of ğ‘š ğ‘– (ğ‘¡).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.5">Memory</head><p>Updater. The message generator and aggregator let the model to encode useful transaction information as memory vectors. In particular, the memory state vector for node ğ‘– is updated at time ğ‘¡ by applying an RNN-type model to the concatenation of the received message and previous memory state:</p><formula xml:id="formula_12">ğ‘  ğ‘– (ğ‘¡) = RNN(ğ‘š ğ‘– (ğ‘¡), ğ‘  ğ‘– (ğ‘¡ -)) (<label>11</label></formula><formula xml:id="formula_13">)</formula><p>where RNN is either GRU or LSTM, and initial state ğ‘  (0) is initialized with a random vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.6">Embedding</head><p>Generator. This module generates the node embedding based on memory states of the node and its k-hop neighborhood, features of the neighborhood, and NEF representations of "virtual" edges between the node and its direct neighbors. Resulting node embeddings can be viewed as autonomous node representations for classification. We propose, similarly to message generator, to add NEF features to node ğ‘– in order to let the model better discriminate neighbors on their relevance or type:</p><formula xml:id="formula_14">ğ‘§ ğ‘– (ğ‘¡) = âˆ‘ï¸ ğ‘— âˆˆğœ‚ 1 ğ‘– â„ 0 (ğ‘  ğ‘– (ğ‘¡), ğ‘  ğ‘— (ğ‘¡), ğ‘’ ğ‘– ğ‘— , ğ‘£ ğ‘– (ğ‘¡), ğ‘£ ğ‘— (ğ‘¡), ğ‘ ğ¸ğ¹ ğ‘– ğ‘— (ğ‘¡))+ âˆ‘ï¸ ğ‘— âˆˆğœ‚ ğ‘˜ ğ‘– ,ğ‘˜&gt;1</formula><p>â„ 1 (ğ‘  ğ‘– (ğ‘¡), ğ‘  ğ‘— (ğ‘¡), ğ‘’ ğ‘– ğ‘— , ğ‘£ ğ‘– (ğ‘¡), ğ‘£ ğ‘— (ğ‘¡)), <ref type="bibr" target="#b11">(12)</ref> where ğ‘£ ğ‘– (ğ‘¡) is a feature vector of node ğ‘– at time ğ‘¡, and â„ 0 and â„ 1 are the units of the neural network. It is typical to use MLPs as â„ 0 and â„ 1 , but the following concatenation by self-attention is more preferable to capture complex dependencies involving NEFs.</p><p>4.1.7 Embedding Decoder. This is the final module, which transforms node embeddings into prediction results for downstream tasks. In this paper, we always use MLP with 3 layers with only node embeddings as inputs, and sigmoid or softmax output layers. For example, the multi-class classification problem for node ğ‘– at time ğ‘¡ is solved using the following equation:</p><formula xml:id="formula_15">ğ‘œğ‘¢ğ‘¡ ğ‘– (ğ‘¡) = Softmax(MLP(ğ‘§ ğ‘– (ğ‘¡))).<label>(13)</label></formula><p>Similarly, the edge prediction task is solved as follows:</p><formula xml:id="formula_16">ğ‘œğ‘¢ğ‘¡ ğ‘– ğ‘— (ğ‘¡) = Sigmoid(MLP(concat(ğ‘§ ğ‘– (ğ‘¡)ğ‘§ ğ‘— (ğ‘¡)))). (<label>14</label></formula><formula xml:id="formula_17">)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Core idea and novelty of proposed model</head><p>Let us provide a high-level description of the model (Fig. <ref type="figure" target="#fig_1">2</ref>). It learns to generate a temporal embedding for each node and decode embeddings into inputs for each classification task. The model assigns a memory vector to each node and generates each node embedding by aggregating memory vectors and other relevant features in a neighborhood of the node. Node memory vectors describe relevant information about interactions involving the node. They are updated using specified node messages, which, in turn, encode information about the last transaction involving the node. This design allows employing gradient descent for training memory and message generating modules. A decoder MLP transforms a pair of node embeddings into the probability of a temporal edge existing between the nodes. Similarly, it can be trained to transform a single node embedding into probabilities of the node belonging to each of the existing classes for a node classification problem. There are two main modifications of the proposed model compared to the baseline TGN framework <ref type="bibr" target="#b11">[12]</ref>. First, we change the message generating function, which provides the model with an additional way to differentiate the messages based on their relevance. The NEF features of an edge contain information about the walk correlation of the two nodes. As it can be used for very accurate link prediction <ref type="bibr" target="#b60">[61]</ref>, it may be also used to classify some messages as being irrelevant, and diminish their effect on the memory update.</p><p>Second, while the original version of the embedding module <ref type="bibr" target="#b11">[12]</ref> allows treating different k-hop neighbors of the node differently if using attention for aggregation, it might be beneficial to provide the NEF features of connections between the node and its closest neighbors. As a result, we again take into account the walk correlation between the node and its neighbor, so that the differences in neighborhood type can be more evident.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EVALUATION FRAMEWORK</head><p>In this section, we discuss our research methodology. This involves the evaluation framework, evaluation pipeline training settings, hyper-parameter choice and description of temporal networks used for the evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Pipeline</head><p>The main contribution of the proposed framework (Fig. <ref type="figure" target="#fig_0">1</ref>) is an easy-to-use unified data processing toolkit for accurate evaluation of temporal network embeddings in downstream tasks under common training settings, which allows to remove contradictions in experimental results reported in research articles in the field. The source code of this tool is publicly available 1 . It is focused on transforming any graph into a universal format that is afterwards fed into the pipeline in either DTDG or CTDG format with following features:</p><p>â€¢ selection of precise batching options;</p><p>â€¢ preparing data for both inductive and transductive settings (with support for bipartite data); â€¢ interfacing with any kind of graph embedding models using an interface for model-framework communication, treating network embedding models as a black-box, which is then passed on to such basic methods as train_model, pre-dict_edge_probabilities, etc. We follow a standard setting, where the number of negative samples is equal to the number of edges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Training settings</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Batch specification.</head><p>The model is trained by passing edges (which includes negative samples) in batches sorted in chronological order.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5.2.3</head><p>Training with temporal data masking. We use 80%-10%-10% train-val-test split with randomized training sets, supported via several data masking schemes. Node masking hides nodes (as well as all connected edges) from training data. The masked nodes for transductive tasks are also removed from the validation and test data, while for inductive tasks masked nodes remain in validation and test data. Edge masking removes a fixed percentage of random edges from the whole dataset. We use two options for both masking techniques: the percentage of masked information is 10% and 70%, representing dense and sparse training settings, respectively. In particular, we report the results for three combinations, in which at least one mask is large, namely 10%-75% node-edge masking, 75%-10% and 75%-75%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.4">Runs and validation.</head><p>Each model/setting/dataset combination was run 10 times with random seeds and node/edge masking.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Comparison with the state-of-the-art models</head><p>In order to evaluate our model (Fig. <ref type="figure" target="#fig_1">2</ref>), we compare it to the baseline TGN framework architectures (configured as described in <ref type="bibr" target="#b11">[12]</ref>), DyRep (supporting long-term and short-term time scales of graph evolution and scaling to large graphs) <ref type="bibr" target="#b44">[45]</ref> and Jodie (working with bipartite graphs using node embeddings for predictive tasks with future time lag) <ref type="bibr" target="#b50">[51]</ref>. Though these models can construct high-quality historic embeddings using information propagated from nodes, they do not take into account edge information and interaction patterns. Hence, we could demonstrate the advantages of our approach in identical training settings in comparison to other state-of-the-art temporal network embeddings. It is important to emphasize that the CAW model cannot be used here because it was  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Datasets</head><p>In our study, we focus on well-known benchmark datasets for temporal networks. Their descriptive statistics are presented in Table <ref type="table" target="#tab_2">1</ref>, taking into account specifics of computing statistics for bipartite graphs of user-item interactions <ref type="bibr" target="#b36">[37]</ref>.</p><p>Labeled datasets. Both Reddit <ref type="bibr" target="#b31">[32]</ref> and Wikipedia <ref type="bibr" target="#b14">[15]</ref> datasets are bipartite graphs representing interactions between users (source) and web resources (target), like posting to a subreddit or editing a wiki page. Text content is encoded as edge features to provide context. Both datasets have a binary target, indicating whether a user was banned or not.</p><p>Non-labeled datasets. The UCI <ref type="bibr" target="#b39">[40]</ref> dataset contains a communication history for the students' forum. The Enron <ref type="bibr" target="#b22">[23]</ref> dataset is constructed over internal e-mail communication of Enron company employees. The Ethereum dataset 2 contains a directed graph of Ethereum blockchain transactions, which were collected from larger public BigQuery dataset 3 ; we collected a dataset containing all transactions, which occurred during a 9-hour span. All of the non-labeled datasets are non-bipartite, with no additional features for edges or nodes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">ABLATION STUDY</head><p>In this Section an ablation study of the proposed architecture (Fig. <ref type="figure" target="#fig_1">2</ref>) is provided to explore the impact of each submodule. Below we specify several additional modules added to interaction features processing pipeline for temporal network embedding via NEF generator,which outlined in Subsection 4. Additionally, we consider alternating three important NEF-related hyper-parameters:</p><p>â€¢ number of generated random walk NEF-samples;</p><p>â€¢ random walk depth;</p><p>â€¢ positional dimension of random-walk-related embeddings. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">EXPERIMENTAL RESULTS</head><p>In this section, we compute mean AUC-ROC and AP for our model and compare it to three other state-of-the-art models (DyRep, Jodie, TGN). Tables 3-Table <ref type="table" target="#tab_8">6</ref> show results for edge prediction task in transductive and inductive settings respectively. Best results are given in bold, while second best are underlined. "Node mask" and "Edge mask" columns specify the portion of nodes or edges used for model testing.  As evident from above figures, the proposed approach (Fig. <ref type="figure" target="#fig_1">2</ref>) consistently outperforms almost all other models. For example, our model achieves the best results in transductive edge prediction (Table <ref type="table" target="#tab_5">3</ref>) except one case, where DyRep achieved better results. Moreover, our method achieves the best results on all datasets for inductive edge prediction (Table <ref type="table" target="#tab_7">5</ref>). It seems that capturing networkwide common patterns with CAW-based features generally offers an improvement over existing models, although in some cases information added by these features may be duplicated by other encoding modules. Table <ref type="table" target="#tab_9">7</ref> and Table <ref type="table" target="#tab_10">8</ref> present the results for node classification tasks in transductive and inductive settings respectively. Though our model generally outperforms existing methods, there is one exception. Indeed, Jodie <ref type="bibr" target="#b50">[51]</ref> shows the best node classification quality on Wikipedia dataset. As a matter of fact, the Jodie model incorporates both static and dynamic node embeddings, which might contributed to better encoding of interests and behavior patterns of the Wikipedia editors</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">BANKING RESULTS</head><p>As an industrial application of the proposed framework, we chose a pre-existing problem posited by a major European banks with a corresponding dataset which consists of transactions between different companies. Each transaction has a timestamp so we can treat it as a temporal edge. This allows us to consider only "edge events". We use the following edge features: transaction amount, timestamp and type of transaction, (e.g. credit or state duty). We one-hot encode the type of transaction, which gives us 50 edge features in total: 49 from one-hot encoding and 1 from transaction amount. These features were normalized.</p><p>At first, we compared our model with the original TGN <ref type="bibr" target="#b11">[12]</ref> on a link prediction task. We obtain the training and validation datasets consisting of transactions from a regional subdivision of a major European bank for a time period about one week. This dataset contain about 2 millions of transactions. We select about 40000 newest transactions to test dataset and all the rest is given to the train i. e. test dataset transactions are always newer then train transactions. We use the following procedure to check performance of our model on test dataset. First, we split training data into small chunks (400 batches) and gradually increase the number of chunks We can see very high performance of dynamic graph approach models. It can be noticed that performance of our model grows a little bit slower than for TGN <ref type="bibr" target="#b11">[12]</ref>, but after about 600000 transactions our model takes the lead. This slight time lag in training is due to our model taking additional time to learn extra information about interactions between nodes via learning CAW <ref type="bibr" target="#b60">[61]</ref> part of our model. This extra information allows to gain better results in the end of training. The architecture of our model and TGN <ref type="bibr" target="#b11">[12]</ref> model allows us to produce node embeddings which can the be used as input for a wide range of possible downstream tasks. For our experiment we have taken prediction of company default as the downstream task. The dataset contains about 5000 companies, some of which will go bankrupt in a specific period of time in the future (180 days) and others will not. To train a classification model we used LightGBM <ref type="bibr" target="#b28">[29]</ref>. The obtained results were averaged over with 5-fold cross-validation and presented in Table <ref type="table" target="#tab_11">9</ref>.</p><p>Here our best model has 2.7% higher AUC ROC when compared to the TGN <ref type="bibr" target="#b11">[12]</ref>. That means that extra information about interactions between nodes captured by CAW <ref type="bibr" target="#b60">[61]</ref> part of our model is useful for classification problems. We see also that best performance is observed with small CAW messages shape. This is related to the structure of our dataset. It's number of edges is comparable with number of nodes, thus the graph is sparse. Bigger CAW messages shapes can be useful for more dense graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">CONCLUSION</head><p>In this work, we proposed a novel model for temporal graph embedding (Fig. <ref type="figure" target="#fig_1">2</ref>) that shows improvement over existing methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b50">51]</ref> on various prediction tasks while preserving the ability to generate node embeddings. Moreover, we implemented a novel experimental framework (Fig. <ref type="figure" target="#fig_0">1</ref>) that can process most kinds of graph data and an arbitrary dynamic graph inference model. Experimental study demonstrates the applicability of our method to solving various node/edge prediction tasks on temporal networks, and significantly improving the existing results.</p><p>In future, it is possible to improve the performance of our framework for its application to the real-life temporal graph of bank's transactions with tens of billions of nodes. It is necessary to study efficient node/edge sampling strategies, choosing those that overcome the limitations of current models when scaling to large graphs while preserving highly-informative edge features propagation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Diagram of the evaluation framework</figDesc><graphic coords="1,329.97,398.01,216.20,143.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Proposed model; numbers denote order of steps</figDesc><graphic coords="4,329.97,83.68,216.22,154.53" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>1 https://github.com/HSE-DynGraph-Research-team/DynGraphModelling</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>1 . 1 :(</head><label>11</label><figDesc>Msg) NEF-Message concatenating NEF features and messages; (Emb) NEF-Embed generates embeddings with NEF features; (RNN) NEF-LSTM including Bi-LSTM into NEF generator, instead of mean pooling across walk elements.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Inductive edge prediction on transactions of regional subdivision of major European bank</figDesc><graphic coords="9,77.82,83.68,192.20,98.38" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>1 , G 2 ,..., G ğ‘‡ , }, where G ğ‘¡ = {V ğ‘¡ , E ğ‘¡ } is the graph at moment ğ‘¡, V ğ‘¡ and E ğ‘¡ are sets of nodes and edges in G ğ‘¡ , respectively.â€¢ Continuous-time dynamic graph (CTDG) or transaction-/stream-like graph is denoted by pair (G, O), where G is a static graph representing an initial state of dynamic graph at time ğ‘¡</figDesc><table /><note><p>0 , and O is event stream of events/transaction, denoted by tuple (ğ‘’ğ‘£ğ‘’ğ‘›ğ‘¡_ğ‘ğ‘œğ‘œğ‘Ÿğ‘‘ğ‘–ğ‘›ğ‘ğ‘¡ğ‘’ğ‘ , ğ‘’ğ‘£ğ‘’ğ‘›ğ‘¡_ğ‘‘ğ‘ğ‘¡ğ‘, ğ‘¡ğ‘–ğ‘šğ‘’ğ‘ ğ‘¡ğ‘ğ‘šğ‘),</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>5.2.1 Negative sampling.It is common to design include negative edge sampling into evaluation of temporal graph prediction tasks. It is employed for balancing edges and non-connected pairs of nodes.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Dscriptive statistics for the dataset. Left to right: whether the graph is bipartite, number of unique nodes (representing users and items) and edges, labels, average degree, number of edge updates per node as source/target, and setting for splitting the temporal network into batches for snapshot DTDG models Bipartite Unique nodes Unique edges Positive labels / Fraction Average degree Average edges per n ode</figDesc><table><row><cell>Batch setting</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 contains</head><label>2</label><figDesc>AUC-ROC standard deviation and mean averaged 10 times. The task measured was inductive edge prediction 2 https://github.com/blockchain-etl/ethereum-etl 3 https://cloud.google.com/bigquery/public-data</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2 :</head><label>2</label><figDesc>Ablation study on Reddit dataset for inductive edge prediction</figDesc><table><row><cell>Enabled modules</cell><cell>AUC-ROC</cell></row><row><cell cols="2">(Msg)+(Emb)+(RNN) 0.894 Â± 0.034</cell></row><row><cell>(Emb)</cell><cell>0.893 Â± 0.035</cell></row><row><cell>(Msg)</cell><cell>0.888 Â± 0.042</cell></row><row><cell>(Emb)+(Msg)</cell><cell>0.878 Â± 0.051</cell></row><row><cell>(Msg)+(RNN)</cell><cell>0.876 Â± 0.047</cell></row><row><cell>(Emb)+(RNN)</cell><cell>0.870 Â± 0.055</cell></row><row><cell>TGN baseline</cell><cell>0.865 Â± 0.065</cell></row><row><cell cols="2">with default hyperparameters. Rather large and representative Red-</cell></row><row><cell cols="2">dit dataset is used in ablation study. Below we provide performance</cell></row><row><cell cols="2">metrics for seven different combinations of modules mentioned</cell></row><row><cell cols="2">above, except "(RNN)", which is considered a standalone TGN with</cell></row><row><cell cols="2">NEF features processing regularization. Here the resulting model</cell></row><row><cell cols="2">significantly improves the performance of standalone modifications</cell></row><row><cell>and TGN baseline.</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 3 :</head><label>3</label><figDesc>Transductive edge prediction, AUC-ROC; best in bold, second-best underlined</figDesc><table><row><cell></cell><cell cols="5">Node mask Edge mask DyRep Jodie TGN Our</cell></row><row><cell></cell><cell>10%</cell><cell>75%</cell><cell>0.774</cell><cell cols="2">0.534 0.638 0.759</cell></row><row><cell>Enron</cell><cell>75%</cell><cell>10%</cell><cell>0.647</cell><cell cols="2">0.528 0.682 0.795</cell></row><row><cell></cell><cell>75%</cell><cell>75%</cell><cell>0.595</cell><cell cols="2">0.537 0.673 0.745</cell></row><row><cell></cell><cell>10%</cell><cell>75%</cell><cell>0.964</cell><cell cols="2">0.704 0.974 0.977</cell></row><row><cell>Reddit</cell><cell>75%</cell><cell>10%</cell><cell>0.973</cell><cell cols="2">0.825 0.977 0.977</cell></row><row><cell></cell><cell>75%</cell><cell>75%</cell><cell>0.959</cell><cell>0.72</cell><cell>0.976 0.977</cell></row><row><cell></cell><cell>10%</cell><cell>75%</cell><cell>0.77</cell><cell cols="2">0.496 0.804 0.808</cell></row><row><cell>UCI</cell><cell>75%</cell><cell>10%</cell><cell>0.731</cell><cell cols="2">0.573 0.709 0.812</cell></row><row><cell></cell><cell>75%</cell><cell>75%</cell><cell>0.781</cell><cell cols="2">0.489 0.776 0.808</cell></row><row><cell></cell><cell>10%</cell><cell>75%</cell><cell>0.966</cell><cell cols="2">0.728 0.968 0.969</cell></row><row><cell>Wikipedia</cell><cell>75%</cell><cell>10%</cell><cell>0.964</cell><cell cols="2">0.737 0.971 0.975</cell></row><row><cell></cell><cell>75%</cell><cell>75%</cell><cell>0.962</cell><cell cols="2">0.719 0.967 0.97</cell></row><row><cell></cell><cell>10%</cell><cell>75%</cell><cell>0.728</cell><cell cols="2">0.914 0.932 0.933</cell></row><row><cell>Ethereum</cell><cell>75%</cell><cell>10%</cell><cell>0.868</cell><cell cols="2">0.929 0.936 0.937</cell></row><row><cell></cell><cell>75%</cell><cell>75%</cell><cell>0.73</cell><cell cols="2">0.919 0.934 0.934</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 4 :</head><label>4</label><figDesc>Transductive edge prediction, AP; best in bold, second-best underlined</figDesc><table><row><cell></cell><cell cols="5">Node mask Edge mask DyRep Jodie TGN Our</cell></row><row><cell></cell><cell>Lean</cell><cell>Strict</cell><cell>0.772</cell><cell cols="2">0.517 0.651 0.762</cell></row><row><cell>Enron</cell><cell>Strict</cell><cell>Lean</cell><cell>0.668</cell><cell cols="2">0.527 0.685 0.787</cell></row><row><cell></cell><cell>Strict</cell><cell>Strict</cell><cell>0.6</cell><cell cols="2">0.524 0.699 0.753</cell></row><row><cell></cell><cell>Lean</cell><cell>Strict</cell><cell>0.965</cell><cell cols="2">0.688 0.973 0.977</cell></row><row><cell>Reddit</cell><cell>Strict</cell><cell>Lean</cell><cell>0.974</cell><cell cols="2">0.832 0.977 0.977</cell></row><row><cell></cell><cell>Strict</cell><cell>Strict</cell><cell>0.96</cell><cell cols="2">0.699 0.977 0.977</cell></row><row><cell></cell><cell>Lean</cell><cell>Strict</cell><cell>0.804</cell><cell cols="2">0.503 0.827 0.826</cell></row><row><cell>UCI</cell><cell>Strict</cell><cell>Lean</cell><cell>0.756</cell><cell>0.546</cell><cell>0.75 0.824</cell></row><row><cell></cell><cell>Strict</cell><cell>Strict</cell><cell>0.812</cell><cell cols="2">0.492 0.809 0.827</cell></row><row><cell></cell><cell>Lean</cell><cell>Strict</cell><cell>0.969</cell><cell cols="2">0.708 0.971 0.972</cell></row><row><cell>Wikipedia</cell><cell>Strict</cell><cell>Lean</cell><cell>0.966</cell><cell cols="2">0.711 0.974 0.976</cell></row><row><cell></cell><cell>Strict</cell><cell>Strict</cell><cell>0.965</cell><cell cols="2">0.695 0.969 0.973</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 5 :</head><label>5</label><figDesc>Inductive edge prediction, AUC-ROC; best in bold, second-best underlined</figDesc><table><row><cell></cell><cell cols="5">Node mask Edge mask DyRep Jodie TGN Our</cell></row><row><cell></cell><cell>10%</cell><cell>75%</cell><cell>0.613</cell><cell cols="2">0.453 0.591 0.677</cell></row><row><cell>Enron</cell><cell>75%</cell><cell>10%</cell><cell>0.672</cell><cell cols="2">0.508 0.705 0.792</cell></row><row><cell></cell><cell>75%</cell><cell>75%</cell><cell>0.666</cell><cell cols="2">0.573 0.702 0.766</cell></row><row><cell></cell><cell>10%</cell><cell>75%</cell><cell>0.832</cell><cell cols="2">0.588 0.823 0.836</cell></row><row><cell>Reddit</cell><cell>75%</cell><cell>10%</cell><cell>0.91</cell><cell cols="2">0.294 0.917 0.924</cell></row><row><cell></cell><cell>75%</cell><cell>75%</cell><cell>0.88</cell><cell cols="2">0.298 0.907 0.917</cell></row><row><cell></cell><cell>10%</cell><cell>75%</cell><cell>0.602</cell><cell cols="2">0.576 0.599 0.642</cell></row><row><cell>UCI</cell><cell>75%</cell><cell>10%</cell><cell>0.723</cell><cell cols="2">0.431 0.725 0.733</cell></row><row><cell></cell><cell>75%</cell><cell>75%</cell><cell>0.718</cell><cell cols="2">0.423 0.735 0.74</cell></row><row><cell></cell><cell>10%</cell><cell>75%</cell><cell>0.95</cell><cell cols="2">0.584 0.945 0.953</cell></row><row><cell>Wikipedia</cell><cell>75%</cell><cell>10%</cell><cell>0.902</cell><cell cols="2">0.462 0.899 0.918</cell></row><row><cell></cell><cell>75%</cell><cell>75%</cell><cell>0.888</cell><cell>0.47</cell><cell>0.894 0.911</cell></row><row><cell></cell><cell>10%</cell><cell>75%</cell><cell>0.519</cell><cell cols="2">0.655 0.677 0.679</cell></row><row><cell>Ethereum</cell><cell>75%</cell><cell>10%</cell><cell>0.483</cell><cell cols="2">0.629 0.656 0.667</cell></row><row><cell></cell><cell>75%</cell><cell>75%</cell><cell>0.494</cell><cell cols="2">0.616 0.654 0.654</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 6 :</head><label>6</label><figDesc>Inductive edge prediction results, AP; best in bold, second-best underlined</figDesc><table><row><cell></cell><cell cols="5">Node mask Edge mask DyRep Jodie TGN Our</cell></row><row><cell></cell><cell>10%</cell><cell>75%</cell><cell>0.649</cell><cell cols="2">0.527 0.627 0.689</cell></row><row><cell>Enron</cell><cell>75%</cell><cell>10%</cell><cell>0.668</cell><cell cols="2">0.524 0.712 0.785</cell></row><row><cell></cell><cell>75%</cell><cell>75%</cell><cell>0.678</cell><cell>0.566</cell><cell>0.71 0.765</cell></row><row><cell></cell><cell>10%</cell><cell>75%</cell><cell>0.858</cell><cell cols="2">0.582 0.851 0.86</cell></row><row><cell>Reddit</cell><cell>75%</cell><cell>10%</cell><cell>0.924</cell><cell cols="2">0.393 0.931 0.936</cell></row><row><cell></cell><cell>75%</cell><cell>75%</cell><cell>0.899</cell><cell cols="2">0.391 0.917 0.928</cell></row><row><cell></cell><cell>10%</cell><cell>75%</cell><cell>0.664</cell><cell cols="2">0.576 0.663 0.69</cell></row><row><cell>UCI</cell><cell>75%</cell><cell>10%</cell><cell>0.762</cell><cell>0.46</cell><cell>0.763 0.771</cell></row><row><cell></cell><cell>75%</cell><cell>75%</cell><cell>0.756</cell><cell cols="2">0.461 0.773 0.773</cell></row><row><cell></cell><cell>10%</cell><cell>75%</cell><cell>0.952</cell><cell cols="2">0.551 0.949 0.954</cell></row><row><cell>Wikipedia</cell><cell>75%</cell><cell>10%</cell><cell>0.914</cell><cell cols="2">0.473 0.905 0.927</cell></row><row><cell></cell><cell>75%</cell><cell>75%</cell><cell>0.9</cell><cell cols="2">0.479 0.906 0.916</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 7 :</head><label>7</label><figDesc>Transductive node classification, AUC-ROC; best in bold, second-best underlined</figDesc><table><row><cell></cell><cell cols="6">Node mask Edge mask DyRep Jodie TGN Our</cell></row><row><cell></cell><cell>10%</cell><cell>75%</cell><cell>0.531</cell><cell cols="3">0.421 0.635 0.659</cell></row><row><cell>Reddit</cell><cell>75%</cell><cell>10%</cell><cell>0.601</cell><cell cols="3">0.435 0.584 0.627</cell></row><row><cell></cell><cell>75%</cell><cell>75%</cell><cell>0.597</cell><cell cols="3">0.439 0.602 0.658</cell></row><row><cell></cell><cell>10%</cell><cell>75%</cell><cell>0.771</cell><cell cols="3">0.842 0.735 0.81</cell></row><row><cell>Wikipedia</cell><cell>75%</cell><cell>10%</cell><cell>0.769</cell><cell cols="2">0.836 0.738</cell><cell>0.8</cell></row><row><cell></cell><cell>75%</cell><cell>75%</cell><cell>0.755</cell><cell>0.855</cell><cell>0.8</cell><cell>0.823</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 8 :</head><label>8</label><figDesc>Inductive node classification, AUC-ROC; best in bold, second-best underlined</figDesc><table><row><cell></cell><cell cols="5">Node mask Edge mask DyRep Jodie TGN</cell><cell>Our</cell></row><row><cell></cell><cell>10%</cell><cell>75%</cell><cell>0.51</cell><cell cols="3">0.456 0.576 0.625</cell></row><row><cell>Reddit</cell><cell>75%</cell><cell>10%</cell><cell>0.544</cell><cell cols="3">0.539 0.495 0.563</cell></row><row><cell></cell><cell>75%</cell><cell>75%</cell><cell>0.521</cell><cell cols="3">0.567 0.592 0.584</cell></row><row><cell></cell><cell>10%</cell><cell>75%</cell><cell>0.749</cell><cell cols="3">0.883 0.638 0.812</cell></row><row><cell>Wikipedia</cell><cell>75%</cell><cell>10%</cell><cell>0.619</cell><cell>0.73</cell><cell cols="2">0.649 0.766</cell></row><row><cell></cell><cell>75%</cell><cell>75%</cell><cell>0.665</cell><cell cols="3">0.743 0.597 0.745</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 9 :</head><label>9</label><figDesc>Node classification results based on node embeddings obtained from TGN and our proposed model with feature dimension ğ‘‘ being multiple of 8 Ã— 2 shape</figDesc><table><row><cell>Node Embeddings</cell><cell>AUC-ROC</cell></row><row><cell>TGN</cell><cell>0.626 Â± 0.072</cell></row><row><cell>Our, ğ‘‘ = 10</cell><cell>0.653 Â± 0.049</cell></row><row><cell>Our, ğ‘‘ = 20</cell><cell>0.598 Â± 0.067</cell></row><row><cell>Our, ğ‘‘ = 100</cell><cell>0.578 Â± 0.094</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGEMENT</head><p>This research was supported in part through computational resources of HPC facilities at NRU <rs type="institution">HSE</rs>. We thank all the colleagues and students who helps with the discussion of our study, in particular, <rs type="person">Andrey Plyuschevskiy</rs> and <rs type="person">Sergey Klyahandler</rs>.</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.00653[cs.SI]</idno>
		<title level="m">Scalable Feature Learning for Networks</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">DynGAN: Generative Adversarial Networks for Dynamic Network Embedding</title>
		<author>
			<persName><forename type="first">A</forename><surname>Maheshwari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Graph Representation Learning Workshop at NeurIPS</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">EvolveGCN: Evolving Graph Convolutional Networks for Dynamic Graphs</title>
		<author>
			<persName><forename type="first">A</forename><surname>Pareja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI-20 Conference on AI</title>
		<meeting>AAAI-20 Conference on AI</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="5363" to="5370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">DySAT: Deep Neural Representation Learning on Dynamic Graphs via Self-Attention Networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Sankar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM IC on WSDM (WSDM &apos;20</title>
		<meeting>ACM IC on WSDM (WSDM &apos;20</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="519" to="527" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Learning Temporal Attention in Dynamic Graphs with Bilinear Interactions</title>
		<author>
			<persName><forename type="first">B</forename><surname>Knyazev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Augusta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.10367</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>stat.ML</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deepwalk: Online learning of social representations</title>
		<author>
			<persName><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIGKDD IC on KDD&apos;20</title>
		<meeting>ACM SIGKDD IC on KDD&apos;20</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="701" to="710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A global geometric framework for nonlinear dimensionality reduction</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>De Silva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Langford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">science</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="page" from="2319" to="2323" />
			<date type="published" when="2000">2000. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting</title>
		<author>
			<persName><forename type="first">B</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI-18. International Joint Conferences on Artificial Intelligence Organization</title>
		<meeting>IJCAI-18. International Joint Conferences on Artificial Intelligence Organization</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3634" to="3640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Gated Residual Recurrent Graph Neural Networks for Traffic Prediction</title>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of AAAI-19 Conference on AI</title>
		<meeting>AAAI-19 Conference on AI</meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="485" to="492" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Inductive Representation Learning on Temporal Graphs</title>
		<author>
			<persName><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.07962[cs.LG]</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><surname>Barros</surname></persName>
		</author>
		<author>
			<persName><surname>Dt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>MendonÃ§a</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><forename type="middle">B</forename><surname>Rf</surname></persName>
		</author>
		<author>
			<persName><surname>Vieira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ziviani</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.01229[cs.LG]</idno>
		<title level="m">A Survey on Embedding Dynamic Graphs</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Temporal Graph Networks for Deep Learning on Dynamic Graphs</title>
		<author>
			<persName><forename type="first">E</forename><surname>Rossi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.10637[cs.LG]</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dynamic graph convolutional networks</title>
		<author>
			<persName><forename type="first">F</forename><surname>Manessi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rozza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Manzo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">97</biblScope>
			<biblScope unit="page">107000</biblScope>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">TemporalGAT: Attention-Based Dynamic Graph Representation Learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Fathy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Knowledge Discovery and Data Mining</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="413" to="423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<ptr target="http://dumps.wikimedia.org/" />
		<title level="m">Wikimedia Foundation. 2010. Wikimedia Downloads</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Highly Liquid Temporal Interaction Graph Embeddings</title>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Web Conference 2021</title>
		<meeting>the Web Conference 2021</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1639" to="1648" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Continuous-time dynamic network embeddings</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">H</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Companion Proceedings of the The Web Conference</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="969" to="976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Modeling dynamic heterogeneous network for link prediction using hierarchical attention with temporal rnn</title>
		<author>
			<persName><forename type="first">H</forename><surname>Xue</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2004.01024[cs.SI]</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Survey on graph embeddings and their applications to machine learning problems on graphs</title>
		<author>
			<persName><forename type="first">I</forename><surname>Makarov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PeerJ Computer Science</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">357</biblScope>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Anonymous walk embeddings</title>
		<author>
			<persName><forename type="first">Sergey</forename><surname>Ivanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evgeny</forename><surname>Burnaev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2186" to="2195" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">GC-LSTM: Graph Convolution Embedded LSTM for Dynamic Link Prediction</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1812.04206[cs.SI]</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">On the Equivalence Between Temporal and Static Graph Representations for Observational Predictions</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ribeiro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2103.07016[cs.LG]</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<idno type="arXiv">arXiv:0810.1355[cs.DS]</idno>
		<title level="m">Community Structure in Large Networks: Natural Cluster Sizes and the Absence of Large Well-Defined Clusters</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Predicting path failure in time-evolving graphs</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIGKDD IC on KDD&apos;19</title>
		<meeting>ACM SIGKDD IC on KDD&apos;19</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1279" to="1289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Graphrnn: Generating realistic graphs with deep autoregressive models</title>
		<author>
			<persName><forename type="first">J</forename><surname>You</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML&apos;18. PMLR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5708" to="5717" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">A3t-gcn: Attention temporal graph convolutional network for traffic forecasting</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.11583[cs.LG]</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Gcn-gan: A non-linear temporal link prediction model for weighted dynamic networks</title>
		<author>
			<persName><forename type="first">K</forename><surname>Lei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE INFOCOM&apos;19</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="388" to="396" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Representation Learning for Dynamic Graphs: A Survey</title>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1" to="73" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Lightgbm: A highly efficient gradient boosting decision tree</title>
		<author>
			<persName><forename type="first">Guolin</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qi</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Finley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weidong</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiwei</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="3146" to="3154" />
			<date type="published" when="2017">2017. 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title/>
		<author>
			<persName><surname>Joseph</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1978">1978</date>
			<publisher>Multidimensional scaling. Sage</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Dynamic Network Embedding: An Extended Approach for Skip-gram based Network Embedding</title>
		<author>
			<persName><forename type="first">L</forename><surname>Du</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI-18</title>
		<meeting>IJCAI-18</meeting>
		<imprint>
			<date type="published" when="2018">2018. 2086-2092</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Inductive representation learning on large graphs</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IC on NIPS&apos;17</title>
		<meeting>IC on NIPS&apos;17</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1025" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Continuous-Time Link Prediction via Temporal Dependent Graph Neural Network</title>
		<author>
			<persName><forename type="first">L</forename><surname>Qu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Web Conference</title>
		<meeting>The Web Conference<address><addrLine>Taipei, Taiwan; NY</addrLine></address></meeting>
		<imprint>
			<publisher>WWW &apos;20). Association for Computing Machinery</publisher>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="3026" to="3032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">T-gcn: A temporal graph convolutional network for traffic prediction</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE ITSS</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="3848" to="3858" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Semi-supervised learning on graphs with generative adversarial nets</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM IC on CIKM&apos;18</title>
		<meeting>ACM IC on CIKM&apos;18</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="913" to="922" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">TemporalNode2vec: Temporal Node Embedding in Temporal Networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Haddad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Complex Networks and Their Applications VIII</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="891" to="902" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Basic notions for the analysis of large two-mode networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Latapy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Magnien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Vecchio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Social Networks</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="31" to="48" />
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Kak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pca versus lda</title>
		<imprint>
			<date type="published" when="2001">2001. 2001</date>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="228" to="233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Semi-Supervised Classification with Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907[cs.LG]</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Using data mining to predict secondary school student performance</title>
		<author>
			<persName><forename type="first">P</forename><surname>Cortez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">M G</forename><surname>Silva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EUROSIS</title>
		<imprint>
			<date type="published" when="2008">2008. 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">DynGEM: Deep Embedding Method for Dynamic Graphs</title>
		<author>
			<persName><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.11273[cs.SI]</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>VeliÄkoviÄ‡</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>stat.ML</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><surname>Goel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.03143[cs.LG]</idno>
		<title level="m">Diachronic Embedding for Temporal Knowledge Graph Completion</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Semi-supervised graph embedding approach to dynamic link prediction</title>
		<author>
			<persName><forename type="first">R</forename><surname>Hisano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Complex Networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="109" to="121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">DyRep: Learning Representations over Dynamic Graphs</title>
		<author>
			<persName><forename type="first">R</forename><surname>Trivedi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR (Poster). OpenReview.net</title>
		<meeting><address><addrLine>LA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Issues in the study of graph embeddings</title>
		<author>
			<persName><forename type="first">Arnold</forename><forename type="middle">L</forename><surname>Rosenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Graphtheoretic Concepts in Computer Science, Hartmut Noltemeier</title>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1981">1981</date>
			<biblScope unit="page" from="150" to="176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Temporal graph offset reconstruction: Towards temporally robust graph representation learning</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bonner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Big Data</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="3737" to="3746" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Temporal neighbourhood aggregation: Predicting future links in temporal graphs via recurrent variational graph convolutions</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bonner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE International Conference on Big Data (Big Data)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5336" to="5345" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Indexing by latent semantic analysis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Deerwester</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ASIS&amp;T</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="391" to="407" />
			<date type="published" when="1990">1990. 1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Temporal network representation learning via historical neighborhoods aggregation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE ICDE&apos;20</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1117" to="1128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Predicting Dynamic Embedding Trajectory in Temporal Interaction Networks</title>
		<author>
			<persName><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m" type="main">Dynamic joint variational graph autoencoders</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mahdavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Khoshraftar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">An</forename><forename type="middle">A</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1910.01963[cs.LG]</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781[cs.CL]</idno>
		<title level="m">Efficient estimation of word representations in vector space</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Nonlinear dimensionality reduction by locally linear embedding</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">science</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="page" from="2323" to="2326" />
			<date type="published" when="2000">2000. 2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Temporal-Relational Classifiers for Prediction in Evolving Domains</title>
		<author>
			<persName><forename type="first">U</forename><surname>Sharan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Neville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eighth IEEE ICDM&apos;08</title>
		<imprint>
			<date type="published" when="2008">2008. 2008</date>
			<biblScope unit="page" from="540" to="549" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">APAN: Asynchronous Propagation Attention Network for Real-time Temporal Graph Embedding</title>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.11545[cs.AI]</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.01926[cs.LG]</idno>
		<title level="m">Diffusion convolutional recurrent neural network: Datadriven traffic forecasting</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Temporal Network Embedding with Micro-and Macro-Dynamics</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM IC on CIKM&apos;19</title>
		<meeting>the ACM IC on CIKM&apos;19<address><addrLine>Beijing, China; NY</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="469" to="478" />
		</imprint>
	</monogr>
	<note>CIKM &apos;19)</note>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Streaming Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yin</forename><forename type="middle">D</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM IC on SIGIR&apos;20 (Virtual Event, China) (SIGIR &apos;20)</title>
		<meeting>ACM IC on SIGIR&apos;20 (Virtual Event, China) (SIGIR &apos;20)<address><addrLine>NY</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="719" to="728" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<monogr>
		<author>
			<persName><forename type="first">Y</forename><surname>Seo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.07659</idno>
		<title level="m">Structured Sequence Modeling with Graph Convolutional Recurrent Networks</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>stat.ML</note>
</biblStruct>

<biblStruct xml:id="b60">
	<monogr>
		<title level="m" type="main">Inductive Representation Learning in Temporal Networks via Causal Anonymous Walks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2101.05974[cs.LG]</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">TI-GCN: A Dynamic Network Embedding Method with Time Interval Information</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Big Data</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="838" to="847" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Graph transformer networks</title>
		<author>
			<persName><forename type="first">Seongjun</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minbyul</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raehyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyunwoo J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="11983" to="11993" />
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">DyGCN: Dynamic Graph Embedding with Graph Convolutional Network</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.02962[cs.LG]</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Motif-Preserving Dynamic Attributed Network Embedding</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Web Conference 2021 (WWW &apos;21)</title>
		<meeting>the Web Conference 2021 (WWW &apos;21)</meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1629" to="1638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Learning Temporal Interaction Graph Embedding via Coupled Memory Networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The Web Conference 2020 (WWW &apos;20)</title>
		<meeting>The Web Conference 2020 (WWW &apos;20)</meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3049" to="3055" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
