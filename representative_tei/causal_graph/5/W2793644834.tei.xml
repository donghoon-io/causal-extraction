<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A KERNEL EMBEDDING-BASED APPROACH FOR NONSTATIONARY CAUSAL MODEL INFERENCE</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Shoubo</forename><surname>Hu</surname></persName>
							<email>sbhu@cse.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="department">The Chinese</orgName>
								<orgName type="institution">University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhitang</forename><surname>Chen</surname></persName>
							<email>chenzhitang2@huawei.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Huawei Noah&apos;s Ark Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Laiwan</forename><surname>Chan</surname></persName>
							<email>lwchan@cse.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="department">The Chinese</orgName>
								<orgName type="institution">University of Hong Kong</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A KERNEL EMBEDDING-BASED APPROACH FOR NONSTATIONARY CAUSAL MODEL INFERENCE</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:16+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Although nonstationary data are more common in the real world, most existing causal discovery methods do not take nonstationarity into consideration. In this letter, we propose a kernel embeddingbased approach, ENCI, for nonstationary causal model inference where data are collected from multiple domains with varying distributions. In ENCI, we transform the complicated relation of a cause-effect pair into a linear model of variables of which observations correspond to the kernel embeddings of the cause-and-effect distributions in different domains. In this way, we are able to estimate the causal direction by exploiting the causal asymmetry of the transformed linear model. Furthermore, we extend ENCI to causal graph discovery for multiple variables by transforming the relations among them into a linear nongaussian acyclic model. We show that by exploiting the nonstationarity of distributions, both cause-effect pairs and two kinds of causal graphs are identifiable under mild conditions. Experiments on synthetic and real-world data are conducted to justify the efficacy of ENCI over major existing methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Causal inference has been given rise to extensive attention and applied in several areas including statistics, neuroscience and sociology in recent years. An efficient approach for causal discovery is to conduct randomized controlled experiments. These experiments, however, are usually very expensive and sometimes practically infeasible. Therefore, causal inference methods using passive observational data take center stage, and many of them have been proposed, especially in the past ten years.</p><p>Existing causal inference methods that use passive observational data can be roughly categorized into two classes according to their objectives. One class of methods aim at identifying the variable that is the cause of the other in a variable pair <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref>, which is often termed a cause-effect pair. Most of the methods in this class first model the relation between the cause and the effect using a functional model with certain assumptions. Then they derive a certain property which only holds in the causal direction and is violated in the anticausal direction to infer the true causal direction. This kind of widely used property is often termed cause-effect asymmetry. For example, the additive noise model (ANM) <ref type="bibr" target="#b0">[1]</ref> represents the effect as a function of the cause with an additive independent noise: Y = f (X) + E Y . The authors showed that there is no model of the form X = g(Y ) + E X that admits an ANM in the anticausal direction for most combinations (f, p(X), p(E Y )). Therefore, the inference of ANM is done by finding the direction that fits ANM better. Similar methods include postnonlinear model (PNL) <ref type="bibr" target="#b1">[2]</ref> and information geometric causal inference (IGCI) <ref type="bibr" target="#b2">[3]</ref>. Recently, a kernel-based, EMD (or abbreviation for EMbeDding) <ref type="bibr" target="#b3">[4]</ref> using the framework of IGCI is proposed. EMD differs from the previous methods in the sense that it does not assume any specific functional model, but it still resorts to find the cause-effect asymmetry.</p><p>The other class of methods aims at recovering the structure of causal graphs. Constraint-based methods <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref>, which belong to this class, exploit the causal Markov condition and have been widely used in the social sciences, medical science, and bioinformatics. However, these methods allow one only to obtain the Markov equivalent class of the graph and are of high computational cost. In 2006, a linear nongaussian acyclic model (LiNGAM) <ref type="bibr" target="#b9">[10]</ref> which exploits the nongaussian property of the noise, was showed to be able to recover the full causal structure by using independent component analysis (ICA) <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>. To avoid the problem that ICA may result in a solution of local optima, different methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14]</ref> were proposed to guarantee the correctness of the causal order of variables in the causal graph.</p><p>Both classes of existing methods are based on the assumption that all observations are sampled from a fixed causal model. By "fixed causal model," we mean that the (joint) distribution of variables and the mechanism mapping cause(s) to effect(s) are unchanged during the data collecting process. For example in an ANM Y = f (X) + E Y , both the distribution of the cause p(X) and the causal mechanism f are assumed to be fixed. Although some of these methods do achieve inspiring results and provide valuable insights for subsequent research, data generated from a varying causal model are much more common in practice and existing methods based on a fixed causal model would come across some problems when applied to varying causal models <ref type="bibr" target="#b14">[15]</ref>. Therefore, we consider causal models where distributions of variables and causal mechanisms vary across domains or over different time periods and call these models non-stationary causal models. An example is the model of daily returns of different stocks. The distribution of the return of each stock varies with the financial status, and the causal mechanisms between different stocks also vary according to the relations between these companies. Recently, a method called Enhanced Constraint-based Procedure (ECBP) was proposed for causal inference of non-stationary causal models <ref type="bibr" target="#b14">[15]</ref>. The authors resorted to an index variable C to quantify the nonstationarity and proposed ECBP, which is built on constraint-based methods to recover the skeleton of the augmented graph, which consists of both observed variables V and some unobserved quantities determined by C. They also showed that it is possible to infer the parent nodes of variables adjacent to C (termed C-specific variables) and proposed a measure to infer the causal direction between each C-specific variable and its parents. However, their method fails to ensure the recovery of the full causal structure, which is due to the limitation of methods that rely on conditional independence test. In contrast, our method, which is proposed originally for cause-effect pairs inference, is also extended to infer the complete causal structure of two kinds of graphs by transforming the nonstationarity into a LiNGAM model.</p><p>In this paper, we introduce a nonstationary causal model and develop algorithms, which we call embedding-based nonstationary causal model inference (ENCI) for inferring the complete causal relations of the model. Our model assumes that the underlying causal relations (i.e. the causal direction of a cause-effect pair or the causal structure of a graph) are fixed, whereas the distributions of variables and the causal mechanisms (i.e. the conditional distribution of the effect given the cause(s)) change across domains or over different time periods. To infer the nonstationary causal model, ENCI reformulates the relation among variables into a linear model in the Reproducing Kernel Hilbert Space (RKHS) and leverages the identifiability of the linear causal model to tackle the original complicated problem. Specifically, for a cause-effect pair, we embed the variation of the density of each variable into an RKHS to transform the original unknown causal model to a linear nongaussian additive model <ref type="bibr" target="#b15">[16]</ref> based on the independence between the mechanism generating the cause and the mechanism mapping the cause to the effect. Then we infer the causal direction by exploiting the causal asymmetry of the obtained linear model. We also extend our approach to discover the complete causal structure of two kinds of causal graphs in which the distribution of each variable and the causal mechanism mapping cause(s) to effect(s) vary and the causal mechanism could be nonlinear. This paper is organized as follows. In section 2, we formally define our model and objective of causal inference. In section 3, some preliminary knowledge of reproducing kernel Hilbert space embedding is introduced. In section 4, we elaborate our methods for cause-effect pairs. In section 5, we extend our methods to two kinds of causal graphs. In section 6, we report experimental results on both synthetic and real-world data to show the advantage of our approach over existing ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Problem Description</head><p>In this section we formalize the nonstationary causal model and the objective of our causal inference task. For a pair of variable X and Y , we consider the case where X is the cause and Y is the effect without loss of generality throughout this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Non-stationary Causal Model</head><p>We assume the data generating process of a cause-effect pair fulfills the following properties:</p><p>• The causal direction between X and Y stays the same throughout the process.</p><p>• Observations are collected from N different domains. The density of the cause (p(X)) and the conditional density of the effect given the cause (p(Y |X)) are fixed within each domain.</p><p>• p(X) and p(Y |X) vary in different domains.</p><p>We call this a nonstationary causal model due to the variation in distributions over domains. The data-generating process is illustrated in Figure <ref type="figure" target="#fig_0">1</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Objective of Non-stationary Causal Model Inference</head><p>Our goal of nonstationary causal model inference is, by exploiting the variation of distributions in different groups, to accurately estimate the causal direction between X and Y . We also extend, our approach to learn the full causal structure of two kinds of causal graphs by transforming their relationship among groups into a LiNGAM model. For clarity, we list some of the notations we use in the following sections in Table <ref type="table" target="#tab_0">1</ref>. </p><formula xml:id="formula_0">p (i) (X), p (i) (Y ) Density of X, Y in group i p(X) Base of the density of X ∆p (i) (X)</formula><p>Variation of the density of X in group i p</p><formula xml:id="formula_1">(i) (Y |X) Conditional density of Y given X in group i p(Y |X) Base of the conditional density of Y given X ∆p (i) (Y |X) Variation of the conditional density of Y given X in group i X , Y domain of variable X, Y µ (i) ⊗X , µ (i) ⊗Y Mean embedding of p (i) (X) in X ⊗ X , p (i) (Y ) in Y ⊗ Y µ ⊗X , µ ⊗Y Mean embedding of p(X) in X ⊗ X , p(Y ) in Y ⊗ Y ∆µ (i) ⊗X , ∆µ (i) ⊗Y Mean embedding of ∆p (i) (X) in X ⊗ X , ∆p (i) (Y ) in Y ⊗ Y</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Hilbert Space Embedding of Distributions</head><p>Kernel embedding-based approaches represent probability distributions by elements in a reproducing kernel Hilbert space (RKHS) and it serves as the main tool in this letter to characterize distributions.</p><p>An RKHS F over X with a kernel k is a Hilbert space of functions f : X → R. Denoting its inner product by •, • F , RKHS F fulfills the reproducing property f (•), k(x, •) F = f (x). People often regard φ(x) := k(x, •) as a feature map of x. Kernel embedding of a marginal density p(X) <ref type="bibr" target="#b16">[17]</ref> is defined as the expectation of its feature map:</p><formula xml:id="formula_2">µ X := E X [φ(X)] = X φ(x)p(x)dx,<label>(1)</label></formula><p>where E X [φ(X)] is the expectation of φ(X). It has been shown that µ X is guaranteed to be an element in RKHS if E X [k(X, X)] &lt; ∞ is satisfied. It is also generalized to joint distribution using tensor product feature spaces. The kernel embedding of a joint density p(X, Y ) is defined as</p><formula xml:id="formula_3">C XY := E XY [φ(X) ⊗ φ(Y )] = X ×Y φ(x) ⊗ φ(y)p(x, y)dxdy.<label>(2)</label></formula><p>Similarly, we have that</p><formula xml:id="formula_4">C XX := E X [φ(X) ⊗ φ(X)].</formula><p>The embedding of conditional densities is viewed as an operator that maps from F to G which is an RKHS over Y <ref type="bibr" target="#b17">[18]</ref>. Imposing that the conditional embedding satisfies the following two properties:</p><formula xml:id="formula_5">µ Y |x := E Y |x [φ(Y )|x] = U Y |X k(x, •),<label>(3)</label></formula><formula xml:id="formula_6">E Y |x [g(Y )|x] = g, µ Y |x G ,<label>(4)</label></formula><p>where g ∈ G and µ Y |x is kernel embedding of marginal density p(Y |X = x), <ref type="bibr" target="#b17">[18]</ref> showed that conditional embedding can be defined as</p><formula xml:id="formula_7">U Y |X := C Y X C -1</formula><p>XX to fulfill equations 3 and 4. In the following sections, we follow the definition of kernel mean embedding and embed distributions in a tensor product space to represent distribution of each group.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Embedding-based Nonstationary Causal Model Inference</head><p>In this section we introduce our proposed approach to infer the causal structure of nonstationary causal models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Basic Idea</head><p>Currently, the most widely used idea of inferring causal direction is to quantify the independence between the mechanism generating the cause and the mechanism mapping the cause to the effect. One way to interpret the independence between these two mechanisms is to measure the independence between the cause and the noise. ANM and PNL lie in this field and LiNGAM methods could also be interpreted from this viewpoint <ref type="bibr" target="#b13">[14]</ref>. We adopt a different interpretation which uses the independence between the marginal distribution of the cause and the conditional distribution of the effect given the cause to capture the independence between these two mechanisms and further exploit causal asymmetry. This kind of independence has also been used in many existing causal inference methods <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref>. We formalize this independence in postulate 1: Postulate 1. The mechanism generating the cause and the mechanism mapping the cause to the effect are two independent natural processes. <ref type="bibr" target="#b2">[3]</ref> proposed this postulate and developed information geometry causal inference (IGCI). IGCI uses the density of the cause to characterize the first mechanism and the derivative of the function mapping the cause to the effect to characterize the second. In our approach, the variation of the marginal density of the cause is used to characterize the first mechanism, which is similar to IGCI. What differs from IGCI is that we use the variation of the conditional density to characterize the second mechanism. In subsequent sections, we introduce how we obtain the variation of densities and how we infer the causal direction based on the independence between them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Decomposition of Distributions</head><p>Given the entire data set G = {G 1 , G 2 , . . . , G N }, which consists of N groups, we make use of the variation of densities in each group. To obtain the variation, we first compute the mean of marginal densities and conditional densities of all groups as</p><formula xml:id="formula_8">p(X) = 1 N N i=1 p (i) (X), p(Y |X) = 1 N N i=1 p (i) (Y |X),<label>(5)</label></formula><p>where p (i) (X) is the density of X and p (i) (Y |X) is the conditional density of Y given X in group i. We call p(X) and p(Y |X) the base of marginal and conditional densities, respectively. Then the variation of density of each group is given by: Definition 1 (Variation of density). For any G i ∈ G, we decompose p (i) (X) and p (i) (Y |X) into two parts: one is the base of the (conditional) density and the other is a varying part, i.e. p (i) (X) = p(X) + ∆p (i) (X) and</p><formula xml:id="formula_9">p (i) (Y |X) = p(Y |X)+∆p (i) (Y |X).</formula><p>We call ∆p (i) (X) and ∆p (i) (Y |X) the variation of the marginal and conditional density of group i, respectively.</p><p>Since the base of densities is the mean of densities of all groups, ∆p (i) (X) and ∆p (i) (Y |X) fulfill the following properties.</p><p>1</p><formula xml:id="formula_10">N N i=1 ∆p (i) (X) ≡ 0, 1 N N i=1 ∆p (i) (Y |X) ≡ 0. (<label>6</label></formula><formula xml:id="formula_11">)</formula><p>Making use of the decomposition of distributions defined in definition 1, we are able to analyze densities of each group with some components fixed, which finally guides us to a fixed linear causal model. We take group i as an example to provides some insights before elaborating the derivations. The marginal density of effect Y is given by</p><formula xml:id="formula_12">p (i) (Y ) = p(x) + ∆p (i) (x) p(Y |x) + ∆p (i) (Y |x) dx,<label>(7)</label></formula><p>where p(X) and p(Y |X) are the same in all groups. Therefore, we would obtain a fixed term p(x)p(Y |x)dx which does not change over i in the expansion of equation 7. Although ∆p (i) (x)p(Y |x)dx and p(x)∆p (i) (Y |x)dx vary over groups, they also consist of p(X) and p(Y |X) which allows us to use the invariant to formulate the relation between them into a fixed causal model. In subsequent sections, we adopt kernel embedding to transform these kinds of invariant into a linear model to infer the causal direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Kernel Embedding of Distributions in Tensor Product Space</head><p>We resort to kernel embedding to represent distributions. The marginal distributions of X and Y of each group are embedded in tensor product space X ⊗ X and Y ⊗ Y, respectively. For simplicity, we use H to represent the tensor product space X ⊗ X and G to represent Y ⊗ Y in subsequent sections. Following the definition of kernel mean embedding, we define the mean embeddings of X and Y of group i in H and G as:</p><p>Definition 2 (tensor mean embedding).</p><formula xml:id="formula_13">µ (i) ⊗X := φ(x) ⊗ φ(x)p (i) (x)dx, µ<label>(i)</label></formula><formula xml:id="formula_14">⊗Y := φ(y) ⊗ φ(y)p (i) (y)dy. (<label>8</label></formula><formula xml:id="formula_15">)</formula><p>where φ(x) is the feature map of x and p (i) (x) is the density of x in group i. Similar notations go for y.</p><p>Definition 2 is the embedding of marginal densities of each group. Since our analysis is conducted on the base and variation of density of each group, we further define the tensor mean embedding of the base and variation of densities:</p><p>Definition 3 (tensor mean embedding of the base and variation of distributions).</p><formula xml:id="formula_16">µ ⊗X := φ(x) ⊗ φ(x)p(x)dx,<label>(9) ∆µ (i)</label></formula><formula xml:id="formula_17">⊗X := φ(x) ⊗ φ(x)∆p (i) (x)dx. (<label>10</label></formula><formula xml:id="formula_18">)</formula><p>µ ⊗X is the same in all groups and we have µ</p><formula xml:id="formula_19">(i) ⊗X = µ ⊗X + ∆µ (i)</formula><p>⊗X from definitions 2 and 3. Similarly, there is µ</p><formula xml:id="formula_20">(i) ⊗Y = µ ⊗Y + ∆µ (i)</formula><p>⊗Y . Definition 2 and 3 together state how marginal distributions are embedded in the tensor product space after decomposition. Next, we show how we make use of these tensor mean embeddings to infer the causal direction between X and Y . To avoid analyzing probability densities directly, we substitute equation 7 into definition 2 to conduct analysis on their embeddings:</p><formula xml:id="formula_21">µ (i) ⊗Y = φ(y) ⊗ φ(y) (p(y|x) + ∆p (i) (y|x))(p(x) + ∆p (i) (x))dx dy = φ(y) ⊗ φ(y) p(y|x) + ∆p (i) (y|x) dy p(x) + ∆p (i) (x) dx ≈ φ(y) ⊗ φ(y)p(y|x)dy p(x)dx + φ(y) ⊗ φ(y)∆p (i) (y|x)dy p(x)dx + φ(y) ⊗ φ(y)p(y|x)dy ∆p (i) (x)dx,<label>(11)</label></formula><p>where we omit the term φ(y) ⊗ φ(y)∆p (i) (y|x)dy ∆p (i) (x)dx. Since the ranges of variables are usually bounded and distributions usually change smoothly instead of drastically in real-world situations, we consider it reasonable to omit the one with two variation terms. Although there exits sets of densities in which the omitted term of certain group would have magnitude comparable to the sum of the remaining three terms when the distribution shifts drastically, we deem it less likely to occur in real situations. Note that this claim is close in spirit to an assumption in <ref type="bibr" target="#b14">[15]</ref> in which the authors assume the nonstationarity can be written as smooth functions of time or domain index. With this claim, we have the tensor mean embedding of the base of distributions as:</p><formula xml:id="formula_22">µ ⊗Y = φ(y) ⊗ φ(y)p(y)dy = φ(y) ⊗ φ(y)   1 N N j=1 p (j) (y)   dy = φ(y) ⊗ φ(y)   1 N N j=1 p(y|x) + ∆p (j) (y|x) p(x) + ∆p (j) (x) dx   dy ≈ φ(y) ⊗ φ(y)   1 N N j=1 p(y|x)p(x) + p(y|x)∆p (j) (x) + ∆p (j) (y|x)p(x) dx   dy = φ(y) ⊗ φ(y) p(y|x)p(x)dx dy,<label>(12)</label></formula><p>where the approximately equal mark is again derived by omitting the one with two variation terms and the last equality is directly derived from the property shown in equation 6. Then we have the tensor mean embedding of the variation of distributions as:</p><formula xml:id="formula_23">∆µ (i) ⊗Y = µ (i) ⊗Y -µ ⊗Y ≈ φ(y) ⊗ φ(y)∆p (i) (y|x)dy p(x)dx + φ(y) ⊗ φ(y)p(y|x)dy ∆p (i) (x)dx,<label>(13)</label></formula><p>which shows the relation between the tensor mean embedding of the variation of the effect and cause. φ(y) ⊗ φ(y)p(y|x)dy and φ(y) ⊗ φ(y)∆p (i) (y|x)dy are matrices of functions of X. In addition, they are both symmetric and positive definite so they admit decomposition:</p><formula xml:id="formula_24">φ(y) ⊗ φ(y)p(y|x)dy = V (X)V T (X) = N H j=1 v j (X)v T j (X),<label>(14)</label></formula><formula xml:id="formula_25">φ(y) ⊗ φ(y)∆p (i) (y|x)dy = ∆U (X)∆U T (X) = N H j=1 ∆u (i) j (X)∆u (i) T j (X),<label>(15)</label></formula><p>where V (X) and ∆U (X) are lower triangular matrices, v j (X) and ∆u (i) j (X) denote the j-th column of V (X) and ∆U (X), respectively; and N H denotes the dimension of V (X). The symbol ∆ indicates the corresponding relation of ∆U (X) to the variation of densities. By assuming that v j (X) and ∆u (i) j (X), j = 1, . . . , N H lie in the space of φ(X), we have v j (X) = A j φ(X) and ∆u (i) j (X) = ∆B (i) j φ(X). A j and ∆B (i) j are matrices containing coefficient mapping from φ(X) to v j (X) and ∆u (i) j (X), respectively. Then we have</p><formula xml:id="formula_26">φ(y) ⊗ φ(y)p(y|x)dy = N H j=1 A j φ(X) ⊗ φ(X)A T j ,<label>(16)</label></formula><formula xml:id="formula_27">φ(y) ⊗ φ(y)∆p (i) (y|x)dy = N H j=1 ∆B (i) j φ(X) ⊗ φ(X)∆B (i) j T .<label>(17)</label></formula><p>By substituting equation 16 and 17 into equation 13, we further obtain</p><formula xml:id="formula_28">∆µ (i) ⊗Y ≈ N H j=1 A j ∆µ (i) ⊗X A T j + N H j=1 ∆B (i) j µ ⊗X ∆B (i) j T ,<label>(18)</label></formula><p>where ∆µ</p><formula xml:id="formula_29">(i)</formula><p>⊗X and µ ⊗X are substituted in according to definition 2 and 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Inferring Causal Directions</head><p>In this section, we discuss how we infer the causal direction using the kernel embedding of decomposed densities. Note again that we consider the case X → Y without loss of generality throughout this letter.</p><p>We start by taking normalized trace τ on both sides of equation 18,</p><formula xml:id="formula_30">τ ∆µ (i) ⊗Y ≈ τ   N H j=1 A j ∆µ (i) ⊗X A T j   + τ   N H j=1 ∆B (i) j µ ⊗X ∆B (i) j T   = τ   N H j=1 A T j A j ∆µ (i) ⊗X   + τ   N H j=1 ∆B (i) j T ∆B (i) j µ ⊗X   = τ A∆µ (i) ⊗X + τ ∆B (i) µ ⊗X ,<label>(19)</label></formula><p>where</p><formula xml:id="formula_31">τ (A) = tr(A)/l A is called the normalized trace of A, l A is the size of A, A = N H j=1 A T j A j and ∆B (i) = N H j=1 ∆B (i) j T ∆B (i) j .</formula><p>Since the independence of the two mechanisms in Postulate 1 is difficult to quantify, we consider to use the density of the cause and the conditional density of the effect given the cause to represent the two mechanisms and adopt the independence between the base and variation of these two densities to infer the causal direction. The independence we rely on is based on the concept of free independence <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>. Definition 4 (Free independence). <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>. Let D be an algebra and ψ : D → R a linear functional on D with ψ(1) = 1. Then A and B are called free if</p><formula xml:id="formula_32">ψ (p 1 (A)q 1 (B)p 2 (A)q 2 (B) • • • ) = 0,<label>(20)</label></formula><p>for polynomials p i , q i , whenever</p><formula xml:id="formula_33">p i (A) = q i (B) = 0.</formula><p>It is straightforward from definition 4 that if A and B are free independent, it holds that ψ(AB) = ψ(A)ψ(B) <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>. Then we have the following two assumptions to characterize the independence in postulate 1:</p><p>Assumption 1. We assume that the tensor mean embedding of the variation of marginal density of the cause (∆µ</p><formula xml:id="formula_34">(i)</formula><p>⊗X , i = 1, . . . , N ) and A is free independent, and the tensor mean embedding of the base of marginal density of the cause (µ ⊗X ) and ∆B (i) , i = 1, . . . , N , is free independent, that is,</p><formula xml:id="formula_35">τ A∆µ (i) ⊗X = τ (A) τ ∆µ (i) ⊗X , i = 1, . . . , N,<label>(21)</label></formula><p>τ</p><formula xml:id="formula_36">∆B (i) µ ⊗X = τ ∆B (i) τ µ ⊗X , i = 1, . . . , N, (<label>22</label></formula><formula xml:id="formula_37">)</formula><p>where N is the number of groups.</p><p>Assumption 1 captures the independence between the mechanism generating the cause and the mechanism mapping the cause to the effect. In equation 21, A depends only on the base of the conditional densities p(Y |X) which corresponds to the second mechanism, and ∆µ</p><formula xml:id="formula_38">(i)</formula><p>⊗X depends only on the variation of the marginal densities of the cause ∆p (i) (X), which corresponds to the first mechanism. Therefore, the free independence between them characterizes the independence in postulate 1. Similarly, we have assumptions shown in equation 23. Assumption 2. Regarding the normalized trace of the tensor mean embedding of variation of marginal densities of the cause in each group as a realization of a random variable τ ∆µ ⊗X and each τ ∆B (i) as a realization of another random variable τ ∆B , we assume that these two random variables are independent, i.e.</p><formula xml:id="formula_39">τ ∆µ ⊗X ⊥ ⊥ τ ∆B . (<label>23</label></formula><formula xml:id="formula_40">)</formula><p>Assumption 2 is also motivated by the independence in postulate 1. Specifically, τ ∆µ ⊗X captures the information of the variation of marginal densities of the cause, and τ ∆B captures the information of the variation of conditional densities. We interpret postulate 1 as the independence between the marginal and conditional. Therefore, this independence between their variations of densities (approximately) holds. With assumption 1, equation 19 becomes</p><formula xml:id="formula_41">τ ∆µ (i) ⊗Y ≈ τ (A) τ ∆µ (i) ⊗X + τ ∆B (i) τ µ ⊗X .<label>(24)</label></formula><p>Since p(x) and p(y|x) are fixed given G, τ µ ⊗X and τ (A) are the same in all groups. We introduce the following notations for simplicity:</p><formula xml:id="formula_42">Notation 1. For any G i ∈ G, we use τ (i)</formula><p>x and τ (i) x→y denotes τ ∆B (i) µ ⊗X , which is the corresponding noise term. c y|x denotes τ (A). We view each τ</p><formula xml:id="formula_43">(i)</formula><p>x as a realization of a random variable τ x . Similarly, we have τ y and x→y . Proposition 1. If the causal direction is X → Y and assumptions 1 and 2 hold, the normalized trace of the tensor mean embeddings of the variation of the densities of the cause (τ x ) and the effect (τ y ) fulfill the following linear nongaussian additive model <ref type="bibr" target="#b15">[16]</ref>:</p><formula xml:id="formula_44">τ y ≈ c y|x τ x + x→y .<label>(25)</label></formula><p>Proof. By adopting notations in notation 1, equation 24 becomes</p><formula xml:id="formula_45">τ (i) y ≈ c y|x τ (i) x + (i) x→y , i = 1, . . . , N.<label>(26)</label></formula><p>We first show that x→y follows nongaussian distributions. According to assumption 1, we have</p><formula xml:id="formula_46">(i) x→y = τ ∆B (i) τ µ ⊗X ,<label>(27)</label></formula><p>where τ µ ⊗X is fixed and thus can be viewed as a constant. From the definition of ∆B (i) we have</p><formula xml:id="formula_47">τ ∆B (i) = 1 N H tr   N H j=1 ∆B (i) j T ∆B (i) j   = 1 N H N H j=1 tr ∆B (i) j T ∆B (i) j . (<label>28</label></formula><formula xml:id="formula_48">)</formula><p>Since tr ∆B</p><formula xml:id="formula_49">(i) j T ∆B (i) j</formula><p>are positive for all j, we have τ ∆B (i) &gt; 0. Therefore, the distribution of</p><formula xml:id="formula_50">(i)</formula><p>x→y is not symmetric and is thus not Gaussian distributed.</p><p>Second, we have τ x is independent of x→y according to the independence between τ ∆µ ⊗X and τ ∆B in assumption 2. Then we conclude equation 25 forms a linear non-Gaussian additive model.</p><p>According to the identifiability of LiNGAM <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b9">10]</ref>, τ y and y→x are dependent. By exploiting the cause-effect asymmetry that the cause is independent of the noise only in the causal direction, we propose the following causal inference approach: embedding-based nonstationary causal model inference (ENCI).</p><p>Causal Inference Approach (ENCI): Given data set G, we compute τ (i)</p><p>x and τ (i)</p><formula xml:id="formula_51">y for i = 1, . . . , N and conclude that X → Y if τ x ⊥ ⊥ x→y , otherwise Y → X if τ y ⊥ ⊥ y→x .</formula><p>Hilbert Schimidt Independence Criterion (HSIC) <ref type="bibr" target="#b21">[22]</ref> is applied to measure the independence between the regressor and its corresponding noise on both hypothetical directions, and we favor the direction with less dependence in practice. The ENCI algorithm is given in algorithm 1. The causal direction is x → y; 7: else if r x→y &gt; r y→x then 8:</p><p>The causal direction is y → x; 9: else 10:</p><p>No decision made. 11: end if</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Empirical Estimations</head><p>In this section, we show how to estimate τ (i)</p><p>x and τ (i)</p><formula xml:id="formula_52">y for i = 1, . . . , N based on the observations. Let Φ (i) = φ(x (i) 1 ), . . . , φ(x (i) Ni ) and Γ (i) = γ(x (i) 1 ), . . . , γ(x (i)</formula><p>Ni ) be the feature matrices of X and Y in group i, respectively, given observations in G. We estimate the mean embedding of p (i) (X) in X ⊗ X as</p><formula xml:id="formula_53">μ(i) ⊗X = 1 N i Φ (i) H Φ (i) H T ,<label>(29)</label></formula><p>where N i is the number of observations in ith group, H = I -1 Ni 11 T and 1 is a column vector of all 1s. Since we have</p><formula xml:id="formula_54">μ⊗X = φ(x) ⊗ φ(x) p(x)dx = φ(x) ⊗ φ(x)   1 N N j=1 p(j) (x)   dx = 1 N N j=1 φ(x) ⊗ φ(x)p (j) (x)dx = 1 N N j=1 μ(j) ⊗X ,<label>(30)</label></formula><p>for estimating the tensor mean embedding of the base of distributions µ ⊗X , the tensor mean embedding of the variation of distributions ∆µ</p><formula xml:id="formula_55">(i)</formula><p>⊗X is estimated as</p><formula xml:id="formula_56">∆μ (i) ⊗X = μ(i) ⊗X -μ⊗X = μ(i) ⊗X - 1 N N j=1 μ(j) ⊗X .<label>(31)</label></formula><p>By taking the normalized trace on both sides of equation 31, we have</p><formula xml:id="formula_57">τ (i) x = τ μ(i) ⊗X -τ 1 N N i=1 μ(i) ⊗X ≈ τ 1 N i Φ (i) H Φ (i) H T - 1 N N j=1 τ 1 N j Φ (j) H Φ (j) H T = 1 N 2 i tr K (i) x H - 1 N N j=1 1 N 2 j tr K (j) x H , (<label>32</label></formula><formula xml:id="formula_58">)</formula><p>where N is the total number of groups, N i is the number of observations in ith group and K (i)</p><p>x = Φ (i) T Φ (i) is the kernel matrix of X in ith group. Similarly, we have</p><formula xml:id="formula_59">τ (i) y == 1 N 2 i tr K (i) y H - 1 N N j=1 1 N 2 j tr K (j) y H ,<label>(33)</label></formula><p>where</p><formula xml:id="formula_60">K (i) y = Γ (i) T Γ (i)</formula><p>is the kernel matrix of X in ith group. We can see that both τ (i)</p><p>x and τ</p><formula xml:id="formula_61">(i)</formula><p>y can be easily calculated from Gram matrix using kernel methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Extending ENCI to Causal Graph Discovery</head><p>In this section, we extend ENCI to causal discovery for two kinds of directed acyclic graphs (DAGs). One is a tree-structured graph in which each node has at most one parent node. The other is multiple-independent-parent graph in which parent nodes of each node are mutually independent. Examples of these two kinds of DAGs are shown in Figure <ref type="figure">2</ref>.</p><formula xml:id="formula_62">x 1 x 2 x 3 x 5 x 8 x 7 x 4 x 9 x 10 x 6 (a) x 1 x 2 x 3 x 4 x 5 x 6 (b)</formula><p>Figure <ref type="figure">2</ref>: Examples of (a) Tree-structured graph (b) Multiple-independent-parent graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Describing Causal Relationship by Directed Acyclic Graphs</head><p>Consider a finite set of random variables X = (X 1 , . . . , X p ) with index set V := {1, . . . , p}. A graph G = (V, E) consists of nodes in V and edges (m, n) in E for any m, n ∈ V. Then we introduce graph terminologies required for subsequent sections. Most of the definitions are from <ref type="bibr" target="#b7">[8]</ref>.</p><p>Edge (m, n) is a directed link from node m to node n. Node m is called a parent of n, and n is called a child of m if (m, n) ∈ E. The parent set of n is denoted by pa(n) and its child set by ch(n). Nodes m, n are called adjacent if either (m, n) ∈ E or (n, m) ∈ E. A path in G is a sequence of distinct vertices m 1 , . . . , n q such that m k and n k+1 are adjacent for all k = 1, . . . , q -1. If (m k , m k+1 ) ∈ E for all k, the path is also called a directed path from m 1 to m q . G is called a partially directed acyclic graph (PDAG) if there is no directed cycle, i.e., there is no pair (m, n) such that there are directed paths from m to n and from n to m. G is called a directed acyclic graph (DAG) if it is a PDAG and all edges are directed.</p><p>General causal graph discovery is very challenging, especially when the relation between a variable pair is a complicated nonlinear stochastic process. In the following section, we show how we discover the causal structure tree-structured graphs (TSG) and multiple-independent-parents graph (MIPG). Note that the causal relation between a variable and its parent node in our model not only could be complicated nonlinear functions but also varies in different groups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Tree-Structured Causal Graph Discovery</head><p>In a TSG G with p nodes, each variable X m and its only parent node pa(X m ) fulfill the linear relation in Equation <ref type="formula" target="#formula_44">25</ref>.</p><p>Thus we have the following proposition for TSG:</p><p>Proposition 2. In a TSG G where each variable X m has only one parent node, the normalized traces of the tensor mean embedding of the variation of densities of all variables (τ xm , m = 1, . . . , p) fulfill a linear nongaussian acyclic model (LiNGAM) <ref type="bibr" target="#b9">[10]</ref> if assumption 1 and 2 hold:</p><formula xml:id="formula_63">τ x ≈ Cτ x + ,<label>(34)</label></formula><p>where τ x = τ x1 , . . . , τ xp T , coefficient matrix C whose element on n-th row and m-th column equals to c xn|xm could be permuted to a lower triangular matrix and = pa(x1)→x1 , . . . , pa(xp)→xp T collects all noise terms pa(xm)→xm , m = 1, . . . , p.</p><p>Proof. First, τ xm , where m = 1, . . . , p, could be arranged in a causal order in which no later variable is the cause of earlier ones due to the acyclicity of the graph. Note that causal order in subsequent sections also means that this condition holds for a sequence of variables. Second, the noise term pa(xm)→xm , where m = 1, . . . , p, follows nongaussian distributions as shown in proposition 1. Thirdly, assumption 2 ensures that τ xm ⊥ ⊥ pa(xm)→xm for m = 1, . . . , p.</p><p>Therefore, the graph formed by τ xm , where m = 1, . . . , p, fulfills the structure of LiNGAM <ref type="bibr" target="#b9">[10]</ref> so we can apply LiNGAM on τ x to infer the causal structure of the causal graph consists of X 1 , . . . , X p .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Multiple-Independent-Parent Graph Discovery</head><p>We extend ENCI to cases where each node could have more than one parent node provided that all its parent nodes are mutually independent.</p><p>Suppose a variable Y in graph G has q independent parent nodes -X 1 , . . . , X q . The marginal density of Y in group i can be obtained from</p><formula xml:id="formula_64">p (i) (Y ) = p (i) (Y |x 1 , . . . , x q )p (i) (x 1 , . . . , x q )dx 1 • • • dx q . (<label>35</label></formula><formula xml:id="formula_65">)</formula><p>Then by substituting</p><formula xml:id="formula_66">p (i) (Y ) into µ (i)</formula><p>⊗Y with p (i) (Y |X 1 , . . . , X q ) decomposed as p (i) (Y |X 1 , . . . , X q ) = p(Y |X 1 , . . . , X q ) + ∆p (i) (Y |X 1 , . . . , X q ) and integrating with respect to Y , we have</p><formula xml:id="formula_67">µ (i) ⊗Y = φ(y) ⊗ φ(y) p(y|x 1 , . . . , x q )p (i) (x 1 , . . . , x q )dx 1 • • • dx q + ∆p (i) (y|x 1 , . . . , x q )p (i) (x 1 , . . . , x q )dx 1 • • • dx q dy = φ(y) ⊗ φ(y)p(y|x 1 , . . . , x q )dy p (i) (x 1 , . . . , x q )dx 1 • • • dx q + φ(y) ⊗ φ(y)∆p (i) (y|x 1 , . . . , x q )dy p (i) (x 1 , . . . , x q )dx 1 • • • dx q . (<label>36</label></formula><formula xml:id="formula_68">)</formula><p>Following the same idea in the previous section, we conduct decomposition on both φ(y) ⊗ φ(y)p(y|X 1 , . . . , X q )dy and φ(y) ⊗ φ(y)∆p (i) (y|X 1 , . . . , X q )dy and thus obtain φ(y) ⊗ φ(y)p(y|X 1 , . . . , X q )dy =</p><formula xml:id="formula_69">N H j=1 v j (X 1 , . . . , X q )v T j (X 1 , . . . , X q ),<label>(37)</label></formula><formula xml:id="formula_70">φ(y) ⊗ φ(y)∆p (i) (y|X 1 , . . . , X q )dy = N H j=1 ∆u (i) j (X 1 , . . . , X q )∆u (i) T j (X 1 , . . . , X q ),<label>(38)</label></formula><p>where v j (X 1 , . . . , X q ) denotes the j-th column of φ(y) ⊗ φ(y)p(y|X 1 , . . . , X q )dy and ∆u (i) j (X 1 , . . . , X q ) denotes the j-th column of φ(y) ⊗ φ(y)∆p (i) (y|X 1 , . . . , X q )dy. By assuming that both v j (X 1 , . . . , X q ) and ∆u (i) j (X 1 , . . . , X q ), j = 1, . . . , N H lie in the space of feature map φ(X 1 , . . . , X q ), we have v j (X 1 , . . . , X q ) = A j φ(X 1 , . . . , X q ) and ∆u (i) j (X 1 , . . . , X q ) = ∆B (i) j φ(X 1 , . . . , X q ). Then they become φ(y) ⊗ φ(y)p(y|X1, . . . , Xq)dy = By plugging in equations 39 and 40, equation 36 becomes</p><formula xml:id="formula_71">µ (i) ⊗Y =   N H j=1 A j φ(x 1 , . . . , x q ) ⊗ φ(x 1 , . . . , x q )A T j   p (i) (x 1 , . . . , x q )dx 1 • • • dx q +   N H j=1 ∆B (i) j φ(x 1 , . . . , x q ) ⊗ φ(x 1 , . . . , x q )∆B (i) j T   p (i) (x 1 , . . . , x q )dx 1 • • • dx q = N H j=1 A j φ(x 1 , . . . , x q ) ⊗ φ(x 1 , . . . , x q )p (i) (x 1 , . . . , x q )dx 1 • • • dx q A T j + N H j=1 ∆B (i) j φ(x 1 , . . . , x q ) ⊗ φ(x 1 , . . . , x q )p (i) (x 1 , . . . , x q )dx 1 • • • dx q ∆B (i) j T . (<label>41</label></formula><formula xml:id="formula_72">)</formula><p>Observing that there exists a common term of integration in each term of the summation in equation 47, we now analyze this integral term in square brackets. Due to mutual independence among variables X k for k = 1, . . . , q, p (i) (X 1 , . . . , X q ) admits the following factorization:</p><formula xml:id="formula_73">p (i) (X 1 , . . . , X q ) = p (i) (X 1 ) • • • p (i) (X q ).</formula><p>(42) Then we adopt Bochner's theorem <ref type="bibr" target="#b22">[23]</ref> in analyzing φ(X 1 , . . . , X q ). Bochner's theorem states that a continuous shift-invariant kernel K(x, y) = k(x -y) is a positive-definite function if and only if k(t) is the Fourier transform of a nonnegative measure ρ(ω). Let α = dρ(ω), p ω = ρ/α, and ω 1 , ω 2 , . . . , ω k be independent samples from p ω . Then the random projection vector φ(X) can be</p><formula xml:id="formula_74">φ(X) = α √ k e -iω T 1 X , . . . , e -iω T k X .<label>(43)</label></formula><p>Similarly, we have</p><formula xml:id="formula_75">φ(X 1 , . . . , X n ) = α √ k e -i(ω T 11 X1+•••+ω T 1n Xn) , . . . , e -i(ω T k1 X1+•••+ω T kn Xn) ,<label>(44)</label></formula><p>which leads to φ(X 1 , . . . , X q ) = φ(X 1 )</p><formula xml:id="formula_76">• • • • • φ(X q ), (45) where φ(X j ) • φ(X k ) denotes the element-wise product. Since (φ(X 1 ) • • • • • φ(X q )) ⊗ (φ(X 1 ) • • • • • φ(X q )) = (φ(X 1 ) ⊗ φ(X 1 )) • • • • • (φ(X q ) ⊗ φ(X q )) ,<label>(46</label></formula><p>) the integration in equation 41 becomes</p><formula xml:id="formula_77">φ(x 1 , . . . , x q ) ⊗ φ(x 1 , . . . , x q )p (i) (x 1 , . . . , x q )dx 1 • • • dx q = (φ(x 1 ) ⊗ φ(x 1 )) • • • • • (φ(x q ) ⊗ φ(x q )) p (i) (x 1 ) • • • p (i) (x q ) dx 1 • • • dx q = φ(x 1 ) ⊗ φ(x 1 ) p(x 1 ) + ∆p (i) (x 1 ) dx 1 • . . . • • • • φ(x q ) ⊗ φ(x q ) p(x q ) + ∆p (i) (x q ) dx q = µ ⊗X1 + ∆µ (i) ⊗X1 • • • • • µ ⊗Xq + ∆µ (i) ⊗Xq . (<label>47</label></formula><formula xml:id="formula_78">)</formula><p>By substituting equation 47 into equation 41 we have</p><formula xml:id="formula_79">µ (i) ⊗Y = N H j=1 A j µ ⊗X1 + ∆µ (i) ⊗X1 • • • • • µ ⊗Xq + ∆µ (i) ⊗Xq A T j + N H j=1 ∆B (i) j µ ⊗X1 + ∆µ (i) ⊗X1 • • • • • µ ⊗Xq + ∆µ (i) ⊗Xq ∆B (i) j T ≈ N H j=1 A j µ ⊗X1 • • • • • µ ⊗Xq + ∆µ (i) ⊗X1 • • • • • µ ⊗Xq + • • • + µ ⊗X1 • • • • • ∆µ (i) ⊗Xq A T j + N H j=1 ∆B (i) j µ ⊗X1 • • • • • µ ⊗Xq ∆B (i) j T ,<label>(48)</label></formula><p>where we omit terms with more than one tensor mean embedding of variation of densities . Following the same idea in equation 13, we compute the variation of tensor embedding of Y by</p><formula xml:id="formula_80">∆µ (i) ⊗Y ≈ µ (i) ⊗Y -µ ⊗Y = N H j=1 A j ∆µ (i) ⊗X1 • • • • • µ ⊗Xq + • • • + µ ⊗X1 • • • • • ∆µ (i) ⊗Xq A T j + N H j=1 ∆B (i) j µ ⊗X1 • • • • • µ ⊗Xq ∆B (i) j T .<label>(49)</label></formula><p>Then by taking normalized trace on both sides of equation 49 we have τ ∆µ</p><formula xml:id="formula_81">(i) ⊗Y ≈ τ   N H j=1 A T j A j ∆µ (i) ⊗X1 • • • • • µ ⊗Xq + • • • + µ ⊗X1 • • • • • ∆µ (i) ⊗Xq   + τ   N H j=1 ∆B (i) j T ∆B (i) j µ ⊗X1 • • • • • µ ⊗Xq   = τ A ∆µ (i) ⊗X1 • • • • • µ ⊗Xq + • • • + µ ⊗X1 • • • • • ∆µ (i) ⊗Xq + τ ∆B (i) µ ⊗X1 • • • • • µ ⊗Xq = τ (A) τ ∆µ (i) ⊗X1 • • • • • µ ⊗Xq + • • • + τ (A) τ µ ⊗X1 • • • • • ∆µ (i) ⊗Xq + τ ∆B (i) τ µ ⊗X1 • • • • • µ ⊗Xq ,<label>(50)</label></formula><p>where</p><formula xml:id="formula_82">A = N H j=1 A T j A j and ∆B (i) = N H j=1 ∆B (i) j T ∆B (i) j .</formula><p>The last equality derives directly from assumption 2. Now we introduce another assumption for further analysis of MIPG. Lemma 1. Two high dimensional square matrices (e.g. A, B) whose elements are generated independently from two random variables fulfill the following property</p><formula xml:id="formula_83">τ (A • B) ≈ τ (A)τ (B).<label>(51)</label></formula><p>Proof. Firstly, the elements of A and B can be viewed as realizations of two underlying random variables; we denote them by X A and X B , respectively. The left hand side of Equation 51 becomes</p><formula xml:id="formula_84">τ (A • B) = 1 l A tr(A • B) = 1 l A l A j=1 A jj B jj ≈ E [X A X B ] ,<label>(52)</label></formula><p>where l A is the size of A and A jj denotes A's element on jth row and jth column. Similarly, we have B jj . Then the right hand side of equation 51 becomes</p><formula xml:id="formula_85">τ (A)τ (B) =   1 l A l A j=1 A jj     1 l B l B j=1 B jj   ≈ E [X A ] E [X B ] ,<label>(53)</label></formula><p>where l A is the size of A and l A = l B . Finally, by adopting the independence between X A and X B , we complete the proof.</p><p>Based on Lemma 1, we make following assumption for MIPG. Assumption 3. We assume that the elements of tensor mean embedding of the variation of density of each parent node (e.g. ∆µ</p><p>⊗X k k = 1, . . . , q) of certain variable and that of the base of densities of other parent nodes (e.g. µ ⊗X l , l = k) are generated independently.</p><p>A basic example implied by assumption 3 is τ (∆µ</p><formula xml:id="formula_87">(i) ⊗X k • µ ⊗X l ) = τ (∆µ (i) ⊗X k )τ (µ ⊗X l ). ∆µ (i)</formula><p>⊗X k depends only on ∆p (i) (X k ) and µ ⊗X l depends only on p(X l ). Based on the mutual independence among parent nodes of variables in MIPG, assumption 3 further states that the tensor mean embedding of the variation of the density of a parent node is independent of that of the base of the density of another parent node. This can be easily extended to cases with more than two terms provided that the independence holds. Under assumption 3, equation 50 becomes</p><formula xml:id="formula_88">τ ∆µ (i) ⊗Y ≈ τ (A) τ ∆µ (i) ⊗X1 τ µ ⊗X2 • • • • • µ ⊗Xq + . . . • • • + τ (A) τ µ ⊗X1 • • • • • µ ⊗Xq-1 τ ∆µ (i) ⊗Xq + τ ∆B (i) τ µ ⊗X1 • • • • • µ ⊗Xq .<label>(54)</label></formula><p>We introduce the following notations for simplicity:</p><p>Notation 2. We denote the kth parent node of Y by pa k (Y ), τ ∆µ</p><formula xml:id="formula_89">(i) ⊗Y by τ (i) y , τ ∆µ (i) ⊗X k by τ (i) pa k (y) , τ (A) τ µ ⊗X1 • . . . µ ⊗X k-1 • µ ⊗X k+1 • • • • • µ ⊗Xq by c y|pa k (y) and τ ∆B (i) τ µ ⊗X1 • • • • • µ ⊗Xq by (i)</formula><p>pa(y)→y . We view each τ (i) y as a realization of a random variable τ y . Similarly, there are variables τ pa k (y) , k = 1, . . . , q and pa(y)→y . Then equation 54 is formalized in the following proposition: Proposition 3. In an MIPG G of p nodes where each variable X m has q m independent parent nodes, if assumption 1 to 3 hold, the normalized traces of the tensor mean embedding of the variation of densities of all variables (τ xm , m = 1, . . . , p) fulfill a linear nongaussian acyclic model (LiNGAM) <ref type="bibr" target="#b9">[10]</ref>,</p><formula xml:id="formula_90">τ x ≈ Cτ x + ,<label>(55)</label></formula><p>where τ x = τ x1 , . . . , τ xp T , coefficient matrix C whose element on nth row and mth column equals to c xn|xm could be permuted to a lower triangular matrix and = pa(X1)→X1 , . . . , pa(Xp)→Xp T .</p><p>Proof. First, τ xm , where m = 1, . . . , p, could be arranged in an causal order due to the acyclicity of the graph. Second, the noise term pa(xm)→xm , where m = 1, . . . , p, follows nongaussian distributions as shown in proposition 1. Thirdly, assumption 2 ensures that τ xm ⊥ ⊥ pa(xm)→xm for m = 1, . . . , p.</p><p>According to proposition 3, we can apply LiNGAM on the normalized traces of the tensor mean embedding of the variation of densities of all variables to infer the causal structure. However, the coefficient matrix C returned by LiNGAM needs to be further adjusted since LiNGAM is not restricted to the two kinds of causal graphs we are considering in this letter. Obviously for TSGs, each row of C contains at most one non-zero element. For MIPGs, each column contains at most one non-zero element since it can be obtained by reversing all directed edges of TSGs. Therefore, we first determine whether the returned coefficient matrix is more likely to be a TSG or MIPG by simply comparing the number of rows and columns with one non-zero element. Then we adjust those rows (columns) that violate the corresponding graph structure. The algorithm of extending ENCI to discover the causal structure of a graph with multiple variables are given in algorithm 2.</p><p>Algorithm 2 ENCI for causal graphs Set elements in the rows with more than one non-zero element to be zero except for the maximal element and return the resulting matrix C EN CI . 7: else if n row &lt; n col then 8:</p><formula xml:id="formula_91">Input: N data groups G = {G 1 , G 2 , . . . ,</formula><p>Set elements in the columns with more than one non-zero element to be zero except for the maximal element and return the resulting matrix C EN CI . 9: else 10:</p><p>Return C as C EN CI . 11: end if <ref type="bibr" target="#b5">6</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Experiment</head><p>We conduct experiments on both synthetic and real data to verify the effectiveness of our proposed causal discovery algorithm. Unless specified, we adopt gaussian kernel with median (d M ) as its kernel width across all subsections. The implementations of ENCI for cause-effect pairs<ref type="foot" target="#foot_0">foot_0</ref> and causal graphs<ref type="foot" target="#foot_1">foot_1</ref> are available online.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Synthetic Pairs</head><p>We generate the cause X from the following family of distributions </p><formula xml:id="formula_92">f 1 (x) = 1 x 2 +1 f 2 (x) = sign(cx) × (cx) 2 f 3 (x) = cos(cxn) f 4 (x) = x 2 f 5 (x) = sin(cx) f 6 (x) = 2 sin(x) + 2 cos(x) f 7 (x) = 4 |x|</formula><p>where c is a random coefficient independently and uniformly sampled from interval [0.8, 1.2]. Overall, p(X) and function f are fixed within each group, whereas they vary in different groups.</p><p>We compare ENCI with ANM, PNL, IGCI and ECBP. These existing methods are applied in two different causal inference schemes: (1) on the entire dataset, which is obtained by combing all groups (ALL) and (2) on each group and choose their majority estimation to be their final causal direction estimation (MV). The experimental results of each setting are shown in Table <ref type="table" target="#tab_2">2</ref>. Note that the accuracies of ECBP are from 50 independent experiments due to its high time complexity and that of other methods are from 100 independent experiments. From the experimental results, we can see that ENCI, IGCI-MV, and ECBP-MV performs best compared with other cases. ANM and PNL could not make correct decision in both mechanisms at the same time, and the accuracy of IGCI-ALL is much lower than IGCI-MV, which is probably because of the influence of nonstationarity. ECBP takes non-stationarity into consideration so it achieves satisfactory accuracy in ECBP-ALL. However, we observe that its performance on multiplicative mechanism is not as good as ENCI in our experimental setting. In this section, we show our experimental results of both kinds of causal graphs.</p><p>In the case of tree-structured graph, we conduct experiments on randomly generated graphs with 10 and 50 variables, respectively. First, the distributions of the root node is determined in the same way as the cause X in the previous section. Then each effect is determined by a multiplicative mechanism from its parent node. The function f is randomly chosen from f 1 to f 7 , and all noise terms follow uniform distribution U(0, 1). Each time, 1000 groups of data are generated in total. Note that samples within each group are generated from a fixed causal model, but the distribution of the nodes and the mappings between them can vary in different groups.</p><p>We compare ENCI with seven existing methods. ECBP <ref type="bibr" target="#b14">[15]</ref>, ICA-LiNGAM <ref type="bibr" target="#b9">[10]</ref>, DirectLiNGAM <ref type="bibr" target="#b12">[13]</ref> and pair-wiseLiNGAM <ref type="bibr" target="#b13">[14]</ref> are directly applied after combining all groups of data. ANM <ref type="bibr" target="#b0">[1]</ref>, PNL <ref type="bibr" target="#b1">[2]</ref> and IGCI <ref type="bibr" target="#b2">[3]</ref> are applied on each pair of adjacent nodes so we only have the proportion of correctly estimated edges (recall) for these three methods. Figure <ref type="figure" target="#fig_5">3</ref> shows one of the estimated results of the methods which are able to recover the causal structure. In each experiment, we compute the recall (and precision) of edge from the estimation results. The mean precision (prc) and recall (rcl) are given in column TSG of Table <ref type="table" target="#tab_3">3</ref> <ref type="foot" target="#foot_2">foot_2</ref> . The results of ECBP on TSG with 10 and 50 variables are the mean of 50 and 20 independent experiments, respectively, due to its high time complexity. The results of other methods are the mean of 100 independent runs.</p><p>x 1</p><p>x 2 x 3 x 5 x 8</p><p>x 7 x 4 x 9</p><p>x 10</p><p>x 6</p><p>(a)</p><p>x 1</p><p>x 2</p><p>x 3 x 4 x 8</p><p>x 6 x 7</p><p>x 5 x 9</p><p>x 10  Next we conduct experiments on graphs that allow each variable to have multiple independent parent nodes. The experimental settings are similar to tree-structured case except that we generate 2000 data groups instead of 1000 and the ground truth of the synthetic network structure is fixed to be the graph on the right hand side of Figure <ref type="figure">2</ref>. The mean precision and recall are given in the MIPG column of Table <ref type="table" target="#tab_3">3</ref>. Note again that the results of ECBP are the mean of 20 independent experiments and that of other methods are the mean of 100 independent experiments.</p><p>The experimental results show a clear advantage of ENCI over ECBP, ICA-LiNGAM, pairwiseLiNGAM and Di-rectLiNGAM in estimating nonstationary causal graph. In both cases, ENCI achieves the highest precisions which are far higher than that of other methods. The recall of ENCI are also much higher compared with ICA-LiNGAM and DirectLiNGAM. Although in some cases ECBP and pairwiseLiNGAM return higher recall, their small precisions indicate that they find a large number of spurious edges, which makes their estimations less reliable. Comparing the recall of ENCI with ANM, PNL and IGCI, we find that ENCI still outperforms ANM and PNL. IGCI always performs the best among these four methods. Note that ANM, PNL and IGCI are not able to estimate the network structure and the recall of ENCI is relatively close to that of IGCI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Real Cause-effect Pairs</head><p>This section and the next present the experimental results on real cause-effect pairs and causal graph, respectively. Note that experiments of applying ENCI on both real cause-effect pairs and real causal graphs are conducted on subsampled groups. In other words, we sampled data groups from the raw single data set to create the non-stationarity artificially and then applied ENCI on those randomly sampled groups to evaluate the performance of ENCI on real data.</p><p>We test the performance of ENCI on real world benchmark cause-effect pairs<ref type="foot" target="#foot_3">foot_3</ref> . There are 106 pairs which come from 41 different data sets. Eight data sets are excluded in our experiment because they consists of either multivariate data or categorical data <ref type="foot" target="#foot_4">5</ref> . The corresponding pairs are of ID 47, 52, 53, 54, 55, 70, 71, 101 and 105. ENCI are compared with ANM <ref type="bibr" target="#b0">[1]</ref>, PNL <ref type="bibr" target="#b1">[2]</ref>, IGCI <ref type="bibr" target="#b2">[3]</ref> and ECBP <ref type="bibr" target="#b14">[15]</ref>. We repeat 100 independent experiments for each pair and compute the percentage of correct inference. Then we compute the average percentage of pairs from the same source as the accuracy of the corresponding data set. In the experiment of ENCI, we apply ENCI on 90 groups, each of which consists of 50 to 60 points randomly sampled from the raw data without replacement. Four methods are directly applied on 90 points randomly sample from raw data without replacement in each experiment. Note that ENCI and IGCI is applied using different configurations, and the best result of each pair is adopted for evaluation. For ENCI, we test kernel width</p><formula xml:id="formula_93">d ∈ {1/10d M , 1/5d M , 1/4d M , 1/3d M , 1/2d M , d M , 2d M , 3d M , 4d M , 5d M , 10d M }</formula><p>, where d M is the median distance. For IGCI, we test different reference measures (i.e. uniform and gaussian) and estimators (i.e. entropy and integral estimation).</p><p>The summary of accuracies on 33 data sets of each method is given in Figure <ref type="figure" target="#fig_6">4</ref> with orange solid line indicating median of accuracies and green dashed line indicating mean of accuracies. It shows that the performance of ENCI is satisfactory, with both median and mean accuracy about 79%. IGCI also performs quite well, especially in terms of median, followed by PNL. ANM and ECBP performs poorly on these real cause-effect pairs, which might be due to their model restrictions. ENCI is much more stable than IGCI although its median accuracy is slightly lower. The results on real cause-effect pairs also indicate that ENCI could achieve satisfactory accuracy when applied on subgroups sampled from original data which does not strictly follow our non-stationary model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Real Causal Graph</head><p>In this section, we test ENCI on a sociological data set from a data repository, General Social Survey<ref type="foot" target="#foot_5">foot_5</ref> .</p><p>This dataset consists of 6 observed variables, x 1 : father's occupation level, x 2 : son's income, x 3 : father's education, x 4 : son's occupation level, x 5 : son's education, x 6 : and number of siblings. We use the status attainment model based on domain knowledge <ref type="bibr" target="#b23">[24]</ref> as the ground truth (see Figure <ref type="figure">5</ref>) and compare ENCI with ICA-LiNGAM, DirectLiNGAM and ECBP.</p><p>Before applying ENCI, we first adopt k-means++ <ref type="bibr" target="#b24">[25]</ref> to cluster the original data into 15 clusters. In this way, we regard points within each cluster to be generated from the same causal model. Then we sample 1500 groups, which consists of 50 points sampled without replacement from each cluster with more than 50 points, and apply ENCI on these sampled groups. For ICA-LiNGAM, DirectLiNGAM and ECBP, we directly apply them on the original data set. We show one of the best results of ENCI (coefficient matrix C obtained from applying LiNGAM on τ xi , i = 1, . . . , 6) and the estimated graph from ICA-LiNGAM, DirectLiNGAM and ECBP in Figure <ref type="figure" target="#fig_8">6</ref>.</p><p>ENCI outperforms ICA-LiNGAM, DirectLiNGAM which is consistent with our expectation since they are developed for linear stationary models. ENCI also outperforms ECBP which may be due to the lack of nonstationarity of the raw data.</p><p>There are two facets of ENCI worth noting from the results of real data experiments of both pairs and causal graphs. First, ENCI is applied on subgroups sampled from the raw data since each set of real data is a single collection of observations and does not contain the form of nonstationarity our model assumes. However, the results of ENCI on real pairs is quiet competitive and it performs much better than LiNGAM family methods in real causal graph. This gives some evidence that our model could achieve satisfactory performance with subtle nonstationarity, which may be simply generated by subsampling a single data set. Second, the reference graph in the real graph experiment does not strictly fulfill the requirements of ENCI, but we obtain acceptable estimation results, which implies that ENCI may be applicable for other kinds of causal graphs.</p><p>x 1</p><p>x 6 x 3</p><p>x 4 x 5</p><p>x 2 (a)</p><p>x 1</p><p>x 6 x 3</p><p>x 4 x 5</p><p>x 2 (b)</p><p>x 1</p><p>x 6 x 3</p><p>x 4 x 5</p><p>x 2 (c) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>In this paper, we introduce the nonstationary causal model and prove the asymmetry of non-stationarity between the causal direction and anti-causal direction based on certain assumptions. By exploiting this asymmetry, we propose a reproducing kernel Hilbert space embedding-based method, ENCI, to infer the causal structure of both cause-effect pairs and two kinds of causal graphs. Theoretical analysis and experiments show the advantage of ENCI over existing methods based on fixed causal models when being applied on nonstationary passive observations.</p><p>Compared with ECBP which is also for non-stationary causal model inference, the theoretical scope of application of ENCI is more restricted in the sense that we require non-stationarity in both p(X) and p(Y |X), whereas ECBP would also work when only one of them is nonstationary. In addition, ENCI requires nonstationarity exists in every variable of a causal graph, whereas ECBP only requires the existence of nonstationarity. However, ENCI outperforms ECBP on the experiments of both real cause-effect pairs and causal graph in which the data generating process does not strictly follow our model assumptions and the nonstationarity among artificial groups is subtle. Therefore, we deem that ENCI could be applied on a much wider scope of problems in reality and achieve satisfactory performance. In this way, ENCI is eligible to join a pool of state-of-the-art algorithms for learning general causal models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Data generating process of non-stationary causal model</figDesc><graphic coords="3,118.80,109.52,374.40,243.58" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>y</head><label></label><figDesc>to represent τ ∆µ (i) ⊗X and τ ∆µ (i) ⊗Y , respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Algorithm 1 y</head><label>1</label><figDesc>ENCI for cause-effect pairs Input: N data groups G = {G 1 , G 2 , . . . , G N } Output: The causal direction 1: Normalize X and Y in each group; 2: Compute τ for i = 1, . . . , N ; 3: Compute residual x→y and y→x by conducting least square regressions; 4: Apply HSIC on τ x and x→y , denote the quotient of testStat and thresh returned by HSIC by r x→y . Similarly we have r y→x . 5: if r x→y &lt; r y→x then 6:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>, . . . , Xq) ⊗ φ(X1, . . . , Xq)A T j ,(39)φ(y) ⊗ φ(y)∆p (i) (y|X1, . . . , Xq)dy = X1, . . . , Xq) ⊗ φ(X1, . . . , Xq)∆B</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>where c 1 ,</head><label>1</label><figDesc>c 2 and c 3 are randomly sampled from a uniformly distributed simplex. When generating a group of data, c 1 to c 3 are firstly sampled to determine the distribution of X. Then 40 ∼ 50 data points are sampled from the corresponding distribution to form a group, and 200 groups are generated in each experiment. The generating mechanism of c 1 to c 3 leads to the independence and difference of distributions in different groups. We conduct experiments with both an additive mechanism, Y = f (X) + E, and a multiplicative mechanism, Y = f (X) × E. E is the standard Gaussian noise. The function mapping X to Y of each group is randomly chosen from f 1 to f 7 ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Examples of estimated results of (a) ENCI (b) ICA-LiNGAM (c) DirectLiNGAM (d) pairwiseLiNGAM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Accuracy of methods on real world cause-effect pairs.</figDesc><graphic coords="17,118.80,356.80,374.42,280.23" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>2 Figure 5 :</head><label>25</label><figDesc>Figure 5: Reference Graph of sociological dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Estimated graph of (a) ENCI (b) ICA-LiNGAM (c) DirectLiNGAM (d) ECBP.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>NotationsSymbol Description</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>G N } Output: The estimated coefficient matrix C EN CI of the causal graph 1: Normalize X m in each group for m = 1, . . . , p; 2: Compute τ xp for i = 1, . . . , N ; 3: Apply LiNGAM on τ x1 , . . . , τ xp and obtain the coefficient matrix C; 4: Denote the number of rows and columns with only one non-zero element by n row and n col , respectively; 5: if n row &gt; n col then</figDesc><table><row><cell>(i) x1 , . . . , τ</cell><cell>(i)</cell></row><row><cell>6:</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Accuracy of synthetic cause-effect pairs</figDesc><table><row><cell>Mechanism</cell><cell>ENCI</cell><cell cols="8">ANM MV ALL MV ALL MV ALL MV ALL PNL IGCI ECBP</cell></row><row><cell>Additive</cell><cell>100</cell><cell>100</cell><cell>63</cell><cell>99</cell><cell>50</cell><cell>100</cell><cell>66</cell><cell cols="2">100 100</cell></row><row><cell>Multiplicative</cell><cell>100</cell><cell>0</cell><cell>26</cell><cell>4</cell><cell>5</cell><cell>100</cell><cell>90</cell><cell>100</cell><cell>88</cell></row><row><cell>6.2 Synthetic Causal Graph</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Accuracy of synthetic cause-effect pairs</figDesc><table><row><cell></cell><cell></cell><cell cols="2">TSG</cell><cell></cell><cell cols="2">MIPG</cell></row><row><cell>Methods</cell><cell cols="2">10 vars</cell><cell cols="2">50 vars</cell><cell cols="2">6 vars</cell></row><row><cell></cell><cell>prc</cell><cell>rcl</cell><cell>prc</cell><cell>rcl</cell><cell>prc</cell><cell>rcl</cell></row><row><cell>ENCI</cell><cell cols="6">74.55 91.56 61.36 89.31 57.17 96.60</cell></row><row><cell>ECBP</cell><cell cols="6">47.23 39.18 47.69 41.12 35.92 98.00</cell></row><row><cell>ICA-LiNGAM</cell><cell>7.41</cell><cell>0.82</cell><cell>5.76</cell><cell cols="3">0.49 30.60 91.60</cell></row><row><cell cols="7">pairwiseLiNGAM 16.82 84.11 3.65 91.16 13.47 40.40</cell></row><row><cell>DirectLiNGAM</cell><cell cols="5">7.16 35.78 0.92 23.10 0.27</cell><cell>0.80</cell></row><row><cell>ANM</cell><cell>-</cell><cell>24.33</cell><cell>-</cell><cell>26.42</cell><cell>-</cell><cell>6.60</cell></row><row><cell>PNL</cell><cell>-</cell><cell>22.44</cell><cell>-</cell><cell>17.76</cell><cell>-</cell><cell>13.20</cell></row><row><cell>IGCI</cell><cell>-</cell><cell>99.33</cell><cell>-</cell><cell>92.43</cell><cell>-</cell><cell>97.33</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://github.com/amber0309/ENCI_cause-effect-pair</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>https://github.com/amber0309/ENCI_causal-graph</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>Note that the precision and recall of ECBP are computed from the skeleton instead of the directed graph.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>https://webdav.tuebingen.mpg.de/cause-effect/.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>Some of the existing methods or their implementations are not applicable to these data</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_5"><p>http://www.norc.org/GSS+Website/</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>We would like to thank <rs type="person">Biwei Huang</rs> and <rs type="person">Kun Zhang</rs> for providing the code of Enhanced Constraint-based Procedure (ECBP).</p></div>
			</div>			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Nonlinear causal discovery with additive noise models</title>
		<author>
			<persName><forename type="first">Patrik</forename><forename type="middle">O</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Joris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Mooij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="689" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On the identifiability of the post-nonlinear causal model</title>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aapo</forename><surname>Hyvärinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the twenty-fifth conference on uncertainty in artificial intelligence</title>
		<meeting>the twenty-fifth conference on uncertainty in artificial intelligence</meeting>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="647" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Information-geometric approach to inferring causal directions</title>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joris</forename><surname>Mooij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Lemeire</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Zscheischler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Povilas</forename><surname>Daniušis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bastian</forename><surname>Steudel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">182</biblScope>
			<biblScope unit="page" from="1" to="31" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Causal discovery via reproducing kernel hilbert space embeddings</title>
		<author>
			<persName><forename type="first">Zhitang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laiwan</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1484" to="1517" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An algorithm for fast recovery of sparse causal graphs</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clark</forename><surname>Glymour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Social science computer review</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="62" to="72" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">From probability to causality</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clark</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Scheines</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Philosophical Studies</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="36" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Causality: models, reasoning and inference</title>
		<author>
			<persName><forename type="first">Judea</forename><surname>Perl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>Cambridge University Press</publisher>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Causation, prediction, and search</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clark</forename><forename type="middle">N</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Scheines</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning bayesian networks from data: An information-theory based approach</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russell</forename><surname>Greiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiru</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">137</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="43" to="90" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A linear non-gaussian acyclic model for causal discovery</title>
		<author>
			<persName><forename type="first">Shohei</forename><surname>Shimizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrik</forename><forename type="middle">O</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aapo</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antti</forename><surname>Kerminen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2003" to="2030" />
			<date type="published" when="2006-10">Oct. 2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Independent component analysis, a new concept</title>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Comon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal processing</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="287" to="314" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Independent component analysis: algorithms and applications</title>
		<author>
			<persName><forename type="first">Aapo</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erkki</forename><surname>Oja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="411" to="430" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Directlingam: A direct method for learning a linear non-gaussian structural equation model</title>
		<author>
			<persName><forename type="first">Shohei</forename><surname>Shimizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takanori</forename><surname>Inazumi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasuhiro</forename><surname>Sogawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aapo</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshinobu</forename><surname>Kawahara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takashi</forename><surname>Washio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrik</forename><forename type="middle">O</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenneth</forename><surname>Bollen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1225" to="1248" />
			<date type="published" when="2011-04">Apr. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Pairwise likelihood ratios for estimation of non-gaussian structural equation models</title>
		<author>
			<persName><forename type="first">Aapo</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><forename type="middle">M</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="111" to="152" />
			<date type="published" when="2013-01">Jan. 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Discovery and visualization of nonstationary causal models</title>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Biwei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiji</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clark</forename><surname>Glymour</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.08056</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Causal inference using nonnormality</title>
		<author>
			<persName><forename type="first">Yutaka</forename><surname>Kano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shohei</forename><surname>Shimizu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the international symposium on science of modeling, the 30th anniversary of the information criterion</title>
		<meeting>the international symposium on science of modeling, the 30th anniversary of the information criterion</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="261" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A hilbert space embedding for distributions</title>
		<author>
			<persName><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Algorithmic Learning Theory</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="13" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Hilbert space embeddings of conditional distributions with applications to dynamical systems</title>
		<author>
			<persName><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenji</forename><surname>Fukumizu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual International Conference on Machine Learning</title>
		<meeting>the 26th Annual International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="961" to="968" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Telling cause from effect based on high-dimensional observations</title>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrik</forename><forename type="middle">O</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:0909.4386</idno>
		<imprint>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Free Random Variables</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">V</forename><surname>Voiculescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Dykema</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nica</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">CRM Monograph Series</title>
		<imprint>
			<date type="published" when="1992">1992</date>
			<publisher>American Mathematical Soc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Free probability theory</title>
		<author>
			<persName><forename type="first">Dan</forename><forename type="middle">V</forename><surname>Voiculescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">American Mathematical Soc</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A kernel statistical test of independence</title>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenji</forename><surname>Fukumizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Choon</forename><surname>Hui Teo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="585" to="592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Fourier analysis on groups</title>
		<author>
			<persName><forename type="first">Walter</forename><surname>Rudin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Socioeconomic background and achievement</title>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">D</forename><surname>Duncan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">L</forename><surname>Featherman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Duncan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Studies in population</title>
		<imprint>
			<publisher>Seminar Press</publisher>
			<date type="published" when="1972">1972</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">k-means++: The advantages of careful seeding</title>
		<author>
			<persName><forename type="first">David</forename><surname>Arthur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sergei</forename><surname>Vassilvitskii</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms</title>
		<meeting>the eighteenth annual ACM-SIAM symposium on Discrete algorithms</meeting>
		<imprint>
			<publisher>Society for Industrial and Applied Mathematics</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="1027" to="1035" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
