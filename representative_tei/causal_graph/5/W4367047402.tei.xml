<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Knowledge Graph Completion with Counterfactual Augmentation</title>
				<funder ref="#_s8cCXpC">
					<orgName type="full">NSFC</orgName>
				</funder>
				<funder ref="#_WeW5YuD">
					<orgName type="full">HKUST-GZU Joint Research Collaboration</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2023-02-25">25 Feb 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Heng</forename><surname>Chang</surname></persName>
							<email>changh17@tsinghua.org.cn</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Tsinghua University</orgName>
								<orgName type="institution" key="instit2">Tsinghua University</orgName>
								<orgName type="institution" key="instit3">University of Science and Technology (Guangzhou)</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jie</forename><surname>Cai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Tsinghua University</orgName>
								<orgName type="institution" key="instit2">Tsinghua University</orgName>
								<orgName type="institution" key="instit3">University of Science and Technology (Guangzhou)</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jia</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Tsinghua University</orgName>
								<orgName type="institution" key="instit2">Tsinghua University</orgName>
								<orgName type="institution" key="instit3">University of Science and Technology (Guangzhou)</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Hong</forename><surname>Kong</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Tsinghua University</orgName>
								<orgName type="institution" key="instit2">Tsinghua University</orgName>
								<orgName type="institution" key="instit3">University of Science and Technology (Guangzhou)</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Knowledge Graph Completion with Counterfactual Augmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-02-25">25 Feb 2023</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3543507.3583401</idno>
					<idno type="arXiv">arXiv:2302.13083v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>causal inference</term>
					<term>knowledge graph completion</term>
					<term>graph augmentation</term>
					<term>graph neural networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Graph Neural Networks (GNNs) have demonstrated great success in Knowledge Graph Completion (KGC) by modeling how entities and relations interact in recent years. However, most of them are designed to learn from the observed graph structure, which appears to have imbalanced relation distribution during the training stage. Motivated by the causal relationship among the entities on a knowledge graph, we explore this defect through a counterfactual question: "would the relation still exist if the neighborhood of entities became different from observation?". With a carefully designed instantiation of a causal model on the knowledge graph, we generate the counterfactual relations to answer the question by regarding the representations of entity pair given relation as context, structural information of relation-aware neighborhood as treatment, and validity of the composed triplet as the outcome. Furthermore, we incorporate the created counterfactual relations with the GNNbased framework on KGs to augment their learning of entity pair representations from both the observed and counterfactual relations. Experiments on benchmarks show that our proposed method outperforms existing methods on the task of KGC, achieving new state-of-the-art results. Moreover, we demonstrate that the proposed counterfactual relations-based augmentation also enhances the interpretability of the GNN-based framework through the path interpretations of predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>â€¢ Computing methodologies â†’ Reasoning about belief and knowledge.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Knowledge can be encoded in knowledge graphs (KGs), which store structured information of real-world entities as nodes and relations as edges. Real-world KGs are usually noisy and incomplete. Because of the nature of incompleteness of KGs, knowledge graph completion (KGC) is designed to predict missing relations <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b20">21]</ref>.</p><p>GNN-based KGC models have shown effects on the KGC task and attracted tremendous attention in recent years <ref type="bibr" target="#b22">[23]</ref>. Many popular GNNs follow an iterative message passing then aggregate scheme to adaptively learn the representations of nodes in graph <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b52">53]</ref>. When learning on a homogeneous graph, the representation of the center node is updated by aggregating the message gathered from the neighbors of the node while preserving the local structure around the node during each iteration, While a homogeneous graph could be viewed as a special version of KGs, GNNs are not able to be directly employed on KGs and are usually modified to model the interactions between entities and multiple relations. However, as shown in Figure <ref type="figure" target="#fig_1">2</ref>, the distribution of relation types is usually imbalanced on KGs. Most of the current GNN-based approaches fail to capture this information and could not generalize well. This failure calls for a better learning regime on KGC.</p><p>In this work, we propose to alleviate this failure with data augmentation on KGs, especially from the perspective of causal inference by answering the following counterfactual question:</p><p>Would the relation still exist if the neighborhood of entities became different from observation? If a GNN-based KGC model learns this causal relationship via answering the counterfactual question above, such knowledge will help to discover causal effects on relation types and improve the accuracy of prediction as well as the generalization ability <ref type="bibr" target="#b55">[56]</ref>. Note that the causality here falls into the scope of causal inference models, where we focus on taking advantage of existing basic yet effective causal models to enhance the task of KGC. This aligns with a line of research works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b55">56]</ref>, and is not related to the explicit semantic relations in KGs.</p><p>A counterfactual question is often associated with three factors: context (as a data point), treatment (as a type of manipulation), and outcome <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b55">56]</ref>. Previous effort <ref type="bibr" target="#b55">[56]</ref> proposes to use whether two nodes live in the same neighborhood to define such treatment on a homogeneous graph. However, we argue that this definition has flaws in KGs since it ignores the specific relation connected entities. Taking Figure <ref type="figure" target="#fig_0">1</ref> as an example. Let the blue and orange ovals be two detected communities by treating the KG as a homogeneous graph. Hedwig and Ron Weasley obviously share the same neighborhood within the ovals if we ignore the relation type on edges. Then the treatment definition from <ref type="bibr" target="#b55">[56]</ref> would be confused and make no distinction between Ron Weasley, who is a person, and Hedwig, which is an owl. As a result, it is even harder to help predict the different relations "pet of " and "friend" that are associated with them and Harry Potter. Therefore, it is non-trivial to directly migrate the treatment definition from a homogeneous graph to KGs due to the multi-relation nature.</p><p>Based on the idea of considering relation types with causal learning on KGs, we propose an instantiation of causal models on KGs with a new definition of treatment for context. By answering the counterfactual questions, we discover the counterfactual relations for all observed contexts and calculate their counterfactual treatments accordingly. Then, we integrate the representation learning of a pair of entities given a query relation with both their factual and counterfactual treatments and propose a counterfactual relation augmented GNN-based framework (KGCF) for KGC. Specifically, since this counterfactual treatment is not available from the observed data (valid triplets on KGs), we match it with the nearest neighbor of an entity pair in hidden embedding space and use the neighbor's factual treatment as a substitution. To further address the imbalanced distribution of relation types, we first embed the entities to an embedding space that is associated with each relation type, then find the nearest neighbors within this embedding space. After obtaining both the factual and counterfactual views of training data, we consociate them into an encoder-decoder style framework to learn better representations and predict missing relations.</p><p>The contributions of this paper are summarized as follows: â€¢ Motivated by the need for data augmentation on KGs, we propose the first instantiation of the causal model for KGs via answering counterfactual questions and considering the relation types.</p><p>â€¢ We present KGCF that utilizes counterfactual relations to augment the representation learning on KGs with special consideration on the imbalanced relation distribution.</p><p>â€¢ Extensive experiments show the superiority of KGCF on KGC task. We further demonstrate that counterfactual relations also enhance the interpretation ability through path-based explanation of predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK 2.1 Knowledge Graph Completion (KGC)</head><p>Popular research KGC models mainly fall into three categories <ref type="bibr" target="#b22">[23]</ref>: embedding-based methods, relation path inference, and rule-based reasoning. While the last two research directions focus on exploring the multi-step relationships, we focus on the recently preliminary direction of KGC in this paper: embedding-based KGC methods with GNNs and cover some of the recent progress in this line of research due to a large amount of literature. Jung et al. <ref type="bibr" target="#b25">[26]</ref> propose a new GNN encoder, which can effectively capture the query-related information from the temporal KG and is interpretable in the reasoning process. Zhang et al. <ref type="bibr" target="#b53">[54]</ref> verify by experiments that the graph structure modeling in GCNs has no significant impact on the performance of the KGC model. Instead, they propose a simple and effective LTE-KGE framework by removing the GCN aggregation. Wang et al. <ref type="bibr" target="#b45">[46]</ref> propose a new model called CFAG, which uses a coarse-grained aggregator (CG-AGG) and a fine-grained generative adversarial net (FG-GAN) to resolve the emergence of new entities. Huang et al. <ref type="bibr" target="#b21">[22]</ref> propose a KG inference framework SS-AGA based on self-supervised graph alignment. Despite the progress of the above-mentioned models, most of them fail to consider the imbalanced distribution of relation types.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Causal Inference</head><p>Finding causal relationships between different variables from observational data is a long-standing scientific problem in the field of statistics and artificial intelligence <ref type="bibr" target="#b50">[51]</ref>. Causal inference for GNNs is an emerging field of research, which can be divided into three categories: counterfactual learning, causal graph-guided representation learning, and causal discovery. Lin et al. <ref type="bibr" target="#b29">[30]</ref> use causal inference to improve the explainability of GNNs. They transform the explainability problem of reasoning decisions in the GNNs into a causal learning task, and then a causal explanation model is trained based on the objective function of Granger causality. Ma et al. <ref type="bibr" target="#b30">[31]</ref> propose a new framework called causal inference under spillover effects, which solves the problem of individual treatment effect estimation with high-order interference on hypergraphs. Zhao et al. <ref type="bibr" target="#b54">[55]</ref> first propose to use causal inference to improve link prediction on homogeneous graphs, which is the most related one to this paper. They propose the concept of counterfactual link prediction (CFLP) and representations are learned from observed and counterfactual links which are used as augmented training data. In contrast, we focus on KGs in this paper, to which CFLP could not be trivially migrated as discussed in Sec. 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Data Augmentation on Knowledge Graph</head><p>Graph Data Augmentation (GraphDA) is to find a mapping function that enriches or changes the information in a given graph <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b23">24]</ref>, which is a relatively underexplored area for KGs. Chen et al. <ref type="bibr" target="#b10">[11]</ref> propose a rumor data augmentation method called graph embedding-based rumor data augmentation (GERDA) to solve the data imbalance of rumor detection, which uses KGs to simulate the rumor generation process from the perspective of knowledge. Tang et al. <ref type="bibr" target="#b40">[41]</ref> propose positive-unlabeled learning with adversarial data augmentation (PUDA). PUDA alleviates the problem of false negatives through positive-unlabeled learning and positive sample sparsity through adversarial data augmentation. Different from them, we propose to use the causal model to generate counterfactual augmentations to facilitate representation learning on KGs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PRELIMINARIES 3.1 Knowledge Graphs (KGs)</head><p>A knowledge graph (KG) is denoted as a set of triplets:</p><formula xml:id="formula_0">G = {(ğ‘’ ğ‘– , ğ‘Ÿ ğ‘— , ğ‘’ ğ‘˜ )} âŠ‚ (E Ã— R Ã— E),</formula><p>where E and R represent a set of entities (nodes) and relations (edges), respectively. ğ‘’ ğ‘– and ğ‘Ÿ ğ‘— are the ğ‘–-th entity and ğ‘—-th relation, and the types of relations could be in great numbers within a KG. We also usually distinguish the entity pair (ğ‘’ ğ‘– , ğ‘’ ğ‘˜ ) with (â„ ğ‘– , ğ‘¡ ğ‘˜ ), considering the direction of the two entities. Note that a homogeneous graph G = (V, E) can be viewed as only one relation type for all edges versions of KGs. Throughout this paper, we use bold terms, ğ‘¾ or ğ’†, to denote matrix/vector representations for weights and entities, respectively. And we select italic terms, ğ‘¤ â„ or ğ›¼, to denote scalars. We can also use a third-order binary ten-</p><formula xml:id="formula_1">sor A âˆˆ {0, 1} | E |Ã— | R |Ã— | E |</formula><p>to uniquely define a KG, which is also known as the adjacency tensor of G. The (ğ‘–, ğ‘—, ğ‘˜) entry A ğ‘– ğ‘—ğ‘˜ = 1 if (â„ ğ‘– , ğ‘Ÿ ğ‘— , ğ‘¡ ğ‘˜ ) is valid or otherwise A ğ‘– ğ‘—ğ‘˜ = 0. The ğ‘—-th frontal slice of A is the adjacency matrix of the ğ‘—-th relation ğ‘Ÿ ğ‘— , which we denote as A ğ‘Ÿ ğ‘— for specification. To ease the notation, we also use A (â„,ğ‘Ÿ,ğ‘¡ ) to denote the entry of a given triplet (â„, ğ‘Ÿ, ğ‘¡) in the adjacency tensor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">GNN-Based framework for KGC</head><p>GNN-Based framework dealing with KGC usually adopts an encoderdecoder style framework <ref type="bibr" target="#b35">[36]</ref>, where GNNs perform as the encoder and embedding-based score functions (e.g., TransE, DistMult, Ro-tatE and ConvE) perform as the decoder. A KG's entities and relations are first represented as embeddings by the GNN encoder. After having the embeddings from GNN (encoder) for entities and relations, KGC models usually simulate how entities and relations interact with the help of a score function (decoder) ğ‘  :</p><formula xml:id="formula_2">E Ã— R Ã— E â†’ R.</formula><p>The decoder then predicts the entries in the adjacency tensors A utilizing the derived representations. The prediction could be viewed as a recovery of the original graph structures since adjacency tensors and graph structures are in bijection. In this way, the score function-based decoder can detect missing relations in the original KG while completing the KG by recovering the graph structure. It is worth mentioning that in some GNN-based frameworks for KGC, such as NBFNet, the aforementioned score functions are implicitly included in the procedure of message passing, and a simple feedforward neural network is employed as decoder on the learned entity pair representations to predict the missing links given the head/tail entity. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">PROPOSED METHOD 4.1 Incorporating GNN-based framework for KGC with Causal Model</head><p>Previous works on GNN-based framework for KGC <ref type="bibr" target="#b22">[23]</ref> have shown that message passing and aggregating scheme is able to generate more structure-enriched representations. However, recalling the example in Figure <ref type="figure" target="#fig_1">2</ref>, the distribution of relations appears to be quite imbalanced in KGs. This imbalanced distribution might lead the learning of GNNs to be trapped into the dominant relation types and fail to generalize to other relations during inference. Nevertheless, with the help of recent progress of data augmentation on graph learning <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b56">57]</ref>, we propose to enhance the generalization ability of current GNN-based learning frameworks on KGC from the perspective of a causal model. Counterfactual causal inference seeks to determine the causal relationship between treatment and outcome by posing counterfactual questions. An illustration of the counterfactual question is "would the outcome be different if the treatment was different?" <ref type="bibr" target="#b33">[34]</ref>. In order to address the counterfactual question, researchers would next develop a causal model and apply causal inference accordingly. For example, the left subfigure from Figure <ref type="figure" target="#fig_2">3</ref> is a typical triangle causal-directed acyclic graph (DAG) model. In this example, we denote the context (confounder) as ğ’ , the treatment as ğ‘» , and the outcome as ğ’€ . Once three of them are decided, the counterfactual inference methods could determine the effect of treatment ğ‘» on the outcome ğ’€ given context ğ’ . Two statistics, individual treatment effect (ITE) and its expectation averaged treatment effect (ATE) <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b55">56]</ref>, are usually employed to measure this effect. Given a binary treatment variable ğ‘‡ = {0, 1}, we denote ğ‘”(z,ğ‘‡ ) as the outcome of z, then</p><formula xml:id="formula_3">ITE(ğ’›) = ğ‘”(ğ’›, 1) -ğ‘”(ğ’›, 0), ATE = E ğ’›âˆ¼ğ’ ITE(ğ’›).</formula><p>Bigger ATE/LTE indicates a stronger causal relationship between treatment and outcome. Recent effort <ref type="bibr" target="#b55">[56]</ref> proposes to develop a causal model to enhance the representation learning on graphs. Through proper definitions of context, treatments, and their corresponding outcomes on graphs, promising performance on plain link prediction problems for homogeneous graphs is obtained.</p><p>On the contrary, in this work, we focus on a more comprehensive version of graphs, the KG, and focus on the KGC task. Given a pair of head entities and relation (â„, ğ‘Ÿ, ?), our model needs to predict the existence of a tail relation ğ‘¡. Hence, we aim to learn an effective representation ğ’› ğ‘Ÿ (â„, ğ‘¡) of head-tail pair (â„, ğ‘¡) given a query relation ğ‘Ÿ . As shown in the right subfigure from Figure <ref type="figure" target="#fig_2">3</ref>, we define the context as the entity pair representation ğ’› ğ‘Ÿ (â„, ğ‘¡), and the outcome in the adjacency tensor A (â„,ğ‘Ÿ,ğ‘¡ ) is the validity of triplet (â„, ğ‘Ÿ, ğ‘¡). Therefore, our causal DAG contains the following nodes: â€¢ ğ’› (Confounder; unobservable): latent entity pair representations; â€¢ T (Treatment; observable): KG structural information (e.g., whether two entities belong to the same neighborhood); â€¢ A (Outcome; observable): triplet existence. And the edges in our DAG are: ğ’› -&gt; T , ğ’› -&gt; A and T -&gt; A.</p><p>In this way, we instantiate the typical causal model on the KG by leveraging the estimated ITE(A (â„,ğ‘Ÿ,ğ‘¡ ) |T (â„,ğ‘Ÿ,ğ‘¡ ) ) to improve the learning of ğ’› ğ‘Ÿ (â„, ğ‘¡). Here, the objective is different from classic causal inference, as well as the definition of the plain link prediction in <ref type="bibr" target="#b55">[56]</ref>, considering the multiple types of relations. More specifically, for each pair of entities (â„, ğ‘¡) given a query relation ğ‘Ÿ , the corresponding ITE can be estimated by</p><formula xml:id="formula_4">ITE (â„,ğ‘¡ ) = ğ‘”(ğ’› ğ‘Ÿ (â„, ğ‘¡), 1) -ğ‘”(ğ’› ğ‘Ÿ (â„, ğ‘¡), 0).<label>(1)</label></formula><p>In the next, we will incorporate the information of ITE from the counterfactual relations to augment the KG and improve the learning of ğ’› ğ‘Ÿ (â„, ğ‘¡), accordingly. Both the entity pair representation ğ’› ğ‘Ÿ (â„, ğ‘¡) and the outcome A (â„,ğ‘Ÿ,ğ‘¡ ) need to consider the presence of multiple types of relations. Furthermore, the imbalanced distribution also makes the design of incorporation to the GNN-based embedding framework on KGC difficult.</p><p>As a preparation for the next step of incorporation, we denote the observed adjacency tensor by A ğ¹ as factual outcomes. In other words, A ğ¹ is constructed with all triplets from the training set. We also denote the unobserved tensor of the counterfactual relations by A ğ¶ğ¹ when the treatment is different from the counterfactual outcomes. For the definition of treatment, we denote</p><formula xml:id="formula_5">T ğ¹ âˆˆ {0, 1} | E |Ã— | R |Ã— | E |</formula><p>as the binary factual treatment tensor, where T ğ¹ (â„,ğ‘Ÿ,ğ‘¡ ) indicates the treatment of the entity pair (â„, ğ‘¡) given a query relation ğ‘Ÿ . Then we denote T ğ¶ğ¹ as the counterfactual treatment matrix where T ğ¶ğ¹ (â„,ğ‘Ÿ,ğ‘¡ ) = 1 -T ğ¹ (â„,ğ‘Ÿ,ğ‘¡ ) . We are interested in two aspects: 1) Estimating the counterfactual outcomes A ğ¶ğ¹ 2) learning from both factual (observed data) outcomes A ğ¹ and counterfactual (augmented data) outcomes A ğ¶ğ¹ to enhance the ability of GNN-based framework on KGC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Treatment Variable on KG</head><p>In this section, we describe how we design to acquire the factual treatment variable T ğ¹ . As in many GNN-based embedding frameworks for KGC, we assume the entities that share similar neighborhoods would tend to share similar relations as well. Therefore, finding the individual treatment for a pair of entities from their neighborhood under a special consideration to relation type is intuitively effective for downstream tasks. Meanwhile, as pointed out by recent works on homogeneous graphs <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b55">56]</ref>, models may fail to identify more global but also important factors purely from the association between local structural information and link presence. Therefore, in this work, we use both local and global structural roles of each entity pair as its treatment.</p><p>In KGs, since the relations are divided into multiple types, we need to define treatment based on the adjacency tensor A ğ‘Ÿ w.r.t the query relation ğ‘Ÿ , rather than simply dealing with a collapsed adjacency matrix as the regular practice in GNN-based approaches <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b44">45]</ref>. Note that though in this work, we define the treatment from the structural information w.r.t the view of each relation ğ‘Ÿ ğ‘— on KGs for illustration, the causal models shown in 3 do not limit the treatment to be structural roles on the graph. The treatment variable T ğ¹ (â„,ğ‘Ÿ,ğ‘¡ ) can be defined from any property of triplet (â„, ğ‘Ÿ, ğ‘¡). Then for each adjacency tensor in A ğ‘Ÿ ğ‘— , we define the binary treatment variable according to whether the two entities in a pair belong to the same community/neighborhood from a global perspective and calculate them separately. In this way, the relation types are explicitly considered in our treatment definition. We denote ğ‘ : V â†’ N as any graph clustering or community detection approach that assigns each entity with an index of the community/cluster/neighborhood that the entity belongs to. The treatment tensor T is defined as T ğ¹ (â„,ğ‘Ÿ,ğ‘¡ ) = 1 if ğ‘ (â„) = ğ‘ (ğ‘¡) on adjacency tensor A ğ‘Ÿ , and T ğ¹ (â„,ğ‘Ÿ,ğ‘¡ ) = 0 otherwise. For the choice of ğ‘, without the loss of generality, an unsupervised approach K-core <ref type="bibr" target="#b31">[32]</ref> that is widely used for clustering graph structure is utilized in our work as an example. K-core is strongly related to the concept of graph degeneracy, which has a long history in graph theory. Given a graph G and an integer ğ‘˜, a K-core of a graph G is a maximal connected subgraph of G in which all vertices have degrees at least ğ‘˜. Equivalently, it is the connected components that are left after all vertices of degree less than ğ‘˜ have been removed.</p><p>The reason that we choose the K-core algorithm falls into three aspects: 1) The core decomposition-based approach can be used to quantify node importance in many different domains efficiently and effectively. Precisely, the K-core algorithm for a graph G can be computed in linear time w.r.t the number of edges E of G, which is of great importance considering the massive entities in a KG. 2) By assigning to each graph node an integer number ğ‘˜ (the core number), K-core could capture how well the given node is connected w.r.t its neighbors, which could also reflect the first-order proximity on graph <ref type="bibr" target="#b39">[40]</ref>, which is a desired structural property for neighborhood. 3) As one of the generally used clustering methods that consider the global graph structural information, the role of each entity pair from a global perspective could also be reserved to complement the local structure information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Counterfactual Relations</head><p>Once we have defined the treatment from the observed space, we can discover the counterfactual treatment T ğ¶ğ¹ and outcome A ğ¶ğ¹ for each pair of entities given a query relation. However, this counterfactual relationship is not available from the observed data. As a substitution, we would like to find T ğ¶ğ¹ and A ğ¶ğ¹ from the nearest observed context as substitution <ref type="bibr" target="#b55">[56]</ref>. It is a common practice to estimate treatment effects from observational data using this form of matching on covariates <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b55">56]</ref>. In this way, for a pair of entities (â„, ğ‘¡) given a relation ğ‘Ÿ as a candidate, we can match their counterfactual treatment T ğ¶ğ¹ (â„,ğ‘Ÿ,ğ‘¡ ) to their nearest neighbor with the opposite treatment. Accordingly, the outcome of their nearest neighbor is treated as counterfactual relation.</p><p>Formally, âˆ€(â„ ğ‘– , ğ‘Ÿ ğ‘— , ğ‘¡ ğ‘˜ ) âˆˆ S, where S is a selected set of entity pair candidates, its counterfactual relation (â„ ğ‘ , ğ‘Ÿ ğ‘— , ğ‘¡ ğ‘ ) with the same type of relationship is (â„ ğ‘ , ğ‘Ÿ ğ‘— , ğ‘¡ ğ‘ ) = arg min</p><formula xml:id="formula_6">(â„ ğ‘ ,ğ‘Ÿ ğ‘— ,ğ‘¡ ğ‘ ) âˆˆ EÃ—E ğ‘‘ ((â„ ğ‘– , ğ‘Ÿ ğ‘— , ğ‘¡ ğ‘˜ ), (â„ ğ‘ , ğ‘Ÿ ğ‘— , ğ‘¡ ğ‘ )) T ğ¹ (â„ ğ‘ ,ğ‘Ÿ ğ‘— ,ğ‘¡ ğ‘ ) = 1 -T ğ¹ (â„ ğ‘– ,ğ‘Ÿ ğ‘— ,ğ‘¡ ğ‘˜ ) ,<label>(2)</label></formula><p>where ğ‘‘ (â€¢, â€¢) is a distance measuring metric between a pair of entity pairs (a pair of contexts) in the factual adjacency tensor A ğ¹ ğ‘Ÿ ğ‘— given a query relation ğ‘Ÿ ğ‘— . Here we omit the index in relation ğ‘Ÿ since the counterfactual relation should have the same relation type as the factual one. Nevertheless, there are still two challenges remaining for the above practice:</p><p>â€¢ It needs ğ‘‚ (ğ‘ 4 ) comparisons for finding the nearest neighbors if we choose the commonly used all ğ‘‚ (ğ‘ 2 ) entity pairs as S = E Ã— E, which is inefficient and infeasible in the application and hard to scale to large graphs. Considering the validity does not hold for all entity pairs, we design to set S as all entity pairs that have appeared in the dataset, but we eliminate the types of relations as well as the validity of the formed triplets, which prevents the risk of data leakage. Thus, the number of comparisons is greatly reduced by this selection of S. â€¢ Directly calculating the closest distance of entity pairs from A ğ¹ is still trapped in the observed space and could not bring extra counterfactual information. Therefore, we propose to find the nearest observed context of entity pairs from hidden low-dimensional embedding space. We take the unsupervised embedding method node2vec <ref type="bibr" target="#b17">[18]</ref> to learn the embedding space for entities since it could effectively preserve the proximity in the graph. This aligns with our second reason for choosing K-core for defining factual treatment in the observed space.</p><p>After the aforementioned two challenges are solved, we propose to use an entity embedding matrix, which is denoted as ğ‘´, to find the counterfactual substitutes of triplets from candidates. Especially, taking the imbalanced distribution into account, we design to learn ğ‘´ w.r.t the proportion ğœ“ ğ‘— of a given relation ğ‘Ÿ ğ‘— in the distribution of all relation types. That is for every A ğ¹ ğ‘Ÿ ğ‘— , we employ node2vec to get the embeddings accordingly as ğ‘´ ğ‘Ÿ ğ‘— , and the overall embedding matrix is defined as:</p><formula xml:id="formula_7">ğ‘´ âˆˆ R EÃ—ğ‘‘ = R âˆ‘ï¸ ğ‘Ÿ ğ‘— ğœ“ ğ‘— ğ‘´ ğ‘Ÿ ğ‘— .</formula><p>where ğ‘‘ is the dimension of embedding. Thus, âˆ€(â„ ğ‘– , ğ‘Ÿ ğ‘— , ğ‘¡ ğ‘˜ ) âˆˆ S, we define its substitution for counterfactual relation (â„ ğ‘ , ğ‘Ÿ ğ‘— , ğ‘¡ ğ‘ ) as (â„ ğ‘ , ğ‘Ÿ ğ‘— , ğ‘¡ ğ‘ ) = arg min</p><formula xml:id="formula_8">(â„ ğ‘ ,ğ‘Ÿ ğ‘— ,ğ‘¡ ğ‘ ) âˆˆ EÃ—E ğ‘‘ (ğ’ ğ‘– , ğ’ ğ‘ ) + ğ‘‘ (ğ’ ğ‘˜ , ğ’ ğ‘ ) T ğ¹ (â„ ğ‘ ,ğ‘Ÿ ğ‘— ,ğ‘¡ ğ‘ ) = 1 -T ğ¹ (â„ ğ‘– ,ğ‘Ÿ ğ‘— ,ğ‘¡ ğ‘˜ ) ,<label>(3)</label></formula><p>where we specify ğ‘‘ (â€¢, â€¢) as the Euclidean distance on the hidden embedding space of ğ‘´, and ğ’ ğ‘– indicates the embedding of entity â„ ğ‘– in the weighted node2vec space and so on. It is worth mentioning that in comparison with the approximated solution in <ref type="bibr" target="#b55">[56]</ref>, our introduced pair candidates could not only enhance the validity of potential triplets but also cast off the need for another hyperparameter for constraining the maximum distance.</p><p>To guarantee that all neighbors are sufficiently comparable (as substitutes) in the hidden embedding space, we do not assign counterfactual treatments for the given entity pair if there does not exist any entity pair satisfying Eq.( <ref type="formula" target="#formula_8">3</ref>), Thus, âˆ€(â„ ğ‘– , ğ‘Ÿ ğ‘— , ğ‘¡ ğ‘˜ ) âˆˆ S the counterfactual treatment matrix T ğ¶ğ¹ and the counterfactual adjacency tensor A ğ¶ğ¹ ğ‘Ÿ ğ‘— given a query relation ğ‘Ÿ ğ‘— are substituted as</p><formula xml:id="formula_9">T ğ¶ğ¹ (â„ ğ‘– ,ğ‘Ÿ ğ‘— ,ğ‘¡ ğ‘˜ ) , A ğ¶ğ¹ ğ‘–,ğ‘—,ğ‘˜ = ï£± ï£´ ï£´ ï£´ ï£´ ï£² ï£´ ï£´ ï£´ ï£´ ï£³ T ğ¹ (â„ ğ‘ ,ğ‘Ÿ ğ‘— ,ğ‘¡ ğ‘ ) , A ğ¹ ğ‘,ğ‘—,ğ‘ , if âˆƒ (â„ ğ‘ , ğ‘Ÿ ğ‘— , ğ‘¡ ğ‘ ) âˆˆ E Ã— E satisfies 3; T ğ¹ (â„ ğ‘– ,ğ‘Ÿ ğ‘— ,ğ‘¡ ğ‘˜ ) , A ğ¹ ğ‘–,ğ‘—,ğ‘˜</formula><p>, otherwise.</p><p>(4) Note that the embeddings ğ‘´ from hidden space is only used once for finding the substitutes for counterfactual treatment and outcome. Both ğ‘´ and the nearest neighbors are also computed only once and remain unchanged during the learning process.</p><p>After the instantiation of the causal model on KGC, we can design our model on how to learn the embeddings for the KG from the counterfactual distributions, which we denote as KGCF. We denote ğ‘ƒ ğ¹ as the factual distribution of the observed contexts and treatments, and ğ‘ƒ ğ¶ğ¹ be the counterfactual distribution that is composed of the observed contexts and opposite treatments. For all possible triplets (â„ ğ‘– , ğ‘Ÿ ğ‘— , ğ‘¡ ğ‘˜ ) and their treatments, the empirical factual distribution Pğ¹ âˆ¼ ğ‘ƒ ğ¹ is defined as Pğ¹ = {((â„ ğ‘– , ğ‘Ÿ ğ‘— , ğ‘¡ ğ‘˜ ), T ğ¹ (â„ ğ‘– ,ğ‘Ÿ ğ‘— ,ğ‘¡ ğ‘˜ ) )} E ğ‘–,ğ‘˜=1 , and define the empirical counterfactual distribution Pğ¶ğ¹ âˆ¼ ğ‘ƒ ğ¶ğ¹ as Pğ¶ğ¹ = {((â„ ğ‘– , ğ‘Ÿ ğ‘— , ğ‘¡ ğ‘˜ ), T ğ¶ğ¹ (â„ ğ‘– ,ğ‘Ÿ ğ‘— ,ğ‘¡ ğ‘˜ ) )} E ğ‘–,ğ‘˜=1 . While existing GNN-based KGC models only use the information from Pğ¹ as input, we benifit from the counterfactual distribution Pğ¶ğ¹ by augment the training data with the counterfactual relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Counterfactual Relations Augmented Learning Framework</head><p>After all the data preparation, as with most of the GNN-based embedding model, our model KGCF consists of two trainable components: a KG encoder ğ‘“ and a relation decoder ğ‘”.</p><p>Encoder. While we aim to learn a pairwise representation ğ’› ğ‘Ÿ (â„, ğ‘¡), the encoder ğ‘“ should capture the important neighborhood information between â„ and ğ‘¡ w.r.t the query relation ğ‘Ÿ , such as the proximity that is preserved by counting different types of random walks similar to the one we use during finding counterfactual relations. This kind of encoded local structure is often captured by path formulation, which is widely used in KG representation learning <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b57">58]</ref> Without the loss of generality, we adopt the most advanced variant NBFNet <ref type="bibr" target="#b57">[58]</ref> as the encoder ğ‘“ in KGCF.</p><p>NBFNet is a path formulation-based graph representation learning framework. In NBFNet, the pair representations of nodes are interpreted as a generalized sum and each path representation is defined as the generalized product of the edge representations in the path. Then NBFNet solves the path formulation efficiently with learned operators with the help of the generalized Bellman-Ford algorithm, then reads out the pair representation as a source-specific message-passing process. In NBFNet, each layer of GNN is defined as</p><formula xml:id="formula_10">ğ’› (0) ğ‘Ÿ (â„, ğ‘¡) â† IND(â„, ğ‘Ÿ, ğ‘¡), ğ’› (ğ‘™+1) ğ‘Ÿ (â„, ğ‘¡) â† AGG MSG ğ’› (ğ‘™) ğ‘Ÿ (â„, ğ‘¡), ğ’˜ (â„, ğ‘Ÿ, ğ‘’) (â„, ğ‘Ÿ, ğ‘’) âˆˆ N ğ‘–ğ‘› (ğ‘’) âˆª ğ’› (0) ğ‘Ÿ (â„, ğ‘¡)<label>(5)</label></formula><p>Table <ref type="table">1</ref>: KGC results. Results of NeuraLP and DRUM are taken from <ref type="bibr" target="#b34">[35]</ref>. Results of RotatE, HAKE, and LowFER are taken from their original papers <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b51">52]</ref>. Results of all GNNs-based models are reproduced with DGL <ref type="bibr" target="#b46">[47]</ref>. Results of the other embedding methods are taken from <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b57">58]</ref>.</p><p>Class Method FB15k-237 WN18RR MRR MR H@1 H@3 H@10 MRR MR H@1 H@3 H@10</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Path-based</head><p>Path Ranking <ref type="bibr" target="#b26">[27]</ref> 0 ğ‘Ÿ (â„, ğ‘¡) is the pair representation from ğ‘™ layer for entity pair (â„, ğ‘¡) given a query relation ğ‘Ÿ . The set N ğ‘–ğ‘› (ğ‘’) contains all triplets (â„, ğ‘Ÿ, ğ‘’) such that (â„, ğ‘Ÿ, ğ‘’) is valid for a preset entity ğ‘’. ğ’˜ (â„, ğ‘Ÿ, ğ‘’) is the edge representation for the triplet (â„, ğ‘Ÿ, ğ‘’). The IND is the indicator function outputs 1  â—‹ ğ‘Ÿ if â„ = ğ‘¡ and 0 â—‹ ğ‘Ÿ otherwise, which aims to provide a non-trivial representation for the heat entity â„ as the boundary condition. The AGG function is instantiated as natural sum, max or min in traditional methods followed by a linear transformation and a non-linear activation, or principal neighborhood aggregation (PNA) in recent efforts <ref type="bibr" target="#b11">[12]</ref>. For the MSG function, the NBFNet incorporates the traditional score functions (e.g., TransE, DistMult, or RotatE) as transformations correspond to the relational operators, in order to model the message passing procedure among entities and relations. Decoder. For the relation decoder that predicts whether a triplet is valid between a pair of entities with a query relation ğ‘Ÿ , we remain consistent with <ref type="bibr" target="#b55">[56]</ref> and adopt a simple decoder based on multilayer perceptron (MLP) to predict the conditional likelihood of the tail entity ğ‘¡. With the pair representation ğ’› ğ‘Ÿ (â„, ğ‘¡) from the output of the last layer in encoder ğ‘“ and the treatments of the triplet, the decoder ğ‘” is defined as</p><formula xml:id="formula_11">pğ¹ (ğ‘¡ |â„, ğ‘Ÿ ) = ğ‘”(Z, T ğ¹ ) = MLP([ğ’› ğ‘Ÿ (â„, ğ‘¡), T ğ¹ (â„,ğ‘Ÿ,ğ‘¡ ) ]),<label>(6)</label></formula><formula xml:id="formula_12">pğ¶ğ¹ (ğ‘¡ |â„, ğ‘Ÿ ) = ğ‘”(Z, T ğ¶ğ¹ ) = MLP([ğ’› ğ‘Ÿ (â„, ğ‘¡), T ğ¶ğ¹ (â„,ğ‘Ÿ,ğ‘¡ ) ]),<label>(7)</label></formula><p>where [â€¢, â€¢] stands for the concatenation of vectors. The outputs are the conditional likelihood of the tail entity ğ‘¡: pğ¹ (ğ‘¡ |â„, ğ‘Ÿ ) and pğ¶ğ¹ (ğ‘¡ |â„, ğ‘Ÿ ) for both factual and counterfactual empirical distribution. The conditional likelihood of the head entity â„ can be predicted by pğ¹ (â„|ğ‘¡, ğ‘Ÿ -1 ) and pğ¶ğ¹ (â„|ğ‘¡, ğ‘Ÿ -1 ) with the same model. Note that only the factual output pğ¹ (ğ‘¡ |â„, ğ‘Ÿ ) will be used in inference. Loss design. During the training process, data samples from the empirical factual/counterfactual distribution Pğ¹ / Pğ¶ğ¹ are fed into decoder ğ‘” and optimized towards ğ‘ ğ¹ (ğ‘¡ |â„, ğ‘Ÿ ) and ğ‘ ğ¶ğ¹ (ğ‘¡ |â„, ğ‘Ÿ ), respectively. We follow the previous works <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b37">38]</ref> to minimize the negative log-likelihood of positive and negative triplets from factual and counterfactual views. The negative samples are generated according to Partial Completeness Assumption (PCA) <ref type="bibr" target="#b15">[16]</ref> as in NBFNet, which creates a negative sample by corrupting one of the entities in a positive triplet. The corresponding losses are defined as:</p><formula xml:id="formula_13">L ğ¹ = -log pğ¹ (â„, ğ‘Ÿ, ğ‘¡) - ğ‘› âˆ‘ï¸ ğ‘–=1 1 ğ‘› log(1 -pğ¹ (â„ â€² ğ‘– , ğ‘Ÿ, ğ‘¡ â€² ğ‘– )),<label>(8)</label></formula><formula xml:id="formula_14">L ğ¶ğ¹ = -log pğ¶ğ¹ (â„, ğ‘Ÿ, ğ‘¡) - ğ‘› âˆ‘ï¸ ğ‘–=1 1 ğ‘› log(1 -pğ¶ğ¹ (â„ â€² ğ‘– , ğ‘Ÿ, ğ‘¡ â€² ğ‘– )). (<label>9</label></formula><formula xml:id="formula_15">)</formula><p>where ğ‘› is the number of negative samples for a single positive sample and (â„ â€² ğ‘– , ğ‘Ÿ, ğ‘¡ â€² ğ‘– ) is the ğ‘–-th negative sample. The negative samples remain the same for factual and counterfactual treatments.</p><p>Since the test data contains only observed (factual) samples, we force the distributions of representations of factual distributions and counterfactual distributions to be similar, which could protect the model from exposing to the risk of covariant shift <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b24">25]</ref>. Similar to <ref type="bibr" target="#b55">[56]</ref>, we minimize the discrepancy distance <ref type="bibr" target="#b32">[33]</ref> between Pğ¹ and Pğ¶ğ¹ to regularize the representation learning:</p><formula xml:id="formula_16">L ğ‘‘ğ‘–ğ‘ ğ‘ = disc( Pğ¹ ğ‘“ , Pğ¶ğ¹ ğ‘“ ), where disc(ğ‘ƒ, ğ‘„) = ||ğ‘ƒ -ğ‘„ || ğ¹ ,<label>(10)</label></formula><p>where || â€¢ || ğ¹ denotes the Frobenius Norm. Pğ¹ ğ‘“ and Pğ¶ğ¹ ğ‘“ are the representations of entity pairs given a query relation learned by graph encoder ğ‘“ from factual distribution and counterfactual distribution, respectively. Therefore, the overall training loss of our proposed KGCF is</p><formula xml:id="formula_17">L = L ğ¹ + ğ›¼ â€¢ L ğ¶ğ¹ + ğ›½ â€¢ L ğ‘‘ğ‘–ğ‘ ğ‘ ,<label>(11)</label></formula><p>where ğ›¼ and ğ›½ are hyperparameters to control the weights of counterfactual outcome estimation loss and discrepancy loss. Recall from Sec. 4.1 that our estimated ATE is formulated as:</p><formula xml:id="formula_18">ATE = 1 E Ã— E E âˆ‘ï¸ ğ‘–=1 E âˆ‘ï¸ ğ‘—=1 {T ğ¹ âŠ™ (A ğ¹ -A ğ¶ğ¹ )<label>(12)</label></formula><formula xml:id="formula_19">+ (1 EÃ—E -T ğ¹ ) âŠ™ (A ğ¶ğ¹ -A ğ¹ )} ğ‘–,ğ‘— .<label>(13)</label></formula><p>We can find from loss Eq.( <ref type="formula" target="#formula_17">11</ref>) that ATE is implicitly optimized during learning from the counterfactual relations. As a result, the learned causal relations help to improve the learning of pair representation.</p><p>Computational complexity. We first analyze the computational complexity of finding counterfactual relations. The complexity of the node2vec for hidden embedding space is bounded by O (ER). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS 5.1 Experimental Setup</head><p>We evaluate KGCF on the task KGC on the most popular datasets: FB15k-237 <ref type="bibr" target="#b41">[42]</ref> and WN18RR <ref type="bibr" target="#b12">[13]</ref>. Both datasets are publicly accessible from DGL 1 <ref type="bibr" target="#b46">[47]</ref>. We use the standard splits of both datasets. Statistics of datasets can be found in the Appendix. Baselines. We compare KGCF against 21 baselines including three types: path-based methods, embedding methods, and GNNs with different scoring functions. Note that LTE <ref type="bibr" target="#b53">[54]</ref> is not exactly a GNNbased model but we include it here since it shares the same design framework with other GNN-based models. Implementation Details.. Our implementation is similar with <ref type="bibr" target="#b57">[58]</ref>. For the encoder, we closely follow the official implementation for NBFNet and keep all architecture details as reported to maintain a fair comparison. In NBFNet, we set GNN layers as 6 and hidden 1 <ref type="url" target="https://github.com/awslabs/dgl-ke/tree/master/examples">https://github.com/awslabs/dgl-ke/tree/master/examples</ref> states as 32. The negative ratio (#negative/#positive) is set to 32. For other details please kindly refer to <ref type="bibr" target="#b57">[58]</ref>. For the decoder, we set MLP as two layers with 64 hidden units and use ReLU as the activation function. The integer ğ‘˜ in K-core is set to 2. The disc(â€¢) function is selected as KL divergence. We use a public implementation for node2vec and the output dimension for node2vec is set to 32. We decide the hyperparameters in loss through grid search within: ğ›¼ âˆˆ {0.001, 0.01, 0.1, 1} and ğ›½ âˆˆ {0.001, 0.01, 0.1, 1}. The batch size is set to 32. We use the Adam optimizer with a learning rate 5ğ‘’ -3 for optimization. Our model is trained on a single Tesla V100 GPU for 20 epochs and reports the results from the model selected based on the best performance on the validation set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">KGC Results</head><p>Table <ref type="table">1</ref> shows the performance of KGCF on the task of KGC in comparison with state-of-the-art baselines. We report the results regarding the filtered ranking protocol <ref type="bibr" target="#b4">[5]</ref>. We have the following observations: 1) Our KGCF achieves the best performance across all measurements on both datasets, which aligns with our motivation that balancing the distribution of relation types with augmentation could contribute to better pair representation learning. 2) The pathbased GNN approaches generally outperform other GNN baselines, which implies that the local subgraph structure that maintains the proximity between an entity pair is more essential to the massagepassing mechanism in the GNN-based KGC framework. This is consistent with the observation from LTE <ref type="bibr" target="#b53">[54]</ref> that the contribution of current convolutional GNN-based designs to KGC is somehow unnecessary and needs further exploration. 3) We further use a two-sided t-test to evaluate the significance of KGCF over NBFNet. Due to limited space here, we only show the result of the p-value on WN18RR here: We can find that KGCF achieves strongly significant improvements (P &lt; 0.005) over NBFNet across all metrics, which statistically confirms that KGCF's scores in Table <ref type="table">1</ref> are significantly better than NBFNet's.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Ablation Studies</head><p>Visualization of prediction. With the help of the interpretation tool from NBFNet, we could interpret the KGC prediction pğ¹ (â„, ğ‘Ÿ, ğ‘¡) of KGCF through paths from â„ to ğ‘¡ that contribute most to the results. The importance of a path interpretation for pğ¹ (â„, ğ‘Ÿ, ğ‘¡) is defined as its weight from a linear model, where the weight is approximated by the sum of the importance of edges in that path. In other words, the top-k path interpretations are equivalent to the top-k longest paths on the edge importance graph. The is computed by the partial derivative of the prediction w.r.t the path: P 1 , P 2 , ..., P ğ‘˜ = top-k</p><formula xml:id="formula_20">P âˆˆ P â„,ğ‘¡ ğœ• pğ¹ (â„, ğ‘Ÿ, ğ‘¡) ğœ• Pğ¹ ,<label>(14)</label></formula><p>where P 1 , P 2 , ..., P ğ‘˜ is the top-k paths acquired from ranking weights. Table <ref type="table" target="#tab_2">2</ref> shows the interpretation of two triplets examples with the path as well as their substitutions that are used for calculating counterfactual treatment SoCR. Note that SoCR does not have to be a valid triplet. Either way, its validity could augment the learning with a counterfactual view of information. This augmentation helps  Our observations from Table <ref type="table" target="#tab_2">2</ref> are: 1) The reasoning over two entities is conducted over the paths among them with both information from factual and counterfactual treatments. Our method provides two additional views of the learned factual and counterfactual relations, which helps enhance the confidence level of reasoning by answering the counterfactual question. 2) The nearest observed context that is used as a substitution for counterfactual estimation is effective on the KG. We can find that similar entities (e.g., the films Bridesmaids and The Hangover) are close to each other in the proximity-preserving embedding space and with analogous context. 3) The interpretation answers our question from Section 1 that the relationship could not exist when the neighborhood information varies. In the first example, Argentina did not bid for the 1976 summer Olympics thus the relation is not valid in counterfactual treatment. This augmented counterfactual view of information helps the learning avoid false prediction because analogical reasoning from only observed factual neighborhoods might infer it as a valid one otherwise. Running time comparison. In complementary of the complexity analysis from Section 4.4, we report the wall time on the FB15k-237 dataset for inference efficiency comparison over a single Tesla V100 GPU. It is obvious that the KGCF is very comparable to the NBFNet encoder, which indicates that our augmented framework brings no burden to the existing GNN-based method. Meanwhile, we can find that the convolutional GNN-based methods are the most efficient since they do not consider a path-based strategy, which results in worse KGC performance conversely. Ablation study on L ğ¶ğ¹ and L ğ‘‘ğ‘–ğ‘ ğ‘ . To investigate how L ğ¶ğ¹ and L ğ‘‘ğ‘–ğ‘ ğ‘ affect the overall KGC performance, we conduct sensitivity analysis on the performance of KGCF w.r.t different combinations of ğ›¼ and ğ›½. While we tune ğ›¼ and ğ›½ through grid search within the same set {0.001, 0.01, 0.1, 1}, we search through all combinations on WN18RR and report the corresponding H@10 performance in Figure <ref type="figure" target="#fig_4">4</ref>. We observe that the performance is relatively stable and Table <ref type="table">4</ref>: Inference time comparison on FB15k-237 test set with a single GPU. We report the average time over 5 splits for each method here. ğ‘  denotes seconds and ğ‘š denotes minutes.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">CONCLUSION</head><p>In this paper, we propose a novel counterfactual relation-based data augmentation framework KGCF for the task of KGC. Given a pair of entities and a query relation, we first explore the factual treatments for the observed context from the view of the proximity of entities. Considering the imbalance of the distribution of relation types, we define the counterfactual relations with the help of their nearest neighbors in the embedding space from relation type weighted node2vec. The counterfactual relations augment the training data with causal relationships and facilitate pair representation learning for KGC. Comprehensive experiments demonstrate that KGCF achieves the SOTA performance on benchmarks as well as endows the interpretability of the path-based GNN models on KGC from a new causal perspective.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example of why simply considering neighborhood information for causal learning from a homogeneous graph view would fail on KGs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure2: Relation type distribution from the train and test split on dataset FB237 and WN18RR, respectively. direction of KGC in this paper: embedding-based KGC methods with GNNs and cover some of the recent progress in this line of research due to a large amount of literature. Jung et al.<ref type="bibr" target="#b25">[26]</ref> propose a new GNN encoder, which can effectively capture the query-related information from the temporal KG and is interpretable in the reasoning process. Zhang et al.<ref type="bibr" target="#b53">[54]</ref> verify by experiments that the graph structure modeling in GCNs has no significant impact on the performance of the KGC model. Instead, they propose a simple and effective LTE-KGE framework by removing the GCN aggregation. Wang et al.<ref type="bibr" target="#b45">[46]</ref> propose a new model called CFAG, which uses a coarse-grained aggregator (CG-AGG) and a fine-grained generative adversarial net (FG-GAN) to resolve the emergence of new entities. Huang et al.<ref type="bibr" target="#b21">[22]</ref> propose a KG inference framework SS-AGA based on self-supervised graph alignment. Despite the progress of the above-mentioned models, most of them fail to consider the imbalanced distribution of relation types.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Our proposed method improves the prediction of missing relations by leveraging a causal model on the KG. Left: Illustration of a typical causal DAG model. Right: Our proposed KGC learning with the causal model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Sensitivity analysis on WN18RR w.r.t ğ›¼ and ğ›½. KGCF is generally robust to the hyperparameters ğ›¼ and ğ›½ for finding the optimal values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>The step of finding counterfactual links with nearest neighbors in Eq.(3) has complexity in proportion to the size of the candidate set |ğ‘† | and entity pairs. Since the computation in 3 can be parallelized, the time complexity is ğ‘‚ (|ğ‘† |E 2 /ğ¶) where ğ¶ is the number of processes. Then for the complexity of the training counterfactual learning model, the NBFNet encoder ğ‘“ has a time complexity of O (Eğ· ğ‘“ + Hğ· 2 ğ‘“ ), where H is the number of training triplets and ğ· ğ‘“ is the dimension of entity representations. While we sample the same number of non-existing triplets as that of observed triplets during training, the complexity of a two-layer MLP decoder ğ‘” is ğ‘‚ (((ğ· ğ‘“ + 1)â€¢ ğ· ğ‘” + 1 â€¢ ğ· ğ‘” )H ) = ğ‘‚ ((ğ· ğ‘“ + 2)ğ· ğ‘” H ), where ğ· ğ‘” is the number of neurons in the MLP hidden layer.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Examples of path interpretations of predictions on FB15k-237. For a valid triplet as a query, we visualize the Substitution of its Counterfactual Relation, which is denoted as SoCR, as well as the top-1 path interpretations and their weights. Inverse relations are denoted with a superscript -1 . Query âŸ¨â„ ğ‘– , ğ‘Ÿ ğ‘— , ğ‘¡ ğ‘˜ âŸ©: âŸ¨Barzil, olympics, 1972 summer olympicsâŸ©, (T ğ¹ â„ ğ‘– ,ğ‘Ÿ ğ‘— ,ğ‘¡ ğ‘˜ = 1, T ğ¶ğ¹ â„ ğ‘– ,ğ‘Ÿ ğ‘— ,ğ‘¡ ğ‘˜ SoCR âŸ¨â„ ğ‘ , ğ‘Ÿ ğ‘— , ğ‘¡ ğ‘ âŸ©: âŸ¨Argentinia, olympics, Bids for the 1976 summer olympicsâŸ© (T ğ¹ â„ ğ‘ ,ğ‘Ÿ ğ‘— ,ğ‘¡ ğ‘ = 0, T ğ¶ğ¹ â„ ğ‘ ,ğ‘Ÿ ğ‘— ,ğ‘¡ ğ‘ Query âŸ¨â„ ğ‘– , ğ‘Ÿ ğ‘— , ğ‘¡ ğ‘˜ âŸ©: âŸ¨Bridesmaids, film release region, Republica PortuguesaâŸ©, (T ğ¹ â„ ğ‘– ,ğ‘Ÿ ğ‘— ,ğ‘¡ ğ‘˜ SoCR âŸ¨â„ ğ‘ , ğ‘Ÿ ğ‘— , ğ‘¡ ğ‘ âŸ©: âŸ¨The Hangover, film release region, NorwegenâŸ© (T ğ¹ â„ ğ‘ ,ğ‘Ÿ ğ‘— ,ğ‘¡ ğ‘ = 1, T ğ¶ğ¹ â„ ğ‘ ,ğ‘Ÿ ğ‘— ,ğ‘¡ ğ‘</figDesc><table><row><cell>= 0)</cell></row><row><cell>= 0)</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Significance test of KGCF over NBFNet on WN18RR. -value P&lt;0.005 P&lt;0.001 P&lt;0.001 P&lt;0.005 avoid inaccurate predictions through directly analogical reasoning, which is the common practice of previous efforts, from only observed factual neighborhoods.</figDesc><table><row><cell>Metric</cell><cell>MRR</cell><cell>H@1</cell><cell>H@3</cell><cell>H@10</cell></row></table><note><p>p</p></note></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>This work was partially supported by <rs type="funder">NSFC</rs> Grant No. <rs type="grantNumber">62206067</rs> and <rs type="funder">HKUST-GZU Joint Research Collaboration</rs> Fund No.<rs type="grantNumber">GZU22EG05</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_s8cCXpC">
					<idno type="grant-number">62206067</idno>
				</org>
				<org type="funding" xml:id="_WeW5YuD">
					<idno type="grant-number">GZU22EG05</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>A OVERALL ALGORITHM OF KGCF.</p><p>Alg. 1 summarizes the overall algorithm of KGCF during the training and inference stages. Note that only the output ğ‘ ğ¹ (ğ‘¡ |â„, ğ‘Ÿ ) from the factual treatments will be used during the inference stage for the task of KGC. ğ’› ğ‘Ÿ (â„, ğ‘¡) = ğ‘“ ((â„, ğ‘Ÿ, ğ‘¡)) with Eq.( <ref type="formula">5</ref>). Get ğ‘ ğ¹ (ğ‘¡ |â„, ğ‘Ÿ ) and ğ‘ ğ¶ğ¹ (ğ‘¡ |â„, ğ‘Ÿ ) via ğ‘” with Eq.( <ref type="formula">6</ref>) and <ref type="bibr" target="#b6">(7)</ref>. Update parameters Î˜ ğ‘“ and Î˜ ğ‘” in ğ‘“ and ğ‘”, respectively, with L (11). end for // inference ğ’› ğ‘Ÿ (â„, ğ‘¡) = ğ‘“ ((â„, ğ‘Ÿ, ğ‘¡)) with Eq.( <ref type="formula">5</ref>). Get ğ‘ ğ¹ (ğ‘¡ |â„, ğ‘Ÿ ) and ğ‘ ğ¶ğ¹ (ğ‘¡ |â„, ğ‘Ÿ ) via ğ‘” with Eq.( <ref type="formula">6</ref>) and <ref type="bibr" target="#b6">(7)</ref>. Output: ğ‘ ğ¹ (ğ‘¡ |â„, ğ‘Ÿ ) for the task of KGC, ğ‘ ğ¶ğ¹ (ğ‘¡ |â„, ğ‘Ÿ ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B STATISTICS OF DATASETS</head><p>Dataset statistics of FB15k-237 and WN18RR for KGC are summarized in Table <ref type="table">5</ref>. We use the standard splits <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b41">42]</ref> for a fair comparison. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C EVALUATION METRICS</head><p>We follow the standard filtered ranking protocol <ref type="bibr" target="#b4">[5]</ref> and evaluate the performance of models by five metrics Mean Reciprocal Rank (MRR), Mean Rank (MR), and Hits@K (H@K). The detailed definitions of five metrics is introduced below: MRR. For a test triplet (â„, ğ‘Ÿ, ğ‘¡), we rank it against all negative triplets (â„ â€² , ğ‘Ÿ, ğ‘¡) or (â„, ğ‘Ÿ, ğ‘¡ â€² ) obtained by corrupting the head (or the tail) of the relation. Let rank â„ (â„, ğ‘Ÿ, ğ‘¡) be the ranking of (â„, ğ‘Ÿ, ğ‘¡) among all head-corrupted relations, and let rank ğ‘¡ (â„, ğ‘Ÿ, ğ‘¡) denote a similar ranking with tail corruptions. MRR is the mean of the reciprocal rank:</p><p>where T is the test set. MR. MR is the mean of both ranks:</p><p>rank â„ (â„, ğ‘Ÿ, ğ‘¡) + rank ğ‘¡ (â„, ğ‘Ÿ, ğ‘¡) , Hits@K. Hits@K measures the proportion of triples in T that rank among top-K after corrupting both heads and tails.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Validating causal inference models via influence functions</title>
		<author>
			<persName><forename type="first">Ahmed</forename><surname>Alaa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihaela</forename><surname>Van</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Der</forename><surname>Schaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="191" to="201" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">LowFER: Low-rank bilinear pooling for link prediction</title>
		<author>
			<persName><forename type="first">Saadullah</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stalin</forename><surname>Varanasi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katherine</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Ann</forename><surname>Dunfield</surname></persName>
		</author>
		<author>
			<persName><forename type="first">GÃ¼nter</forename><surname>Neumann</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="257" to="268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Counterfactual representation learning with balancing weights</title>
		<author>
			<persName><forename type="first">Serge</forename><surname>Assaad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuxi</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyang</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shounak</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikhil</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Henao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lawrence</forename><surname>Carin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duke</forename></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="1972">2021. 1972-1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">TuckER: Tensor Factorization for Knowledge Graph Completion</title>
		<author>
			<persName><forename type="first">Ivana</forename><surname>BalaÅ¾eviÄ‡</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>EMNLP-IJCNLP</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5185" to="5194" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multi-relational data</title>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>Garcia-Duran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Line graph neural networks for link prediction</title>
		<author>
			<persName><forename type="first">Lei</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jundong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Not All Low-Pass Filters are Robust in Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Heng</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yatao</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shiji</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<biblScope unit="page">34</biblScope>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Spectral graph attention network with fast eigen-approximation</title>
		<author>
			<persName><forename type="first">Heng</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Somayeh</forename><surname>Sojoudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Management (CIKM)</title>
		<meeting>the 30th ACM International Conference on Information &amp; Knowledge Management (CIKM)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2905" to="2909" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Adversarial Attack Framework on Graph Embedding Models with Limited Knowledge</title>
		<author>
			<persName><forename type="first">Heng</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honglei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering (TKDE)</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A restricted black-box adversarial framework towards attacking graph embedding models</title>
		<author>
			<persName><forename type="first">Heng</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingyang</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Honglei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on Artificial Intelligence (AAAI)</title>
		<meeting>the AAAI conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="3389" to="3396" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Rumor knowledge embedding based data augmentation for imbalanced rumor detection</title>
		<author>
			<persName><forename type="first">Xiangyan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Duoduo</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dazhen</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donglin</forename><surname>Cao</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.ins.2021.08.059</idno>
		<ptr target="https://doi.org/10.1016/j.ins.2021.08.059" />
	</analytic>
	<monogr>
		<title level="j">Information Sciences</title>
		<imprint>
			<biblScope unit="volume">580</biblScope>
			<biblScope unit="page" from="352" to="370" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Principal neighbourhood aggregation for graph nets</title>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Cavalleri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominique</forename><surname>Beaini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>LiÃ²</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Petar</forename><surname>VeliÄkoviÄ‡</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="13260" to="13271" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Convolutional 2d knowledge graph embeddings</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Dettmers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pasquale</forename><surname>Minervini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pontus</forename><surname>Stenetorp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Data augmentation for deep graph learning: A survey</title>
		<author>
			<persName><forename type="first">Kaize</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanghang</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huan</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.08235</idno>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fast rule mining in ontological knowledge bases with AMIE ++</title>
		<author>
			<persName><forename type="first">Luis</forename><surname>GalÃ¡rraga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christina</forename><surname>Teflioudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katja</forename><surname>Hose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabian</forename><forename type="middle">M</forename><surname>Suchanek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The VLDB Journal</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="707" to="730" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">AMIE: association rule mining under incomplete evidence in ontological knowledge bases</title>
		<author>
			<persName><forename type="first">Luis</forename><surname>Antonio GalÃ¡rraga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christina</forename><surname>Teflioudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katja</forename><surname>Hose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabian</forename><surname>Suchanek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd international conference on World Wide Web</title>
		<meeting>the 22nd international conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="413" to="422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Efficient and expressive knowledge base completion using subgraph feature extraction</title>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Mitchell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1488" to="1498" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">node2vec: Scalable feature learning for networks</title>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="855" to="864" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Implicit graph neural networks</title>
		<author>
			<persName><forename type="first">Fangda</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Somayeh</forename><surname>Sojoudi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><forename type="middle">El</forename><surname>Ghaoui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NeurIPS)</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="11984" to="11995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">AutoGL: A Library for Automated Graph Learning</title>
		<author>
			<persName><forename type="first">Chaoyu</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeyang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yijian</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiyan</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR 2021 Workshop on Geometrical and Topological Representation Learning</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Rule learning from knowledge graphs guided by embedding models</title>
		<author>
			<persName><forename type="first">Thinh</forename><surname>Vinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daria</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><forename type="middle">H</forename><surname>Stepanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Evgeny</forename><surname>Gad-Elrab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerhard</forename><surname>Kharlamov</surname></persName>
		</author>
		<author>
			<persName><surname>Weikum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Semantic Web Conference</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="72" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Multilingual Knowledge Graph Completion with Self-Supervised Adaptive Graph Alignment</title>
		<author>
			<persName><forename type="first">Zijie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoming</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyu</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanqing</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthik</forename><surname>Subbian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2203.14987</idno>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A survey on knowledge graphs: Representation, acquisition, and applications</title>
		<author>
			<persName><forename type="first">Shaoxiong</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Erik</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pekka</forename><surname>Marttinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="494" to="514" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Power up! Robust Graph Convolutional Network via Graph Powering</title>
		<author>
			<persName><forename type="first">Ming</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Somayeh</forename><surname>Sojoudi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence (AAAI)</title>
		<meeting>the AAAI Conference on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="8004" to="8012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning representations for counterfactual inference</title>
		<author>
			<persName><forename type="first">Fredrik</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uri</forename><surname>Shalit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3020" to="3029" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">T-gap: Learning to walk across time for temporal knowledge graph completion</title>
		<author>
			<persName><forename type="first">Jaehun</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinhong</forename><surname>Jung</surname></persName>
		</author>
		<author>
			<persName><surname>Kang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2012.10595</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Relational retrieval using a combination of path-constrained random walks</title>
		<author>
			<persName><forename type="first">Ni</forename><surname>Lao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page" from="53" to="67" />
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Predicting path failure in time-evolving graphs</title>
		<author>
			<persName><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhichao</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengyun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lujia</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery &amp; data mining</title>
		<meeting>the 25th ACM SIGKDD international conference on knowledge discovery &amp; data mining</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1279" to="1289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Semi-Supervised Hierarchical Graph Classification</title>
		<author>
			<persName><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongfeng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Rong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Generative causal explanations for graph neural networks</title>
		<author>
			<persName><forename type="first">Wanyu</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baochun</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="6666" to="6679" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning Causal Effects on Hypergraphs</title>
		<author>
			<persName><forename type="first">Jing</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mengting</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Longqi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jundong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brent</forename><surname>Hecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaime</forename><surname>Teevan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1202" to="1212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The core decomposition of networks: Theory, algorithms and applications</title>
		<author>
			<persName><forename type="first">Christos</forename><surname>Fragkiskos D Malliaros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Apostolos</forename><forename type="middle">N</forename><surname>Giatsidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michalis</forename><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName><surname>Vazirgiannis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The VLDB Journal</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="61" to="92" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">Yishay</forename><surname>Mansour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehryar</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Afshin</forename><surname>Rostamizadeh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:0902.3430</idno>
		<title level="m">Domain adaptation: Learning bounds and algorithms</title>
		<imprint>
			<date type="published" when="2009">2009. 2009</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Morgan</surname></persName>
		</author>
		<author>
			<persName><surname>Winship</surname></persName>
		</author>
		<title level="m">Counterfactuals and causal inference</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">DRUM: End-To-End Differentiable Rule Mining On Knowledge Graphs</title>
		<author>
			<persName><forename type="first">Ali</forename><surname>Sadeghian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammadreza</forename><surname>Armandpour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daisy</forename><forename type="middle">Zhe</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="15347" to="15357" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rianne</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European semantic web conference</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">End-to-end structure-aware convolutional networks for knowledge base completion</title>
		<author>
			<persName><forename type="first">Chao</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinbo</forename><surname>Bi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="3060" to="3067" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">RotatE: Knowledge Graph Embedding by Relational Rotation in Complex Space</title>
		<author>
			<persName><forename type="first">Zhiqing</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi-Hong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian-Yun</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Rethinking Graph Neural Networks for Anomaly Detection</title>
		<author>
			<persName><forename type="first">Jianheng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiajin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziqi</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">LINE: Large-scale Information Network Embedding</title>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingzhe</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiaozhu</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on World Wide Web</title>
		<meeting>the 24th International Conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1067" to="1077" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Positive-Unlabeled Learning with Adversarial Data Augmentation for Knowledge Graph Completion</title>
		<author>
			<persName><forename type="first">Zhenwei</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shichao</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongchun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuzhen</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Hoehndorf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangliang</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2205.00904</idno>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Observed versus latent features for knowledge base and text inference</title>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd workshop on continuous vector space models and their compositionality</title>
		<meeting>the 3rd workshop on continuous vector space models and their compositionality</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="57" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Complex embeddings for simple link prediction</title>
		<author>
			<persName><forename type="first">ThÃ©o</forename><surname>Trouillon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johannes</forename><surname>Welbl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ã‰ric</forename><surname>Gaussier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Bouchard</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2071" to="2080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Causal effect models for realistic individualized treatment and intention to treat rules</title>
		<author>
			<persName><forename type="first">J</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maya</forename><forename type="middle">L</forename><surname>Van Der Laan</surname></persName>
		</author>
		<author>
			<persName><surname>Petersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The international journal of biostatistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2007">2007. 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Composition-based Multi-Relational Graph Convolutional Networks</title>
		<author>
			<persName><forename type="first">Shikhar</forename><surname>Vashishth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumya</forename><surname>Sanyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitin</forename><surname>Vikram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Partha</forename><surname>Talukdar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Exploring Relational Semantics for Inductive Knowledge Graph Completion</title>
		<author>
			<persName><forename type="first">Changjian</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaofei</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Linhua</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeliang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Sha</surname></persName>
		</author>
		<idno type="DOI">10.1609/aaai.v36i4.20337</idno>
		<ptr target="https://doi.org/10.1609/aaai.v36i4.20337" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2022-06">2022. Jun. 2022</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="4184" to="4192" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Deep Graph Library: A Graph-Centric, Highly-Performant Package for Graph Neural Networks</title>
		<author>
			<persName><forename type="first">Minjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Da</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zihao</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quan</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mufei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinjing</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lingfan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Gai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianjun</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Karypis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Machine learning for treatment assignment: Improving individualized risk attribution</title>
		<author>
			<persName><forename type="first">Jeremy</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Finn</forename><surname>Kuusisto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kendrick</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Page</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AMIA Annual Symposium Proceedings</title>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="page">1306</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Embedding entities and relations for learning and inference in knowledge bases</title>
		<author>
			<persName><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Differentiable learning of logical rules for knowledge base reasoning</title>
		<author>
			<persName><forename type="first">Fan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="2316" to="2325" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">Liuyi</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhixuan</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaliang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidong</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.2002.02770</idno>
		<ptr target="https://doi.org/10.48550/ARXIV.2002.02770" />
	</analytic>
	<monogr>
		<title level="j">A Survey on Causal Inference</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Learning hierarchy-aware knowledge graph embeddings for link prediction</title>
		<author>
			<persName><forename type="first">Zhanqiu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianyu</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongdong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="3065" to="3072" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Deep learning on graphs: A survey</title>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering (TKDE)</title>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Rethinking Graph Convolutional Networks in Knowledge Graph Completion</title>
		<author>
			<persName><forename type="first">Zhanqiu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jieping</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Web Conference 2022</title>
		<meeting>the ACM Web Conference 2022</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="798" to="807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Counterfactual graph learning for link prediction</title>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Jiang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2106.02172</idno>
		<imprint>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Learning from counterfactual links for link prediction</title>
		<author>
			<persName><forename type="first">Tong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daheng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenhao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Meng</forename><surname>Jiang</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="26911" to="26926" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Graph contrastive learning with adaptive augmentation</title>
		<author>
			<persName><forename type="first">Yanqiao</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Web Conference 2021</title>
		<meeting>the Web Conference 2021</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2069" to="2080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Neural bellman-ford networks: A general graph neural network framework for link prediction</title>
		<author>
			<persName><forename type="first">Zhaocheng</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zuobai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis-Pascal</forename><surname>Xhonneux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
