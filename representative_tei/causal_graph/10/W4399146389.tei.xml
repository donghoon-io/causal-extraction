<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dynamic Causal Disentanglement Model for Dialogue Emotion Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2023-09-13">13 Sep 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yuting</forename><surname>Su</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yichen</forename><surname>Wei</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Weizhi</forename><surname>Nie</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Sicheng</forename><surname>Zhao</surname></persName>
						</author>
						<author>
							<persName><roleName>Senior Member</roleName><forename type="first">Anan</forename><surname>Liu</surname></persName>
						</author>
						<title level="a" type="main">Dynamic Causal Disentanglement Model for Dialogue Emotion Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-09-13">13 Sep 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2309.06928v1[cs.CL]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Emotion Detection; Structural Causal Model, Dynamic Causal Disentanglement Model</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Emotion detection is a critical technology extensively employed in diverse fields. While the incorporation of commonsense knowledge has proven beneficial for existing emotion detection methods, dialogue-based emotion detection encounters numerous difficulties and challenges due to human agency and the variability of dialogue content. In dialogues, human emotions tend to accumulate in bursts. However, they are often implicitly expressed. This implies that many genuine emotions remain concealed within a plethora of unrelated words and dialogues. In this paper, we propose a Dynamic Causal Disentanglement Model based on hidden variable separation, which is founded on the separation of hidden variables. This model effectively decomposes the content of dialogues and investigates the temporal accumulation of emotions, thereby enabling more precise emotion recognition. First, we introduce a novel Causal Directed Acyclic Graph (DAG) to establish the correlation between hidden emotional information and other observed elements. Subsequently, our approach utilizes pre-extracted personal attributes and utterance topics as guiding factors for the distribution of hidden variables, aiming to separate irrelevant ones. Specifically, we propose a dynamic temporal disentanglement model to infer the propagation of utterances and hidden variables, enabling the accumulation of emotion-related information throughout the conversation. To guide this disentanglement process, we leverage the ChatGPT-4.0 and LSTM networks to extract utterance topics and personal attributes as observed information. Finally, we test our approach on two popular datasets in dialogue emotion detection and relevant experimental results verified the model's superiority.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>With the ongoing advancements in computer technology and increasing technological sophistication, human-computer communication has spread to a wide range of applications. It is essential for robots to sense users' emotions and provide coherent, empathetic responses. This underscores the significant potential value of dialogue emotion detection across diverse fields. Emotion detection can be used effectively in recommendation systems, medical contexts, education, and beyond. With the proliferation of interactive machines, the importance of emotion detection is steadily increasing. Due to the wide spectrum of emotional expressions, particularly within varying topics and contexts, a single utterance may contain different emotional states, thus presenting a challenge for emotion detection.</p><p>Yuting Su, Yichen Wei, Weizhi Nie, and Anan Liu are with the School of Electrical and Information Engineering, Tianjin University. Sicheng Zhao is with the School of Software, Tsinghua University.</p><p>Weizhi Nie is the Corresponding author, Email: weizhinie@tju.edu.cn. With the exploration of NLP technology, researchers proposed some methods to solve emotional detection problems. Khare et al. <ref type="bibr" target="#b0">[1]</ref> utilized CNN networks and proposed an emotion detection method based on time-frequency representation, using various convolutional neural networks for automatic feature extraction and classification. Wang et al. <ref type="bibr" target="#b1">[2]</ref> extracted emotion features using wavelet packet analysis from speech signals for speaker-independent emotion detection. Jiao et al. <ref type="bibr" target="#b2">[3]</ref> proposed an attention gated hierarchical memory network for real-time emotion detection processing. Huang et al. <ref type="bibr" target="#b3">[4]</ref> proposed a Bi-LSTM network that can efficiently utilize past and future input features to classify. Majumder et al. <ref type="bibr" target="#b4">[5]</ref> proposed a new method based on recurrent neural networks that keeps track of the individual party states throughout the conversation and utilizes this information for emotion classification. Metallinou et al. <ref type="bibr" target="#b5">[6]</ref> focused on temporary emotional context and demonstrated the effectiveness of context-sensitive in emotion detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Motivation</head><p>We find that prevailing techniques in dialogue emotion detection directly utilize the text of dialogue utterances for feature extraction and modeling. This approach, however, results in words unrelated to emotion in dialogues impeding accurate emotion detection. We view these unrelated words as spurious correlation information. Furthermore, we recognize that keywords in preceding utterances can exert influence on the emotion detection of subsequent utterances. Thus, acquiring contextual information from prior dialogues plays a pivotal role in guiding emotion detection. As is shown in Fig. <ref type="figure" target="#fig_0">1</ref>, it is a dialogue between two friends from the MELD dataset. We marked the interfering words in the dialogue utterances in red color. For example, "stars in a drag show" may mean a joyful and lively performance scene, but in fact, the emotion label of the utterance is Sadness. We find the distant relationship mentioned in the previous dialogue can better explain the speaker's sadness in this utterance. We posit that the utilization of the previous context and the exclusion of spurious correlation information in utterances can significantly improve the accuracy of emotion detection.</p><p>In our work, to avoid the impact of spurious correlation information, we propose a Causal Disentanglement Model. Our objective is to model hidden variables which propagate to generate the utterances, and to separate hidden variables unrelated to emotions from other variables. The causal relationship is illustrated in Fig. <ref type="figure" target="#fig_1">2</ref> Among the hidden variables constituting the utterance U , namely s, v, and z, we designate s as the hidden variable associated with emotions and contributing to utterance composition, v as the hidden variable linked to dialogue topics related to emotions, and z as the hidden variable unrelated to emotions. As depicted in the figure, both s and v have a causal relationship with the emotion E, while v possesses a causal relationship with the dialogue topic F . We apply ChatGPT to complete the text feature extraction task, i.e., extraction of the dialogue topic F . Subsequently, we augment the dataset with the extracted topic text and topic feature vectors for future research. Notably, throughout each time stage, the evolution and generation of all hidden variables are influenced by the auxiliary variable P , which represents personal attribute information derived from previous conversations using an LSTM network. In addition, we propose a sequential VAE framework <ref type="bibr" target="#b6">[7]</ref> to reconstruct dialogue utterance U and dialogue topic F as part of our model optimization process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Contributions</head><p>This paper makes the following main contributions:</p><p>• We propose a Dynamic Causal Disentanglement Model, which aims to separate hidden variables unrelated to emotions from dialogue utterances. Our model significantly improves the robustness of emotional features. • We apply ChatGPT to complete the feature extraction task and add the topic text and feature vectors to the existing dataset. • We evaluate the performance of our method on the IEMOCAP and MELD datasets. The experimental results indicate that our method has significant improvements compared to the most advanced methods. In Section II, we present the relevant work. In Section III, we briefly introduce the background of the causal model. In Section IV, we introduce the specific content of the model and the calculation formula of the methodology in detail. In Section V, we present key experiments. In Section VI, we present the conclusions of our work and discuss future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Emotion detection</head><p>Emotion detection represents a prominent and actively researched subfield within natural language processing, finding widespread application in diverse domains, including the medical field, intelligent devices, and the Internet of Things <ref type="bibr" target="#b7">[8]</ref>- <ref type="bibr" target="#b10">[11]</ref>. The initial research on dialogue emotion detection is mainly based on audio features and video features <ref type="bibr" target="#b11">[12]</ref>. Recently, the development of deep learning drives the research on dialogue emotion detection in text <ref type="bibr" target="#b12">[13]</ref> and speech <ref type="bibr" target="#b13">[14]</ref>.</p><p>Long Short Term Memory (LSTM) networks <ref type="bibr" target="#b14">[15]</ref> is a temporal recurrent neural network. It is one of the most common and effective methods for dealing with sequential tasks. Poria et al. <ref type="bibr" target="#b15">[16]</ref> proposed a detection model-based LSTM that enables discourse to capture contextual information from the surrounding environment within the same video. Li et al. <ref type="bibr" target="#b16">[17]</ref> proposed a multimodal attention-based BLSTM network framework for detection. They propose Attentionbased Bidirectional Long Short-Term Memory Recurrent Neural Networks (LSTM-RNNs) to automatically learn the best temporal features that are useful for detection. Huang et al. <ref type="bibr" target="#b17">[18]</ref> propose a novel Hierarchical LSTMs for Contextual Emotion Detection (HRLCE) model.</p><p>In recent studies, the incorporation of common sense knowledge has been increasingly prevalent. In tasks involving inference, the integration of common sense knowledge facilitates the deduction of implicit information from textual content. Ghosal et al. <ref type="bibr" target="#b18">[19]</ref> proposed COSMIC, a new framework that includes different commonsense elements to assist in detection. Zhong et al. <ref type="bibr" target="#b19">[20]</ref> leveraged commonsense knowledge to enrich the transformer encoder. Li et al. <ref type="bibr" target="#b20">[21]</ref> proposed a conversation modeling module to accumulate information from the conversation and proposed a knowledge integration strategy to integrate the conversation-related commonsense knowledge generated from the event-based knowledge graph. Bosselut et al. <ref type="bibr" target="#b21">[22]</ref> proposed commonsense transformers (COMET) which are used to learn to generate rich and diverse commonsense descriptions in natural language.</p><p>Graph models are widely applied to process and analyze semantics and relations in the texts. The latest progress in graph model research has also been introduced into the field of emotion detection. RGAT <ref type="bibr" target="#b22">[23]</ref> can capture the speaker dependency and the sequential information with the relational position encodings which provide the RGAT model with sequential information reflecting the relational graph structure. IGCN <ref type="bibr" target="#b23">[24]</ref> applied the incremental graph structure to imitate the process of dynamic conversation. Choi et al. <ref type="bibr" target="#b24">[25]</ref> utilized the residual network (ResNet)-based, intrautterance feature extractor and the GCN-based, interutterance feature extractor to fully exploit the intra-inter informative features.</p><p>Nevertheless, these approaches fail to disentangle features causally related to emotions from dialogue utterances, which results in spurious correlations and diminished robustness in feature extraction. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Causal Hidden Markov Model</head><p>The Causal Hidden Markov Model is a common statistical model, which is widely utilized in many time series problems, including image processing, natural language processing, bioinformatics, and other fields. The Causal Hidden Markov Model is an extension of the Markov process that integrates hidden states. In contrast to the standard Markov Model, the Causal Hidden Markov Model excels in capturing the causal relationship between observation sequences and hidden state sequences by acquiring knowledge of the transition probabilities and probability distributions of observations and hidden states. This enhanced capability contributes to a more effective modeling of time series data. Many methods are proposed based on Causal Hidden Markov Models. Khiatani et al. <ref type="bibr" target="#b25">[26]</ref> constructed the Hidden Markov Model to learn the relationship between weather states and hidden state sequences, achieving more accurate weather prediction. ZHANG et al. <ref type="bibr" target="#b26">[27]</ref> utilized the Factorial Hidden Markov Model Based on the Gaussian Mixture Model for non-invasive load detection. Guo et al. <ref type="bibr" target="#b27">[28]</ref> attempted to accurately predict the chloride ion concentration that causes bridge degradation, and in the face of random detection time intervals, proposed a predictive Hidden semi-Markov Model to achieve prediction. Zhao et al. <ref type="bibr" target="#b28">[29]</ref> proposed a Causal Conditional Hidden Markov Model to predict multimodal traffic flow, which respectively designs a prior network and a posterior network to mine the causal relation in the hidden variables inference stage. Mak et al. <ref type="bibr" target="#b29">[30]</ref> proposed a Causal Topology Design Hidden Markov Model to solve viewpoint variation issue, which can help view independent multiple silhouette posture recognition. Suphalakshmi et al. <ref type="bibr" target="#b30">[31]</ref> utilized a Full Causal Two Dimensional Hidden Markov Model with a novel 2D Viterbi algorithm for image segmentation and classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PRELIMINARIES</head><p>We commence with an introduction to Structural Causal Models (SCMs). The SCM refers to a causal graph represented by a directed acyclic graph associated with structural equations. The causal graph is denoted as G := (V, E), where V and E represent node set and edge set respectively. In the edge set, each arrow x → y (x, y ∈ E) indicates that x has a direct influence on y and changes the distribution of y. The structural equation contains the production relation of each variable in the point set. In the node-set, for V := {v 1 , . . . , v k }, we define p(v i ) to represent the set of parent nodes of v i , and the set of the parent nodes will affect it. Through the production relation, we define the relation as</p><formula xml:id="formula_0">v i ← f i (p(v i ))</formula><p>where f i denotes causal mechanisms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. METHODOLOGY</head><p>Inspired by the Causal Hidden Markov Model <ref type="bibr" target="#b31">[32]</ref>, we propose the Dynamic Causal Disentanglement Model to model dialogues as a novel approach for modeling dialogues. Our modifications and enhancements to the model are tailored to facilitate the detection of emotional content within dialogue utterances. In our Dynamic Causal Disentanglement Model, we denote observed variable f t ∈ F , u t ∈ U , p t ∈ P as the dialogue topics, dialogue utterances, and personal attributes at time stage t. The e t ∈ E is the predicted emotion label at time stage t. We disentangle the utterance into hidden variables s t , v t , z t . To observe the progress of dialogues clearly, we design an LSTM network to learn the features of the speaker's attributes at the current time stage. Due to the recent popularity of ChatGPT, we apply ChatGPT-4.0 to extract dialogue topic features.</p><p>In this section, we introduce the Dynamic Causal Disentanglement Model in detail and elucidate the methods used for learning hidden variables within the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Dynamic Causal Disentanglement Model</head><p>To describe the dialogue progresses, we introduce the causal directed acyclic graph (CDAG) shown in Fig. <ref type="figure" target="#fig_1">2</ref>. In the CDAG, the causal relation of variable m is formulated as In causal learning, we propose prior and posterior networks to learn the distribution of variables to achieve disentanglement. In the prior network, we input the concatenated vectors into the GRU and subsequently design two independent Fully Connected (FC) layers to obtain the mean and the log variance vectors. Within the posterior network, we input concatenated vectors into the FC layer, similar to the prior network, to obtain mean and log variance vectors. The hidden variable data obtained from the posterior network is then utilized for the reconstruction of observed variables and the prediction of labels.</p><formula xml:id="formula_1">f m := (p a (m), ε m ), where p a (m) is parent nodes of node</formula><p>m and ε m is independent exogenous variables. All variables form a Hidden Markov model. We define the model as follows:</p><formula xml:id="formula_2">z t ← f z (P t , ε t z ), v t ← f v (P t , ε t v ), s t ← f s (P t , ε t s ), P t ← f P (ε t P ), F t ← f F (v t , ε t F ), E t ← f E (s t , v t , ε t E ), U t ← f U (s t , v t , z t , ε t U ).</formula><p>To enhance detection accuracy, our objective is to disentangle emotion-related information. We introduce hidden variables s t , v t , and z t to construct the observation variables U t , F t , and E t at respective time stages. Concurrently, the hidden variables are influenced by the auxiliary variable P t , which undergoes continuous updates throughout the ongoing dialogue. s t represents the hidden variable associated with emotions and contributing to utterance composition, v t represents the hidden variable linked to dialogue topics related to emotions and z t represents the hidden variable unrelated to emotions. Collectively, these hidden variables form a comprehensive utterance representation. The hidden variables s t and v t , which impact emotional assessment, are related to the emotion label E t , whereas the hidden variable v t pertaining to the dialogue topic solely influences the dialogue topic F t . Improved predictive accuracy can be achieved by isolating the hidden variables contributing to the utterance and eliminating extraneous variables. For simplicity, we define h := {s, v, z} ; o := {P, F, U }. According to Causal Markov Condition <ref type="bibr" target="#b32">[33]</ref>, we calculate the factorization of joint distribution as:</p><formula xml:id="formula_3">p(h ≤T , o ≤T , E ≤T ) = T t=1 p(E t |s t , v t )• p(h t |h t-1 , P t ) • p(U t |h t ) • p(F t |v t ) .<label>(1)</label></formula><p>We aim to analyze the distribution of hidden variables from the prior distribution p ψ (h t |h t-1 , P t ). Due to the difficulty of calculation, we choose to utilize posterior distribution q ϕ (h ≤T |o ≤T , E ≤T ) for approximation <ref type="bibr" target="#b33">[34]</ref>. p(E t |s t , v t ), p(U t |h t ), p(F t |v t ) are the processing of learned hidden variables in the generation network. After defining the joint probability distribution, we propose a sequential VAE framework to learn our causal model, as shown in Fig. <ref type="figure" target="#fig_2">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1)</head><p>Prior Network: For establishing prior distributions, we introduce a prior network into our model. This prior network takes both the observed variable P and the hidden variables from the preceding time step as inputs. We utilize Gated Recurrent Units (GRUs) <ref type="bibr" target="#b34">[35]</ref> to propagate hidden variables, enhancing the model's capacity to capture long-term dependencies. After the GRUs unit, we design independent Fully Connected layers (FCs) to obtain the mean and the log variance vectors of the hidden variables. We utilize the hidden variable h ∈ {s, v, z} to illustrate the disentanglement propagation process of GRUs as follows:</p><formula xml:id="formula_4">r p t = σ(W p r • [h t-1 , P t ] + b p r ), k p t = σ(W p z • [h t-1 , P t ] + b p k ), hp t = tanh(W p • [r t ⊙ h t-1 , P t ] + b p ), h p t = (1 -k p t ) ⊙ h p t-1 + k p t ⊙ hp t .</formula><p>(2) r p t and z p t represent the status of the reset gate and update gate. Hidden update status h p t is determined by candidate hidden status hp t and update gate status z p t . In the above formula, σ represents the sigmoid function, and ⊙ represents the elementwise multiplication operation. All hidden variables are learned in the same method.</p><p>2) Posterior Network: In probability graph models, solving the posterior distribution of hidden variables through Bayesian formulas can be challenging. Consequently, we opt for using a known simple distribution to approximate the intricate distribution that requires inference. We utilize distinct units to learn posterior distributions as follows:</p><formula xml:id="formula_5">s q t = W q s • [s t-1 , U t , F t , P t ] + b q s , v q t = W q v • [v t-1 , U t , F t , P t ] + b q v , z q t = W q z • [s t-1 , U t , P t ] + b q s .<label>(3)</label></formula><p>Similar to prior networks, we apply Fully Connected layers (FCs) to obtain the mean and the log variance vectors of the hidden variables. These vectors will be utilized for reconstruction in the following section.</p><p>3) Generation Network: In the generative network, we employ the updated hidden variables to reconstruct the observed variables and predict emotion labels. To guarantee the representation ability of learned variables, we utilize s q t , v q t , z q t to obtain the reconstructed utterance Û , and v q t to obtain the reconstructed conversation topic F . We optimize our model by approximating the reconstructed variables Û and F to the observed variables U and F .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Learning Method</head><p>The optimization target is the Evidence Lower Bound (ELBO). ELBO consists of evidence and KL divergence and maximizing ELBO is tantamount to minimizing the KL divergence, intending to approximate the posterior distribution as closely as possible to the true posterior distribution. ELBO is represented as:</p><formula xml:id="formula_6">E p(o ≤T ,E ≤T ) E q ϕ (h ≤T |o ≤T ,E ≤T ) log (p ψ (o ≤T , E ≤T )) -D (q ϕ (h ≤T |o ≤T , E ≤T ) ∥ p ψ (h ≤T |o ≤T , E ≤T )) ,<label>(4)</label></formula><p>where D(• ∥ •) denotes KL divergence. After simplifying the evidence and KL divergence, we obtain:</p><formula xml:id="formula_7">E p(o ≤T ,E ≤T ) E q ϕ (h ≤T |o ≤T ,E ≤T ) log p ψ (h ≤T , o ≤T , E ≤T ) q ϕ (h ≤T |o ≤T , E ≤T ) .</formula><p>(5) According to Eq.1, we know the prior distribution p ψ . We consider the mean and the log variance vectors in the prior network as the learned results, p ψ (α t |α t-1 , P t ) for each t is distributed as N (µ ψ (α t-1 , P t ), Σ ψ (α t-1 , P t )). In the posterior distribution, the posterior is given by:</p><formula xml:id="formula_8">q ϕ (h ≤T |o ≤T , E ≤T ) = T t=1 [q ϕ (E t |s t , v t ) • q ϕ (h t |h t-1 , o t )] q ϕ (E ≤T |o ≤T )</formula><p>.</p><p>(6) Under the similar reparameterization, q ϕ (h t |h t-1 , o t ) for each t is distributed as N (µ(h t-1 , o t ), Σ(h t-1 , o t )). Substituting Eq.1 and Eq.6 in Eq.5, we reformulate the ELBO as:</p><formula xml:id="formula_9">E p(o ≤T ,E ≤T ) log q ϕ (E ≤t |o ≤T ) + L q ϕ ,p ψ .<label>(7)</label></formula><p>In Eq.7, following the derivation based on probability theory, we know that q ϕ (E ≤T |o ≤T ): and L q ϕ ,p ψ denotes as follows:</p><formula xml:id="formula_10">T t=1 [q ϕ (E t |s t , v t ) • q ϕ (h t |h t-1 , o t )] dh 0 . . . dh T ,<label>(8)</label></formula><formula xml:id="formula_11">L q ϕ ,p ψ = T t=1 E q ϕ (h t |h t-1 , o t ) [(L 1 + L 2 + L 3 )] ,<label>(9)</label></formula><p>where the L 1 , L 2 , L 3 are respectively defined as:</p><formula xml:id="formula_12">L 1 = log p ψ (U t |h t ) • p ψ (F t |v t ), L 2 = log p ψ (E t |s t , v t ) q ϕ (E t |s t , v t ) , L 3 = log p ψ (h t |h t-1 , P t-1 ) q ϕ (h t |h t-1 , o t ) .</formula><p>We parameterize the p ψ (E t |s t , v t ) as q ϕ (E t |s t , v t ), making L 2 degenerate to 0. At each time stage t, the L 1 denotes the reconstruction loss, while the L 3 denotes the KL divergence of the hidden variables.</p><p>C. Feature extraction based on ChatGPT-4.0</p><p>ChatGPT is a large-scale natural language processing model developed by OpenAI. Following extensive training, ChatGPT exhibits exceptional competence in both comprehending and generating natural language text, making it versatile for diverse natural language processing tasks. For the task of dialogue emotion detection, we rely on ChatGPT for pertinent text feature extraction.</p><p>In our approach, obtaining the dialogue utterance's topic as the observation variable is essential. As the conversation unfolds, the topic continuously evolves, necessitating the acquisition of topic features at each time stage. We have selected ChatGPT-4.0 for this task due to its superior contextual understanding in text processing tasks. Specifically, it excels in providing more precise topic information by comprehending preceding dialogues. A visual representation of topic extraction is depicted in Fig. <ref type="figure" target="#fig_3">4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. EXPERIMENT</head><p>We apply our method to IEMOCAP and MELD datasets. Subsequently, we conducted ablation experiments and robustness analyses. Initially, we present the data conditions and implementation details. A. Datasets IEMOCAP <ref type="bibr" target="#b35">[36]</ref> is a multimodal conversation dataset collected by the SAIL laboratory at the University of Southern California. This dataset comprises 151 dialogues, totaling 7433 sentences. The dataset is annotated with six emotion categories: neutral, happy, sad, angry, frustrated, and excited. Non-neutral emotions constitute a majority at 77%. IEMOCAP stands as the most widely utilized dataset in the field of dialogue emotion detection, characterized by its high-quality annotations. However, it is worth noting that the dataset has a relatively limited data size.</p><p>MELD <ref type="bibr" target="#b36">[37]</ref> is an extension of the EmotionLines dataset <ref type="bibr" target="#b37">[38]</ref>, augmented with additional audio and video information. The MELD dataset comprises 1433 multi-party dialogues and 13708 utterances extracted from the television series "Friends". This dataset provides annotations for utterances, categorizing them into seven distinct emotion categories: neutral, joy, surprise, sadness, anger, disgust, and fear. Non-neutral emotions constitute a majority at 53%. MELD is distinguished by its high-quality annotations and the incorporation of multimodal data. Statistics of splits in two datasets are shown in Table .I.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Baseline and State-of-the-Arts</head><p>BC-LSTM <ref type="bibr" target="#b15">[16]</ref> proposes a bidirectional LSTM network to capture contextual content, generate context aware discourse representations, and utilize them for sentiment classification. However, this model only considers context dependencies and ignores speaker dependencies.</p><p>CMN <ref type="bibr" target="#b38">[39]</ref> utilizes the dependency relationships of speakers to identify emotions. It designs different GRU networks for two speakers to model utterance context from dialogue history and inputs current utterances into different memory networks to learn utterance representation. However, this model is limited to conversations with only two participants.</p><p>ICON <ref type="bibr" target="#b39">[40]</ref> is an extension of CMN that utilizes another GRU to connect the GRU network outputs of two speakers for explicit speaker modeling. Similarly, it can only model conversations between two speakers.</p><p>ConGCN <ref type="bibr" target="#b40">[41]</ref> constructs a heterogeneous graph convolutional neural network. The utterances and speakers of a conversation are represented by nodes. The graph contains edges between utterance nodes and edges between utterance and speaker nodes. This method models context and speakersensitive dependencies for emotion detection.</p><p>KET <ref type="bibr" target="#b19">[20]</ref> is a knowledge-enriched transformer for emotion detection. It dynamically leverages common sense knowledge by using a context aware graph attention mechanism.</p><p>DialogueRNN <ref type="bibr" target="#b4">[5]</ref> is a benchmark model for dialogue emotion detection. It is a recurrent network that utilizes three GRU networks to model the speaker, the context from the preceding utterances, and the emotion of the preceding utterance to track the emotion of each speaker.</p><p>DialogueGCN <ref type="bibr" target="#b41">[42]</ref> represents the structure and interaction of a conversation by constructing a graph structure between participants and utilizes graph convolutional neural networks for message passing and feature learning of the graph structure.</p><p>KES <ref type="bibr" target="#b42">[43]</ref> utilizes a self-attention layer specialized for enhanced semantic text features with external commonsense knowledge and two networks based on LSTM for tracking individual internal state and context external state to learn interactions between interlocutors participating in a conversation.</p><p>TL-ERC <ref type="bibr" target="#b43">[44]</ref> utilizes a generative conversational model to transfer emotional knowledge. It transfers the parameters of a trained hierarchical model to an emotion detection classifier. SS-HiT <ref type="bibr" target="#b44">[45]</ref> is a novel semantic and sentiment hierarchical transformer for emotion detection. Each utterance token is represented as matrices with both semantic and sentiment word embeddings. Then, fuse utterance tokens as token features to further capture the long dependent utterance-level information through transformer encoders for prediction.</p><p>Sentic GAT <ref type="bibr" target="#b45">[46]</ref> is a context-and sentiment-aware framework, which designs a dialogue transformer (DT) network with hierarchical multihead attention to capture the intra-and inter-dependency relationship in context. In addition, commonsense knowledge is dynamically represented by the contextand sentiment-aware graph attention mechanism based on sentimental consistency.</p><p>EmpaGen <ref type="bibr" target="#b46">[47]</ref> proposes an auxiliary empathic multiturn dialogue generation to enhance dialogue emotion understanding, which is the first attempt to utilize empathy-based dialogue generation for the emotion detection task.</p><p>GGCN <ref type="bibr" target="#b47">[48]</ref> is a growing graph convolution network that proposes a prior knowledge extraction framework to get auxiliary information. The method considers utterance connections and emotion accumulation for emotional detection.</p><p>IEIN <ref type="bibr" target="#b48">[49]</ref> is an iterative emotional interaction model that utilizes iteratively predicted emotional labels instead of real emotional labels to continuously correct predictions and provide feedback on inputs during the iteration process.</p><p>SDTN <ref type="bibr" target="#b49">[50]</ref> dynamically tracks the local and global speaker states as the conversation progresses to capture implicit stimulation of emotional shift. It mainly contains the speaker interaction tracker based on GRU and the emotion state decoder module based on the conditional random field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Implementation Details</head><p>In this section, we will furnish a comprehensive overview of the implementation of our proposed method. We preextract topic features and personal attribute features from the prior dialogue content. For the extraction of personal attribute features, we design an LSTM network and specify the feature vector dimension as 64.</p><p>We utilize ChatGPT-4.0 API with Python 3 on a single Nvidia RTX 3060 GPU and an Intel i7 CPU. We configure ChatGPT-4.0 as a text work assistant for the task of text extraction. Following the role assignment, we provide the prompt: "I will give you a set of daily dialogue, understand the meaning of the entire dialogue, and extract the topic from each utterance in conjunction with the previous context. Use concise vocabulary to accurately express the topic." Additionally, we specify detailed format requirements to facilitate future text collection and dataset construction. To ensure a comprehensive understanding of dialogue content and context, we input entire dialogues into the model, instructing it to extract topic information for each utterance. However, in the case of lengthy dialogues with numerous utterances, the model occasionally miscounted them, leading to a mismatch between the number of generated topics and utterances. To address this, we segment long dialogues into batches, each containing 20 utterances. This approach allows ChatGPT-4.0 to better grasp the context and accurately determine the number of utterances. We apply the bert-base-uncased model for processing English text to convert topic text information into 768-dimension feature vectors for subsequent experiments, which is one of the pre-trained BERT models <ref type="bibr" target="#b50">[51]</ref>. Then we utilize two linear layers to reduce the dimensionality of the feature vectors to 64 dimensions.</p><p>Then, we construct the Dynamic Causal Disentanglement Model by combining all observed variables. The parameter matrices of the observation variable extraction model and the model are randomly initialized and continuously optimized during training. Throughout the training process, we utilize Adam as the optimizer with the weight attenuation set to 0.00005 and the learning rate set to 0.001. The model is trained for 80 epochs. We develop the evaluations with Python 3 and PyTorch 1.13.0 on a single Nvidia RTX 3060 GPU and an Intel i7 CPU.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D. Comparison with State-of-the-Arts</head><p>We compare the performance of our proposed model with the current state-of-the-art methods. The experimental results show that our model performs better than other state-of-the-art models on two benchmark datasets.</p><p>IEMOCAP: IEMOCAP is a dyadic interactive dialogues dataset consisting of long conversations and it contains many non-neutral emotions. We evaluate our model on the IEMO-CAP dataset, and the experiment results are shown in Table .II. Our model achieves a new state-of-the-art F1-score of 68.9% and accuracy of 68.8% for the emotion detection task on the IEMOCAP dataset. We compare the results of our proposed model with the state-of-the-art methods. Although our model still needs to improve the performance in some emotional categories, it is worth noting that our model has made significant improvements in both Happy and Frustrated emotions. Compared with the IEIN method, our model demonstrates a critical improvement of 1.2% in the average F1-score of Happy emotion. Compared with the state-of-the-art methods, our model achieves improvements in the accuracy of 0.7% and the average F1-score of 0.8% for the Neutral emotion. Compared with the GGCN method, our model achieves a significant improvement of 0.5% and 1.9% in the average F1score of the Excited and Frustrated emotions. In general, our approach outperforms the state-of-the-art methods.</p><p>MELD: The MELD dataset comprises multiparty conversations extracted from television series. Notably, many MELD dialogues involve more than five participants, with a maximum of nine, resulting in limited utterances per participant and posing challenges for contextual dependency modeling. Furthermore, the dataset exhibits a substantial presence of nonneutral emotions. In contrast to the IEMOCAP dataset, MELD  dialogues are notably shorter, with an average length of 10 utterances, intensifying the challenges in emotion detection. Importantly, we find that the CMN and ICON methods, designed for dyadic conversations, are not suitable for MELD. These emotion detection models achieve poor results on this dataset.</p><p>Our model achieves a new state-of-the-art F1-score of 67.5%, and our model obtains the best results among the compared methods, the experiment results are shown in Table.III. Our model has made significant progress in Anger, Joy, and Surprise emotions. Although the F1-score of our model is not as good as IEIN and GGCN, compared to most methods, we still make progress in Disgust and Fear emotions. We observe that the topic features extracted by ChatGPT-4.0 substantially enhance our model's performance, enabling a deeper understanding of utterance meanings and the speakers' psychology in brief conversations involving multiple participants. Consequently, our model demonstrates exceptional performance on the MELD dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E. Ablation Study</head><p>To underscore the effectiveness of hidden variables disentanglement and the significance of external variables in our approach, we undertake an ablation study. The comparison results are in Table .IV, we conclude that leveraging relevant hidden variables derived from disentanglement aids in emotion detection. Our approach demonstrates significant improvements in accuracy and F1-score when compared to models lacking disentanglement. The result validates our idea and also proves the effectiveness of disentanglement.</p><p>We observe that when employing LSTM-extracted topic features and personal attribute features separately, the models incorporating personal attribute features outperformed those relying on topic features by 1.7% and 1.3% in terms of F1scores for both datasets, respectively. Our analysis suggests that personal attribute features, serving as auxiliary variables in the propagation of hidden variables, elucidate the distribution of hidden variables, thereby exerting a more pronounced influence on result enhancement. We find that in comparison to the model using theme features extracted by LSTM, the models incorporating theme features extracted by ChatGPT-4.0 and BERT exhibited improvements of 0.9% and 0.8% in F1score on both datasets, respectively, demonstrating the superior ability of ChatGPT-4.0 in text processing.</p><p>The model combining two observation variables and disentanglement achieves the best results, which verifies the effectiveness of our proposed method and all observed variables. Overall, the introduction of observed variables and the disentanglement of hidden variables collectively enhance the accuracy of emotion detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F. Discussion on Prediction Accuracy of Long Dialogue Utterances</head><p>To further assess the stability of our proposed model, we calculate the accuracy of emotion detection within each period, as illustrated in Fig. <ref type="figure" target="#fig_4">5</ref>. Specifically, for the IEMOCAP dataset, we divide every five utterances into a time batch, and due to the extended length of dialogues in this dataset, we analyze the first forty utterances. In the case of MELD, a dataset featuring shorter dialogues, we segment each utterance into a time batch and analyze the first eight utterances.</p><p>We observe that the emotion detection methods BC-LSTM and DialogueRNN exhibit lower accuracy in the early stages of a dialogue, with accuracy gradually improving as the dialogue progresses. The results suggest that these contextual modeling methods require a greater number of utterances to comprehend ongoing dialogues due to the changing dialogue topics and contexts. In contrast, our method effectively mitigates this challenge. By disentangling hidden variables related to emotions at the outset of the dialogue, our model achieves high accuracy in the early time batch. Furthermore, our prediction accuracy remains stable and consistently high, facilitated by the incorporation of dialogue topics and personal attributes. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G. Effectiveness of Disentanglement</head><p>To demonstrate the reliability of the related variables after disentanglement, we implement a further validation method for s, v, and z. Specifically, we first train the Dynamic Causal Disentanglement Model and the observation variable extraction model to obtain the distribution of hidden variables. Then, we independently train six independent classifiers for prediction by different combinations of hidden variables, and the experiment results are shown in Table .V. We find that the results of the prediction containing z are lower than those without z, which proves the existence of spurious correlation information in the utterances. The experiments conducted with s and v both yield favorable results. The accuracy and F1 score obtained from variable s surpass those extracted from variable v on two datasets, suggesting that s harbors more salient feature information about emotional factors. The experimental results obtained by combining s and v are the best, affirming the validity and effectiveness of our approach.</p><p>To intuitively observe the influence of hidden variables on emotional features, we visualize emotional features acquired under varying conditions on the IEMOCAP dataset to examine emotion distribution within the feature space. Employing the t-SNE method <ref type="bibr" target="#b51">[52]</ref>, we project high-dimensional text features into a two-dimensional space, utilizing distinct colors for emotion labels, as depicted in Fig. <ref type="figure" target="#fig_5">6</ref>. In the two-dimensional space after dimensionality reduction, the inter-point distances serve to reflect high-dimensional data similarities. Observing the distribution of the text feature vectors obtained by preprocessing, we find most utterances with different emotions are bounded by each other, but a small number of utterances still exhibit a messy distribution. Simultaneously, we observe that utterances with non-neutral emotions and utterances with neutral emotions are close in two-dimensional space, implying their higher similarity in high-dimensional feature vectors. In contrast, the performance of the text feature vectors learned by the hidden variable s, v, and z demonstrates some enhancement. Utterances with neutral emotions tend to stay away from utterances with other emotions. However, the introduction of unrelated variable z constrains more precise boundary distinctions. Notably, we find that the text feature vectors learned by relevant variables s and v with the separation of the unrelated z achieve the best performance. The distinction between utterances with different emotions is more accurate, and the distance in the two-dimensional space is far, indicating the low similarity of high-dimensional features. The result further validates our perspective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>H. Visualization of Confusion Matrix and Analysis</head><p>We show the visualization to observe the comparison of emotion classification results between different methods. The confusion matrixes of different methods on the IEMOCAP and MELD datasets are shown in Fig. <ref type="figure" target="#fig_6">7</ref>.</p><p>In the case of the IEMOCAP dataset, experimental results reveal a notably high misprediction rate between the Neutral and Frustrated emotions. Our analysis indicates that, in some instances, humans may not express their frustration directly in their utterances, making it significantly different from sadness. This observation underscores that a speaker's emotion cannot be fully comprehended based solely on dialogue utterances. Our method makes up for the shortcomings of BC-LSTM, DialogueRNN, and other methods and we achieve a significant improvement of 0.8% and 1.9% in the average F1-score of the Neutral and Frustrated emotion, which demonstrates our model has a more comprehensive understanding of dialogue utterance. Furthermore, our method enhances F1-score accuracy across other emotional categories. We attribute this improvement to the separation of interfering words and the utilization of ChatGPT-4.0 for topic extraction. In the MELD dataset, the imbalance in emotion labels poses a challenge Fig. <ref type="figure">8</ref>. The importance of unrelated word separation and topic extraction in dialogue emotion detection. The phrase "crush on you" is a mention of the past rather than an expression of love. The reply "oh" is a response of sadness rather than a common surprise. Our method can comprehensively understand the meaning of dialogue utterances.</p><p>to emotion detection, leading to many non-neutral emotions being incorrectly classified as Neutral in the validation set. Through a thorough examination of the confusion matrix, We observe that our model substantially enhances the accuracy of Fear and Disgust emotions detecting, in contrast to other models incapable of detecting these specific emotions. Our model achieves remarkable enhancements in accuracy and F1scores across multiple emotional categories.</p><p>We choose a dialogue from the MELD dataset to validate our viewpoint, as shown in Fig. <ref type="figure">8</ref>. The phrase "crush on you" mentioned in the first sentence does not convey an expression of love but rather refers to a concerning reference to events from the past, which may mislead our prediction. Within the dialogue, when Ross responds with "oh" upon being characterized as geeky, this response initially appears surprising. However, considering the context from the preceding text, we discern that Ross failed to make a favorable impression on Rachel, with whom he had a romantic interest. This understanding, combined with the extracted topic, indicates that his response carries an underlying sense of disappointment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. CONCLUSION</head><p>We propose a Dynamic Causal Disentanglement model for emotion detection in dialogues. In our model, we introduce hidden variables to disentangle hidden variables and learn the causal relationships between utterances and emotions. We optimize our model by maximizing the ELBO. Experimental results and analysis on two popular datasets demonstrate the high accuracy and robustness of our model. In future research, we intend to incorporate more precise representations of dialogue as observational variables to further enhance the analysis of dialogues, with the expectation of achieving even greater performance.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. The motivation diagram from the MELD dataset. Red-marked words are interfering words in the dialogue, whose meanings may mislead utterance emotion detection. Removing interference and using contextual information can better understand the speaker's psychology and emotions.</figDesc><graphic coords="1,311.98,175.62,257.03,178.15" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Directed acyclic graph for emotion detection. s, v, and z represent hidden variables associated with emotions and utterance composition, dialogue topics related to emotions, and variables unrelated to emotions, respectively. Personal attributes Pt influence the distribution of the hidden variables st, vt, and zt, which collectively form the utterances Ut. Following disentanglement, we reconstruct both the utterances Ut and the topics Ft for optimization purposes. At each time step, st and vt are employed to derive the emotion label Et.</figDesc><graphic coords="3,50.88,56.07,510.26,146.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig.3. The network structure of Dynamic Causal Disentanglement Model. In causal learning, we propose prior and posterior networks to learn the distribution of variables to achieve disentanglement. In the prior network, we input the concatenated vectors into the GRU and subsequently design two independent Fully Connected (FC) layers to obtain the mean and the log variance vectors. Within the posterior network, we input concatenated vectors into the FC layer, similar to the prior network, to obtain mean and log variance vectors. The hidden variable data obtained from the posterior network is then utilized for the reconstruction of observed variables and the prediction of labels.</figDesc><graphic coords="4,79.23,56.07,453.54,188.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. The process of extracting conversation topics utilizing ChatGPT-4.0.</figDesc><graphic coords="5,321.84,56.07,231.33,274.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Prediction accuracy of long dialogue utterances on the IEMOCAP and MELD datasets. The blue, green, and red lines represent BC-LSTM, DialogueRNN, and our model respectively.</figDesc><graphic coords="8,315.81,458.45,100.42,75.43" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. The feature mappings of high-dimensional vectors obtained by t-SNE. Dots of various colors represent utterances with distinct emotion labels.</figDesc><graphic coords="9,56.50,61.05,175.30,175.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Confusion matrix graph of emotion detection with different methods on the IEMOCAP and MELD datasets.</figDesc><graphic coords="10,56.50,253.48,175.30,175.30" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="11,100.37,56.07,411.26,312.66" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE II Comparison</head><label>II</label><figDesc>with state-of-the-art methods on the IEMOCAP dataset.</figDesc><table><row><cell>Methods</cell><cell cols="2">Happy</cell><cell>Sad</cell><cell></cell><cell cols="2">Neutral</cell><cell cols="2">Angry</cell><cell cols="2">Excited</cell><cell cols="2">Frustrated</cell><cell cols="2">Avg.</cell></row><row><cell></cell><cell>Acc.</cell><cell>F1</cell><cell>Acc.</cell><cell>F1</cell><cell>Acc.</cell><cell>F1</cell><cell>Acc.</cell><cell>F1</cell><cell>Acc.</cell><cell>F1</cell><cell>Acc.</cell><cell>F1</cell><cell>Acc.</cell><cell>F1</cell></row><row><cell>BC-LSTM [16]</cell><cell>18.8</cell><cell>28.7</cell><cell>67.8</cell><cell>71.2</cell><cell>60.4</cell><cell>54.0</cell><cell>59.4</cell><cell>61.4</cell><cell>57.9</cell><cell>63.0</cell><cell>68.2</cell><cell>60.9</cell><cell>59.1</cell><cell>58.4</cell></row><row><cell>CMN [39]</cell><cell>25.0</cell><cell>30.4</cell><cell>55.9</cell><cell>62.4</cell><cell>52.9</cell><cell>52.4</cell><cell>61.8</cell><cell>59.8</cell><cell>55.5</cell><cell>60.3</cell><cell>71.1</cell><cell>60.7</cell><cell>56.6</cell><cell>56.1</cell></row><row><cell>ICON [40]</cell><cell>22.2</cell><cell>29.9</cell><cell>58.8</cell><cell>64.6</cell><cell>62.8</cell><cell>57.4</cell><cell>64.7</cell><cell>63.0</cell><cell>58.9</cell><cell>63.4</cell><cell>67.2</cell><cell>60.8</cell><cell>59.1</cell><cell>58.5</cell></row><row><cell>KET [20]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>59.6</cell></row><row><cell>DialogueRNN [5]</cell><cell>25.7</cell><cell>33.2</cell><cell>75.1</cell><cell>78.8</cell><cell>58.6</cell><cell>59.2</cell><cell>64.7</cell><cell>65.3</cell><cell>80.3</cell><cell>71.9</cell><cell>61.2</cell><cell>58.9</cell><cell>63.4</cell><cell>62.8</cell></row><row><cell>DialogueGCN [42]</cell><cell>40.6</cell><cell>42.8</cell><cell>89.1</cell><cell>84.5</cell><cell>61.9</cell><cell>63.5</cell><cell>67.5</cell><cell>64.2</cell><cell>65.5</cell><cell>63.1</cell><cell>64.2</cell><cell>67.0</cell><cell>65.3</cell><cell>64.2</cell></row><row><cell>KES [43]</cell><cell>-</cell><cell>47.7</cell><cell>-</cell><cell>84.6</cell><cell>-</cell><cell>64.3</cell><cell>-</cell><cell>62.5</cell><cell>-</cell><cell>73.3</cell><cell>-</cell><cell>63.5</cell><cell>-</cell><cell>66.3</cell></row><row><cell>IEIN [49]</cell><cell>-</cell><cell>53.2</cell><cell>-</cell><cell>77.2</cell><cell>-</cell><cell>61.3</cell><cell>-</cell><cell>61.5</cell><cell>-</cell><cell>69.2</cell><cell>-</cell><cell>60.9</cell><cell>-</cell><cell>64.4</cell></row><row><cell>GGCN [48]</cell><cell>56.6</cell><cell>49.8</cell><cell>85.3</cell><cell>85.5</cell><cell>62.6</cell><cell>63.8</cell><cell>61.3</cell><cell>63.1</cell><cell>73.5</cell><cell>75.2</cell><cell>65.0</cell><cell>67.4</cell><cell>67.9</cell><cell>68.7</cell></row><row><cell>SS-HiT [45]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>61.0</cell></row><row><cell>Our</cell><cell>53.5</cell><cell>54.4</cell><cell>78.8</cell><cell>78.2</cell><cell>63.5</cell><cell>65.1</cell><cell>66.0</cell><cell>64.4</cell><cell>76.3</cell><cell>75.7</cell><cell>69.8</cell><cell>69.3</cell><cell>68.8</cell><cell>68.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>TABLE III Comparison</head><label>III</label><figDesc>with state-of-the-art methods on the MELD dataset.</figDesc><table><row><cell>Methods</cell><cell>Anger</cell><cell>Disgust</cell><cell>Fear</cell><cell>Joy</cell><cell>Neutral</cell><cell>Sadness</cell><cell>Surprise</cell><cell>Avg.</cell></row><row><cell>BC-LSTM [16]</cell><cell>40.5</cell><cell>0.0</cell><cell>0.0</cell><cell>52.9</cell><cell>76.0</cell><cell>24.2</cell><cell>47.6</cell><cell>57.1</cell></row><row><cell>ConGCN [41]</cell><cell>46.8</cell><cell>10.6</cell><cell>8.7</cell><cell>53.1</cell><cell>76.7</cell><cell>28.5</cell><cell>50.3</cell><cell>59.4</cell></row><row><cell>KET [20]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>58.2</cell></row><row><cell>DialogueRNN [5]</cell><cell>46.8</cell><cell>0.81</cell><cell>0.0</cell><cell>54.6</cell><cell>76.2</cell><cell>26.3</cell><cell>49.6</cell><cell>58.7</cell></row><row><cell>DialogueGCN [42]</cell><cell>43.0</cell><cell>1.2</cell><cell>1.0</cell><cell>53.6</cell><cell>76.0</cell><cell>24.3</cell><cell>46.4</cell><cell>57.5</cell></row><row><cell>KES [43]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>66.5</cell></row><row><cell>TL-ERC [44]</cell><cell>48.9</cell><cell>8.7</cell><cell>11.2</cell><cell>60.4</cell><cell>80.4</cell><cell>25.6</cell><cell>53.8</cell><cell>61.9</cell></row><row><cell>SS-HiT [45]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>59.3</cell></row><row><cell>Sentic GAT [46]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>59.2</cell></row><row><cell>EmpaGen [47]</cell><cell>50.2</cell><cell>10.9</cell><cell>9.5</cell><cell>61.8</cell><cell>79.2</cell><cell>27.6</cell><cell>55.6</cell><cell>62.4</cell></row><row><cell>IEIN [49]</cell><cell>48.9</cell><cell>19.4</cell><cell>3.3</cell><cell>56.6</cell><cell>77.5</cell><cell>23.6</cell><cell>53.7</cell><cell>60.7</cell></row><row><cell>GGCN [48]</cell><cell>49.8</cell><cell>15.5</cell><cell>14.3</cell><cell>63.1</cell><cell>82.9</cell><cell>41.2</cell><cell>66.3</cell><cell>67.3</cell></row><row><cell>SDTN [50]</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>66.1</cell></row><row><cell>Our</cell><cell>50.9</cell><cell>16.2</cell><cell>10.8</cell><cell>65.0</cell><cell>82.6</cell><cell>38.6</cell><cell>67.4</cell><cell>67.5</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>TABLE V</head><label>V</label><figDesc>Prediction results of different hidden variables.</figDesc><table><row><cell></cell><cell>Variables</cell><cell></cell><cell cols="2">IEMOCAP</cell><cell>MELD</cell></row><row><cell>s</cell><cell>v</cell><cell>z</cell><cell>Acc.</cell><cell>F1</cell><cell>F1</cell></row><row><cell></cell><cell></cell><cell></cell><cell>59.8</cell><cell>60.2</cell><cell>59.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell>66.1</cell><cell>66.4</cell><cell>66.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell>66.4</cell><cell>66.5</cell><cell>66.8</cell></row><row><cell></cell><cell></cell><cell></cell><cell>62.9</cell><cell>63.2</cell><cell>63.4</cell></row><row><cell></cell><cell></cell><cell></cell><cell>63.8</cell><cell>63.8</cell><cell>63.7</cell></row><row><cell></cell><cell></cell><cell></cell><cell>68.8</cell><cell>68.9</cell><cell>67.5</cell></row></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Time-frequency representation and convolutional neural network-based emotion recognition</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">K</forename><surname>Khare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Bajaj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="2901" to="2909" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Wavelet packet analysis for speaker-independent emotion recognition</title>
		<author>
			<persName><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">398</biblScope>
			<biblScope unit="page" from="257" to="264" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Real-time emotion recognition via attention gated hierarchical memory network</title>
		<author>
			<persName><forename type="first">W</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="8002" to="8009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Bidirectional lstm-crf models for sequence tagging</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.01991</idno>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Dialoguernn: An attentive rnn for emotion detection in conversations</title>
		<author>
			<persName><forename type="first">N</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gelbukh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="6818" to="6825" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Context-sensitive learning for enhanced audiovisual emotion classification</title>
		<author>
			<persName><forename type="first">A</forename><surname>Metallinou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wollmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Katsamanis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Eyben</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schuller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Affective Computing</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="184" to="198" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Uaed: Unsupervised abnormal emotion detection network based on wearable mobile device</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Network Science and Engineering</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Global-local-feature-fused driver speech emotion detection for intelligent cockpit in automated driving</title>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Vehicles</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A virtual emotion detection architecture with two-way enabled delay bound toward evolutional emotion-based iot services</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ben-Othman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mokdad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Bellavista</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Mobile Computing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1172" to="1181" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Tsception: a deep learning framework for emotion detection using eeg</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">A P</forename><surname>Wai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T.-S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 international joint conference on neural networks (IJCNN)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Semantic audiovisual data fusion for automatic emotion recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Datcu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Rothkrantz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="411" to="435" />
		</imprint>
	</monogr>
	<note>Emotion recognition: a pattern analysis approach</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Text emotion detection using machine learning algorithms</title>
		<author>
			<persName><forename type="first">V</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Shirode</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mirchandani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2023 8th International Conference on Communication and Electronics Systems (ICCES)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="1264" to="1268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep analysis for speech emotion recognization</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bhattacharya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Borah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">K</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Das</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 Second International Conference on Computer Science, Engineering and Applications (ICCSEA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Context-dependent sentiment analysis in user-generated videos</title>
		<author>
			<persName><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th annual meeting of the association for computational linguistics</title>
		<meeting>the 55th annual meeting of the association for computational linguistics</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="873" to="883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Exploring temporal representations by leveraging attention-based bidirectional lstm-rnns for multi-modal emotion recognition</title>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Processing &amp; Management</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">102185</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Ana at semeval-2019 task 3: Contextual emotion detection in conversations through hierarchical lstms and bert</title>
		<author>
			<persName><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Trabelsi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><forename type="middle">R</forename><surname>Zaïane</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1904.00132</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Cosmic: Commonsense knowledge for emotion identification in conversations</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ghosal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gelbukh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2010.02795</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Knowledge-enriched transformer for emotion detection in textual conversations</title>
		<author>
			<persName><forename type="first">P</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Miao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1909.10681</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Enhancing emotion inference in conversations with commonsense knowledge</title>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge-Based Systems</title>
		<imprint>
			<biblScope unit="volume">232</biblScope>
			<biblScope unit="page">107449</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Comet: Commonsense transformers for automatic knowledge graph construction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Rashkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Malaviya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Celikyilmaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.05317</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Relation-aware graph attention networks with relational position encodings for emotion recognition in conversations</title>
		<author>
			<persName><forename type="first">T</forename><surname>Ishiwatari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yasuda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Miyazaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Goto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7360" to="7370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">I-gcn: incremental graph convolution network for conversation emotion detection</title>
		<author>
			<persName><forename type="first">W</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="4471" to="4481" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Residual-based graph convolutional network for emotion recognition in conversation for smart internet of things</title>
		<author>
			<persName><forename type="first">Y.-J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y.-W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B.-G</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Big Data</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="279" to="288" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Weather forecasting using hidden markov model</title>
		<author>
			<persName><forename type="first">D</forename><surname>Khiatani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Ghose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 International Conference on Computing and Communication Technologies for Smart Nation (IC3TSN</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="220" to="225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Non-intrusive load monitoring using factorial hidden markov model based on gaussian mixture model</title>
		<author>
			<persName><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhaoxia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2020 IEEE Power &amp; Energy Society General Meeting (PESGM)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A predictive hidden semi-markov model for bridges subject to chloride-induced deterioration</title>
		<author>
			<persName><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE 21st International Conference on Software Quality, Reliability and Security Companion (QRS-C)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="751" to="756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Causal conditional hidden markov model for multimodal traffic prediction</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.08249</idno>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Causal hidden markov model for view independent multiple silhouettes posture recognition</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Mak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><forename type="middle">H</forename><surname>Tay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 11th International Conference on Hybrid Intelligent Systems (HIS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="78" to="84" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A full causal two dimensional hidden markov model for image segmentation</title>
		<author>
			<persName><forename type="first">A</forename><surname>Suphalakshmi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Narendran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Anandhakumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE International Advance Computing Conference</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="442" to="445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Causal hidden markov model for time series disease forecasting</title>
		<author>
			<persName><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="12" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Causality</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Cambridge university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Variational inference: A review for statisticians</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kucukelbir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Mcauliffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American statistical Association</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">518</biblScope>
			<biblScope unit="page" from="859" to="877" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Iemocap: Interactive emotional dyadic motion capture database</title>
		<author>
			<persName><forename type="first">C</forename><surname>Busso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Bulut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kazemzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Mower</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Narayanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Language resources and evaluation</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="335" to="359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.02508</idno>
		<title level="m">Meld: A multimodal multi-party dataset for emotion recognition in conversations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Emotionlines: An emotion corpus of multi-party conversations</title>
		<author>
			<persName><forename type="first">S.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C.-C</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-W</forename><surname>Ku</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.08379</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Conversational memory network for emotion recognition in dyadic dialogue videos</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L.-P</forename><surname>Morency</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zimmermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the conference</title>
		<meeting>the conference</meeting>
		<imprint>
			<publisher>NIH Public Access</publisher>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page">2122</biblScope>
		</imprint>
	</monogr>
	<note>Meeting</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Icon: Interactive conversational memory network for multimodal emotion detection</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zimmermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 conference on empirical methods in natural language processing</title>
		<meeting>the 2018 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2594" to="2604" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Modeling both context-and speaker-sensitive dependence for emotion detection in multi-speaker conversations</title>
		<author>
			<persName><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<biblScope unit="page" from="5415" to="5421" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Dialoguegcn: A graph convolutional neural network for emotion recognition in conversation</title>
		<author>
			<persName><forename type="first">D</forename><surname>Ghosal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Majumder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Chhaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Gelbukh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.11540</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Utilizing external knowledge to enhance semantics in emotion detection in conversation</title>
		<author>
			<persName><forename type="first">F</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>She</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Access</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="154" to="947" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Conversational transfer learning for emotion recognition</title>
		<author>
			<persName><forename type="first">D</forename><surname>Hazarika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zimmermann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Fusion</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Sentiment and semantic hierarchical transformer for utterance-level emotion recognition</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 International Joint Conference on Information and Communication Engineering (JCICE)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="130" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Context-and sentiment-aware networks for emotion recognition in conversation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Cambria</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="699" to="708" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A multiturn complementary generative framework for conversational emotion recognition</title>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Intelligent Systems</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="5643" to="5671" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Long dialogue emotion detection based on commonsense knowledge graph guidance</title>
		<author>
			<persName><forename type="first">W</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">An iterative emotion interaction network for emotion recognition in conversations</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Qin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th international conference on computational linguistics</title>
		<meeting>the 28th international conference on computational linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4078" to="4088" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Sdtn: Speaker dynamics tracking network for emotion recognition in conversation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="1" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Bert: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-W</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.04805</idno>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
