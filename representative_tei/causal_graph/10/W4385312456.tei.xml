<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Optimization and Interpretability of Graph Attention Networks for Small Sparse Graph Structures in Automotive Applications</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2023-05-25">25 May 2023</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Marion</forename><surname>Neumeier</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">CARISSMA Institute of Automated Driving</orgName>
								<orgName type="institution" key="instit2">Technische Hochschule Ingolstadt</orgName>
								<address>
									<postCode>85049</postCode>
									<settlement>Ingolstadt</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Andreas</forename><surname>Tollkühn</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">AUDI AG</orgName>
								<address>
									<postCode>85057</postCode>
									<settlement>Ingolstadt</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Sebastian</forename><surname>Dorn</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">AUDI AG</orgName>
								<address>
									<postCode>85057</postCode>
									<settlement>Ingolstadt</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Michael</forename><surname>Botsch</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">CARISSMA Institute of Automated Driving</orgName>
								<orgName type="institution" key="instit2">Technische Hochschule Ingolstadt</orgName>
								<address>
									<postCode>85049</postCode>
									<settlement>Ingolstadt</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Wolfgang</forename><surname>Utschick</surname></persName>
							<email>utschick@tum.de</email>
							<affiliation key="aff2">
								<orgName type="institution">Technical University of Munich</orgName>
								<address>
									<postCode>80333</postCode>
									<settlement>Munich</settlement>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Optimization and Interpretability of Graph Attention Networks for Small Sparse Graph Structures in Automotive Applications</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2023-05-25">25 May 2023</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2305.16196v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>For automotive applications, the Graph Attention Network (GAT) is a prominently used architecture to include relational information of a traffic scenario during feature embedding. As shown in this work, however, one of the most popular GAT realizations, namely GATv2, has potential pitfalls that hinder an optimal parameter learning. Especially for small and sparse graph structures a proper optimization is problematic. To surpass limitations, this work proposes architectural modifications of GATv2. In controlled experiments, it is shown that the proposed model adaptions improve prediction performance in a node-level regression task and make it more robust to parameter initialization. This work aims for a better understanding of the attention mechanism and analyzes its interpretability of identifying causal importance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>In recent years, Graph Neural Networks (GNNs) have gained increasing popularity. GNNs are a class of deep learning architectures that leverage the representation of graph-structured data by incorporating the structure into the embedding. An eminent GNN realization is the Graph Attention Network (GAT) <ref type="bibr" target="#b27">[28]</ref>, which performs attentional message-passing. Similar to the attention mechanism for sequential data, GATs update node features of a graph by weighting the features of its neighboring nodes using assigned attention scores. If designed and trained properly, the attention mechanism can improve a network's robustness by attending to only important nodes in graph-structured data <ref type="bibr" target="#b13">[14]</ref>. Due to the inherent selection characteristic of the attention mechanism, several works claim that high attention scores indicate highly relevant information and conclude a certain interpretability <ref type="bibr" target="#b27">[28]</ref>  <ref type="bibr" target="#b21">[22]</ref> [6] <ref type="bibr" target="#b23">[24]</ref>. However, recent works on sequential data using the attention mechanism have questioned this type of interpretability <ref type="bibr" target="#b10">[11]</ref>  <ref type="bibr" target="#b24">[25]</ref>. Results suggest that the input-depended computation of attention matrices is less important than typically thought. While attention mechanisms have shown to be very powerful architectural components, there is dissent about how much they actually learn to attend and select relevant information. In general, the interpretability of data-driven architectures is a significant research topic <ref type="bibr" target="#b19">[20]</ref>.</p><p>While most state-of-the-art prediction methods in the automotive field are based on conventional machine learning approaches, graph-based approaches have become very popular <ref type="bibr" target="#b20">[21]</ref>  <ref type="bibr" target="#b12">[13]</ref> [15] <ref type="bibr" target="#b18">[19]</ref>. Representing a traffic scenario as graph-structured data allows to include relational information, which GNN architectures are able to inherently incorporate into the embedding. Particularly, the ability of GATs to weigh different relations context-adaptively seems to be very beneficial and could potentially identify relevant inter-dependencies <ref type="bibr" target="#b25">[26]</ref>. Beside the very prominent example of graph-based attentional aggregation in VectorNet <ref type="bibr" target="#b8">[9]</ref>, various works use GATs to model high-level interactions of agent trajectories and/or map features <ref type="bibr" target="#b17">[18]</ref> [15] <ref type="bibr" target="#b11">[12]</ref>. In comparison to other application fields like, e. g., brain analysis, the graph structures in automotive applications are rather small and sparse. As shown in this work, however, especially small and sparse graph architectures are likely to hinder appropriate parameter optimization in the GAT such that even simple graph problems cannot be solved reliably. To be aware of these constrains in GATs and know how to mitigate these effects is therefore of great importance. With focus on the application in the automotive domain, this work identifies key weaknesses of one of the most relevant GAT realizations, namely GATv2 <ref type="bibr" target="#b25">[26]</ref>, and proposes architectural changes to surpass limitations. Experiments show, that the model adaptions improve performance and reinforce robustness to parameter initialization. Additionally, the attention assignment of the GAT is investigated with regard to (w. r .t.) its interpretability as relevance indication. In order to evaluate if the attention mechanism learns to generalize relevant dependencies within a graph structure, ground truth data of the attention scores are required. In real-world datasets, however, this information is hardly accessible. Such annotations are extremely hard to define and, if possible, require expensive labeling procedures. For this work, therefore, a synthetic graph dataset is generated. The simulated dataset allows to evaluate the attention mechanism in a controlled environment.</p><p>Contribution. This work contributes towards a better understanding of graph-based attentional information aggregation. The main contributions are as follows:</p><p>• Identification of potential pitfalls of GATv2 <ref type="bibr" target="#b25">[26]</ref> with focus on automotive applications. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. RELATED WORK</head><p>This section reviews related work w. r. t. the expressive power of GNNs and, in particular, GATs, as well as the understanding and interpretability of attention mechanisms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Expressive power of GNNs</head><p>Over the past years, several works have addressed the discriminative power of many common and popular GNN frameworks <ref type="bibr" target="#b11">[12]</ref> [3] [14] <ref type="bibr" target="#b17">[18]</ref>. It is well known that GNNs generally are prone to oversmoothing, which gradually decreases their performance with increasing number of layers <ref type="bibr" target="#b3">[4]</ref>. For this reason, most GNN implementations are kept shallow in practice. Subsequently, standalone layers should be maximally powerful and provide beneficial filter characteristics. However, Balcilar et al. <ref type="bibr" target="#b0">[1]</ref> showed that most GNNs operate as a low-pass filter purely and only few networks allow bandpass-filtering. In order to leverage the bandpass-filtering capacity, numerous computational layers are required. Yet, this results in a trade-off between avoiding oversmooting issues and computational power. GATs have shown to be able to alleviate oversmoothing through the attentive message passing operations <ref type="bibr" target="#b16">[17]</ref>. If the attention mechanism generalizes to operate selectively instead of solely averaging neighboring nodes, oversmoothing can be prevented. While there are several approaches to compute attention scores in graphs, the mechanism GATv2 presented by Brody et al. <ref type="bibr" target="#b25">[26]</ref> is strictly more expressive than others <ref type="bibr" target="#b27">[28]</ref>  <ref type="bibr" target="#b26">[27]</ref>. Furthermore, it has been shown that the popular GAT architecture proposed in <ref type="bibr" target="#b27">[28]</ref> only performs a limited kind of attention, where the ranking of the attention scores is unconditional on the query node.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Interpretability of the attention mechanism</head><p>Similar to the attention mechanism for sequential data <ref type="bibr" target="#b15">[16]</ref> [27], graph attention is supposed to capture important relations in graphs. Meaning, the attention score assigned to a neighboring node in a graph indicates the degree of its importance w. r. t. the reference node. In the most common approaches, every node attends to its neighbors by considering its own representation as a query. Although various works have leveraged performance by using the GAT, the degree of performance improvements are inconsistent across datasets and it's still not fully understood what the graph attention mechanism learns <ref type="bibr" target="#b6">[7]</ref>. Recently, there has been an increasing amount of research addressing the expressive power of the graph attention mechanism <ref type="bibr" target="#b25">[26]</ref> [1] and the generation of interpretable explanations for predictions <ref type="bibr" target="#b28">[29]</ref>. Knyazev et al. <ref type="bibr" target="#b13">[14]</ref> showed for special graph-pooling based attention algorithms <ref type="bibr" target="#b29">[30]</ref> [8] that under typical conditions the effect of the attention mechanism is negligible and that performance is very sensitive to initialization of the attention model. By using simulated data, the authors analyzed performance and the assigned attention scores for solving different graph classification tasks. In contrast to the analyzed approaches of <ref type="bibr" target="#b13">[14]</ref>, currently popular GAT realizations use the softmax function to compute attention scores. The softmax is an approximation to the arg max function. The advantage of using softmax is that the function is continuous and differentiable, which allows a more stable training characteristic <ref type="bibr" target="#b9">[10]</ref>. A similar result on the relevance of input-specific attention using sequential data was proposed in <ref type="bibr" target="#b10">[11]</ref>. In their work, the authors analyzed the importance of the attention assignment in pre-trained language models and showed that the attention mechanism is not as important as typically thought. By replacing the inputdependent attention mechanism with a constant attention matrix, all models still achieved competitive performance. Since attention mechanisms weigh the representation of different inputs, it is often assumed that attention scores can be used for interpreting a model and identify causal relations. Serrano et al. <ref type="bibr" target="#b24">[25]</ref>, however, showed that the scores do not necessarily indicate importance. Contrarily to these findings, Renz et al. <ref type="bibr" target="#b23">[24]</ref> proposed a transformer-based approach for path planning, where the most relevant objects for the decision are identified by using the assigned attention scores on the object level. Their results show, that their approach can focus on the most relevant object in the scene, even when it is not geometrically close. Hence, there is still a big dissent about the interpretability of assigned attention scores.</p><p>In this work, the interpretation of attention scores of GATs as relevance indicator is investigated. Knowledge about which relations mainly influenced a networks prediction provides valuable insights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. PRELIMINARIES</head><p>Graph definition: Let G = (V, E) be a graph composed of nodes V = {1, . . . , n} and edges E ⊆ V × V. An edge from a node j to a node i is represented by (i, j) ∈ E. If all edges are bidirectional the graph is denoted as undirected; and directed if otherwise. The graph G can be represented through its adjacency matrix A = {0, 1} |V|×|V| . If the graph is weighted, an additional weight matrix W ∈ R |V|×|V| , indicating the weighting of each edge, can be defined.</p><p>Graph Neural Networks: A GNN is based on the concept of message-passing. Each node i ∈ V of the graph structure is assigned an initial feature h (0) i ∈ R d . The network updates the node features through successively applying an AGGREGATE and UPDATE step:</p><formula xml:id="formula_0">c (k) i = AGGREGATE (k) {h (k-1) j : j ∈ N i }<label>(1)</label></formula><formula xml:id="formula_1">h (k) i = UPDATE (k) h (k-1) i , c (k) i .<label>(2)</label></formula><p>In the aggregation step of node i the network aggregates the features of its neighboring nodes j ∈ N i into the latent variable c</p><p>(k)</p><p>i . In the update step the information of the neighboring nodes c (k) i is combined with the current features of node i to update its features. One repetition of these two steps can be regarded as one layer k = {1, . . . , K}. Different types of GNNs differ in the definition of the aggregation and update function.</p><p>Graph Attention Networks: In GATs, the features of the neighboring nodes are aggregated by computing attention scores α ij for every edge (i, j). As shown in Fig. <ref type="figure" target="#fig_0">1</ref>, the idea is to perform a weighted sum over the neighboring nodes j ∈ N i , where the attention score α ij indicates the importance of the features of neighbor j to the node i. Note, that if node i has a self-connection, it formally is considered as a neighboring node to itself.</p><p>There are different ways to compute attention scores. A very popular variant is the GATv2 <ref type="bibr" target="#b25">[26]</ref>, where the network updates the features of node i as shown in Eq. 3-5. The equations correspond to the default implementation of GATv2 <ref type="bibr" target="#b25">[26]</ref> in the PyTorch Geometric framework <ref type="bibr" target="#b22">[23]</ref>.</p><formula xml:id="formula_2">e( hi , hj ) = a T LeakyReLU Θ R hi + Θ L hj<label>(3)</label></formula><formula xml:id="formula_3">α ij = softmax j (e( hi , hj ))<label>(4)</label></formula><formula xml:id="formula_4">h ′ i = b + j∈Ni α ij Θ L hj<label>(5)</label></formula><p>The attention scores are determined by the scoring function (Eq. 3), where d+1) for p ∈ {R, L} are learned and hq = [1, h T q ] T for q ∈ {i, j} are node representations. In Eq. 4, the resulting values of the scoring function e( hi , hj ) are normalized using softmax such that j α ij = 1. The normalized attention scores are used to update the feature representation by computing a weighted sum as described in Eq. 5, where b ∈ R d ′ is a learnable parameter. Since this work focuses on one-layered realizations, layer indexing is neglected and the updated node representation is denoted as h ′ i . However, if several layers are used, the updated feature representation h ′ i is usually passed through an additional activation function <ref type="bibr" target="#b25">[26]</ref>. Initially, each node representation is parameterized based on the corresponding data features h q = x q .</p><formula xml:id="formula_5">a ∈ R d ′ , Θ p ∈ R d ′ ×(</formula><p>In comparison to the originally proposed GAT <ref type="bibr" target="#b27">[28]</ref>, GATv2 <ref type="bibr" target="#b25">[26]</ref> allows dynamic attention assignment. The static form of GAT is only able to perform a limited kind of attention, where the ranking of the attention scores is unconditional on the query node <ref type="bibr" target="#b25">[26]</ref>. The attention mechanism introduced by <ref type="bibr" target="#b26">[27]</ref> has been shown to be strictly weaker than GATv2 <ref type="bibr" target="#b25">[26]</ref>. Therefore, the focus of this paper lies in the analyzation of GATv2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. PITFALLS OF GATV2 AND ARCHITECTURAL OPTIMIZATION</head><p>In the following, potential pitfalls of GATv2 are identified and resulting limitations are elaborated. Additionally, architectural optimizations to surpass the shortcomings are proposed. The following subsections identify three key factors that limit the expressive power and interpretability of GATv2. While subsections A and B are mainly relevant for automotive applications, subsection C reveals a general pitfall.</p><p>For a better understanding, the aspects are supported by an automotive example: Let G be an unweighted graph, where nodes represent participants of a traffic scenario and edges indicate inter-dependencies. In particular, vehicle (node) i and the nearby traffic participants j ∈ N i are involved in an intersection scenario. Each node feature holds the position and velocity of the respective participant. Graph G contains self-connections, such that features of the query node i are considered in the update function Eq. 5. On basis of the scenario context G, GATv2 is used to extract relevant scenario information and enrich graph node representations for downstream tasks like future motion prediction. Due to the attention mechanism, GATv2 dynamically assigns weights to each relation. For an interpretable feature embedding, relations to especially relevant neighboring traffic participants should be assigned high attention scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Attention assignment for query node</head><p>In certain scenarios, the future motion of a vehicle (query node) i might depend equally on its own motion and one particular traffic participant (node) j. Under the assumption that attention scores indicate relevance, in such scenarios, identical attention scores should be assigned to the corresponding query node i and neighboring node j. While equal attention scores α ii = α ij obviously result if h i = h j , this is a very particular case. In the introduced graph setup G, for example, two participants cannot have the exactly same features, as this would mean they are at the same position. Nevertheless, since the attention scores are computed based on non-injective operations, the same attention scores α ii = α ij for node i and j can theoretically be induced even if h i ̸ = h j . In practice, however, this might be challenging to learn and require additional model complexity. In applications where high relevance for the query node i is apparent, including it into the attention sharing introduces avertible complexity. By assigning a fixed attention score, e. g. α ii = 1, to the query node and performing the attention assignment exclusively based on the neighboring nodes j ∈ N i \ i, the optimization constraints are relaxed and, hence, learning can be eased.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Subtraction not possible</head><p>The update function Eq. 5 restricts the updated feature representation h ′ i to be a weighted sum of all neighboring nodes. Since all neighboring nodes j ∈ N i , potentially including node i, are transformed by the same matrix Θ L , no feature subtraction is possible. In the automotive domain, however, important information often arises from feature differences, e. g., distance between traffic participants or relative velocities. The weighted sum of the participant's position and/or velocity is rather inconclusive. Hence, different transformations for the features of query node i and the neighboring nodes j in the update function are reasonable. By taking this and the subsection A into account, the update function Eq. 5 of GATv2 is adapted to</p><formula xml:id="formula_6">h ′ i = b + Θ n hi + j∈Ni,i̸ =j α ij Θ L hj ,<label>(6)</label></formula><p>where</p><formula xml:id="formula_7">Θ n ∈ R d ′ ×(d+1)</formula><p>is a learnable parameter. The introduced parameter Θ n enables to compute subtractions of the neighboring node's features and the query node i.</p><p>As suggested in subsection A, the attention value defaults to α ii = 1. In the following, GAT variants that update the node features using Eq. 6 are referred to as GAT-Θ n .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Gradient for Θ R sensitive to initialization</head><p>As shown in this subsection, the performance of GATv2 is highly sensitive to network initialization. Due to the model definition of GATv2, the gradient for the parameter Θ R is prone to become zero; especially in sparse and simple graph structures. A zero gradient effects that the values of Θ R are not adopted during the optimization step. While this characteristic is desired if the parameter is close to an optimal parameterization, a zero gradient due to initialization and/or model design is unintended. As a consequence of GATv2 architecture proposed in <ref type="bibr" target="#b25">[26]</ref> (as well as <ref type="bibr" target="#b27">[28]</ref>), the parameter Θ R can potentially stay unchanged during a complete training or its optimization can stagnate early on in the training process although parameterization is not optimal. If the parameter Θ R is not learned properly, the underlying attention mechanism assigns attention scores unconditioned on the query node i and expressiveness is limited.</p><p>Note, that the gradient always depends on the loss function L. The following elaboration holds for the common case, that the loss is defined as the final prediction error and no intermediate loss is introduced.</p><p>To prove the proposition that the learning process is sensitive to initialization, the gradient B Θ R for the parameter Θ R = [Θ R b , Θ Rw ], composed of the weight Θ Rw and bias Θ R b , is computed. Based on the chain rule of calculus, the gradients w. r. t. the model's parameters can be determined. * In the one-layered GATv2, the gradient B Θ R w. r. t. the node i of feature dimension</p><formula xml:id="formula_8">d = 1, such that Θ R ∈ R d ′ ×2 , is defined as B Θ t Rw,i = ∂L ∂h ′ i •   h i a t Si j,k α ij α ik (A j -A k )(s t ij -s t ik )  <label>(7)</label></formula><p>and</p><formula xml:id="formula_9">B Θ t R b ,i = ∂L ∂h ′ i •   a t Si j,k α ij α ik (A j -A k )(s t ij -s t ik )   ,<label>(8)</label></formula><p>where superscript t = {1, . . . , d ′ } indicates the selected component of a vector representation, A q = t (Θ L hq ) t for q ∈ {j, k} and S i = [N i ] 2 is the set of all the subsets of N i with exactly two elements and no pairing repetitions. The parameter s ij is the derivative of the LeakyRELU activation.</p><p>In GATv2 the derivative of the LeakyRELU is</p><formula xml:id="formula_10">∂LeakyRELU(x ij ) ∂x ij = s t ij = 1, if x t ij &gt; 0 s t ij = s n , if x t ij &lt; 0 ,<label>(9)</label></formula><p>where x t ij = (Θ R hi + Θ L hj ) t and s n is the slope of the LeakyRELU for negative values of x t ij . While there are various parameterizations such that B Θ t Rw,i = B Θ t R b ,i = 0, the gradient B Θ t R is especially likely to become zero due to the term (s t ij -s t ik ). For the gradient ∀i ∈ V :</p><formula xml:id="formula_11">B Θ t R,i it holds that if sign(x t ij ) = sign(x t ik ) ∀{j, k} ∈ S i (10) =⇒ B Θ t R,i = 0.<label>(11)</label></formula><p>If the intermediate representations x t ij and x t ik for all pairings {j, k} ∈ S i have the same sign, the component t of the gradient B Θ R,i becomes zero. The derivatives Eq. 7 and Eq. 8 show, that this characteristic is not limited to certain graph or experimental setups, but is a general pitfall of GATv2. The occurrence of this condition strongly depends on the data and the parameter initialization. Due to the concept of shared weights, the overall gradient for updating</p><formula xml:id="formula_12">Θ R is B Θ R = |V| i B Θ R,i .<label>(12)</label></formula><p>Hence, the gradient B Θ R is generally less likely to become zero in dense graphs or graphs that consist of many nodes. In fact, however, most of real-world data structures are sparse <ref type="bibr">[5] [2]</ref>. Also in automotive applications, where often star graphs are used to represent traffic scenarios, graphs are usually sparse and small. If and only if ∃ {j, k} ∈ S i ∋ sign(x t ij ) ̸ = sign(x t ik ) the gradient B Θ t R,i ̸ = 0 is considered in optimizing the network parameters. If the majority of the pairings (or none in the worst-case) don't meet this requirement, the training process might fail to optimize Θ R . Ultimately the attention score assignment becomes unconditioned on the query node i. This leads to the assumption that only experiments, which are based on an attention score assignment independent of the query node i can be resolved reliably. In the following, two possible solutions to bypass this problem are elaborated. One conceivable activation function, which partly satisfies both requirements is the softplus activation function softplus(x) = ln (1 + e x ) .</p><p>(13)</p><p>In the following, if the LeakyRELU in the aggregation function Eq. 3 is replaced by the softplus, the GAT realization is denoted with the superscript + , e. g., GAT-Θ + n . 2) Adapt update function: Instead of introducing a new parameter Θ n in the update function as done in GAT-Θ n , Θ R can be used to transform the query node such that</p><formula xml:id="formula_13">h ′ i = b + Θ R hi + j∈Ni,i̸ =j α ij Θ j hj . (<label>14</label></formula><formula xml:id="formula_14">)</formula><p>By doing this, the gradient B Θ R gets an additional gradient component which arises from</p><formula xml:id="formula_15">∂(Θ R hi) ∂Θ R</formula><p>in the update function and is only zero if hi = 0. In fact, using parameter Θ R in the update function is reasonable since this maps the node features into the same representation space that the attention score assignment is based on. GAT realizations that update the node features using Eq. 14 are referred to as GAT-Θ R .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. DATASET AND EXPERIMENTS</head><p>In order to evaluate the expressive power and interpretability of attention scores, a synthetically generated dataset D is introduced. Real world datasets hardly provide ground truth attention scores which are required to assess the attention mechanism. The controlled environment of the synthetic dataset, however, enables access to accurate ground truth attention values. On basis of the dataset D, node-level regression tasks should be solved using GATv2. Throughout the experiments, the graph architecture G = (V, E) is not varied. The graph is specified as star graph, such that each node has a bidirectional connection to one central node but no connection to any other node. Additionally, the central node is provided a self-connection. In Fig. <ref type="figure" target="#fig_0">1</ref>, the experimental star graph architecture is illustrated.</p><p>The m-th sample of the dataset is defined as</p><formula xml:id="formula_16">D m := (X m , Y m , A m ), where X m = {x m 1 , . . . , x m |V| } T contains the input features x i ∈ R d for each node i ∈ V and A m ∈ {1, 0} |V|×|V| the true attention scores. Y m = {y m 1 , .</formula><p>. . , y m |V| } T contains the ground truth for the node-level regression task, where y m i ∈ R d is the ground truth for node i. As formulated in Eq. 15 and Eq. 16, y m i is the feature difference of the query node i and its most relevant neighboring node j. The relevance of the node pairings are simulated by the relevance function r(x i , x j ).</p><formula xml:id="formula_17">x m r = arg max j∈Ni r(x m i , x m j )<label>(15)</label></formula><formula xml:id="formula_18">y m i = x m r -x m i .<label>(16)</label></formula><p>In the individual experiments, the complexity of the simulated relevance function r(x i , x j ) is varied. As indicated in Eq. 15, all experiments are defined such that only one node out of all neighboring nodes is to be selected for solving the regression task correctly. Hence, the ground truth attention for node i is an one-hot encoded vector</p><formula xml:id="formula_19">α m i := col i A m with x 0 x 1 x 2 α 00 α 01 α 02        h ′ 0 0 h ′ 1 0 . . . h ′ d ′ -1 0        . . . ŷ f ϕ (h ′ 0 )</formula><p>Fig. <ref type="figure">2</ref>: Exemplary graph setup during the experiment with N 0 = {0, 1, 2} and the additional feed-forward layer f ϕ which is required only if</p><formula xml:id="formula_20">d ′ ̸ = d.</formula><p>1 at the index of the relevant node and 0 everywhere else. The simulation is designed such that two nodes are never equally relevant. The basis of the simulated relevance assignment is the sinus function, where</p><formula xml:id="formula_21">r(x m i , x m j ) = sin(x m j -x m i ). (<label>17</label></formula><formula xml:id="formula_22">)</formula><p>For each data sample m, the values ∀i ∈ V : x i are randomly sampled from the value range X . Through adaption of the value range X different concepts of relevance are introduced (c. f. Fig. <ref type="figure" target="#fig_2">3</ref>). The experiments of this work, evaluate the performance in the node-level regression task based on two different value ranges X , i. e., complexities of relevance functions. For better traceability, the experiments are intentionally kept simple and the input feature dimension is defined as scalar d = 1 such that x m i , y m i ∈ R. For the central node, which is designated as i = 0, it holds that x m j -x m 0 &gt;= 0. Due to the star architecture of the graph only the central node has multiple neighbors. All other nodes only have the central node as neighbor and would inherently always select the correct node. Without loss of generality, training and evaluation is therefore only based on the central node. Since GATs are based on the concept of shared weights, the learned logic is scalable to more complex graph structures. An one-layered GAT g Θ (X m ) is intended to adapt its model weights Θ during the training process to approximate the correct mapping of Eq. 15-16. Each model is given enough degrees of freedom to theoretically be able to solve the task. However, an one-layered GAT (Eq. 3-5) inevitably faces a dimension misfit if the dimension of the regression task ground truth d = 1 is unequal to the latent dimension d ′ required to solve the task. To bypass this problem, an additional learnable feed-forward layer</p><formula xml:id="formula_23">f ϕ : R d ′ → R d is added to the node-level output of the GAT if d ′ ̸ = d.</formula><p>As indicated in Fig. <ref type="figure">2</ref>, the feed-forward layer f ϕ maps the updated feature of node i to the regression task dimension. Since the feed-forward layer f ϕ gets only the updated node feature h ′ i , the correct node selection and feature aggregation has to be learned in the GAT model.</p><p>Experiment I: Strictly Monotonic Relevance Function By restricting the value range to X I ∈ [0; 0.5π], the resulting relevance function is strictly monotonic, such that a &lt; b → r(a) &lt; r(b). In Fig. <ref type="figure" target="#fig_2">3</ref> (a) the relevance function over the defined value range is plotted. In this experiment the required latent dimension to solve the selection task is d ′ = 1. Therefore, no additional learnable layer is required. The prediction ŷm i is the updated feature of node i:</p><formula xml:id="formula_24">ŷm i = g Θ (X m ) i .<label>(18)</label></formula><p>In the automotive application example, this would equal the task of selecting the most relevant neighboring vehicle j for vehicle i based on their relative velocity: The higher the relative velocity, the higher the importance assigned to node j by the simulation. The regression task is to output their relative velocity in the updated node feature h ′ i := g Θ (X m ) i . Experiment II: Parabolic Relevance Function By setting the value range to X II ∈ [0; π] a parabolic relevance function is achieved, which is slightly more complex than the strictly monotonic function. Fig. <ref type="figure" target="#fig_2">3 (b)</ref> shows the resulting relevance function over the defined value range. For the GAT to be able to approximate the relevance function, the latent dimension has to be set to d ′ = 2. Since this definition causes a dimension misfit in the regression task, an additional learnable layer f ϕ is introduced for this experiment such that</p><formula xml:id="formula_25">ŷm i = f ϕ (g Θ (X m ) i ) . (<label>19</label></formula><formula xml:id="formula_26">)</formula><p>A LeakyRELU is applied at the beginning of layer ϕ In the automotive application example, this would equal the task of selecting the closest neighboring vehicle j ∈ N i , which the vehicle i can react on, and compute their distance.</p><p>In the experiments, graph G consists of |V| = 3 nodes in total, with node i = 0 being the central node and j ∈ N 0 = {0, 1, 2} its neighboring nodes. For training as well as for testing M = 20 000 data samples are generated. Each model implementation is trained with unsupervised attention only, using the loss function</p><formula xml:id="formula_27">L(ŷ m , y m ) = ŷm i -y m i .<label>(20)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. EVALUATION</head><p>The dataset D is used for an empirical study of the performance and attention interpretability of one-layered, single-headed GAT realizations. In addition to GATv2, model variants based on the proposed architectural adaptions, namely GAT-Θ R , GAT-Θ n , GAT-Θ + R and GAT-Θ + n , are evaluated. Results of the different experiments are shown in Table <ref type="table">I</ref>. The True Positive Rate (TPR) where TP is the number of true positives and FP of false negatives, indicates the percentage of test samples in which a node is correctly identified to be most relevant. If the highest attention score is assigned to the correct node it's considered a TP. The TPR is an unbiased estimator of the node selection accuracy. Performance of the node-level regression task is evaluated trough the mean prediction error</p><formula xml:id="formula_28">TPR = TP TP + FN ,<label>(21)</label></formula><formula xml:id="formula_29">ME = 1 M M m=1 (ŷ m i -y m i ),<label>(22)</label></formula><p>the error variance σ 2 as well as the mean maximal error.</p><p>In experiment I, all GAT realizations perfectly manage to learn selecting the relevant neighboring node from all neighboring nodes j ∈ N i with TPR = 1.0. Due to the strictly monotonic nature of the relevance function, the attention assignment is substantially independent on the query node i. It holds that arg max j r(h i , h j ) = arg max j r(h j ) and subsequently, for selecting the relevant neighboring node the GAT parameter Θ R doesn't have to be optimized. Under those circumstances the proneness of its gradient B Θ R to become zero does not negatively impact the performance. However, as the confidence histograms in Fig. <ref type="figure" target="#fig_3">4</ref>  less selective than the models with proposed architectural adaptions. Results of the confidence histogram are used to evaluate if the attention values can be interpreted as a measure of node relevance. A perfect selection model would assign the value of α ij = 1.0 to the most relevant node and zero to all others. The histogram of GATv2, however, suggests that GATv2 is non-selective and generally assigns comparatively low attention scores to the crucial node. Even though all other models assign an attention score between 0.9 -1.0 to the most relevant node in more than 80 % of the samples, low attention scores still occur. It therefore has to be concluded that the highest attention values can suggest the most relevant node but assigned attention values are not equivalent to the corresponding degree of node relevance. Since GATv2 is unable to compute subtractions in the update function, the true mapping cannot be approximated properly. As a consequence GATv2, has a higher mean and maximal error in oppose to all other models. Note, that the limited expressivity of GATv2 update function enhances its non-selective characteristic.</p><p>In experiment II, the relevance function is non-monotonic and, hence, query node information is crucial. Although the graph problem is only slightly more challenging, GATv2 performance drastically decreases. Only in about 50 % of the test samples the relevant node is identified correctly and regression task accuracy is significantly inferior. Due to a lack of gradient, the parameter Θ R isn't learned properly. Consequently, the attention assignment is optimized unconditioned on the query node parameter Θ R . The hypothesis is supported by the fact, that by only introducing a new parameter in the update function as in GAT-Θ n , the TPR does not improve. While the extra parameter helps to improve regression task performance, it does not resolve the gradient issue of parameter Θ R . The gradient B Θ R is still prone to become zero. By additionally replacing the LeakyRELU as in GAT-Θ + n , however, performance and TPR substantially improve. Also the adaptions made in GAT-Θ R , GAT-Θ + R benefit the selection accuracy as well as regression task performance. Yet, GAT-Θ + n outperforms all other GAT variants. When analyzing the confidence histogram in Fig. <ref type="figure" target="#fig_3">4</ref> it can be seen that in experiment II the selective characteristics decreases for all models. This supports the presumption, that while the assigned attention score can be a useful measure of a node's significance, it may not always directly correlate with the node's overall importance within the graph context. To show that the replacement of the LeakyRELU with the softplus makes the realizations strictly more robust to parameter initialization, the models are trained based on E = 100 random parameter initializations. In Fig. <ref type="figure" target="#fig_4">5</ref>, the ME and TPR of GATv2, GAT-Θ n and GAT-Θ + n are evaluated for each initialization e ∈ {0, . . . , E -1}. The figure indicates, that only fortunate initializations enable the GAT-Θ n to be trained successfully, resulting in a good prediction performance, whereas the performance of GAT-Θ + n is more robust. To avoid an overloaded visualization in Fig. <ref type="figure" target="#fig_4">5</ref>, the models GAT-Θ R and GAT-Θ + R are not considered. The boxplots in Fig. <ref type="figure">6</ref>, however, depicts the performance including spread and skewness for all GAT realizations. Both plots indicate, that GAT-Θ + n generally outperforms the other models in the regression task and is more accurate in selecting the relevant node. GATv2 is the least performant model. As can be seen in Fig. <ref type="figure">6</ref>, the variations GAT-Θ + n and GAT-Θ + R , using the softplus activation, outperform their respective variants GAT-Θ n and GAT-Θ R . Furthermore, the comparatively high performance variances of GAT-Θ n , GAT-Θ R indicate an increased sensitivity to parameter initialization. Since GAT-Θ n/R and GAT-Θ + n/R only differ in their choice of activation function, it becomes clear that the LeakyRELU generally hinders the optimization process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. CONCLUSION</head><p>In the automotive domain, GATs have become a popular approach to consider relational information within traffic scenarios for representation learning. This work identifies pitfalls of the established GATv2 that potentially impede the learning process. While real-world traffic scenarios are usually based on complex inter-dependencies, it is shown that GATv2 fails in solving already simple graph problems. Empirical evaluation based on different experiments evinces that the identified architectural pitfalls limit expressivity of GATv2 and model performance is highly sensitive to initialization. By adapting the model architecture as proposed in this work, limitations are removed and training is more robust to parameter initialization. Additionally, the interpretability of assigned attention scores as measure of importance is evaluated. Results align with the findings of literature, that there is no general correlation between the assigned attention scores and node relevance. While attention scores can evidence relevance, it's not a reliable indication. Such architectural limitations should be considered very well to avoid false expectations. The experiments of this work are based on synthetic and simple graph scenarios. The presented setup is appropriate for the elaboration of the fundamental pitfalls and limitations. In future works, the proposed model implementations will be tested and evaluated on real-world datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Concept of attentional message passing on graphs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1 )</head><label>1</label><figDesc>Alternative activation function: The problem arises from the fact, that the derivative of the LeakyRELU results in one of two possible values. By replacing the LeakyRELU with an alternative activation function the problem can be circumvented. An ideal activation function meets the following two requirements. Injective Derivative The derivative of the activation function should be an injective function such that ∀ a, b ∈ X, f (a) = f (b) =⇒ a = b. An injective derivative prevents a zero gradient due to the term (s t ij -s t ik ). Non-saturating To support the selective nature of the attention mechanism, the activation function should not be saturating, hence, it should satisfy lim |x|→∞ |∇f (x)| ̸ = 0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Relevance function used for simulation, where (a) represents the strictly monotonic relevance function and (b) the parabolic relevance function.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Confidence histograms for experiment I and experiment II. If a node is correctly identified as the most relevant neighboring node, the assigned attention score α ij is considered as sample in the corresponding histogram bin. Architecture TPR ME ±σ 2 Max. Error</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: Performance robustness for E = 100 initializations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>1 TPRFig. 6 :</head><label>16</label><figDesc>Fig. 6: Boxplots for the ME and TPR based on the results of E = 100 random initializations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>Evaluation of performance and interpretability of the graph-based attention mechanism based on controlled experiments. In this work, vectors are denoted as bold lowercase letters and matrices as bold capital letters.</figDesc><table /><note><p><p>• Proposition of architectural modifications to surpass limitations.</p>•</p></note></figure>
		</body>
		<back>

			
			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"> *  <p>The complete derivation of the gradients is published at <ref type="url" target="https://mariiilyn.github.io/2023/04/22/Gradient-Derivation-in-Graph-">https://mariiilyn.github.io/2023/04/22/Gradient-Derivation-in-Graph-</ref>Attention-Networks.html.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Analyzing the expressive power of graph neural networks in a spectral perspective</title>
		<author>
			<persName><forename type="first">M</forename><surname>Balcilar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Renton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Héroux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gaüzère</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Honeine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A linear programming approach for estimating the structure of a sparse linear genetic network from transcript profiling data</title>
		<author>
			<persName><forename type="first">S</forename><surname>Bhadra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Bhattacharyya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">R</forename><surname>Chandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">S</forename><surname>Mian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Algorithms for molecular biology : AMB</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Universal function approximation on graphs</title>
		<author>
			<persName><forename type="first">R</forename><surname>Brüel Gabrielsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><surname>Lin</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">772</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Measuring and relieving the over-smoothing problem for graph neural networks from the topological view</title>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="3438" to="3445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Listing k-cliques in sparse real-world graphs</title>
		<author>
			<persName><forename type="first">M</forename><surname>Danisch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Balalau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sozio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 World Wide Web Conference, ser. WWW &apos;18. Republic and Canton of Geneva, CHE: International World Wide Web Conferences Steering Committee</title>
		<meeting>the 2018 World Wide Web Conference, ser. WWW &apos;18. Republic and Canton of Geneva, CHE: International World Wide Web Conferences Steering Committee</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="589" to="598" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Attentive cross-modal paratope prediction</title>
		<author>
			<persName><forename type="first">A</forename><surname>Deac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sormanni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational Biology</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="536" to="545" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">How to find your friendly neighborhood: Graph attention design with self-supervision</title>
		<author>
			<persName><forename type="first">Dongkwan</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alice</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Graph u-nets</title>
		<author>
			<persName><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36 th International Conference on Machine (ICLM)</title>
		<meeting>the 36 th International Conference on Machine (ICLM)</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">36</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Vectornet: Encoding hd maps and agent dynamics from vectorized representation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Deep learning, ser. Adaptive computation and machine learning</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>The MIT Press</publisher>
			<pubPlace>Cambridge, Massachusetts and London, England</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">How much does attention actually attend?questioning the importance of attention in pretrained transformers</title>
		<author>
			<persName><forename type="first">M</forename><surname>Hassid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Rotem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kasai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Montero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Schwartz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2022 Conference on Empirical Methods in Natural Language Processing</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">How powerful are graph neural networks?</title>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Cooperative behavior planning for automated driving using graph neural networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Klimke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Volz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Buchholz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="167" to="174" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Understanding attention and generalization in graph neural networks</title>
		<author>
			<persName><forename type="first">Boris</forename><surname>Knyazev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Graham</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohamed</forename><forename type="middle">R</forename><surname>Amer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on Neural Information Processing Systems (NIPS)</title>
		<meeting>the 33rd International Conference on Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="4202" to="4212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multi-agent trajectory prediction with graph attention isomorphism neural network</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Sisbot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Oguchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Effective approaches to attention-based neural machine translation</title>
		<author>
			<persName><forename type="first">M.-T</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<ptr target="http://arxiv.org/pdf/1508.04025v5" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Scattering gcn: Overcoming oversmoothness in graph convolutional networks</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wenkel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Ranzato</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">F</forename><surname>Hadsell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Balcan</surname></persName>
		</editor>
		<editor>
			<persName><surname>Lin</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">508</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Multi-agent trajectory prediction with heterogeneous edge-enhanced graph attention network</title>
		<author>
			<persName><forename type="first">X</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Lv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="9554" to="9567" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Scene spatio-temporal graph convolutional network for pedestrian intention estimation</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">Y</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Bighashdel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Jancura</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Dubbelman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 IEEE Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="874" to="881" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Variational autoencoder-based vehicle trajectory prediction with an interpretable latent space</title>
		<author>
			<persName><forename type="first">M</forename><surname>Neumeier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Botsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tollkühn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Berberich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Intelligent Transportation Systems Conference (ITSC)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="820" to="827" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A multidimensional graph fourier transformation neural network for vehicle trajectory prediction</title>
		<author>
			<persName><forename type="first">M</forename><surname>Neumeier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Tollkühn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Botsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Utschick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 IEEE 25th International Conference on Intelligent Transportation Systems (ITSC)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="687" to="694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Attentive explanations: Justifying decisions and pointing to the evidence (extended abstract)</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.07373</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>in arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Gatv2conv</title>
		<author>
			<persName><forename type="first">Pytorch</forename><surname>Geometric</surname></persName>
		</author>
		<ptr target="https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torchgeometric.nn.conv.GATv2Conv" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Plant: Explainable planning transformers via object-level representations</title>
		<author>
			<persName><forename type="first">K</forename><surname>Renz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chitta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O.-B</forename><surname>Mercea</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Koepke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">6th Conference on Robot Learning (CoRL)</title>
		<imprint>
			<biblScope unit="page">2022</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Is attention interpretable?</title>
		<author>
			<persName><forename type="first">S</forename><surname>Serrano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Korhonen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Traum</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Màrquez</surname></persName>
		</editor>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Stroudsburg, PA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="2931" to="2951" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">How attentive are graph attention networks</title>
		<author>
			<persName><forename type="first">Shaked</forename><surname>Brody</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uri</forename><surname>Alon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eran</forename><surname>Yahav</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">10</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Gnnexplainer: Generating explanations for graph neural networks</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Bourgeois</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zitnik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Hierarchical graph representation learning with differentiable pooling</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Morris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems, S. Bengio</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><surname>Grauman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Cesa-Bianchi</surname></persName>
		</editor>
		<editor>
			<persName><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
