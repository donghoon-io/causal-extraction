<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Neighborhood Intervention Consistency: Measuring Confidence for Knowledge Graph Link Prediction</title>
				<funder ref="#_dGzgjWg">
					<orgName type="full">National Natural Science Foundation in China</orgName>
				</funder>
				<funder ref="#_cm5uavy">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_tXS3Q7U">
					<orgName type="full">Fundamental Research Fund for Central University</orgName>
				</funder>
				<funder ref="#_E3xe5HZ">
					<orgName type="full">Australian Research Council</orgName>
					<orgName type="abbreviated">ARC</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Kai</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Software Technology</orgName>
								<orgName type="department" key="dep2">Key Laboratory for Ubiquitous Network and Service Software of Liaoning Province</orgName>
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<postCode>116620</postCode>
									<settlement>Dalian Liaoning</settlement>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">Macquarie University</orgName>
								<address>
									<postCode>2109</postCode>
									<settlement>Sydney</settlement>
									<region>NSW</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Yu</forename><surname>Liu</surname></persName>
							<email>yuliu@mail.dlut.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Software Technology</orgName>
								<orgName type="department" key="dep2">Key Laboratory for Ubiquitous Network and Service Software of Liaoning Province</orgName>
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<postCode>116620</postCode>
									<settlement>Dalian Liaoning</settlement>
									<country key="CN">P.R. China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Quan</forename><forename type="middle">Z</forename><surname>Sheng</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">Macquarie University</orgName>
								<address>
									<postCode>2109</postCode>
									<settlement>Sydney</settlement>
									<region>NSW</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Neighborhood Intervention Consistency: Measuring Confidence for Knowledge Graph Link Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:19+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Link prediction based on knowledge graph embeddings (KGE) has recently drawn a considerable momentum. However, existing KGE models suffer from insufficient accuracy and hardly evaluate the confidence probability of each predicted triple. To fill this critical gap, we propose a novel confidence measurement method based on causal intervention, called Neighborhood Intervention Consistency (NIC). Unlike previous confidence measurement methods that focus on the optimal score in a prediction, NIC actively intervenes in the input entity vector to measure the robustness of the prediction result. The experimental results on ten popular KGE models show that our NIC method can effectively estimate the confidence score of each predicted triple. The top 10% triples with high NIC confidence can achieve 30% higher accuracy in the state-of-the-art KGE models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Knowledge graphs (KGs), which record real-world factual triples in the form of (head entity, relation, tail entity), have been widely applied in various AI domains <ref type="bibr" target="#b2">[Lin et al., 2020;</ref><ref type="bibr" target="#b11">Zhao et al., 2020]</ref>. To achieve automatic KG completion, link prediction based on the knowledge graph embedding (KGE) technologies have recently drawn considerable attention <ref type="bibr">[Zhang et al., 2020;</ref><ref type="bibr" target="#b6">Ruffinelli et al., 2020]</ref>. Given an entity and a relation (we call it an e-r query), a typical KGE model scores all candidate entities and selects the entity with the optimal score to compose a new triple. However, the new predicted triples cannot be added into KGs directly because of the insufficient accuracy and unreliable confidence measurement <ref type="bibr" target="#b7">[Safavi et al., 2020]</ref>.</p><p>Researchers are recently devoted to improving the confidence measurement of KGE models using probability calibration methods <ref type="bibr" target="#b5">[Platt, 1999;</ref><ref type="bibr" target="#b2">Guo et al., 2017]</ref>. <ref type="bibr">Tabacof and Costabello [Tabacof and Costabello, 2020]</ref> find that KGE models are not well-calibrated, and the probability estimates for triple classification are unreliable. <ref type="bibr">Safavi et al. [Safavi Figure 1:</ref> Reliability diagrams of the RotH model <ref type="bibr" target="#b0">[Chami et al., 2020]</ref> before and after calibration on the FB15k237 dataset, in which predicted triples are grouped into 10 bins according to their confidence scores. The blue bars refer to the average accuracy of triples in the same confidence level, while triple proportion (in marigold) is a percentage of the total triple quantity. The confidence score is the optimal score via a Sigmoid function. <ref type="bibr">et al., 2020]</ref> demonstrate that calibration techniques can significantly reduce the calibration error of KGE models in the relation prediction task. These research efforts mainly work on classification tasks in the KGE domain, only predicting in a few categories. However, link prediction is a more challenging "learning to rank" problem, aiming to find the target entity from tens of thousands of candidate entities.</p><p>There are two main problems restricting the effectiveness of the probabilistic calibration methods for reliable link prediction in the KGE domain:</p><p>• Unsuitable confidence measurement. Previous measuring methods focus on the optimal score or compare it with other scores in the score sequence. However, as shown in Fig. <ref type="figure">1</ref>(a), high optimal scores lead to high confidence but cannot achieve the same level accuracy. This is due to the fact that the score of each candidate only indicates its relative order among candidates in one prediction.</p><p>• Unreliable calibration metrics. Expected Calibration Error (ECE) <ref type="bibr" target="#b4">[Niculescu-Mizil and Caruana, 2005]</ref> is commonly utilized to evaluate the calibration effect, but is not suitable for link prediction. Although ECE is greatly reduced after calibration in Fig. <ref type="figure">1</ref>(b), most of the triples are compressed into the low confidence level, making it very hard to extract high-accuracy triples.</p><p>In this paper, we propose a novel confidence measurement method based on causal intervention, called Neighborhood Intervention Consistency (NIC). Different from previous methods focusing on the optimal scores, we evaluate the confidence score by verifying the robustness of prediction results. Benefiting from a causal inference analysis, we propose a NIC framework actively intervening the scoring process of KGE models. Specifically, we generate a series of neighborhood vectors for an input entity by adjusting the entity vector's value in different dimensions and observing whether the output sequences of the model changes or matches the original one. On this basis, we design several types of neighborhood intervention values and a dimension selection strategy for high-dimensional KGE models.</p><p>Furthermore, our link prediction experiments exploit new evaluation metrics to verify whether the predicted triples are valuable for KG completion, including the top 10% accuracy and confidence variance. Ten popular KGE models are selected for the evaluation, including six high-dimensional models and four low-dimensional ones. The experimental results show that NIC outperforms previous confidence measurement methods before and after calibration. The calibrated NIC score can effectively overcome the drawback of low confidence variance of the previous methods. Besides, the top 10% triples with high NIC confidence can achieve 30% higher accuracy in the state-of-the-art KGE models. Finally, we verify the optimal choices of the intervention value and prove that the dimension selection strategy can effectively balance the computational efficiency and prediction accuracy for high-dimensional KGE models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Knowledge Graph Embeddings</head><p>Let E and R denote the set of entities and relations, a knowledge graph (KG) G is a collection of factual triples (h, r, t), where h, t ∈ E and r ∈ R. |E| and |R| refer to the number of entities and relations in G, respectively. Knowledge Graph Embeddings aim to represent each entity e ∈ E (or relation r ∈ R) as a d-dimensional continuous vector, and learn a scoring function f : E × R × E → R to score each triple. Most KGE models are trained by minimizing a negative sampling loss, to make the score of the qualified triple higher than those of negative samples <ref type="bibr" target="#b10">[Wang et al., 2017]</ref>.</p><p>Link Prediction. Generalized link prediction tasks include entity prediction and relation prediction. In this paper, we focus on the more challenging entity prediction task. Given an e-r query (e, r), the typical link prediction aims to find the target entity m ∈ E satisfying that (e, r, m) or (m, r, e) belongs to knowledge graph G. As illustrated in Fig. <ref type="figure" target="#fig_0">2(a)</ref>, a KGE model needs to score all candidate triples and output a sorted score sequence. In the sequence, the entity with the optimal score is selected as m to construct the new triple.</p><p>Typical KGE Models. Various KGE models have been proposed, such as i) translation-based <ref type="bibr">TransE [Bordes et al., 2013]</ref> and RotatE <ref type="bibr" target="#b8">[Sun et al., 2019]</ref>; ii) factorization-based DistMult <ref type="bibr" target="#b10">[Yang et al., 2015]</ref>, ComplEx <ref type="bibr" target="#b9">[Trouillon et al., 2016]</ref> and TuckER <ref type="bibr" target="#b0">[Balazevic et al., 2019]</ref>; and iii) CNN- </p><formula xml:id="formula_0">Model Score Function Model Score Function TransE h + r -t TransH Dhyp(h ⊕ r, t) DistMult h T diag(Mr)t DistH Dhyp(h • r, t) ComplEx Re(h cT diag(M c r )t c ) RotH Dhyp(Rot(r)h, t) ConvE f (vec(f ([ h; r] * ω))W)t RefH Dhyp(Ref (r)h, t) RotatE h • r c -t TuckER W × h × r × t</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Probability Calibration</head><p>The calibration of KGE models focuses on predicting confidence scores representing the actual correctness probabilities of predicted triples. Under an ideal situation, if the model predicts that a triple is true with a 0.9 confidence score, it should be correct 90% of the time. Model calibration requires reliable confidence measurement and effective calibration methods to fix the calibration errors.</p><p>Confidence Measurement Methods. In the KGE domain, two confidence measurement methods, SigmoidMax (SIG) and TopKSoftmax (TOP), have been applied in very recent literature <ref type="bibr">[Tabacof and Costabello, 2020;</ref><ref type="bibr" target="#b7">Safavi et al., 2020]</ref>. Given the sorted score sequence S of a prediction, the two methods measure the confidence score by: p sig (S) = 1/(1 + e -max(S) )</p><p>(1)</p><formula xml:id="formula_1">p top (S) = max(e S[:K] /( K i=1 e Si ))<label>(2)</label></formula><p>Both of them are computed by the single sequence S and focus on the optimal score.</p><p>Calibration Methods. There are also two typical calibration methods. Platt scaling <ref type="bibr" target="#b5">[Platt, 1999]</ref> inputs prediction scores into a logistic regression, and learns scalar weights to output a confidence score for each sample. Isotonic regression <ref type="bibr">[Guo et al., 2017]</ref> is a non-parametric calibration method, which fits a non-decreasing piece-wise constant function to the model output. Both methods can effectively improve calibration, but depend on reliable confidence measurement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Neighborhood Intervention Consistency</head><p>We discuss a novel confidence measurement method, Neighborhood Intervention Consistency (NIC), in this section. First, a causal graph is utilized to reveal the fundamental motivation of this method in Sec. 3.1, and then we introduce the basic framework of NIC in Sec. 3.2. After that, we detail two key technical components, neighborhood intervention and dimension selection, in Sec. 3.3 and 3.4, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Motivation and Causal Inference</head><p>For link prediction, a confidence score should reflect the possibility of whether the predicted triple is the optimal choice in the prediction. We argue that a high confidence contains two meanings: i) significance: the optimal score should obviously outperform the others; and ii) robustness: a disturbance of the input data should not affect the prediction results.</p><p>The two confidence measurement methods mentioned in Sec. 2.2 aim to measure the significance from the output score sequence. However, they fail in the link prediction task because the patterns among different scores vary unstably in different sequences. Therefore, we focus on the second aspect, the robustness of score sequences. Inspired by the causality theory <ref type="bibr">[Pearl and Mackenzie, 2018]</ref>, rather than observing the association among different scores in the single sequence, our approach actively intervenes the scoring process by using multiple similar input vectors, and then judges the consistency of the output sequences.</p><p>To intervene in the scoring process, the causes affecting the generation of score sequences should be analyzed. Given a trained KGE model M and multiple e-r queries Q = {(e i , r)|e i ∈ E, r ∈ R} with the same relation r, we formulate the causalities in the KGE model with a Structural Causal Model (SCM) <ref type="bibr" target="#b5">[Pearl, 2000]</ref>. As illustrated in Fig. <ref type="figure" target="#fig_0">2</ref>(b), the entity vector e are the exogenous variables containing d dimensions, and each dimensional value as a variable takes part in the whole inference process. After the calculations in M, the transformed entity vector e are the endogenous variables of e and also the direct cause of the score sequence S. Note that, one variable in the transformed vector e is determined by not only that of the same dimension in e, but also the other dimensional variables.</p><p>Benefiting from the causal graph, we can pinpoint the roles of different parts in the KGE model. To intervene in the score sequence, we use the causal intervention: P (S|e, do(e i ). This process modifies one of the dimensions in the input entity vector and so as to exclude the effect of this variable on the score sequence. For d-dimensional embeddings, we can generate d different neighborhood input vectors by modifying each dimensional variable, and formally we have:</p><formula xml:id="formula_2">P (S|do(e)) = 1 d d i=0 P (S|e, do(e i ))<label>(3)</label></formula><p>It should be noted that we intervene the input vector e rather than the direct cause e , because before modifying an endogenous variable in e , its exogenous variable has also affected the other dimensions of e .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">The NIC Framework</head><p>Inspired by the causal intervention process, we propose the new confidence measurement which aims to evaluate the robustness of score sequences. The NIC framework is illustrated in Fig. <ref type="figure" target="#fig_0">2</ref>(c) and contains three main steps:</p><p>Step 1. Neighborhood Intervention. To achieve causal intervention, we first modify each dimensional values of the input entity vector e in turn and generate d different neighborhood vectors. Specifically, the neighborhood vectors N e ∈ R d×d is computed by:</p><formula xml:id="formula_3">N e = (1 -I d ) × e + I d × v(e),<label>(4)</label></formula><p>where I d ∈ R d×d is an identity matrix, and v(e) is a function that outputs an intervention value used to replace the original value in e. We utilize multiple kinds of intervention values for different entity vectors, which will be detailed in Sec. 3.3.</p><p>Step 2. Sequence Generation. In this step, we first gather the original score sequence by inputting the original entity vector e into the KGE model. As the size of entity set is huge, we extract the top K scores S and the corresponding entities {e i ∈ E|0 &lt; i K} from the sorted sequence. Then, using the narrowed entity set and the neighborhood vectors N e (instead of e) as inputs, we can gather neighborhood score sequences S N = {S Ni |0 &lt; i d} from the model. The relative ranking of the K candidate entities in each sequence, is denoted by (S Ni ) = Argsort(S Ni ), which contains the sorted candidate indexes in length K.</p><p>Step 3. Computing Consistency. The ranking (S Ni ) of each neighborhood sequence is expected to be consistent with (S) of the original sequence, especially the part with higher scores. For this, we define the consistency value ς between the original sequence S and each S Ni , i.e.,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ς(S, S</head><formula xml:id="formula_4">N i ) = J j=0 σ(S)j(1 -Sgn(| (S)j -(S N i )j|)),<label>(5)</label></formula><p>where σ(•) and Sgn(•) refer to the Softmax function and the Sign function, respectively. The consistency value ς = 1 when the two sequences having the same candidate index in the top J positions. Otherwise, a mismatching at the higher position would cause a larger decrease to ς. Therefore, ς can indicates the robustness of prediction results before and after intervention. The hyperparamter J determines the number of candidates when measuring consistency, it is usually lower than K to ensure the candidate rankings having enough differentiation.</p><p>Finally, given the consistency value of each neighborhood sequence, the NIC score p nic is defined by:</p><formula xml:id="formula_5">p nic (S) = 1 d d i=0 (ς(S, S Ni )).<label>(6)</label></formula><p>The value domain of p nic (S) is [0, 1], such that it can be used to measure the confidence score in KGE models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Neighborhood Intervention</head><p>In the first step of the NIC framework, the i-th neighborhood vector is generated by modifying the i-th dimensional value of the original entity vector. There are still issues on how to define the intervention value v to replace the original value.</p><p>The most straightforward way is setting v = 0 for all entity vectors, such that the i-th dimension of the i-th neighborhood vector is equal to zero. However, we find that its performance is not ideal in our empirical studies. Therefore, we further propose several intervention values by considering the statistical specificity of different entity vectors. Given the entity embedding vector e, there are four kinds of intervention values, including zero, mean, maximum, and minimum, i.e., {0, avg(e), max(e), min(e)}. There are more choices of intervention values, but we argue that the above four are representative. In Sec. 4, we will compare the effectiveness of different intervention values.</p><p>The intervention value cannot be selected randomly, because the NIC score should maintain uniqueness for the same input. In addition, as the first attempt of neighborhood intervention, this paper focuses on intervening one dimension in each neighborhood vector. It might be feasible to modify multiple values in the entity vector to generate more neighborhood vectors, which will be our future investigation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Dimension Selection</head><p>We consider the computational complexity of NIC. Measuring the consistency by intervening each dimension costs much less than the original prediction, as long as setting a small value for the hyperparameter K. But the time complexity of NIC is still linearly dependent on the embedding dimension size d. Facing a high-dimensional KGE model (with more than 200 dimensions), NIC using all neighborhood vectors would cause high calculation cost.</p><p>Therefore, we design a dimension selection strategy to accelerate the NIC calculation. Based on a hypothesis that a dimension with lower variance hardly reflects the difference among entities, we measure the variance of each dimension. Then, we select a limited number of high-variance dimensions, which contribute more in the d-dimensional vector.</p><p>Specifically, given the entity embedding matrix E ∈ R |E|×d of a trained KGE model, E i,j refers to the j-th dimension of the i-th entity and the weight w j of the j-th dimension is defined as:</p><formula xml:id="formula_6">w j = 1 |E| |E| i=0 (E i,j -avg(E :,j )) 2 (7)</formula><p>Given the weight vector w = [w 0 , w 1 , . . . , w d ], we can maintain the top D dimensions with the highest weights and set others to zero. Normalized by a Softmax function, the processed vector can be utilized to adjust the contribution of different dimensions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Experimental Setup</head><p>Our experimental studies are conducted on two widely used datasets, WN18RR <ref type="bibr" target="#b0">[Bordes et al., 2014]</ref>  Ten KGE models (see Table <ref type="table" target="#tab_0">1</ref>) are trained by following their original settings with the binary cross-entropy loss. For the six high-dimensional KGE models, such as TransE and TuckER, we set their embedding dimensions as 200, while the four low-dimensional models' embedding dimension is 32. We select the hyper-parameters in the NIC framework via grid search. Specifically, we empirically select the number of remeasured entities K among {3, 5, 10, 100} and the position number J for computing sequence consistency among {1, 3, 5, 10}. All experiments are performed on Intel Core i7-7700K CPU @ 4.20GHz and NVIDIA GeForce GTX1080 Ti GPU, and implemented in Python using the PyTorch framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Evaluation Metrics</head><p>Considering ECE (Expected Calibration Error) <ref type="bibr" target="#b4">[Niculescu-Mizil and Caruana, 2005]</ref> cannot adequately reflect the calibration effects in the link prediction task, we introduce three additional new evaluation metrics:</p><p>• CVar: the variance of confidence scores of all triples, • T10%MRR: the average inverse rank of the top 10% high-confidence triples, and • T10%ACC: the average accuracy (equal to Hits@1) of the top 10% high-confidence triples. The CVar metric makes up for the drawback of ECE. The lower ECE and higher CVar jointly indicate that the confidence score can match the correctness probability better. T10%MRR and T10%ACC are designed for the demand of practical KG completion. Instead of manually screening the roughly-predicted triples, KG builders expect the highconfidence triples having high enough accuracy. Therefore, higher T10%MRR and T10%ACC scores indicate a better model performance for link prediction.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Experimental Results of the Ten Models</head><p>We first verify our NIC performance compared with existing confidence measurement methods, SigmoidMax (SIG) and TopKSoftmax (TOP), utilizing ten different KGE models on two datasets. NIC by default uses the intervention value "maximum" and no dimension weights. Given a trained KGE model and test triples, three confidence measurement methods are utilized respectively and then calibrated by two calibration methods, Platt scaling and Isotonic regression. To the best of our knowledge, this is the most comprehensive experimental study for the calibration of the link prediction task. We analyze the results in details in the rest of this section.</p><p>Table <ref type="table" target="#tab_2">3</ref> shows the ECE and T10%ACC results of the ten KGE models after calibrated by isotonic regression. In terms of the ECE metric, all three confidence measurement methods can achieve low ECE values after calibration. NIC surpasses the other two methods on most of models, especially the 32-dimensional models. Comparing T10%ACC and the original accuracy ACC, all three methods can help distinguish high-accuracy triples. Our NIC outperforms the other two methods and significantly improves by more than 30% in some of the KGE models. Besides, TuckER and RotatE with 200 dimensions achieves the best ACC on two datasets, respectively. The RefH model outperforms the other lowdimensional models and achieves much higher T10%ACC.</p><p>In summary, fitting both low-and high-dimensional conditions, NIC can maintain the high-confidence triples having much higher accuracy. When only the most accurate triples are required in practical applications, we can use lowdimensional models with NIC measurements to achieve highefficient link prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Detailed Comparison in the RotH Model</head><p>We then concentrate on the confidence calibration in a single model, to better compare three confidence measurement methods using two calibration methods. Following the previous work, reliability diagrams bin all predicted triples by confidence scores into ten equally-sized regions of [0,1]. To reflect the number distribution, we further add triple proportion (in marigold) to each confidence bin. The reliability diagrams for the RotH model using different confidence measurement methods before and after calibration are shown in Fig. <ref type="figure" target="#fig_1">3</ref>. For other models, we observe the similar patterns.</p><p>The left three diagrams in each group illustrate the con- fidence distribution before calibration. On the two datasets, the average accuracy of SIG and TOP has obvious differences from the corresponding confidence level, while the accuracy of NIC is relatively close to the confidence level before calibration. It indicates that the conventional method used in other calibration tasks, is not suitable for confidence measurement in link prediction.</p><p>Comparing two calibration methods, Isotonic regression achieves lower ECE and higher CVar in most of experiments. After calibrated, the ECE scores of SIG and TOP significantly decrease but the confidence variances are relatively smaller than NIC's, especially on FB15k237. In contrast, NIC enables a precise alignment of confidence and accuracy after the calibration. Besides, NIC keeps a more divergent confidence distribution with the confidence variance more than 0.06 on FB15k237 and 0.11 on WN18RR.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Verification of Two Components</head><p>We deeply verify the effectiveness of two NIC components: Neighborhood Intervention. We compare T10%MRR and ECE of NIC using four intervention values, i.e., zero, mean, maximum, and minimum. The results are shown in Table <ref type="table" target="#tab_3">4</ref>. For the four low-dimensional KGE models, using the maximum or minimum values achieve the best ECE in most models, and the zero and mean values perform slightly worse. Especially on FB15k237, their results are more than 50% lower than that of maximum and minimum. The similar trend can be found in the T10%MRR results, the maximum and minimum intervention precedes the others on RotH, TransH and DistH. The reason might be that the maximum and minimum values can provide better differentiation, while the others cannot change the original value significantly.</p><p>Dimension Selection. We select the RotH model with 256 dimensions to verify the effectiveness of dimension selection. Fig. <ref type="figure" target="#fig_2">4</ref> shows the T10%ACC and T10%MRR results on FB15K237. We can see that the T10%ACC of Weight(all) are slightly better than that of the original NIC (i.e., NIC-Max). As the improvement is not obvious, weighted summing is not needed when using all neighborhood vectors. However, the dimensional weight is valuable to reduce the number of neighborhood vectors for efficiency. When using only 32 neighborhood vectors, Weight(32) achieves slightly lower prediction accuracy than NIC-Max, but still outperforms significantly the other two methods (TopkSoftmax and Sigmoid- Max). With the increase of the dimensions, T10%ACC and T10%MRR gradually grow until reaching around 160 dimensions and then remain stable. It is feasible to select part of neighborhood vectors to take care of both accuracy and efficiency at the same time. Especially for the high-dimensional KGE models, the dimension selection strategy can significantly improve the computational efficiency of our proposed confidence measurement method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>Recent knowledge graph embedding (KGE) models can be rarely applied in the KG completion tasks in practice due to low prediction accuracy and unreliable confidence measurement. In this paper, we present a novel confidence measurement framework, namely Neighborhood Intervention Consistency (NIC). Based on the causal intervention, NIC actively intervenes the input entity vector to measure the prediction robustness. The experimental results show that our NIC method can effectively estimate the prediction accuracy while keeping an acceptable variance. Furthermore, NIC achieves 30% higher accuracy for the top 10% highestconfidence triples in the state-of-the-art KGE models. In the future, we will further improve our method by intervening multiple dimensions in one neighborhood vector, and by taking both statistical significance and prediction robustness into the consideration.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: (a) An illustration of link prediction process. (b) Causal graph of a KGE model with an intervention. In the causal graph, the direct links denote the causalities between the two nodes: cause → effect. (c) An illustration of the main NIC framework (d = 4).</figDesc><graphic coords="3,66.15,53.99,218.69,188.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Reliability diagrams of the RotH model using different confidence measurement and calibration methods on two datasets.</figDesc><graphic coords="5,54.00,221.62,503.97,205.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: T10%ACC and T10%MRR of the RotH model using different dimension selection strategies on FB15K237. Weight(N) refers to using the neighborhood vectors whose dimension weight ranks in top N, Weight(all) means weighted summing all neighborhood vectors and NIC-Max is the original NIC method equally treating each vector.</figDesc><graphic coords="6,321.08,54.00,230.85,150.91" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>The score functions of 10 different KGE models, where γ &gt; 0 is the margin value, • denotes the element-wise Hadmard product, * denotes the convolution operator, × denotes the tensor product, e c denotes a complex-number vector. N is the number of fact triples T , and N T is the number of negative samples T . D hyp is the hyperbolic distance, ⊕ is Möbius addition operation, Rot() and Ref () refer to the specific transformation operation. Based on the hyperbolic space, Chami et al.<ref type="bibr" target="#b0">[Chami et al., 2020]</ref> recently propose two KGE models, RotH and RefH, which perform well in low-dimensional KGE situations. To ensure the breadth of the low-dimensional KGE evaluation, we further extend two hyperbolic-based variants, TransH and DistH, inspired by TransE and DistMult. Table1displays the scoring functions of the ten KGE models used in this paper.</figDesc><table><row><cell>based ConvE [Dettmers et al., 2018] and ConvKB [Nguyen</cell></row><row><cell>et al., 2017]. The major difference among them is the scoring</cell></row><row><cell>function, because the negative sampling strategy or the loss</cell></row><row><cell>function is generally universal.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>and FB15k237 [Toutanova and Chen, 2015]. The statistics of the datasets are given in Table 2. With Train, Valid, and Test, we refer to the number of triples in the training, validation and test sets. Statistics of the datasets.</figDesc><table><row><cell>Dataset</cell><cell>|R|</cell><cell>|E|</cell><cell>#Train</cell><cell>#Valid</cell><cell>#Test</cell></row><row><cell cols="6">FB15k237 237 14, 541 272, 115 17, 535 20, 466</cell></row><row><cell>WN18RR</cell><cell cols="2">11 40, 943</cell><cell>86, 845</cell><cell>3, 034</cell><cell>3, 134</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Model calibration results for the link prediction task on the FB15k237 and WN18RR datasets. The best score among three measurement methods is in Bold and the best ACC among models is underlined.</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell>FB15K237</cell><cell></cell><cell></cell><cell></cell><cell>WN18RR</cell><cell></cell><cell></cell></row><row><cell>Methods</cell><cell>Dim</cell><cell cols="3">ECE↓ SIG TOP NIC SIG TOP NIC T10%ACC↑</cell><cell>ACC↑</cell><cell cols="3">ECE↓ SIG TOP NIC SIG TOP NIC T10%ACC↑</cell><cell>ACC↑</cell></row><row><cell>TransE</cell><cell></cell><cell>.078 .015</cell><cell>.009 .457 .695</cell><cell>.714</cell><cell>.152</cell><cell>.259 .003</cell><cell>.003 .278 .125</cell><cell>.123</cell><cell>.012</cell></row><row><cell>DistMult</cell><cell></cell><cell>.010 .007</cell><cell>.009 .758 .813</cell><cell>.842</cell><cell>.202</cell><cell>.074 .022</cell><cell>.018 .528 .715</cell><cell>.720</cell><cell>.372</cell></row><row><cell>ComplEx ConvE</cell><cell>200d</cell><cell>.011 .008 .013 .007</cell><cell>.008 .790 .840 .006 .805 .842</cell><cell>.866 .870</cell><cell>.202 .237</cell><cell>.055 .019 .011 .011</cell><cell>.016 .737 .956 .013 .591 .486</cell><cell>.968 .620</cell><cell>.395 .400</cell></row><row><cell>RotatE</cell><cell></cell><cell>.043 .025</cell><cell>.021 .544 .665</cell><cell>.714</cell><cell>.241</cell><cell>.012 .017</cell><cell>.016 .985 .975</cell><cell>.991</cell><cell>.428</cell></row><row><cell>TuckER</cell><cell></cell><cell>.015 .006</cell><cell>.008 .854 .863</cell><cell>.888</cell><cell>.266</cell><cell>.031 .028</cell><cell>.027 .700 .601</cell><cell>.827</cell><cell>.443</cell></row><row><cell>RotH</cell><cell></cell><cell>.025 .029</cell><cell>.012 .761 .817</cell><cell>.838</cell><cell>.223</cell><cell>.017 .018</cell><cell>.016 .875 .902</cell><cell>.918</cell><cell>.428</cell></row><row><cell>RefH TransH</cell><cell>32d</cell><cell>.020 .019 .040 .030</cell><cell>.013 .801 .854 .021 .663 .705</cell><cell>.869 .798</cell><cell>.219 .217</cell><cell>.012 .016 .014 .015</cell><cell>.020 .928 .868 .012 .244 .109</cell><cell>.948 .252</cell><cell>.414 .081</cell></row><row><cell>DistH</cell><cell></cell><cell>.035 .027</cell><cell>.021 .662 .697</cell><cell>.703</cell><cell>.202</cell><cell>.021 .015</cell><cell>.012 .704 .760</cell><cell>.796</cell><cell>.399</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>.065 .130 .131 .038 .039 .057 .058 RefH .049 .052 .119 .120 .043 .041 .085 .086 TransH .077 .078 .145 .147 .457 .455 .541 .542 DistH .095 .087 .181 .182 .086 .085 .133 .133 TMRR RotH .785 .778 .782 .780 .915 .901 .904 .904 RefH .830 .840 .844 .846 .943 .935 .951 .951 TransH .740 .727 .735 .718 .026 .030 .029 .029 DistH .642 .657 .646 .646 .804 .799 .788 .788 ECE and T10%MRR of four uncalibrated models using NIC with different intervention values on two datasets. The Avg refers to the mean value. The best scores are in Bold.</figDesc><table><row><cell>Methods</cell><cell>FB15K237 Max Min Zero Avg Max Min Zero Avg WN18RR</cell></row><row><cell>RotH</cell><cell>.063</cell></row><row><cell>ECE</cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_0"><p>Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence </p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This research is supported by the <rs type="funder">National Natural Science Foundation in China</rs> (Grant: <rs type="grantNumber">61672128</rs>) and the <rs type="funder">Fundamental Research Fund for Central University</rs> (Grant: <rs type="grantNumber">DUT20TD107</rs>). <rs type="person">Quan Z. Sheng</rs> has been partially supported by <rs type="funder">Australian Research Council (ARC)</rs> <rs type="grantName">Future Fellowship Grant</rs> <rs type="grantNumber">FT140101247</rs>, and <rs type="grantName">Discovery Project Grant</rs> <rs type="grantNumber">DP200102298</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_dGzgjWg">
					<idno type="grant-number">61672128</idno>
				</org>
				<org type="funding" xml:id="_tXS3Q7U">
					<idno type="grant-number">DUT20TD107</idno>
				</org>
				<org type="funding" xml:id="_E3xe5HZ">
					<idno type="grant-number">FT140101247</idno>
					<orgName type="grant-name">Future Fellowship Grant</orgName>
				</org>
				<org type="funding" xml:id="_cm5uavy">
					<idno type="grant-number">DP200102298</idno>
					<orgName type="grant-name">Discovery Project Grant</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A semantic matching energy function for learning with multi-relational data</title>
		<author>
			<persName><surname>Balazevic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing, EMNLP-IJCNLP</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing, EMNLP-IJCNLP<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2019. 2019. 2013. 2013. 2014. 2014. July 5-10, 2020. 2020</date>
			<biblScope unit="volume">94</biblScope>
			<biblScope unit="page" from="6901" to="6914" />
		</imprint>
	</monogr>
	<note>Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Convolutional 2d knowledge graph embeddings</title>
		<author>
			<persName><surname>Dettmers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32th AAAI Conference on Artificial Intelligence</title>
		<meeting>the 32th AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="1811" to="1818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">KGNN: knowledge graph neural network for drug-drug interaction prediction</title>
		<author>
			<persName><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Ninth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017. 2020</date>
			<biblScope unit="volume">2020</biblScope>
			<biblScope unit="page" from="2739" to="2745" />
		</imprint>
	</monogr>
	<note>Proceedings of the 34th International Conference on Machine Learning</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A novel embedding model for knowledge base completion based on convolutional neural network</title>
		<author>
			<persName><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference of the North American Chapter of the Association for Computational Linguistics</title>
		<meeting>the 2017 Conference of the North American Chapter of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="327" to="333" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Pearl and Mackenzie, 2018] Judea Pearl and Dana Mackenzie. The Book of Why: the New Science of Cause and Effect</title>
		<author>
			<persName><forename type="first">Niculescu-Mizil</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Caruana ; Alexandru</forename><surname>Niculescu-Mizil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd International Conference on Machine learning</title>
		<meeting>the 22nd International Conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2005">2005. 2005. 2018</date>
			<biblScope unit="page" from="625" to="632" />
		</imprint>
	</monogr>
	<note>Predicting good probabilities with supervised learning. American Association for the Advancement of Science</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods</title>
		<author>
			<persName><forename type="first">John</forename><surname>Pearl</surname></persName>
		</author>
		<author>
			<persName><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Judea Pearl. Causality: Models, Reasoning, and Inference</title>
		<imprint>
			<publisher>Cambridge University Press</publisher>
			<date type="published" when="1999">2000. 2000. 1999</date>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="61" to="74" />
		</imprint>
	</monogr>
	<note>Platt, 1999</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">You can teach an old dog new tricks! on training knowledge graph embeddings</title>
		<author>
			<persName><surname>Ruffinelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Conference on Learning Representations</title>
		<meeting>the Eighth International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
		<respStmt>
			<orgName>ICLR</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Evaluating the calibration of knowledge graph embeddings for trustworthy link prediction</title>
		<author>
			<persName><surname>Safavi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Online</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2020-11-16">2020. 2020. November 16-20, 2020. 2020</date>
			<biblScope unit="page" from="8308" to="8321" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Toutanova and Chen, 2015] Kristina Toutanova and Danqi Chen. Observed versus latent features for knowledge base and text inference</title>
		<author>
			<persName><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd Workshop on Continuous Vector Space Models and their Compositionality</title>
		<meeting>the 3rd Workshop on Continuous Vector Space Models and their Compositionality</meeting>
		<imprint>
			<date type="published" when="2015">2019. 2019. 2020. 2015</date>
			<biblScope unit="page" from="57" to="66" />
		</imprint>
	</monogr>
	<note>Proceedings of the Seventh International Conference on Learning Representations</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Complex embeddings for simple link prediction</title>
		<author>
			<persName><surname>Trouillon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33th International Conference on Machine Learning</title>
		<meeting>the 33th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="2071" to="2080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Transrhs: A representation learning method for knowledge graphs with relation hierarchical structure</title>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Twenty-Ninth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2015">2017. 2017. 2015. 2015. 2020</date>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="2987" to="2993" />
		</imprint>
	</monogr>
	<note>Proceedings of the Third International Conference on Learning Representations</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Knowledge graphs enhanced neural machine translation</title>
		<author>
			<persName><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJ-CAI 2020</title>
		<meeting>the Twenty-Ninth International Joint Conference on Artificial Intelligence, IJ-CAI 2020</meeting>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
			<biblScope unit="page" from="4039" to="4045" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
