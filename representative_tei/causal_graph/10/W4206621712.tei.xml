<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Effect Generation Based on Causal Reasoning</title>
				<funder ref="#_3Z2gNMv">
					<orgName type="full">Research Grants Council of Hong Kong</orgName>
				</funder>
				<funder ref="#_qvCVxVP">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder ref="#_BJCHDW5 #_5fkQBjB #_kc4PNN3 #_efHfwHr #_ZfAefZQ">
					<orgName type="full">unknown</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Feiteng</forename><surname>Mu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">The Department of Computing</orgName>
								<orgName type="institution">The Hong Kong Polytechnic University</orgName>
								<address>
									<country key="HK">Hong Kong</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wenjie</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">The Department of Computing</orgName>
								<orgName type="institution">The Hong Kong Polytechnic University</orgName>
								<address>
									<country key="HK">Hong Kong</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Zhipeng</forename><surname>Xie</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Fudan University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Effect Generation Based on Causal Reasoning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:21+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Causal reasoning aims to predict the future scenarios that may be caused by the observed actions. However, existing causal reasoning methods deal with causalities on the word level. In this paper, we propose a novel eventlevel causal reasoning method and demonstrate its use in the task of effect generation. In particular, we structuralize the observed causeeffect event pairs into an event causality network, which describes causality dependencies. Given an input cause sentence, a causal subgraph is retrieved from the event causality network and is encoded with the graph attention mechanism, in order to support better reasoning of the potential effects. The most probable effect event is then selected from the causal subgraph and is used as guidance to generate an effect sentence. Experiments show that our method generates more reasonable effect sentences than various well-designed competitors.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Causal reasoning is the process of observing an action and reasoning future scenarios that may be potentially caused by it <ref type="bibr" target="#b18">(Radinsky et al., 2012)</ref>. Earlier causal reasoning methods <ref type="bibr" target="#b19">(Roemmele et al., 2011;</ref><ref type="bibr" target="#b11">Luo et al., 2016)</ref> collect causally related word pairs (e.g., earthquake→tsunami) to build the statistical models of causality, and then predict effects words for given cause words. Recently, <ref type="bibr" target="#b23">(Xie and Mu, 2019)</ref> uses causal embedding to predict possible effect words of the input causes. <ref type="bibr" target="#b10">(Li et al., 2020)</ref> proposed the lexically-constrained beam-search to generate possible effects given provided word guidance. However, all these methods tend to reason causalities at word-level.</p><p>Causalities between word pairs are not always self-contained (i.e., intelligible) when they are extracted without the context <ref type="bibr" target="#b5">(Hashimoto et al., 2014)</ref>). For example, "quarrel→break" is not selfcontained since this is not intelligible without the context: "They always quarrel→They break up". A word-level causal reasoning method may only predict the unintelligible effect of "break" conditioned "quarrel". Considering this deficiency, a better way is to enhance causal reasoning with causal events <ref type="bibr" target="#b18">(Radinsky et al., 2012;</ref><ref type="bibr" target="#b24">Zhao et al., 2017;</ref><ref type="bibr" target="#b12">Martin et al., 2018;</ref><ref type="bibr" target="#b0">Ammanabrolu et al., 2020)</ref>. However, an observed causal event is very likely to appear only once, which brings about huge sparsity to causalities and great difficulty to the eventlevel causal reasoning. To solve this problem, we structuralize observed causal events into an event causality network, where similar events are clustered together. Given an input cause sentence, a causal subgraph is retrieved and is encoded with the graph attention mechanism, in order to support better effect reasoning. As such, we are able to predict the most reasonable effect event based on the event causality network. The predicted effect event contains the skeleton information, with the detailed context neglected in the event extraction process. So we further rewrite the predicted effect event to an effect sentence in order to fill in the missing information.</p><p>The contributions of this paper are twofold: i) we devise a effect generation method which is based on causal event reasoning (EGCER) to generate effect sentences for given input cause sentences, ii) experiments demonstrate that our model achieves better performances compared among various welldesigned baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Event Causality Network Construction</head><p>In this paper, we use causal events to bridge the causalities between input sequences and generated sequences. Hence, we must first collect sufficient cause-effect sentence pairs so that from each sentence pair a cause-effect event pair can be eventified. Then we construct an event causality network based on the extracted causal event pairs. The construction process includes two steps: 1) Event Eventification, 2) Events Structuralization.</p><p>Event Eventification: Following <ref type="bibr" target="#b4">(Do et al., 2011;</ref><ref type="bibr" target="#b1">Asghar, 2016;</ref><ref type="bibr" target="#b11">Luo et al., 2016;</ref><ref type="bibr" target="#b6">Hassanzadeh et al., 2019)</ref>, we make use of a few highprecision causal connectives to extract cause-effect sentence pairs, for example 'because', 'as a result', etc. Then we extract causal event pairs from causal sentence pairs based on dependency analysis 1 . We adopt the commonly used 4-tuple event representation (s, v, o, m) <ref type="bibr" target="#b16">(Pichotta and Mooney, 2016)</ref> where v denotes the verb, s denotes the head noun of the subject, o denotes the head noun of the direct object or the adjective, and m denotes the head noun of the prepositional or indirect object.</p><p>Events Structuralization: We structuralize the extracted causal event pairs into an event causality network, in which semantically similar events are clustered together. We use event abstractions to judge whether two events are semantically similar. The abstraction of an event is obtained by generalizing its components to their categories in linguistic resources. Specifically, the verb in each event is generalized to its class in VerbNet <ref type="bibr" target="#b20">(Schuler, 2005)</ref>. The other components are generalized by the WordNet <ref type="bibr" target="#b13">(Miller, 1995)</ref> synset two levels up in the inherited hypernym hierarchy. In addition, we explicitly use the semantic-similarity based inferring rule. For example, assume we have observed that A has the same abstraction with B, and a causal relation holds from A to C, then it is most likely to conclude that there may be a causal relation from B to C. Such a manipulation significantly reduces the sparsity of causalities in the event causality network, and hence supports better reasoning about the effect events. The weight of an edge in our event causality network is derived by the following rules:</p><p>1) If the edge between the event pair (e i , e j ) is extracted from the dataset, the weight w ij of this edge is w ij = 1;</p><p>2) If the edge of (e i , e j ) is inferred based on the semantic-similarity between (e i , e k ) and the causal relation between (e k , e j ), we have w ij = sim(e i , e k ), where sim(e i , e k ), calculated by the path-similarity measure in WordNet, is the semantic-similarity score between e i and e k .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Effect Generation</head><p>Task Discription: The goal of effect generation consists of predicting the an effect event for the input cause and rewriting the predicted effect event 1 <ref type="url" target="https://spacy.io/">https://spacy.io/</ref>  Effect Event Predictor: Given the cause sentence X, a bidirectional GRU model <ref type="bibr" target="#b3">(Cho et al., 2014)</ref> is used to reads the sequence X from both directions and computes hidden states</p><formula xml:id="formula_0">-→ h x i and ← - h x i for the token x i . The final hidden vectors of X is H X = {h x 1 , • • • , h xm }, where h x i = [ -→ h x i ; ← - h x i ].</formula><p>We then eventify the cause event from X, and match the event abstraction in the event causality network. Once the abstraction is matched, a L-hop causal-related subgraph CG is preserved. The neighborhood information in CG represents the causality tendencies, which are useful for reasoning the most reasonable effect event. We use a simple graph neural network (GNN) <ref type="bibr" target="#b7">(Kipf and Welling, 2016;</ref><ref type="bibr" target="#b22">Veličković et al., 2017)</ref> to capture the neighborhood information. Specifically, the lth layer's vectors of e i and its neighbors are pooled to obtain the vector of e i on the (l + 1)-th layer with a activation function σ (ReLU by default):</p><formula xml:id="formula_1">z l i = W l e l i e l+1 i = σ( N CG j=1 exp(w ij (z l i T • z l j ) k exp(w kj (z l k T • z l j ) z l j ),<label>(1)</label></formula><p>where W l is a parameter, • denotes the inner product of the two vectors, w ij is the weight of the edge (e i , e j ), e l i is the vector of e i at l-th layer, e 0 i = [e s i ; e v i ; e o i ; e m i ] is the concated word embedding of all components of e i .</p><p>The final hidden vector e L i (i = 1, • • • , N CG ) of events are used to select the guided effect event e Y by e Y = max i cs i , where</p><formula xml:id="formula_2">cs i = e L i T • h X is the causal score between each candidate event e i and X, h X = 1 m m k=1 h x k is the mean-pooling representation of X.</formula><p>Effect Event Rewriter: The predicted e Y contains the skeleton information, we want retain all tokens of e Y when generating the effect sentence to avoiding the causal information carried by e Y degrading to word-level. Inspired by <ref type="bibr" target="#b14">(Mou et al., 2016;</ref><ref type="bibr" target="#b12">Martin et al., 2018)</ref>, we rewrite e Y = (s, v, o, m) into the effect sentence which conforms to the format of <ref type="bibr">[_s][_v][_o][_m]</ref>, where blanks indicate the place words should be added to in order to make a sentence richer in content. We use a decoder with attention mechanism <ref type="bibr" target="#b2">(Bahdanau et al., 2014)</ref> to generate words in each blank until generating the "&lt;eos&gt;" token.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>English Wikipedia(Enwiki): We extract causeeffect sentence pairs from the English Wikipedia corpus<ref type="foot" target="#foot_0">foot_0</ref> , resulting in about 80K pairs. We split all pairs into training/validation/test with the ratio of 8:1:1, and tune parameters on the validation data.</p><p>The training data is used to construct the event causality network. We retrieve 2-hop causal subgraphs according to input cause sentences because it is the most commonly used setting. The percentage of the test samples whose gold effect events exist in the retrieved causal subgraphs is 70.8%.</p><p>COPA Benchmark: The Choice of Plausible Alternatives (COPA) <ref type="bibr" target="#b19">(Roemmele et al., 2011)</ref> dataset consists of 1,000 multiple-choice questions (500 for validation and 500 for testing) requiring causal reasoning in order to answer correctly. Each question is composed of a premise and two alternatives, and the task is to select a more plausible alternative as a cause (or an effect) of the premise. We use the most plausible alternative and its premise to collect cause-effect sentence pairs. The COPA causes are used to retrieve causal subgraphs from our event causality network, leading to 186 COPA pairs with their corresponding causal subgraphs. The percentage of the samples whose gold effect events exist in causal subgraphs is 11.2%. Because there is no released training data for the COPA task, we train all models on Enwiki and evaluate them on COPA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baselines and Evaluation</head><p>Baselines: We compare our method with state-ofthe-art text generation methods, including GPT2 <ref type="bibr" target="#b17">(Radford et al., 2019)</ref>, BART <ref type="bibr" target="#b8">(Lewis et al., 2019)</ref>, CopyNet <ref type="bibr" target="#b25">(Zhu et al., 2017)</ref> and CausalBERT <ref type="bibr" target="#b10">(Li et al., 2020)</ref>. Details can be seen in Appendix A.</p><p>Metrics: For automatic evaluation, we use metrics including BLEU-4 <ref type="bibr" target="#b15">(Papineni et al., 2002)</ref>, Distinct-n <ref type="bibr" target="#b9">(Li et al., 2015)</ref> to evaluate the generated effect sentences. Abstraction-Matching (AbsMat) evaluates the percentage of the generated effect sequences that have the same abstraction as the corresponding gold effect sequences.</p><p>For the manual evaluation, we examine whether the generated sequence is a plausible effect of the input, which is denoted as plausibility (Plau). Details can be seen in Appendix B.</p><p>Result: The result is shown in Table <ref type="table" target="#tab_0">1</ref>, where EGCER achieves the best results. BART performs better than GPT2 due to the adopted encoderdecoder architecture. Based on the event skeletons provided by the effect event predictor, CopyNet and EGCER are aware of the topic which should be generated, and hence perform better than BART and GPT2. CopyNet performs worse than EGCER because CopyNet cannot cover all tokens of the retrieved event, as a result, the causal information in the generated sequence is incomplete. We also find that CopyNet tends to copy an event token repeatedly. CausalBert performs worse than EGCER because it is based on the word-level causal analysis, which can also be found in Section 4.3. Given the effect event, EGCER sees a more complete scenario, hence generate a more reasonable effect sentence.</p><p>The result of the manual evaluation is also shown in Table <ref type="table" target="#tab_0">1</ref>. As for EGCER, we find that it may sometimes generate negation expressions or grammatical errors, as a result, the generated sequence is not a plausible effect even if the retrieved event is plausible. The proportion of the generated sequences in this case is about 21%. We speculate that the errors in data preprocessing and the insufficiently powerful generator are the possible reasons. In the future, we will further improve generators in order to generate more high-quality effect sentences. It can also be found that EGCER performs far worse on COPA than on Enwiki, this is because a great gap exists between these two datasets. However, EGCER is still superior to any other model, which demonstrates event-level causal reasoning contributes to the effect sentence generation.  Appendix C presents a case with generations of different models. CausalBERT generates "missing bus" given "missing" as guidance. However, from the input we can see that this person may be in a car, therefor the generated sequence is not an effect. That is CausalBERT, which is based on the word-level analysis, generates causal inconsistent sequence. In contrast, our method successfully predicts the expected effect event "(he,missed,meeting)", and generates the correct effect sentence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Visualization</head><p>We extract a part of CG according to the input cause, and visualize the causal scores cs using event vectors on the first and second layers of GNN respectively, as shown in Figure <ref type="figure" target="#fig_2">2a</ref> and<ref type="figure" target="#fig_2">2b</ref>. In Figure <ref type="figure" target="#fig_2">2a</ref>, the "(was, late, work)" receives the highest score, followed by "(he, encountered, jam)" and "(was, late, meeting)" in one-hop reasoning. And, the "(leader, scolded, him)" receives the lowest score. Noted that "(he, encountered, jam)" is actually not an effect event. However, in Figure <ref type="figure" target="#fig_2">2b</ref>, the "(he, missed, meeting)" receives the highest score, followed by "(was, late, work)", "(was, late, meeting)" and "(leader, scolded, him)" in two-hop reasoning. The "(he, encountered, jam)" and "(rain, is, heavy)" receive lower scores. This makes sense because they are not effect events at all. This shows that the multi-layer GNN can well capture multihop causal relationships and thus are able to select the plausible effect events.  To understand the importance of the key components of our approach, we perform an ablation study by training multiple ablated versions of our model, including the one without weights of edges in the retrieved causal subgraph, the one without the 2nd-layer of GNN, and the one without GNN. The results are provided in Table <ref type="table" target="#tab_2">2</ref>. When the GNN module is gradually ablated, the performance of the model gradually degrades. This demonstrates that all modules of our multi-layer GNN effectively contribute to effect sentence generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>We present an event-level causal reasoning based effect generation method to generate the plausible effect sentences for the input cause sentences. Experiments show that our method performs better than competitors in capturing the causal semantics which should be generated. In the future, we would like to develop more effective approaches to enhance the effect event reasoning, and more powerful generators to generate the effect sentences with higher quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Experiment Setting</head><p>We concat cause-effect sentence pairs and finetune GPT2(117M) in a language model setting. BART is finetuned with the encoder-decoder setting. Both GPT2 and BART are implemented by transformers 3 . CopyNet employs the copy mechanism which either copies tokens from the retrieved event or generates words from the vocabulary. CausalBERT employs the lexically-constrained beam-search to generate possible effects for provided word guidance. ConceptNet <ref type="bibr" target="#b21">(Speer and Havasi, 2012)</ref> is used to retrieve causal relevant constraints for Causal-BERT.</p><p>Our effect event predictor consists of a 2-layer bidirectional GRU for encoding input sequences and a 2-layer GNN for updating event representations. Our event rewriter is a GRU decoder. The predictor and the rewriter do not share parameters, and their hidden sizes are set to 512. The word embedding size is 300. We use the Adam optimizer with the mini-batch size of 96. The learning rate is 0.001.</p><p>We use the gold effect event to supervise our event predictor. The objective is:</p><formula xml:id="formula_3">J 1 = -log p(e Y |X, CG).</formula><p>(2)</p><p>For our event rewriter, the objective is to maximize the estimated probability of the gold effect sequence:</p><formula xml:id="formula_4">J 2 = P (Y |e Y , X) = t -log p(y t |y &lt;t ).<label>(3)</label></formula><p>The final loss function is the combination of the above two J = J 1 + J 2 (4)</p><p>B Details for Manual Evaluation 100 samples are randomly selected from the Wikipedia test set and COPA, respectively, and distribute them to the two graduate students from the NLP field. Each student is asked to give a score from {0, 0.5, 1} for the (input, generation) pair, given the following guidelines. Assign 0 to the pair if the generation can never be considered as a possible effect of the input, assign 0.5 to the pair if the generation is a possible effect of the input but has certain grammatical errors and assign 1 to the pair if the generation is a possible effect of the 3 <ref type="url" target="https://huggingface.co/">https://huggingface.co/</ref> input and there is no grammatical error. We average scores over the two annotators. The cohen's kappa scores on Enwiki and COPA are 0.65 and 0.63, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Generation Example</head><p>Input cause he encountered a heavy traffic jam.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>GPT2</head><p>the lighthouse was closed over three weeks.</p><p>BART he was delayed for over an hour.</p><p>CopyNet he missed missed the meeting.</p><p>CausalBert causing him to miss bus.</p><p>EGCER he missed the important meeting.</p><p>Table <ref type="table">3</ref>: A case with generations of different models.</p><p>Given the input cause, CauseBERT generates the unexpected sequences by using "missing" as constraint, which demonstrates that word-level causal analysis is not always self-contained. CopyNet repeatedly generates the "missed" token. EGCER rewrites the predicted effect event "(he, missed, meeting)" into the reasonable effect sentence.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The overview of EGCER.</figDesc><graphic coords="2,313.23,70.87,204.08,80.35" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>(a) The causal scores calculated using the event vectors on the first layer of GNN.(b) The causal scores calculated using the event vectors on the second layer of GNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The darker blue indicates the higher causal score.</figDesc><graphic coords="4,184.80,289.39,99.77,50.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Automatic and manual evaluation results.</figDesc><table><row><cell>Model</cell><cell></cell><cell>EnWiki</cell><cell></cell><cell>COPA</cell><cell></cell></row><row><cell></cell><cell cols="5">BLEU-4 Distinct-1/2 AbsMat Plau BLEU-4 Distinct-1/2 AbsMat Plau</cell></row><row><cell>GPT2</cell><cell>0.69</cell><cell>5.57/16.82</cell><cell>0.3 0.08</cell><cell>1.35 22.61/44.25</cell><cell>0.2 0.02</cell></row><row><cell>BART</cell><cell>1.28</cell><cell>8.23/24.83</cell><cell>1.7 0.11</cell><cell>1.22 22.37/43.71</cell><cell>0.5 0.04</cell></row><row><cell>CausalBERT</cell><cell>0.74</cell><cell>5.33/22.23</cell><cell>8.5 0.12</cell><cell>0.92 22.39/52.56</cell><cell>3.7 0.06</cell></row><row><cell>CopyNet</cell><cell cols="2">2.85 10.63/39.82</cell><cell>16.4 0.17</cell><cell>1.18 32.74/75.17</cell><cell>2.6 0.04</cell></row><row><cell>EGCER(ours)</cell><cell cols="2">4.90 13.99/43.58</cell><cell>26.4 0.27</cell><cell>1.74 48.08/83.97</cell><cell>5.3 0.07</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 2 :</head><label>2</label><figDesc>Ablation study on the Enwiki testset.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>https://dumps.wikimedia.org/enwiki/20201020/enwiki-20201020-pages-articles.xml.bz2</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="6">Acknowledgements</head><p>The work described in this paper was supported by and <rs type="funder">Research Grants Council of Hong Kong</rs>(<rs type="grantNumber">PolyU/5210919</rs>, <rs type="grantNumber">PolyU/15207920</rs>, <rs type="grantNumber">PolyU/15207821</rs>), <rs type="funder">National Natural Science Foundation of China</rs> (<rs type="grantNumber">61672445</rs>, <rs type="grantNumber">62076212</rs>, <rs type="grantNumber">62076072</rs>)   and PolyU internal grants (<rs type="grantNumber">ZVQ0</rs>). We are grateful to the anonymous reviewers for their valuable comments.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_3Z2gNMv">
					<idno type="grant-number">PolyU/5210919</idno>
				</org>
				<org type="funding" xml:id="_BJCHDW5">
					<idno type="grant-number">PolyU/15207920</idno>
				</org>
				<org type="funding" xml:id="_qvCVxVP">
					<idno type="grant-number">PolyU/15207821</idno>
				</org>
				<org type="funding" xml:id="_5fkQBjB">
					<idno type="grant-number">61672445</idno>
				</org>
				<org type="funding" xml:id="_kc4PNN3">
					<idno type="grant-number">62076212</idno>
				</org>
				<org type="funding" xml:id="_efHfwHr">
					<idno type="grant-number">62076072</idno>
				</org>
				<org type="funding" xml:id="_ZfAefZQ">
					<idno type="grant-number">ZVQ0</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Story realization: Expanding plot events into sentences</title>
		<author>
			<persName><forename type="first">Prithviraj</forename><surname>Ammanabrolu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ethan</forename><surname>Tien</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wesley</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaochen</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lara</forename><forename type="middle">J</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">O</forename><surname>Riedl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7375" to="7382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Automatic extraction of causal relations from natural language texts: a comprehensive survey</title>
		<author>
			<persName><forename type="first">Nabiha</forename><surname>Asghar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07895</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.1078</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Minimally supervised event causality identification</title>
		<author>
			<persName><forename type="first">Quang</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yee</forename><surname>Seng Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2011 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="294" to="303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Toward future scenario generation: Extracting event causality exploiting semantic relation, context, and association features</title>
		<author>
			<persName><forename type="first">Chikara</forename><surname>Hashimoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kentaro</forename><surname>Torisawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julien</forename><surname>Kloetzer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Motoki</forename><surname>Sano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">István</forename><surname>Varga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong-Hoon</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutaka</forename><surname>Kidawara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="987" to="997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Answering binary causal questions through large-scale text mining: An evaluation using cause-effect pairs from human experts</title>
		<author>
			<persName><forename type="first">Oktie</forename><surname>Hassanzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Debarun</forename><surname>Bhattacharjya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Feblowitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kavitha</forename><surname>Srinivas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Perrone</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirin</forename><surname>Sohrabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Katz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5003" to="5009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Semisupervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal ; Abdelrahman Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ves</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.13461</idno>
		<imprint>
			<date type="published" when="2019">Marjan Ghazvininejad,. 2019</date>
			<pubPlace>Bart</pubPlace>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">Jiwei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.03055</idno>
		<title level="m">A diversity-promoting objective function for neural conversation models</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Guided generation of cause and effect</title>
		<author>
			<persName><forename type="first">Zhongyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Van Durme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Commonsense causal reasoning between short texts</title>
		<author>
			<persName><forename type="first">Zhiyi</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Seung-Won</forename><surname>Kenny Q Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongyuan</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KR</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="421" to="431" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Event representations for automated story generation with deep neural nets</title>
		<author>
			<persName><forename type="first">Prithviraj</forename><surname>Lara J Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Ammanabrolu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shruti</forename><surname>Hancock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brent</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><forename type="middle">O</forename><surname>Harrison</surname></persName>
		</author>
		<author>
			<persName><surname>Riedl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Wordnet: a lexical database for english</title>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">A</forename><surname>Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="39" to="41" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Sequence to backward and forward sequences: A content-introducing approach to generative short-text conversation</title>
		<author>
			<persName><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiping</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ge</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi</forename><surname>Jin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.00970</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Bleu: a method for automatic evaluation of machine translation</title>
		<author>
			<persName><forename type="first">Kishore</forename><surname>Papineni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Salim</forename><surname>Roukos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Todd</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Jing</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th annual meeting of the Association for Computational Linguistics</title>
		<meeting>the 40th annual meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="311" to="318" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning statistical scripts with lstm recurrent neural networks</title>
		<author>
			<persName><forename type="first">Karl</forename><surname>Pichotta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raymond</forename><forename type="middle">J</forename><surname>Mooney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirtieth AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">OpenAI blog</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning causality for news events prediction</title>
		<author>
			<persName><forename type="first">Kira</forename><surname>Radinsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sagie</forename><surname>Davidovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shaul</forename><surname>Markovitch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st international conference on World Wide Web</title>
		<meeting>the 21st international conference on World Wide Web</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="909" to="918" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Choice of plausible alternatives: An evaluation of commonsense causal reasoning</title>
		<author>
			<persName><forename type="first">Melissa</forename><surname>Roemmele</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cosmin</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Bejan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">S</forename><surname>Gordon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI spring symposium: logical formalizations of commonsense reasoning</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="90" to="95" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Verbnet: A broadcoverage</title>
		<author>
			<persName><forename type="first">Karin Kipper</forename><surname>Schuler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
	<note>comprehensive verb lexicon</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Representing general relational knowledge in conceptnet 5</title>
		<author>
			<persName><forename type="first">Robyn</forename><surname>Speer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Catherine</forename><surname>Havasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC&apos;12)</title>
		<meeting>the Eighth International Conference on Language Resources and Evaluation (LREC&apos;12)</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="3679" to="3686" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Lio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10903</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">Graph attention networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Distributed representation of words in cause and effect spaces</title>
		<author>
			<persName><forename type="first">Zhipeng</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feiteng</forename><surname>Mu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="7330" to="7337" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Constructing and embedding abstract event causality networks from text snippets</title>
		<author>
			<persName><forename type="first">Sendong</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sean</forename><surname>Massung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengxiang</forename><surname>Zhai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth ACM International Conference on Web Search and Data Mining</title>
		<meeting>the Tenth ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="335" to="344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<author>
			<persName><forename type="first">Wenya</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaixiang</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhangbin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezheng</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.04264</idno>
		<title level="m">Flexible end-to-end dialogue system for knowledge grounded conversation</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
