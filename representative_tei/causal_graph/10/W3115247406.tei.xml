<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
				<funder ref="#_kH4JtPw">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_39R2DAB">
					<orgName type="full">National Science Foundation</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Department of Information Systems</orgName>
								<orgName type="institution" key="instit1">University of Maryland</orgName>
								<orgName type="institution" key="instit2">Baltimore County</orgName>
								<address>
									<settlement>Baltimore</settlement>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Department of Electrical Engineering and Computer Science</orgName>
								<orgName type="institution">Catholic University of America</orgName>
								<address>
									<settlement>Washington</settlement>
									<region>D.C</region>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:30+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Causality discovery</term>
					<term>Ensemble learning</term>
					<term>Data parallelism</term>
					<term>Granger causality</term>
					<term>Dynamic Bayesian Network</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Causality discovery mines cause-effect relationships among different variables of a system and has been widely used in many disciplines including climatology and neuroscience. To discover causal relationships, many data-driven causality discovery methods, e.g., Granger causality, PCMCI and Dynamic Bayesian Network, have been proposed. Many of these causality discovery approaches mine time series data and generate a directed causality graph where each graph edge denotes a causeeffect relationship between the two connected graph nodes. Our benchmarking of different causality discovery approaches with real-world climate data show these approaches often generate quite different causality results with the same input dataset due to their internal learning mechanism differences. Meanwhile, there are ever-increasing available data in virtually every discipline, which makes it more and more difficult to use existing causality discovery algorithms to produce causality results within reasonable time. To address these two challenges, this paper utilizes data partitioning and ensemble techniques, and proposes a twophase hybrid causality ensemble framework. The framework first conducts phase 1 data ensemble for partitioned data and then conducts phase 2 algorithm ensemble from data ensemble results. To achieve scalability, we further parallelize the ensemble approaches via the Spark big data analytics engine. Our experiments show that our proposed approaches achieve good accuracy through ensemble and high scalability through dataparallelization in distributed computing environments.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Scalable and Hybrid Ensemble-Based Causality Discovery</head><p>Pei Guo * , Achuna Ofonedu † , Jianwu Wang *</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>I. INTRODUCTION</head><p>Causality <ref type="bibr" target="#b20">[21]</ref> is a fundamental research topic studying cause-effect relationships among different components of a system and causality study can help explain why the system has certain behaviors. Causality learning/discovery has been widely studied and applied in many disciplines including climatology and neuroscience.</p><p>Many data-driven causality learning approaches have been proposed, such as Granger causality <ref type="bibr" target="#b11">[12]</ref>, PCMCI <ref type="bibr" target="#b23">[24]</ref>, Dynamic Bayesian Network <ref type="bibr" target="#b18">[19]</ref>, and Convergent Cross Mapping <ref type="bibr" target="#b31">[32]</ref>. These approaches often mine time series data of two or more variables in a system and produce their predictions on cause-effect relationship among these variables. For instance, the work at <ref type="bibr" target="#b25">[26]</ref> uses Granger causality to study cause-effect relationships among multiple climate variables and shows that sea surface temperature changes at pacific ocean near equator, an indicator of the El Niño-Southern Oscillation (ENSO) climate phenomenon <ref type="bibr" target="#b12">[13]</ref> can cause abnormal surface temperature, pressure and precipitation remotely.</p><p>One challenge with the variety of different causal discovery approaches/algorithms is that these approaches often lead to divergent causality conclusions from the same dataset, which makes it difficult to explain and use data-driven causality discovery results. There have been some studies comparing different causality discovery methods <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b32">[33]</ref>. For example, the experiments on comparing three causality discovery algorithms show there are only 83% overlapping among the results on average <ref type="bibr" target="#b14">[15]</ref>. Yet there is still a lack of comprehensive framework to effectively integrate these diverse algorithms.</p><p>The other challenge to be tackled by this paper is the ever-increasing volume and dimension of available data for causality discovery. For instance, total worldwide climate data volume is projected to increase from 5 PB in 2010 to 350 PB in 2030 <ref type="bibr" target="#b19">[20]</ref>. It is more and more difficult to use existing causality discovery algorithms to handle the increasing dimensionality and resolution of these climate datasets. Meanwhile, data volume is just one factor for time complexity of many causality discovery algorithms. As an example, a popular Granger causality algorithm's execution time grows quadratically with the increase of either of the three factors: data record number, variable number and time lag number <ref type="bibr" target="#b3">[4]</ref>. Parallel causality discovery is crucial as a solution to reduce computation time.</p><p>To address the above two challenges, this paper applies data partitioning and ensemble techniques to achieve scalable and accurate causality learning. Ensemble learning <ref type="bibr" target="#b22">[23]</ref> is a meta machine learning algorithm which combines multiple base or individual learners in order to get better overall learning accuracy. In this paper, we propose a two-phase hybrid causality ensemble learning framework by first partitioning data into smaller sizes and conducting phase 1 data ensemble for each data partition and then conducting phase 2 algorithm ensemble from phase 1 ensemble results. The framework can be easily parallelized through big data engines like Spark <ref type="bibr" target="#b0">[1]</ref> and is adaptable to different ensemble approaches. To the best of our knowledge, this study is the first supporting both scalable and ensemble learning for causality discovery. The implementations of our work is open-sourced at <ref type="bibr" target="#b1">[2]</ref>.</p><p>The contributions of this paper are as follows.</p><p>• We propose a two-phase hybrid causality ensemble framework by first conducting phase 1 data ensemble for partitioned data and then conducting phase 2 algorithm ensemble from phase 1 data ensemble results. The framework can combine learning results from different data partitions (namely data ensemble), and different algorithms (namely algorithm ensemble). • Based on the above framework, we propose an approach for parallel causality ensemble learning via Spark <ref type="bibr" target="#b0">[1]</ref> and the MapReduce programming model <ref type="bibr" target="#b9">[10]</ref>. • We did experiments to evaluate our proposed scalable ensemble framework and approach, which shows that our approach can achieve both perfect accuracy and almost linear speedup.</p><p>The rest of the paper is organized as follows. The background is introduced in Section II. The two-phase hybrid causality ensemble learning framework is explained in Section III. Section IV contains the ensemble approach based on the scalable causality ensemble framework. Section V describes the parallelization of our implementation. The experiments and evaluations are in Section VI, with related work discussion in Section VII. Finally, Section VIII concludes our paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>II. BACKGROUND</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Ensemble Learning</head><p>Ensemble learning <ref type="bibr" target="#b22">[23]</ref> is a meta machine learning algorithm which uses multiple learning methods to obtain better predictive performance than learning from any of the constituent methods. Since 1990, ensemble learning methods have become a major learning paradigm because of both empirical good performances in real-world applications and theoretical proof on its advantages <ref type="bibr" target="#b24">[25]</ref>. Many state-of-art data mining approaches/packages, e.g. random forest and XGBoost <ref type="bibr" target="#b7">[8]</ref>, are based on ensemble learning.</p><p>Many ensemble learning algorithms have been proposed and they mainly vary in the following three aspects: 1) what are base/individual learners, 2) how each base learner learns from input data, 3) how to combine results of base learners. For base learner selection, if base learners used in an ensemble learning belong to the same type, e.g. decision tree or neural network, the ensemble algorithm is called homogeneous ensemble. Otherwise, it is called heterogeneous ensemble. Regarding how each base learner learns, there are three main approaches and they mostly differ in how input data is fed to base learner. The first approach, called stacking ensemble <ref type="bibr" target="#b30">[31]</ref>, uses the same input data for all base learners. Bagging ensemble <ref type="bibr" target="#b5">[6]</ref>, as the second approach, uses different sampling results from the original input data for different base learners. The third approach is boosting ensemble <ref type="bibr" target="#b10">[11]</ref> which uses multiple base learners iteratively and, in each iteration, assigns higher weight to data whose learning accuracy was low in previous iterations. On base learner combination, common methods are majority voting and weighted majority voting <ref type="bibr" target="#b22">[23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Causality Discovery Methods</head><p>Existing causal relationships discovery methods can be categorized into two types depending on the input datasets types: 1) learning from multivariate independent and identically distributed (i.i.d.) data and 2) learning from multivariate time-series data. The learning results from a multivariate causality approach can be denoted as a directed graph where each graph edge represents a cause-effect relationship conditioned on all other variables in the graph. In this subsection, we explain three multivariate causality discovery approaches towards time-series input data, namely multivariate (graphical) Granger causality <ref type="bibr" target="#b3">[4]</ref>, PCMCI <ref type="bibr" target="#b23">[24]</ref> and dynamic Bayesian network <ref type="bibr" target="#b18">[19]</ref> and their algorithm details. Because they all belong to the same casualty discovery category and their learning results can be modeled as directed graphs, we could conduct ensemble learning using these algorithms as base learners which will be explained in later sections. (2) Next, the accuracy of predicting y t using Equation (1) and Equation (2) are compared to check which regression works better. In most cases, statistical hypothesis test methods such as F -test or Chi-squared (χ 2 ) test are utilized to get a p-value to determine statistical significance.</p><p>The above pairwise Granger causality is proven to work well on discovery between each pair of variables. However, most datasets in research contain more than two variables. When the scientists intend to discover the causality among a subset or the whole set of a multivariate dataset, pairwise Granger causality ignores the causalities with other untested variables, which could generate spurious causal relationships such as confounding variable <ref type="bibr" target="#b21">[22]</ref> and indirect causal relationship <ref type="bibr" target="#b17">[18]</ref>.</p><p>To address the limitations of the pairwise Granger causality method, multivariate Granger causality discovery, a.k.a. graphical Granger causality discovery, fits a vector autoregressive model (VAR) to time series data <ref type="bibr" target="#b15">[16]</ref>, compared to linear regression models in pairwise Granger causality. To demonstrate multivariate Granger causality model, we denote X P l=1 as lagged variables of time series variable x from time lag 1 to maximum lag P , and similarly Y P l=1 from y, Z P l=1 from z. The joint VAR model for multivariate Granger causality is shown as as follows:</p><formula xml:id="formula_0">y t = A 1 • Y P l=1 + B 1 • X P l=1 + ε 1t x t = C 1 • X P l=1 + D 1 • Y P l=1 + ε 2t<label>(3)</label></formula><p>with the prediction error covariance matrix being:</p><formula xml:id="formula_1">CovM atrix = var(ε 1t ) cov(ε 1t , ε 2t ) cov(ε 2t , ε 1t ) var(ε 2t )<label>(4)</label></formula><p>Besides lagged variables X P l=1 and Y P l=1 , when a new variable z is taken into account, the new VAR model is:</p><formula xml:id="formula_2">   y t = A 2 • Y P l=1 + B 2 • Z P l=1 + C 2 • X P l=1 + ε 3t z t = D 2 • Y P l=1 + E 2 • Z P l=1 + F 2 • X P l=1 + ε 4t x t = G 2 • Y P l=1 + H 2 • Z P l=1 + I 2 • X P l=1 + ε 5t<label>(5)</label></formula><p>Correspondingly, the prediction error covariance matrix of VAR model in ( <ref type="formula" target="#formula_2">5</ref>) is:</p><formula xml:id="formula_3">Σ =   var(ε 3t ) cov(ε 3t , ε 4t ) cov(ε 3t , ε 5t ) cov(ε 4t , ε 3t ) var(ε 4t ) cov(ε 4t , ε 5t ) cov(ε 5t , ε 3t ) cov(ε 5t , ε 4t ) var(ε 5t )  <label>(6)</label></formula><p>The next step, similar to the pairwise Granger causality testing, is to test whether introducing z can improve the prediction of y and how significant the improvement is. From the VAR model in Equation ( <ref type="formula" target="#formula_0">3</ref>) of variable y and x, and the VAR model in Equation ( <ref type="formula" target="#formula_2">5</ref>) of variable y, z, and x, the conditional Granger causality test from z to y conditioned on x, denoted as (z → y|x), is:</p><formula xml:id="formula_4">F -test(var(ε 1t ), var(ε 3t ))<label>(7)</label></formula><p>From F -test in Equation ( <ref type="formula" target="#formula_4">7</ref>), a p-value can be used to compare with a threshold to conclude whether z Granger causes y conditioned on x.</p><p>2) PCMCI: PCMCI is a causal discovery method described in <ref type="bibr" target="#b23">[24]</ref> which identifies relevant variables for conditioning and estimates causality graph from time series data. The method makes use of a "time series graph" made of nodes representing the state variables at different time-lags. If the time lag is denoted by l, a causal link is notated x t-l -→ y t , and this link exists if x t-l is not conditionally independent of y t given the past of all variables. Assuming the causal structure does not change over time, the same links are present at each time step.</p><p>The parents P(x) of a variable x are defined as the set of all nodes with a link towards x. However, estimating these parents directly by testing for conditional independence on the whole past is problematic due to high-dimensionality and because conditioning on irrelevant variables leads to biases.</p><p>PCMCI estimates causal links by a two-step procedure <ref type="bibr" target="#b23">[24]</ref>: 1. Condition-selection: For every variable α, estimate a superset of parents P(α t ) with an iterative Markov discovery algorithm <ref type="bibr" target="#b26">[27]</ref> such as P C 1 algorithm. The condition-selection step reduces the dimensionality and avoids conditioning on irrelevant variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Momentary conditional independence (MCI):</head><p>To test whether x t-l -→ y t with MCI, it evaluates:</p><formula xml:id="formula_5">x t-l ⊥ y t | P(y t ), P(x t-l )<label>(8)</label></formula><p>Equation ( <ref type="formula" target="#formula_5">8</ref>) checks momentary conditional independence conditions between x t-l and y t , and checks whether or not x t-l and y t are not conditionally independent given P(y t ) and P(x t-l ).</p><p>3) Dynamic Bayesian Network: Bayesian network <ref type="bibr" target="#b4">[5]</ref> is one of many probabilistic graphical models which consists of a directed acyclic graph (DAG) and conditional probability distributions (CPDs) associated with each node in the model. A Bayesian network can be used to make predictions and decisions under uncertainty. A dynamic Bayesian network <ref type="bibr" target="#b18">[19]</ref> is similar to a Bayesian network but with a temporal extension, making it an appropriate graphical model to use for temporal datasets. The two main steps to creating a probabilistic graphical model are structure learning and parameter learning.</p><p>In this paper, we adopt the approach in <ref type="bibr" target="#b32">[33]</ref> for dynamic Bayesian network learning. The approach first expands variable set by adding new variables for each original variable through time lagging. For instance, P new variables can be created from original variable x: x t-i for i from 1 to maximum lag P . With the expanded variable set, the K2 algorithm <ref type="bibr" target="#b8">[9]</ref> is used to search through all possible causality graph structures and identify which structure has the highest possibility to produce the data. In this score-based structure learning approach, Bayesian information criterion (BIC) scoring is used. Next, after causality graph is generated for expanded variable set, the causality graph is simplified by removing lagged variable and combining the causality edges. For instance, two edges x t-2 -→ y t-1 and x t-3 -→ y t are combined to one edge x -→ y in the final graph.</p><p>Moreover, for the sake of computational time, the time series data is partitioned into bins. Each bin defines a set of sub ranges, then the data is assigned to each labeled bin. For example, if the lowest value of the dataset is -5, and the highest value is 5. With the total bin number 10, a value of 1.2351 can be placed in a bin labeled 7, whose range is <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2)</ref>. This approach increases the state counts of each variable and allows for faster computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>III. A TWO-PHASE HYBRID CAUSALITY ENSEMBLE FRAMEWORK</head><p>To deal with both increasing volume of available input data and increasing variety of available causality discovery algorithms, we propose a two-phase hybrid causality ensemble framework to achieve ensemble of both multiple causality discovery algorithms as base learners and multiple data partitions as base learner's input data. Before diving into the details of this two-phase ensemble framework, we first explain how ensemble could be done with only data ensemble and algorithm ensemble. We note most causality discovery algorithms generate not only cause-effect relationships, but also time lag and probability of each relationship. In this paper, we only focus on structure causality ensemble, namely how multiple directed graphs can be combined into one, and leave the time lag and probability ensemble for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Algorithm Ensemble for Causality Discovery</head><p>Algorithm ensemble approach deals with algorithm variety by applying different causality discovery algorithms as base learners with the same input data and later combining all base learner results. Each causality discovery algorithm mines the same time series dataset and produces its own directed graph where nodes denote time series variables and each directed edge denotes a cause-effect relationship between the two connected variables. Because each base learner works on the same input data, the nodes of result graphs are the same for different base learners. But different base learners could produce different causality edges. Then by applying a certain base learner combination method, such as majority voting, we can derive a new directed graph as ensemble result. The nodes in the ensemble graph are the same with the results in each base learner. For graph edges, we can iterate all possible edges of the graph and decide whether this edge should be in the ensemble graph by combining corresponding edges in base learner graph result. If we use majority voting as combination method, an edge will be in ensemble graph only if the edge appears in more than half of base learner graphs.</p><p>By applying algorithm ensemble, the ensemble result is often more accurate than utilizing only one single causality discovery algorithm. However, when the size of input time series dataset gets larger, the execution time of algorithm level ensemble increases dramatically because every base learner will take longer time to finish. Thus, a non-scalable algorithm ensemble approach is not enough to meet the challenge of dealing with the increasing data size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Data Ensemble for Causality Discovery</head><p>Data ensemble approach deals with data volume challenge by first partitioning data into smaller datasets, then using the same causality discovery algorithm as base learners with data partitions, and later combining all base learner results. Data partitioning is often done horizontally, not vertically, so that each data partition can still have all variables needed for multivariate causality learning. Because input data are often time series, data partitioning can be easily done by splitting the overall time ranges into smaller time ranges. Similar to algorithm ensemble, the nodes of resulting causality graph are the same for different base learners and edges of the graphs might be different. Then we can derive ensemble graph using the same base learner combination method in the previous subsection. The limitation of this approach is that it does not deal with variety of causality learning algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Two-Phase Hybrid Data-Algorithm Ensemble for Causality Discovery</head><p>To address the challenges of diverse causality discovery results and increasing data size, we further integrate data ensemble and algorithm ensemble into one framework as illustrated in Figure <ref type="figure" target="#fig_0">1</ref>, which conducts two-phase hybrid ensemble. We implement this generic framework as dataalgorithm ensemble, which means it conducts data ensemble first in phase 1 and then algorithm ensemble in phase 2. In the causality ensemble framework, the input data is first partitioned into different data slices from 1 to N . Then, phase 1 causality computation is executed to get N phase 1 ensemble result for each causality method. Next, all the phase 1 data ensemble results are combined into one final output through phase 2 algorithm ensemble.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>IV. APPROACH OF TWO-PHASE HYBRID CAUSALITY ENSEMBLE FRAMEWORK</head><p>Based on the two-phase hybrid causality ensemble framework explained in previous section, the data-algorithm causality ensemble approach is developed as illustrated in Figure <ref type="figure" target="#fig_2">2</ref>. This data-algorithm ensemble approach is designed to effectively learn causal relationships from three data-driven causality learning approaches: multivariate Granger causality (M GC), P CM CI and dynamic Bayesian network (DBN ).  The data-algorithm ensemble approach (see Figure <ref type="figure" target="#fig_2">2</ref>) denotes that data ensemble happens in phase 1, then algorithm ensemble happens in phase 2. In this approach, the input data is first partitioned into N slices. Then, each of the causality discovery method (M GC, P CM CI and DBN ) is executed on all the partitioned data to get one causality output directed graph for each data slices. For example, M GC outputs M GC Result 1 , M GC Result 2 , ... M GC Result N .</p><p>Different methods are executed in serial in the order of M GC, P CM CI, DBN . The outputs from all partitioned data slices corresponding to each causality method are collected for phase 1 data ensemble. The phase 1 ensemble results are computed by majority voting. In the following step, phase 1 ensemble results of each causality method (M GC Ensemble, P CM CI Ensemble and DBN Ensemble) are combined using ensemble methods again into get a phase 2 algorithm ensemble causality result graph as final output. Count i appearance in E M GC , E P CM CI and E DBN as n i 8:</p><p>if n i &gt;= 2 then The data-algorithm ensemble approach includes two algorithms: Algorithm 1 (Data-Algorithm Ensemble) the twophase hybrid ensemble approach, which regards to the full process in Figure <ref type="figure" target="#fig_2">2</ref> and Algorithm 2 (Data-Algorithm Phase 1) for phase 1 data ensemble corresponding to each phase 1 ensemble block in Figure <ref type="figure" target="#fig_2">2</ref>.</p><p>The input of the Data-Algorithm Ensemble (Algorithm 1) includes different causality discovery methods, which are multivariate Granger causality (M GC), PCMCI (P CM CI) and Dynamic Bayesian Network (DBN ), time series input data D, and the number of data partitions N . The logic of Algorithm 1 for the whole ensemble process is as follows. In line 1, the input dataset D is first partitioned into N slices by its timestamp as {d} = d The phase 1 data ensemble in the data-algorithm ensemble approach, namely Data-Algorithm Phase 1 is shown in Algorithm 2. Its inputs include the specific causality discovery method Causality, and the partitioned time-series dataset {d}. In lines 1-3, the causality discovery method executes for each data partition d i in {d} to output a causality edge set E i from Causality(d i ). Since this causality edge set contains edges from each partition, in lines 5-10, phase 1 ensemble method loops to check if the number of a given edge e j appears in more than half of the partition edge set. For instance, if there are 10 partitions, and a causality edge (x 1 , x 2 ) appears 6 times in all the partition edge set, it is added to the phase 1 ensemble output E causality as in line 8 then be output as in line 11.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. PARALLEL TWO-PHASE HYBRID CAUSALITY ENSEMBLE LEARNING VIA SPARK BIG DATA ENGINE</head><p>The above two-phase hybrid causality ensemble approach is further implemented in parallel via Spark <ref type="bibr" target="#b0">[1]</ref> to achieve scalability to deal with big data in two aspects: 1) automatic data partitioning and 2) parallel function mapping.</p><p>Regarding the data partitioning part in our parallel implementation, the data is first load into Spark as resilient distributed dataset (RDD); then it is automatically partitioned by timestamp of each record, as in the phase 2 algorithm ensemble of data-algorithm ensemble, in Algorithm 1 line 1. More specifically, every data partition, as a chunk of the large distributed dataset, is assigned an index i for phase 1 ensemble in next step.</p><p>For parallel function mapping, the parallelization of dataalgorithm ensemble is implemented in its phase 1 data ensemble, as in Algorithm 2 lines 1-3. With Spark RDD partitioning, now each data partition d i becomes an RDD partition. Next, these RDD partitions are mapped to be transformed by the causality discovery method Causality in parallel, then be reduced as the edge set E i for later phase 2 ensemble computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VI. EXPERIMENTS</head><p>The experiments were conducted on top of the HPCF2018 cluster at the University of Maryland, Baltimore County <ref type="bibr" target="#b2">[3]</ref>, where each computing node containing two 18-core CPUs and 384 GB memory. For our experiment environment, one cluster contains one master node and several worker nodes. Moreover, the Spark programs are managed by Slurm workload manager in standalone cluster mode. For software, Python (version 3.6.8), Spark (version 2.4) are used. For Spark configurations, each node contains one executor, each driver/executor's memory is 200GB, and partition number is set as 48.</p><p>For test data, we created four synthetic datasets to evaluate our proposed algorithms' performance. One important reason for synthetic dataset generation is to know causality ground truth so we could evaluate learning result accuracy. Similar to the synthetic dataset generation approach for Granger causality and DBN evaluation in <ref type="bibr" target="#b32">[33]</ref>, we generated our synthetic dataset based on linear and nonlinear causal dependency Equation ( <ref type="formula">9</ref>) and Equation <ref type="bibr" target="#b9">(10)</ref>, where εs are random noises. The causality graph for the equation can be found at Figure <ref type="figure">3</ref> and Figure <ref type="figure">4</ref>. The linear and nonlinear datasets with different sizes (namely 1 million and 10 million for row numbers) were generated using the same equations correspondingly.</p><formula xml:id="formula_6">                   x 1 (t) = 0.95 • √ 2 • x 1 (t -1) -0.90 • x 1 (t -2) + ε 1 x 2 (t) = 0.5 • x 2 (t -1) + ε 2 x 3 (t) = -0.5 • x 1 (t -1) + 0.25 • √ 2 • x 3 (t -1) + 0.25 • √ 2 • x 2 (t -1) + ε 3 x 4 (t) = -0.95 • x 4 (t -1) -0.25 • √ 2 • x 3 (t -1) + ε 4 x 5 (t) = 0.5 • x 1 (t -1) + 0.95 • x 2 (t -2) -0.25 • √ 2 • x 3 (t -1) + 0.5 • x 5 (t -1) + ε 5 (9)                        x 1 (t) = 0.125 • √ 2 • exp(-x 1 (t -1) 2 /2) + ε 1 x 2 (t) = 1.2 • exp(-x 1 (t -1) 2 /2) + ε 2 x 3 (t) = -1.05 • exp(-x 1 (t -1) 2 /2) + 0.2 • √ 2 exp(-x 2 (t -2) 2 /2) + ε 3 x 4 (t) = -1.15 • exp(-x 1 (t -2) 2 /2) + 0.2 • √ 2 • exp(-x 4 (t -1) 2 /2) + 1.35 • exp(-x 3 (t -1) 2 /2) + ε 4 x 5 (t) = -1.15 • exp(-x 2 (t -1) 2 /2) + ε 5<label>(10)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Baseline Approaches and Parameter Setting</head><p>We employed seven baseline approaches in our experiments. The first three were single causality discovery approaches: Multivariate Granger causality (M GC), P CM CI and Dynamic Bayesian Network (DBN ). The next three were corresponding data ensemble approaches for each of the three single causality discovery approaches following the way described in Section III-B. The last one was an algorithm ensemble approach by combining all the three single causality discovery approaches following the way described in Section III-A. For experiment parameter settings, we set the maximum time lagging as 3 for synthetic data and the p-value threshold as 0.05 for both M GC and P CM CI tests. Besides, the total bin number for DBN was set as 5 to reduce computation time. In P CM CI method, we utilized its different conditional independence tests for linear and nonlinear causality discovery. For nonlinear conditional independence tests, as we had a large dataset, RCOT test was applied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Accuracy Evaluation</head><p>We employ Structural Hamming Distance (SHD) metric <ref type="bibr" target="#b28">[29]</ref> to compare accuracy of different approaches. SHD is a common metric to measure the difference between two directed graphs with the same node set. SHD value is defined as the total step count of three types of actions needed to transform from one direct graph to another direct graph: 1) reversing an edge's direction, 2) removing an extra edge, 3) adding a missing edge. We calculate SHD between ground truth graph and each learned graph. The lower SHD value means the more similarity between the two graphs, so the algorithm that generates the learned graph is more accurate.</p><p>We measured the accuracy of single causality discovery method of M GC, P CM CI and DBN and the three data ensemble baseline approaches and the algorithm ensemble causality ensemble approach. The results were shown columns 2-8 of Table <ref type="table">I</ref>. For linear datasets, we could see from the table that both data ensemble and algorithm approach could achieve the same or better accuracy than single causality discovery approaches. For nonlinear datasets, data ensemble approaches still performs better in accuracy; however, algorithm ensemble could perform a little bit worse due to two algorithm making the same wrong prediction on certain edges.</p><p>The accuracy of two-phase hybrid causality ensemble approach was shown in column 9 of Table <ref type="table">I</ref>   algorithm/data ensemble baseline approaches, our two-phase causality ensemble approach achieves perfect accuracy since their SHD values are all zero. In linear experiments, compared to data ensemble and algorithm ensemble baseline approaches, our two-phase hybrid causality ensemble approach could get the same or better results. In nonlinear experiments, two-phase hybrid ensemble approach achieves better accuracy than both data ensemble and algorithm ensemble. They both perform better than all the baseline approaches in accuracy for all the datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Scalability Evaluation</head><p>We conducted scalability experiments for our proposed twophase hybrid causality ensemble approaches given different sizes of datasets at a distributed computing environment mentioned above with 5, 7 9 nodes.</p><p>1) Execution Time: The execution times of all the baseline algorithms are shown in Table <ref type="table">II</ref>      The Spark based parallel implementations of the three dataensemble baseline approaches use the same techniques in Section V. We measured their execution times as in columns 2-4 of parallel experiments execution time tables. We also recorded the execution times of data-algorithm ensemble showing in column 5 of all execution time tables for parallel experiments.</p><p>We note Tables III, IV, V, VI show data-level parallel ensemble PCMCI is slower than our two-phase ensemble. By checking the execution logs, we found it is because at the runtime the Spark session encountered idle time for executors in the cluster, thus the computation time is fairly long. However, we did not see the same behavior in the twophase ensemble experiments. The reason for this unexpected result will be further investigated.</p><p>2) Speed Up: By comparing the execution times our parallel hybrid approaches in Tables III, IV, V, VI with the execution times of our serial algorithm ensemble baseline approach in Table <ref type="table">II</ref>, we evaluated the speed ups of our parallel hybrid ensemble approaches. The algorithm ensemble baseline was executed on a single node. As shown in Figures <ref type="figure" target="#fig_6">5</ref> and<ref type="figure" target="#fig_7">6</ref>, both achieved near linear speed up. Figure <ref type="figure" target="#fig_6">5</ref> shows the speed ups of two-phase ensemble in comparison to algorithm ensemble baseline for 10M row linear dataset. With 8 worker nodes, the speed up is more than 32 times. Similarly, Figure <ref type="figure" target="#fig_7">6</ref> shows speed up of two-phase ensemble compared to algorithm ensemble baseline for 10M nonlinear dataset. Its speed up, when running with 8 worker nodes, reaches 17 times compared to the baseline. Our approaches can achieve better than linear speedup because the time complexity of each baseline algorithm is worse than O(n). For instance, Granger causality algorithm's execution time grows quadratically with the increase of the data record number <ref type="bibr" target="#b3">[4]</ref>. By splitting data into N partitions, the execution time for each data partition is less than 1/N of the baseline serial approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VII. RELATED WORK</head><p>There have been many studies on ensemble learning and scalable/parallel machine learning. But we believe our work is the first study dealing with both algorithm variety and data volume for causality discovery. We also did not find many studies directly on ensemble learning for causality. Because  causality graph can be categorized as a type of probabilistic graphic model, we first discuss and compare with related work on ensemble learning for probabilistic graphic models in the first subsection. We further discuss and compare additional big data parallel ensemble learning work beyond probabilistic graphic models.</p><p>To achieve probabilistic graphical model ensemble, using the three categories explained in Section II, existing ensemble learning approaches can also be categorized into 1) algorithm ensemble for work at <ref type="bibr" target="#b16">[17]</ref>, 2) data ensemble work at <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b29">[30]</ref>, and 3) hybrid ensemble for both data and algorithm at <ref type="bibr" target="#b27">[28]</ref> and <ref type="bibr" target="#b6">[7]</ref>. In algorithm ensemble category, <ref type="bibr" target="#b16">[17]</ref> supports parallel ensemble learning of multiple classifiers on the same data. As a data ensemble approach, <ref type="bibr" target="#b13">[14]</ref> first splits the training data, then trains Bayesian sub-networks in parallel, finally does boosting as ensemble method on the trained sub-networks to get the learning result. <ref type="bibr" target="#b29">[30]</ref> is also a data ensemble approach for Bayesian network learning from big datasets to achieve better scalability and accuracy. As a hybrid ensemble approach, <ref type="bibr" target="#b27">[28]</ref> conducts two-phase (algorithm ensemble for each data partition and data ensemble for multiple data partitions) Bayesian network ensemble learning. The main differences of this work and <ref type="bibr" target="#b27">[28]</ref> are: 1) this work first conducts data ensemble among all data partitions and then algorithm ensemble for different algorithms where <ref type="bibr" target="#b27">[28]</ref> first conducts algorithm ensemble then data ensemble; 2) our algorithm-level ensemble belongs to heterogeneous ensemble because each learning algorithm uses its own causality discovery models, while <ref type="bibr" target="#b27">[28]</ref> belongs to homogeneous ensemble with different learning algorithms of the same Bayesian network model; 3) this paper targets causality instead of Bayesian learning.</p><p>Besides the probabilistic graphic model related ensemble studies in the subsection, most big data parallel ensemble learning algorithms are tree based where different trees can be trained in parallel with a data subset, then results from multiple trees are ensembled via majority voting (e.g., <ref type="bibr" target="#b6">[7]</ref>) or tree boosting (e.g., XGBoost <ref type="bibr" target="#b7">[8]</ref>). There are two main approaches of data partitioning: horizontal data partitioning based on rows and vertical data partitioning based on columns. <ref type="bibr" target="#b6">[7]</ref> contains horizontal data partitioning and parallel learning among the data partitions. Input data is first partitioned vertically to divide training data features to independent subsets. Then each task loads the data from one feature subset to train an independent tree and multiple trees can be trained in parallel. For XGBoost <ref type="bibr" target="#b7">[8]</ref>, parallel training is done via horizontally partitioned data and they differ in how different trees are ensembled. As a comparison, parallelization in our hybrid ensemble approaches is done via horizontal data partitioning because all features are needed for each training and our data has time dependency. Further, multiple learning algorithms are employed in our data-algorithm ensemble while the above related works only employ the same learning algorithm for different data partitions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VIII. CONCLUSIONS</head><p>Causality discovery is a fundamental research topic in many disciplines and discovered cause-effect relationships can help explain why a system has certain behavior or state. Nowadays, data-driven causality discovery faces two challenges: 1) the large volume of datasets to be learned from and 2) the variety of causality discovery algorithms. To deal with these two challenges, this paper proposes a two-phase hybrid ensemble causality learning framework and an implementation approach for scalable ensemble causality learning. Experiments show our algorithms outperform baseline ones in terms of both accuracy and execution time.</p><p>For future work, we will focus on the following aspects. First, we will extend the work to further enable ensemble of time lag and probability of causal edges. Second, we will study how to best select from many available causality learning algorithms, i.e., through diversity measurement, for better ensemble result accuracy. Further, we plan to apply the framework and algorithms to real-world climate applications and evaluate their effectiveness through the applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Two-phase hybrid ensemble framework for causality discovery.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Illustration of data-algorithm ensemble learning approach.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Algorithm 1 :</head><label>1</label><figDesc>Data-Algorithm Ensemble (Data-Algorithm Ensemble) Input: Different causality discovery methods: Multivariate Granger causality: M GC, PCMCI: P CM CI, Dynamic Bayesian Network: DBN , Time series data: D, Number of data partitions: N Output: A directed causality graph: G = (V, E) 1: Partition data D into N partitions as {d} = d 1 , d 2 , ..., d N 2: Get E M GC = Data-Algorithm P hase 1(M GC, {d}) 3: Get E P CM CI = Data-Algorithm P hase 1(P CM CI, {d}) 4: Get E DBN = Data-Algorithm P hase 1(DBN, {d}) 5: ## Phase 2 edge ensemble: 6: for unique edges {e i } in E M GC , E P CM CI and E DBN do 7:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>end for 12: Output G = (V, E)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>x1Fig. 3 .Fig. 4 .</head><label>34</label><figDesc>Fig. 3. Linear synthetic data ground truth causal graph.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Speed up of two-phase ensemble compared to algorithm ensemble baseline for 10M row linear dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Speed up of two-phase ensemble compared to algorithm ensemble baseline for 10M row nonlinear dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>1) Multivariate Granger Causality: Granger causality, as a predictive model in economics, was proposed in 1969 by Nobel Laureate Clive W. Granger. By definition, in Granger causality, one time series x Granger causes another time series y, if and only if the regression for y based on past values of both x and y is statistically significant than the regression of y only based on past values of y itself. To demonstrate the definition, let the lagged variable x be x t-i for i from 1 to maximum lag P ; and similarly, the lagged y is represented by y t-i . To test Granger causality, in first step, the following two linear regressions functions are fitted as follows: y t = a 11 • y t-1 + a 12 • y t-2 + ... + a 1P • y t-P + ε 1 (1) y t = a 21 •y t-1 +...+a 2P •y t-P +b 21 •x t-1 +...+b 2P •x t-P +ε 2</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>1 , d 2 , ..., d N where the time interval of each slice is only 1/N of the original time series. Then it calls Algorithm 2 (Data-Algorithm Phase 1) to execute each causality discovery method to get phase 1 ensemble causality edge set E M GC , E P CM CI and E DBN from all the data partitions in lines 2-4. Finally, in lines 6-11, phase 2 ensemble result is computed by majority voting on edge set of all causality mining methods, E M GC , E P CM CI and E DBN , that if two or more causality ensemble edge sets contain the same edge, this edge is added into final output graph G = (V, E) with V denoting nodes and E as edges in line 12. for unique edges {e j } in all E i do</figDesc><table><row><cell cols="2">Algorithm 2: Phase 1 Ensemble for Data-Algorithm</cell></row><row><cell cols="2">Ensemble (Data-Algorithm Phase 1)</cell></row><row><cell cols="2">Input: Causality discovery method: Causality,</cell></row><row><cell cols="2">Data partition set: {d}</cell></row><row><cell cols="2">Output: A set of directed edges in Graph</cell></row><row><cell></cell><cell>corresponding to causality discovery method:</cell></row><row><cell></cell><cell>E causality</cell></row><row><cell cols="2">1: for each data partition d i in {d} do</cell></row><row><cell>2:</cell><cell>Get causality edge set from causality computation:</cell></row><row><cell></cell><cell>E i = Causality(d i )</cell></row><row><cell cols="2">3: end for</cell></row><row><cell cols="2">4: ## Phase 1 edge ensemble:</cell></row><row><cell>5: 6:</cell><cell>Count e j appearance in all E i as n j</cell></row><row><cell>7:</cell><cell>if n j &gt; N/2 then</cell></row><row><cell>8:</cell><cell>Add e j to E causality</cell></row><row><cell>9:</cell><cell>end if</cell></row><row><cell cols="2">10: end for</cell></row><row><cell cols="2">11: Output E causality</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>for linear and nonlinear, 1M and 10M dataset testing. The execution times for parallel experiments are shown as in Table III and Table IV for 1M and 10M records of linear dataset. For nonlinear dataset, the execution times are recorded as in Table V and Table VI for 1M and 10M data correspondingly.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>TABLE V EXECUTION</head><label>V</label><figDesc>TIME: PARALLEL EXPERIMENTS ON 1M NONLINEAR DATA</figDesc><table><row><cell></cell><cell>Data-level</cell><cell>Data-level</cell><cell>Data-level</cell><cell>Two-phase</cell></row><row><cell>Nonlinear 1M</cell><cell>Parallel Ensemble</cell><cell>Parallel Ensemble</cell><cell>Parallel Ensemble</cell><cell>Ensemble</cell></row><row><cell></cell><cell>MGC</cell><cell>PCMCI</cell><cell>DBN</cell><cell>Data-Algorithm</cell></row><row><cell>4 Worker Nodes</cell><cell>0m20.195s</cell><cell>13m3.590s</cell><cell>2m19.261s</cell><cell>7m25.565s</cell></row><row><cell>6 Worker Nodes</cell><cell>0m19.066s</cell><cell>11m5.580s</cell><cell>1m28.426s</cell><cell>5m31.534s</cell></row><row><cell>8 Worker Nodes</cell><cell>0m18.492s</cell><cell>8m1.691s</cell><cell>1m1.877s</cell><cell>4m13.503s</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>ACKNOWLEDGMENT This work is supported by grant <rs type="projectName">CyberTraining: DSE: Cross-Training of Researchers in Computing, Applied Math-ematics and Atmospheric Sciences using Advanced Cyberinfrastructure Resources</rs> (<rs type="grantNumber">OAC-1730250</rs>), and grant <rs type="projectName">CAREER: Big Data Climate Causality Analytics</rs> (<rs type="grantNumber">OAC-1942714</rs>) from the <rs type="funder">National Science Foundation</rs>. The execution environment is provided through the <rs type="institution" subtype="infrastructure">High Performance Computing Facility</rs> at <rs type="institution">UMBC</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_kH4JtPw">
					<idno type="grant-number">OAC-1730250</idno>
					<orgName type="project" subtype="full">CyberTraining: DSE: Cross-Training of Researchers in Computing, Applied Math-ematics and Atmospheric Sciences using Advanced Cyberinfrastructure Resources</orgName>
				</org>
				<org type="funded-project" xml:id="_39R2DAB">
					<idno type="grant-number">OAC-1942714</idno>
					<orgName type="project" subtype="full">CAREER: Big Data Climate Causality Analytics</orgName>
				</org>
			</listOrg>

			<listOrg type="infrastructure">
				<org type="infrastructure">					<orgName type="extracted">High Performance Computing Facility</orgName>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><surname>Homepage -Apache</surname></persName>
		</author>
		<ptr target="http://spark.apache.org" />
		<title level="m">Spark Project</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2020" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">ensemble causality learning</title>
		<ptr target="https://github.com/big-data-lab-umbc/" />
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2020" to="2025" />
		</imprint>
		<respStmt>
			<orgName>Scalable Ensemble Learning for Causality Discovery</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<ptr target="https://hpcf.umbc.edu/" />
		<title level="m">The UMBC High Performance Computing Facility (HPCF)</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2020" to="2023" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Temporal causal modeling with graphical granger methods</title>
		<author>
			<persName><forename type="first">A</forename><surname>Arnold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Abe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;07</title>
		<meeting>the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD &apos;07<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="66" to="75" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">I</forename><surname>Ben-Gal</surname></persName>
		</author>
		<title level="m">Bayesian Networks. Encyclopedia of Statistics in Quality and Reliability</title>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Bagging predictors</title>
		<author>
			<persName><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="123" to="140" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A parallel random forest algorithm for big data in a spark cloud computing environment</title>
		<author>
			<persName><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Bilal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Parallel and Distributed Systems</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="919" to="933" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Xgboost: A scalable tree boosting system</title>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining</title>
		<meeting>the 22nd acm sigkdd international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="785" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A bayesian method for the induction of probabilistic networks from data</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">F</forename><surname>Cooper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Herskovits</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="309" to="347" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Mapreduce: simplified data processing on large clusters</title>
		<author>
			<persName><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="107" to="113" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Experiments with a new boosting algorithm</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">icml</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="page" from="148" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Investigating causal relations by econometric models and cross-spectral methods</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">W</forename><surname>Granger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econometrica</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="424" to="438" />
			<date type="published" when="1969">1969</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">El niño-southern oscillation</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">J</forename><surname>Holbrook</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">N</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Davidson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Hobday</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Lough</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mcgregor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">B</forename><surname>Power</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Riseby</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A parallel bayesian network learning algorithm for classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Xiong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">th IEEE International Conference on Software Engineering and Service Science (ICSESS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="259" to="263" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Evaluation of Data-driven Causality Discovery Approaches among Dominant Climate Modes</title>
		<author>
			<persName><forename type="first">S</forename><surname>Hussung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mahmud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sampath</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<idno>HPCF-2019-12</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
			<pubPlace>Baltimore County</pubPlace>
		</imprint>
		<respStmt>
			<orgName>UMBC High Performance Computing Facility, University of Maryland</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Luetkepohl</surname></persName>
		</author>
		<title level="m">The New Introduction to Multiple Time Series Analysis</title>
		<imprint>
			<date type="published" when="2005-01">01 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A parallel algorithm for bayesian network structure learning from large data sets</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">L</forename><surname>Madsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Salmerón</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Langseth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Nielsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl.-Based Syst</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="page" from="46" to="55" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Memory matters: A case for granger causality in climate variability studies</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">C</forename><surname>Mcgraw</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">A</forename><surname>Barnes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Clim</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3289" to="3300" />
			<date type="published" when="2018-01">Jan. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Dynamic bayesian networks: representation, inference and learning</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Russell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Climate data challenges in the 21st century</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">T</forename><surname>Overpeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">A</forename><surname>Meehl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Easterling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">science</title>
		<imprint>
			<biblScope unit="volume">331</biblScope>
			<biblScope unit="issue">6018</biblScope>
			<biblScope unit="page" from="700" to="702" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Causality: Models, Reasoning and Inference</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Cambridge University Press</publisher>
			<pubPlace>New York, NY, USA</pubPlace>
		</imprint>
	</monogr>
	<note>2nd edition</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Simpson&apos;s paradox, confounding, and collapibility. Causality: models, reasoning and inference</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="173" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Ensemble learning</title>
		<author>
			<persName><forename type="first">R</forename><surname>Polikar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Ensemble machine learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="1" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Detecting causal associations in large nonlinear time series datasets</title>
		<author>
			<persName><forename type="first">J</forename><surname>Runge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Nowack</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kretschmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Flaxman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Sejdinovic</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1702.07007v2" />
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2018" to="2024" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The strength of weak learnability</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="197" to="227" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Hybrid causality analysis of enso&apos;s global impacts on climate variables based on data-driven analytics and climate model simulation</title>
		<author>
			<persName><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Earth Science</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page">233</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Measuring and testing dependence by correlation of distances</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">J</forename><surname>Székely</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">L</forename><surname>Rizzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">K</forename><surname>Bakirov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2007">10 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Penbayes: A multilayered ensemble approach for learning bayesian network structure from big data</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Altintas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4400</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The max-min hill-climbing bayesian network structure learning algorithm</title>
		<author>
			<persName><forename type="first">I</forename><surname>Tsamardinos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">E</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">F</forename><surname>Aliferis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="31" to="78" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A scalable data science workflow ap-proach for big data bayesian network learning</title>
		<author>
			<persName><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Altintas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 IEEE/ACM International Symposium on Big Data Computing (BDC 2014)</title>
		<meeting>the 2014 IEEE/ACM International Symposium on Big Data Computing (BDC 2014)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="16" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Stacked generalization</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">H</forename><surname>Wolpert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="241" to="259" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Distinguishing time-delayed causal interactions using convergent cross mapping</title>
		<author>
			<persName><forename type="first">H</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">R</forename><surname>Deyle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">J</forename><surname>Gilarranz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Sugihara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific reports</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">14750</biblScope>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Granger causality vs. dynamic bayesian network inference: a comparative study</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Denby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMC Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">122</biblScope>
			<date type="published" when="2009-04">Apr. 2009</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
