<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">One Fits All: Learning Fair Graph Neural Networks for Various Sensitive Attributes</title>
				<funder ref="#_CExMFAD">
					<orgName type="full">Guangdong Basic and Applied Basic Research Foundation, China</orgName>
				</funder>
				<funder ref="#_QmnVBGX">
					<orgName type="full">Shenzhen Science and Technology Program</orgName>
				</funder>
				<funder ref="#_WjWuCTs">
					<orgName type="full">Tencent AI Lab</orgName>
				</funder>
				<funder ref="#_rjE8hC8">
					<orgName type="full">National Key R&amp;D Program of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2025-06-09">9 Jun 2025</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Yuchang</forename><surname>Zhu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Jintang</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Yatao</forename><surname>Bian</surname></persName>
							<email>yatao.bian@gmail.com</email>
						</author>
						<author>
							<persName><forename type="first">Zibin</forename><surname>Zheng</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Liang</forename><surname>Chen</surname></persName>
							<email>chenliang6@mail.sysu.edu.cn</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Sun Yat-sen University Guangzhou</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Tencent AI Lab Shenzhen</orgName>
								<orgName type="institution">Sun Yat-sen University Guangzhou</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">Sun Yat-sen University Guangzhou</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Sun Yat-sen University Guangzhou</orgName>
								<address>
									<addrLine>KDD &apos;24 August 25-29</addrLine>
									<postCode>2024</postCode>
									<settlement>Barcelona</settlement>
									<country>China Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">One Fits All: Learning Fair Graph Neural Networks for Various Sensitive Attributes</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2025-06-09">9 Jun 2025</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3637528.3672029</idno>
					<idno type="arXiv">arXiv:2406.13544v3[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:23+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Fairness</term>
					<term>Graph Neural Networks</term>
					<term>Invariant Learning S-related information</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recent studies have highlighted fairness issues in Graph Neural Networks (GNNs), where they produce discriminatory predictions against specific protected groups categorized by sensitive attributes such as race and age. While various efforts to enhance GNN fairness have made significant progress, these approaches are often tailored to specific sensitive attributes. Consequently, they necessitate retraining the model from scratch to accommodate changes in the sensitive attribute requirement, resulting in high computational costs. To gain deeper insights into this issue, we approach the graph fairness problem from a causal modeling perspective, where we identify the confounding effect induced by the sensitive attribute as the underlying reason. Motivated by this observation, we formulate the fairness problem in graphs from an invariant learning perspective, which aims to learn invariant representations across environments. Accordingly, we propose a graph fairness framework based on invariant learning, namely FairINV, which enables the training of fair GNNs to accommodate various sensitive attributes within a single training session. Specifically, FairINV incorporates sensitive attribute partition and trains fair GNNs by eliminating spurious correlations between the label and various sensitive attributes. Experimental results on several real-world datasets demonstrate that FairINV significantly outperforms state-of-the-art fairness approaches, underscoring its effectiveness. 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>â€¢ Computing methodologies â†’ Knowledge representation and reasoning.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Graph neural networks (GNNs) have achieved tremendous success in processing graph-structured data <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30]</ref>, such as citation networks <ref type="bibr" target="#b38">[39]</ref> and social networks <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b40">41]</ref>. Consequently, this advancement has led to their application across diverse domains, including fraud detection <ref type="bibr" target="#b14">[15]</ref> and recommender systems <ref type="bibr" target="#b44">[45]</ref>. However, recent studies <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b39">40]</ref> have unveiled a concerning trend that GNNs make discriminatory decisions toward the specific protected groups defined by sensitive attributes, e.g., race, and age. This phenomenon, termed the group fairness problem of GNNs, hinders the application of GNNs in high-stake scenarios.</p><p>To improve the fairness of GNNs, considerable efforts have been devoted to debiasing the training data <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b32">33]</ref> or learning fair GNNs directly <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b11">12]</ref>, referred to as the pre-process and in-process approaches, respectively. Within these two methodological categories, common implementations encompass adversarial learning <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b42">43]</ref>, distribution alignment among various protected groups <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b17">18]</ref>, graph-structured data modification <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b39">40]</ref>, and edge reweighting <ref type="bibr" target="#b30">[31]</ref>. Despite significant progress, these approaches are often tailored to specific sensitive attributes, as shown in Figure <ref type="figure" target="#fig_0">1(a)</ref>. Consequently, training GNN models from scratch becomes imperative when faced with fairness requirement alterations in sensitive attributes, such as transitioning from age-based considerations to gender-related factors. Take loan approvals in a credit card network as an example, according to fairness policies, initially trained GNNs are designed to make fair decisions toward the protected groups divided by age, e.g., age â‰¤ 25 and age &gt; 25. However, when policies change to focus on gender, necessitating fair treatment between male and female groups, the previously tailored model optimized for age fairness becomes inadequate. Hence, this mandates retraining the GNN model to ensure fairness regarding gender, which is a laborious and computationally intensive process.</p><p>In summary, there is a significant demand for a universal graph fairness approach that trains fair GNNs across various sensitive attributes in a single training session. Achieving such an approach entails addressing the following challenges: (1) Generalization to various sensitive attributes. Previous studies aim to achieve fairness tailored for the specific sensitive attribute. Additionally, these approaches always require accessing the sensitive attributes in the training process, which is impractical in real-world scenarios due to legal limitations <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b25">26]</ref>. Correspondingly, our first challenge is to design a fairness framework that achieves fairness w.r.t. various sensitive attributes in a single training session without accessing the sensitive attribute, as shown in Figure <ref type="figure" target="#fig_0">1</ref>(b). ( <ref type="formula" target="#formula_2">2</ref>) Full fairness. According to Section 3.2, two causal pathways (ğ‘† â†’ ğ‘Œ and ğ‘† â†’ G â†’ ğ‘Œ ) demonstrate how the sensitive attribute ğ‘† influences the labels ğ‘Œ , misleading the trained GNNs to capture the sensitive attribute information for predictions. In this regard, ğ‘† is a confounder. To achieve full fairness, blocking these two causal effects appears to be a straightforward solution. However, it is challenging due to the presence of underlying spurious correlations between unobservable variables ğ‘† and ğ‘Œ . As discussed in Section 3.2, prior works failed to eliminate both causal effects concurrently. Inspired by INV-REG <ref type="bibr" target="#b34">[35]</ref>, backdoor adjustment implemented by data partition presents a promising approach to tackle this challenge.</p><p>In this work, we first formulate the graph fairness issue from an invariant learning <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b43">44]</ref> perspective, where sensitive attributes as environments. Building upon this formulation, we propose a universal graph fairness framework named FairINV. To overcome the first challenge, FairINV jointly optimizes a fair GNN for multiple sensitive attributes inferred automatically via sensitive attribute partition. To overcome the second challenge, FairINV incorporates invariant learning optimization objectives building upon sensitive attribute partition to remove confounding effects induced by ğ‘†. Specifically, the optimization objective of FairINV gives rise to equal predictions of trained GNNs across environments (sensitive attributes). In summary, FairINV mitigates spurious correlations between various sensitive attributes and the label. Our contributions can be summarized as follows:</p><p>â€¢ We study the fairness issue on graphs from an invariant learning perspective. To the best of our knowledge, this is the first attempt to explore graph fairness from this particular perspective. â€¢ We introduce FairINV, a universal graph fairness framework that inherits the spirit of graph invariant learning. An unsupervised sensitive attributes partition of FairINV facilitates fairness improvement in terms of various sensitive attributes.</p><p>â€¢ We conduct experiments on several real-world datasets to validate the effectiveness of FairINV. Experimental results show that FairINV can train a fair GNN toward various sensitive attributes in a single training session.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK 2.1 Fairness in Graph Neural Networks</head><p>The fairness of GNNs includes group fairness <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b49">50]</ref> and individual fairness <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b22">23]</ref>. Our study focuses on the group fairness aspect, emphasizing equitable model decisions for each protected group partitioned by the sensitive attribute. Recent studies improving group fairness in GNNs typically segregate into pre-process <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b48">49]</ref> and in-process <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b47">48]</ref> approaches. Preprocess approaches aim to mitigate biases in training data before training downstream tasks. To mitigate biases, techniques like adversarial learning <ref type="bibr" target="#b32">[33]</ref>, and distribution alignment <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b46">47]</ref> serve as optimization objectives for debiasing training data. Additionally, some heuristic approaches modify the training graph <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b39">40]</ref> or reweight edge <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b30">31]</ref> by either enhancing connections between diverse groups or reducing connections within the same groups.</p><p>In-process approaches aim to train fair GNNs through the fairnessaware framework. Similar to pre-process approaches, in-process approaches also incorporate adversarial learning <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b11">12]</ref> and distribution alignment <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref> to learn GNNs. Despite significant progress, these approaches are tailored to the specific sensitive attribute, lacking considerations for various sensitive attributes. Despite Bose et al.'s <ref type="bibr" target="#b5">[6]</ref> work of a compositional adversarial framework using a set of sensitive-invariant filters, it necessitates prior knowledge of considered sensitive attributes and their specific values for each individual. In contrast, our work learns fair GNNs toward various sensitive attributes in a single training session without accessing sensitive attributes, which remains under-explored for prior works.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Invariant Learning for Fairness</head><p>Guided by the independent causal mechanism assumption <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref>, invariant learning, capable of capturing invariances across various environments, stands as a significant approach facilitating out-ofdistribution (OOD) generalization <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b41">42]</ref>. The core idea behind invariant learning is to learn causal information that stays invariant across different environments while disregarding spurious correlations that exhibit variability <ref type="bibr" target="#b7">[8]</ref>. However, there is limited research exploring the application of invariance learning in fairness. Adragna et al. <ref type="bibr" target="#b0">[1]</ref> empirically illustrate how invariant risk minimization in invariant learning can contribute to building fair machine learning models. Ma et al. <ref type="bibr" target="#b34">[35]</ref> point out the fairness-related bias in face recognition stemming from confounding demographic attributes. Then, they iteratively partition data to annotate confounders and learn invariant features to remove the confounding effect. Yet, these explorations of invariant learning in fairness predominantly focus on Euclidean data. Conversely, significant efforts <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b33">34]</ref> have addressed the out-of-distribution problem within graph structures from an invariant learning perspective. However, the effectiveness of invariant learning in ensuring graph fairness remains an underexplored area. To the best of our knowledge, our work is the first to explore the graph fairness problem utilizing graph invariant learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PRELIMINARIES</head><p>In this section, we first introduce the detailed notations used in this work. Then, we give a causal analysis for our study problem, followed by the problem formulation of this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Notations</head><p>Let G = (V, E, X) denote an undirected and unweighted attributed graph, where V is a set of nodes and E is a set of edges. Meanwhile, |V | = ğ‘› and |E | = ğ‘š represent the number of nodes and edges, respectively. X âˆˆ R ğ‘›Ã—ğ‘‘ represents the node attribute matrix without the sensitive attribute ğ‘† where ğ‘‘ is the node attribute dimension. A âˆˆ {0, 1} ğ‘›Ã—ğ‘› is the adjacency matrix where A ğ‘¢ğ‘£ = 1 indicates the edge connection ğ‘’ ğ‘¢ğ‘£ âˆˆ E between the node ğ‘¢ and the node ğ‘£, and A ğ‘¢ğ‘£ = 0 otherwise. Nodes with the same sensitive attribute value belong to the same protected group. Most GNNs follow the message-passing mechanism, which aggregates messages from their neighbors, and can be summarized as follows:</p><formula xml:id="formula_0">h (ğ‘™ ) ğ‘¢ = UPD (ğ‘™ ) ( {h (ğ‘™ -1) ğ‘¢ , AGG (ğ‘™ ) ( {h (ğ‘™ -1) ğ‘£ : ğ‘£ âˆˆ N (ğ‘¢ ) } ) } ),<label>(1)</label></formula><p>where ğ‘™ is the layer number, AGG (ğ‘™ ) (â€¢) and UPD (ğ‘™ ) (â€¢) denote aggregation function and update function in ğ‘™-th layer, respectively. N (ğ‘¢) denote the set of nodes adjacent to node ğ‘¢. While our approach is applicable to various downstream tasks, in this paper, we exemplify its application using the node classification downstream task to illustrate the proposed methodology. A GNN model ğ‘“ , consisting of an encoder ğ‘“ ğ‘” and a linear classifier ğ‘“ ğ‘ , takes a graph G as input and outputs the node predicted label Å¶ = ğ‘“ ğ‘ (ğ‘“ ğ‘” (G)). The goal of ğ‘“ is to predict Å¶ such that it is as close as possible to the ground truth labels ğ‘Œ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Causal Analysis</head><p>To broaden insights, we construct a structural causal model <ref type="bibr" target="#b36">[37]</ref> (SCM) to analyze the group fairness issue in graph-structured data. Figure <ref type="figure" target="#fig_2">2</ref>(a) illustrates the causal relationship among the graph G, the sensitive attribute ğ‘†, and the node label ğ‘Œ . In the SCM, there are two causal pathways by which the sensitive attribute ğ‘† affects the node label ğ‘Œ , leading to issues of group fairness in GNNs. A detailed description of these two causal pathways is provided below.  â€¢ ğ‘† â†’ G â†’ ğ‘Œ . This causal pathway describes the influence of ğ‘† on the formation of graph-structured data G, which subsequently impacts the predictions of node labels ğ‘Œ by the trained GNN. Specifically, the path ğ‘† â†’ G represents the impact of ğ‘† on the generation process of graph-structured data.</p><p>In this context, the data exhibit two primary phenomena: (1) the graph topology exhibits sensitive homophily <ref type="bibr" target="#b20">[21]</ref>, where connected nodes are more likely to share the same sensitive attribute ğ‘†.</p><p>(2) Non-sensitive node attributes may implicitly convey information about ğ‘†. For instance, if ğ‘† represents gender, certain attributes like height, while not directly sensitive, become relevant in inferring an individual's gender. The path G â†’ ğ‘Œ encapsulates the training process of GNNs, wherein the network may inherit and subsequently propagate biases (information related to ğ‘†) present in the training data. â€¢ ğ‘† â†’ ğ‘Œ . This causal pathway illustrates the underlying correlation between the node label and the sensitive attribute. This correlation often originates from societal discrimination against protected groups, leading to biased predictions in the trained GNN. For instance, given a social network dataset, the task is to predict the user's occupational field. The dataset predominantly comprises occupations of females as nurses and males as engineers. Consequently, the GNN trained on this dataset tends to predict engineering as the occupational field for males and nursing for females, thus revealing a gender bias in its predictions. This phenomenon can be attributed to the ğ‘† â†’ ğ‘Œ causal pathway inherent in the dataset. In summary, discriminatory decisions in GNNs stem from the two causal pathways discussed above. In this regard, the pathway ğ‘Œ â† ğ‘† â†’ G is a backdoor path, with ğ‘† acting as a confounder. This pathway may mislead the trained GNN to utilize the sensitive attribute for predictions, known as the spurious confounding effect. To remove this effect, a straightforward yet challenging solution involves eliminating ğ‘† â†’ G â†’ ğ‘Œ and ğ‘† â†’ ğ‘Œ . However, prior works have not successfully removed both pathways simultaneously. As shown in Figure <ref type="figure" target="#fig_2">2</ref>(b), pre-process methods primarily focus on reducing the information related to the sensitive attribute in the training data, effectively removing the path ğ‘† â†’ G. Conversely, in-process methods strive to develop a fair GNN that makes decisions independently of ğ‘†, akin to removing the path ğ‘† â†’ ğ‘Œ . Another approach to mitigate confounding effects is the backdoor adjustment, achieved by partitioning the training data into different splits. In our scenarios, we partition nodes into distinct demographic groups and learn GNNs invariant across these groups. Drawing inspiration from a fairness study in face recognition <ref type="bibr" target="#b34">[35]</ref>, we attempt to formulate the graph fairness issue from an invariant learning perspective. Leveraging the environment inference capabilities of invariant learning, we can unsupervisedly infer the sensitive attribute of nodes, facilitating group partitioning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Problem Formulation</head><p>In this subsection, we formulate the graph fairness problem from an invariant learning perspective. Our work focuses on the node-level task. Following the setting of EERM <ref type="bibr" target="#b43">[44]</ref>, we investigate the impact of the node's ego-graph on the centered node. Specifically, given a single graph G = (V, E, X), we have a set of ego-graph D = D ğ‘’ from various environment E ğ‘ğ‘™ğ‘™ , where</p><formula xml:id="formula_1">D ğ‘’ = {G ğ‘’ ğ‘£ , ğ‘¦ ğ‘’ ğ‘£ } is graphs from environment ğ‘’. G ğ‘’</formula><p>ğ‘£ and ğ‘¦ ğ‘’ ğ‘£ are the ego-graph and the node label of node ğ‘£. The invariant learning aims to learn GNNs to generalize to all unseen environments. Denote ğ‘“ as a GNN model consisting of an encoder and a classifier, Å·ğ‘£ = ğ‘“ (G ğ‘£ ) as the predicted label of node ğ‘£, and R ğ‘’ (â€¢) as the empirical risk under environment ğ‘’. Formally, the invariant learning on the node level is to minimize:</p><formula xml:id="formula_2">min ğ‘“ max ğ‘’ âˆˆ E ğ‘ğ‘™ğ‘™ R ğ‘’ (ğ‘“ )<label>(2)</label></formula><p>where</p><formula xml:id="formula_3">R ğ‘’ (ğ‘“ ) = E ğ‘’ G ğ‘£ ,ğ‘¦ ğ‘£ [ğ‘™ (ğ‘“ (G ğ‘£ ), ğ‘¦ ğ‘£ )], ğ‘™ (â€¢, â€¢)</formula><p>is the loss function. Based on the above minimization objective, the trained GNN performs equally across all environments. Similarly, the goal of fairness on the graph is to have the model equally treat different demographic groups divided by the sensitive attribute ğ‘†. In this regard, the centered node (the ego-graph) with different sensitive attribute values or under different sensitive attributes can be regarded as a graph under different environments. Naturally, a fairness problem on graphs can be formulated as a form of invariant learning.</p><p>In this work, we aim to learn fair GNNs toward various sensitive attributes in a single training session. With the formulation of invariant learning, our goal is transformed into learning GNNs invariant across different sensitive attributes and sensitive attribute values. Formally, our goal is to minimize:</p><formula xml:id="formula_4">min ğ‘“ max ğ‘† âˆˆ S ğ‘ğ‘™ğ‘™ R ğ‘† (ğ‘“ )<label>(3)</label></formula><p>where</p><formula xml:id="formula_5">R ğ‘† (ğ‘“ ) = E ğ‘† G ğ‘£ ,ğ‘¦ ğ‘£ [ğ‘™ (ğ‘“ (G ğ‘£ ), ğ‘¦ ğ‘£ )]</formula><p>is the empirical risk under sensitive attribute ğ‘†. S ğ‘ğ‘™ğ‘™ is a set of ğ‘†. For instance, assume that gender and race are sensitive attributes, S ğ‘ğ‘™ğ‘™ includes male, female, white, black, and yellow people environments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">PRESENT WORK: FAIRINV</head><p>In this section, we discuss how to learn a GNN towards fairness w.r.t. various sensitive attributes in a single training session through our proposed method FairINV. Specifically, we first give a brief overview of FairINV and then make a detailed description of the components of FairINV. Furthermore, we provide the training algorithm to shed insights into the process of FairINV.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Overview</head><p>FairINV focuses on the node-level task, aiming to learn GNNs invariant across various sensitive attributes within a single training session, thereby achieving fairness on graph-structured data. As shown in Figure <ref type="figure" target="#fig_3">3</ref>, our proposed method FairINV comprises two modules, i.e., sensitive attribute partition (SAP) and sensitive invariant learning (SIL). The SAP module partitions nodes into different subsets by inferring variant ego-subgraphs for each centered node. It should be noted that the sampling of ego-subgraphs can be disregarded due to the message-passing mechanism, which effectively aggregates the representations of neighboring nodes to update its own representation. To optimize this module, we employ the Invariant Risk Minimization (IRM) objective <ref type="bibr" target="#b3">[4]</ref>, maximizing it to guide the SAP module in capturing the worst-case environment. This process can be seen as inferring the sensitive attribute value of nodes. Since the maximization of the IRM objective is executed in an unsupervised manner, we can iteratively predict sensitive attributes multiple times. This iterative process enables FairINV to achieve fairness with respect to various sensitive attributes in a single training session. Due to the formulation of the fairness problem from an invariant learning perspective in Section 3.3, we can naturally tackle this problem through invariant learning. Specifically, based on the partition results of SAP, the SIL module learns a GNN invariant across different sensitive attribute partitions through a variance-based loss. The objective of being invariant across different sensitive attribute partitions implies the equitable treatment of different demographic groups, thereby achieving fair decisionmaking. Overall, the SAP module is akin to data augmentation, facilitating the process of the SIL module. Due to such a training paradigm, FairINV follows the same inference process as vanilla GNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Sensitive Attributes Partition</head><p>Existing methods are designed for the specific sensitive attribute while assuming accessible sensitive attributes. However, these methods are impractical in real-world scenarios due to legal restrictions. To overcome this challenge, there is a need to infer the sensitive attribute value for each node in an unsupervised manner. Inferring the sensitive attribute value multiple times can be regarded as obtaining multiple sensitive attribute values, e.g., gender, and race, facilitating the achievement of fairness w.r.t. various sensitive attributes. Unfortunately, learning a sensitive attribute inference model without access to the sensitive attribute ground truth is a non-trivial task.</p><p>Inspired by the unsupervised environment inference in invariant learning <ref type="bibr" target="#b9">[10]</ref>, we aim to maximize variability across environments to achieve the sensitive attribute partition. Based on our formulation of the fairness problem from an invariant learning perspective, the sensitive attributes can be seen as environments in invariant learning. Nodes with different sensitive attribute values can be considered as being in different environments. Thus, maximizing variability across environments indicates inferring a worst-case sensitive attribute partition, where GNNs exhibit the worst fairness performance towards the demographic group divided by the sensitive attribute. However, directly inferring sensitive attributes partition through the aforementioned maximization objective is impractical due to the interactive nature of graph-structured data. Following the inspiration from GIL <ref type="bibr" target="#b27">[28]</ref>, identifying variant subgraphs as auxiliary information for sensitive attribute partition may provide a desirable solution.</p><note type="other">ğ¿ ğ¿ ğ¿ Graph GNNs Backbone Classifier Version 2</note><p>Following the above idea, we construct the SAP module to infer the sensitive attribute value of each node. Specifically, the SAP module consists of a pre-trained GNN backbone ğœ‘, a variant inference model ğœ“ , and a sensitive attribute inference model ğ‘. With an expected structure identical to the GNN model to be trained, ğœ‘ serves as an Empirical Risk Minimization-trained (ERM-trained) reference model. In other words, it is trained on the node classification task in a semi-supervised manner to capture spurious correlations between variant patterns and node labels. Given an attributed graph G = (V, E, X) with unknown sensitive attribute values, we sample an ego-graph set {G ğ‘£ } ğ‘£ âˆˆ V , where G ğ‘£ is the ego-graph of the centered node ğ‘£. ğœ‘ takes G ğ‘£ as input and outputs the node representation h ğ‘£ = ğœ‘ (G ğ‘£ ). Due to the similar process between ego-graphs sampling and the message-passing mechanism of GNN, the sampling of ego-graphs can be disregarded. For two connected nodes ğ‘¢ and ğ‘£ in G, ğœ“ takes the concatenation of node representations h ğ‘¢ and h ğ‘£ as input to measure the variant score of edge ğ‘’ ğ‘¢ğ‘£ . Assuming inferring the sensitive attribute ğ‘˜ times, the variant score in the ğ‘–-th inferring can be formulated as:</p><formula xml:id="formula_6">ğ‘¤ ğ‘– ğ‘¢ğ‘£ = ğœ (ğœ“ ([h ğ‘¢ , h ğ‘£ ])),<label>(4)</label></formula><p>where [â€¢, â€¢] denotes the concatenation operation, and ğœ (â€¢) is a sigmoid function. In this context, ğœ“ can be implemented as a linear layer, measuring the probability that edge ğ‘’ ğ‘¢ğ‘£ belongs to the variant pattern. According to Eq. ( <ref type="formula" target="#formula_6">4</ref>), we can obtain a variant score vector w ğ‘– âˆˆ R | E | , which includes variant scores for all edges. w ğ‘– represents variant patterns, i.e., variant subgraphs, capturing the variant correlation between the graph structure and node labels under different sensitive attribute groups. After inferring ğ‘˜ times, we have a variant score vector set {w ğ‘– } ğ‘˜ ğ‘–=1 , representing the variant correlation for various sensitive attributes. Accordingly, we can use these variant patterns to infer sensitive attributes. Specifically, we employ a GNN classifier as the sensitive attribute inference model ğ‘ to generate the sensitive attribute partition. Given the ğ‘–-th variant score vector w ğ‘– , the ğ‘–-th sensitive attribute partition P ğ‘– can be formulated as:</p><formula xml:id="formula_7">P ğ‘– = ğ‘(G, w ğ‘– , ğ‘Œ ),<label>(5)</label></formula><p>where</p><formula xml:id="formula_8">P ğ‘– âˆˆ R | V | Ã—ğ‘¡</formula><p>, and ğ‘¡ is the number of sensitive attribute groups. For instance, in the case of a sensitive attribute like gender, ğ‘¡ = 2.</p><p>To achieve accurate partitioning of sensitive attributes, optimizing ğœ“ and ğ‘ with well-defined objectives is crucial. Our goal is to capture variant patterns that result in significant performance differences across different sensitive attribute groups. Consequently, aligning with the approach of EIIL <ref type="bibr" target="#b9">[10]</ref>, we employ the IRM objective as the optimization objective of ğœ“ and ğ‘. Formally, the optimization objective of SAP can be formulated as follows: max</p><formula xml:id="formula_9">ğœƒ ğœ“ ,ğœƒ ğ‘ âˆ¥â–½ ğ‘¤ R ğ‘† (ğ‘¤ â€¢ ğœ‘, ğ‘)âˆ¥,<label>(6)</label></formula><p>where ğ‘¤ denotes a constant scalar multiplier of 1 for each output dimension, and the empirical risk R ğ‘† (ğœ‘, ğ‘) can be formulated as:</p><formula xml:id="formula_10">R ğ‘† (ğœ‘, ğ‘) = âˆ‘ï¸ ğ‘£ âˆˆ V q ğ‘£ (ğ‘†)L (ğœ‘ (G ğ‘£ ), ğ‘¦ ğ‘£ ),<label>(7)</label></formula><p>where q ğ‘£ (ğ‘†) : ğ‘ ğ‘£ (ğ‘† |G ğ‘£ , w ğ‘– , ğ‘¦ ğ‘£ ) denotes a soft per-partition risk and is a node-level implementation of Eq. ( <ref type="formula" target="#formula_7">5</ref>). Notably, the application of the IRM objective enables inferring sensitive attributes in an unsupervised manner. The inferred sensitive attributes correspond to the demographic group partition with the worst-case fairness performance. In such an unsupervised manner, we can partition sensitive attributes ğ‘˜ times to identify the top ğ‘˜ worst-case partitions, denoted by {P ğ‘– } ğ‘˜ ğ‘–=1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Towards Fairness via Invariant Learning</head><p>In the SIL module, we aim to learn a fair GNN model ğ‘“ including a GNN backbone Î¦ and a classifier ğ‘” from an invariant learning perspective. Prior works have revealed that training model ğ‘“ in an ERM paradigm inevitably results in the capturing of spurious correlations.</p><p>In our scenarios, such spurious correlations are the correlation between the sensitive attribute and node labels, being uncovered as variant patterns through the SAP module. Naturally, based on P ğ‘– , we guarantee the variance across the sensitive attribute groups to optimize ğ‘“ , which is motivated by the objective of EERM <ref type="bibr" target="#b43">[44]</ref>.</p><p>In other words, this objective guides the model to leverage the invariant patterns to yield equal performance on different sensitive attribute groups ğ‘†. Given an attributed graph G = (V, E, X), we can obtain variant score vector set {w ğ‘– } ğ‘˜ ğ‘–=1 and sensitive attributes partition set {P ğ‘– } ğ‘˜ ğ‘–=1 . In the forward of the training pipeline, ğ‘“ takes G, w ğ‘– as input to predict node labels Å¶ = ğ‘“ (G, w ğ‘– ), ğ‘– = 1, 2, ..., ğ‘˜. Thus, optimization objectives of ğ‘“ can be formulated as follows: min</p><formula xml:id="formula_11">ğœƒ ğ‘“ ğ‘‰ ğ‘ğ‘Ÿ ({L ğ‘† ğ‘ğ‘™ğ‘  ( Å¶ , ğ‘Œ )} ğ‘† âˆˆ S ğ‘ğ‘™ğ‘™ ) + ğ›¼ğ‘€ğ‘’ğ‘ğ‘›({L ğ‘† ğ‘ğ‘™ğ‘  ( Å¶, ğ‘Œ )} ğ‘† âˆˆ S ğ‘ğ‘™ğ‘™ ),<label>(8)</label></formula><p>where ğ‘‰ ğ‘ğ‘Ÿ (â€¢) and ğ‘€ğ‘’ğ‘ğ‘›(â€¢) are variance and mean functions, respectively. The sensitive attribute group ğ‘† is derived from P. L ğ‘† ğ‘ğ‘™ğ‘  (â€¢, â€¢) is the classification loss function under ğ‘† and we employ a binary cross-entropy function as L ğ‘ğ‘™ğ‘  in all experiments. ğ›¼ is a hyperparameter to balance two loss terms.</p><p>In Eq.( <ref type="formula" target="#formula_11">8</ref>), the variance loss term aims to minimize the performance difference between various sensitive attribute groups while the mean loss term ensures the predicted accuracy across all sensitive attribute groups.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Training Algorithm</head><p>To further help understand our proposed framework FairINV, we summarize the detailed training algorithm of FairINV, as shown in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">EXPERIMENTS</head><p>In this section, we conduct node classification experiments on several commonly used fairness datasets, including German, Bail, Pokec-z, Pokec-n, and NBA. Table <ref type="table" target="#tab_0">1</ref> presents the statistical information of these datasets. In our experiments, we aim to answer the following three questions: RQ1: Can FairINV improve fairness while maintaining utility performance? RQ2: How does FairINV achieve fairness across various sensitive attributes in a single training session? RQ3: How do relevant hyperparameters and components impact FairINV?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Experimental Settings</head><p>5.1.1 Datasets. Five real-world fairness datasets, namely German, Bail <ref type="bibr" target="#b1">[2]</ref>, Pokec-z, Pokec-n, and NBA <ref type="bibr" target="#b11">[12]</ref>, are employed in our experiments. We give a brief overview of these datasets as follows:</p><p>â€¢ German [5] is constructed by <ref type="bibr" target="#b1">[2]</ref>. H â† ğœ‘(G);</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4:</head><p>for ğ‘¡ = 1 to ğ‘’ğ‘ğ‘œğ‘â„ ğ‘†ğ´ğ‘ƒ do 5:</p><formula xml:id="formula_12">ğ‘¤ ğ‘– ğ‘¢ğ‘£ â† ğœ (ğœ“ ([h ğ‘¢ , h ğ‘£ ])), h ğ‘¢ , h ğ‘£ âˆˆ H, ğ‘’ ğ‘¢ğ‘£ âˆˆ E; 6:</formula><p>// Sensitive attribute partition 7:</p><formula xml:id="formula_13">P ğ‘– â† ğ‘(G, w ğ‘– , ğ‘Œ ), w ğ‘– = {ğ‘¤ ğ‘– ğ‘¢ğ‘£ |ğ‘¢, ğ‘£ âˆˆ V, ğ‘’ ğ‘¢ğ‘£ âˆˆ E}; 8:</formula><p>Calculate loss function according to Eq.( <ref type="formula" target="#formula_9">6</ref>); for ğ‘– = 1 to ğ‘˜ do Update parameters of ğ‘“ by gradient descent; 23: end for 24: return ğ‘“ ; attribute, the goal of German is to classify clients into two credit risks (high or low).</p><p>â€¢ Bail <ref type="bibr" target="#b1">[2]</ref> is a defendants dataset, where defendants in this dataset are released on bail during 1990-2009 in U.S states <ref type="bibr" target="#b21">[22]</ref>. We regard nodes as defendants and edges are decided by the similarity of past criminal records and demographics. Considering "race" as the sensitive attribute, the task is to predict whether defendants will commit a crime after release (bail vs. no bail). â€¢ Pokec-z/n <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b40">41]</ref> is derived from a popular social network application in Slovakia, where Pokec-z and Pokec-n are social network data in two different provinces. Nodes denote users with features such as gender, age, interest, etc. Edge represents the friendship between users. Considering "region" as the sensitive attribute, the task is to predict the working field of the users. â€¢ NBA <ref type="bibr" target="#b11">[12]</ref> is derived from a Kaggle dataset comprising approximately 400 NBA basketball players from the 2016-2017 season. Nodes denote NBA basketball players with features such as performance statistics, age, etc. Edge represents the relationship between these players on Twitter. Considering "nationality (U.S. and overseas players)" as the sensitive attribute, the goal is to predict whether the salary of the player is over the median.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Baselines.</head><p>We compare FairINV with four state-of-the-art fairness methods, including EDITS, NIFTY, FairGNN, and FairVGNN. Among these four methods, EDITS can be summarized as the preprocessing fairness method, while the remaining baselines represent the in-processing approach. A brief overview of these methods is shown as follows:</p><p>â€¢ EDITS <ref type="bibr" target="#b13">[14]</ref> modify graph-structured data by minimizing the Wasserstein distance between two demographics. â€¢ NIFTY <ref type="bibr" target="#b1">[2]</ref> is a fair and stable graph representation learning method. The core idea behind NIFTY is learning GNNs to keep stable w.r.t. the sensitive attribute counterfactual. â€¢ FairGNN <ref type="bibr" target="#b11">[12]</ref> aims to learn fair GNNs with limited sensitive attribute information. To achieve this goal, FairGNN employs the sensitive attribute estimator to predict the sensitive attribute while improving fairness via adversarial learning.</p><p>â€¢ FairVGNN <ref type="bibr" target="#b42">[43]</ref> learns a fair GNN by mitigating the sensitive attribute leakage using adversarial learning and weight clamping technologies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.3">Evaluation Metrics.</head><p>To evaluate the utility performance, we use AUC and F1 scores. Additionally, we employ two commonly used fairness metrics, i.e., Î” ğ·ğ‘ƒ = |ğ‘ƒ ( Å· = 1|ğ‘  = 0) -ğ‘ƒ ( Å· = 1|ğ‘  = 1)| <ref type="bibr" target="#b15">[16]</ref> and <ref type="bibr" target="#b19">[20]</ref>, to evaluate the fairness performance.</p><formula xml:id="formula_14">Î” ğ¸ğ‘‚ = |ğ‘ƒ ( Å· = 1|ğ‘¦ = 1, ğ‘  = 0) -ğ‘ƒ ( Å· = 1|ğ‘¦ = 1, ğ‘  = 1)|</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.4">Implementation Details.</head><p>For all methods, including FairINV, we use a multi-layer GNN model consisting of a GNN backbone Î¦ and a 1-layer linear classifier ğ‘”. To validate the generalizability of FairINV on various backbones, we employ the following GNN backbones: a 1-layer GCN <ref type="bibr" target="#b24">[25]</ref>, a 1-layer GIN <ref type="bibr" target="#b45">[46]</ref>, and a 2-layer GraphSAGE <ref type="bibr" target="#b18">[19]</ref>. Here, the hidden dimension of all GNN backbones is set to 16 for all datasets. Hyperparameter settings for all baseline methods adhere to the guidelines provided by the respective authors. We conduct all experiments 5 times and reported average results.</p><p>For FairINV, we utilize the Adam optimizer with the learning rate ğ‘™ğ‘Ÿ = 1 Ã— 10 -2 , epochs=1000, and the weight decay = 1 Ã— 10 -5 . Using the same optimizer, the learning rate ğ‘™ğ‘Ÿ ğ‘ ğ‘ for training the SAP modules are set to {0.1, 0.1, 0.01, 0.5, 0.1} for German, Bail, Pokec-z, Pokec-n, and NBA datasets, respectively. Meanwhile, we set the balanced parameter ğ›¼ to {10, 10, 10, 1, 1} for German, Bail, Pokec-z, Pokec-n, and NBA datasets, respectively. The partition times ğ‘˜ and the number of sensitive attribute groups ğ‘¡ are fixed at 3 and 2 for all datasets. In the SAP module, a 1-layer linear layer is used as the variant inference model ğœ“ . We employ a model with the same structure as the GNN model (Î¦ and ğ‘”) as a sensitive attribute inference model ğ‘. Meanwhile, ğœ‘ has the same structure as the GNN model (Î¦ and ğ‘”) and is trained by minimizing the cross-entropy loss function. For ğœ‘ and the SAP module, we set the training epoch to 500. Due to all baselines using the sensitive attribute, FairINV incorporates the sensitive attribute into the original node features for fair comparison. Moreover, all evaluations of FairINV are conducted on a single NVIDIA RTX 4090 GPU with 24GB memory. All models are implemented with PyTorch and PyTorch-Geometric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Comparison Study</head><p>To answer RQ1, we conduct a comparison study between FairINV and four baseline methods for the node classification task across five datasets. Specifically, we verify the effectiveness of FairINV on three GNN backbones, i.e., GIN, and GraphSAGE. Limited by the space, we only present the comparison results on the GCN backbone and provide more results in Appendix A. As shown in Table <ref type="table" target="#tab_3">2</ref>, the following observations can be seen: ( <ref type="formula" target="#formula_0">1</ref>) FairINV outperforms all baseline methods in terms of both utility and fairness in most cases. <ref type="bibr" target="#b1">(2)</ref> In instances where FairINV exhibits relatively lower performance, the best-performing baseline method surpasses FairINV by a slight margin. (3) FairINV improves fairness while maintaining utility performance, as evidenced by the performance improvement compared with vanilla GCN.</p><p>The first two observations verify the effectiveness of FairINV on fairness performance, simultaneously showcasing the state-of-theart performance achieved by FairINV. As for the last observation, the potential explanation lies in FairINV's adherence to the invariance principle <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b37">38]</ref>, i.e., (1) sufficiency property and (2) invariance property. The sufficiency property emphasizes the necessity of adequate predictive abilities for the downstream task, which explains the preservation of the utility performance of FairINV. Meanwhile, the invariance property assumes consistency across different environments, signifying the invariance across the sensitive attribute groups in FairINV. Consequently, this property serves as the underlying reason for FairINV's superior fairness performance. Overall, leveraging invariant learning, FairINV captures invariant subgraphs with sufficient information for the downstream task while learning to be invariant across different sensitive attribute groups. Thus, FairINV improves fairness while preserving utility performance. In addition, as shown in Appendix A, similar results can be observed from the experiments on GIN and GraphSAGE backbones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Generalizing to Various Sensitive Attributes</head><p>To answer RQ2, we generalize FairINV to various sensitive attribute scenarios. Specifically, we employ FairINV once to train a GNN model and then evaluate the fairness performance of this GNN model toward various sensitive attributes. Table <ref type="table" target="#tab_4">3</ref> presents the results of various sensitive attributes and inferior results compared to vanilla GCN are marked with a gray background. We only present results on four datasets except for the NBA dataset due to the lack of suitable node features as the sensitive attribute. When the sensitive attribute is "Age", we set the median of age as the threshold to obtain binary values for the sensitive attribute. Furthermore, we provide the comparison results of FairINV and baseline methods in multi-sensitive attribute scenarios, as detailed in the Appendix B.</p><p>We make the following observations from this table : (1) Fair-INV achieves superior performance compared with vanilla GCN in terms of both fairness and utility. This observation demonstrates that FairINV improves the fairness of GNNs towards various sensitive attributes in a single training session. (2) In some instances, FairINV exhibits slightly inferior fairness performance compared to vanilla GCN. We attribute this to the fact that the sensitive attribute groups partitioned by the SAP module are unrelated to the sensitive attributes we have selected. This is primarily due to the model itself making fairly equitable decisions concerning the sensitive attributes we have chosen. In other words, when grouped according to the selected sensitive attributes, the variability values are relatively small. Consequently, the sensitive attribute groups partitioned by  SAP when maximizing variability are unrelated to the groups corresponding to such low variability. For the results with "Age" as the sensitive attribute on the Pokec-n dataset, despite decisions of the model being extremely unfair with respect to the sensitive attribute, FairINV does not improve fairness. We attribute this to the aggressive partitioning of age into binary-sensitive attributes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Ablation Study</head><p>To answer RQ3, we conduct an ablation study to investigate the impact of each component of FairINV on improving fairness and maintaining utility. Specifically, we investigate the effect of three components including the variant inference model ğœ“ , the SAP module, and the SIL module, denoted by ğ¹ğ‘ğ‘–ğ‘Ÿğ¼ ğ‘ğ‘‰ -ğ‘‰ ğ¼ , ğ¹ğ‘ğ‘–ğ‘Ÿğ¼ ğ‘ğ‘‰ -ğ‘†ğ´ğ‘ƒ , and ğ¹ğ‘ğ‘–ğ‘Ÿğ¼ ğ‘ğ‘‰ -ğ‘†ğ¼ ğ¿ . ğ¹ğ‘ğ‘–ğ‘Ÿğ¼ ğ‘ğ‘‰ -ğ‘‰ ğ¼ removes ğœ“ , replacing w predicted by ğœ“ with random numbers. ğ¹ğ‘ğ‘–ğ‘Ÿğ¼ ğ‘ğ‘‰ -ğ‘†ğ´ğ‘ƒ removes the SAP module, replacing P predicted by ğ‘ with the sensitive attribute ground truth. ğ¹ğ‘ğ‘–ğ‘Ÿğ¼ ğ‘ğ‘‰ -ğ‘†ğ¼ ğ¿ removes the SIL module, replacing the objective shown in Eq. ( <ref type="formula" target="#formula_11">8</ref>) with minimizing the IRM objective shown in Eq. <ref type="bibr" target="#b5">(6)</ref>. Figure <ref type="figure" target="#fig_5">4</ref> presents the ablation results on five datasets.</p><p>From this figure, we observe that the removal of the SIL module leads to a decline in both utility and fairness, implying the significant impact of SIL on FairINV. Furthermore, from the results of ğ¹ğ‘ğ‘–ğ‘Ÿğ¼ ğ‘ğ‘‰ -ğ‘†ğ´ğ‘ƒ , even when using the ground truth of sensitive attributes to replace the predicted sensitive attribute partition P by SAP, the performance of FairINV is still affected. This experimental phenomenon is consistent with previous research results on invariant learning without environmental labels. Finally, we find that removing the variant inference model affects the fairness performance of FairINV, indicating the importance of the variant inference model in capturing variant patterns.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Hyperparameters Sensitivity</head><p>To further answer RQ3, we investigate the parameter sensitivity of FairINV w.r.t. two hyperparameters, i.e., the balanced parameter ğ›¼ and the learning rate ğ‘™ğ‘Ÿ ğ‘ ğ‘ of SAP. Notably, the setting of ğ‘™ğ‘Ÿ ğ‘ ğ‘ benefits from the independent training of SAP. We vary ğ›¼ and ğ‘™ğ‘Ÿ ğ‘ ğ‘ within the range of {0.001, 0.01, 0.1, 0.5, 1, 10, 100}. We only illustrate results on the Bail and Pokec-z datasets due to similar observations on other datasets. We observe that, with a wide range of variations in two parameters, the performance of FairINV remains stable. However, a sharp decline in both utility and fairness performance is noted when the value of ğ›¼ is less than 0.01.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Training Time Comparison</head><p>To further investigate the computational cost of FairINV compared to baseline methods, we conduct a training time comparison experiment across all datasets. Specifically, we repeat each method five times and record the total training time. We set ğ‘˜ to 1 and 3 to implement two variants of FairINV, namely FairINV-1, and FairINV-3, representing FairINV trained for the single sensitive attribute and three-sensitive attribute scenarios, respectively. As shown in Table <ref type="table" target="#tab_5">4</ref>, FairINV exhibits lower computational costs on   Furthermore, we also observe that the training time of FairINV-3 is significantly longer than that of FairINV-1. A possible explanation for this phenomenon is the high computational cost associated with </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A COMPARISON FOR VARIOUS GNN BACKBONES</head><p>To further investigate the generalizability of FairINV across various GNN backbones, we conduct comparative experiments using GIN <ref type="bibr" target="#b45">[46]</ref> and GraphSAGE <ref type="bibr" target="#b18">[19]</ref> backbones. As shown in Tables <ref type="table" target="#tab_6">5</ref> and<ref type="table" target="#tab_7">6</ref>, we compare FairINV with three fairness baseline methods, including NIFTY <ref type="bibr" target="#b1">[2]</ref>, FairGNN <ref type="bibr" target="#b11">[12]</ref>, and FairVGNN <ref type="bibr" target="#b42">[43]</ref>. From these two tables, we can observe that FairINV consistently outperforms the three fairness baseline methods in most cases. Furthermore, upon summarizing the comparison results across the three backbones, we find that most fairness methods, including FairINV, consistently enhance both utility and fairness performance on the Bail dataset. This observation suggests an underlying relationship between fairness and utility in the Bail dataset, providing a promising avenue for future research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B COMPARISON FOR MULTI-SENSITIVE ATTRIBUTES</head><p>We further present a comparison of FairINV and two baseline methods in various sensitive attribute scenarios, as shown in Table <ref type="table" target="#tab_8">7</ref>.</p><p>Due to the single sensitive attribute setting of these two methods, it is necessary to extend them by modifying the optimization objectives. For NIFTY <ref type="bibr" target="#b1">[2]</ref>, we simultaneously flap various sensitive attributes to construct the counterfactual graph. For FairGNN <ref type="bibr" target="#b11">[12]</ref>, we train multiple sensitive attribute estimators and discriminators simultaneously. Although existing methods can be extended to multi-sensitive attribute scenarios, their performance might be negatively affected since they are not explicitly designed for multiple sensitive attributes. From Table <ref type="table" target="#tab_8">7</ref>, we can observe that FairINV is the only method that can achieve fairness and maintain utility. In most cases, FairINV's fairness performance is better than baseline methods.  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An illustration of comparison between prior works and our work (FairINV). (a) Prior works (pre-process and in-process) achieve fairness toward the specific sensitive attribute; (b) Our work trains fair GNNs toward various sensitive attributes in a single training session without accessing the sensitive attribute.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Structural causal model for GNNs prediction. (a) The fairness issue on graphs can be caused by two causal pathways, i.e., ğ‘† â†’ ğ‘Œ and ğ‘† â†’ G â†’ ğ‘Œ ; (b) Prior works either exclusively eliminate the causal pathway ğ‘† â†’ G or exclusively eradicate the causal pathway ğ‘† â†’ ğ‘Œ ; (c) FairINV tackles the fairness issue through blocking both two causal pathways.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The overview of FairINV. FairINV includes two stages: Sensitive Attributes Partition (SAP) and Sensitive Invariant Learning (SIL).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>18 :Å¶</head><label>18</label><figDesc>â† ğ‘“ (G, w ğ‘– );19:Calculate loss function according to Eq.(8) and P ğ‘– ;</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The results of ablation study on all datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Parameters sensitivity analysis on Bail.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Parameters sensitivity analysis on Pokec-z.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>Datasets statistics.</figDesc><table><row><cell cols="2">Dataset German</cell><cell>Bail</cell><cell cols="2">Pokec-z Pokec-n</cell><cell>NBA</cell></row><row><cell>#Nodes</cell><cell>1,000</cell><cell>18,876</cell><cell>67,796</cell><cell>66,569</cell><cell>403</cell></row><row><cell>#Edges</cell><cell>22,242</cell><cell cols="2">321,308 617,958</cell><cell>583,616</cell><cell>21,242</cell></row><row><cell>#Attr.</cell><cell>27</cell><cell>18</cell><cell>277</cell><cell>266</cell><cell>95</cell></row><row><cell>Sens.</cell><cell>Gender</cell><cell>Race</cell><cell>Region</cell><cell>Region</cell><cell>Nationality</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>Training Algorithm of FairINVInput: G = (V, E, X) without the sensitive attribute ğ‘†, node labels Y, the pre-trained GNN backbone ğœ‘, the variant inference model ğœ“ , the sensitive attribute inference model ğ‘, GNN model ğ‘“ ={Î¦, ğ‘”}, partition time ğ‘˜, and hyperparameters ğ›¼. Output: Trained inference GNN model ğ‘“ .</figDesc><table /><note><p>Specifically, German includes clients' data in a German bank, e.g., gender, and loan amount. Nodes represent clients in the German bank. The edges in the German dataset are constructed according to individual similarity. Regarding "gender" as the sensitive Algorithm 1 1: // SAP module 2: for ğ‘– = 1 to ğ‘˜ do 3:</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Comparison results of FairINV and baseline fairness methods on GCN backbone. In each row, the best result is indicated in bold, while the runner-up result is marked with an underline. OOM: out-of-memory on a GPU with 24GB memory.</figDesc><table><row><cell></cell><cell cols="7">Datasets Metrics Vanilla GCN</cell><cell></cell><cell cols="2">EDITS</cell><cell>NIFTY</cell><cell>FairGNN</cell><cell>FairVGNN</cell><cell>FairINV</cell></row><row><cell></cell><cell cols="3">German</cell><cell cols="2">AUC F1 Î” ğ·ğ‘ƒ (â†“)</cell><cell cols="2">65.90 Â± 0.83 77.32 Â± 1.20 36.29 Â± 4.64</cell><cell cols="3">69.89 Â± 3.23 67.77 Â± 4.30 82.01 Â± 0.91 81.43 Â± 0.54 2.38 Â± 1.36 2.64 Â± 2.25</cell><cell>67.35Â±2.13 82.01Â±0.26 3.49Â±2.15</cell><cell>72.38 Â± 1.09 69.11 Â± 1.80 81.94 Â± 0.26 82.36 Â± 0.35 1.44 Â± 2.04 0.76 Â± 1.24</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Î” ğ¸ğ‘œ (â†“)</cell><cell cols="2">31.35 Â± 4.39</cell><cell cols="3">3.03 Â± 1.77</cell><cell>2.52 Â± 2.88</cell><cell>3.40Â±2.15</cell><cell>1.51 Â± 2.11</cell><cell>0.15 Â± 0.29</cell></row><row><cell></cell><cell cols="3">Bail</cell><cell cols="2">AUC F1 Î” ğ·ğ‘ƒ (â†“)</cell><cell cols="2">87.13 Â± 0.31 78.98 Â± 0.67 9.18 Â± 0.59</cell><cell cols="3">87.92 Â± 1.83 79.62 Â± 1.80 87.27 Â± 0.76 79.45 Â± 1.48 67.19 Â± 2.63 77.67 Â± 1.33 79.56 Â± 0.29 78.80 Â± 3.71 87.05 Â± 0.39 88.53 Â± 1.83 8.03 Â± 0.97 3.52 Â± 0.72 6.72 Â± 0.60 6.31 Â± 0.77 3.58 Â± 1.61</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Î” ğ¸ğ‘œ (â†“)</cell><cell cols="2">4.43 Â± 0.37</cell><cell cols="3">5.80 Â± 0.73</cell><cell>2.82 Â± 0.82</cell><cell>4.49 Â± 1.00</cell><cell>5.12 Â± 1.40</cell><cell>2.15 Â± 1.24</cell></row><row><cell></cell><cell cols="3">Pokec-z</cell><cell cols="2">AUC F1 Î” ğ·ğ‘ƒ (â†“)</cell><cell cols="2">76.42 Â± 0.13 70.32 Â± 0.20 3.91 Â± 0.35</cell><cell></cell><cell cols="2">OOM</cell><cell>71.59 Â± 0.17 76.02 Â± 0.15 67.13 Â± 1.66 68.84 Â± 3.46 3.06 Â± 1.85 2.93 Â± 2.83</cell><cell>75.52 Â± 0.06 70.45 Â± 0.57 70.78 Â± 0.50 75.79 Â± 0.08 3.30 Â± 0.87 2.70 Â± 0.96</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Î” ğ¸ğ‘œ (â†“)</cell><cell cols="2">4.59 Â± 0.34</cell><cell></cell><cell></cell><cell>3.86 Â± 1.65</cell><cell>2.04 Â± 2.27</cell><cell>3.19 Â± 1.00</cell><cell>2.23 Â± 0.66</cell></row><row><cell></cell><cell cols="3">Pokec-n</cell><cell cols="2">AUC F1 Î” ğ·ğ‘ƒ (â†“)</cell><cell cols="2">73.87 Â± 0.08 65.55 Â± 0.13 2.83 Â± 0.46</cell><cell></cell><cell cols="2">OOM</cell><cell>69.43 Â± 0.31 73.49 Â± 0.28 61.55 Â± 1.05 64.80 Â± 0.89 5.96 Â± 1.80 2.26 Â± 1.19</cell><cell>72.72 Â± 0.93 62.35 Â± 1.14 4.38 Â± 1.73</cell><cell>73.55 Â± 0.16 65.19 Â± 0.62 1.24 Â± 0.64</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Î” ğ¸ğ‘œ (â†“)</cell><cell cols="2">3.66 Â± 0.43</cell><cell></cell><cell></cell><cell>7.75 Â± 1.53</cell><cell>3.21 Â± 2.28</cell><cell>6.74 Â± 1.87</cell><cell>2.80 Â± 0.78</cell></row><row><cell></cell><cell cols="3">NBA</cell><cell cols="2">AUC F1 Î” ğ·ğ‘ƒ (â†“)</cell><cell cols="2">66.09 Â± 0.98 61.32 Â± 2.53 28.80 Â± 4.17</cell><cell cols="3">65.91 Â± 5.19 68.88 Â± 0.93 72.53 Â± 0.96 64.73 Â± 2.34 61.42 Â± 8.26 67.41 Â± 2.92 60.18 Â± 20.18 60.32 Â± 5.79 67.56 Â± 1.30 66.18 Â± 2.84 6.09 Â± 5.1 5.41 Â± 2.78 6.44 Â± 6.74 3.48 Â± 3.62 1.98 Â± 3.15</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Î” ğ¸ğ‘œ (â†“)</cell><cell cols="2">20.00 Â± 9.43</cell><cell></cell><cell cols="2">6.0 Â± 5.33</cell><cell>3.43 Â± 1.78</cell><cell>2.64 Â± 2.61</cell><cell>3.33 Â± 3.65</cell><cell>2.67 Â± 3.89</cell></row><row><cell cols="2">Vanilla</cell><cell cols="3">FairINV VI</cell><cell>FairINV SAP</cell><cell>FairINV SIL</cell><cell></cell><cell cols="2">FairINV</cell></row><row><cell>$8&amp;</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>DP</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>G e r m a n B a il</cell><cell>P o k e c</cell><cell>z</cell><cell>P o k e c</cell><cell>n N B A</cell><cell></cell><cell>G e r m a n B a il</cell><cell>P o k e c</cell><cell>z</cell><cell>P o k e c</cell><cell>n N B A</cell></row><row><cell cols="4">D$8&amp;SHUIRUPDQFH</cell><cell></cell><cell></cell><cell cols="4">E DPSHUIRUPDQFH</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Results of various sensitive attributes. The results in which FairINV exhibits inferior performance compared to vanilla GCN are highlighted with a gray background. Sens.Attr.: Sensitive Attribute</figDesc><table><row><cell>Datasets</cell><cell></cell><cell>Vanilla GCN</cell><cell>FairINV</cell><cell>Vanilla GCN</cell><cell>FairINV</cell><cell>Vanilla GCN</cell><cell>FairINV</cell><cell>Vanilla GCN</cell><cell>FairINV</cell></row><row><cell></cell><cell>Sens.Attr.</cell><cell>Age</cell><cell></cell><cell cols="2">Gender</cell><cell cols="2">ForeignWorker</cell><cell cols="2">Single</cell></row><row><cell></cell><cell>AUC</cell><cell>65.90 Â± 0.83</cell><cell>69.11 Â± 1.80</cell><cell>65.90 Â± 0.83</cell><cell>69.11 Â± 1.80</cell><cell>65.90 Â± 0.83</cell><cell>69.11 Â± 1.80</cell><cell>65.90 Â± 0.83</cell><cell>69.11 Â± 1.80</cell></row><row><cell>German</cell><cell>F1</cell><cell>77.32 Â± 1.20</cell><cell>82.36 Â± 0.35</cell><cell>77.32 Â± 1.20</cell><cell>82.36 Â± 0.35</cell><cell>77.32 Â± 1.20</cell><cell>82.36 Â± 0.35</cell><cell>77.32 Â± 1.20</cell><cell>82.36 Â± 0.35</cell></row><row><cell></cell><cell>Î” ğ·ğ‘ƒ (â†“)</cell><cell>20.18 Â± 5.17</cell><cell>0.48 Â± 0.38</cell><cell>36.29 Â± 4.64</cell><cell>0.76 Â± 1.24</cell><cell>8.45 Â± 7.05</cell><cell>2.31 Â± 3.81</cell><cell>34.13 Â± 3.29</cell><cell>2.64 Â± 4.15</cell></row><row><cell></cell><cell>Î” ğ¸ğ‘‚ (â†“)</cell><cell>15.83 Â± 3.47</cell><cell>0.46 Â± 0.52</cell><cell>31.35 Â± 4.39</cell><cell>0.15 Â± 0.29</cell><cell>5.66 Â± 2.61</cell><cell>2.34 Â± 4.09</cell><cell>27.26 Â± 4.96</cell><cell>1.48 Â± 2.96</cell></row><row><cell></cell><cell>Sens.Attr.</cell><cell>Race</cell><cell></cell><cell cols="2">Gender</cell><cell cols="2">MARRIED</cell><cell cols="2">WORKREL</cell></row><row><cell></cell><cell>AUC</cell><cell>87.13 Â± 0.31</cell><cell>88.53 Â± 1.83</cell><cell>87.13 Â± 0.32</cell><cell>88.53 Â± 1.83</cell><cell>87.13 Â± 0.33</cell><cell>88.53 Â± 1.83</cell><cell>87.13 Â± 0.34</cell><cell>88.53 Â± 1.83</cell></row><row><cell>Bail</cell><cell>F1</cell><cell>78.98 Â± 0.67</cell><cell>78.80 Â± 3.71</cell><cell>78.98 Â± 0.68</cell><cell>78.80 Â± 3.71</cell><cell>78.98 Â± 0.69</cell><cell>78.80 Â± 3.71</cell><cell>78.98 Â± 0.70</cell><cell>78.80 Â± 3.71</cell></row><row><cell></cell><cell>Î” ğ·ğ‘ƒ (â†“)</cell><cell>9.18 Â± 0.59</cell><cell>3.58 Â± 1.61</cell><cell>11.51 Â± 0.22</cell><cell>12.09 Â± 5.58</cell><cell>2.36 Â± 0.54</cell><cell>3.40 Â± 1.23</cell><cell>0.34 Â± 0.09</cell><cell>0.55 Â± 0.47</cell></row><row><cell></cell><cell>Î” ğ¸ğ‘‚ (â†“)</cell><cell>4.43 Â± 0.37</cell><cell>2.15 Â± 1.24</cell><cell>1.95 Â± 0.22</cell><cell>3.53 Â± 2.12</cell><cell>3.13 Â± 0.52</cell><cell>4.81 Â± 2.34</cell><cell>1.25 Â± 0.28</cell><cell>1.18 Â± 0.67</cell></row><row><cell></cell><cell>Sens.Attr.</cell><cell cols="2">Gender</cell><cell cols="2">Region</cell><cell>Age</cell><cell></cell><cell cols="2">Hair color indicator</cell></row><row><cell></cell><cell>AUC</cell><cell>76.42 Â± 0.13</cell><cell>75.79 Â± 0.08</cell><cell>76.42 Â± 0.13</cell><cell>75.79 Â± 0.08</cell><cell>76.42 Â± 0.13</cell><cell>75.79 Â± 0.08</cell><cell>76.42 Â± 0.13</cell><cell>75.79 Â± 0.08</cell></row><row><cell>Pokec-z</cell><cell>F1</cell><cell>70.32 Â± 0.20</cell><cell>70.78 Â± 0.50</cell><cell>70.32 Â± 0.20</cell><cell>70.78 Â± 0.50</cell><cell>70.32 Â± 0.20</cell><cell>70.78 Â± 0.5</cell><cell>70.32 Â± 0.20</cell><cell>70.78 Â± 0.50</cell></row><row><cell></cell><cell>Î” ğ·ğ‘ƒ (â†“)</cell><cell>3.15 Â± 0.24</cell><cell>2.38 Â± 1.03</cell><cell>3.91 Â± 0.35</cell><cell>2.70 Â± 0.96</cell><cell>33.09 Â± 0.57</cell><cell>27.49 Â± 3.29</cell><cell>18.70 Â± 0.9</cell><cell>15.63 Â± 1.09</cell></row><row><cell></cell><cell>Î” ğ¸ğ‘‚ (â†“)</cell><cell>5.25 Â± 0.48</cell><cell>5.08 Â± 0.98</cell><cell>4.59 Â± 0.34</cell><cell>2.23 Â± 0.66</cell><cell>36.32 Â± 0.7</cell><cell>29.19 Â± 3.68</cell><cell>18.73 Â± 0.8</cell><cell>14.31 Â± 1.76</cell></row><row><cell></cell><cell>Sens.Attr.</cell><cell cols="2">Gender</cell><cell cols="2">Region</cell><cell>Age</cell><cell></cell><cell cols="2">Hair color indicator</cell></row><row><cell></cell><cell>AUC</cell><cell>73.87 Â± 0.08</cell><cell>73.55 Â± 0.16</cell><cell>73.87 Â± 0.08</cell><cell>73.55 Â± 0.16</cell><cell>73.87 Â± 0.08</cell><cell>73.55 Â± 0.16</cell><cell>73.87 Â± 0.08</cell><cell>73.55 Â± 0.16</cell></row><row><cell>Pokec-n</cell><cell>F1</cell><cell>65.55 Â± 0.13</cell><cell>65.19 Â± 0.62</cell><cell>65.55 Â± 0.13</cell><cell>65.19 Â± 0.62</cell><cell>65.55 Â± 0.13</cell><cell>65.19 Â± 0.62</cell><cell>65.55 Â± 0.13</cell><cell>65.19 Â± 0.62</cell></row><row><cell></cell><cell>Î” ğ·ğ‘ƒ (â†“)</cell><cell>6.36 Â± 0.20</cell><cell>6.44 Â± 0.70</cell><cell>2.83 Â± 0.46</cell><cell>1.24 Â± 0.64</cell><cell>40.05 Â± 0.73</cell><cell>40.11 Â± 0.83</cell><cell>14.96 Â± 0.68</cell><cell>12.8 Â± 1.90</cell></row><row><cell></cell><cell>Î” ğ¸ğ‘‚ (â†“)</cell><cell>13.18 Â± 0.41</cell><cell>13.13 Â± 1.26</cell><cell>3.66 Â± 0.43</cell><cell>2.80 Â± 0.78</cell><cell>42.82 Â± 0.60</cell><cell>43.53 Â± 0.50</cell><cell>12.49 Â± 0.51</cell><cell>12.3 Â± 1.65</cell></row><row><cell></cell><cell></cell><cell>$8&amp;</cell><cell></cell><cell>DP</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>lr sp</cell><cell></cell><cell>lr sp</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">(a) AUC performance</cell><cell cols="3">(b) Î” ğ·ğ‘ƒ performance</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Comparison of training time for both baseline methods and FairINV.</figDesc><table><row><cell cols="7">Datasets EDITS NIFTY FairGNN FairVGNN FairINV-1 FairINV-3</cell></row><row><cell cols="2">German 82.88 s</cell><cell>94.91 s</cell><cell>50.42 s</cell><cell>539.54 s</cell><cell>56.20 s</cell><cell>114.97 s</cell></row><row><cell>Bail</cell><cell cols="2">338.28 s 106.22 s</cell><cell>373.54 s</cell><cell>988.19 s</cell><cell>92.60 s</cell><cell>200.71 s</cell></row><row><cell>Pokec-z</cell><cell>OOM</cell><cell>139.83 s</cell><cell>302.11 s</cell><cell>963.39 s</cell><cell>113.38 s</cell><cell>210.29 s</cell></row><row><cell>Pokec-n</cell><cell>OOM</cell><cell>132.03 s</cell><cell>261.73 s</cell><cell>1072.48 s</cell><cell>102.33 s</cell><cell>191.87 s</cell></row><row><cell>NBA</cell><cell>81.50 s</cell><cell>92.08 s</cell><cell>46.65 s</cell><cell>534.02 s</cell><cell>57.64 s</cell><cell>112.75 s</cell></row><row><cell cols="7">the SAP module. Overall, the experimental results demonstrate that</cell></row><row><cell cols="7">FairINV has lower computational costs than the baseline methods.</cell></row><row><cell cols="3">6 CONCLUSION</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="7">In this work, we investigate the universal fairness problem, i.e.,</cell></row><row><cell cols="7">training a fair GNN toward various sensitive attributes in a single</cell></row><row><cell cols="7">training session. To address this problem, we first formulate such</cell></row><row><cell cols="7">a problem from a graph invariant learning point of view. Then,</cell></row><row><cell cols="7">we propose a universal graph fairness approach, namely, FairINV.</cell></row><row><cell cols="7">The core idea behind FairINV is to eliminate spurious correlations</cell></row><row><cell cols="7">between the sensitive attributes and labels in a graph variant learn-</cell></row><row><cell cols="7">ing way. Experiments on several real-world datasets validate the</cell></row><row><cell cols="7">effectiveness of FairINV in both fairness and utility performance.</cell></row><row><cell cols="7">We leave validation on other downstream tasks, e.g., edge-level, as</cell></row><row><cell cols="7">future works. In addition, due to FairINV only focusing on group</cell></row><row><cell cols="7">fairness, future works will focus on considering fine-grained fair-</cell></row><row><cell cols="4">ness, e.g., individual fairness.</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 5 :</head><label>5</label><figDesc>Comparison results of FairINV and baseline fairness methods on GIN backbone. In each row, the best result is indicated in bold, while the runner-up result is marked with an underline.</figDesc><table><row><cell cols="3">Datasets Metrics Vanilla GIN</cell><cell>NIFTY</cell><cell>FairGNN</cell><cell>FairVGNN</cell><cell>FairINV</cell></row><row><cell></cell><cell>AUC</cell><cell>71.86 Â± 1.55</cell><cell cols="3">66.70 Â± 4.91 72.78 Â± 1.19 69.23 Â± 3.08</cell><cell>70.08 Â± 2.17</cell></row><row><cell>German</cell><cell>F1 Î” ğ·ğ‘ƒ (â†“)</cell><cell>82.35 Â± 0.55 14.92 Â± 5.52</cell><cell>80.33 Â± 3.76 5.28 Â± 6.67</cell><cell cols="3">81.48 Â± 1.55 82.41 Â± 0.62 82.57 Â± 0.22 15.63 Â± 5.2 2.71 Â± 3.16 1.02 Â± 1.17</cell></row><row><cell></cell><cell>Î” ğ¸ğ‘‚ (â†“)</cell><cell>8.24 Â± 6.31</cell><cell>7.39 Â± 8.49</cell><cell>10.0 Â± 5.51</cell><cell>0.91 Â± 1.41</cell><cell>0.17 Â± 0.34</cell></row><row><cell></cell><cell>AUC</cell><cell>75.69 Â± 7.75</cell><cell>79.49 Â± 6.65</cell><cell cols="3">83.96 Â± 0.61 86.33 Â± 1.05 86.05 Â± 0.81</cell></row><row><cell>Bail</cell><cell>F1 Î” ğ·ğ‘ƒ (â†“)</cell><cell cols="5">64.26 Â± 8.73 65.20 Â± 11.22 73.10 Â± 1.28 87.47 Â± 0.50 75.66 Â± 3.01 8.44 Â± 2.94 5.38 Â± 1.16 8.93 Â± 1.63 6.95 Â± 0.41 7.35 Â± 1.71</cell></row><row><cell></cell><cell>Î” ğ¸ğ‘‚ (â†“)</cell><cell>6.57 Â± 1.36</cell><cell>4.00 Â± 2.21</cell><cell>6.65 Â± 1.77</cell><cell>6.97 Â± 1.18</cell><cell>4.80 Â± 1.21</cell></row><row><cell></cell><cell>AUC</cell><cell cols="2">75.04 Â± 0.39 72.52 Â± 2.66</cell><cell cols="2">74.70 Â± 1.21 74.51 Â± 0.12</cell><cell>74.90 Â± 1.28</cell></row><row><cell>Pokec-z</cell><cell>F1 Î” ğ·ğ‘ƒ (â†“)</cell><cell>68.45 Â± 1.23 3.24 Â± 2.09</cell><cell>67.93 Â± 1.26 3.56 Â± 2.95</cell><cell cols="3">67.30 Â± 0.77 69.70 Â± 0.57 67.47 Â± 1.98 3.96 Â± 1.47 1.93 Â± 1.23 1.66 Â± 1.16</cell></row><row><cell></cell><cell>Î” ğ¸ğ‘‚ (â†“)</cell><cell>4.26 Â± 2.27</cell><cell>3.51 Â± 2.20</cell><cell>5.22 Â± 1.51</cell><cell>2.71 Â± 1.20</cell><cell>2.06 Â± 0.89</cell></row><row><cell></cell><cell>AUC</cell><cell>74.06 Â± 0.62</cell><cell>72.12 Â± 1.65</cell><cell cols="3">73.25 Â± 1.04 72.71 Â± 0.48 74.39 Â± 0.48</cell></row><row><cell>Pokec-n</cell><cell>F1 Î” ğ·ğ‘ƒ (â†“)</cell><cell>62.39 Â± 0.51 2.64 Â± 1.28</cell><cell>60.25 Â± 4.53 3.34 Â± 1.78</cell><cell cols="3">60.88 Â± 2.99 65.56 Â± 1.03 62.09 Â± 2.37 2.25 Â± 1.33 6.13 Â± 1.59 1.37 Â± 0.91</cell></row><row><cell></cell><cell>Î” ğ¸ğ‘‚ (â†“)</cell><cell>6.77 Â± 2.36</cell><cell>6.88 Â± 2.11</cell><cell>2.68 Â± 1.59</cell><cell>7.00 Â± 1.80</cell><cell>2.03 Â± 2.04</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 6 :</head><label>6</label><figDesc>Comparison results of FairINV and baseline fairness methods on GraphSAGE backbone. In each row, the best result is indicated in bold, while the runner-up result is marked with an underline.</figDesc><table><row><cell cols="3">Datasets Metrics Vanilla GraphSAGE</cell><cell>NIFTY</cell><cell>FairGNN</cell><cell>FairVGNN</cell><cell>FairINV</cell></row><row><cell></cell><cell>AUC</cell><cell>74.41 Â± 0.80</cell><cell cols="4">68.45 Â± 3.8 75.25 Â± 0.9 73.79 Â± 1.67 73.64 Â± 2.88</cell></row><row><cell>German</cell><cell>F1 Î” ğ·ğ‘ƒ (â†“)</cell><cell>80.74 Â± 1.81 26.89 Â± 6.23</cell><cell cols="4">77.35 Â± 0.69 79.45 Â± 2.69 82.20 Â± 0.48 82.49 Â± 0.23 5.93 Â± 7.03 27.45 Â± 4.59 2.98 Â± 2.75 0.34 Â± 0.58</cell></row><row><cell></cell><cell>Î” ğ¸ğ‘‚ (â†“)</cell><cell>18.36 Â± 6.91</cell><cell cols="2">5.27 Â± 4.02 20.21 Â± 4.48</cell><cell>1.38 Â± 0.89</cell><cell>0.17 Â± 0.34</cell></row><row><cell></cell><cell>AUC</cell><cell>90.79 Â± 1.14</cell><cell cols="4">91.18 Â± 1.32 91.48 Â± 0.28 92.01 Â± 0.68 91.85 Â± 0.43</cell></row><row><cell>Bail</cell><cell>F1 Î” ğ·ğ‘ƒ (â†“)</cell><cell>80.82 Â± 1.81 2.45 Â± 1.31</cell><cell cols="4">80.54 Â± 1.52 81.41 Â± 0.54 83.85 Â± 1.15 81.59 Â± 1.66 6.19 Â± 1.64 1.52 Â± 0.85 3.00 Â± 1.55 0.49 Â± 0.43</cell></row><row><cell></cell><cell>Î” ğ¸ğ‘‚ (â†“)</cell><cell>1.77 Â± 0.68</cell><cell cols="2">4.75 Â± 1.62 1.44 Â± 0.84</cell><cell>1.48 Â± 1.34</cell><cell>0.66 Â± 0.49</cell></row><row><cell></cell><cell>AUC</cell><cell>78.69 Â± 0.44</cell><cell cols="3">77.05 Â± 0.53 77.86 Â± 0.93 78.67 Â± 0.57</cell><cell>78.22 Â± 0.68</cell></row><row><cell>Pokec-z</cell><cell>F1 Î” ğ·ğ‘ƒ (â†“)</cell><cell>70.54 Â± 1.26 4.99 Â± 1.41</cell><cell cols="4">65.19 Â± 3.18 68.83 Â± 3.88 72.78 Â± 0.73 70.48 Â± 2.32 3.65 Â± 0.94 5.48 Â± 1.15 3.08 Â± 1.73 3.76 Â± 1.29</cell></row><row><cell></cell><cell>Î” ğ¸ğ‘‚ (â†“)</cell><cell>5.17 Â± 1.68</cell><cell>3.87 Â± 1.21</cell><cell>5.61 Â± 1.48</cell><cell>3.85 Â± 1.90</cell><cell>3.46 Â± 0.99</cell></row><row><cell></cell><cell>AUC</cell><cell>75.99 Â± 0.39</cell><cell cols="4">72.31 Â± 1.67 75.12 Â± 1.03 75.22 Â± 0.63 76.08 Â± 0.31</cell></row><row><cell>Pokec-n</cell><cell>F1 Î” ğ·ğ‘ƒ (â†“)</cell><cell>63.03 Â± 1.49 1.02 Â± 0.67</cell><cell cols="4">61.73 Â± 1.4 64.84 Â± 1.86 65.91 Â± 1.43 66.22 Â± 1.61 6.66 Â± 1.40 1.93 Â± 1.14 4.94 Â± 1.91 0.99 Â± 0.90</cell></row><row><cell></cell><cell>Î” ğ¸ğ‘‚ (â†“)</cell><cell>2.65 Â± 1.20</cell><cell>9.16 Â± 1.86</cell><cell>2.6 Â± 2.19</cell><cell>6.26 Â± 2.71</cell><cell>1.49 Â± 1.11</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 7 :</head><label>7</label><figDesc>Comparison of FairINV and baseline methods in various sensitive attributes scenarios. Sens.Attr.: Sensitive Attribute.</figDesc><table><row><cell>Datesets</cell><cell></cell><cell>Vanilla GCN</cell><cell>NIFTY</cell><cell>FairGNN</cell><cell>FairINV</cell><cell>Vanilla GCN</cell><cell>NIFTY</cell><cell>FairGNN</cell><cell>FairINV</cell><cell>Vanilla GCN</cell><cell>NIFTY</cell><cell>FairGNN</cell><cell>FairINV</cell></row><row><cell></cell><cell>Sens.Attr.</cell><cell></cell><cell>Age</cell><cell></cell><cell></cell><cell></cell><cell cols="2">Gender</cell><cell></cell><cell></cell><cell cols="2">Single</cell></row><row><cell></cell><cell>AUC</cell><cell>65.90 Â± 0.83</cell><cell cols="3">49.26 Â± 6.68 75.13 Â± 0.84 69.11 Â± 1.80</cell><cell>65.90 Â± 0.83</cell><cell cols="3">55.87 Â± 8.19 75.69 Â± 0.60 69.11 Â± 1.80</cell><cell>65.90 Â± 0.83</cell><cell>56.08 Â± 8.46</cell><cell cols="2">75.69 Â± 0.60 69.11 Â± 1.80</cell></row><row><cell>German</cell><cell>F1 Î” ğ·ğ‘ƒ (â†“)</cell><cell>77.32 Â± 1.20 20.18 Â± 5.17</cell><cell>82.0 Â± 0.71 0.13 Â± 0.25</cell><cell cols="2">76.83 Â± 3.09 82.36 Â± 0.35 20.67 Â± 3.40 0.48 Â± 0.38</cell><cell>77.32 Â± 1.20 36.29 Â± 4.64</cell><cell cols="3">81.88 Â± 0.39 77.91 Â± 5.16 82.36 Â± 0.35 1.75 Â± 1.82 36.07 Â± 5.75 0.76 Â± 1.24</cell><cell>77.32 Â± 1.20 34.13 Â± 3.29</cell><cell>82.03 Â± 0.37 1.19 Â± 1.71</cell><cell cols="2">78.17 Â± 5.15 82.36 Â± 0.35 33.37 Â± 7.83 2.64 Â± 4.15</cell></row><row><cell></cell><cell>Î” ğ¸ğ‘‚ (â†“)</cell><cell>15.83 Â± 3.47</cell><cell>0.34 Â± 0.67</cell><cell>18.24 Â± 3.95</cell><cell>0.46 Â± 0.52</cell><cell>31.35 Â± 4.39</cell><cell>1.53 Â± 0.93</cell><cell>27.84 Â± 5.71</cell><cell>0.15 Â± 0.29</cell><cell>27.26 Â± 4.96</cell><cell>0.61 Â± 0.69</cell><cell>25.91 Â± 7.93</cell><cell>1.48 Â± 2.96</cell></row><row><cell></cell><cell>Sens.Attr.</cell><cell></cell><cell cols="2">Hair color indicator</cell><cell></cell><cell></cell><cell cols="2">Region</cell><cell></cell><cell></cell><cell>AGE</cell><cell></cell></row><row><cell></cell><cell>AUC</cell><cell>76.42 Â± 0.13</cell><cell cols="3">74.04 Â± 0.46 70.39 Â± 1.07 75.79 Â± 0.08</cell><cell>76.42 Â± 0.13</cell><cell cols="3">74.04 Â± 0.46 70.25 Â± 0.89 75.79 Â± 0.08</cell><cell>76.42 Â± 0.13</cell><cell>74.04 Â± 0.46</cell><cell cols="2">70.31 Â± 1.07 75.79 Â± 0.08</cell></row><row><cell>Pokec-z</cell><cell>F1 Î” ğ·ğ‘ƒ (â†“)</cell><cell>70.32 Â± 0.20 18.7 Â± 0.90</cell><cell cols="3">69.90 Â± 0.34 43.73 Â± 24.28 70.78 Â± 0.50 17.29 Â± 7.62 10.80 Â± 7.58 15.63 Â± 1.09</cell><cell>70.32 Â± 0.20 3.91 Â± 0.35</cell><cell cols="3">69.9 Â± 0.34 41.98 Â± 23.68 70.78 Â± 0.5 8.15 Â± 3.28 2.80 Â± 1.11 2.70 Â± 0.96</cell><cell>70.32 Â± 0.20 33.09 Â± 0.57</cell><cell cols="3">69.90 Â± 0.34 41.34 Â± 28.04 70.78 Â± 0.50 35.71 Â± 13.32 22.08 Â± 18.8 27.49 Â± 3.29</cell></row><row><cell></cell><cell>Î” ğ¸ğ‘‚ (â†“)</cell><cell>18.73 Â± 0.80</cell><cell cols="3">14.81 Â± 6.47 10.19 Â± 7.21 14.31 Â± 1.76</cell><cell>4.59 Â± 0.34</cell><cell>7.57 Â± 2.93</cell><cell>3.45 Â± 1.38</cell><cell>2.23 Â± 0.66</cell><cell>36.32 Â± 0.70</cell><cell cols="3">33.68 Â± 13.4 23.42 Â± 19.31 29.19 Â± 3.68</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="7">ACKNOWLEDGEMENTS</head><p>The research is supported by the <rs type="funder">National Key R&amp;D Program of China</rs> under grant No. <rs type="grantNumber">2022YFF0902500</rs>, the <rs type="funder">Guangdong Basic and Applied Basic Research Foundation, China</rs> (No. <rs type="grantNumber">2023A1515011050</rs>), <rs type="funder">Shenzhen Science and Technology Program</rs> (<rs type="grantNumber">KJZD20231023094501003</rs>), and <rs type="funder">Tencent AI Lab</rs> (<rs type="grantNumber">RBFR2024004</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_rjE8hC8">
					<idno type="grant-number">2022YFF0902500</idno>
				</org>
				<org type="funding" xml:id="_CExMFAD">
					<idno type="grant-number">2023A1515011050</idno>
				</org>
				<org type="funding" xml:id="_QmnVBGX">
					<idno type="grant-number">KJZD20231023094501003</idno>
				</org>
				<org type="funding" xml:id="_WjWuCTs">
					<idno type="grant-number">RBFR2024004</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Fairness and robustness in invariant learning: A case study in toxicity classification</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>Adragna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elliot</forename><surname>Creager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Madras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.06485</idno>
		<imprint>
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Towards a unified framework for fair and stable graph representation learning</title>
		<author>
			<persName><forename type="first">Chirag</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Himabindu</forename><surname>Lakkaraju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="j">Uncertainty in Artificial Intelligence</title>
		<imprint>
			<biblScope unit="page" from="2114" to="2124" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Invariant risk minimization games</title>
		<author>
			<persName><forename type="first">Kartik</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karthikeyan</forename><surname>Shanmugam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kush</forename><surname>Varshney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amit</forename><surname>Dhurandhar</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="145" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">LÃ©on</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.02893</idno>
		<title level="m">Invariant risk minimization</title>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Asuncion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Newman</surname></persName>
		</author>
		<title level="m">UCI machine learning repository</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Compositional fairness constraints for graph embeddings</title>
		<author>
			<persName><forename type="first">Avishek</forename><surname>Bose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><surname>Hamilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="715" to="724" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fairness without demographics through knowledge distillation</title>
		<author>
			<persName><forename type="first">Junyi</forename><surname>Chai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taeuk</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoqian</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="19152" to="19164" />
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">When Does Group Invariant Learning Survive Spurious Correlations?</title>
		<author>
			<persName><forename type="first">Yimeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruibin</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhi-Ming</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanyan</forename><surname>Lan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="7038" to="7051" />
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning causally invariant representations for out-of-distribution generalization on graphs</title>
		<author>
			<persName><forename type="first">Yongqiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yonggang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yatao</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Kaili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Binghui</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tongliang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="22131" to="22148" />
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Environment inference for invariant learning</title>
		<author>
			<persName><forename type="first">Elliot</forename><surname>Creager</surname></persName>
		</author>
		<author>
			<persName><forename type="first">JÃ¶rn-Henrik</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2189" to="2200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Fairmod: Fair link prediction and recommendation via graph modification</title>
		<author>
			<persName><forename type="first">Sean</forename><surname>Current</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuntian</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Saket</forename><surname>Gurukar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srinivasan</forename><surname>Parthasarathy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2201.11596</idno>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Say no to the discrimination: Learning fair graph neural networks with limited sensitive attribute information</title>
		<author>
			<persName><forename type="first">Enyan</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suhang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM International Conference on Web Search and Data Mining</title>
		<meeting>the 14th ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="680" to="688" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Individual fairness for graph neural networks: A ranking based approach</title>
		<author>
			<persName><forename type="first">Yushun</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanghang</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jundong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 27th ACM SIGKDD Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="300" to="310" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Edits: Modeling and mitigating data bias for graph neural networks</title>
		<author>
			<persName><forename type="first">Yushun</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ninghao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Jalaian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jundong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM Web Conference 2022</title>
		<meeting>the ACM Web Conference 2022</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1259" to="1269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Enhancing graph neural network-based fraud detectors against camouflaged fraudsters</title>
		<author>
			<persName><forename type="first">Yingtong</forename><surname>Dou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yutong</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM international conference on information &amp; knowledge management</title>
		<meeting>the 29th ACM international conference on information &amp; knowledge management</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="315" to="324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fairness through awareness</title>
		<author>
			<persName><forename type="first">Cynthia</forename><surname>Dwork</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toniann</forename><surname>Pitassi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Reingold</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd innovations in theoretical computer science conference</title>
		<meeting>the 3rd innovations in theoretical computer science conference</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="214" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Fair graph auto-encoder for unbiased graph representations with wasserstein distance</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kunpeng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanjie</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 IEEE International Conference on Data Mining (ICDM)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1054" to="1059" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning Fair Representations via Distance Correlation Minimization</title>
		<author>
			<persName><forename type="first">Dandan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaojie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baoxiang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongyuan</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Inductive representation learning on large graphs. Advances in neural information processing systems</title>
		<author>
			<persName><forename type="first">Will</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhitao</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Equality of opportunity in supervised learning</title>
		<author>
			<persName><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nati</forename><surname>Srebro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Chasing Fairness in Graphs: A GNN Architecture Perspective</title>
		<author>
			<persName><forename type="first">Zhimeng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaotian</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zirui</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Na</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ali</forename><surname>Mostafavi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2312.12369</idno>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The effect of race/ethnicity on sentencing: Examining sentence type, jail length, and prison length</title>
		<author>
			<persName><forename type="first">L</forename><surname>Kareem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tina</forename><forename type="middle">L</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><surname>Freiburger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Ethnicity in Criminal Justice</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="179" to="196" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Inform: Individual fairness on graph mining</title>
		<author>
			<persName><forename type="first">Jian</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingrui</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><surname>Maciejewski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanghang</forename><surname>Tong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD international conference on knowledge discovery &amp; data mining</title>
		<meeting>the 26th ACM SIGKDD international conference on knowledge discovery &amp; data mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="379" to="389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Crosswalk: Fairnessenhanced node representation learning</title>
		<author>
			<persName><forename type="first">Ahmad</forename><surname>Khajehnejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Moein</forename><surname>Khajehnejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mahmoudreza</forename><surname>Babaei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krishna</forename><forename type="middle">P</forename><surname>Gummadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adrian</forename><surname>Weller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baharan</forename><surname>Mirzasoleiman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="11963" to="11970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fairness without demographics through adversarially reweighted learning</title>
		<author>
			<persName><forename type="first">Preethi</forename><surname>Lahoti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Beutel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jilin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kang</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Flavien</forename><surname>Prost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nithum</forename><surname>Thain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuezhi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ed</forename><surname>Chi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="728" to="740" />
			<date type="published" when="2020">2020. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning to discover social circles in ego networks</title>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<date type="published" when="2012">2012. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning invariant graph representations for out-of-distribution generalization</title>
		<author>
			<persName><forename type="first">Haoyang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziwei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="11828" to="11841" />
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">What&apos;s Behind the Mask: Understanding Masked Graph Modeling for Graph Autoencoders</title>
		<author>
			<persName><forename type="first">Jintang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruofan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wangbin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changhua</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zibin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weiqiang</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">KDD</title>
		<imprint>
			<biblScope unit="page" from="1268" to="1279" />
			<date type="published" when="2023">2023</date>
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">A Graph is Worth 1-bit Spikes: When Graph Contrastive Learning Meets Spiking Neural Networks</title>
		<author>
			<persName><forename type="first">Jintang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huizhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruofan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zulun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baokun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changhua</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zibin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">On dyadic fairness: Exploring and mitigating bias in graph connections</title>
		<author>
			<persName><forename type="first">Peizhao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengyu</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongfu</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Fairlp: Towards fair link prediction on social network graphs</title>
		<author>
			<persName><forename type="first">Yanying</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiuling</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hui</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International AAAI Conference on Web and Social Media</title>
		<meeting>the International AAAI Conference on Web and Social Media</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="628" to="639" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning fair graph representations via automated data augmentations</title>
		<author>
			<persName><forename type="first">Hongyi</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhimeng</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Youzhi</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuiwang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Na</forename><surname>Zou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">FLOOD: A flexible invariant learning framework for out-of-distribution generalization on graphs</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fuli</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yunshan</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuan</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="1548" to="1558" />
		</imprint>
	</monogr>
	<note>Tat-Seng Chua, and Qing He</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Invariant Feature Regularization for Fair Face Recognition</title>
		<author>
			<persName><forename type="first">Jiali</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongqi</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kagaya</forename><surname>Tomoyuki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Suzuki</forename><surname>Tomoki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karlekar</forename><surname>Jayashree</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sugiri</forename><surname>Pranata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hanwang</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="20861" to="20870" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Causal inference by using invariant prediction: identification and confidence intervals</title>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>BÃ¼hlmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolai</forename><surname>Meinshausen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society Series B: Statistical Methodology</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="page" from="947" to="1012" />
			<date type="published" when="2016">2016. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Elements of causal inference: foundations and learning algorithms</title>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>SchÃ¶lkopf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Invariant models for causal transfer learning</title>
		<author>
			<persName><forename type="first">Mateo</forename><surname>Rojas-Carulla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>SchÃ¶lkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Peters</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1309" to="1342" />
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<author>
			<persName><forename type="first">Oleksandr</forename><surname>Shchur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maximilian</forename><surname>Mumme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aleksandar</forename><surname>Bojchevski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>GÃ¼nnemann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1811.05868</idno>
		<title level="m">Pitfalls of graph neural network evaluation</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Fairdrop: Biased edge dropout for enhancing fairness in graph representation learning</title>
		<author>
			<persName><forename type="first">Indro</forename><surname>Spinelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simone</forename><surname>Scardapane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Hussain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aurelio</forename><surname>Uncini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="344" to="354" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Data analysis in public social networks</title>
		<author>
			<persName><forename type="first">Lubos</forename><surname>Takac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michal</forename><surname>Zabovsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International scientific conference and international workshop present day trends of innovations</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Provably invariant learning without domain information</title>
		<author>
			<persName><forename type="first">Xiaoyu</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Yong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengyu</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Qu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xihe</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Yinghui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Qi</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="33563" to="33580" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Improving fairness in graph neural networks via mitigating sensitive attribute leakage</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuying</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yushun</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huiyuan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jundong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tyler</forename><surname>Derr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<date type="published" when="1938">2022. 1938-1948</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Handling distribution shifts on graphs: An invariance perspective</title>
		<author>
			<persName><forename type="first">Qitian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hengrui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junchi</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Wipf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2202.02466</idno>
		<imprint>
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Graph neural networks in recommender systems: a survey</title>
		<author>
			<persName><forename type="first">Shiwen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wentao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xu</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Cui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput. Surveys</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="1" to="37" />
			<date type="published" when="2022">2022. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">Keyulu</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weihua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefanie</forename><surname>Jegelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1810.00826</idno>
		<title level="m">How powerful are graph neural networks? arXiv preprint</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Obtaining Dyadic Fairness by Optimal Transport</title>
		<author>
			<persName><forename type="first">Moyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenyan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoling</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangfeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2022 IEEE International Conference on Big Data (Big Data)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="4726" to="4732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">The Devil is in the Data: Learning Fair Graph Neural Networks via Partial Knowledge Distillation</title>
		<author>
			<persName><forename type="first">Yuchang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jintang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zibin</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th ACM International Conference on Web Search and Data Mining</title>
		<meeting>the 17th ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="1012" to="1021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">FairAGG: Toward Fair Graph Neural Networks via Fair Aggregation</title>
		<author>
			<persName><forename type="first">Yuchang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jintang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zibin</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computational Social Systems</title>
		<imprint>
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Fair Graph Representation Learning via Sensitive Attribute Disentanglement</title>
		<author>
			<persName><forename type="first">Yuchang</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jintang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zibin</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liang</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM on Web Conference 2024</title>
		<meeting>the ACM on Web Conference 2024</meeting>
		<imprint>
			<date type="published" when="2024">2024</date>
			<biblScope unit="page" from="1182" to="1192" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
