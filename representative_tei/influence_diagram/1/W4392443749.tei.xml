<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="fr">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main"></title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<affiliation key="aff0">
								<address>
									<settlement>Alexis Vincent Julien Robinson Arnaud Tom Adrien Paul Théo Cassandre Camille Kathleen Kim, et tous ceux qui se reconnaîtront</settlement>
									<region>Léa Jessica</region>
								</address>
							</affiliation>
						</author>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Il est difficile d'écrire des remerciements pour tant de personnes qui ont compté dans mon épanouissement personnel et professionnel durant ces trois belles années de thèse. J'ai appris de tous ces échanges avec vous, qu'ils soient dans le cadre de la thèse ou en dehors, et j'espère n'oublier personne dans cette tentative.</p><p>En premier lieu, je tiens à remercier mes directeurs de thèse, Frédéric Meunier et Axel Parmentier. Merci à toi Frédéric pour m'avoir donné l'envie de faire une thèse et pour ton suivi rigoureux, notamment pendant la rédaction du manuscrit en plein confinement. Nos discussions étaient toujours enrichissantes pour moi grâce à tes connaissances et la passion que tu transmets. Aussi c'était un réel plaisir d'avoir avec toi de réjouissantes digressions sur l'histoire des mathématiques ou sur le jazz, plus particulièrement la discographie de Miles Davis. Merci à toi Axel pour m'avoir si bien accompagné quotidiennement durant ma thèse. Je souhaite à tous les doctorants d'avoir un directeur de thèse comme toi. Travailler avec toi est un vrai bonheur, notamment grâce à ton enthousiaste, ta disponibilité et ta curiosité. Les après-midi devant un tableau à chercher des preuves me manqueront, tout comme nos déjeuners à discuter de voyages et randonnées. J'ajoute à cela un excellent souvenir de notre voyage à Tokyo, avec notamment un fou rire mémorable dans un restaurant.</p><p>I thank Andrea Lodi and Patrick Jaillet for being the referees of my thesis and for their helpful remarks. Merci à toi Patrick pour m'avoir accueilli au MIT pendant quelques semaines qui ont été très enrichissantes pour moi. Je souhaite également remercier chaleureusement les examinateurs qui ont accepté de faire partie de mon jury : Georgina Hall, Vianney Perchet et Guillaume Obozinski.</p><p>Je remercie particulièrement Guillaume pour ses précieux conseils sur l'après-thèse. Durant ma thèse, j'ai eu l'opportunité de collaborer sur un projet avec Axel, Guillaume, ainsi que Vincent Leclère et Joseph Salmon. Je vous remercie tous pour m'avoir accueilli sur ce beau projet, j'ai beaucoup appris à vos côtés.</p><p>Je tiens à remercier les membres du département de Recherche Opérationnelle d'Air France, avec qui j'ai eu le plaisir de travailler ou d'échanger tout au long de ma thèse. Je remercie particulièrement Paul Louis Vincenti qui a appuyé le projet après mon stage et Solène Richard qui a suivi ma thèse et mis en avant les résultats auprès du reste du département de RO et des responsables de la maintenance. Merci à Laurent Demeestere, Jules Humbert, Robin Dupont et Patrick Marshall pour toute leur aide, notamment dans l'utilisation de Spark et Hadoop sur le cluster. Je remercie aussi Jules Dubois et Léo Pallud pour nos discussions sur l'ordonnancement des tâches de maintenance, ainsi que tous les membres des l'équipe de maintenance prédictive i avec qui j'ai eu l'occasion d'interagir. Je souhaite remercier les responsables du département Julie Pozzi, Benoit Robillard et Marine Le Touzé. Merci à tous les membres du département avec qui j'ai eu le plaisir de prendre un café, prendre une bière ou jouer au foot, et en particulier Pierre, Blaise, Alexandre, Guillaume, Benoît, Valentina, Magdalena, Ferran, Kevin, et tout ceux que je m'excuse d'oublier.</p><p>Merci à tous les chercheurs du Cermics avec qui j'ai eu le plaisir de discuter. Un immense merci à Isabelle, qui sait toujours être juste et bienveillante avec tout le monde. J'adresse tous mes remerciements aux doctorants du Cermics. On a vraiment passé de bons moments ensemble, vous avez rendu les journées agréables grâce à des discussions toujours intéressantes et drôles. Un merci particulier à vous avec qui j'ai passé des moments privilégiés qui resteront de super souvenirs : Marion, Alexandre, Benoit, Henri, Adèle, Sébastien, Oumaïma, William, Thomas, Adrien, Maël et tout ceux que j'oublie ! Je souhaite remercier tous mes amis avec qui je passe des moments exceptionnels et qui sont toujours là pour moi. Merci à vous pour ces soirées à refaire le monde autour d'une bière, à rigoler sur des anecdotes qu'on aime se remémorer, à s'écharper dans des débats politiques et footballistiques, ça me fait toujours autant plaisir de vous voir.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="fr">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>This thesis develops algorithms for stochastic optimization problems such as Markov Decision Processes (MDPs) or Partially Observable Markov Decision Processes (POMDPs), and uses them to give a solution for the airplane maintenance problem at Air France. The research was conducted throughout a scientific chair between Air France and École des Ponts ParisTech.</p><p>We introduce a generic predictive maintenance problem for systems with several components evolving over time when a decision maker chooses dynamically which components to maintain at each maintenance slot. His actions are based on partial observations of each component and are linked by capacity constraints. The objective is to find a "memoryless" policy, which is a mapping from observations to actions, minimizing the expected failure costs and maintenance costs over a finite horizon. We formalize such a problem as a weakly coupled POMDP, which models each component as a POMDP. Finding an optimal memoryless policy for the weakly coupled POMDP is difficult for two reasons. First, even when the system has a single component, the problem is already NP-hard. Second, due to the curse of dimensionality, when the number of components grows the POMDP becomes quickly intractable. Our main contributions are mixed-integer linear formulations for POMDPs that give an optimal memoryless policy, as well as valid inequalities that are based on a probabilistic interpretation of the dependences between the random variables. In addition, we introduce an mixed-integer linear formulation that breaks the curse of dimensionality and that induces a "good" policy for weakly coupled POMDP.</p><p>In fact, the MDPs and POMDPs lie in the broad class of stochastic optimization problems where the uncertainty is assumed to satisfy a given structure called influence diagram. More precisely, given random variables considered as vertices of an acyclic digraph, a probabilistic graphical model defines a joint probability distribution via the conditional probability distribution of vertices given their parents. In influence diagrams, the random variables are represented by a probabilistic graphical model whose vertices are partitioned into three types: chance, decision and utility vertices. The decision maker chooses the probability distribution of the decision vertices conditionally to their parents in order to maximize his expected utility. Our main contributions are mixed-integer linear formulations for solving the maximum expected utility problem in influence diagrams, as well as valid inequalities, which lead to a computationally efficient algorithm. It generalizes our results on POMDPs to any influence diagrams. We also show that the linear relaxation yields an optimal integer solution for instances that can be solved by the "single policy update," the default algorithm for addressing the maximum expected utility problem in influence diagrams.</p><p>iii The airplane maintenance problem at Air France is a predictive maintenance problem with capacity constraints. Applying the weakly coupled POMDP policy requires to estimate the weakly coupled POMDP parameters. Based on a dataset of historical sensor data, we propose a statistical methodology to cast the airplane maintenance problem as a weakly coupled POMDP. Our approach has the advantage of being interpretable by the maintenance engineers. The numerical experiments show that our maintenance policy give "good" numerical results compared to those obtained by using Air France's maintenance policy.</p><p>Key words: predictive maintenance, partially observable Markov decision processes, probabilistic graphical models, influence diagrams, stochastic optimization, mixed-integer linear programming</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Résumé</head><p>Cette thèse développe des algorithmes pour des problèmes d'optimisation stochastiques tels que les processus de décision markoviens (MDPs) ou les processus de décision markoviens partiellement observables (POMDPs) <ref type="foot" target="#foot_0">1</ref> , et les utilise pour donner une solution au problème de maintenance des avions chez Air France. Cette recherche a été menée dans le cadre d'une chaire scientifique entre Air France et l'École des Ponts ParisTech.</p><p>Nous introduisons un problème générique de maintenance prédictive d'un système à plusieurs composants évoluant dans le temps où un décideur choisit dynamiquement les composants à réparer à chaque plage de maintenance. Basées sur des observations partielles de chaque composant, ses actions sont couplées par des contraintes de capacité. L'objectif est de trouver une politique "sans mémoire", c'est-à-dire une application qui associe à l'observation courante (et non l'historique des observations et des actions) une action, qui minimise les coûts de panne et les coûts de maintenance espérés sur un horizon fini. Nous formalisons ce problème sous la forme de POMDPs faiblement couplés où chaque composant est modélisé comme un POMDP.</p><p>Trouver une politique optimale sans mémoire pour les POMDPs faiblement couplés est difficile pour deux raisons. Premièrement, même lorsque le système ne comporte qu'un seul composant, le problème est déjà NP-difficile. Deuxièmement, en raison de la malédiction de la dimension, lorsque le nombre de composants augmente, le POMDP devient rapidement insoluble. Nos principales contributions sont des formulations linéaires en nombres entiers pour les POMDPs qui donnent des politiques optimales sans mémoire, ainsi que des inégalités valides qui sont basées sur une interprétation probabiliste des dépendances entre les variables aléatoires du problème. De plus, nous introduisons une formulation linéaire en nombres entiers qui casse la malédiction de la dimension et qui induit une "bonne" politique pour les POMDPs faiblement couplés.</p><p>En réalité les MDPs et les POMDPs font partie d'une large classe de problèmes d'optimisation stochastique où l'incertitude satisfait une certaine structure qu'on appelle diagramme d'influence. Plus précisément, en considérant les variables aléatoires comme les sommets d'un digraphe acyclique, un modèle graphique probabiliste définit une distribution de probabilité jointe comme le produit des distributions de probabilités conditionnelles des sommets sachant leurs parents. Dans les diagrammes d'influence, les variables aléatoires sont représentées par un modèle graphique probabiliste dont les sommets sont divisés en trois types : les sommets chances, décisions et utilités. Le décideur choisit la distribution de probabilité des sommets décisions conditionnellement à leurs parents afin de maximiser son utilité espérée. Nos principales con-tributions sont des formulations linéaires en nombres entiers pour résoudre le problème de l'utilité maximum espérée dans un diagramme d'influence, ainsi que des inégalités valides, qui conduisent à une formulation efficace. Cela généralise nos résultats sur les POMDPs à tout diagrammes d'influence. Nous prouvons également que la relaxation linéaire de notre programme donne une solution optimale en nombres entiers pour les cas qui peuvent être résolus par "single policy update", l'algorithme par défaut pour résoudre le problème de l'utilité maximum espérée dans un diagramme d'influence.</p><p>Le problème de maintenance des avions chez Air France est un problème de maintenance prédictive avec des contraintes de capacité. L'application de notre politique pour les POMDPs faiblement couplés produite sur le problème de maintenance des avions à Air France nécessite d'estimer les paramètres des POMDPs faiblement couplés. À partir d'un historique de données brutes enregistrées par des capteurs, nous introduisons une méthodologie statistique qui permet de transformer le problème de maintenance des avions en des POMDPs faiblement couplés. Notre approche a l'avantage d'être interprétable par les ingénieurs de la maintenance. Les expérimentations numériques montrent que notre politique de maintenance donne de "bons" résultats numériques comparés à ceux obtenus en utilisant la politique de maintenance d'Air France.</p><p>Mots clés : maintenance prédictive, processus de décision markovien partiellement observable, modèle graphique probabiliste, diagramme d'influence, optimisation stochastique, programmation linéaire en nombres entiers 1.3 Modeling of the predictive maintenance problem with capacity constraints for a system with M components. The blue and red arrows respectively represent the stochastic deterioration of the components and the maintenance policy. At each maintenance slot, each component is in a degradation state and the decision maker has access to a partial observation on each component. Then, the decision maker applies a maintenance policy (red arrows) that leads to a maintenance decision that impacts the degradation state of each component. . . . . . . 5</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>List of Figures</head><p>1. <ref type="bibr" target="#b3">4</ref> The two influence diagrams modeling the medical example and the POMDP example. The circle, square and diamond vertices respectively represent the chance, decision and utility vertices. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9</p><p>2.1 Le problème de maintenance d'un avion : on cherche une politique de maintenance (flèches rouges) qui prend en entrée les données brutes de M équipements et retourne la décision de maintenance en sortie. . . . . . . . . . . . . . . . . . . .  <ref type="bibr">(5.14)</ref> in Algorithm 3 at time t = 3 and t = 4. The decision maker observes h 3 and takes action Act 3 8 (h 3 ). Then, the decision maker observes h 4 and takes action Act 4  9 (h 4 ). The black points indicate the time steps and the red point corresponds to the time when the decision is taken. The black hatched lines represent the past at the current time (red). The red square indicates the horizon taken into account in the optimization problem. 76 1 An example of a decision tree that takes as inputs the average (avg) pressure and the standard deviation (std) of the temperature. It returns a discrete label in {1, 2, 3}. Each label corresponds to a cluster. Clusters 1 and 2 correspond to normal behavior (blue), and cluster 3 corresponds to high-failure risk (red). . . . . . 10. <ref type="bibr" target="#b1">2</ref> The four elements of our approach: the feature extraction (in purple), the prediction model (in blue), the decision tree (in green) and the policy (in red). . . . . . . 10.3 A graphical representation of our approach using the notation introduced in Section 10.2. First, the feature extraction φ from sensor data is represented by purple dashed arcs. Second, the probabilistic dependences of our Gaussian HMMs are represented by blue plain arcs for each equipment. Third, the mapping of our decision tree f is represented by dotted green arcs. Fourth, the policy of the de- A first line focuses on the decision-making process of a generic predictive maintenance problem, which is formalized as a dynamic sequential optimization problem under uncertainty. The decision maker dynamically chooses a restricted number of components to maintain at each maintenance slot. The uncertainty comes from the failures, modeled as random events, which happen during the operating days of the system. Each component is assumed to behave independently but the maintenance decisions on the different components are weakly linked by capacity constraints. This problem lies in the broad class of problems that involve many independent subprocesses that are only weakly linked at each time step. These problems are called weakly coupled dynamic programs <ref type="bibr" target="#b1">[2]</ref>. A new additional feature from the predictive maintenance problem is the fact that the decisions are based on noisy observations, which makes the system partially observable. We propose several methods with guarantees to compute "good" solutions of these optimization problems.</p><p>The second line of research that structures this dissertation focuses on the study of stochastic optimization problems where the uncertainty is known to satisfy some structure called influence diagram. In particular, this class of problems includes the weakly coupled dynamic program under partial observations induced by the airplane maintenance problem. We propose an exact algorithm, which is based on tools from Operations Research and Machine Learning and which exploits this uncertainty structure.</p><p>Finally, the third line of research addresses some statistical challenges that come from the practical problem at Air France. Indeed, casting the airplane maintenance problem as a weakly coupled dynamic program with partial observations requires to learn a hidden Markov model that captures the evolution of the equipment's deterioration over time. In addition, Air France's requirement is that this statistical model has to be interpretable. Based on the dataset of signal values and failure dates on the whole fleet of airplanes, we introduce a methodology that learns such a statistical model that can be interpreted by the maintenance engineers.</p><p>Section 1.1 introduces informally the predictive maintenance problem with capacity constraints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">The predictive maintenance problem with capacity constraints</head><p>Section 1.2 summarizes our main mathematical contributions to the optimization problem of weakly coupled dynamic programs with partial observations. Section 1.3 details our theoretical contributions to the optimization problem in influence diagrams. Section 1.4 describes our contributions that address the statistical challenges raised by the airplane maintenance problem.</p><p>While this dissertation starts in Part I by investigating solution algorithms for these weakly coupled dynamic programs with partial observations, Part III details how we exploit those in the airplane maintenance problem at Air France. Part II contains our work on the optimization problem in influence diagrams. Each chapter of this dissertation contains a section with bibliographical remarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">The predictive maintenance problem with capacity constraints</head><p>In the industry, planning the maintenance of a system consists in choosing when to intervene during the system's operating period and which actions should be carried out during this intervention. In our case, dates of the maintenance are already scheduled by a flight scheduling tool. Thus, the predictive maintenance problem we consider focuses on the second issue of the planning problem, i.e., deciding which actions to take at each scheduled maintenance slot.</p><p>The system we consider has multiple components, each of them evolving over time. In general, the systems considered are mechanical, which ensures that each component deteriorates over time and fails after several operating hours/days. The failures happen during the operating days of the components of the system between two maintenance slots. At each maintenance slot, the decision maker chooses which components to maintain and his choices are based on the observed degradation state of each component. This degradation state characterizes the different performance rates of the component and is described through a discrete indicator, whose values range from the state "perfect functioning" to the state "failure." The deterioration process of a component is stochastic in the sense that the component transits from a degradation state s to a more critical degradation state s according to a probability distribution.</p><p>An additional widely used assumption in the industry is to consider that this deterioration is Markovian, i.e., the state at a time only depends on the state at the previous time. Figure <ref type="figure" target="#fig_68">1</ref>.2 illustrates the modeling of the deterioration process of a component.</p><p>However, in many practical problems the decision maker has only access to a partial observation of the degradation state of a component, which is also a discrete noisy indicator. Indeed, this indicator usually corresponds to error messages resulting from the detection of an abnormal behavior of physical measurements of the components. It makes this partial observation very sensitive to measurement errors and noisy regarding to the degradation state. Figure <ref type="figure" target="#fig_68">1</ref>.2 illustrates these noisy observations by the squiggly blue arrows. Such a stochastic model is called a Hidden Markov Chain. It follows that at each maintenance slot the decision is taken based on the partial observations of the components instead of the degradation states as shown in Figure <ref type="figure" target="#fig_68">1</ref>.3.</p><p>Between two maintenance slots, the components are assumed to evolve independently, i.e., the deteriorating process of a component does not influence the deterioration process of the others. In an industrial context, the components are linked by the maintenance decisions through At each degradation state, the decision maker has access to a noisy observation that is randomly emitted (squiggly blue arrows).</p><p>capacity constraints. This is the case of our applied problem: To be feasible, the maintenance decisions of the airplane maintenance problem have to satisfy the following constraint.</p><p>The number of maintained components at each maintenance slot is not greater than K .</p><p>While considering the maintenance problem of each component independently is easier, these constraints force the decision maker to consider the evolution of the whole system. If a component is maintained, then its degradation state recovers to "perfect functioning." Otherwise, it keeps on evolving to a more degraded state. Maintaining a component leads to a maintenance cost that is significantly lower than the failure cost.</p><note type="other">Figure 1</note><p>.3 summarizes the predictive maintenance problem with capacity constraints. Choosing a maintenance policy consists in choosing at each future maintenance slots, for each possible partial observation, a maintenance decision satisfying the capacity constraints. Then, the goal of the decision maker is to choose a maintenance policy that minimizes the expected costs expressed as the sum of the maintenance costs and the failure cost. Chapter 3 is devoted to formalize mathematically the predictive maintenance problem with capacity constraints. It raises two scientific challenges: How do we cast the airplane maintenance problem as a predictive maintenance with capacity constraints (Section 1.4), and, how do we find a "good" maintenance policy (Section 1.2)?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Decision processes with partial observations</head><p>We start our discussion on choosing a "good" maintenance policy by introducing an adequate mathematical model for the predictive maintenance problem with capacity constraints. Since the deterioration process of the component is Markovian and the decision maker has only access to a partial observation, it is natural to formalize the problem within the Partially Observ- Figure <ref type="figure" target="#fig_68">1</ref>.3 -Modeling of the predictive maintenance problem with capacity constraints for a system with M components. The blue and red arrows respectively represent the stochastic deterioration of the components and the maintenance policy. At each maintenance slot, each component is in a degradation state and the decision maker has access to a partial observation on each component. Then, the decision maker applies a maintenance policy (red arrows) that leads to a maintenance decision that impacts the degradation state of each component.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Decision processes with partial observations</head><note type="other">Maintenance</note><p>able Markov Decision Process (POMDP) framework.</p><p>The POMDP framework. We start by considering a system with one component. In such problems, at each time step, the component is in a state s in some finite state space X S . The decision maker does not observe s, but has access to an observation o that belongs to some finite observation space X O , and is randomly emitted with probability p(o|s). Based on this observation, the decision maker chooses an action a from some finite action space X A . The component then transits randomly to a new state s in X S with probability p(s |s, a) and the decision maker obtains an immediate reward r (s, a, s ). The goal of the decision maker is to find a policy δ t a|o , which represents a conditional probability of taking action a in X A given the observation o at time t , maximizing the expected total reward over a finite horizon T .</p><p>The problem on which we focus is </p><formula xml:id="formula_0">max δ∈∆ E δ T t =1 r (S t , A t , S t +1 ) , (<label>1</label></formula><formula xml:id="formula_1">, s T +1 in X S × X O × X A T × X S .</formula><p>Problem (1.1) is known as the POMDP problem with memoryless policies, meaning that the action taken at a time is only based on the current observation instead of the history of observations and actions. POMDPs generalize the Markov Decision Processes (MDPs), where the decision maker has access to the state of the system. While MDPs can be solved in polynomial time, the POMDP problem with memoryless policies is NP-hard. Unless P=NP, the fact that the decision maker has only access to a partial observation makes the problem harder to solve.</p><p>One contribution in this dissertation is an Mixed-Integer Linear Program (MILP), which is a standard tool in Operations Research, that provides an optimal policy of the POMDP problem with memoryless policies. It formulates an optimization problem described by a linear objective function, linear constraints and integer variables with the following canonical form:</p><formula xml:id="formula_2">min x c T x subject to Ax b x 0 x ∈ Z p × R n-p ,</formula><p>where b and c are real vectors and A is a real matrix. While the MDP maximization problem can be solved using a linear program (see, e.g., <ref type="bibr" target="#b123">[124]</ref>), i.e., by relaxing the integrality constraints, we use integer variables to address the POMDP maximization problem <ref type="bibr">(1.1)</ref>. Such integer formulations can be solved efficiently using off-the-shell optimization solvers. Since these solvers use the Branch-and-Bound algorithm, it is natural to derive valid inequalities that improve the quality of the bounds obtained when solving the problem without integrality constraints. A second contribution is a collection of valid inequalities that help the resolution of the MILP formulation. Numerical experiments show its efficiency.</p><p>The POMDP framework for the predictive maintenance problem with capacity constraints.</p><p>It is natural to assume that each component behaves as a POMDP. For each component the degradation state, the partial observation and the maintenance decision (see Figure <ref type="figure" target="#fig_68">1</ref>.3) are respectively represented by the state, the observation and the action of a POMDP. For each component, all the conditional probabilities p(o|s) and p(s |s, a) are computed using the parameters of the hidden Markov model. Given a system with M components, the problem consists in M subproblems that are weakly linked by the capacity constraints that affect the actions taken on each component. For instance, the capacity constraint "the number of maintained components at each maintenance slot is not greater than K " is modeled using the constraint M m=1 a m K , where a m is the action on component m and equal to 1 if component m is maintained and 0 otherwise. Since the actions taken on each component are linked, a maintenance policy has to be considered on the whole system. At each time t , the maintenance policy is of</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3.">Decision making with structured uncertainty</head><p>the form δ t a|o , where a = (a 1 , . . . , a M ) and o = (o 1 , . . . , o M ) respectively represent the action and observation of the system. For each component m, a transition (s, a, s ) leads to an immediate reward defined as follows r m (s, a, s ) = -Cost m (maintenance)1 maintain (a) -Cost m (failure)1 failure (s ), where 1 x (y) is equal to 1 if x = y and 0 otherwise.</p><p>If the decision maker has access to the degradation state of each component, then each subproblem reduces to a Markov decision process and the problem on the whole system is a weakly coupled MDP <ref type="bibr" target="#b104">[105]</ref>. In our case, the fact that the decision maker has only access to a partial observations requires to extend this notion to weakly coupled POMDPs. In addition to the partially observable aspect, a second difficulty when solving such a problem comes from the curse of dimensionality. Indeed, the size of the system space grows exponentially with the number of components of the system, which makes the usual algorithms computationally expensive. To figure out this difficulty, one observes that encoding a maintenance policy δ requires a table of the size roughly equals to T M m=1 |X m O ||X m A |, which is in general intractable. To get around this difficulty, we introduce the notion of implicit policy. In opposition to explicit policies that fully encode the policy in the solution of a single optimization problem, the implicit policies are the ones for which given an observation o, each vector (δ t a|o ) a is computed through a tailored optimization problem. It has a practical interest because the decision maker does not need to compute the policies for all possible observations. Indeed, once an observation o is revealed at time t , it only requires to compute the vector (δ t a|o ) a . Another contribution described in Part I of this dissertation is an mixed-integer linear formulation that leverages the one we introduce for the POMDP problem with memoryless policies and that is used to compute a "good" implicit policy for the weakly coupled POMDP problem. Like the formulations of Adelman and Mersereau <ref type="bibr" target="#b1">[2]</ref> for weakly coupled MDPs, the main advantage of our formulation is to contain a polynomial number of variables and constraints, which breaks the curse of dimensionality. Again, we improve the formulation by adding a collection of inequalities in our MILP formulation. In addition, we provide theoretical guarantees about the optimal value of our MILP formulation by computing a lower bound and an upper bound.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.3">Decision making with structured uncertainty</head><p>MDPs and POMDPs (Section 1.2) are specific cases of discrete stochastic optimization problems where the probability distribution of the random variables is assumed to satisfy some structure specified through an influence diagram. Influence diagrams form a flexible tool that enables to model a large class of stochastic optimization problems where the "nature" is assumed to be structured. We introduce the optimization problem in influence diagrams through a toy example.</p><p>The example is illustrated in Figure <ref type="figure" target="#fig_68">1</ref>.4a. Consider a patient who consults a doctor. The patient suffers from a disease D, which the doctor wants to diagnose. This disease appears on a patient with probability P(D), and leads to observable clinical symptoms represented by S, which are randomly emitted with probability P(S|D). Based on the observation of the symptoms S, the doctor has to decide which treatment T to apply on the patient. The treatment T and the Chapter 1. Introduction disease D give a response R 0 of the patient with probability P(R 0 |T, D), which leads to a reward r 0 (R 0 ) indicating the presence of undesirable side effects. Then, based on the response R 0 , the doctor decides if the disease requires a stronger medical intervention I . It finally gives a result R 1 with probability P(R 1 |I , D), indicating if the intervention on the disease D was a success, and it leads to a reward r 1 (R 1 ). The disease D, the symptoms S, the responses R 0 and R 1 , are represented by random variables that are not controlled by the doctor. They are called chance variables because their values are chosen by the "nature," i.e., everything that is not controlled by the doctor. In opposition to chance variables, the treatment T and the medical intervention I are represented by random variables called decision variables, whose values get to be chosen by the doctor. The functions r 0 and r 1 are the utility functions that depend respectively on the responses R 0 and R 1 . The goal of the doctor is to choose a strategy δ = δ T (T |S), δ I (I |R 0 ) maximizing his total expected utility E δ r 0 (R 0 )+r 1 (R 1 ) where the expectation is taken according to the probability distribution P δ (D, S, T, R 0 , I , R 1 ) = P(D)P(S|D)δ T (T |S)P(R 0 |T, D)δ I (I |R 0 )P(R 1 |I , D).</p><p>This type of problem can be encoded graphically using a directed acyclic graph, whose set of vertices V contains three types of vertices: chance (V c ), decision (V a ) and utility vertices (V r ) corresponding respectively to chance variables, decision variables and utility functions. An influence diagram, introduced by Howard and Matheson <ref type="bibr" target="#b55">[56]</ref>, is a directed acyclic graph over these vertices such that there are no outgoing arcs from a utility vertex. Figure <ref type="figure" target="#fig_68">1</ref>.4a illustrates the influence diagram that models the medical decision problem we described. Each chance variable X v , represented by a chance vertex v, is associated with a conditional probability distribution P(X v |X pa(v) ) of X v given X pa(v) , where X pa(v) is the vector of random variables represented by the parents of v in the influence diagram, the tails of the incoming arcs. Each utility function r v , represented by a utility vertex v, is associated with a deterministic function r v that maps each instantiation X pa(v) to a real value. The choices of the decision maker are then modeled using a strategy δ = (δ v ) v∈V a that is a vector of conditional probability distributions δ v (X v |X pa(v) ) = P(X v |X pa(v) ). Given a strategy δ, the joint probability distribution over all the random variables writes down</p><formula xml:id="formula_3">P δ (X V c ∪V a = x V c ∪V a ) = v∈V c P(X v = x v |X pa(v) = x pa(v) ) v∈V a δ v (X v = x v |X pa(v) = x pa(v) ). (1.4)</formula><p>The problem solved by the decision maker is the Maximum Expected Utility problem</p><formula xml:id="formula_4">max δ∈∆ E δ v∈V r r v (X pa(v) ) ,<label>(1.5)</label></formula><p>where ∆ is the set of possible conditional probability distributions for each decision vertex.</p><p>The modeling power of an influence diagram can also be observed on the POMDPs described in Section 1.2. The chance variables are the states (S t ) 1 t T +1 and observations (O t ) 1 t T , the decision variables are the actions (A t ) 1 t T , and finally the utility variables are the rewards r (S t , A t , S t +1 ) 1 t T . The probability distribution P δ of (1.2) can be written as (1.4) using the influence diagram represented in Figure <ref type="figure" target="#fig_68">1</ref>.4b. The goal is to choose δ maximizing the expected total reward E δ T t =1 r (S t , A t , S t +1 ) where the expectation is taken according to the probability distribution P δ . This problem can be represented using the influence diagram in Figure <ref type="figure" target="#fig_68">1</ref>.4b.</p><p>It leads us to the question of finding a strategy that solves the maximum expected utility prob-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.4.">Statistical methodology for the airplane maintenance problem</head><formula xml:id="formula_5">D S T R 0 r 0 I R 1 r 1 (a)</formula><p>The medical decision making problem lem in an influence diagram. This question raises two scientific challenges. First, evaluating a given strategy is already difficult. The difficulty of evaluating a strategy is the difficulty of solving the inference problem in an influence diagram, whose difficulty is exponential in the treewidth. Given a feasible strategy δ, a subset of vertices C ⊆ V, the inference problem consists in computing the marginal probability P δ (X C = x C ). Second, optimizing over the set of strategies ∆ is also difficult because of the huge number of possible vectors of conditional probability distributions.</p><formula xml:id="formula_6">s 1 o 1 r 1 a 1 s 2 o 2</formula><p>Part II focuses on addressing this second challenge. It generalizes the results obtained for the POMDPs of Section 1.2 to any maximum expected utility problem in an influence diagram. We emphasize that the results we obtain are based on the combination of several tools from probabilistic graphical model theory and integer programming. A first contribution described in Part II is an mixed-integer linear program that gives an optimal strategy of the maximum expected utility problem in influence diagrams. While the usual solutions are based on dynamic programming like algorithms along the graph (see, e.g., <ref type="bibr" target="#b75">[76,</ref><ref type="bibr" target="#b80">81,</ref><ref type="bibr" target="#b88">89]</ref>), our approach is based on mathematical programming. A second contribution is a collection of valid inequalities that improves our integer linear formulation. A third contribution is a characterization of the "easy" case of influence diagrams, i.e., the influence diagrams such that the maximum expected utility problem becomes tractable. The results in Part II have been partially published in Parmentier et al. <ref type="bibr" target="#b116">[117]</ref> and some of their proofs are based on other technical results in the preprint Cohen and Parmentier <ref type="bibr" target="#b26">[27]</ref>. Several results have been added after the publication of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.4">Statistical methodology for the airplane maintenance problem</head><p>Finally we wish to use of the policy of weakly coupled POMDP mentioned in Section 1.2 in our applied problem. However, modeling the maintenance problem of a real-life system as a predictive maintenance problem as in Section 1.1 is not immediate. It is particularly the case of the airplane maintenance problem at Air France. Indeed, the available data at each maintenance slots are the sensors data, which correspond to the signal values recorded during flights. An example of sensor signal is the value of the pressure in an equipment recorded at 1Hz during a flight. Hence, our dataset contains several years of sensor data and several failure dates. Consequently, expressing the airplane maintenance problem as a predictive maintenance problem with capacity constraints requires to compute the hidden Markov models of the equipment deterioration processes from our dataset. We propose a preliminary statistical work to cast the airplane maintenance problem (Figure <ref type="figure" target="#fig_68">1</ref>.1) as a predictive maintenance problem with capacity constraints (Figure <ref type="figure" target="#fig_68">1</ref>.3).</p><p>It raises three practical issues. First, in the predictive maintenance problem with capacity constraints the decision maker has access to a discrete indicator as partial observation, while the observations in the airplane maintenance problem correspond to sensor data, which are continuous and high-dimensional. Hence, it requires to develop a methodology that enables to transform the signal values into discrete indicators.</p><p>The second issue concerns the learning phase. Based on the historical data, the aim is to estimate the parameters that fully define the adequate hidden Markov model for each equipment, which is required as inputs of the predictive maintenance problem with capacity constraints.</p><p>The third issue is about the interpretability of the approach. Indeed, even if they do not impact the safety of the flights, <ref type="foot" target="#foot_1">1</ref> the maintenance decisions we support are not benign. Maintenance operations such as unmounting the landing gear of a long-haul airplane are expensive and time consuming. And if a failure happens, Air France will have to cancel the next flights operated by the airplane, which is even more expensive. Thus, if we want our model to be used in practice, the predictions of the statistical model, the hidden Markov model, as well as the maintenance decisions resulting from a maintenance policy, must be trusted by the maintenance engineers. An additional difficulty in this point is the fact that the dataset we consider contains a small amount of failures because Air France tries to avoid them as much as possible and we have a small amount of sensor data of the behavior of an equipment right before it fails. Consequently, we cannot validate the statical model using experimental validations.</p><p>Our approach is described in Chapter 10 and addresses these issues by combining four steps that use several statistical tools. First, since the data we handle are high-dimensional and noisy, we transform the collection of time series recorded during a flight by a reasonable number of relevant features. For instance, if a sensor records the evolution of the pressure in an equipment during a flight, an example of feature is the average or the standard deviation of the pressure over the flight length. Second, we learn the parameters of a Gaussian Hidden Markov model that predicts the evolution of the vector of features. Such a model is a hidden Markov model where the observations are continuous and are emitted according to a Gaussian probability distribution. Third, we transform this Gaussian hidden Markov model into a hidden Markov model with discrete observations using a decision tree. It takes a vector of continuous observations in input and returns a discrete indicator. The mechanism inside the decision tree is a combination of binary splitting rules, such as "is the average pressure above 20 bars?," that leads to a discrete indicator, whose values correspond to different failure risk levels.</p><p>These binary rules can be easily understood and validated by maintenance engineers, which addresses the third issue. Since this indicator is discrete, it also addresses the first issue. Finally, based on the Gaussian hidden Markov model and the decision tree we compute the parameters of the hidden Markov model with the discrete indicators, which addresses the second issue. Using this approach enables to cast the airplane maintenance problem at Air France as a predictive maintenance problem with capacity constraints. The numerical experiments show that our maintenance policy give "good" numerical results compared to those obtained by using Air France's maintenance policy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Introduction (Français)</head><p>Plusieurs thématiques de Recherche Opérationnelle sont développées dans cette thèse et motivées par un problème de maintenance de notre partenaire industriel Air France.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contexte.</head><p>La quantité de données disponibles sur les systèmes industriels a énormément augmenté durant les dernières années. Le but de la maintenance prédictive est d'exploiter ces données afin de prédire les pannes, de réparer les équipements avant qu'ils ne tombent en panne et de réduire le coûts globaux de maintenance. Les aspects clés de la modélisation d'un problème de maintenance prédictive dépendent essentiellement du contexte industriel. Par exemple, les différences dans le nombre d'équipements à entretenir, les coûts de maintenance, les coûts de panne, ou la qualité des données disponibles peuvent amener des modélisations très différentes. Dans cette thèse, nous développons un cadre d'optimisation guidée par les données pour le problème de maintenance des avions chez Air France.</p><p>La maintenance prédictive pour Air France. Les performances des compagnies aériennes dépendent de la capacité à assurer tous les vols avec leurs avions de manière fiable. Comme on peut l'observer sur la figure 1.1, les planning horaires des avions sont denses, avec plusieurs vols entre chaque plage de maintenance. Sur les avions de nouvelle génération, des signaux échantillonnés à 1Hz sont enregistrés par des capteurs durant les vols. Pour chaque équipement, Air France a accès à un ensemble de séries temporelles qui correspondent à différents types de signaux comme des températures, des pressions, des intensités, ou des signaux binaires. Lorsqu'un avion arrive en maintenance, Air France utilise ces données afin de choisir quels équipements doivent être réparés. Les décisions de maintenance doivent assurer un équilibre entre les coûts de sur-maintenance et les coûts dus aux pannes sur les équipements. En outre, comme les plages de maintenance sont rares, Air France doit prioriser entre les équipements.</p><p>Les sujets de recherche traités dans cette thèse. Le problème de maintenance des avions chez Air France est de proposer une méthodologie d'aide à la décision pour trouver une politique de maintenance (en rouge sur la figure 2.1), qui prend en entrée les données brutes disponibles au début de la plage de maintenance, et retourne en sortie les équipements à réparer. Pour cela, nous n'avons pas de modèle ou de simulateur de l'évolution des équipements, mais nous avons accès à un historique de plusieurs années de données brutes et quelques dates de panne. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Équipement M</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Politique de maintenance</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Opérations de maintenance</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Opérations de maintenance</head><p>Figure <ref type="figure" target="#fig_18">2</ref>.1 -Le problème de maintenance d'un avion : on cherche une politique de maintenance (flèches rouges) qui prend en entrée les données brutes de M équipements et retourne la décision de maintenance en sortie.</p><p>Le problème que nous considérons est un problème d'optimisation stochastique multi-étapes guidée par les données. Cela nous mène vers trois axes de recherche au carrefour de la recherche opérationnelle et l'apprentissage machine.</p><p>Un premier axe de recherche s'intéresse au processus de décision d'un problème de maintenance prédictive que nous formalisons comme un problème d'optimisation séquentiel avec incertitudes. Pour chaque plage de maintenance, le décideur choisit dynamiquement un nombre restreint de composants à réparer. L'incertitude vient des pannes, qu'on modélise comme des événements aléatoires qui peuvent apparaître durant les jours de fonctionnement du système. On suppose que les composants évoluent de manière indépendante mais les décisions de maintenance prises sur chacun des composants sont liées par des contraintes de capacité. Ce problème appartient à une large classe de problèmes d'optimisation séquentielle avec plusieurs processus indépendants qui sont faiblement liés à chaque pas de temps par les décisions. Ces problèmes sont appelés programmes dynamiques faiblement couplés <ref type="bibr" target="#b1">[2]</ref>. Une particularité du problème de maintenance prédictive est que les décisions sont prises à partir d'observations bruitées, ce qui rend le système partiellement observable. Nous proposons plusieurs méthodes avec des garanties qui calculent de "bonnes" solutions pour ces problèmes d'optimisation.</p><p>Le second axe de recherche qui structure cette thèse se porte sur l'étude des problèmes d'optimisation stochastique où l'aléa a une certaine structure appelée diagramme d'influence. Cette classe de problème contient les programmes dynamiques faiblement couplés avec observations partielles qui modélisent le problème de maintenance des avions. Nous proposons un algorithme exact basé sur des outils de recherche opérationnelle et d'apprentissage machine exploitant la structure de l'aléa.</p><p>Enfin, le troisième axe de recherche aborde des questions statistiques qui viennent du problème pratique d'Air France. En effet, transformer le problème de maintenance des avions comme des programmes dynamiques faiblement couplés avec observations partielles néces-2.1. Le problème de maintenance prédictive avec contraintes de capacité site d'apprendre un modèle statistique qui représente l'évolution de la détérioration de chaque équipement au cours du temps. De plus, les exigences d'Air France imposent que ce modèle statistique soit interprétable. À l'aide d'une base de données brutes et de plusieurs dates de panne sur l'ensemble de la flotte d'avions, on propose une méthodologie qui apprend un modèle statistique qui peut être interprété par les ingénieurs de la maintenance.</p><p>La section 2.1 introduit de manière informelle le problème de maintenance prédictive avec contraintes de capacité. La section 2.2 résume nos principales contributions pour le problème d'optimisation des programmes dynamiques faiblement couplés avec observations partielles. La section 2.3 détaille nos contributions théoriques au problème d'optimisation dans les diagrammes d'influence. La section 2.4 décrit nos contributions pour résoudre les questions statistiques qui proviennent du problème de maintenance des avions.</p><p>Cette thèse commence en partie I par proposer des algorithmes pour les programmes dynamiques faiblement couplés avec observations partielles, alors que la partie III explique comment nous les exploitons pour le problème de maintenance des avions chez Air France. Le contenu de la partie II correspond à nos travaux sur le problème d'optimisation dans les diagrammes d'influence. A la fin de chaque chapitre de cette thèse, on trouvera une section contenant des remarques bibliographiques en rapport avec le sujet du même chapitre.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Le problème de maintenance prédictive avec contraintes de capacité</head><p>Dans l'industrie, planifier la maintenance d'un système revient à choisir quand on souhaite intervenir sur le système durant ses périodes de fonctionnement et choisir quelles actions réaliser durant ces interventions. Dans notre cas, les dates de maintenance de chaque avion sont déjà fixées par le programme de vol. C'est pourquoi le problème de maintenance prédictive que nous considérons se concentre seulement sur la deuxième problématique : choisir les actions à prendre durant des plages de maintenance déjà fixées.</p><p>Le système que nous considérons contient plusieurs composants qui évoluent chacun au cours du temps. En général, les systèmes considérés sont mécaniques. Cela nous assure que chaque composant se détériore au cours du temps et tombe en panne après plusieurs heures/jours de fonctionnement. Les pannes apparaissent durant les jours de fonctionnement des composants du système entre deux plages de maintenance. À chaque plage de maintenance, le décideur choisit les composants à réparer et ses choix sont basés sur une observation de l'état de dégradation de chaque composant. Cet état de dégradation caractérise les différents modes de performance de chaque composant et est représenté par un indicateur discret dont les valeurs vont de l'état "fonctionnement optimal" à l'état "panne". Le processus de détérioration d'un composant est stochastique dans le sens où un composant passe d'un état de dégradation s à un état de dégradation plus critique s selon une distribution de probabilité. Les composants évoluent de manière indépendante entre deux plages de maintenance, c-à-d, le processus de détérioration d'un composant n'influence pas le processus de détérioration des autres composants. Dans un contexte industriel, les composants sont couplés par les décisions de maintenance qui doivent respecter des contraintes de capacité. C'est le cas de notre problème : les décisions de maintenance du problème de maintenance des avions doivent satisfaire la contrainte suivante Le nombre de composants qu'on peut réparer durant une plage de maintenance ne peut dépasser K .</p><p>Alors qu'il serait plus simple de considérer le problème de maintenance de chacun des composants indépendamment, ces contraintes forcent le décideur à considérer l'évolution du système complet. Lorsqu'on répare un composant, on suppose que son état de dégradation retourne à l'état de "fonctionnement optimal". Sinon, il continue à évoluer vers un état plus dégradé. Réparer un composant amène un coût de maintenance qui est significativement moins élevé que le coût de panne.</p><p>Le problème de maintenance prédictive avec contraintes de capacité est résumé sur la figure 2.3. Choisir une politique de maintenance revient à choisir pour chaque plage de main-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Processus de décisions avec observations partielles</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Décision de maintenance</head><p>Composant M . . .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>. . . Composant 2</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>État de dégradation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Observation partielle</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Politique de maintenance</head><p>Composant 1</p><p>Figure <ref type="figure" target="#fig_18">2</ref>.3 -Modélisation du problème de maintenance prédictive avec contraintes de capacité pour un système de M composants. Les flèches bleues et rouges représentent respectivement la détérioration stochastique des composants et la politique de maintenance. À chaque plage de maintenance, chaque composant est dans un état de dégradation et le décideur a accès à une observation partielle pour chaque composant. Puis le décideur applique une politique de maintenance (flèches rouges) qui retourne une décision de maintenance qui impacte l'état de dégradation de chaque composant.</p><p>tenance, pour chaque observation partielle possible, une décision de maintenance qui satisfait les contraintes de capacité. Alors, l'objectif du décideur est de choisir une politique de maintenance qui minimise l'espérance des coûts qui s'écrivent comme la somme des coûts de maintenance et des coûts de panne. Dans le chapitre 3 on formalise mathématiquement le problème de maintenance prédictive avec contraintes de capacité. Cela soulève alors deux questions scientifiques : Comment transforme-t-on le problème de maintenance des avions en un problème de maintenance prédictive avec contraintes de capacité (section 2.4), et, comment trouve-t-on une "bonne" politique de maintenance (section 2.2) ?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Processus de décisions avec observations partielles</head><p>Nous débutons notre discussion sur le choix d'une "bonne" politique de maintenance en introduisant une modélisation mathématique adéquate pour le problème de maintenance prédictive avec contraintes de capacité. Comme le processus de détérioration d'un composant est markovien et que le décideur a seulement accès à une observation partielle, on formalise de manière naturelle le problème comme un Processus de Décision Markovien Partiellement Observable (POMDP) <ref type="foot" target="#foot_2">1</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Chapter 2. Introduction (Français)</head><p>Le cadre des POMDPs. On commence par considérer un système avec un composant. Pour de tels problèmes, à chaque pas de temps, le composant est dans un état s qui appartient à un espace d'états fini X S . Le décideur n'observe pas s, mais a accès à une observation o qui appartient à un espace d'observation fini X O , et qui est émis aléatoirement avec probabilité p(o|s). À partir de cette observation, le décideur choisit une action a qui appartient à un espace d'action fini X A . Le composant évolue aléatoirement vers un nouvel état s dans X S avec probabilité p(s |s, a) et le décideur reçoit une récompense r (s, a, s ). L'objectif du décideur est de trouver une politique δ t a|o , qui représente la probabilité conditionnelle de prendre une action a dans X A sachant l'observation o au temps t , qui maximise l'espérance totale de la récompense jusqu'à un horizon fini T. </p><formula xml:id="formula_7">P δ (S t = s t ,O t = o t , A t = a t ) 1 t T , S T +1 = s T +1 = p(s 1 ) T t =1 p(o t |s t )p(s t +1 |s t , a t )δ t a t |o t (2.2) pour tout (s t , o t , a t ) 1 t T , s T +1 dans X S × X O × X A T × X S .</formula><p>Le problème (2.1) correspond au problème POMDP avec politique sans mémoire, ce qui signifie que l'action prise à un certain temps ne dépend que de l'observation courante au lieu de dépendre de l'historique des observations et des actions passées. Les POMDPs sont une généralisation des Processus de Décision Markovien (MDPs) où le décideur a accès à l'état courant du système. Alors que les MDPs peuvent être résolus en temps polynomial, le problème POMDP avec politiques sans mémoire est NP-difficile <ref type="bibr" target="#b86">[87]</ref>. À moins que P=NP, le fait que le décideur ait seulement accès à une observation partielle rend le problème plus difficile à résoudre.</p><p>Une contribution de cette thèse est un Programme Linéaire en Nombres Entiers (PLNE), un outil standard de recherche opérationnelle, qui donne une politique optimale du problème POMDP avec politiques sans mémoire. Un tel problème d'optimisation se formule par une fonction objectif linéaire, des contraintes linéaires et des variables entières avec la forme canonique suivante : min Alors que le problème de maximisation d'un MDP peut être résolu par un programme linéaire (voir, p. ex., <ref type="bibr" target="#b123">[124]</ref>), c-à-d, sans contraintes d'intégrité, notre PLNE résout le problème de maximisation du POMDP (2.1). Ces formulations en nombres entiers peuvent être résolues de  Si un décideur a accès à l'état de dégradation de chaque composant, alors chaque sous-problème est un problème de MDP et le problème sur le système complet est un ensemble de MDPs faiblement couplés <ref type="bibr" target="#b104">[105]</ref>. Dans notre cas, le fait que le décideur ait uniquement accès à des observations partielles nécessite d'introduire la notion de POMDPs faiblement couplés. En plus de l'aspect partiellement observé, une deuxième difficulté dans la résolution d'un tel problème vient de la malédiction de la dimension. En effet, les tailles des espaces grandissent exponentiellement avec le nombre de composants du système, ce qui rend les algorithmes usuels coûteux en ressources informatiques. Pour comprendre cette difficulté, on peut observer qu'encoder une politique de maintenance δ revient à stocker une table de taille environ égale à T M m=1 |X m O ||X m A |, ce qui est en générale infaisable. Pour contourner cette difficulté, on introduit la notion de politique implicite. À l'inverse des politiques explicites qui encodent complètement la politique comme une solution d'un problème d'optimisation, les politiques implicites sont des politiques telles que, pour chaque observation o, le vecteur (δ t a|o ) a est calculé à l'aide d'un problème d'optimisation adapté. Cela a notamment un intérêt pratique parce que le décideur n'a pas besoin de calculer les valeurs des politiques pour toutes les observations. En effet, une fois qu'une observation o est révélée au temps t , on a seulement besoin de calculer le vecteur (δ t a|o ) a . Une autre contribution décrite dans la partie I de cette thèse est une formulation PLNE basée sur celle qu'on a introduit pour le problème de POMDP avec politiques sans mémoire et on l'utilise pour calculer une "bonne" politique implicite pour le problème de POMDPs faiblement couplés. Comme les formulations de Adelman and Mersereau <ref type="bibr" target="#b1">[2]</ref> pour les MDPs faiblement couplés, l'avantage principal de notre formulation est de contenir un nombre polynomial de variables et de contraintes, ce qui casse la malédiction de la dimension. Une fois de plus, on améliore notre formulation en ajoutant un ensemble d'inégalités valides pour notre formulation PLNE. De plus, on donne des garanties théoriques sur la valeur optimale de notre formulation PLNE en calculant une borne inférieure et une borne supérieure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Prise de décision avec une incertitude structurée</head><p>Les MDPs et les POMDPs (voir section 2.2) sont des cas particuliers de problème d'optimisation stochastique discret où la distribution de probabilité des variables aléatoires satisfait une certaine structure qu'on appelle diagramme d'influence. Le diagramme d'influence est un outil flexible qui permet de modéliser un grand ensemble de problèmes d'optimisation stochastique où on suppose que la "nature" est structurée. On introduit le problème d'optimisation dans les diagrammes d'influence à travers un exemple simple.</p><p>Cet exemple est représenté en figure 2.4a. On considère un patient qui va consulter un docteur. Le patient souffre d'une maladie D, que le docteur souhaite déterminer. Cette maladie apparaît sur un patient avec la probabilité P(D), et elle donne des symptômes observables S, qui sont émis aléatoirement avec probabilité P(S|D). À partir de l'observation des symptômes S, le docteur décide quel traitement T appliquer au patient. Le traitement T et la maladie D donnent une réponse R 0 du patient avec probabilité P(R 0 |T, D), avec une récompense r 0 (R 0 ) indiquant la présence ou non d'effets secondaires indésirables. Ensuite, à partir de cette réponse R 0 le docteur décide si la maladie nécessite une opération médicale plus importante I . On obtient une nouvelle réponse R 1 avec probabilité P(R 1 |I , D), qui indique si l'opération sur la maladie D a été un succès ou non, et on modélise cela avec une récompense r 1 (R 1 ). La maladie D, les symptômes S, les réponses R 0 et R 1 , représentent les variables aléatoires sur lesquels le docteur n'a pas de contrôle. Elles sont appelées variables chances parce que leurs valeurs sont choisies par la "nature", c-à-d, tout ce sur quoi le docteur n'a pas le contrôle. À l'inverse des variables chances, le traitement T et l'opération médicale I sont représentés par des variables aléatoires qu'on appelle variables de décision, dont les valeurs sont choisies par le docteur. Les fonctions r 0 et r 1 sont appelées fonctions d'utilité qui dépendent respectivement des réponses R 0 et R 1 . L'objectif du docteur est de choisir une stratégie δ = δ T (T |S), δ I (I |R 0 ) qui maximise l'espérance de son utilité E δ r 0 (R 0 ) + r 1 (R 1 ) où l'espérance est prise selon la distribution de probabilité P δ (D, S, T, R 0 ,</p><formula xml:id="formula_8">I , R 1 ) = P(D)P(S|D)δ T (T |S)P(R 0 |T, D)δ I (I |R 0 )P(R 1 |I , D).</formula><p>Ce type de problème peut être encodé graphiquement à l'aide d'un graphe orienté acyclique, dont l'ensemble de sommets V contient trois sortes de sommets : les sommets chances (V s ), décisions (V a ) et utilités (V r ) qui correspondent respectivement aux variables chances, de décision et d'utilité. Le diagramme d'influence, qui a été introduit par Howard and Matheson <ref type="bibr" target="#b55">[56]</ref>, est un graphe orienté acyclique sur cet ensemble de sommets et où il n'y a pas d'arcs sortants  des sommets utilités. La figure 2.4a représente le diagramme d'influence qui modélise le problème de décision médicale qu'on a décrit précédemment. Pour chaque variable chance X v , représenté par un sommet chance v, on associe une probabilité conditionnelle P(X v |X pa(v) ) de X v sachant X pa(v) , où X pa(v) est un vecteur de variables aléatoires qui correspondent aux parents de v dans le diagramme d'influence, c'est-à-dire l'ensemble de sommets qui ont un arc sortant vers v. Pour chaque fonction utilité r v , représentée par le sommet utilité v, on associe une fonction déterministe r v qui à chaque valeur instanciée de X pa(v) retourne un réel. Les choix du décideur sont modélisés à l'aide d'une stratégie δ = (δ v ) v∈V a qui est un vecteur de probabilités conditionnelles δ v (X v |X pa(v) ) = P(X v |X pa(v) ). Étant donnée une stratégie δ, la distribution de probabilité jointe sur l'ensemble des variables aléatoires s'écrit</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Prise de décision avec une incertitude structurée</head><formula xml:id="formula_9">D S T R 0 r 0 I R 1 r 1 (a) Le problème de décision médicale s 1 o 1 r 1 a 1 s 2 o 2</formula><formula xml:id="formula_10">P δ (X V c ∪V a = x V c ∪V a ) = v∈V c P(X v = x v |X pa(v) = x pa(v) ) v∈V a δ v (X v = x v |X pa(v) = x pa(v) ). (2.4)</formula><p>Le problème d'optimisation que le décideur cherche à résoudre est le problème du maximum d'utilité espérée</p><formula xml:id="formula_11">max δ∈∆ E δ v∈V r r v (X pa(v) ) ,<label>(2.5)</label></formula><p>où ∆ est l'ensemble des probabilités conditionnelles possibles pour chaque sommet décision.</p><p>On peut observer la capacité de modélisation des diagrammes d'influence sur l'exemple des POMDPs décrit dans la section 2. ) où l'espérance est prise selon la distribution de probabilité P δ . On s'intéresse alors à la question de trouver une stratégie qui résout le problème du maximum d'utilité espérée dans un diagramme d'influence. Ce problème soulève deux questions scientifiques. Premièrement, évaluer une stratégie est déjà difficile. En effet, cela revient à résoudre le problème d'inférence dans un diagramme d'influence. Étant donnés une stratégie réalisable δ et un sous-ensemble de sommets C ⊆ V, le problème d'inférence consiste à calculer la probabilité marginale P δ (X C = x C ). Deuxièmement, optimiser sur l'ensemble des stratégies possibles ∆ est aussi difficile du fait que le nombre possible de vecteurs de probabilités conditionnelles est très important.</p><p>Dans la partie II, on se focalise sur la deuxième question. On généralise les résultats obtenus pour les POMDPs de la section 2.2 à tout problème du maximum d'utilité espérée dans un diagramme d'influence. Nos résultats sont basés sur plusieurs outils de la théorie des modèles graphiques probabilistes et la programmation mathématique en nombres entiers. Une première contribution décrite dans la partie II est un PLNE qui donne une stratégie optimale du problème du maximum d'utilité espérée dans un diagramme d'influence. Alors que les solutions proposées dans la littérature sont basées sur des algorithmes de type programmation dynamique sur le graphe (voir, p. ex., <ref type="bibr" target="#b75">[76,</ref><ref type="bibr" target="#b80">81,</ref><ref type="bibr" target="#b88">89]</ref>), notre approche est basée sur la programmation mathématique. Une seconde contribution est un ensemble d'inégalités valides pour notre formulation en nombres entiers. Une troisième contribution est une caractérisation du cas "simple" de diagramme d'influence, c-à-d, les diagrammes d'influence tels que le problème du maximum d'utilité espérée est plus facile à résoudre. Une partie des résultats de la partie II a été publiée dans Parmentier et al. <ref type="bibr" target="#b116">[117]</ref>. Plusieurs nouveaux résultats ont été ajoutés après la publication de cet article.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Une méthodologie statistique pour le problème de maintenance des avions</head><p>On souhaite utiliser notre politique pour les POMDPs faiblement couplés, mentionnée dans la section 2. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Part I Integer programming for predictive maintenance 3 Predictive maintenance with capacity constraints</head><p>This chapter formalizes the predictive maintenance problem with capacity constraints as a Partially Observable Markov Decision Process (POMDP). We recall the context of such a problem. We consider a system composed of multiple deteriorating components, which deteriorates over time. We suppose that there are scheduled decision times, or maintenance slots, where a decision maker decides which components to maintain. Due to the maintenance capacity, only a limited number of components can be maintained. At each maintenance slot, the decision maker has access to a discrete observation of the system. His decision is modeled using a maintenance policy, which maps an observation to a decision. The goal of the decision maker is to find a maintenance policy minimizing the costs expressed as the sum of the expected failure costs and the expected maintenance costs. We use this objective function because in practice the airline knows the failure costs and the maintenance costs.</p><p>The POMDP is a natural way to see the predictive maintenance problem <ref type="bibr" target="#b23">[24]</ref>. However, when the system has several components, the size of its state space becomes exponential in the number of components, which makes the corresponding POMDP intractable. This issue is known as the curse of dimensionality <ref type="bibr" target="#b11">[12]</ref>. To address this issue, we exploit the fact that the system can be decomposed into several components. To do so, we formalize the predictive maintenance problem with capacity constraints on a system with multiple components and capacity constraints using weakly coupled POMDP that we introduce and which is analogs of the weakly coupled dynamic programs of Adelman and Mersereau <ref type="bibr" target="#b1">[2]</ref>. <ref type="foot" target="#foot_5">1</ref> In this formalization, we assume that the observations are discrete and all the model parameters are known. In Chapter 10, we will use the weakly coupled POMDP to address the concrete predictive maintenance problem of Air France and this assumption will be discussed.</p><p>Chapter 3 is organized as follows.</p><p>• Section 3.1 recalls the necessary background on POMDPs.</p><p>• Section 3.2 introduces the notion of weakly coupled POMDP as a special case of POMDP.</p><p>• Section 3.3 describes how we formalize the predictive maintenance problem with capacity constraints using a weakly coupled POMDP. • Section 3. <ref type="bibr" target="#b3">4</ref> shows examples that can be formalized as a weakly coupled POMDP. • Finally, Section 3.5 contains bibliographical remarks on maintenance problems, and the use of POMDPs for maintenance optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Background on POMDP</head><p>In this section, we recall the POMDP problem and the POMDP problem with memoryless policies. In Section 3.3, we explain why we choose a memoryless policy for the predictive maintenance problem and several numerical experiments in Section 4.5 support our choice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">POMDP parameters</head><p>We recall here the definition of a POMDP. </p><formula xml:id="formula_12">X S × X A × X S → R,</formula><p>which we will also view as a vector r = r (s, a, s</p><formula xml:id="formula_13">) ∈ R X S ×X A ×X S .</formula><p>We denote by p the vector of probabilities p = p(s), p(o|s), p(s |s, a) s,s ∈X S ,o∈X O a∈X A</p><p>. A POMDP is parametrized by the fifth tuple (X S , X O , X A , p,r).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">POMDP problem</head><p>Let (X S , X O , X A , p,r) be a POMDP. Given a finite horizon T ∈ Z + , the choices made by the decision maker are modeled using a policy δ = (δ 1 , . . . , δ T ), where δ t is the conditional probability distribution of taking action A t at time t given the history of observations and actions</p><formula xml:id="formula_14">H t = (O 1 , A 1 , . . . , A t -1 ,O t ) in X t H := (X O × X A ) t × X O , i.e., δ t a|h := P(A t = a|H t = h),</formula><p>for any a in X A and h in X t H . We denote by ∆ his the set of policies</p><formula xml:id="formula_15">∆ his = δ ∈ R T t =1 X t H ×X A : a∈X A δ t a|h = 1 and δ t a|h 0, ∀h ∈ X t H , a ∈ X A , t ∈ [T ] .</formula><p>In ∆ his , "his" refers to policies that take into account the history of observations and actions by opposition to memoryless policies which will be introduced below. A policy δ ∈ ∆ his leads to the probability distribution </p><formula xml:id="formula_16">P δ on (X S × X O × X A ) T × X S such that P δ (S t = s t ,O t = o t , A t = a t ) 1 t T , S T +1 = s T +1 = p(</formula><formula xml:id="formula_17">∆ ml = δ ∈ R T ×X A ×X O : a∈X A δ t a|o = 1 and δ t a|o 0, ∀o ∈ X O , a ∈ X A .</formula><p>In ∆ ml , "ml" refers to memoryless. A policy δ ∈ ∆ ml leads to the probability distribution P δ on We denote by E δ the expectation according to P δ . The goal of the decision maker is to find a memoryless policy δ in ∆ ml maximizing the expected total reward over the finite horizon T .</p><formula xml:id="formula_18">(X S × X O × X A ) T × X S</formula><p>The POMDP problem with memoryless policy is exactly the following:</p><formula xml:id="formula_19">max δ∈∆ ml E δ T t =1 r (S t , A t , S t +1 ) (P ml )</formula><p>The definition of ∆ ml ensures that ∆ ml ⊆ ∆ his . The policy is said to be memoryless because the decision maker takes its decision given the current observation instead of the full history of observations and actions. To clarify the distinction between ∆ ml and ∆ his , we say that a policy belonging to ∆ his is a history-dependent policy. It is known that (P ml ) is NP-hard <ref type="bibr" target="#b86">[87]</ref>. In Section 4.5, we provide numerical experiments showing that memoryless policies perform well on different kind of problems modeled as a POMDP, especially the maintenance problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Weakly coupled POMDP</head><p>Like MDPs, the POMDPs suffer from the curse of dimensionality. We can observe this phenomenon when we consider systems that consist of a collection of smaller subsystems or components. It is the case of the predictive maintenance problem we want to formalize within the POMDP framework. In order to catch and leverage the specific structure of systems with sev-eral components, we introduce a special case of POMDP, the weakly coupled POMDP. We give its formal definition in this paragraph.</p><p>A weakly coupled POMDP models a system composed of M components, each of them evolving independently as a POMDP. </p><formula xml:id="formula_20">X S = X 1 S × • • • × X M S and the observation space X O = X 1 O × • • • × X M O .</formula><p>The spaces X S and X O represent the state space and the observation space of the full system. We assume that the action space X A can be written</p><formula xml:id="formula_21">X A = a ∈ X 1 A × • • • × X M A : M m=1 D m (a m ) b ,<label>(3.2)</label></formula><p>where X m A is the individual action space of component m, and D m : X m A → R q is a given function for each component m in [M ], and b ∈ R q is a given vector for some finite integer q. Without loss of generality, we assume that b 0 in the remaining of this thesis 2 .</p><p>Each component is assumed to evolve independently, hence the joint probability of emission factorizes as</p><formula xml:id="formula_22">P(O t = o|S t = s) = M m=1 p m (o m |s m ),<label>(3.3)</label></formula><p>and the joint probability of transition factorize as Hence, the weakly coupled POMDP problem with memoryless policies is the following:</p><formula xml:id="formula_23">P(S 1 = s) =</formula><formula xml:id="formula_24">max δ∈∆ ml E δ T t =1 r (S t , A t , S t +1 ) (P wc ml )</formula><p>where the expectation is taken according to P δ defined in (3.1). Note that unless X A = there always exists a feasible policy of (P wc ml ). A weakly coupled POMDP is fully parametrized by</p><formula xml:id="formula_25">(X m S , X m O , X m A , p m , r m , D m ) m∈[M ] , b .</formula><p>(P wc ml ) is equivalent to (P ml ) on the state space </p><formula xml:id="formula_26">X S = X 1 S × • • • × X M S ,</formula><formula xml:id="formula_27">= a ∈ X 1 A × • • • × X M A : M m=1 D m (a m ) b .</formula><p>Then, it follows that X A = XA .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Formalizing the predictive maintenance problem with capacity constraints</head><formula xml:id="formula_28">X 1 O ×• • •×X M</formula><p>O , and the action space X A defined in <ref type="bibr">(3.2)</ref>. This notion of weakly coupled POMDP is an extension of the weakly coupled dynamic program introduced by Adelman and Mersereau <ref type="bibr" target="#b1">[2]</ref> to the case where the decision maker has only access to a partial observation of the system state. The definition (3.2) of the action space for weakly coupled POMDP is exactly the same as Bertsimas and Mišić [15, Eq. ( <ref type="formula" target="#formula_191">7</ref>)] for weakly coupled dynamic programs. Note that we can also defined the weakly coupled POMDP problem with history-dependent policies (P wc his ). Remark 1. In the definition of POMDP, we could have considered a variant where the observation O t may depend on A t -1 given S t and the emission probability distribution becomes P(O t = o|A t -1 = a , S t = s) := p(o|a , s). All the mathematical programming formulations and theoretical results in this thesis can be extended to this case. We choose to consider the case above to lighten the notation.</p><p>Remark 2. In many stochastic problems, the action space does not decompose along the components, i.e., we can no longer assume that</p><formula xml:id="formula_29">X A ⊆ X 1 A × • • • × X M A .</formula><p>It turns out that, using a transformation, we can always convert such a problem into a weakly coupled POMDP. This transformation is similar to the one introduced by Bertsimas and Mišić <ref type="bibr" target="#b14">[15,</ref><ref type="bibr">Sec 4.3]</ref>. We set the individ-</p><formula xml:id="formula_30">ual action spaces X m A = X A for each component m in [M ]. For any (a 1 , . . . , a M ) ∈ X 1 A × • • • × X M</formula><p>A , we enforce the following linking constraints a m = a m+1 for all m in [M ]. Therefore, the action space can be written a ∈</p><formula xml:id="formula_31">X 1 A × • • • × X M A : a m = a m+1 , ∀m ∈ [M -1]</formula><p>, which has the requested form (3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Formalizing the predictive maintenance problem with capacity constraints</head><p>In this section we formalize the predictive maintenance problem with capacity constraints using a weakly coupled POMDP. It requires to define the parameters (</p><formula xml:id="formula_32">X m S , X m O , X m A , p m , r m , D m ) m∈[M ] , b .</formula><p>The system is composed of M components. </p><formula xml:id="formula_33">X O = X 1 O × • • • × X M O .</formula><p>Component m starts in state s with probability p m (s). At each time t , component m is in state S m t = s, and it emits an observation O m t = o with probability p m (o|s). Then, the decision maker takes an action A t = A 1 t , . . . , A M t in a finite space X A , where A m t is a binary variable equal to 1 when component m is maintained. At each maintenance slot, the decision maker can maintain at most K components. Hence, we write the action space X A as follows</p><formula xml:id="formula_34">X A = a = (a 1 , . . . , a M ) ∈ {0, 1} M : M m=1 a m K . (3.6)</formula><p>Therefore, X A contains only one scalar constraint (q = 1) and satisfies (3. </p><formula xml:id="formula_35">r m (s, a, s ) = -1 s m F (s )C m F -1 1 (a)C m R ,<label>(3.8)</label></formula><p>for any s, s ∈ X m S and a ∈ X m A . We assume that the reward decomposes additively as (3.5). Since our motivation was to find the right framework to formalize the maintenance problem, it would have been more natural to define (P ml ) as a minimization problem. However, we choose to stick to the practice of using rewards, which is common in the literature on POMDPs.</p><p>We model the choices of the decision maker using a memoryless policy. The interest of memoryless policies is threefold. First, maintenance technicians can easily apply them in practice. Second, even if finding an optimal memoryless policy remains NP-hard <ref type="bibr" target="#b86">[87]</ref>, the resulting optimization problem is more tractable. Third, even if in the general case memoryless policies are not optimal, there is a broad class of systems for which memoryless policies perform well <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b83">84]</ref> and we provide numerical experiments in Section 4.5 on instances from the literature showing that it is the case for maintenance problems.</p><p>Given a finite horizon T , the predictive maintenance problem with capacity constraints consists in finding a policy in ∆ ml that solves (P wc ml ) with X A defined in (3.2), (p m ) m∈[M ] satisfying (3.7), and (r m ) m∈[M ] defined in <ref type="bibr">(3.8)</ref>.</p><p>Remark 3. In fact, we could have modeled the predictive maintenance problem with capacity constraints as a weakly coupled POMDP over an infinite horizon with a discounted reward. Indeed, in practice, the predictive maintenance problem consists in planning the maintenances over a large horizon, which can be modeled by an infinite horizon planning problem. In this case, the predictive maintenance problem with capacity constraints consists in finding a policy that maximizes the total expected discounted reward over infinite horizon max δ∈∆ his E δ ∞ t =1 γ t -1 r (S t , A t , S t +1 ) where γ &lt; 1 is a discount factor. However, in practice Air France would like to modify the maintenance planning over a short horizon without discounting. Since the observations arrive dynamically, Air France would like to modify its planning in an "on-line" manner due to unexpected changes detected in the behavior of an equipment. In addition, as we will explain in Chapter 10, the parameters of the POMDPs are estimated through a statistical methodology and sometime the resulting long term predictions can be inaccurate. It pushes us to consider POMDP with finite horizon. This choice is supported by Walraven and Spaan [158, Section 3.1], which showed that casting a finite horizon problem with infinite horizon problem with discounting can lead to widely suboptimal policies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Examples modeled as a weakly coupled POMDP</head><p>In this section we describe several multi-stage stochastic optimization problems that can be formalized as a weakly coupled POMDP. More examples of POMDPs applications can be found for instance in the survey of Cassandra <ref type="bibr" target="#b23">[24]</ref>.</p><p>Example 1. Multi-armed and Restless Bandit problems are classical resource allocation problems where there are several arms, each of them evolving independently as a MDP, and at each time the decision maker has to activate a subset of arms so as to maximize its expected discounted reward over infinite horizon. We can consider regular multi-armed bandit problem, where only the activated arm states transit randomly and give an immediate reward, or the restless multi-armed bandit problem, where all the arm states transit randomly and give an immediate reward. When the decision maker has only access to a partial observation on each arm instead of the arm state, the problem becomes a partially observable multi-armed bandit problem <ref type="bibr" target="#b78">[79]</ref>. In this case, each arm evolves individually and independently as a POMDP. Such a problem enables to model practical applications such as clinical trials. In this setting, each component represents a medical treatment and activating a component corresponds to testing the treatment. The state of a medical treatment corresponds to its efficiency and the observation corresponds to a noisy measure of the efficiency of a medical treatment.</p><p>We can formalize the partially observable multi-armed bandit problem within our weakly coupled POMDP framework. Let M be the number of arms. At each time t , the decision maker has to activate K &lt; M arms. Since each component evolves as POMDP, we use the same notation to represent the state and the observation of Section 3.2. We define the individual action space X m A = {0, 1} of arm m and the full action space is</p><formula xml:id="formula_36">X A = a ∈ X 1 A × • • • × X M A : M m=1 a m = K .</formula><p>In the case of the regular bandit problems, the immediate reward of component m satisfies r m (s, 0, s ) = 0, and the transition probabilities satisfy p m (s |s, 0) equals 1 if s = s and 0 otherwise, for every s, s ∈ X m S . The goal of the decision maker is to find a policy δ in ∆ ml (or ∆ his ) maximizing the total expected discounted reward over infinite horizon E δ T t =1 r (S t , A t , S t +1 ) , where T is a finite horizon.</p><p>Note that the the predictive maintenance problem with capacity constraints can also be modeled as a restless partially observable multi-armed bandit problem. Indeed, it suffices to add a fictive component M + 1 corresponding to performing no action, with no reward, and a single state coinciding with a single observation. Then, the action space defined in (3.6) can be</p><formula xml:id="formula_37">written X A = a ∈ X 1 A × • • • × X M A : M +1 m=1 a m = K .</formula><p>Example 2. Consider a supplier that delivers a product to M stores. At each time t , we denote by S m t the inventory level of store m. Unfortunately, due to "inventory records inaccuracy" <ref type="bibr" target="#b102">[103]</ref> from various uncertainties, the supplier does not observe directly this inventory level. He has instead only access to a noisy observation O m t of the inventory level of store m. We assume that the inventory level of store m has a known limited capacity C m . Hence, we set X m S := {0, . . . ,C m }. Then O m t = o is an noisy observation of the current inventory level, whose value belongs to X m O := X m S and is randomly emitted given a current state S m t = s according to a known probability p m (o|s) = P O m t = o|S m t = s . At each time, the supplier has to decide the quantity to produce and to deliver automatically to each store. We denote by A m t the delivered quantity of product to store m, which belongs to the individual action space X m A := X m S . The production has to satisfy resource constraints (raw materials, staff, etc.). Hence, the set of feasible actions has the form </p><formula xml:id="formula_38">X A := a ∈ X 1 A × • • • × X M A : M m=1 h m a m H</formula><formula xml:id="formula_39">r m (s, a, s ) = price m (s+a-s )-waste m max s + a -C m , 0 -shortage m E P m D max D m -(s + a), 0 ,</formula><p>where price m is the selling price per unit, waste m is the wastage cost per unit and shortage m is the shortage cost per unit. It leads us to model this problem as a weakly coupled POMDP. The goal of the supplier is to find a policy δ in ∆ ml (or ∆ his ) maximizing the total expected reward over a finite horizon T . This example has been introduced by Kleywegt et al. <ref type="bibr" target="#b73">[74]</ref> for fully observable inventory levels and Mersereau <ref type="bibr" target="#b102">[103]</ref> justifies the relevance of the POMDP framework for the stochastic inventory control problem.</p><p>Example 3. Consider a nurse assignment problem for home health care. A medical center follows M patients at home on a daily basis over a given period of time T . On day t , we denote by S m t the health state of patient m, whose value belongs to a finite state space X m S . The medical center does not directly observe the health state of each patient. However, at each time t , the medical center has access to a partial observation O m t corresponding to a signal sent by a machine which diagnoses patient m. We assume that this signal is discrete and noisy. Hence, X m O is a finite space and an observation o is randomly emitted given a state s ∈ X m S according to the probability p m (o|s). At each time, the medical center has to assign nurses to patient. There are K 1 available nurses with skill 1 and K 2 available nurses with skill 2. On day t , we denote by A m t the action taken by the medical center on patient m, whose value belongs to X m A = {0, 1, 2, 3} and the following meaning. </p><formula xml:id="formula_40">A m t =              0 if</formula><formula xml:id="formula_41">X A = a ∈ X 1 A × • • • × X M A : M m=1 1 1 (a m ) + 1 3 (a m ) K 1 and M m=1 1 2 (a m ) + 1 3 (a m ) K 2 .</formula><p>Now we can define the immediate reward function</p><formula xml:id="formula_42">r m (s, a, s ) = -cost m 1 (1 1 (a) + 1 3 (a)) -cost m 2 (1 2 (a) + 1 3 (a)) -emergency m 1 s m critic (s ),</formula><p>where cost m i is the cost induced by sending a nurse with skill i ∈ {1, 2} to patient m, s m critic is the critical health state of patient m and emergency m is the cost induced by an emergency because patient m reaches its critical health state. It leads us to model this problem as a weakly coupled POMDP. The goal of the medical center is to find a policy δ in ∆ ml (or ∆ his ) maximizing its total expected reward over a finite horizon T .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Bibliographical remarks</head><p>In the past decades, the optimization in maintenance problems of deteriorating systems have been widely studied in academic research. For a complete description of the various types of maintenance problems, see for instance the surveys of Alaswad and Xiang <ref type="bibr" target="#b2">[3]</ref>, Valdez-Flores and Feldman <ref type="bibr" target="#b153">[154]</ref>, Wang <ref type="bibr" target="#b158">[159]</ref>. The predictive maintenance problem with capacity constraints belongs to the big family of multistage stochastic optimization problems. Since describing such problems is beyond the scope of this thesis, we refer the interested reader to the tutorial of Powell <ref type="bibr" target="#b121">[122]</ref>.</p><p>Partially Observable Markov Decision Processes. The POMDP framework can be adapted for a wide range of various applications such as maintenance problems <ref type="bibr" target="#b41">[42]</ref> or clinical decision making <ref type="bibr" target="#b36">[37]</ref>. More decision making problems which can be modeled as a POMDP are described in the survey by Cassandra <ref type="bibr" target="#b23">[24]</ref>. Modeling a deteriorating system using partially observable continuous time Markov chains has been done in the literature <ref type="bibr" target="#b69">[70,</ref><ref type="bibr" target="#b70">71,</ref><ref type="bibr" target="#b89">90,</ref><ref type="bibr" target="#b90">91]</ref>. Kim et al. <ref type="bibr" target="#b71">[72]</ref> showed the efficiency of an optimal policy of such a formalization in the mining industry. In all these works, the Markov chains have at most three states. Many scientific works show that the use of POMDPs to model a maintenance problem gives promising results in practice <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b68">69]</ref>. In particular, it is showed that modeling the maintenance problem as a POMDP instead of a MDP leads to lower maintenance costs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Memoryless policies.</head><p>As mentioned in Section 3.3, in addition to their practical advantages (tractability and easy to apply) the memoryless policies perform well on a broad class of systems in practice <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b83">84,</ref><ref type="bibr" target="#b105">106,</ref><ref type="bibr" target="#b163">164]</ref>. It has been even proved that for contextual MDPs, which are special cases of POMDPs, there always exists an optimal policy that is memoryless <ref type="bibr" target="#b76">[77]</ref>. On the other hand, Littman <ref type="bibr" target="#b86">[87]</ref> identifies some pathological cases where the memoryless policies do not work well. Memoryless policies have received much interest with the increasing popularity of reinforcement learning <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b58">59,</ref><ref type="bibr" target="#b142">143]</ref>. Indeed, in this context the decision maker does not have access to the parameters p and r. Hence, he has to optimize his policy and estimate the parameters at the same time. Azizzadenesheli et al. <ref type="bibr" target="#b6">[7]</ref> explain why in this context the memoryless policies are of interest and especially the stochastic memoryless policies, which map an observation to a probability distribution over the set of decisions, in opposition to deterministic memoryless policies, which map an observation to a decision. In particular, the stationary policies, i.e., δ t = δ 1 for all t in [T ], are much more appreciated in the reinforcement learning community <ref type="bibr" target="#b107">[108]</ref>.</p><p>Weakly coupled dynamic programs. As mentioned in Example 1, the predictive maintenance problem with capacity constraints can also be formalized as a restless partially observable multi-armed bandits problem, which is a special case of the weakly coupled POMDP problem.</p><p>When the decision maker observes the arm states, the problem corresponds to a restless multiarmed bandit. Such a problem lies in the broad class of multi-stage stochastic optimization problems that decompose into independent subproblems, which are linked by a collection of constraints on the action space. Precisely, each subsystem is a MDP on disjoint state spaces but the actions taken on each subsystem at each time are linked by a collection of constraints. When these constraints bind weakly, such a problem becomes a weakly coupled dynamic program <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b104">105]</ref>. It enables to model various types of practical problems such as stochastic inventory routing with limited vehicle capacity <ref type="bibr" target="#b73">[74]</ref>, stochastic multi-product dispatch problems <ref type="bibr" target="#b111">[112]</ref>, scheduling problems <ref type="bibr" target="#b161">[162]</ref>, resources allocation <ref type="bibr" target="#b48">[49]</ref>, revenue management <ref type="bibr" target="#b151">[152]</ref> and others. More examples can be found for instance in the dissertation of Hawkins <ref type="bibr" target="#b54">[55]</ref>.</p><p>In our case, the decision maker has only access to a partial observation of the arm's state. If the arms are rested, such a problem has been introduced as POMDP multi-armed bandit problem by Krishnamurthy and Wahlberg <ref type="bibr" target="#b78">[79]</ref>. If the arms are restless, like in our maintenance problem, our formalization of the predictive maintenance problem is related to the model considered in the recent works of Mehta et al. <ref type="bibr" target="#b100">[101]</ref>, Meshram et al. <ref type="bibr" target="#b103">[104]</ref>, Mehta et al. <ref type="bibr" target="#b101">[102]</ref> and Abbou and Makis <ref type="bibr" target="#b0">[1]</ref>. In these works, the model considered has at most two states and three observations, which is lower than the number of states and observations we consider in practice (see Part III). Parizi and Ghate <ref type="bibr" target="#b115">[116]</ref> recently proposed a similar framework for weakly coupled POMDP problem where the decision maker maximizes his expected total discounted reward over infinite horizon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Integer programming for POMDPs</head><p>We have seen in Chapter 3 that the POMDP problem with memoryless policies (P ml ), which we recall below, is suitable to formalize the predictive maintenance problem with capacity constraints. max</p><formula xml:id="formula_43">δ∈∆ ml E δ T t =1 r (S t , A t , S t +1 ) (P ml )</formula><p>Since (P ml ) is NP-hard <ref type="bibr" target="#b86">[87]</ref>, it leaves the question of how to solve it. To the best of our knowledge, there is no integer programming approach in the literature addressing the POMDP problem with memoryless policies. In this chapter, we introduce an mixed-integer linear program (MILP) that models (P ml ). In addition, we introduce some valid inequalities improving the relaxation of the MILP. These valid inequalities come from a probabilistic interpretation of the dependence between the random variables in the POMDP. In Part II, we will show that we can extend these valid inequalities to any stochastic optimization problem where we have access to the probabilistic dependences between the random variables of the problem. The numerical experiments show that the MILP together with the valid inequalities can be solved using off-the-shell solvers. We also show in this chapter that the linear relaxation of our integer formulation corresponds to the so-called MDP approximation <ref type="bibr" target="#b53">[54]</ref>, i.e., the fully observed MDP relaxation of the POMDP. Then, we use this result to give guarantees about the cost of using a memoryless policy instead of a history-dependent policy. Chapter 4 is organized as follows.</p><p>• Section 4.1 introduces an MILP that exactly models (P ml ).</p><p>• Section 4.2 introduces an extended formulation with new valid inequalities that improve the resolution of the MILP. • Section 4.3 shows how our MILP is related to POMDP with history-dependent policies (P his ).</p><p>In particular it gives an interpretation in terms of information relaxation. • Section 4.4 introduces an MILP that also models (P ml ). • Section 4.5 presents numerical results, showing the efficiency of the approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Integer program for POMDPs with memoryless policies</head><p>We are given a POMDP (X S , X O , X A , p,r) and a finite horizon T ∈ Z + . We denote by v * ml the optimal value of (P ml ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">An exact Nonlinear Program (NLP)</head><p>We introduce the following NLP with a collection of variables</p><formula xml:id="formula_44">µ = (µ 1 s ) s , (µ t soa ) s,o,a , (µ t sas ) s,a,s t , δ = δ t a|o a,o t . max µ,δ T t =1 s,s ∈X S a∈X A r (s, a, s )µ t sas s.t. o∈X O µ t soa = s ∈X S µ t sas ∀s ∈ X S , a ∈ X A , t ∈ [T ] s∈X S ,a∈X A µ t sas = o∈X O ,a∈X A µ t +1 s oa ∀s ∈ X S , t ∈ [T ] µ 1 s = o∈X O ,a∈X A µ 1 soa ∀s ∈ X S µ 1 s = p(s) ∀s ∈ X S µ t sas = p(s |s, a) s ∈X S µ t sas ∀s, s ∈ X S , a ∈ X A , t ∈ [T ] µ t soa = δ t a|o p(o|s) o ∈X O ,a ∈X A µ t so a ∀s ∈ X S , o ∈ X O , a ∈ X A , t ∈ [T ] δ ∈ ∆ ml (4.1a) (4.1b) (4.1c) (4.1d) (4.1e) (4.1f) (4.1g)<label>(4.1h)</label></formula><p>Given a policy δ ∈ ∆ ml , we say that µ is the vector of moments of the probability distribution P δ induced by δ when</p><formula xml:id="formula_45">µ 1 s = P δ (S 1 = s), ∀s ∈ X S µ t soa = P δ (S t = s,O t = o, A t = a), ∀s ∈ X S , o ∈ X O , a ∈ X A , ∀t ∈ [T ] µ t sas = P δ (S t = s, A t = a, S t +1 = s ), ∀s, s ∈ X S , a ∈ X A , ∀t ∈ [T ]. (4.2)</formula><p>Thanks to the properties of probability distributions, such vector of moments (4.2) of P δ satisfy the constraints of NLP (4.1). Conversely, given a feasible solution of NLP (4.1), Theorem 4.1 ensures that µ is the vector of moments of P δ . This theorem tells even more. We denote by z * the optimal value of NLP (4.1).</p><p>Theorem 4.1. Let (µ, δ) be a feasible solution of NLP (4.1). Then µ is the vector of moments of the probability distribution P δ induced by δ, and (µ, δ) is an optimal solution of NLP (4.1) if and only if δ is an optimal policy of (P ml ). In particular, v * ml = z * .</p><p>Proof. Let (µ, δ) be a feasible solution of NLP (4.1). We prove by induction on t that µ 1</p><formula xml:id="formula_46">s = P δ S 1 = s , µ t soa = P δ S t = s,O t = o, A t = a and µ t sas = P δ S t = s, A t = a, S t +1 = s . At time t = 1,</formula><p>the statement is immediate. Suppose that it holds up to t -1. Constraints (4.1g), (4.1c) and induction hypothesis ensure that</p><formula xml:id="formula_47">µ t soa = δ t a|o p(o|s) o ,a µ t so a = δ t a|o p(o|s) s ,a µ t -1 s a s = δ t a|o p(o|s) s ,a P δ S t -1 = s , A t -1 = a , S t = s = δ t a|o p(o|s)P δ (S t = s) = P δ (S t = s,O t = o, A t = a),</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Integer program for POMDPs with memoryless policies</head><p>where the last equality comes from the conditional independences and the law of total probability. Constraints (4.1b),(4.1f) and the induction hypothesis ensure that :</p><formula xml:id="formula_48">µ t sas = p(s |s, a) s µ t sas = p(s |s, a) o µ t soa = p(s |s, a) o P δ (S t = s,O t = o, A t = a) = P δ (S t = s, A t = a, S t +1 = s )</formula><p>where the last equality comes from the conditional independences and the law of total probability. Consequently,</p><formula xml:id="formula_49">T t =1 s,s ∈X S a∈X A r (s, a, s )P δ S t = s, A t = a, S t +1 = s = E δ T t =1 r (S t , A t , S t +1 ) ,</formula><p>which implies that δ is optimal if and only if (µ, δ) is optimal for NLP (4.1) and v * ml = z * . It achieves the proof. </p><formula xml:id="formula_50">µ 1 s = p(s|o), ∀s ∈ X S , µ 1 soa = δ 1 a|o 1 o (o) o ∈X O ,a ∈X A µ 1 so a , ∀s ∈ X S , o ∈ X O , a ∈ X A .<label>(4.3)</label></formula><p>This remark will be useful in Chapter 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Turning the NLP into an MILP</head><p>We define the set of deterministic memoryless policies ∆ d ml as</p><formula xml:id="formula_51">∆ d ml = δ ∈ ∆ ml : δ t a|o ∈ {0, 1}, ∀o ∈ X O , ∀a ∈ X A , ∀t ∈ [T ] .<label>(4.4)</label></formula><p>The following proposition states that we can restrict our policy search in (P ml ) to the set of deterministic memoryless policies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposition 4.2. [9, Proposition 1]</head><p>There always exists an optimal policy for (P ml ) that is deterministic, i.e., max</p><formula xml:id="formula_52">δ∈∆ ml E δ T t =1 r (S t , A t , S t +1 ) = max δ∈∆ d ml E δ T t =1 r (S t , A t , S t +1 ) . (4.5)</formula><p>Theorem 4.1 ensures that (P ml ) and NLP (4.1) are equivalent, and in particular admit the same optimal solution in terms of δ. However, NLP (4.1) is hard to solve due to the nonlinear constraints (4.1g). By Proposition 4.5, we can add the integrality constraints of ∆ d ml in (4.1), and, by a classical result in integer programming, we can turn NLP (4.1) into an equivalent MILP by replacing constraint (4.1g) by its McCormick relaxation <ref type="bibr" target="#b99">[100]</ref>, without changing its optimal value. The McCormick's inequalities for Equation (4.1g) are </p><formula xml:id="formula_53">µ t soa p(o|s) o ∈X O ,a ∈X A µ t so a ∀s ∈ X S , o ∈ X O , a ∈ X A , t ∈ [T ] µ t soa δ t a|o ∀s ∈ X S , o ∈ X O , a ∈ X A , t ∈ [T ] µ t soa p(o|s) o ∈X O ,a ∈X A µ t so a + δ t a|o -1 ∀s ∈ X S , o ∈ X O , a ∈ X A , t ∈ [T ]. (<label>4</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Valid cuts</head><p>To introduce our valid cuts in this section, we start by explaining why the linear relaxation of MILP (4.7) is not sufficient to define a feasible solution of NLP (4.1).</p><p>It turns out that given a feasible solution (µ, δ) of the linear relaxation of MILP (4.7), the vector µ is not necessarily the vector of moments of the probability distribution P δ induced by δ. Indeed, since the coordinates of the vector δ are continuous variables, the McCormick's constraints (4.6) are, in general, no longer equivalent to the bilinear constraints (4.1g). Then, (µ, δ) is no longer a feasible solution of NLP (4.1), which implies that µ is not the vector of moments of the probability distribution P δ .</p><p>Intuitively, it means that the feasible set of the linear relaxation of MILP (4.7) is too large. Actually, we can reduce the feasible set of the linear relaxation of MILP (4.7) by adding valid cuts. To do so, we introduce new variables (µ t s a soa ) s ,a ,s,o,a t and the equalities</p><formula xml:id="formula_54">s ∈X S ,a ∈X A µ t s a soa = µ t soa , ∀s ∈ X S , o ∈ X O , a ∈ X A , a∈X A µ t s a soa = p(o|s)µ t -1 s a s , ∀s , s ∈ X S , o ∈ X O , a ∈ X A , µ t s a soa = p(s|s , a , o) s∈X S µ t s a soa ,∀s , s ∈ X S , o ∈ X O , a , a ∈ X A , (4.8a) (4.8b) (4.8c) where p(s|s , a , o) = P(S t = s|S t -1 = s , A t -1 = a ,O t = o),</formula><p>for any s, s ∈ X S , a ∈ X A and o ∈ X O . Note that p(s|s , a , o ) does not depend on the policy δ and can be easily computed during a preprocessing using Bayes rules. Therefore, constraints The MILP formulation obtained by adding equalities (4.8) in MILP (4.7) is an extended formulation, and has much more constraints than the initial MILP (4.7). Its linear relaxation therefore takes longer to solve. Equalities (4.8) strengthen the linear relaxation, and numerical experiments in Section 4.5 show that these equalities enable to speed up the resolution of MILP (4.7).</p><p>Proof of Proposition 4.3. Let (µ, δ) be a feasible solution of MILP (4.7). We define</p><formula xml:id="formula_55">µ t s a soa = δ t a|o p(o|s)µ t -1 s a s for all (s , a , s, o, a) ∈ X S × X A × X S × X O × X A , t ∈ [T ]</formula><p>. These new variables satisfy constraints in (4.8) :</p><formula xml:id="formula_56">a∈X A µ t s a soa = a∈X A δ t a|o p(o|s)µ t -1 s a s = p(o|s)µ t -1 s a s a ∈X A ,s ∈X S µ t s a soa = a ∈X A ,s ∈X S µ t -1 s a s δ t a|o p(o|s) = δ t a|o p(o|s) o ∈X O ,a ∈X O µ t so a = µ t soa</formula><p>The remaining constraint (4.8c) is obtained using the following observation : Now we prove that there exists a solution µ of the linear relaxation of MILP (4.7) that does not satisfy equalities <ref type="bibr">(4.8)</ref>. We define such a solution (µ, δ). We set µ 1 s = p(s) for all s in X S , and for all t in [T ]:</p><formula xml:id="formula_57">µ 1 soa = p(o|s)µ 1 s , if a = φ(s) 0, otherwise , if t = 1, µ t sas = p(s |s, a) o∈X O µ t soa µ t soa = p(o|s) s ∈X S ,a ∈X A µ t -1 s a s , if a = φ(s) 0, otherwise , if t 2, δ t a|o =    s∈X S µ t soa s∈X S ,a∈X A µ t soa if s∈X S ,a∈X A µ t soa = 0 1 ã (a), otherwise<label>(4.9)</label></formula><p>(4.10)</p><p>where φ : X S → X A is an arbitrary mapping and ã is an arbitrary element in X A . We prove that µ is a feasible solution of the linear relaxation of MILP (4.7).</p><p>First, it is easy to see constraints ( </p><formula xml:id="formula_58">) s ∈X S ,a ∈X A µ t -1 s a s δ t a|o ,</formula><p>where we used definition (4.10) from the first to second line. Third, (4.6c) holds because</p><formula xml:id="formula_59">µ t soa -p(o|s) s ∈X S a ∈X A µ t -1 s a s s ∈X S 0 µ t s oa -p(o|s ) s ∈X S ,a ∈X A µ t -1 s a s = s ,s ∈X S ,a ∈X A p(o|s )µ t -1 s a s (δ t a|o -1) δ t a|o -1,</formula><p>which yields (4.6c). Therefore, (µ, δ) is a solution of the linear relaxation of MILP (4.7). Now, we prove that such a solution does not satisfy cuts (4.8). We define the new variables:</p><formula xml:id="formula_60">µ t s a soa =    µ t -1 s a s µ t soa o ∈X O ,a ∈X A µ t so a if o ∈X O ,a ∈X A µ t so a = 0 0 otherwise</formula><p>Hence, µ satisfies constraints (4.8a) and (4.8b). However, constraint (4.8c) is not satisfied in general. Indeed, since the mapping φ is arbitrary, we can set φ such that p(s|s , a , o) &gt; 0 and µ t s a soa = 0. Therefore, there exists a solution µ of the linear relaxation of MILP (4.7) that does not satisfy cuts (4.8). It achieves the proof.</p><p>Probabilistic interpretation. Given a feasible solution (µ, δ) of the linear relaxation of (4.7), µ can be interpreted as the vector of moments of a probability distribution Q µ over (X S × X O × X A ) T × X S . However, as it has been mentioned above, the vector µ does not necessarily correspond to the vector of moments of P δ , which is due to the fact that (µ, δ) does not necessarily satisfy the nonlinear constraints (4.1g). Besides, constraints (4.1g) happen to be equivalent to the property that, according to Q µ , action A t is independent from state S t given observation O t .</p><p>(4.11)</p><p>Hence, given a feasible solution (µ, δ) of the linear relaxation of MILP (4.7), the distribution Q µ does not necessarily satisfy the conditional independences <ref type="bibr">(4.11)</ref>. Remark that (4.11) implies</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Strengths of the relaxations</head><p>the weaker result that, according to Q µ , A t is independent from S t given O t , A t -1 and S t -1 . (4.12) Proposition 4.3 says that the independences in (4.12) are not satisfied in general by a feasible solution (µ, δ) of the linear relaxation of MILP (4.7), but that we can enforce them using linear equalities (4.8) on (µ, δ) in an extended formulation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Strengths of the relaxations</head><p>It turns out that the linear relaxation of MILP (4.7) is related to the fully observed POMDP, which corresponds to the case when the decision maker directly observes the state of the system at each time. Hence, the problem becomes a MDP and it is called the MDP approximation <ref type="bibr" target="#b53">[54]</ref>.</p><p>We introduce a collection of variables µ = (µ 1 s ) s , (µ t sas ) s,a,s ) t and the following linear program which is known to solve exactly a MDP (e.g. d'Epenoux <ref type="bibr" target="#b37">[38]</ref>).</p><formula xml:id="formula_61">max µ T t =1 s,s ∈X S a∈X A r (s, a, s )µ t sas s.t. µ 1 s = a ∈X A ,s ∈X S µ 1 sa s ∀s ∈ X S s ∈X S ,a ∈X A µ t s a s = a ∈X A ,s ∈X S µ t +1 sa s ∀s ∈ X S , t ∈ [T ] µ 1 s = p(s) ∀s ∈ X S µ t sas = p(s |s, a) s ∈X S µ t sas ∀s ∈ X S , a ∈ X A , t ∈ [T ] (4.13a) (4.13b) (4.13c) (4.13d) (4.13e)</formula><p>In Linear program (4.13), the variables (µ 1 s ) s and (µ t sas ) sas respectively represent the probability distribution of S 1 and (S t , A t , S t +1 ) for any t in [T ]. Theorem 4.4 below states that the linear relaxation of MILP (4.7) is equivalent to the MDP approximation of (P ml ). We introduce the following quantities:</p><p>z * R : the optimal value of the linear relaxation of MILP (4.7). z * R c : the optimal value of the linear relaxation of MILP (4.7) with inequalities (4.8). v * his : the optimal value of (P his ). v * MDP : the optimal value of linear program (4.13), which is the optimal value of the MDP approximation.</p><p>Theorem 4.4. Let (µ, δ) be feasible solution of the linear relaxation of MILP (4.7). Then (µ, δ) is an optimal solution of the linear relaxation of MILP (4.7) if and only if µ is an optimal solution of linear program (4.13). In particular, z * R = v * MDP . In addition, the following inequalities hold:</p><formula xml:id="formula_62">z * v * his z * R c z * R . (4.14)</formula><p>Inequality (4.14) ensures that by solving MILP (4.7), we obtain an integrality gap z * Rz * that bounds the approximation error v * hisz * due to the choice of a memoryless policy instead of a policy that depends on all history of observations and actions. In addition, Theorem 4.4 ensures that the integrality gap z * R cz * obtained using valid inequalities (4.8) gives a tighter bound on the approximation error.</p><p>Proof of Theorem 4.4. We first prove the equivalence between the linear relaxation of our MILP (4.7) and its MDP approximation. Note that in both problem, the objective function are the same. Hence, we only need to prove that we can construct a feasible solution from a problem to another.</p><p>Let (µ, δ) be a feasible solution of the linear relaxation of MILP ( <ref type="formula" target="#formula_44">4</ref> </p><formula xml:id="formula_63">z * z * R c z * R .</formula><p>It remains to prove the two following inequalities. ). The proof is based on a probabilistic interpretation of the valid inequalities <ref type="bibr">(4.8)</ref>. It suffices to proves that for any policy δ in ∆ his , the probability distribution P δ satisfies the weak conditional independences (4.12). Let δ ∈ ∆ his . The probability distribution P δ over the random variables (S t , A t ,O t ) 1 t T according to δ is exactly</p><formula xml:id="formula_64">z * v * his v * his z * R c<label>(4.15)</label></formula><formula xml:id="formula_65">P δ ((S t = s t ,O t = o t , A t = a t ) 1 t T ) = P δ (S 1 = s 1 ) T t =1 P δ (S t +1 = s t +1 |S t = s t , A t = a t ) P δ (O t = o t |S t = s t )δ t a t |h t (4.17)</formula><p>where</p><formula xml:id="formula_66">h t = {O 1 = o 1 , A 1 = a 1 ,O 2 = o 2 , . . . ,O t = o t }</formula><p>is the history of observations and actions. Note that the policy at time t is the conditional probability δ t a t |h t = P δ (A t = a t |H t = h t ). We define:</p><formula xml:id="formula_67">µ 1 s = P δ (S 1 = s) µ t soa = P δ (S t = s,O t = o, A t = a) µ t sas = P δ (S t = s, A t = a, S t +1 = s ) µ t s a soa = P δ (S t -1 = s , A t -1 = a , S t = s,O t = o, A t = a)</formula><p>We define the policy δ using (4.10). It is easy to see that constraints of (4.7) are satisfied. Fur-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Strengths of the relaxations</head><p>thermore, we have δ ∈ ∆ ml . It remains to prove that equalities (4.8) are satisfied. By definition of a probability distribution, we directly see that constraints (4.8a) are satisfied. We prove (4.8b) and (4.8c). We compute the left-hand side of (4.8b):</p><formula xml:id="formula_68">a∈X A µ t s a soa = a∈X A P δ (S t -1 = s , A t -1 = a , S t = s,O t = o, A t = a) = a∈X A s 1 ,...,s t -2 h t -1 P δ ((S i = s i ,O i = o i , A i = a i ) 1 i t -2 , S t -1 = s ,O t -1 = o , A t -1 = a , S t = s,O t = o, A t = a) = p(o|s)p(s|s , a ) s 1 ,...,s t -2 h t -1 P δ ((S i = s i ,O i = o i , A i = a i ) 1 i t -2 , S t -1 = s ,O t -1 = o t -1 , A t -1 = a ) a∈X A δ a|h t = p(o|s)p(s|s , a ) s 1 ,...,s t -2 h t -1 P δ ((S i = s i ,O i = o i , A i = a i ) 1 i t -2 , S t -1 = s ,O t -1 = o , A t -1 = a ) = p(o|s)p(s|s , a )P δ (S t -1 = s , A t -1 = a ) = p(o|s)µ t -1 s a s</formula><p>where we used the definition of the probability distribution (4.17) at the third equation. Therefore, constraints (4.8b) are satisfied by µ. To prove that constraints (4.8c) are satisfied, we prove that</p><formula xml:id="formula_69">P δ (S t = s t |S t -1 = s t -1 , A t -1 = a t -1 ,O t = o t , A t = a t ) = P δ (S t = s t |S t -1 = s t -1 , A t -1 = a t -1 ,O t = o t ) We compute P δ (S t = s t |S t -1 = s , A t -1 = a ,O t = o, A t = a): P δ (S t = s t |S t -1 = s t -1 , A t -1 = a t -1 ,O t = o t , A t = a t ) = P δ (S t -1 = s t -1 , A t -1 = a t -1 , S t = s t ,O t = o t , A t = a t ) P δ (S t -1 = s t -1 , A t -1 = a t -1 ,O t = o t , A t = a t ) = s 1 ,...,s t -2 h t -1 P δ ((S i = s i ,O i = o i , A i = a i ) 1 i t ) s 1 ,...,s t -2 ,s t h t -1 P δ ((S i = s i ,O i = o i , A i = a i ) 1 i t ) = s 1 ,...,s t -2 h t -1 δ t a t |h t p(o t |s t )p(s t |s t -1 , a t -1 )P δ ((S i = s i ,O i = o i , A i = a i ) 1 i t -1 ) s 1 ,...,s t -2 ,s t h t -1 δ t a t |h t p(o t |s t )p(s t |s t -1 , a t -1 )P δ ((S i = s i ,O i = o i , A i = a i ) 1 i t -1 ) = p(o t |s t )p(s t |s t -1 , a t -1 ) s 1 ,...,s t -2 h t -1 δ t a t |h t P δ ((S i = s i ,O i = o i , A i = a i ) 1 i t -1 ) s t p(o t |s t )p(s t |s t -1 , a t -1 ) s 1 ,...,s t -2 h t -1 δ t a t |h t P δ ((S i = s i ,O i = o i , A i = a i ) 1 i t -1 ) = p(o t |s t )p(s t |s t -1 , a t -1 ) s t p(o t |s t )p(s t |s t -1 , a t -1 )</formula><p>where the last line goes from the fact that the term δ t a t |h t</p><formula xml:id="formula_70">P δ ((S i = s i ,O i = o i , A i = a i ) 1 i t -1 )</formula><p>does not depend on s t . Hence, constraints (4.8c) are satisfied by µ. We deduce that µ is a feasible solution of MILP (4.7) satisfying the valid inequalities (4.8). Therefore,</p><formula xml:id="formula_71">E δ T t =1 r (S t , A t , S t +1 ) = T t =1 s,a,s P δ (S t = s, A t = a, S t +1 = s )r (s, a, s ) z * R c</formula><p>By maximizing over δ the left-hand side, we obtain v * his z * R c . It achieves the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Value functions for POMDPs with memoryless policies</head><p>The aim of this section is to introduce an MILP that models (P ml ) using a new type of variables.</p><p>The formulation obtained is related to the linear programming duality and the link with the linear programming will be describe in Part II.</p><p>Like in Section 4.1, we introduce a NLP, then we get an MILP using McCormick inequalities and finally valid inequalities. In this section, some of the proofs will be written from a probabilistic point of view. We could have used this kind of proofs in Section 4.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">An exact NLP</head><p>We introduce a collection of variables λ = (λ s ) s , (λ t soa ) s,o,a , (λ t sas ) s,a,s t , δ = δ t a|o o,a t and the following NLP.</p><formula xml:id="formula_72">max λ,δ s∈X S p(s)λ s s.t. λ s = o∈X O ,a∈X A p(o|s)δ 1 a|o λ 1 soa ∀s ∈ X S λ t soa = s ∈X S p(s |s, a)λ t sas ∀s ∈ X S , o ∈ X O , a ∈ X A , t ∈ [T ] λ t sas = r (s, a, s ) + o ∈X O ,a ∈X A p(o |s )δ t +1 a |o λ t +1 s o a ∀s, s ∈ X S , a ∈ X A , t ∈ [T -1] λ T sas = r (s, a, s ) ∀s, s ∈ X S , a ∈ X A δ ∈ ∆ ml . (4.18a) (4.18b) (4.18c) (4.18d) (4.18e) (4.18f)</formula><p>Given δ ∈ ∆ ml , we say that λ is the vector of value functions of the probability distribution P δ induced by δ when</p><formula xml:id="formula_73">λ s = E δ T t =1 r (S t , A t , S t +1 )|S 1 = s , ∀s ∈ X S λ t soa = E δ T t =t r (S t , A t , S t +1 )|S t = s,O t = o, A t = a , ∀s ∈ X S , o ∈ X O , a ∈ X A , t ∈ [T ] λ t sas = E δ T t =t r (S t , A t , S t +1 )|S t = s, A t = a, S t +1 = s , ∀s, s ∈ X S , a ∈ X A , t ∈ [T ]. (4.19a) (4.19b) (4.19c)</formula><p>Thanks to the properties of the conditional expectation, the vector of value functions of P δ satisfies the constraints of NLP (4.18). Conversely, given a feasible solution (λ, δ) of NLP (4.18), Theorem 8.17 below ensures that λ is the vector of value functions of P δ . This theorem tells even more. We denote by z * vf the optimal value of NLP (4.18).</p><p>Theorem 4.5. Let (λ, δ) be a feasible solution of NLP <ref type="bibr">(4.18)</ref>. Then λ is the vector of value functions (4.19) of the distribution P δ , and (λ, δ) is an optimal solution of NLP (4.18) if and only if δ is an optimal policy of (P ml ). In particular, v * ml = z * vf .</p><p>Proof. Let (λ, δ) be a feasible solution of NLP (4.18). Then δ is a feasible solution of (P ml ).</p><p>We now prove that λ satisfies (4. <ref type="bibr" target="#b18">19</ref>), from which we can deduce that E δ T t =1 r (S t , A t , S t +1 ) = s∈X S p(s)λ s . We prove the result using a backward induction from t = T until t = 1, where the induction hypothesis at time t is that λ satisfies (4.19) for t in t , T . At time t = T , the induction hypothesis is true.</p><p>We assume that the induction hypothesis is true until t + 1 T . At time t , constraints (4.18e) ensure that</p><formula xml:id="formula_74">λ t sas = r (s, a, s ) + o ∈X O ,a ∈X A p(o |s )δ t +1 a |o λ t +1 s o a = r (s, a, s ) + o ∈X O ,a ∈X A p(o |s )δ t +1 a |o E δ t =t +1 r (S t , A t , S t +1 )|S t +1 = s ,O t +1 = o , A t +1 = a = r (s, a, s ) + E δ t =t +1 r (S t , A t , S t +1 )|S t +1 = s = r (s, a, s ) + E δ t =t +1 r (S t , A t , S t +1 )|S t = s, A t = a, S t +1 = s = E δ t =t r (S t , A t , S t +1 )|S t = s, A t = a, S t +1 = s</formula><p>for all s, s ∈ X S and a ∈ X A . The fourth equality comes from the fact that all random variables (S t ,O t , A t ) t t +1 are conditionally independent from (S t , A t ) given S t +1 . It proves that λ satisfies <ref type="bibr">(4.19c)</ref>. Constraints (4.18c) ensure that </p><formula xml:id="formula_75">λ t soa = s ∈X S p(s |s, a)λ t sas = s ∈X S p(s |s, a)E δ t =t r (S t , A t , S t +1 )|S t = s, A t = a, S t +1 = s = E δ t =t r (S t , A t , S t +1 )|S t = s, A t = a = E δ t =t r (S t , A t , S t +1 )|S t = s,O t = o, A t = a for all s ∈ X S , o ∈ X O ,</formula><formula xml:id="formula_76">λ s = o∈X O ,a∈X A p(o|s)δ 1 a|o E δ T t =1 r (S t , A t , S t +1 )|S t = s,O t = o = E δ T t =1 r (S t , A t , S t +1 )|S t = s .</formula><p>It proves that λ satisfies (4.19a). Hence, λ is the vector of value functions of the distribution P δ induced by δ.</p><p>Therefore, δ is optimal if and only if (λ, δ) is optimal for NLP (4.18) and v * ml = z * vf .</p><p>Remark 5. In fact, we can reduce the number of variables of NLP (4.18). Indeed, Theorem 8.17 ensures that if (λ, δ) is a feasible solution of NLP (4.18), then λ satisfies </p><formula xml:id="formula_77">λ t soa = E δ T t =t r (S t , A t , S t +1 )|S t = s,O t = o, A t = a for every s ∈ X S , o ∈ X O , a ∈ X A and t ∈ [T ].</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Turning the NLP into an MILP</head><p>In this section, we assume that we have access to a vector b lb and a vector b ub such that for every policy</p><formula xml:id="formula_78">δ in ∆ ml , b lb λ b ub ,</formula><p>where λ is the vector of value functions of P δ . In Section 4.4.3, we explain how we compute b lb and b ub . These bounds play a key role because they drive the quality of the linear relaxation of MILP (4.21).</p><p>Theorem 8.17 ensures that NLP (4.18) exactly models (P ml ), and in particular both problems have the same optimal solution in terms of δ. However, NLP (4.18) is hard to solve due to the nonlinear constraints (4.18d). By Proposition 4.5, we can add the integrality constraints of ∆ d ml in (4.18), and, by a classical result in integer programming, we can turn NLP (4.18) into an equivalent MILP by replacing the nonlinear terms in constraint (4.18d) by its McCormick relaxation, without changing its optimal value. To do so, we introduce variables α = (α t soa ) s,o,a t and the following inequalities, which are the McCormick inequalities for constraints (4.18d).</p><formula xml:id="formula_79">α t soa b ub,t soa δ t a|o ∀s ∈ X S , o ∈ X O , a ∈ X A , t ∈ [T ] α t soa λ t soa ∀s ∈ X S , o ∈ X O , a ∈ X A , t ∈ [T ] α t soa λ t soa + b ub,t soa (δ t a|o -1) ∀s ∈ X S , o ∈ X O , a ∈ X A , t ∈ [T ] α t soa b lb,t soa δ t a|o ∀s ∈ X S , o ∈ X O , a ∈ X A , t ∈ [T ] (4.20a) (4.20b) (4.20c) (4.20d)</formula><p>For convenience, we denote by McCormick α, λ, δ the McCormick linear inequalities (4.20). Thus, we get that (P ml ) is equivalent to the following MILP:</p><formula xml:id="formula_80">max λ,α,δ s∈X S p(s)λ s s.t. λ s = o∈X O ,a∈X A p(o|s)α 1 soa ∀s ∈ X S λ t sas = r (s, a, s ) + o ∈X O ,a ∈X A p(o |s)α t +1 so a ∀s ∈ X S , t ∈ [T -1] λ t soa = s ∈X S p(s |s, a)λ t sas ∀s ∈ X S , o ∈ X O , a ∈ X A , t ∈ [T ] McCormick α, λ, δ δ ∈ ∆ d ml (4.21)</formula><p>Remark 6. Let δ be a feasible solution of (P ml ). Then, Theorem 4.1 and Theorem 8.17 respectively ensure that there exists a unique vector of moments µ and a unique vector of value functions λ of the probability distribution P δ . Furthermore, µ and λ are linked by the following relation.</p><formula xml:id="formula_81">s,s ∈X S ,a∈X A λ t sas µ t sas = E δ T t =t r (S t , A t , S t +1 ) , ∀t ∈ [T ].</formula><p>In Part II, we give another interpretation of the vector of value functions. It turns out that in some specific cases like MDPs, the value functions represent the variables of some related dual linear programs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3">Computing bounds on the value functions</head><p>The aim of this section is to explain how we compute the vector of lower bounds b lb and the vector of upper bounds b ub of the vector of value functions λ. We define b lb as follows.</p><formula xml:id="formula_82">                   b lb,T sas = r (s, a, s ), ∀s, s ∈ X S , a ∈ X A b lb,t sas = r (s, a, s ) + o ∈X O p(o |s ) min a ∈X A b lb,t+1 s o a , ∀s, s ∈ X S , a ∈ X A , t ∈ [T -1] b lb,t soa = s p(s |s, a)b lb,t s,a,s , ∀s ∈ X S , o ∈ X O , a ∈ X A , t ∈ [T ] b lb s = o∈X O p(o|s) min a ∈X A b lb,1 s,o,a , ∀s ∈ X S (4.22)</formula><p>And we define b ub as follows. Proof. We prove the result for the upper bounds. It suffices to replace the max operator by min operator for the lower bounds. Let (λ, δ) be a feasible solution of MILP (4.21). We prove the result by induction on t . The result is true at t = T. Suppose that the induction hypothesis holds until t . By definition of the vector of value functions, we have:</p><formula xml:id="formula_83">                   b ub,T sas = r (s, a, s ), ∀s, s ∈ X S , a ∈ X A b ub,t sas = r (s, a, s ) + o ∈X O p(o |s ) max a ∈X A b ub,t+1 s o a , ∀s, s ∈ X S , a ∈ X A , t ∈ [T -1] b ub,t soa = s ∈X S p(s |s, a)b ub,t s,a,s , ∀s ∈ X S , o ∈ X O , a ∈ X A , t ∈ [T ] b ub s = o∈X O p(o|s) max a ∈X A b ub,1 s,o,a , ∀s ∈ X S</formula><formula xml:id="formula_84">λ t sas = r (s, a, s ) + o ,a p(o |s )δ t a |o λ t +1 s o a r (s, a, s ) + o ,a p(o |s )δ t a |o b ub,t+1 s o a r (s, a, s ) + o p(o |s ) max a ∈X A b ub,t+1 s o a = b ub,t sas ,</formula><p>where the first inequality comes from the induction hypothesis and the second inequality comes from the fact that the δ is deterministic. In addition, we have:</p><formula xml:id="formula_85">λ t soa = s p(s |s, a)λ t sas s p(s |s, a)b ub,t sas = b ub,t soa</formula><p>It achieves the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.4">Strengthening the linear relaxation</head><p>In this section, we introduce an MILP that uses the conditional probabilities We introduce this formulation in three steps to obtain another formulation that uses these independences. First, we introduce another NLP that gives an optimal strategy of (P ml ). Second, we turn this NLP into an MILP using the McCormick's inequalities. Third, we compute a vector of lower bounds b lb,c and upper bounds b ub,c of any feasible solution and we prove that these bounds are respectively greater than b lb and smaller than b ub .</p><formula xml:id="formula_86">P(S t |S t -1 , A t -1 ,O t ),</formula><p>A nonlinear formulation. We introduce the variables (λ t soa ) and the following NLP:</p><formula xml:id="formula_87">max λ,δ s∈X S p(s)λ s s.t. λ s = o∈X O ,a∈X A p(o|s)δ 1 a|o λ 1 soa ∀s ∈ X S λ t soa = s ∈X S p(s |s, a)λ t sas ∀s ∈ X S , o ∈ X O , a ∈ X A , t ∈ [T ] λ t sas = r (s, a, s ) + s ∈X S ,o ∈X O , a ∈X A p(o |s )p(s |s, a, o )δ t +1 a |o λ t +1 s o a ∀s, s ∈ X S , a ∈ X A , t ∈ [T -1] λ T sas = r (s, a, s ) ∀s, s ∈ X S , a ∈ X A δ ∈ ∆ ml . (4.24a) (4.24b) (4.24c) (4.24d) (4.24e) (4.24f)</formula><p>Note that there always exists a feasible solution of NLP <ref type="bibr">(4.24)</ref>. The constraints of NLP (4.24) are similar to the ones of NLP (4.18) except that Constraints (4.24d) differ from Constraints (4.18d).</p><p>Indeed, Constraints (4.24d) can be written explicitly:</p><formula xml:id="formula_88">λ t sas = r (s, a, s )+ o ∈X O ,a ∈X A δ t +1 a |o P δ (O t +1 = o |S t +1 = s ) s ∈X S P δ (S t +1 = s |S t = s, A t = a,O t +1 = o )λ t +1 s o a</formula><p>We replaced λ t +1 s o a by the expected value</p><formula xml:id="formula_89">s ∈X S P δ (S t +1 = s |S t = s, A t = a,O t +1 = o )λ t +1 s o a . It</formula><p>turns out a feasible solution of NLP (4.24) is not necessarily a vector of value functions. Fortunately, Proposition 4.7 below ensures that despite the loss of the value function property of Theorem 8.17, any feasible policy δ gives the same objective value for (P ml ) and NLP (4.18).</p><p>Proposition 4.7. Let (λ, δ) be a feasible solution of NLP <ref type="bibr">(4.24)</ref>. Then λ satisfies</p><formula xml:id="formula_90">E δ T t =t r (S t , A t , S t +1 ) = s,s ∈X S a∈X A P δ (S t = s, A t = a, S t +1 = s )λ t sas E δ T t =t r (S t , A t , S t +1 ) = s∈X S ,o∈X O a∈X A P δ (S t = s,O t = o, A t = a)λ t soa .</formula><p>In particular, s∈X S p(s)λ</p><formula xml:id="formula_91">1 s = E δ T t =1 r (S t , A t , S t +1 ) .</formula><p>Proof. Let (λ, δ) be a feasible solution of NLP (4.24). We prove the result by a backward induction on t . It is immediate for t = T. Suppose that the result holds until t + 1. We have </p><formula xml:id="formula_92">δ t +1 a |o λ t +1 s o a = s,a,s P δ (S t = s, A t = a, S t +1 = s )r (s, a, s ) + s,s ,o ,a,a P δ (S t = s, A t = a)p(s |s, a)p(o |s )δ t +1 a |o s p(s |s, a, o ) =1 λ t +1 s o a = s,a,s P δ (S t = s, A t = a, S t +1 = s )r (s, a, s ) + s,s ,o a,a P δ (S t = s, A t = a, S t +1 = s ,O t +1 = o , A t +1 = a )λ t +1 s o a = s,a,s P δ (S t = s, A t = a, S t +1 = s )r (s, a, s ) + s ,o ,a P δ (S t +1 = s ,O t +1 = o , A t +1 = a )λ t +1 s o a =E δ T t =t +1 r (S t ,A t ,S t +1 ) = E δ T t =t r (S t , A t , S t +1 ) It follows that E δ T t =t r (S t , A t , S t +1 ) = s,o,a P δ (S t = s,O t = o, A t = a)λ t soa .</formula><p>Finally, we have the following computation</p><formula xml:id="formula_93">s λ 1 s p(s) = s,o,a p(s)p(o|s)δ 1 a|o λ 1 soa = E δ T t =1 r (S t , A t , S t +1 ) ,</formula><p>which achieves the proof. Now, we are able to write a theorem for NLP (4.24) that is similar to Theorem 8.17. We denote by z * vf c the optimal value of NLP (4.24).</p><p>Theorem 4.8. Let (λ, δ) be a feasible solution of NLP <ref type="bibr">(4.24)</ref>. Then, (λ, δ) is an optimal solution of NLP (4.24) if and only if δ is an optimal policy of (P ml ). In particular, v</p><formula xml:id="formula_94">* ml = z * vf c .</formula><p>Proof. The proof is immediate from Proposition 4.7.</p><p>Turning NLP (4.24) into an MILP. Again, we can linearize the constraints (4.24d) by introducing the variables α and the McCormick's inequalities <ref type="bibr">(4.20)</ref>. We obtain the following MILP:</p><formula xml:id="formula_95">max λ,α,δ s∈X S p(s)λ s s.t. λ s = o∈X O ,a∈X A p(o|s)α 1 soa ∀s ∈ X S λ t sas = r (s, a, s ) + s ∈X S ,o ∈X O a ∈X A p(o |s)p(s |s, a, o )α t +1 s o a ∀s ∈ X S , t ∈ [T -1] λ t soa = s ∈X S p(s |s, a)λ t sas ∀s ∈ X S , o ∈ X O , a ∈ X A , t ∈ [T ] McCormick α, λ, δ δ ∈ ∆ d ml (4.25a) (4.25b) (4.25c) (4.25d)</formula><p>Computing bounds. We define b lb,c and b ub,c as follows: For any s, s in X</p><formula xml:id="formula_96">S , o in X O and a in X A ,            b lb,c,T sas = r (s, a, s ), b lb,c,t sas = r (s, a, s ) + o ∈X O p(o |s ) min a ∈X A s ∈X S p(s |s, a, o )b lb,c,t +1 s o a , ∀t ∈ [T -1], b lb,c,t soa = s ∈X S p(s |s, a)b lb,c,t sas , ∀t ∈ [T ],</formula><p>and, As we will show in the numerical experiments in Section 4.5, the optimal value of the linear relaxation of MILP (4.25) is always non greater than the optimal value of the linear relaxation of MILP (4.21).</p><formula xml:id="formula_97">           b ub,c,T sas = r (s, a, s ), b ub,c,t sas = r (s, a, s ) + o ∈X O p(o |s ) max a ∈X A s ∈X S p(s |s, a, o )b ub,c,t +1 s o a ∀t ∈ [T -1], b ub,c,t soa = s ∈X S p(s |s, a)b ub,c,t sas ∀t ∈ [T ].</formula><p>Proof of Proposition 4.9. We prove the result for b ub,c . It will hold for b lb,c by reversing the inequality symbol and replacing the max operator by the min operator. Let (λ, α, δ) be a feasible solution of MILP <ref type="bibr">(4.25)</ref>. It suffices to prove the result for the vector (λ t soa ) s,o,a,t because Proposition 4.6 ensures that the other coordinates satisfy the inequality. Again, we prove the result by induction on t . The result holds for t = T. Suppose that the induction hypothesis holds until t .</p><p>It implies that for any s ∈ X S , o ∈ X O and a ∈ X A : </p><formula xml:id="formula_98">λ t soa = s p(</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Numerical experiments</head><p>We now provide experiments showing the practical efficiency of our approaches to POMDPs. In Section 4.5.1, we evaluate on random instances the performances of MILP (4.7) and the efficiency of the valid inequalities (4.8) to help its resolution. In Section 4.5.2, we evaluate the performances of MILP (4.7) on instances from the literature on POMDPs. In particular, we show that memoryless policies perform well on instances from maintenance problem. The instances can be found here. <ref type="foot" target="#foot_6">1</ref> All linear programs have been implemented in Julia with JuMP interface <ref type="bibr" target="#b40">[41]</ref> and solved using Gurobi 9.0 <ref type="bibr" target="#b51">[52]</ref>. Experiments have been run on a server with 192Gb of RAM and 32 cores at 3.30GHz.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.1">Random instances</head><p>The Numerical experiments on random POMDP. We solve MILP (4.7) with and without valid equalities (4.8), MILP (4.7) (basic formulation) and MILP (4.25) (strengthened formulation). Algorithms were stopped after 3600s. Table <ref type="table" target="#tab_26">4</ref>.1 reports the results on the random instances.</p><p>The first four columns indicate the size of state space X S , observation space X O , action space X A and time horizon T . The fifth column indicates the mathematical program used to solve (P ml ), ether basic or strengthened. In the last three columns, we report the integrality gap, the final gap, the percentage of instances solved and the computation time. Note that for each instance of POMDP (X S , X O , X A , p,r), we solve it with horizon T ∈ {10, 20}. For each tuple (X S , X O , X A ), the results in Table <ref type="table" target="#tab_26">4</ref>.1 are averaged over 30 instances (X S , X O , X A , p,r) where the p and r are randomly generated as mentioned above. Table <ref type="table" target="#tab_26">4</ref>.1 shows that the MILP formulation (4.7) that uses the vector of moments is more efficient than MILP formulation (4.21) that uses the vector of value function. However, for almost all instances the integrality gap of MILP (4.21) is lower than the one of MILP (4.7), which indicates that the linear relaxation of MILP (4.21) is in general tighter. The results also show that the valid inequalities introduced for MILP (4.7) and MILP (4.25) are efficient because adding them significantly reduces the integrality gap. In addition, Inequality 4.14 ensures that the integrality gaps of (4.7) reported in Table <ref type="table" target="#tab_26">4</ref>.1 are also bounds of the relative gap between v * ml and v * his .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.2">Numerical experiments on instances from the literature</head><p>In this section, we evaluate the efficiency of MILP (4.7) on instances of POMDP drawn from the literature and we compare its performances with one of the state-of-the-art POMDP solver SARSOP of Kurniawati et al. <ref type="bibr" target="#b79">[80]</ref>. In fact, SARSOP solver gives an approximate history-dependent policy for the discounted infinite horizon POMDP problem (see Remark 3). To adapt this policy for the finite horizon POMDP problem, we proceed as Dujardin et al. <ref type="bibr" target="#b39">[40]</ref>: We compute a policy using SARSOP solver with a discount factor γ = 0.999 and we compute the expected sum of rewards over the T time steps by simulation of the history-dependent policy. We perform 10000 POMDPs.jl of Egorov et al. <ref type="bibr" target="#b42">[43]</ref>.</p><p>Instances. All the instances can be found at the link <ref type="url" target="http://pomdp.org/examples/">http://pomdp.org/examples/</ref> and further descriptions of each instance are available in the indicated literature on the same website.</p><p>In particular, it contains two instances bridge-repair and machine that model maintenance problems. The first one, introduced by Ellis et al. <ref type="bibr" target="#b43">[44]</ref>, consists of the maintenance of a bridge.</p><p>The modeling is almost similar to our one in Chapter 3 except that there are more available actions and they consider only one machine. Instead of just choosing whether or not to maintain the bridge, the decision maker chooses whether or not to inspect the bridge and, if so, whether or not to maintain it. The second one, introduced by Cassandra [23, Appendix H.3], consists of planning the maintenance of a machine with 4 deteriorating components. Again, the decision maker can choose to inspect before performing a maintenance of the machine. In addition, the action "maintenance" is distinguished in two different actions: repair, which consists in maintaining internal components, and replace, which consists in replacing the machine by a new one. It leads to the set of available actions X A = {operate, inspect, repair, replace}.</p><p>Metrics. We give two metrics to evaluate MILP (4.7) against the SARSOP policy. We want to compare the optimal value z * of MILP (4.7) with the value z SARSOP obtained by using the SARSOP policy. In addition, Theorem 4.4 says that z * and z SARSOP are lower bounds of v * his . We also compare these values with z * R c , the optimal value of the linear relaxation of MILP (4.7) with valid inequalities (4.8). By Theorem 4.4, the value of z * R c is an upper bound of z * and v * his , and consequently an upper bound of z SARSOP . It leads to the relative gap g (z)</p><formula xml:id="formula_99">= z R c -z z * R c</formula><p>for any z belonging to {z * , z SARSOP }.</p><p>All the results are reported in Table <ref type="table" target="#tab_26">4</ref>. Numerical results. One may observe that in most cases the optimal value obtained with our MILP is close to the upper bound z R c . Thanks to Theorem 4.4, it says that memoryless policies perform well on finite horizon for these instances. In particular, the gap is noticeably small on the instance of maintenance problem bridge-repair. However, as mentioned in Section 3.5, one can observe that the memoryless policies fail on instances of navigation problems <ref type="bibr" target="#b86">[87]</ref>.</p><p>We observe this phenomenon on instances of navigation problems, where the goal is to find a target in a maze, and there are a large number of states relatively to a small number of observations. It is fairly natural: using a memoryless policy in a maze is misleading because if the decision maker meets a wall, he will act as it is the first time he meets a wall, and then will always take the same actions. It seems that on these instances, the SARSOP policies work best on larger horizons, which is expected since the SARSOP policy is built for an infinite horizon problem. The results in saying that the point-based algorithms for infinite discounted POMDP, such as SARSOP, can be inefficient on finite horizon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Bibliographical remarks</head><p>POMDP with history-dependent policies. The original POMDP problem (P his ) has received a lot of interest in the literature, and, the state-of-the-art algorithms solving (P his ) are based on two fundamental results. First, a POMDP is equivalent to a MDP in the belief state space [42, <ref type="bibr">Theorem 4]</ref>. The belief state is the posterior probability distribution of the state given past decisions and observations and the belief state space corresponds to the unit simplex. It follows that a POMDP is MDP with a continuous state space and the Bellman's equation can be written on the belief state space (see <ref type="bibr" target="#b77">[78,</ref><ref type="bibr">Eq.</ref> (2.15)]). The second fundamental result is that the value function of this Bellman's equation is piecewise linear and concave. It enables to derive an exact dynamic programming algorithms for POMDPs with finite <ref type="bibr" target="#b143">[144]</ref> or infinite horizon <ref type="bibr" target="#b146">[147]</ref>. The most important ones are the Witness algorithm <ref type="bibr" target="#b63">[64,</ref><ref type="bibr" target="#b85">86]</ref> and the Incremental Pruning algorithm <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b167">168]</ref>. However, these exact algorithms become quickly intractable when the size of the spaces grows. While several heuristics use value function approximations in dynamic programming <ref type="bibr" target="#b53">[54,</ref><ref type="bibr" target="#b144">145]</ref>, point-based algorithms approximate the belief state space with a finite subset and derive value iteration on its belief points <ref type="bibr" target="#b79">[80,</ref><ref type="bibr" target="#b119">120,</ref><ref type="bibr" target="#b139">140,</ref><ref type="bibr" target="#b145">146]</ref>. Aras et al. <ref type="bibr" target="#b5">[6]</ref> proposed a mixed-integer programming approach giving an optimal history-dependent policy that models exactly P his . Once again, solving such a program is computationally expensive even for small state spaces and small observation spaces. For more details on POMDP solutions, see for instance the surveys of Monahan <ref type="bibr" target="#b106">[107]</ref>, Ross et al. <ref type="bibr" target="#b129">[130]</ref>, Shani et al. <ref type="bibr" target="#b139">[140]</ref> or more recently the book of Krishnamurthy <ref type="bibr" target="#b77">[78]</ref>. In a recent work, Walraven and Spaan <ref type="bibr" target="#b157">[158]</ref> point out the reasons why the existing state-of-the-art algorithms for solving the POMDP problem over infinite horizon with discounted rewards fail to generalize to POMDP problem over a finite horizon without discounting.</p><p>POMDP with memoryless policies. As we explained in Chapter 3, choosing a memoryless policy restricts the policy space but the resulting problem (P ml ) is NP-hard <ref type="bibr" target="#b86">[87]</ref>. Littman <ref type="bibr" target="#b86">[87]</ref> proposed a branch-and-bound heuristic that explores the policy space ∆ ml and Meuleau et al. <ref type="bibr" target="#b105">[106]</ref> generalized it into an exact greedy algorithm for solving the problem with infinite horizon and discounted reward. In the reinforcement learning community, several policy iteration like algorithms have been proposed to find a stationary stochastic policy <ref type="bibr" target="#b58">[59,</ref><ref type="bibr" target="#b83">84,</ref><ref type="bibr" target="#b142">143]</ref>.</p><p>In the case of solving the MDP problem, the classical linear programming formulation (4.13) <ref type="bibr" target="#b37">[38]</ref> uses the moments of the distribution as variables. This formulation is called "dual formulation" in the book of Puterman <ref type="bibr" target="#b123">[124]</ref> because it is the dual of the well-known linear formulation of the Bellman equation for finite horizon MDP (see, e.g., <ref type="bibr" target="#b123">[124]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Integer programming for weakly coupled POMDPs</head><p>The goal of this chapter is to introduce mathematical programming formulations and algorithms to find a "good" policy for a weakly coupled POMDP (P wc ml ). We recall that (P wc ml ) has been introduced in Section 3.2 and can be formulated as max</p><formula xml:id="formula_100">δ∈∆ ml E δ T t =1 r (S t , A t , S t +1 ) (P wc ml )</formula><p>In this chapter, we denote by v * ml the optimal value of (P wc ml ). Since a weakly coupled POMDP is a POMDP on the state space X S , the observation space X O and the action space X A , we could in principle apply MILP (4.7) to solve (P wc ml ). However, the number of constraints and variables grows exponentially with the number of components M , and becomes quickly intractable. Even the linear relaxation of MILP (4.7) becomes intractable. Indeed, Theorem 4.4 ensures that the linear relaxation of MILP (4.7) is equivalent to its MDP approximation. In the case of a weakly coupled POMDP, its MDP approximation is equivalent to the weakly coupled dynamic program of Adelman and Mersereau <ref type="bibr" target="#b1">[2]</ref>, which is intractable even for small values of M .</p><p>To address this issue, we introduce a new MILP formulation based on the results of Chapter 4. This formulation is an approximation of (P wc ml ) and has a tractable number of constraints and variables. The approximation is based on a relaxation of the linking constraint in the definition of the action space (3.2). To evaluate the quality of the approximation, we introduce formulations whose optimal values are lower bound and upper bound of (P wc ml ), which are also bounds of the optimal value of the integer program. Numerical experiments on medium scale instances of multi-armed bandits show that these bounds are close. In the previous chapter, MILP <ref type="bibr">(4.7)</ref> gave explicitly an optimal policy. By explicitly, we mean that we had to solve only a single integer program, and its optimal solution provides the policy δ. This is no more the case here. The values taken by a solution of our integer formulation do not give policy that is a conditional probability distribution over the action space (3.2) given an observation. To address this issue, we introduce the implicit policies, i.e., the policies that are defined through a family of tailored optimization problems indexed by the possible history of observations and actions. At a given time step, the corresponding MILP is built, then solved and an action is retrieved from the optimal solution. These implicit policies will be used in Part III for the maintenance problem at Air France. In this chapter, we do not use a formulation with value function variables like in the previous chapter. Chapter 5 is organized as follows • Section 5.1 introduces the MILP, which is an approximation of (P wc ml ). • Section 5.2 extends the valid inequalities (4.8) of MILP (4.7) to the new MILP. • Section 5.3 shows how the linear relaxation of our MILP is related to POMDP with historydependent policies (P his ). • Section 5.4 introduces the shared upper bound and the shared lower bound on the optimal value of the approximation and (P wc ml ). • Section 5.5 shows how we build an implicit feasible policy for (P wc ml ) based on our integer formulation.</p><p>• Section 5.6 introduces a rolling horizon heuristic that exploits the history-dependent policy towards a practical solution. • Section 5.7 provides several numerical experiments on Multi-armed Bandits which are partially observable.</p><p>For convenience, given a POMDP (X S , X O , X A , p,r) and a finite horizon T , we define respectively the feasible sets of NLP (4.1) and MILP (4.7) as</p><formula xml:id="formula_101">Q(T,X S , X O , X A , p) and Q d (T, X S , X O , X A , p).</formula><p>We write respectively Q and Q d when (T, X S , X O , X A , p) is clear from the context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">An approximate integer program</head><formula xml:id="formula_102">Consider a weakly coupled POMDP (X m S , X m O , X m A , p m , r m , D m ) m∈[M ] , b .</formula><p>Based on the results of Chapter 4, a naive approach is to use MILP (4.7) on the POMDP X S , X O , X A , p , r , where ). As we explained in the introduction, the number of variables and the number constraints of MILP (4.7) are exponential in the number of components M , which makes its resolution or the resolution of its linear relaxation intractable. In this section, we propose an approximate integer program that breaks this curse of dimensionality.</p><formula xml:id="formula_103">X S = X 1 S × • • • × X M S , X O = X 1 O × • • • × X M O ,</formula><p>We introduce variables</p><formula xml:id="formula_104">τ m = (τ 1,m s ) s , (τ t ,m sas ) s,a,s , (τ t ,m soa ) s,o,a , (τ t ,m a ) a t ∈[T ] , δ m = (δ t ,m ) t ∈[T ] for all m in [M ]</formula><p>, and the following MILP.</p><formula xml:id="formula_105">max τ,δ T t =1 M m=1 s,s ∈X m S a∈X m A r m (s, a, s )τ t ,m sas s.t. τ m , δ m ∈ Q d T, X m S , X m O , X m A , p m ∀m ∈ [M ] s∈X m S ,o∈X m A τ t ,m soa = τ t ,m a ∀a ∈ X m A , m ∈ [M ], t ∈ [T ] M m=1 a∈X m A D m (a)τ t ,m a b ∀t ∈ [T ]</formula><p>(5.1a) there is no guarantee that there exists a feasible policy δ of (P wc ml ) such that the variables (τ m ) m∈[M ]</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Valid inequalities</head><p>represent the moments of the distribution P δ on the whole system. It is the reason why we denote by τ the approximate vector of moments instead of µ, which we used in Chapter 4.</p><p>In fact, the optimal value of MILP (5.1), denoted by z IP , is neither an upper bound, nor a lower bound of v * ml in general. A feasible solution (τ m , δ m ) m∈[M ] of MILP (5.1) defines a policy δ m on each component m in [M ]. Unfortunately, in general, we are not able to derive from it a feasible policy of (P wc ml ). Indeed, for any observation o, an element a satisfying δ t ,m a m |o m = 1 for any m in</p><p>[M ] and t in [T ] does not necessary belong to X A . Conversely, a feasible solution δ of (P wc ml ) does not define unambiguously a deterministic policy for each component m. In Appendix A, we provide numerical examples of this phenomenon. However, z IP is a good approximation for v * ml : in Section 5.4 we introduce a common lower bound and a common upper bound of z IP and v * ml and numerical experiments in Section 5.7 show that these bounds turn out to be close in some practical applications.</p><p>In fact, going from MILP (4.7) to MILP (5.1) we performed three approximations:</p><formula xml:id="formula_106">(A) Consider "local" variables (τ m , δ m ) in Q T, X m S , X m O , X m A , p m for each component m, (B) Consider deterministic policies δ m ∈ ∆ d,m , (C) Transform the linking constraint M m=1 D m (A m t ) b, which is almost sure, into the con- straint in expectation M m=1 E δ m D m (A m t ) b.</formula><p>The size of MILP (5.1) is tractable for two reasons. First, there is a polynomial number of constraints. Indeed, the number of constraints of MILP (5.1 </p><formula xml:id="formula_107">) is O T M m=1 |X m S ||X m O ||X m A | .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Valid inequalities</head><p>Since Equalities (4.8) are valid for MILP (4.7), we can naturally derive similar equalities to tighten the linear relaxation of MILP (5.1). We introduce new variables τ t ,m s a soa and the following linear inequalities.</p><formula xml:id="formula_108">s ∈X m S ,a ∈X m A τ t ,m s a soa = τ t ,m soa , ∀s ∈ X m S , o ∈ X m O , a ∈ X m A , a∈X m A τ t ,m s a soa = p m (o|s)p m (s|s , a )τ t -1,m s a , ∀s, s ∈ X m S , o ∈ X m O , a ∈ X m A , τ t ,m s a soa = p m (s|s , a , o) s∈X m S τ t ,m s a soa , ∀s, s ∈ X m S , o ∈ X m O , a, a ∈ X m A .</formula><p>(5.2) Proposition 5.1. Inequalities (5.2) are valid for MILP (5.1), MILP (5.3) and NLP (5.5), and there exists solution of the linear relaxation of (5.1) that does not satisfy constraints (5.2).</p><p>Proof. Proposition 4.3 ensures that equalities (5.2) are valid on each component. Hence, these inequalities are valid for MILP (5.1), MILP (5.3) and NLP (5.5). Proposition 4.3 also ensures that there are solutions of the linear relaxation of (4.7) that do not satisfy constraints (4.8) on each</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">An upper bound and a lower bound</head><formula xml:id="formula_109">z LB := max τ,δ T t =1 M m=1 s,s ∈X m S a∈X m A r m (s, a, s )τ t ,m sas s.t. τ m , δ m ∈ Q d T, X m S , X m O , X m A , p m ∀m ∈ [M ] M m=1 a∈X m A D m (a)δ t ,m a|o m b ∀o ∈ X O , t ∈ [T ]</formula><p>(5.3a)</p><p>(5.3b)</p><p>(5.3c)</p><formula xml:id="formula_110">MILP (5.</formula><p>3) is obtained by using approximations (A) and (B). Note that the difference between MILP (5.1) and MILP (5.3) is that we replace the moments τ m in constraints (5.1d) by the policy δ m in constraints (5.3c). While constraints (5.1d</p><formula xml:id="formula_111">) ensure that M m=1 E δ m D m (A m t ) b, con- straints (5.3c) ensure that M m=1 E δ m D m (A m t )|O m t = o m b for any o ∈ X O .</formula><p>Consequently, the linking constraint of X A is satisfied almost surely in MILP (5.3). We denote by z LB the optimal value of MILP (5.3). It follows from Theorem 5.3 that MILP (5.3) gives a policy δ that is feasible for (P wc ml ), which is interesting in itself. Theorem 5.3 tells us even more: MILP (5.3) restricts to the "decomposable" deterministic policies, i.e., policies δ that can be written δ = M m=1 δ m . However, MILP (5.3) has a exponential number of constraints, which makes it intractable for a large number of components. Indeed, there are </p><formula xml:id="formula_112">T × M m=1 |X m O | constraints (5.3c).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M m=1 a∈X</head><formula xml:id="formula_113">m A D m (a)τ t ,m a = M m=1 a∈X m A ,o∈X m O D m (a)δ t ,m a|o τ t ,m o = M m=1 a∈X m A D m (a m )E τ t ,m [δ t ,m a m |O m t ] b</formula><p>The first equality is a consequence of the tightness of the McCormick constraints (4.6a)-(4.6c).</p><p>The second equality comes from the fact that the variables (τ t ,m o ) o∈X m O define a probability distribution over X m O . Finally, the last inequality results from</p><formula xml:id="formula_114">M m=1 a∈X m A D m (a)E τ t ,m [δ t ,m a|O m t ] M m=1 a∈X m A D m (a) max o∈X m O (δ t ,m a|o ) b</formula><p>Therefore, the inequality holds (5.1d) and MILP (5.1) is a relaxation of MILP (5.3).</p><p>Second, we show that (τ m , δ m ) m∈[M ] is a feasible solution of (P wc ml ). We define a policy over</p><formula xml:id="formula_115">X A × X O . δ t a|o = M m=1 δ t ,m a m |o m (5.4) for all a ∈ X A , o ∈ X O and t ∈ [T ]. It suffices to prove that δ belongs to ∆. Let o ∈ X O and t ∈ [T ]. a∈X A δ t a|o = a∈X A M m=1 δ t ,m a m |o m = a∈X A M m=1 δ t ,m a m |o m = 1</formula><p>The second equality comes from the fact that for any a</p><formula xml:id="formula_116">∈ X A such that M m=1 D m (a m ) &gt; b, M m=1 δ t ,m</formula><p>a m |o m = 0 because of Constraints (5.3c). Therefore, δ is a feasible policy of (P wc ml ). Since the objective functions are the same, the inequalities z LB v * ml and z LB z IP hold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.2">An upper bound through a nonlinear formulation</head><p>Using the same notation as in MILP (5.1), we introduce the following NLP.</p><formula xml:id="formula_117">z UB := max τ,δ T t =1 M m=1 s,s ∈X m S a∈X m A r m (s, a, s )τ t ,m sas s.t. τ m , δ m ∈ Q T, X m S , X m O , X m A , p m ∀m ∈ [M ] s∈X m S ,o∈X m A τ t ,m soa = τ t ,m a ∀a ∈ X m A , m ∈ [M ], t ∈ [T ] M m=1 a∈X m A D m (a)τ t ,m a b ∀t ∈ [T ]</formula><p>(5.5a)</p><formula xml:id="formula_118">(5.5b)<label>(5.5c)</label></formula><p>(5.5d) NLP (5.5) is obtained by using approximations (A) and (C). Once again, variables τ m can be interpreted as the vector of moments of the probability distribution P δ m on component m but there is no guarantee that it defines a joint probability distribution over the whole system. However, Theorem 5.4 ensures that it gives a relaxation of (P wc ml ). We denote by z UB the optimal value of NLP (5.5).</p><p>Theorem 5.4. NLP (5.5) is a relaxation of (P wc ml ) and MILP (5.1). In particular, v * ml z UB and z IP z UB . NLP (5.5) is a Quadratically Constrained Quadratic Program (QCQP) due to constraints (5.5b) and is in general non-convex. Hence, it is hard to solve NLP (5.5) in practice because it requires to use a Spatial Branch-and-Bound However, the number of variables and the number of constraints are polynomial. Thanks to the recent advances of QCQP solvers such as Gurobi 9.0 <ref type="bibr" target="#b51">[52]</ref>, we are able to solve NLP (5.5) to optimality in a reasonable computation time for small instances. For larger instances, the solver is no longer efficient because it reaches the limits of a Spatial Branch-and-Bound <ref type="bibr" target="#b84">[85]</ref>.</p><p>Proof of Theorem 5.4. First, we prove that NLP (5.5) is a relaxation of MILP (5.1). Let (τ m , δ m ) m∈[M ] be a feasible solution of MILP (5.1). We prove that (τ m , δ m ) m∈[M ] is a feasible solution of NLP <ref type="bibr">(5.5)</ref>. It suffices to prove that for all m ∈ [M ], (τ m , δ m ) satisfies constraints (4.1g). It comes from the tightness of the McCormick inequalities (4.6a)-(4.6c) when the policy is deterministic. Hence, it is a relaxation with the same objective function. Therefore, the inequality z IP z UB holds.</p><p>Second, we prove that NLP (5.5) is a relaxation of (P wc ml ). Let δ be a feasible policy of (P wc ml ). We want to define a solution of the non-linear program (5.5). We extend the domain of δ to X A by</p><formula xml:id="formula_119">setting δ t a|o = 0 when m∈[M ] D m (a m ) &gt; b, for all o ∈ X O .</formula><p>It is easy to see that δ is still a policy in X A . Theorem 4.1 ensures that there exists µ such that (µ, δ) is a feasible solution of MILP (4.7).</p><p>We define the variables τ m on component m ∈ [M ] by induction</p><formula xml:id="formula_120">τ 1,m s = s -m ∈X -m S µ 1 s , τ 1,m soa = s -m ∈X -m S o -m ∈X -m O a -m ∈X -m A δ t a|o m =m p m (o m |s m )τ 1,m s m p m (o|s)τ 1,m s , τ t ,m sas = o ∈X m O ,a ∈X m A τ t ,m s o a , τ t +1,m soa = s -m ∈X -m S o -m ∈X -m O a -m ∈X -m A δ t a|o m =m p m (o m |s m )τ t ,m s m p m (o|s) s ∈X m S ,a ∈X m A τ t ,m s a s ,</formula><p>and the policy</p><formula xml:id="formula_121">δ m δ t ,m a|o = s -m ∈X -m S o -m ∈X -m O a -m ∈X -m A δ t a|o m =m p m (o m |s m )τ t ,m s m , for all a ∈ X m A , o ∈ X m O and t ∈ [T ]. By definition of τ m , if δ m is in ∆ m , then the constraints (5.5b) are satisfied by (τ m , δ m ) m∈[M ] . We prove that δ m is in ∆ m ml . a∈X m A δ t ,m a|o = a∈X m A s -m ∈X -m S o -m ∈X -m O a -m ∈X -m A δ t a|o m =m p m (o m |s m )τ t ,m s m = s -m ∈X -m S o -m ∈X -m O m =m p m (o m |s m )τ t ,m s m = 1 for all o ∈ X m O , m ∈ [M ] and t ∈ [T ].</formula><p>The last equality comes from the fact that by induction we have that s∈X m S τ t ,m s = 1. Therefore, δ m ∈ ∆ m ml . It remains to prove that constraints (5.5d) are satisfied by (τ m , δ m ) m∈ <ref type="bibr">[M ]</ref> . We compute the lefthand side of constraint (5.5d).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>M m=1 a∈X</head><formula xml:id="formula_122">m A D m (a)τ t ,m a = M m=1 a∈X m A D m (a) s∈X S ,o∈X O a ∈X A :a m =a m δ t a |o M m =1 p m (o m |s m )τ t ,m s m = s∈X S ,o∈X O a ∈X A δ t a |o M m =1 p m (o m |s m )τ t ,m s m M m=1 a∈X m A D m (a m ) = =1 s∈X S ,o∈X O a∈X A δ t a|o M m =1 p m (o m |s m )τ t ,m s m M m=1 D m (a m ) b b</formula><p>Therefore, constraints (5.5d) are satisfied. Consequently, NLP (5.5) is a relaxation of (P wc ml ). In addition, the objective functions are equal. We deduce that v * ml z UB .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.3">A tractable upper bound through Lagrangian relaxation</head><p>When the number of components increases, computing the upper bound z UB becomes quickly intractable. Fortunately, we can obtain a weaker upper bound of v * ml and z IP that is more tractable to compute. To do so, we use the Lagrangian relaxation technique on constraints (5.5d) of NLP <ref type="bibr">(5.5)</ref>. This technique has already been used in the literature on weakly coupled dynamic programs to compute upper bounds <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b164">165]</ref>.</p><p>We denote by β = (β t ) t ∈[T ] the dual variables associated with constraints (5.5d). If we relax constraints (5.5d), then we obtain the Lagrangian function We introduce a collection of nonnegative variables β = (β t ) t ∈[T ] with β t ∈ R q + for any t ∈ [T ] and the following Lagrangian function</p><formula xml:id="formula_123">L τ, δ, β = T t =1 M m=1 s,s ∈X m S a∈X m A r m (s, a, s )τ t ,m sas + T t =1 (β t ) T b - M m=1 a∈X m A D m (a)τ t ,m a , for any (τ m , δ m ) m∈[M ]</formula><p>. Then, we introduce the dual function G : R</p><formula xml:id="formula_124">T ×q + → R, with values G β := max τ,δ T t =1 M m=1 s,s ∈X m S a∈X m A r m (s, a, s )τ t ,m sas + T t =1 (β t ) T b - M m=1 a∈X m A D m (a)τ t ,m a s.t. τ m , δ m ∈ Q T, X m S , X m O , X m A , p m ∀m ∈ [M ]<label>(5.6)</label></formula><p>By weak duality, for any β, the dual function (5.6) provides an upper bound obtained by using Approximation (A).</p><p>We now explain how to compute the dual function. As it is usually the case for Lagrangian relaxation, for every β ∈ R T ×q + , the maximum in the computation of G(β) decomposes over the sum of the maximum over each component. However, the formulations obtained for each component are still nonlinear. Fortunately, the following proposition ensures that we can linearize the formulation without changing the value of the dual function.</p><p>Proposition 5.5. For all β ∈ R T ×q + , the dual function can be written as</p><formula xml:id="formula_125">G β = T t =1 (β t ) T b + M m=1 G m (β) (5.7)</formula><p>where G m (β) is the quantity</p><formula xml:id="formula_126">G m β := max τ m ,δ m T t =1 s,s ∈X m S a∈X m A r m (s, a, s ) -(β t ) T D m (a) τ t ,m sas s.t. τ m , δ m ∈ Q d T, X m S , X m O , X m A , p m</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">An upper bound and a lower bound</head><p>Proof. Let β ∈ R T q + . Then, the value function G in β can be written:</p><formula xml:id="formula_127">G β = max (τ m ,δ m )m∈[M] T t =1 M m=1 s,s ∈X m S a∈X m A r m (s, a, s ) -(β t ) T D m (a) τ t ,m sas + T t =1 (β t ) T b s.t. τ m , δ m ∈ Q T, X m S , X m O , X m A , p m ∀m ∈ [M ]</formula><p>Since the second term does not depend on (τ m , δ m ) m∈[M ] , we only consider the maximization on the first term. In such a problem, there are no linking constraints between the components, which enables to decompose the maximization operator along the components as follows.</p><formula xml:id="formula_128">G β = M m=1 max τ m ,δ m T t =1 s,s ∈X m S a∈X m A r m (s, a, s ) -(β t ) T D m (a) τ t ,m sas + T t =1 (β t ) T b s.t. τ m , δ m ∈ Q T, X m S , X m O , X m A , p m</formula><p>Theorem 4.1 ensures that the optimization subproblem above on component m corresponds to</p><formula xml:id="formula_129">a POMDP problem with memoryless policies of POMDP X m S , X m O , X m A , p m , r where r m (s, a, s ) = r m (s, a, s )-(β t ) T D m (a) for any s, s ∈ X m S and a ∈ X m A .</formula><p>Thanks to Proposition 4.2, the subproblem on component m can be solved using deterministic policies. Therefore, we can replace</p><formula xml:id="formula_130">Q T, X m S , X m O , X m A , p m by Q d T, X m S , X m O , X m A , p m for any component m and we obtain G β = T t =1 (β t ) T b + M m=1 G m (β),</formula><p>which achieves the proof.</p><p>It follows from Proposition 5.5 that the dual function can be computed by solving MILP (4.7) on each component of the system, which is in general easier than solving NLP (5.5). As stated in the following proposition, it gives us an upper bound that is not worse than the optimal value of the linear relaxation of MILP (5.1). We denote respectively by z R and z R c the optimal values of the linear relaxation of MILP (5.1) and the linear relaxation of MILP (5.1) with valid inequalities (5.2), and we define the Lagrangian relaxation z LR := min β∈R T q + G(β).</p><p>Proposition 5.6. The value of the Lagrangian relaxations of MILP (5.1) and NLP (5.5) are equal and the following inequalities hold:</p><formula xml:id="formula_131">z UB z LR z R c z R</formula><p>Proof. Thanks to Proposition 5.5, the dual functions of MILP (5.1) and NLP (5.5) are equal. It follows that the value of the Lagrangian relaxations are equal.</p><p>First, the inequality z UB z LR comes from weak duality (see e.g. Bertsekas [13, Proposition 5.1.3]). Second, to show the second inequality z LR z R , it suffices to observe that the dual function G(β) of NLP (5.5) is also the dual function of MILP (5.1). Indeed, in the expression of</p><formula xml:id="formula_132">G m (β) we can replace Q T, X m S , X m O , X m A , p m by Q d T, X m S , X m O , X m</formula><p>A , p m because the re always exists an optimal policy that is deterministic on the POMDP It remains to prove that z LR z R c and z R c z R . The second one comes from the fact that we have a smaller feasible set in the linear relaxation by adding the valid inequalities. The first one comes by adding valid inequalities (5.2) in the expression of G m (β), which is possible since the inequalities are valid, and by using the same arguments (weak duality and Geoffrion's Theorem) we conclude that z LR z R c . Now we propose two methods to compute the value of the Lagrangian relaxation z LR : the subgradient algorithm and a column generation algorithm. Depending on the user preferences, both approaches have their advantages and drawbacks. In this thesis, we choose to use the column generation algorithm, which turns to be efficient on numerical experiments (see Section 5.7).</p><formula xml:id="formula_133">X m S , X m O , X m A , p m , r m -β T D m . A classical result</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Subgradient algorithm.</head><p>Since the dual function G(β) is convex in β, we can apply the classical subgradient methods (see e.g. Bertsekas <ref type="bibr" target="#b12">[13]</ref>), which is known to converge to min β∈R T q</p><formula xml:id="formula_134">+ G(β)</formula><p>for a step length carefully chosen. Note that each step requires to solve M times MILP (5.1) to evaluate the dual function. Even if the convergence to the optimum of such an algorithm can be slow, we obtain quickly an upper bound that is tighter than z R and z R c .</p><p>Column generation algorithm. Thanks to Geoffrion's Theorem [29, Theorem 8.2], the value of the Lagrangian relaxation MILP (5.1) satisfies the following Dantzig-Wolfe reformulation:</p><formula xml:id="formula_135">z LR = max (τ m ,δ m ) m∈[M ] M m=1 T t =1 s,s ∈X m S a∈X m A r m (s, a, s )τ t ,m sas s.t. (τ m , δ m ) ∈ Conv Q d (T, X m S , X m O , X m A , p m ) , ∀m ∈ [M ] M m=1 a∈X m A D m (a)τ t ,m a b, ∀t ∈ [T ],<label>(5.11)</label></formula><p>where we included the constraints</p><formula xml:id="formula_136">s∈X m S ,o∈X m A τ t ,m soa = τ t ,m a in the set Q d (T, X m S , X m O , X m A , p m )</formula><p>and Conv(X ) denotes the convex hull of a set X . We can compute this formulation using an exact column generation algorithm.</p><p>Using the definition of the convex hull, we can reformulate MILP (5.11) as the following master</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">An upper bound and a lower bound</head><p>problem:</p><formula xml:id="formula_137">z LR = max (ω m ,τ m ,δ m ) m∈[M ] M m=1 T t =1 s,s ∈X m S a∈X m A r m (s, a, s )τ t ,m sas s.t. (τ m , δ m ) = (τ,δ)∈Q d,m ω m τ,δ (τ, δ) ∀m ∈ [M ] (τ,δ)∈Q d,m ω m τ,δ = 1 ∀m ∈ [M ] M m=1 a∈X m A D m (a)τ t ,m a b ∀t ∈ [T ] ω m τ,δ 0 ∀(τ, δ) ∈ Q d,m , ∀m ∈ [M ]</formula><p>(5.12a)</p><p>(5.12b)</p><p>(5.12c)</p><p>(5.12d)</p><p>(5.12e)</p><p>where</p><formula xml:id="formula_138">Q d,m = Q d (T, X m S , X m O , X m A , p m ) for every m in [M ].</formula><p>It follows that the pricing subproblem on component m writes down:</p><formula xml:id="formula_139">z m := max (τ,δ) T t =1 s,s ∈X m S a∈X m A r m (s, a, s ) -β T t D m (a) τ t sas s.t. (τ, δ) ∈ Q d,m ,<label>(5.13)</label></formula><p>where</p><formula xml:id="formula_140">β = (β t ) t ∈[T ] ∈ R T q</formula><p>+ is the vector dual variables of the linking constraint (5.12d). We denote by π = (π m ) m∈[M ] the vector of dual variables of Constraints (5.12c). It follows that the reduced cost can be written c = M m=1 z m + π m . Now we are able to derive the column generation algorithm. We assume that X A = (otherwise the decision maker cannot choose any action). Hence, there exists at least one element a ∈</p><formula xml:id="formula_141">X 1 A × • • • × X M A such that M m=1 D m (a m ) b.</formula><p>Let a e be such an element in X A .</p><p>Algorithm 1 returns an optimal solution of the master problem (5.12) and the value of the Lagrangian relaxation z LR . We omit the proof in this thesis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.4">Interpretation of the bounds</head><p>In this section, we summarize the links between the feasible sets of MILP (5.3), MILP (5.1), NLP (5.5) and (P wc ml ). It enables to give a probabilistic interpretation of the bounds z LB and z UB . First, MILP (5.3) corresponds to a restriction of (P wc ml ) because we choose policies that are deterministic on each component and all the actions induced by the feasible policy satisfies the linking constraints (3.2) almost surely.</p><p>Second, NLP (5.5) allows stochastic policies, and constraint (5.5b) ensures that the linking constraints in (3.2) are satisfied in expectation. Therefore, z LB and z UB are respectively a "deterministic" approximation and a "in expectation" approximation of (P wc ml ). Compute</p><formula xml:id="formula_142">τ m such that (τ m , δ m ) ∈ Q d,m 7:</formula><p>z m ← ∞ and π m ← ∞ 8: end for 9: while M m=1 z m + π m &gt; 0 do 10:</p><p>Add column:</p><formula xml:id="formula_143">Q m ← Q m ∪ {(τ m , δ m )} 11:</formula><p>Solve master problem (5.12) restricted to (Q m ) m∈[M ] to obtain dual variables (β, π)</p><formula xml:id="formula_144">12:</formula><p>z ← Optimal value of the restricted master problem All the formulations we propose have a polynomial number of variables. Each formulation has benefits and drawbacks. While MILP (5.3) is an MILP with an exponential number of constraints, NLP (5.5) is a NLP with a polynomial number of constraints. The presence of linking constraints between the components in the MILP (5.3), MILP (5.1), and the NLP (5.5), increases the difficulty to solve a linear or nonlinear formulation. Indeed, these constraints are also called "complicating constraints" <ref type="bibr" target="#b27">[28]</ref> because it prevents of solving a single POMDP problem on each component individually. Since there are no linking constraints in the integer program (5.7) that computes the dual function, it is the easiest to solve.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5">Deducing an history-dependent policy from MILP (5.1)</head><p>In MILP (5.1), we consider "local" policies δ m on each component m in [M ]. However, in general, given a vector of "local" policies (δ m ) m∈[M ] , there is no guarantee that there exists a policy δ that coincides with δ m for every components m in [M ]. In this section we describe how we build an implicit policy that is feasible for (P wc his ). We recall that an implicit policy of (P wc his ) is a policy δ such that each value δ t a|h is computed using an tailored optimization problem. Consider a weakly coupled POMDP (  Then we define the implicit history-dependent policy δ IP as follows:</p><formula xml:id="formula_145">X m S , X m O , X m A , p m , r m , D m ) m∈[M ] ,</formula><formula xml:id="formula_146">δ t ,IP a|h = 1, if a = Act t T (h) 0, otherwise , ∀h ∈ X t H , a ∈ X A , t ∈ [T ]<label>(5.14)</label></formula><p>It is not clear that the policy (5.14) is a feasible policy of (P wc his ) because it is not immediate to see that the action returned by Algorithm 2 belongs to X A . The theorem below ensures that the implicit policy (5.14) is a feasible policy of (P wc his ) giving a higher expected reward than z IP . We denote by ν IP the total expected reward induced by policy δ IP .</p><p>Theorem 5.7. The implicit policy δ IP defined in (5.14) is a feasible policy of (P wc his ) and the inequality z IP ν IP v * his holds.</p><p>Theorem 5.7 has a strong interest in practice because it enables to exploit the implicit policy <ref type="bibr">(5.14)</ref> in an rolling horizon heuristic, which we describe in Section 5.6. Theorem 5.7 enables to deduce a feasible policy of (P wc his ) from MILP (5.1).   The last inequality comes from the fact that (τ m , δ m ) m∈[M ] satisfies constraint (5.1d).</p><formula xml:id="formula_147">Remark 7. Given a POMDP (X S , X O , X A , p,</formula><p>Now we prove the inequalities. The inequality ν IP v * his holds because δ IP ∈ ∆ his . It remains to show that z IP ν IP . We do it using a backward induction. Let (τ * m , δ * m ) m∈[M ] be an optimal solution of MILP (5.1). We denote by P t (h t ) the feasible set of the optimization problem solved in Act IP,t  T (h t ), for every t in [T ]. We consider the following induction hypothesis at time t :</p><formula xml:id="formula_148">max (τ m ,δ m ) m∈[M ] ∈P t (h t ) M m=1 E δ m T t =t r m (S m t , A m t , S m t +1 )|H m t = h m t E δ IP M m=1 T t =t r m (S m t , A m t , S m t +1 )|H t = h t</formula><p>If t = T , then consider left-hand side is exactly equal to the right-hand side.</p><formula xml:id="formula_149">max (τ m ,δ m ) m∈[M ] ∈P T (h T ) M m=1 E δ m r (S m T , A m T , S m T +1 )|H m T = h m T = max (τ m ,δ m ) m∈[M ] ∈P T (h T ) M m=1 E δ m r (S m T , A m T , S m T +1 )|S m T ∼ p m (•|h m T ) = E δ m,IP M m=1 r m (S m T , A m T , S m T +1 )|H T = h T</formula><p>The first equality comes from the fact that the belief state is a sufficient statistic of the history. It proves the induction hypothesis for t = T .</p><p>Suppose that the induction hypothesis holds from t + 1. We compute the term in t :</p><formula xml:id="formula_150">max (τ m ,δ m ) m∈[M ] ∈P t (h t ) M m=1 E δ m T t =t r m (S m t , A m t , S m t +1 )|H m t = h m t = max (τ m ,δ m ) m∈[M ] ∈P t (h t ) M m=1 E δ t ,m r m (S m t , A m t , S m t +1 )|H m t = h m t + M m=1 a m t ,o m t +1 P δt ,m O m t +1 = o m t +1 , A m t = a m t |H m t = h m t × does not depend on δ t ,m E δ m     T t =t +1 r m (S m t , A m t , S m t +1 )| H m t = h m t , A m t = a m t ,O m t +1 = o m t +1 H m t +1 =h m t +1     E δ t ,IP M m=1 r m (S m t , A m t , S m t +1 )|H t = h t + a t ,o t +1 P δt ,IP (O t +1 = o t +1 , A t = a t |H t = h t ) × induction hypothesis max (τ m ,δ m )∈P t +1 (h t +1 ) M m=1 E δ m T t =t +1 r m (S m t , A m t , S m t +1 )|H m t +1 = h m t +1 M m=1 E δ m,IP r m (S m t , A m t , S m t +1 )|H m t = h m t</formula><p>The first inequality above comes from the fact that there exists an optimal solution where δ t ,IP is the policy at time t by definition of δ IP and by decomposing the maximum operator in the sum of the second term. This latter operation can be done since</p><formula xml:id="formula_151">E δ m T t =t +1 r m (S m t , A m t , S m t +1 )|H m t +1 = h m t +1</formula><p>does not depend on the policy δ t ,m for t &lt; t + 1. It proves the backward induction. Finally, given an optimal feasible solution (τ * m , δ * m ) m∈[M ] of MILP (5.1), we get that:</p><formula xml:id="formula_152">z IP = M m=1 E δ * m t =1 r m (S m t , A m t , S m t +1 ) = E E δ * m M m=1 T t =1 r m (S m t , A m t , S m t +1 )|O m 1 = o m 1 E max (τ m ,δ m ) m∈[M ] ∈P 1 (h 1 ) M m=1 E δ m t =1 r m (S m t , A m t , S m t +1 )|O m 1 = o m 1 M m=1 E δ m,IP t =1 r m (S m t , A m t , S m t +1 ) = ν IP</formula><p>The first inequality comes from the inversion of the maximum operator and the expectation. It achieves the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6">Rolling horizon heuristic</head><p>When the horizon T is long it is computationally interesting to embed the implicit policy (5.14) in a rolling horizon heuristic, which consists in repeatedly solving an optimization problem with a shorter horizon at each time step and to take action at the current time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7.">Numerical experiments</head><p>Indeed, in theory, computing the implicit feasible policy (5.14) requires to compute Act t T (h) for every history h ∈ X t H and every time t in [T ]. It leads to two computational difficulties.</p><p>First, since the size of X t H is exponential, computing the feasible policy becomes quickly prohibitive. Second, if the finite horizon T is too large, then solving MILP (5.1) in Step 4 becomes intractable.</p><p>In practice, at each time t the decision maker receives an observation o t in X O and takes an action a t in X A . Hence, it only requires to compute Act t T (h t ) where h t = (h t -1 , a t -1 , o t ), which addresses the first issue. To address the second issue, we compute MILP (5.1) using a smaller rolling horizon T r &lt; T . The following algorithm shows how we use the feasible policy in practice:</p><formula xml:id="formula_153">Algorithm 3 Rolling horizon heuristic. 1: Input: T , T r , (X m S , X m O , X m A , p m , r m , D m ) m∈[M ] , b 2: for t = 1, . . . , T do 3:</formula><p>Receive observation o t</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4:</head><p>Take action Act t t +T r (h t ) 5: end for Figure <ref type="figure" target="#fig_27">5</ref>.2 illustrates two consecutive iterations of Algorithm 3 with a rolling horizon T r = 5. This type of rolling horizon heuristic is commonly used for multistage optimization problems in Operations Research <ref type="bibr" target="#b91">[92,</ref><ref type="bibr" target="#b126">127,</ref><ref type="bibr" target="#b127">128,</ref><ref type="bibr" target="#b135">136]</ref>. Numerical experiments in Chapter 10 show the efficiency of the implicit policy (5.14).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7">Numerical experiments</head><p>In this section, we provide numerical experiments on the partially observable multi-armed bandits problem introduced in Example 1. All mathematical programs have been written in Julia <ref type="bibr" target="#b17">[18]</ref> with the JuMP <ref type="bibr" target="#b40">[41]</ref> interface and solved using Gurobi 9.0. <ref type="bibr" target="#b51">[52]</ref> with the default settings. Experiments have been run on a server with 192Gb of RAM and 32 cores at 3.30GHz. We show the quality of the approximation (5.1) by comparing the values of z LB , z IP , z UB , z LR , z R c and z R . The results on a predictive maintenance problem with capacity constraints are reported in Part III. Bertsimas and Mišić <ref type="bibr" target="#b14">[15]</ref> explained the benefits of using such transition probabilities and reward functions. We generate small-scale instances with M ∈ {2, 3} arms and n = 4 states, and medium-scale instances with M = 5 arms and n = 4 states.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Instances</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Metrics.</head><p>For each instance, we compute the value z IP , the lower bound z LB and the upper bounds z UB , z LR , z R c and z R . Given an instance, we define the relative gaps with the largest</p><formula xml:id="formula_154">upper bound z R : g LB = z R -z LB z R c , g IP = z R -z IP z R c , g UB = z R c -z UB z R c and g LR = z R c -z LR z R c</formula><p>. Then, we define respectively the metrics G mean (g), G 95 (g) and G max (g) as the mean, the 95-th percentile and the maximum over a set of instances, for each gap g in g LB , g IP , g UB , g LR . In general, the lower the values of the metrics, the closer the bound is to the upper bound z R c . In particular, thanks to Theorem 5.4 and Proposition 5.6the metrics g LB and g LR tell how close are the values of z IP and v * ml . Since the computation of z UB becomes quickly difficult when the sizes of the instance increase, we only compute the values of g UB on small instances. We also report the mean computation time over the Table <ref type="table" target="#tab_32">5</ref>.2 summarizes the results averaged over the 10 instances of each set. For all the mathematical programs, we set the computation time limit to 3600 seconds. If the resolution has not terminated before this time limit, then we keep the best feasible solution obtained at the end of the resolution. It explains why for some instances we obtain a smaller gap with lower bound (5.3) than with MILP (5.1). The Lagrangian relaxation value z LR is computed using a column generation approach.</p><p>One can observe in Table <ref type="table" target="#tab_32">5</ref>.2 that for almost a large part of the instances, the values of z LB , z IP , z UB , and z LR are close in general. It shows that our formulations have optimal values that are close to the optimal value v * ml of (P wc ml ). In addition, the best bound obtained on the value of z IP is very close to the value of the lower bound z LB . Thanks to Theorem 5.3, it means that most of the multi-armed bandit instances admit optimal policies that are "decomposable" (see Section 5.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7.1">Simulations of the implicit policy</head><p>The aim of this section is to show how the value returned by matheuristic 3 is close to the optimal value v * his , and that policy δ IP can be computed in a reasonable amount of time on largescale instances of a practical problem. We evaluate the performances of the history-dependent policy (5.14) by running Algorithm 3 on a maintenance problem taken from the literature. Like Walraven and Spaan [157, Section 5.2], we consider a road authority that performs maintenance on M bridges, each of them evolving independently over a finite horizon H . Each bridge is modeled as a POMDP <ref type="bibr" target="#b43">[44]</ref>, and the authority must chooses at most K bridges to maintain at each decision time. As mentioned in Section 3.3, this problem can be modeled as a weakly coupled POMDP.</p><p>Instances Like Walraven and Spaan <ref type="bibr" target="#b156">[157]</ref>, we build our instances of weakly coupled POMDP from the bridge-repair instance of Ellis et al. <ref type="bibr" target="#b43">[44]</ref> in which the decision maker has to perform maintenance on a bridge. In our problem, there are only two actions available on each bridge: either structural repair or keep. For each bridge m, the sizes of state space, observation space and action space are respectively  Table <ref type="table" target="#tab_32">5</ref>.3 summarizes the results. For all the mathematical programs, we set the computation time limit to 3600 seconds and a final gap tolerance (MIPGap parameter in Gurobi) of 1%, which is enough for the use of our matheuristic. If the resolution has not terminated before this time limit, then we keep the best feasible solution at the end of the resolution.</p><formula xml:id="formula_155">T g M = 2 M = 3 M = 5 G mean (g) G 95 (g) G max (g) Time(s) G mean (g) G 95 (g) G max (g) Time(s) G mean (g) G 95 (g) G max (g)</formula><p>One may observe that for all instances, the matheuristic involving our MILP (5.1) delivers promising results even in the most challenging instance (M = 20). In particular, the values of G R c IP show that the policy δ IP gives an optimality gap (in the set of history-dependent policies) of at most 10% on the large-scale instance, which is satisfying regarding the complexity of the optimization problem. In Table <ref type="table" target="#tab_32">5</ref>.3, the negative values of G R c IP result from error approximations due to the Monte-Carlo simulations. It can also be noted that the gap G LR IP is takes negative values for some instances, which shows that ν IP can take larger values than the Lagrangian relaxation for some instances. It highlights the benefit of using the belief state updates in the definition of δ IP . In addition, even for the largest instances (M = 15 or M = 20) and for T = 5, the average time per action of Act IP,t t +T r (h t ) is on the order of 1.0 second; this amount of time is still feasible even if the 24 decision times are close together.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.8">Bibliographical remarks</head><p>(P wc ml ) lies in the broad class of multi-stage stochastic optimization problems with high-dimensional state spaces. As mentioned in Section 3.5, (P wc ml ) is a weakly coupled dynamic program where the decision maker has only access to partial observations of the system.</p><p>When the decision maker has access to the system's state, (P wc ml ) becomes a weakly coupled dynamic program. It is well known that such a problem can be solved using dynamic programming approach by writing the Bellman's equation. Since the size of the state space and action space are exponential in the number of components of the system, the main challenge of solving weakly coupled dynamic programs is the curse of dimensionality as mentioned in Chapter 3, which makes, in general, the exact dynamic programming approaches intractable. To address this challenge, several approximate dynamic programming methods have been proposed <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b104">105,</ref><ref type="bibr" target="#b122">123]</ref>. These approaches are mainly based on an approximation of the value functions. Other approaches to solve weakly coupled dynamic program consist in deriving heuristic policies from relaxations. These relaxations are of two types: Lagrangian relaxations of the linking constraints in the definition (3.2) of X A <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b54">55,</ref><ref type="bibr" target="#b164">165]</ref>, and relaxations of the nonanticipativity constraints <ref type="bibr" target="#b19">[20]</ref>, which assume that the decision maker has access to the future outcomes. Recently, Bertsimas and Mišić <ref type="bibr" target="#b14">[15]</ref> gives the tightest upper bound for decomposable MDPs with discounted rewards over infinite horizon, which is a more general case where the action space does not necessary decompose along the component action spaces (see remark 2). Their approach and our approach are both based on the linear formulation in terms of moments for Markov Decision Process (4.13).</p><p>When the decision maker has access to the system's state, the predictive maintenance problem with capacity constraints corresponds to the restless multi-armed bandit problem in finite horizon, which is a special case of weakly coupled dynamic program. Such problem is known to be PSPACE-hard <ref type="bibr" target="#b113">[114]</ref>. The usual heuristic policies to solve restless multi-armed bandit problem are the index policies introduced by Gittins <ref type="bibr" target="#b49">[50]</ref>. An index policy consists in computing an index for each arm separately and selecting the arms with the highest indices. It enables to decompose the computation. Whittle index policy <ref type="bibr" target="#b161">[162]</ref> is the most used policy, which is practically efficient in various applications. Other index policies based on polyhedral approaches have been proposed <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b110">111]</ref>.</p><p>In fact, the predictive maintenance problem with capacity constraints is at least as difficult as the restless multi-armed bandit problem. In addition to the curse of dimensionality, the fact that the system is partially observable represents a second challenge. To address this challenge, new index policies have been proposed <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b66">67,</ref><ref type="bibr" target="#b103">104]</ref>. Their approach are based on the fact that restless partially observable multi-armed bandit is restless multi-armed bandit where the state space of each component is its belief state space. However, the results proposed hold when the state spaces and observation spaces contain at most two elements.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Part II Integer programming for influence diagrams 6 Maximum Expected Utility in influence diagrams</head><p>This chapter introduces Influence Diagrams, which form a flexible tool that enables to model discrete stochastic optimization problems, including Markov Decision Processes (MDPs) and Partially Observable MDPs (POMDPs) as standard examples. More precisely, given random variables considered as vertices of an directed acyclic graph, a directed probabilistic graphical model defines a joint distribution via the conditional distributions of vertices given their parents. In influence diagrams, the random variables are represented by the set of vertices of an acyclic directed graph that is partitioned into three types of vertices: chance, decision and utility vertices. It is assumed that the probability distributions of the chance and utility vertices conditionally to their parents are known. The decision maker chooses the probability distribution of the decision vertices conditionally to their parents in order to maximize the expected utility. Through examples, we show the modeling power of the influence diagrams and we describe the main results and solution algorithms we propose to solve such a maximization problem. These results have been partially published in Parmentier et al. <ref type="bibr" target="#b116">[117]</ref>. Several results have been added in this dissertation after the publication of the paper.</p><p>Chapter 6 is organized as follows:</p><p>• Section 6.1 recalls the definition of directed graphical model and the maximum expected utility problem in influence diagrams. We also provide examples of discrete stochastic optimization problem that can be modeled using influence diagrams. • Section 6.2 introduces the key notions required to read the main results of Part II. • Section 6.3 states the main results of Part II. First, we present an mixed-integer linear formulation for solving exactly the maximum expected utility problem. Second, we introduce valid inequalities for our formulation, which lead to a computationally efficient algorithm. Third, we show that the linear relaxation of our integer formulation yields optimal integer solutions for instances that can be solved by the "single policy update," the standard algorithm for addressing influence diagrams.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">The Influence Diagrams</head><p>Now we recall the framework of influence diagrams (more details can be found in Koller and Friedman <ref type="bibr" target="#b74">[75,</ref><ref type="bibr">Chapter 23]</ref>). We choose to use the terminology and notation of the probabilistic graphical model literature <ref type="bibr" target="#b155">[156]</ref> instead of that of graph theory or combinatorial optimization.</p><p>Let G = (V, A) be a directed graph with a set of vertex V and a set of oriented arcs A. A parent (resp. child) of a vertex v is a vertex u such that (u, v) (resp. (v, u)) belongs to A; we denote by pa(v) the set of parents vertices (resp. ch(v) the set of children vertices). The family of v, denoted by fa(v), is the set {v} ∪ pa(v).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.1">The framework of parametrized influence diagram</head><p>Let G = (V, A) be an acyclic directed graph, and, for each vertex v in V , let X v be a random variable taking value in a finite state space X v . For any C ⊆ V, let X C denote (X v ) v∈C and X C be the Cartesian product X C = v∈C X v . A directed probabilistic graphical model (or more concisely directed graphical model) is an acyclic directed graph G and the collection of probability distributions P of the random vector X V such that there exists a collection of conditional probability distributions p v|pa(v) v∈V satisfying</p><formula xml:id="formula_156">P(X V = x V ) = v∈V p v|pa(v) (x v |x pa(v) ). (6.1)</formula><p>When a probability distribution P satisfies (6.1), we say that P factorizes according to G. When P factorizes according to G, then</p><formula xml:id="formula_157">P X v = x v |X pa(v) = x pa(v) = p v|pa(v) (x v |x pa(v)</formula><p>). The following fundamental property holds: Given a collection of conditional distributions p v|pa(v) v∈V , Equation (6.1) uniquely defines a probability distribution on X V .</p><p>Let (V a ,V c ,V r ) be a partition of V where V c is the set of chance vertices, V a is the set of decision vertices, and V r is the set of utility vertices. The utility vertices have no descendants.</p><p>For ease of notation we denote by V s the union of V c and V r . Letters a, r, and s respectively stand for action, reward, and stochastic in V a , V r , and V s . An influence diagram is a directed acyclic graph G = (V, A) together with a partition V = V a ∪ V c ∪ V r . For convenience, we will sometimes denote an influence diagram by G = (V s ,V a , A). Consider a set of conditional probability distributions p = p v|pa(v) v∈V c ∪V r , and a collection of reward functions r = {r v } v∈V r , with r v : X v → R. We define a Parametrized Influence Diagram (PID) as the quadruplet (G, X V , p,r).</p><p>This notion has not been introduced in the literature. We introduce it to distinguish properties due to the parametrization from properties due to the graph itself. We will sometimes refer the parameters (X V , p,r) as ρ for conciseness.</p><p>Let ∆ v denote the set of conditional probability distributions δ v|pa(v) on X v given X pa(v) . Given a collection of conditional probability distributions p, a strategy δ in ∆ = v∈V a ∆ v uniquely defines a probability distribution P δ on X V through</p><formula xml:id="formula_158">P δ (X V = x V ) = v∈V s p v|pa(v) (x v |x pa(v) ) v∈V a δ v|pa(v) (x v |x pa(v) ). (6.2)</formula><p>The vector δ v|pa(v) of vertex v in V a is the policy in v. Note that the probability P δ factorizes according to G since it satisfies (6.1). Let E δ denote the expectation of X V according to P δ . We denote by MEU(G, ρ) the Maximum Expected Utility problem associated to the PID (G, ρ), which is defined as follows:</p><formula xml:id="formula_159">max δ∈∆ E δ v∈V r r v (X v ) .</formula><p>MEU(G, ρ)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">The Influence Diagrams</head><p>A strategy δ ∈ ∆ d ⊂ ∆ is deterministic if, for every v ∈ V a , and every</p><formula xml:id="formula_160">x v , x pa(v) ∈ X v × X pa(v) , δ v|pa(v) (x v |x pa(v)</formula><p>) is a Dirac measure. Hence, every strategies δ in ∆ d satisfies</p><formula xml:id="formula_161">δ v|pa(v) (x fa(v) ) ∈ {0, 1}, ∀x fa(v) ∈ X fa(v) , ∀v ∈ V a . (6.3)</formula><p>It is well known that there always exists an optimal solution to MEU(G, ρ) that is deterministic <ref type="bibr" target="#b87">[88,</ref><ref type="bibr">Lemma C.1]</ref>.</p><p>Solving MEU(G, ρ) is difficult for two reasons. First, evaluating a given strategy is already difficult. The difficulty of evaluating a strategy is the difficulty of solving the inference problem on the underlying probabilistic graphical model. Given a feasible strategy δ, a subset of vertices C ⊆ V and x C in X C , the inference problem consists in computing the marginal probability of</p><p>x C according to P δ , i.e., P δ (X C = x C ). This a special case of the inference problem in a directed graphical model and it is well-known that such a problem is NP-hard <ref type="bibr" target="#b29">[30]</ref>. A good indicator of the difficulty of solving the inference problem is the treewidth of the graph <ref type="bibr" target="#b128">[129]</ref>. Mauá et al. <ref type="bibr" target="#b97">[98]</ref> showed that when the underlying graph has a bounded treewidth, the inference problem becomes polynomial, and then evaluating a strategy becomes also polynomial.</p><p>Second, optimizing over the set of strategies ∆ is also difficult. Mauá et al. <ref type="bibr" target="#b97">[98,</ref><ref type="bibr">Theorem 4]</ref> showed that even with a treewidth of 2, MEU(G, ρ) is NP-hard. It means that even when the inference problem is polynomial, MEU(G, ρ) is NP-hard.</p><p>Remark 8. A common practice in the literature is to define the reward function of the utility vertices v ∈ V r as deterministic functions f (x pa(v) ) of their parents. This case can of course be modeled in our setting: for each v in V r , it suffices to define state spaces X v = X pa(v) , conditional probabilities p v|pa(v) (x v |x pa(v) ) to be equal to 1 if x v = x pa(v) and 0 otherwise, and reward functions r (x v ) := f (x v ).</p><p>Remark 9. In an influence diagram G = (V s ∪ V a , A), one says that there is perfect recall of previous actions, when, there exists an ordering of V a , say {v 1 , . . . , v m }, consistent with the partial order defined by the directed acyclic graph G, such that the set of parents of each decision vertex contains the preceding decision vertices and their parents in this ordering, that is, fa(v j ) ⊆ pa(v i ) for any j &lt; i . In the absence of perfect recall, many authors have used the expression Limited Memory Influence Diagram (LIMID) to characterize the corresponding influence diagram <ref type="bibr" target="#b80">[81]</ref>. In this thesis, we consider the general case of LIMIDs but we refer to them as Influence Diagrams throughout the thesis, following the convention adopted in Koller and Friedman <ref type="bibr" target="#b74">[75,</ref><ref type="bibr">Chapter 23]</ref>. Many natural situations such as POMDPs have the perfect-recall property.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2">Examples</head><p>Now we introduce some examples, shown in Figure <ref type="figure" target="#fig_44">6</ref>.1, and Figure <ref type="figure" target="#fig_44">6</ref>.2, which illustrate how we can model stochastic optimization problems using influence diagrams. In particular, the POMDPs with memoryless policy presented in Chapter 4 is a special case of influence diagrams and P ml can be read as MEU(G, ρ) on POMDPs. <ref type="foot" target="#foot_7">1</ref> To represent an influence diagram we use the convention of Koller and Friedman <ref type="bibr" target="#b74">[75]</ref>: the chance vertices, decision vertices and utility vertices are respectively represented by circles, squares and diamond.</p><p>Example 4. Consider a maintenance optimization problem in which at time t a machine is in degradation state S t . The action A t taken by the decision maker according to the current state is typically a binary decision that is to either perform maintenance on it or not. The problem is considered over a finite horizon with scheduled maintenance slots. State and decision together lead to a new (random) state S t +1 , and the triple (S t , A t , S t +1 ) induces a reward R t . This is an example of an MDP which is probably the simplest type of influence diagram, represented in Figure <ref type="figure" target="#fig_44">6</ref>.1a. In Part I, we described a more complex problem, where the actual state S t of the machine is often not known, and the decision maker has access instead to an observation O t that only carries partial information about the state, which leads to a POMDP. As illustrated in Figure <ref type="figure" target="#fig_44">6</ref>.1b, we model the decision using a memoryless policy, i.e., the decision A t is taken based on observation O t .</p><p>Example 5. Figure <ref type="figure" target="#fig_44">6</ref>.2a depicts an influence diagram modeling the media investment strategy of a political party for the next elections. The national committee starts in a n by deciding how much to invest into national media coverage and which budget it gives to regional committees. Based on the national popularity rating v n after the interventions on national media, regional committees i decide which fraction a r i of their funds they allocate to regional media and local committees. Based on regional popularity rating v r i after the interventions on regional media, each local committee j decides how much to invest in local meetings and local media a j . The goal consists in maximizing the expected total number of local elections r j won.</p><p>Example 6. Consider two chess players : Alice and Bob. They are used to play chess and for each game they bet a symbolic coin. However, they can decline to play. Suppose that Alice wants to play chess every day. <ref type="foot" target="#foot_9">2</ref> In this context, the decision maker is Bob. On day t , Alice has a current confidence level S t . The day of the game, based on her current confidence level, Alice has a certain level of motivation denoted by M t . When Bob meets with Alice, Bob makes the decision to play depending on Alice's demeanor, denoted U t , which depends on her current level of motivation. Then Bob can accept or decline the challenge, and his decision is denoted by A t . We denote by V t the winner (getting a reward r t ). If Bob declines the challenge, there is no winner and no reward. Then, Alice's next confidence level is affected by the result of the game and her previous confidence level. This stochastic decision problem can be modeled as the maximum expected utility problem in the influence diagram shown in Figure <ref type="figure" target="#fig_44">6</ref>.2b.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Junction Trees and moments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1">Junction Trees and Rooted Junction Trees (RJTs)</head><p>In order to state the main results of Part II, we recall the notion of junction tree and we introduce the new notion of Rooted Junction Tree (RJT). The moralized graph G = (V , E ) of a directed graph G = (V, A) is the undirected graph defined by</p><formula xml:id="formula_162">V = V and (u, v) ∈ E if (u, v) ∈ A or ch(u) ∩ ch(v) = .</formula><p>We denote by M (G) = (V, M (A)) the moralized graph of G. By definition, the families of G are cliques of M (G).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Junction Trees and moments</head><formula xml:id="formula_163">s 1 r 1 a 1 s 2 r 2 a 2 s 3 r 3 a 3 s 4</formula><p>(a) A Markov decision process (MDP)   Junction tree. Let G = (V, A) be a directed acyclic graph. An undirected graph T = (V, E) with V ⊆ 2 V is a junction tree on G if:</p><formula xml:id="formula_164">s 1 o 1 r 1 a 1 s 2 o 2</formula><formula xml:id="formula_165">a n v n a r 1 v r 1 a r 2 v r 2 a 1 a 2 a 3 r 1 r 2 r 3 (a) Investment for local elections s 1 o 1 u 1 a 1 v 1 r 1 s 2 o 2 u 2 a 2 v 2 r 2 s 3 o 3 u 3 a 3 v 3</formula><formula xml:id="formula_166">(i) C ∈V C = V .</formula><p>(ii) For every edge in (u, v) ∈ M (A), there is C ∈ V such that {u, v} ⊆ C . (iii) T is a tree.</p><p>(iv) T satisfies the running intersection property, i.e., given two vertices C 1 and C 2 in V, any vertex C on the unique undirected path from</p><formula xml:id="formula_167">C 1 to C 2 in T satisfies C 1 ∩C 2 ⊆ C</formula><p>Rooted Junction Tree. Let G = (V, A) be a directed acyclic graph and let T = (V, E) be a junction tree on G together with a root. For any v ∈ V , let T v be the subgraph of T induced by the vertices C of V, which are also called clusters, containing v. The running intersection property ensures that T v is a tree. Let G = (V, A) be the directed rooted tree obtained by orienting the edges of the tree T from the root to the leafs. It also defines a rooted subtree (i) its underlying undirected graph is a junction tree on G, (ii) for all v ∈ V , we have fa(v) ⊆ C v .</p><formula xml:id="formula_168">G v = (V v , A v )</formula><p>Let G be an RJT on G, and v a vertex of V . Given C ∈ V, let the offspring of C be defined by In Chapter 7, we will describe the main properties of the RJT and the benefits of introducing it to model MEU(G, ρ).</p><formula xml:id="formula_169">offspring(C ) = {v ∈ V : C v = C },</formula><p>The width of a junction tree T = (V, A) corresponds to its maximal cluster size minus one and is denoted by w(T ), i.e., w(T</p><formula xml:id="formula_170">) = max C ∈V |C |-1.</formula><p>The width of a rooted junction tree G is the width of its underlying undirected graph and is also denoted by w(G). The treewidth (resp. rooted treewidth) of a graph G is the minimum width over all possible junction trees (resp. RJTs) of G.</p><p>We denote respectively by w * (G) and w * rt (G) the treewidth of G and the rooted treewidth of G. Although ({V }, ) is an RJT and many others gradual RJTs can be build easily, the concept of RJT has only practical interest if it is possible to construct RJTs with small width. In Section 7.5.1, we introduce an heuristic algorithm that builds an gradual RJT with a controlled width.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2">The moments on RJTs</head><p>To solve MEU(G, ρ) we introduce the notion of moments on a rooted junction tree of an influence diagram. This moments will be use as variables in the next sections. Let G = (V, A) be</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Main results</head><p>an influence diagram, and ρ a parametrization on G. Let δ be a feasible strategy on G. The moment µ C of a subset of variables X C with C ⊆ V corresponds to the expected value of the indicator function of a value x C in X C according to P δ , i.e.,</p><formula xml:id="formula_171">µ C (x C ) := E δ 1 {X C =x C } (x C ) = P δ (X C = x C ).</formula><p>We introduce the notation µ = µ C (x C ) x C ∈X C ,C ∈V . Given an RJT G = (V, A) of G and a parametrization ρ on G, we introduce the set of achievable moments on G of G by a strategy in ∆:</p><formula xml:id="formula_172">M G G, ρ = µ ∈ R C ∈V X C : ∃δ ∈ ∆, ∀C ∈ V, µ C (x C ) = P δ (X C = x C ) .</formula><p>Similarly, we introduce the set of achievable deterministic moments M d G G, ρ where we replaced ∆ by ∆ d above. When ρ and G are clear from the context, we write more compactly M G and M d G .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.3">The value functions on RJTs</head><p>We introduce new variables to build an alternative integer program. Let G = (V, A) be an influence diagram, G = (V, A) a gradual RJT of G, and ρ a parametrization on G. For convenience we extend the definition of the reward function on G as follows</p><formula xml:id="formula_173">r C (x C ) = r v (x v ) if C = C v and v ∈ V r , 0 otherwise. (<label>6.4)</label></formula><p>Let δ be a feasible strategy on G. The value function λ C of a subset of variables X C with C ⊆ V corresponds to the expected reward on the subtree G C induced by root C given a value x C in X C according to P δ , i.e.,</p><formula xml:id="formula_174">λ C (x C ) := E δ C ∈V C r C (x C )|X C = x C .</formula><p>We introduce the notation λ = (λ C (x C )) x C ∈X C ,C ∈V . Given an RJT G = (V, A) of G and a parametrization ρ on G, we introduce the set of achievable value functions on G by a strategy in ∆ as F G (G, ρ)</p><p>as follows:</p><formula xml:id="formula_175">F G G, ρ = λ ∈ R C ∈V X C : ∃δ ∈ ∆, ∀C ∈ V,λ C (x C ) = E δ C ∈V C r C (x C )|X C = x C .</formula><p>Similarly, we introduce the set of achievable deterministic value functions F d G G, ρ where we replaced ∆ by ∆ d above. When ρ and G are clear from the context, we write more compactly F G and F d G .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Main results</head><p>Now we state the main results of Part II. In this section, we consider a PID (G, ρ). We denote by G = (V, A) an gradual RJT on G. To keep notations light in this chapter, we write the type of sum x C \C µ C (x C ) more compactly as x C \C µ C for any vertex C ,C in V and x C \C ∈ X C \C . In addition, we introduce the notation 〈r v ,</p><formula xml:id="formula_176">µ v 〉 = x v ∈X v r v (x v )µ v (x v )</formula><p>for any vertex v in V r .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.1">Integer programs using moments on G</head><p>Non linear program for MEU(G, ρ). We now introduce the following non linear program.</p><formula xml:id="formula_177">max µ,δ v∈V r 〈r v , µ v 〉 s.t. x C µ C = 1, ∀C ∈ V x C \C µ C = x C \C µ C , ∀ C ,C ∈ A µ Čv = x v µ C v ∀v ∈ V µ C v = µ Čv p v|pa(v) ∀v ∈ V s µ C v = µ Čv δ v|pa(v) ∀v ∈ V a µ 0, δ ∈ ∆ (6.5a) (6.5b) (6.5c) (6.5d) (6.5e) (6.5f) (6.5g)</formula><p>In Program (6.5), all the equalities should be understood functionally, e.g.,</p><formula xml:id="formula_178">µ C v = µ Čv p v|pa(v) for all v ∈ V s means that µ C v (x C v ) = µ Čv (x Čv ) p v|pa(v) (x v |x pa(v) ), ∀x C v ∈ X C v , for all v ∈ V s .</formula><p>We will use such functional (in)equalities throughout this dissertation. Now we state the first result of Part II which will be proved in Chapter 8. Thanks to the properties of probability distributions, given a strategy δ, the vector of moments of P δ satisfies the constraints of NLP (6.5). Conversely, given a feasible solution of NLP (4.1), Theorem 6.1 ensures that µ is the vector of moments of P δ . Theorem 6.1. Let (µ, δ) be a feasible solution of NLP (6.5). Then µ is the vector of moments of P δ induced by δ. Furthermore, µ, δ is an optimal solution of NLP (6.5) if and only if δ is an optimal strategy of MEU(G, ρ). In particular, NLP (6.5) and MEU(G, ρ) have the same optimal value.</p><p>As an immediate consequence, Theorem 6.1 ensures that: a vector µ belongs to the set of achievable moments M(G) if and only if there exists a strategy δ in ∆ such that (µ, δ) is a feasible solution of NLP (6.5). As we will see in Chapter 7, the key assumption ensuring the validity of Theorem 6.1 is the fact that G is an RJT. Note that Theorem 6.1 extends Theorem 4.1 (in Chapter 4), to any influence diagrams. If we write the NLP (6.5) on POMDP with memoryless policy of Chapter 4 we obtain NLP (4.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mixed-integer linear formulation for MEU(G, ρ).</head><p>The nonlinearity of NLP (6.5) comes from bilinear constraints (6.5f). Given two continuous variables z, y 0 and x ∈ {0, 1}, we introduce the notation McCormick z = x y which indicates that we replace the bilinear constraint z = x y by the McCormick's linear inequalities <ref type="bibr" target="#b99">[100]</ref>. Since there always exists an optimal strategy of MEU(G, ρ) that is deterministic, we can turn NLP (6.5) into the following MILP using McCormick's inequalities without changing its optimal value. max µ,δ v∈V r 〈r v , µ v 〉 s.t. µ satisfies (6.5b) -(6.5e) The natural lower and upper bounds on the moments induced by any strategy δ are respectively 0 and 1. In Chapter 8, we describe an approach to derive tighter bounds on the moments of a distribution that strengthen the linear relaxation of MILP 6.6.</p><formula xml:id="formula_179">McCormick µ C v = µ Čv δ v|pa(v) ∀v ∈ V a δ ∈ ∆ d (6.</formula><p>The approach obviously has limitations. Indeed, any exact method to solve MEU(G, ρ) must compute the exact value of E δ v∈V r r v (X v ) when it evaluates a strategy δ. Since the exact algorithms to solve the inference problem are exponential in the treewidth w * (G), this type of method is limited in practice to graphs with moderate treewidth. The approach to solving MEU(G, ρ) relies on the rooted junction trees that we introduced, and is therefore practically limited to influence diagrams with moderate rooted treewidth w * rt (G). This is an additional limitation since w * rt (G) can be significantly larger than w * (G). Indeed, even if w * (G) is bounded, w * rt (G) can be unbounded. We however show that the approach works well on applications for which w * rt (G) is of the same order of magnitude as w * (G) like those considered in Examples 4, 5 and 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.2">Valid cuts for the MILP</head><p>Now we introduce valid inequalities for MILP <ref type="bibr">(6.6)</ref>. The technique we introduce here systematizes our approach in Section 4.2 to construct valid inequalities for MILP (4.7). Actually using a suitable gradual RJT for POMDP with memoryless policies we could recover exactly the valid inequalities (4.8).</p><p>Let G = (V, A) be an influence diagram and C a subset of vertices in V . A set of variables X D such that D ⊆ C is strategy independent set in C if it satisfies the following property:</p><p>For every parametrization ρ such that (G, ρ) is a PID, P δ (X D |X C \D ) does not depend on δ.</p><p>For convenience, we say that D is strategy independent in C when X D is strategy independent in C . It turns out the following lemma establishes a fundamental stability property. Let G = (V, A) be a gradual RJT of G. We introduce the following linear equalities:</p><formula xml:id="formula_180">µ C = µ C ⊥ ⊥ p C ⊥ ⊥ |C ⊥ ⊥ , ∀C ∈ V, (6.7)</formula><p>Now, we state the second result of Part II. Proposition 6.4. Equalities (6.7) are valid for MILP (6.6).</p><p>In addition, we will show in Chapter 8 that the valid cuts (6.7) are the strongest linear equalities we can obtain of the form µ C = µ C \D p D|C \D where D ⊆ C . Again, note that Proposition 6.4 extends Proposition 4.3 to any influence diagrams. Given a carefully chosen RJT on POMDP with memoryless policies, equalities (6.7) correspond to equalities (4.8).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.3">Integer programs using value functions</head><p>Without loss of generality, we assume that G has a single root vertex. Otherwise, we add an isolated dummy vertex v 0 to the original influence diagram, which we allow us to extend the RJT by adding the cluster vertex C 0 = {v 0 }. If v 0 ∉ V s , then we can add a vertex v 0 and a cluster vertex C 0 = {v 0 }, the random variable X v 0 equals to 1 almost surely, and an arc (C 0 ,C 0 ). Hence, without loss of generality, we assume that v 0 ∈ V s . For simplicity, we denote by x 0 := x v 0 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Non linear program for MEU(G, ρ). We now introduce the following non linear program</head><formula xml:id="formula_181">. max λ,δ 〈p 0 , λ C 0 〉 s.t. λ C v = r C v + u∈V s : C u ∈V s ∩ch(C v ) x u λ C u p u|pa(u) + u∈V a : C u ∈V s ∩ch(C v ) x u λ C u δ u|pa(u) , ∀v ∈ V δ ∈ ∆ (6.8a) (6.8b) (6.8c)</formula><p>Now we state the third result of Part II, which will be proved in Chapter 8. Thanks to the properties of the conditional expectation, the vector of value functions of P δ satisfies the constraints of NLP (6.8). Conversely, given a feasible solution (λ, δ) of NLP (6.8), Theorem 6.5 ensures that λ is the vector of value functions of P δ . Theorem 6.5. Let (λ, δ) be a feasible solution of NLP (6.8). Then λ is the vector of value functions of the probability distribution P δ induced by δ. Furthermore, (λ, δ) is an optimal solution of NLP (6.8) if and only if δ is an optimal policy of MEU(G, ρ). In particular, NLP (6.8) and MEU(G, ρ) have the same optimal value.</p><p>As an immediate consequence, Theorem 6.5 ensures that: a vector λ belongs to the set of achievable value functions F(G) if and only if there exists a strategy δ in ∆ such that (λ, δ)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Main results</head><p>is a feasible solution of NLP (6.8). Theorem 6.5 extends Theorem 8.17 (in Chapter 4) to any influence diagrams. If we write the NLP (6.8) on POMDP with memoryless policy of Chapter 4, we obtain NLP (4.18).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Mixed-integer linear formulation for MEU(G, ρ).</head><p>The nonlinearity of NLP (6.8) comes from the bilinear terms λ C u δ u|pa(u) in constraints (6.8b). Since there always exists an optimal strategy of MEU(G, ρ) that is deterministic, we can turn NLP (6.8) into a MILP using McCormick's inequalities. To do so we introduce variables α = (α C (x C )) x C ∈X C ,C ∈V and the following MILP.</p><formula xml:id="formula_182">max λ,α,δ 〈p 0 , λ C 0 〉 s.t. λ C v = r C v + u∈V s : C u ∈V s ∩ch(C v ) x u λ C u p u|pa(u) + u∈V a : C u ∈V s ∩ch(C v ) x u α C u , ∀v ∈ V McCormick α C v = λ C v δ v|pa(v) , ∀v ∈ V a δ ∈ ∆ d (6.9)</formula><p>Given a strategy δ ∈ ∆, it follows from Theorem 6.5 that (λ, δ) is a feasible solution of MILP (6.6)</p><p>if and only if λ belongs to F d G . Note that MILP (6.9) generalizes MILP (4.21) described in Section 4.1.2 to any influence diagrams. To write McCormick's inequalities, it requires to compute lower and upper bounds on the value functions that only depend on parameters ρ. The natural lower and upper bounds on the value function induced by any strategy δ are respectively v∈V r min x v r v (x v ) and v∈V r max x v r v (x v ). In Chapter 8 we show how to improve these bounds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.4">Polynomial cases of Influence Diagrams</head><p>Soluble influence diagrams. The third result in Part II is based on the notion of soluble influence diagrams. To define a soluble influence diagram, we need to introduce the notion of local optimum strategy. Consider a PID (G, ρ) with G = (V, A), V = V s ∪ V a and a parametrization ρ. Given a strategy (δ u ) u∈V a and a decision vertex v, we denote by δ -v the partial strategy (δ u ) u∈V a \v . A strategy δ is a local optimum if</p><formula xml:id="formula_183">δ v ∈ arg max δ v ∈∆ v E δ v ,δ -v u∈V r r u (X u ) for each vertex v in V a .</formula><p>It is a global optimum if it is an optimal solution of MEU(G, ρ). The problem of finding a local optimum is "easy" in the following sense: Suppose that we have an oracle that gives us the result of the inference problem in polynomial time, then a local optimum can be computed in polynomial time <ref type="bibr" target="#b74">[75,</ref><ref type="bibr">Proposition 23.2]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 6.2. An influence diagram G is soluble if for every parametrization ρ of G, every local optimum is a global optimum.</head><p>Because of the remark above, MEU(G, ρ) is "easy" to solve when G is soluble. Alternate definitions of soluble influence diagrams are available in the literature (see Section 6.4). In the literature, soluble influence diagrams are also defined using tools of graphical models, which are introduced in Chapter 7. Thanks to these definitions, the following proposition is common knowledge in the literature. Proposition 6.6. Deciding if an influence diagram G is soluble can be done in polynomial time.</p><p>Now, we state our fourth result, which provides a link between a soluble influence diagrams and MILP (6.6).</p><p>Theorem 6.7. If G is soluble, then there exists an RJT, such that, for every parametrization ρ, an optimal solution of the linear relaxation of MILP (6.6) with the valid inequalities (6.7) induces an optimal solution of MEU(G, ρ) and both problems have the same optimal values. Such an RJT can be computed in polynomial time.</p><p>Theorem 6.7 confirms that, when the inference problem is tractable (small rooted-treewidth), solving MEU(G, ρ) is tractable for every parametrization ρ when G is soluble.</p><p>As mentioned for Theorem 6.1, given a parametrization ρ and a RJT G the set of achievable moments M G (G, ρ) fully characterizes the solutions of NLP (6.5), which is a Quadratically Constrained Quadratic Program (QCQP). Hence, the convexity of the set M G G, ρ is a measure of the ability to solve (6.5). Our fifth result provides a characterization of soluble influence diagrams in terms of convexity of the set M G G, ρ . Theorem 6.8. An influence diagram G is soluble if and only if there exists an RJT G such that for every parametrization ρ on G, the set of achievable moments M G G, ρ is a polytope. In this case, such an rooted junction tree G can be computed in polynomial time.</p><p>While Theorem 6.8 provides a necessary and sufficient condition on influence diagrams for being soluble, Theorem 6.7 gives only a necessary condition. In fact, it turns out the linear relaxation of MILP (6.6) with valid inequalities (6.7) provides optimal solutions for a slightly larger class of influence diagrams, including some PID with a non-convex set of achievable moments. In Chapter 9, we describe an example showing this phenomenon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.5">Dual formulations for the linear relaxations.</head><p>The linear relaxations of MILP (6.6) with and without valid inequalities (6.7) play a key role to derive bounds. It turns out the dual of these relaxations can be formulated using value functions variables in a linear formulation which is closely related to a linear formulation of dynamic programming algorithm Using the variables representing the value functions we introduce the two following linear programs:</p><formula xml:id="formula_184">min λ 〈λ C 0 , p 0 〉 s.t. λ C v r C v + u∈V s : C u ∈ch(C v ) x u λ C u p u|pa(u) + u∈V a : C u ∈ch(C v ) λ C u ∀v ∈ V.</formula><p>(6.10)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Bibliographical remarks</head><formula xml:id="formula_185">min λ 〈λ C 0 , p 0 〉 s.t. λ C v r C v + u∈V s : C u ∈ch(C v ) x u λ C u p u|pa(u) + u∈V a : C u ∈ch(C v ) x C ⊥ ⊥ u λ C u p C ⊥ ⊥ u |C ⊥ ⊥ u ∀v ∈ V. (6.11)</formula><p>Now, we state the sixth main result of Part II.</p><p>Theorem 6.9. The following properties hold:</p><p>(i) Linear program (6.10) is the dual of the linear relaxation of MILP (6.6) where variable δ has been removed. (ii) Linear program <ref type="bibr">(6.11)</ref> is the dual of the linear relaxation of MILP (6.6) with valid inequalities (6.7) where variable δ has been removed.</p><p>Furthermore, the strong duality holds in both cases.</p><p>Theorem 6.9 says that there is a duality relation between the moments variables and value functions variables. In addition, it follows from Theorem 6.9 that if G is soluble, then by Theorem 6.7 the linear program (6.11) induces also an optimal solution of MEU(G, ρ). Linear program <ref type="bibr">(6.11)</ref> is equivalent to the linear formulation of dynamic programming approach on the RJT G. In the case of MDPs, by writing linear program (6.11) we recover exactly the well-known linear formulation of dynamic programming on MDP (see, e.g., Puterman <ref type="bibr" target="#b123">[124]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Bibliographical remarks</head><p>Influence diagrams were introduced by Howard and Matheson <ref type="bibr" target="#b55">[56]</ref> [see also 57] to model stochastic optimization problems using a probabilistic graphical model framework. Originally, the decision makers were assumed to have perfect recall <ref type="bibr" target="#b61">[62,</ref><ref type="bibr" target="#b136">137,</ref><ref type="bibr" target="#b140">141]</ref> of the past actions.</p><p>Lauritzen and Nilsson <ref type="bibr" target="#b80">[81]</ref> relaxed this assumption and provided a simple (coordinate descent) algorithm to find a good strategy: the Single Policy Update (SPU) algorithm. By relaxing the perfect recall assumption, these authors referred the resulting influence diagrams as limited memory influence diagrams. However, we follow the convention of Koller and Friedman <ref type="bibr" target="#b74">[75]</ref> who still call them influence diagrams. In general, SPU finds a locally optimal strategy in a finite number of iterations, and requires to perform exact inference, so that it is therefore limited by the treewidth <ref type="bibr" target="#b24">[25]</ref>. Lauritzen and Nilsson <ref type="bibr" target="#b80">[81]</ref> also introduced a notion of soluble influence diagram using tools of directed graphical models. In particular, the authors proved that being soluble is a sufficient condition for SPU to converge to an optimal solution in a finite and polynomial number of iterations. Koller and Milch <ref type="bibr" target="#b75">[76]</ref> generalized their notion of soluble influence diagram to make this condition necessary and sufficient.</p><p>More recently, Mauá and Campos <ref type="bibr" target="#b94">[95]</ref> and Mauá and Cozman <ref type="bibr" target="#b95">[96]</ref> have introduced a new algorithm, Multiple Policy Update, which has both an exact and a heuristic version and relies on a concept of dominance to discard partial solutions. It can be interpreted as a generalization of SPU where several decisions are considered simultaneously. Later on, Khaled et al. <ref type="bibr" target="#b67">[68]</ref> proposed a similar approach, in the spirit of Branch-and-Bound, while Liu <ref type="bibr" target="#b87">[88]</ref> introduced heuristics based on approximate variational inference. Usually, inference computations in influence diagrams are done within "valuation algebra" on the pair probability-utility <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b61">62]</ref>, which is an abstract framework that facilitates computations in graphical models <ref type="bibr" target="#b138">[139]</ref>. Lee et al. <ref type="bibr" target="#b82">[83]</ref> propose an inference algorithm providing upper bounds on the MEU which uses the same "valuation algebra" for influence diagrams <ref type="bibr" target="#b34">[35]</ref>. Even if the inference computations in influence diagrams are commonly done using valuation algebra, we use instead the marginal polytope because it is useful for mathematical programming approaches <ref type="bibr" target="#b147">[148,</ref><ref type="bibr" target="#b155">156]</ref>.</p><p>Finally, the problem of solving an influence diagram can be polynomially transformed into a "maximum a posteriori" (MAP) problem, which is well-known in the graphical model community. Hence, it can be solved using popular MAP solvers such as toulbar2 <ref type="bibr" target="#b57">[58]</ref>. For further details about the transformation, see Antonucci and Zaffalon <ref type="bibr" target="#b3">[4]</ref>, Cano et al. <ref type="bibr" target="#b20">[21]</ref> and Maua <ref type="bibr" target="#b93">[94]</ref>.</p><p>Finding an optimal strategy for an influence diagram has been shown to be NP-hard even when restricted to influence diagrams of treewidth no greater than two, or to trees with binary variables <ref type="bibr" target="#b96">[97,</ref><ref type="bibr" target="#b98">99]</ref>. Note that even obtaining an approximate solution is also NP-hard <ref type="bibr" target="#b96">[97]</ref>.</p><p>Beyond the classical linear programming formulation for MDPs, mathematical programming formulations have been proposed for some special cases of influence diagrams, including decomposable or weakly coupled MDPs <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b54">55]</ref> and POMDP with perfect recall and short horizon <ref type="bibr" target="#b4">[5]</ref>. As noted in the first part, the special case of POMDP with memoryless policies extends the work of Bertsimas and Mišić <ref type="bibr" target="#b14">[15]</ref> to POMDPs since it also relies on variables that corresponds to moments or distributions. The variables of the other formulations correspond to time averages <ref type="bibr" target="#b16">[17]</ref> or value functions <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b54">55]</ref>, which makes these formulations harder to generalize to influence diagrams.</p><p>Credal networks are generalizations of probabilistic graphical models where the parameters of the model are not known exactly. MILP formulations for credal networks that could be applied to influence diagrams have been introduced by de Campos and Cozman <ref type="bibr" target="#b31">[32]</ref>, de Campos and Ji <ref type="bibr" target="#b32">[33]</ref>. However, the number of variables they require is exponential in the pathwidth, which is non-smaller and can be arbitrarily larger than the width of the tree we are using <ref type="bibr" target="#b132">[133,</ref><ref type="bibr">Theorem 4]</ref>, and the linear relaxation of their MILP is not as good as the one of the MILP we propose, and does not yield an integer solution on soluble influence diagrams.</p><p>Examples 4, 5 and 6 are sequential decisions problems in stochastic optimization. Many different solution approaches have been proposed for these kinds of problems under different names in different academic communities. While describing these approaches is beyond the scope of this thesis, we refer the interested reader to the tutorial of Powell <ref type="bibr" target="#b121">[122]</ref>. In particular, the literature on the POMDP example 4 has been detailed in Part I.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Graphical models and rooted junction tree properties</head><p>This chapter is dedicated to introduce first the tools and the intermediate results to prove Theorem 6.1, and second the algorithm mentioned in Section 6.2.1 to build a gradual RJT with a controlled width. As mentioned above, to evaluate a strategy δ in an influence diagram, we need to encode the probability P δ using a vector of moments. It raises the problem of characterizing the probability distributions factorizing on a directed graph. Once such a problem has been addressed the proof of Theorem 6.1 will be eased. Given a probability factorizing on a directed graphical model, we can derive the corresponding vector of moments and such a vector necessary satisfies some properties that we will describe in this chapter. However, the main difficulty in proving Theorem 6.1 is to understand how a vector of moments satisfying such properties is sufficient to encode a probability distribution factorizing on a directed graphical model.</p><p>This chapter is organized as follows:</p><p>• Section 7.1 introduces basic notation from graph theory. • Section 7.2 recalls the main theorem characterizing a probability distribution factorizing on a directed graphical model, in terms of conditional independences between the random variables. • Section 7.3 recalls the notion of junction tree and a "local" version of the conditional independences that is necessarily satisfied by a vector of moments in the marginal polytope of a junction tree. We show that these independences are not sufficient on a junction tree to ensure the factorization of the probability distribution. • Section 7.4 introduces several properties of RJTs. In particular, we show that the local independences on a RJT of Section 7.3 are sufficient to ensure a global factorization on a directed graphical model. • Section 7.5 introduces an algorithm that builds a gradual RJT with a controlled width. In addition, it provides a characterization of the gradual RJT built by the algorithm. This characterization will be useful for the proofs of Theorem 6.7 and Theorem 6.8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Graph notation</head><p>This section introduces notation for graphs, which are mostly those one commonly used in the combinatorial optimization community <ref type="bibr" target="#b134">[135]</ref>. A (simple) directed graph G is a pair (V, A)</p><p>where V is the set of vertices and A ⊆ V 2 the set of arcs. We write</p><formula xml:id="formula_186">u → v when (u, v) ∈ A. Let [k] := {1, . . . , k}. A path is a sequence of vertices v 1 , . . . , v k such that v i → v i +1 , for any i ∈ [k-1]. A</formula><p>path between two vertices u and v is called a u-v path. We write u G v to denote the existence of a u-v path in G, or simply u v when G is clear from context. We write u v if there is an arc</p><formula xml:id="formula_187">u → v or v → u. A trail is a sequence of vertices v 1 , . . . , v k such that v i v i +1 , for all i ∈ [k -1].</formula><p>A vertex u is an ancestor (resp. a descendant) of v if there exists a u-v path (resp. a v-u path). We denote respectively by anc(v) and des(v) the set of ancestors and descendants of v. Finally, let anc(v) = {v} ∪ anc(v), and des(v) = {v} ∪ des(v). For a set of vertices C , the parent set of C , again denoted by pa(C ), is the set of vertices u that are parents of a vertex v ∈ C . We define similarly fa(C ), ch(C ), anc(C ), anc(C ), des(C ), and des(C ). Note that we sometimes indicate in subscript the graph according to which the parents, children, etc., are taken. For instance, pa G (v) denotes the parents of v in G.</p><p>A cycle is a path v 1 , . . . , v k such that v 1 = v k . The underlying undirected graph is connected if there exists a path between any pair of vertices. An acyclic graph is a graph which has no cycle. An undirected graph is a tree if it is connected and acyclic. A directed graph is a directed tree if its underlying undirected graph is a tree. A rooted tree is a directed tree such that all vertices have a common ancestor referred to as the root of the tree. <ref type="foot" target="#foot_10">1</ref> In a rooted tree, all vertices but the root have exactly one parent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Directed graphical model</head><p>In this thesis, given three random variables X , Y , Z , the notation (X ⊥ ⊥ Y |Z ) P stands for "X is independent from Y given Z " according to the probability distribution P of the random vector (X , Y , Z ). We underline that (•) P corresponds to independence according to the probability distribution P, and should not be confused with the notation (•) G that is more frequently used in the literature and stands for d-separation according to the graph G.</p><p>As mentioned in Chapter 6, evaluating a strategy in an influence diagram requires to solve the inference problem in a directed graphical model, which is equivalent to compute the probability distribution P δ given a strategy δ. Since an influence diagram is a directed graphical model, we consider more generally probability distributions that factorize according to a directed graphical model. A well-known sufficient condition for a distribution to factorize as a directed graphical model is that each vertex is independent from its non-descendants given its parents as stated in the following fundamental proposition. Theorem 7.1. <ref type="bibr" target="#b74">[75,</ref><ref type="bibr">Theorem 3.1,</ref><ref type="bibr">p. 62]</ref>. Let X V be a random variable on X V . Then its distribution P factorizes according to a directed acyclic graph G = (V, A), i.e., P satisfies (6.1) if and only if</p><formula xml:id="formula_188">X v ⊥ ⊥ X V \des G (v) |X pa(v) P for all v in V. (7.1)</formula><p>Note that this result is sometimes considered as the counterpart of the theorem of Hammersley</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.">Moments on junction trees</head><p>and Clifford <ref type="bibr" target="#b74">[75,</ref><ref type="bibr">Theorem 4.2,</ref><ref type="bibr">p. 116]</ref> for directed graphical models. Theorem 7.1 plays a key role in the subsequent results of this chapter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3">Moments on junction trees</head><p>The goal of this section is to recall a result of probabilistic graphical model theory that explains the role of the vector of moments in directed graphical models. Such a result is key in proving Theorem 6.1. We start by introducing useful definitions required to present this result.</p><p>Given a probability distribution P on X V , we define the vector of moments µ = (µ</p><formula xml:id="formula_189">C (x C )) x C ∈X C ,C ∈V of P as follows: µ C (x C ) = x V \C P(X V = x V ) ∀x C ∈ X C , C ∈ V, (7.2)</formula><p>where V is a subset of 2 V . Given V ⊆ 2 V , we say that a vector µ derives from a probability distribution on X V if there exists P defined on X V such that µ satisfies (7.2). We denote by P µ such a probability distribution on X V . Given a junction tree T = (V, E), we define its associated marginal polytope</p><formula xml:id="formula_190">M 0 (T ) =      µ : µ C 0 and x C µ C (x C ) = 1, ∀x C ∈ X C , ∀C ∈ V,</formula><p>and</p><formula xml:id="formula_191">x C 1 \C 2 µ C 1 = x C 2 \C 1 µ C 2 , ∀ C 1 ,C 2 ∈ E,      , (<label>7.3)</label></formula><p>where, as before, T if and only if µ derives from a probability distribution P µ on X V . Moreover, this probability distribution is unique and defined by</p><formula xml:id="formula_192">x C 1 \C 2 µ C 1 is the vector x C 1 \C 2 ∈X C 1 \C 2 µ C 1 (x C 1 \C 2 , x C 1 ∩C 2 ) x C 1 ∩C 2 ∈X C 1 ∩C 2 . The constraints x C 1 \C 2 µ C 1 = x C 2 \C 1 µ C 2 in</formula><formula xml:id="formula_193">P µ (X V = x V ) = C ∈V µ C (x C ) S∈S µ S (x S )</formula><p>.</p><p>In fact, M 0 (T ) is usually called the local polytope (see e.g. Wainwright and Jordan <ref type="bibr" target="#b155">[156]</ref>) and the marginal polytope is the set of vector of moments deriving from a probability distribution over all the random variables. Thanks to Proposition 7.2, when T is a junction tree, the local polytope coincides with the marginal polytope. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4">Moments on rooted junction trees 7.4.1 Main properties</head><p>Thanks to Theorem 7.1, a necessary and sufficient condition for a probability distribution to factorize is to satisfy global independences <ref type="bibr">(7.1)</ref>. In this section, we show that we still have such a necessary and sufficient condition when we replace the global independences (7.1) by "local" independences introduced on an RJT G = (V, A) as follows: We say that a vector µ in the marginal polytope M 0 (G) satisfies "local" independences on G if for all C ∈ V, we have An important remark is that Theorem 7.3 is not true when we consider a junction tree instead of an RJT. Consider a probability distribution P factorizing according to a directed graphical model G = (V, A). Given a junction tree T = (V, E), we obtain that: The vector of moments of P necessarily belongs to the marginal polytope M 0 (T ) and satisfies <ref type="bibr">(7.4)</ref>. However, the reverse is not true. Indeed, a vector µ belonging to the marginal polytope M 0 (T ) and satisfying <ref type="bibr">(7.4)</ref> does not ensure that the unique probability distribution P µ from which µ is derived factorizes according to G. For instance, on the junction tree of In this section we present further technical results on RJT that are useful to prove Theorem 7.3. We start with generic properties of RJT.</p><formula xml:id="formula_194">X v ⊥ ⊥ X C \des(v) |X pa(v) µ C , for all v in V such that fa(v) ⊆ C . (<label>7</label></formula><p>Proposition 7.4. Let G be an RJT on G.</p><p>1. If there is a path from u to v in G, then there is a path from C u to C v in G. µ C and q v|pa(v) is defined as</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">If des</head><formula xml:id="formula_195">G (u) ∩ des G (v) = ,</formula><formula xml:id="formula_196">x C \fa(v) µ C</formula><p>x C \pa(v) µ C when the denominator is non-zero, and as 0 otherwise.</p><p>Proof. Let be a topological order on C such that u ∈ C \D and v ∈ D implies u v. Such a topological order exists since des(D) ∩C = D. We have</p><formula xml:id="formula_197">µ C = µ C \D v∈D P µ (X v |X u , u ∈ C , u ≺ v) = µ C \D v∈D P µ (X v |X pa(v) ),</formula><p>where the first equality is the chain rule and the second follows from the hypothesis of the lemma.</p><p>Proof of Theorem 7.3. Let G be an RJT on G. Let C 1 , . . . ,C n be a topological ordering on G, let C i = j i C j , and C &lt;i = C i \C i . Let τ be a vector of moments satisfying the hypothesis of the theorem, and for each v in V , let q v|pa(v) be equal to</p><formula xml:id="formula_198">x C \fa(v) τ Cv x C \pa(v) τ</formula><p>Cv if the denominator is non-zero, and to 0 otherwise. We show by induction on i that</p><formula xml:id="formula_199">µ C i = v∈C i q v|pa(v) is such that τ C i = x C i \C i µ C i for all i i .</formula><p>Suppose the result true for all j &lt; i , with the convention that µ 0 = 1. We immediately deduce from the induction hypothesis that τ</p><formula xml:id="formula_200">C i = x C i \C i µ C i for all i &lt; i . There only remains to prove that τ C i = x C &lt;i µ C i .</formula><p>By definition of an RJT, we have fa(offspring(C i )) ⊆ C i . Proposition 7.4 implies that des(offspring(C i )) ∩ C i ⊆ offspring(C i ). Indeed let u be in des(offspring(C i )) ∩ C i . Then there is a C i -C u path as u ∈ des(C i ), and a C u -C i path as u ∈ C i . Hence C u = C i and u ∈ offspring(C i ). In addition, τ satisfies (7.4). Hence, τ C i is a distribution on C i such that each vertex in offspring(C i ) is independent from its non-descendants given its parents. By applying Lemma 7.5 with D = offspring(C i ), we have</p><formula xml:id="formula_201">τ C i = τ C i \offspring(C i ) v∈offspring(C i ) q v|pa(v) . Let C j be the parent of C i in G, we have τ C i \offspring(C i ) = x C j \C i τ C j = x C &lt;i \C i µ C &lt;i ,</formula><p>the first equality coming from the fact that (τ C ) C ∈V belongs to the marginal polytope, and the second from the induction hypothesis. Thus,</p><formula xml:id="formula_202">x C &lt;i µ C i = x C &lt;i v∈V i q v|pa(v) = x C &lt;i \C i µ C &lt;i v∈offspring(C i ) q v|pa(v) = τ C i \offspring(C i ) v∈offspring(C i ) q v|pa(v) = τ C i ,</formula><p>which gives the induction hypothesis, and the theorem.</p><p>When the RJT is gradual, Theorem 7.3 together with Lemma 7.5 gives the following corollary.</p><p>Corollary 7.6. Let µ be a vector of moments in the marginal polytope of a gradual RJT G = (V, A) on G = (V, A). The probability distribution P µ on X V factorizes according to G if and only if for all v ∈ V , all x pa(v) in X pa(v) such that µ pa(v) (x pa(v) ) = 0, and all x C v \pa(v) , we have</p><formula xml:id="formula_203">µ C v (x C v ) = µ v|pa(v) (x v |x pa(v) ) µ Čv (x Čv ), where µ v|pa(v) (x v |x pa(v) ) := µ fa(v) (x fa(v) )</formula><p>µ pa(v) (x pa(v) ) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Jensen et al. [62, beginning of Section 4</head><p>] introduced the concept of strong junction tree which is similar to our concept of RJT, but they do not have the suitable properties for our approach.<ref type="foot" target="#foot_12">foot_12</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5">Building a gradual RJT</head><p>As mentioned in Chapter 6, the concept of rooted junction tree has only practical interest if it is possible to construct RJTs with a small width. In this section, we introduce an algorithm that builds a gradual RJT with a controlled width. Such a gradual RJT is minimal in a certain sense we describe here. In particular, we give a characterization of the gradual RJT built by the algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5.1">An algorithm to build a gradual RJT</head><p>We start by introducing a necessary condition of the RJTs. Any RJT must satisfy, for all u, v ∈ V,</p><formula xml:id="formula_204">the implication ∃w ∈ V s.t . C v C w and u ∈ fa(w) and C u C v ⇒ u ∈ C v ,<label>(7.5)</label></formula><p>where C</p><p>C denotes the existence of a C -C path in the RJT G considered. This notation will be used throughout this section. Indeed, since u ∈ C u and fa(w) ⊂ C w by definition, and since C u C v C w , the running intersection property implies u ∈ C v . This motivates Algorithm 4, a simple gradual RJT construction algorithm which propagates iteratively elements present in each cluster vertex to their parent cluster vertex, and which thereby produces an RJT which is minimal in the sense that the implication in <ref type="bibr">(7.5</ref>) is strengthened to an equivalence, as stated in Proposition 7. </p><formula xml:id="formula_205">C v ← fa(v) ∪ w:(v,w)∈A Čw 5: if Č v = then 6:</formula><p>u ← max Čv u is the maximal element of Čv ⊂ V according to 7:</p><formula xml:id="formula_206">A ← A ∪ (u, v) 8:</formula><p>end if 9: end for</p><formula xml:id="formula_207">10: A ← {(C u ,C v ) | (u, v) ∈ A } 11: Return G = (C v ) v∈V , A</formula><p>The algorithm proceeds as follows: Let be an arbitrary topological order on G, and max C denote the maximum of C for the topological order . The algorithm maintains a set C v for each vertex v, which coincide at the end of the algorithm with the vertices C v in the RJT produced. We recall that Č v is the set C v \{v}. As an illustration, for any topological order on the graph of the chess example of The following proposition shows that Algorithm 4 produces an RJT G = (V, A) which is minimal for , in the sense that it satisfies a converse of (7.5).</p><p>Proposition 7.7. Algorithm 4 produces an RJT such that the root vertex C v of v is C v , satisfying offspring(C v ) = {v}, that admits as a topological order, and such that (u ∈ C v ) ⇒ (u v). Moreover, its cluster vertices are minimal in the sense that</p><formula xml:id="formula_208">u ∈ C v ⇐⇒ ∃w ∈ V s.t . C v C w and u ∈ fa(w), C u C v . (7.6)</formula><p>Although Proposition 7.7 does not give a guarantee about the width of the RJT built by Algorithm 4, the equivalence (7.6) ensures that given a topological ordering the cluster vertices contain only the required vertices to ensure the running intersection property.</p><p>Remark 10. Algorithm 4 takes in input a topological order on G. For a practical use, we recommend to use Algorithm 7 in Appendix B, which builds simultaneously the RJT and a "good" topological order.</p><p>Proof of Proposition 7.7. Algorithm 4 obviously converges given that it has only a finite number of iterations. If G is not connected, the algorithm is equivalent to its separate application on each of the connected components, which each yield a tree. W.l.o.g., we prove properties of the algorithm under the assumption that G is connected. To simplify notations, we denote C v by C v , and check that it indeed corresponds to the root vertex of v.</p><p>We first prove that is a topological order on G. First, remark that (u</p><formula xml:id="formula_209">∈ C v ) ⇒ (u v). Indeed, if u ∈ C v , then either</formula><p>Step 4 of the algorithm ensures that u ∈ fa(v) and u v or u ∉ fa(v) and there exists x such that u ∈ C x and C v → C x . But by Step 6 of Algorithm 4, the fact that C v → C x entails that v is the maximal element of C x \{x} for the topological order, so that u ≺ v.</p><formula xml:id="formula_210">Furthermore, Step 6 ensures that (C u ,C v ) ∈ A implies u ∈ C v .</formula><p>We deduce from the previous result that (C u ,C v ) ∈ A implies u v, and is a topological order on G.</p><p>Then we show that (7.6) holds. We first show that (u</p><formula xml:id="formula_211">∈ C v ) ⇒ C u C v and u ∈ C for any C on path C u C v .</formula><p>Either u = v and this is obvious, or u ∈ pa G (C v ); and by recursion either C u C v or u ∈ C r with C r the root of the tree which is also the first element in the topological order. But, unless u = r , this is excluded given that u ∈ C r implies u r . Note that this shows that C u is indeed the unique minimal element of the set {C : u ∈ C } for the partial order defined by the arcs of the tree. To show the first part of (7.6), we just need to note that either u ∈ fa(v) and the result holds, or there must exist x such that C v → C x and u ∈ C x and by recursion, there exists w such that C v C w and u ∈ fa(w).</p><p>Finally, we prove that we have constructed an RJT. Indeed, if two vertices C v and C v contain u then since G is singly connected, the trail connecting C v and C v must be composed of vertices on the paths C v C u and C u C v , and we have shown in the previous paragraph that that u belongs to any C on C v C u and C u C v , and so the running intersection property holds. Finally, property (ii) of Definition 6.1 must holds because the fact that C u is minimal among all cluster vertices containing u together with the running intersection property entails that the cluster vertices containing u are indeed a subtree of G with root C u .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.5.2">Characterizing the built RJT</head><p>Proposition 7.7, in addition to proving the correctness of Algorithm 4, provides the benefit of using Algorithm 4, but it characterizes the content of the cluster vertices based on the topology of the obtained RJT, which is itself produced by the algorithm (note that the composition of cluster vertices depends only on via the partial order of the tree). The cluster vertices of any RJT and those produced by Algorithm 4 admit however more technical characterizations using only and the information in G, which we present this section. These characterizations will be useful in the proofs of Chapter 9. For each vertex v in V , we introduce We start by proving (7.7a). Let v and w be vertices such that w v and that there is a v-w trail Q in V v . Let s 0 , . . . , s k be the vertices where Q has a v-structure <ref type="foot" target="#foot_15">5</ref>  We now prove (7.7b). Let u and v be two vertices such that u v and there is a u-v trail P with P \{u} ⊆ V v . Let w be the vertex right after u on P . We have u ∈ fa(w), w v and there is a v-w trail in V v , which implies C v C w by (7.7a). But, since u v, the u-v trail is also in V u , which similarly shows that C u C v . So by (7.5) we have proved (7.7b). Proof. Note that some visual elements of the proof are given in Figure <ref type="figure" target="#fig_41">7</ref>.3. It is sufficient to prove the following inclusions</p><formula xml:id="formula_212">T v = {w ∈ V v | there is a v-w trail in V v }.</formula><formula xml:id="formula_213">des G (C v ) ⊆ {C w : w ∈ T v }, C v ⊆ {u v : ∃w ∈ T v , u ∈ fa(w)}. (7.8a) (7.8b)</formula><p>Indeed, note that by Proposition 7.7, the obtained tree is an RJT so that, by Proposition 7.8, the reverse inequalities hold.</p><p>We prove the result by backward induction on (7.8b) and (7.8a). For a leaf</p><formula xml:id="formula_214">C v of G, des G (C v ) =</formula><p>{C v } so that (7.8a) holds trivially and C v = fa(v) so that (7.8b) holds because u ∈ fa(v) implies u v. Then, assume the induction hypothesis holds for all children of a vertex C v .</p><p>We first show (7.8a) for C v , i.e. that (C v C w ) ⇒ (w ∈ T v ) (see Figure <ref type="figure" target="#fig_41">7</ref>.3). Let C x be the child of C v on the path C v C w . By Proposition 7.7, we have v ≺ x, so that V x ⊂ V v . Then, using the induction hypothesis, by (7.8b), (v ∈ C x ) implies that there is a v-x trail in V v , and a.  by (7.8a), C x C w implies there is a trail x-w in V x , so there is a v-w trail in v ≺ x in V v , which shows the result.</p><formula xml:id="formula_215">v</formula><p>We then show (7.8b) for C v (see Figure <ref type="figure" target="#fig_41">7</ref>.3). Indeed if u ∈ C v , either u ∈ fa(v) and u is in the RHS of (7.8b), or there exists a child of C v , say C x such that u ∈ C x and u ≺ v, because the algorithm imposes v = max (C x \{x}). Since C v C x there exists a path v-x in V v , and using induction, by (7.8b), (u ∈ C x ) implies that ∃w such that u ∈ fa(w) and there exists a trail w-x in T v . But we have shown in Proposition 7.7 that (v ∈ C x ) ⇒ (v x), so T x ⊂ T v and we have shown that there exists a v-w trail in T v with u v and u ∈ fa(w), which shows the result.</p><p>Finally we conclude this section with Figure <ref type="figure" target="#fig_41">7</ref>.2, which shows an example of influence diagram whose rooted-treewidth is equal to 3 and larger than its pathwidth, which is equal to 2. Figure <ref type="figure" target="#fig_41">7</ref>.2 also provides the path decomposition of width 2 and the RJT of width 3. This kind of v-shaped graphs are especially challenging for our approach.  </p><formula xml:id="formula_216">s t -1 v t -1 -</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Integer programming on the junction tree polytope</head><p>The main goal of this chapter is to prove Theorem 6.1 and Proposition 6.4. It requires to encode the probability distribution P δ of a strategy δ using a set of constraints satisfied by a vector of moments µ. In Chapter 7, we introduced some local independences <ref type="bibr">(7.4)</ref>, which are sufficient to ensure that the probability distribution P µ of a vector of moments µ in the marginal polytope of a RJT, factorizes on G. The key aspect in this chapter is to show that given a strategy δ, any vector of moments µ satisfying the constraints of (6.5) will satisfy <ref type="bibr">(7.4)</ref>, ensuring that the probability distribution P µ factorizes according to the directed acyclic graph. The proof of Theorem 8.1 then follows.</p><p>This chapter is organized as follows:</p><p>• Section 8.1 gives a proof of Theorem 6.1, which is based on showing that the probability distribution from which derives the feasible vector of moments of MILP (6.6), factorizes according to the influence diagram. Using notation of Chapter 7 we prove that NLP (6.6) models exactly MEU(G, ρ). • Section 8.2 is dedicated to prove Proposition 6.4. Using notation of Chapter 7, we prove the validity of inequalities (6.7) for MILP <ref type="bibr">(6.6)</ref>. To the best of our knowledge, such "independence cuts" have not been proposed in combinatorial optimization. We believe that this idea of leveraging conditional independence to obtain valid cuts is fairly general and could be extended to other contexts. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">Integer programming using the moments</head><p>For convenience, we start by introducing notation that is useful in this section. Then we obtain NLP (6.5) in Section 8.1.2, and then linearize it into MILP (6.6) in Section 8.1.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1.1">Notation</head><p>In this section, we introduce two sets to lighten the writing of the mathematical programs introduced in Section 6.1. Consider an influence diagram G = (V s ,V a , A). Let ρ = (X V , p,r) be a parametrization of G such that X V = v∈V X v the support of the vector of random variables attached to all vertices of G, p = {p v|pa(v) } v∈V s is the collection of fixed and assumed known conditional probabilities, and r = {r v } v∈V r is the collection of reward functions 1 r v : X v → R.</p><p>The reward functions r will also be viewed as vectors r v ∈ R X v . Given a gradual RJT G of G, we introduce the following variant of the marginal polytope M 0 (G) defined in <ref type="bibr">(7.3)</ref>.</p><formula xml:id="formula_217">M0 (G) = (µ C v , µ Čv ) v∈V : (µ C v ) v∈V ∈ M 0 (G) and µ Čv = x v µ C v ,</formula><p>where moments µ Čv have been introduced. This is for convenience, and all the results could have been written using M 0 (G). By abuse of notation, we say that µ ∈ M(G) instead of (µ C v , µ Čv ) v∈V ∈ M0 (G). We also introduce the polytope</p><formula xml:id="formula_218">P(G,ρ) = µ ∈ M0 (G) : µ C v = µ Čv p v|pa(v) for all v ∈ V s . (8.1)</formula><p>We omit the dependence of P in ρ when it is clear from the context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1.2">An exact Non Linear Program</head><p>Using the notation introduced in Section 8.1.1 we write NLP (6.5) more concisely as follows max</p><formula xml:id="formula_219">µ,δ v∈V r 〈r v , µ v 〉 s.t. µ ∈ P δ ∈ ∆ µ C v = δ v|pa(v) µ Čv , ∀v ∈ V a . (8.2a) (8.2b) (8.2c) (8.2d)</formula><p>Note that the constraints δ ∈ ∆, which are being positive and summing to 1, are implied by the other ones in NLP (8.2) (or (6.5)). 2 We have Theorem 6.1 which we recall here with the notation introduced above: Theorem 8.1. Let (µ, δ) be a feasible solution of NLP (8.2). Then µ is the vector of moments of P δ induced by δ. Furthermore, µ, δ is an optimal solution of NLP (8.2) if and only if δ is an optimal policy of MEU(G, ρ). In particular, NLP (8.2) and MEU(G, ρ) have the same optimal 1 We remind the reader that V r is the set of utility vertices as introduced in Section 6.1.</p><p>2 Indeed, constraint µ ∈ P(G,X , p,G) ensures that, for each v ∈ V a , µ C v is a distribution. And con-</p><formula xml:id="formula_220">straint µ C v = δ v|pa(v)</formula><p>µ Čv ensures that, if x pa(v) has non-zero probability according to that distribution, i.e.,</p><p>x Čv \x pa(v)</p><formula xml:id="formula_221">µ Čv (x Čv \x pa(v) , x pa(v) ) &gt; 0., then δ v|pa(v) (•|x pa(v)</formula><p>) is a conditional probability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1.3">MILP formulation</head><p>Now we detail how we turn NLP 8.2 (or equivalently NLP (6.5)) into MILP (6.6). We recall that there always exists at least one optimal strategy which is deterministic (and therefore integral) for MEU(G, ρ). Therefore, we can add integrality constraint (6. </p><formula xml:id="formula_222">P δ X Čv = x Čv b Čv (x Čv ) ∀δ ∈ ∆, ∀v ∈ V a , ∀x Čv ∈ X Čv . (8.4)</formula><p>For such a vector b, we say that, for a given vertex v, (µ</p><formula xml:id="formula_223">C v , δ v|pa(v) ) satisfies McCormick's in- equalities (see Section 8.3) if      µ C v µ Čv + (δ v|pa(v) -1) b Čv , µ C v δ v|pa(v) b Čv , µ C v µ Čv . (McCormick(v, b))</formula><p>Note that the last inequality µ C v µ Čv can be omitted in our case as it is implied by the marginalization constraint µ Čv = x v µ C v in the definition of M0 (G). Given the upper bounds provided by b, we introduce the polytope of valid moments and decisions satisfying all McCormick constraints:</p><formula xml:id="formula_224">Q b (G, X V , p,G) = (µ, δ) ∈ M0 (G) × ∆ : McCormick(v, b) is satisfied for all v ∈ V a . (8.5)</formula><p>For convenience, we write Q b when (X V , p,G) is clear from the context. Thus by using Mc-Cormick on Constraints (8.2d), we get that MEU(G, ρ) is equivalent to the following MILP, which is a rewriting of MILP (6.6): max </p><formula xml:id="formula_225">µ,δ v∈V r 〈r v , µ v 〉 s.t. µ ∈ P δ ∈ ∆ d (µ, δ) ∈ Q b . (<label>8</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Valid cuts</head><p>Classical techniques in integer programming, such as branch-and-bound algorithms, rely on solving the relaxation of the MILP to obtain a lower bound on the value of the objective. For MILP <ref type="bibr">(8.6)</ref> the relaxation is likely to be poor, and so the MILP is not well solved by off-theshelf solvers. Indeed as explained in Remark 11, when b = 1, the McCormick inequalities completely fail to capture, in the linear relaxation of the MILP <ref type="bibr">(8.6)</ref>, the conditional independence encoded by the nonlinear constraints (8.2d). Further, improving the bound b does not completely address the issue. In this section, our goal is to prove Proposition 6.4. We introduce the inequalities (6.7) and we prove their validity for MILP <ref type="bibr">(8.6)</ref>. It strengthens the relaxation and ease the MILP resolution in practice (see <ref type="bibr">Section 8.6)</ref>. Recall that a valid cut for an MILP is an (in)equality that is satisfied by any solution of the MILP, but not necessarily by solutions of its linear relaxation. A family of valid cuts is stronger than another when the former yields a polytope strictly included in the latter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2.1">Constructing valid cuts</head><p>By restricting ourselves to vectors of moments µ ∈ P, we have imposed </p><formula xml:id="formula_226">P µ (X v |X Čv ) = p v|pa(v) for all v in V s ,</formula><formula xml:id="formula_227">(X v |X pa(v) ) = P δ (X v |X pa(v) ) = δ v|pa(v) for v ∈ V a .</formula><p>A key question is therefore whether we can enforce some conditional probabilities implied by the nonlinear constraints, and thus lost in the linear relaxation of MILP (8.6), through linear constraints. This seems possible because, as an indirect consequence of setting the conditional distributions p v|pa(v) for v ∈ V s , there are other conditional probability distributions whose value does not depend on δ. We characterize such conditional probability distributions using the notion of strategy independent set introduced in Chapter 6, <ref type="foot" target="#foot_16">3</ref> which we recall here: Let C be a subset of vertices in V . A set of variables X D such that D ⊆ C is strategy independent set in C if it satisfies the following property:</p><p>For all parametrization ρ such that (G, ρ) is a PID, P δ (X D |X C \D ) does not depend on δ.</p><p>For such a subset D, the following proposition ensures that we can add linear constraints in define P ⊥ ⊥ as the polytope we obtain when we strengthen P with the valid cuts:</p><formula xml:id="formula_228">P ⊥ ⊥ (G, X V , p,G) = µ ∈ P : µ C v = p C ⊥ ⊥ v |C ⊥ ⊥ v x C ⊥ ⊥ v µ C v for all v ∈ V a . (8.8)</formula><p>Given a vector b satisfying (8.4) we introduce the following strengthened version of the MILP <ref type="bibr">(8.6)</ref>.</p><formula xml:id="formula_229">max µ, δ v∈V r 〈r v , µ v 〉 subject to µ ∈ P ⊥ ⊥ , δ ∈ ∆ d , (µ, δ) ∈ Q b .</formula><p>(8.9)</p><p>Figure <ref type="figure" target="#fig_53">8</ref>.1 provides an example of an influence diagram where the introduction of valid cut of the form (6.7) reduces the size of the initial polytope. Indeed the cluster C = {a, u, v, b} leads to C ⊥ ⊥ = {u}, and the resulting cut (6.7) is not implied by the linear inequalities of (8.6).</p><p>Indeed, suppose that</p><formula xml:id="formula_230">X a = X v = {0}, while X u = X b = {0, 1}.</formula><p>Then the solution defined by µ auvb (0, i , 0, i ) = 0.5 and µ auvb (0, i , 0, 1-i ) = 0 for i ∈ {0, 1} is in the linear relaxation of MILP (8.6) but does not satisfy (6.7). To compute C ⊥ ⊥ , we have used the characterization provided in Section 8.2.2.</p><p>Remark 12. In the definition of P ⊥ ⊥ , we decided to introduce valid cuts of the form (8.7) only for sets of vertices C of the form C v with v ∈ V a . This is to strike a balance between the number of constraints added and the number of independences enforced. This choice is however heuristic, and it could notably be relevant to introduce constraints of the form (6.7) for well chosen C C v .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2.2">Characterization of C ⊥ ⊥</head><p>In this section, we show the existence of C ⊥ ⊥ by giving its closed form. In order to characterize C ⊥ ⊥ , we need some concepts from graphical model theory. The first notions make it possible to identify conditional independence from properties of the graph. Let D ⊆ V be a subset of vertices. Let P be a trail</p><formula xml:id="formula_231">v 1 • • • v n . We say that v i is a v-structure on P if 1 &lt; i &lt; n and the subtrail v i -1 v i v i +1 is of the form v i -1 → v i ← v i +1 . A trail v 1 • • • v n is active given D if, for all v-structure v i -1 → v i ← v i +1 on P , v i</formula><p>or one of its descendant is in D, and any vertex of the trail that is not a v-structure is not in D. Two sets of vertices B 1 and B 2 are said to be d-separated by D in G, and we will denote this property by The other notion we need is the one of augmented model <ref type="bibr" target="#b74">[75,</ref><ref type="bibr">Chapter 21]</ref>. Informally, the idea behind augmented models is as follows: the strategies specify some conditional distributions in the model; the statement that some conditional probabilities do not depend on the strategy is a priori to be understood for a functional notion of independence which is different from probabilistic independence. However, and somewhat surprisingly, by considering a randomized version of the strategy (and thus of the corresponding conditional distribution) the two notions of independence actually coincide. This motivates to "augment" the graph G with additional vertices associated with policies themselves, viewed as random variables, and which are each a parent of the vertex whose conditional distribution they define. The fact that the set of introduced policy variables are conditionally independent of a set of variables X D given X C \D in the augmented graph turns out to be equivalent to the fact that the value P δ (X D |X C \D ) does not depend on the choice of δ.</p><formula xml:id="formula_232">B 1 ⊥ B 2 | D, if there is no active trail between B 1 and B 2 given D. We have X B 1 ⊥ ⊥ X B 2 | X D</formula><p>Formally, consider (G, ρ), a PID with G = (V s ,V a , A), and let V = V a ∪ V s . For each v ∈ V a , we introduce a vertex ϑ v and a corresponding random variable θ v . The variable θ v takes its value in the space ∆ v of conditional distributions on X v given X pa(v) . Let G † be the digraph with vertex set V G † = V ∪ϑ V a , where ϑ V a = {ϑ v } v∈V a , and arc set A G † = A∪{(ϑ v , v), ∀v ∈ V a }. Such a graph G † , called augmented graph, is illustrated on Figure <ref type="figure" target="#fig_53">8</ref>.2, where vertices in G † \G are represented as rectangles with rounded corners. The augmented model of (G, ρ) is the collection of probability distributions factorizing on G † such that X v is defined as in ρ for each v in V , X ϑ v = ∆ v , and</p><formula xml:id="formula_233">P G † X v = x v |X pa G † (v) = x pa G † (v) = θ o v (x v |x pa(v) ) if v ∈ V a , p v|pa(v) (x v |x pa(v) ) if v ∈ V s ,</formula><p>where</p><formula xml:id="formula_234">x pa G † (v) = (x pa G (v) , θ o v ) for v ∈ V a , and x pa G † (v) = x pa G (v) for v ∈ V s .</formula><p>A probability distribution of the augmented model is specified by choosing the marginal distributions of the θ v , for v ∈ V a . In the rest of the thesis, we denote by P G † the distribution of the augmented model with uniformly distributed θ v (•|x pa(v) ) for each v in V a , and each value</p><formula xml:id="formula_235">of x pa G (v) ∈ X pa G (v) .</formula><p>With these definitions, a strategy δ can now be interpreted as a value taken by θ V a , and we have</p><formula xml:id="formula_236">P δ (X D = x D |X M = x M ) = P G † (X D = x D |X M = x M , θ V a = δ),<label>(8.10)</label></formula><p>for any set D, M ⊆ V . Note that in general</p><formula xml:id="formula_237">P G † (X D = x D |X M = x M ) is the expected value over θ V a of P θ V a (X D = x D |X M = x M ).</formula><p>The following result, which is an immediate consequence of (8.10), characterizes the pairs (D, M ) such that for any parametrization ρ the conditional probability distribution P δ (X D |X M ) does not depend on strategy δ.</p><p>Proposition 8.5. We have</p><formula xml:id="formula_238">P δ (X D |X M ) = P G † (X D |X M )</formula><p>for any parametrization ρ on G, any strategy δ, and any M such that</p><formula xml:id="formula_239">P δ (X M ) &gt; 0 if and only if D is d-separated from ϑ V a given M in G † .</formula><p>Note that this is a particular case of a result known in the causality theory for graphical models [see, e.g. 75, Proposition 21.3]. The following corollary gives a new characterization of the strategy independent sets:</p><formula xml:id="formula_240">Corollary 8.6. Given a set C , a subset D of C is strategy independent in C if and only if D is d-separated from ϑ V a given C \D in G † .</formula><p>Proof. If D is a strategy independent set in C . Then, the property (8.10) ensures that for any parametrization ρ,</p><formula xml:id="formula_241">P G † (X D = x D |X M = x M , θ V a = δ)</formula><p>does not depend on δ. Hence, for any parametrization ρ and any strategy δ we obtain</p><formula xml:id="formula_242">P δ (X D |X C \D ) = P G † (X D |X C \D ). Proposition 8.5 ensures that D is d-separated from ϑ V a given C \D in G † . If D is d-separated from ϑ V a given M in G † .</formula><p>Then, Koller and Friedman <ref type="bibr" target="#b74">[75,</ref><ref type="bibr">Theorem 3.3]</ref> ensures that for any probability distribution P factorizing on G † , we have (</p><formula xml:id="formula_243">X D ⊥ ⊥ ϑ V a |X C \D ) P .</formula><p>Hence, we obtain Proof. It suffices to prove that ϑ V a ⊥D ∪ D |C \(D ∪ D ). Suppose that it is not true, i.e., there exists an active trail between ϑ V a and D ∪ D that is active given C \(D ∪ D ). Let P be such a trail. Hence, there exist two vertices u, v ∈ V such that u ∈ D ∪ D and v ∈ V a , and P is active between u and ϑ v given C \(D ∪ D ). Without loss of generality, we suppose that P is minimal in the sense that P ∩ (D ∪ D ) = {u}. Indeed, otherwise we consider the nearest vertex of P that belongs to D ∪ D and the trail from this vertex to ϑ v is also active given C \(C ∪ C ). Suppose that u ∈ D. Since D is a strategy independent set in C , we have ϑ v ⊥u|C \D. Hence, the trail P is not active given C \D. Therefore, either there is a v-structure of P with no descendant in C \D or there is a vertex on P that is not a v-structure and that belongs to C \D. Since P is active given C \(D ∪ D ), it means that in all the v-structures of P , there is at least one descendant that is in C \(D ∪ D ) ⊆ C \D. Therefore, all the v-structures are active given C \D.</p><formula xml:id="formula_244">P G † (X D |X C \D , θ V a ) = P G † (X D |X C \D )</formula><p>We deduce that there is a vertex on P that is not a v-structure and that belongs to C \D. Since P is active given C \(D ∪ D ), we have x ∈ D \D. It contradicts the fact that P is minimal.</p><p>Given a set C , the stability property of Lemma 6.2 enables to define C ⊥ ⊥ as the largest inclusionwise strategy independent set in C . In fact, we can give a full characterization of C ⊥ ⊥ as stated in the following theorem.</p><p>Theorem 8.7.</p><formula xml:id="formula_245">C ⊥ ⊥ is equal to v ∈ C : v ⊥ ϑ V a |C \{v} .</formula><p>With this characterization, the reader can check the value of C ⊥ ⊥ on the example of Figure <ref type="figure" target="#fig_53">8</ref>.1.</p><p>If we want to use the valid cuts in (8.8) in practice, we must compute C ⊥ ⊥ and p C ⊥ ⊥ |C ⊥ ⊥ efficiently. Theorem 8.7 ensures that C ⊥ ⊥ is easy to compute using any d-separation algorithm (and more efficient algorithms are presumably possible), and Proposition 8.5 ensures that, if we solve the inference problem on the RJT for an arbitrary strategy, e.g., one where decisions are made with uniform probability, we can deduce p C ⊥ ⊥ |C ⊥ ⊥ from the distribution µ C obtained. Theorem 8.7 is an immediate corollary of the following lemma.  Proof of Theorem 8.7. We first prove that</p><formula xml:id="formula_246">G s 1 o 1 r 1 a 1 ϑ 1 s 2 o 2 r 2 a 2 ϑ 2 s 3 o 3</formula><formula xml:id="formula_247">B ⊥ C \(B ∪ M ) | M (8.11) is M * := v ∈ C \B : v ⊥ B |C \(B ∪ {v}) . Furthermore, a set M ⊆ C satisfies (8.11) if and only if M * ⊆ M .</formula><formula xml:id="formula_248">C ⊥ ⊥ = C \C ⊥ ⊥ corresponds to the Markov Blanket of ϑ a V in C . It suffices to show that ϑ a V ⊥ C \(B ∪ C ⊥ ⊥ )|C ⊥ ⊥ . Since C ⊥ ⊥ is strategy independent, Corol- lary 8.6 ensures that ϑ a V ⊥ C ⊥ ⊥ |C ⊥ ⊥ which is equivalent to ϑ a V ⊥ C \C ⊥ ⊥ |C ⊥ ⊥ . Since ϑ a V ∩ C = , we obtain ϑ a V ⊥ C \ ϑ ∪C ⊥ ⊥ |C ⊥ ⊥ .</formula><p>It ensures that C ⊥ ⊥ satisfies <ref type="bibr">(8.11)</ref>. By definition of C ⊥ ⊥ , C ⊥ ⊥ is the smallest inclusion-wise subset of C satisfying <ref type="bibr">(8.11)</ref>. Therefore, Lemma 8.8 ensures that C ⊥ ⊥ is the Markov Blanket of ϑ a V in C and can be written</p><formula xml:id="formula_249">C ⊥ ⊥ = v ∈ C : v ⊥ ϑ a V |C \{v} . We conclude that C ⊥ ⊥ = C \C ⊥ ⊥ = v ∈ C : v ⊥ ϑ a V |C \{v} .</formula><p>The characterization of Theorem 8.7 gives a closed form of C ⊥ ⊥ , which helps to compute C ⊥ ⊥ . In addition, it is key in proving Proposition 6.3 which we recall here:</p><formula xml:id="formula_250">Proposition 6.3. Given a set C in V , C ⊥ ⊥ can be computed in O |C |(|V | + |A|) .</formula><p>Proof. Theorem 8.7 ensures that computing C ⊥ ⊥ requires to find all vertices v in C that are dseparated from ϑ a V given C \{v}. Given a vertex v ∈ C , computing the set {u ∈ C : ϑ a V ⊥ u|C \{v} can be done in O(|V | + |A|) using the Bayes-Ball algorithm <ref type="bibr" target="#b137">[138,</ref><ref type="bibr">Theorem 4]</ref>. By running the algorithm for each vertex v in C , the total complexity is in O(|C |(|V | + |A|)), which achieves the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3">McCormick Relaxation</head><p>McCormick inequalities allow to turn NLP (8.2) into MILP <ref type="bibr">(8.6)</ref>. Further good bounds b on the vector of moments ease the resolution of MILP <ref type="bibr">(8.6)</ref>. In this section we first discuss these relaxation, show that in the NLP loose bounds are useless while tight bounds improve the MILP formulation. Finally, we give an algorithm to compute good quality bounds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3.1">Review of McCormick's relaxation</head><p>For the sake of completeness we briefly recall McCormick's relaxation, and condition for exactness if all of the variables but one are binary. Proposition 8.9. Consider the variables (x, y, z) ∈ [0, 1] 3 and the following constraint z = x y (8.12)</p><p>Further, assume that we have an upper bound y b. We call McCormick(8.12) the following set of constraints</p><formula xml:id="formula_251">z y + xb -b z y z bx<label>(8.13a) (8.13b) (8.13c)</label></formula><p>If x, y and z satisfy Equation (8.12), then they also satisfy Equation (8.13). If x is a binary variable (that is x ∈ {0, 1}) and Equation (8.13) is satisfied, then so is Equation (8.12). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3.2">Choice of bounds in McCormick inequalities</head><p>Using b Čv = 1 leads to loose constraints Since µ Čv is a probability distribution, 1 is an immediate upper bound on µ Čv . Let Q 1 be the polytope Q b obtained using bounds vector b defined by b Čv = 1 for all v in V a .</p><p>Proposition 8.10. Let µ be in P (resp. P ⊥ ⊥ ). Then there exists δ in ∆ such that (µ, δ) belongs to Q 1 , and the linear relaxation of MILP (8.6) (resp. MILP (8.9)) with b = 1 is equal to max</p><formula xml:id="formula_252">µ∈P v∈V r 〈r v , µ v 〉 (resp. max µ∈P ⊥ ⊥ v∈V r 〈r v , µ v 〉).</formula><p>Proof. We write the proof with µ ∈ P and it is exactly the same if µ ∈ P ⊥ ⊥ . Let v be a vertex in V a , and let We have</p><formula xml:id="formula_253">δ v|pa(v) (x fa(v) ) = µ fa(v) (x fa(v) ) µ pa(v) (x pa(v) ) if µ pa(v) (x pa(v) ) = 0, 1 e v (x v ) otherwise,</formula><formula xml:id="formula_254">µ C v (x C v ) -µ Čv (x Čv ) x Cv \fa(v) µ C v (x C v \fa(v) , x fa(v) ) -µ Čv (x C v \fa(v) , x pa(v) ) 0 = µ fa(v) (x fa(v) ) -µ pa(v) (x pa(v) ) = 1 µ pa(v) (x pa(v) ) (δ v|pa(v) (x fa(v) ) -1) δ v|pa(v) (x fa(v) ) -1 which yields µ C v µ Čv + (δ v|pa(v) -1)b Čv . Besides, if µ C v (x C v ) 0</formula><p>, following the definition of δ and given that µ pa(v) (x pa(v) ) 1, we have</p><formula xml:id="formula_255">δ v|pa(v) (x fa(v) ) µ fa(v) (x fa(v) ) µ C v (x C v ),</formula><p>and the constraint µ C v δ v|pa(v) b Čv is satisfied.</p><p>Finally, µ C v µ Čv follows from the marginalization constraint µ Čv = x v µ C v in the definition of the marginal polytope.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>McCormick inequalities with well-chosen bounds are useful</head><p>This Suppose that all variables are binary, that s is Bernoulli with parameter 1  2 , that P(</p><formula xml:id="formula_256">X t = 1|X s ) = 1 + εX s -ε(1 -X s ), that X w indicates if X s = X a ,</formula><p>and that the objective is to maximize E δ (X w ), and has value 1  2 + ε. An optimal policy consists in choosing X a = X t . An optimal solution of the linear relaxation of MILP (8.6) on P without McCormick inequalities, has value 1. Whereas an optimal solution with McCormick inequality and b st (x s , x t ) = 1 2 + ε1 x s =x t has value 1 2 + ε. However, on this simple example, the McCormick inequalities are implied by the valid inequalities of Section 8.2. This is no more the case on the influence diagram of Figure <ref type="figure" target="#fig_53">8</ref>.3.b, where r is a Bernoulli of parameter 0.5 and X s = X r X b +(1-X r )(1-X b ), and the remaining of the parameters are defined as previously. Using the same bounds, this new example leads to exactly the same results as before.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.3.3">Algorithm to compute good quality bounds</head><p>This section provides a dynamic programming equation to compute bounds b Čv on µ Čv that are smaller than 1. Let G be a RJT, and C 1 , . . . ,C n be a topological order on G. Let C k be a vertex in G, C j be the parent of</p><formula xml:id="formula_257">C k and C i the parent of C j (i &lt; j &lt; k). If k = 1, then C i = C j = C k = C 1 .</formula><p>We introduce the notation</p><formula xml:id="formula_258">C a j = (C j \(C i ∪ C k )) ∩ V a .</formula><p>We define inductively on k the functions bk :</p><formula xml:id="formula_259">X C k → [0, 1] as follows.          b1 (x C 1 ) = v∈C 1 ∩V s p(x v |x pa(v) ) bk (x C k ) = x pa(C a j ) max x C a j x (C i ∪C j )\(C k ∪C a j ) bi (x C i ) v∈((C j ∪C k )\C i )∩V s p(x v |x pa(v) ) for k &gt; 1 Proposition 8.11. Let µ be in M(G). We have µ C k (x C k ) bk (x C k ) for all i and x C k in X C k .</formula><p>As a consequence, b Čv defined as x v bC v provides an upper bound on µ Čv that can be used in McCormick constraints.</p><p>Proof of Proposition 8.11. We prove the result by induction. Let (µ, δ) be a feasible solution of NLP 8.2 and C j be the parent of C k in G, and C i the parent of C j (i &lt; j &lt; k).</p><p>If k = 1, then the result is obtained by using δ v 1 for all v ∈ V a .</p><p>We assume now that the induction is true until k &gt; 1. We have</p><formula xml:id="formula_260">µ C k (x C k ) = x (C i ∪C j )\C k µ C i (x C i ) v∈((C j ∪C k )\C i )∩V s p(x v |x pa(v) ) v∈((C j ∪C k )\C i )∩V a δ v (x v |x pa(v) ) max δ (C j ∪C k )\C i x (C i ∪C j )\C k µ C i (x C i ) v∈((C j ∪C k )\C i )∩V s p(x v |x pa(v) ) v∈((C j ∪C k )\C i )∩V a δ v (x v |x pa(v) ) max δ C j \(C i ∪C k ) x (C i ∪C j )\C k µ C i (x C i ) v∈((C j ∪C k )\C i )∩V s p(x v |x pa(v) ) v∈(C j \(C i ∪C k ))∩V a δ v (x v |x pa(v) ) max δ C j \(C i ∪C k ) x (C i ∪C j )\C k b C i (x C i ) v∈((C j ∪C k )\C i )∩V s p(x v |x pa(v) ) v∈(C j \(C i ∪C k ))∩V a δ v (x v |x pa(v) )<label>(8.14) (8.15) (8.16) (8.17)</label></formula><p>From (8.14) to (8.15), we maximize over the policies in (C i ∪ C j )\C k . From (8.15) to <ref type="bibr">(8.16)</ref>, we bound all policies in C k ∩ V a by 1. Then (8.17) is obtained by using the induction assumption.</p><formula xml:id="formula_261">Let α : X fa(C a j ) → R be such that for all x C a j ∈ X C a j , α(x C a j ) = x (C i ∪C j )\(C k ∪fa(C a j )) bC i (x C i ) v∈((C j ∪C k )\C i )∩V s p(x v |x pa(v) ).</formula><p>Then, (8.17) becomes</p><formula xml:id="formula_262">µ C k (x C k ) max δ C a j x fa(C a j ) α(x fa(C a j ) ) v∈C a j δ v (x v |x pa(v) )</formula><p>Now, we can suppose that offspring(C v ) = {v}. Therefore, |C a j | 1 and the maximum above can be decomposed into the sum.</p><formula xml:id="formula_263">µ C k (x C k ) x pa(C a j ) max δ C a j x C a j α(x fa(C a j ) ) v∈C a j δ v (x v |x pa(v) ) x pa(C a j ) max x C a j α(x fa(C a j ) )<label>(8.18) (8.19)</label></formula><p>where from (8.18) to (8.19) we use a local maximization. Therefore, we obtain the result</p><formula xml:id="formula_264">µ C k (x C k ) x pa(C a j ) max x C a j x (C i ∪C j )\(C k ∪fa(C a j )) bC i (x C i ) v∈((C j ∪C k )\C i )∩V s p(x v |x pa(v) )</formula><p>Note that bk (x C k ) is computed via an order two recursion from bi (x C i ) where i is the grandparent of k, which can be generalized to higher order if stricter bound are needed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.4">Strength of the relaxations and their interpretation in terms of graph</head><p>In this section we provide interpretations of the linear relaxations of MILP (8.6) and MILP (8.9) in terms of graphs. Given an influence diagram G = (V, A), we introduce the sets of arcs and influence diagrams </p><formula xml:id="formula_265">A = A ∪ (u, v) : v ∈ V a and u ∈ C v \fa(v) , G = (V, A), A ⊥ ⊥ = A ∪ (u, v) : v ∈ V a and u ∈ C ⊥ ⊥ v \fa(v) and G ⊥ ⊥ = (V, A ⊥ ⊥ ).</formula><formula xml:id="formula_266">M G (G , ρ) = µ ∈ P : ∃δ ∈ ∆ G , µ C v = µ Čv δ v|pa G (v) for all v in V a .</formula><p>Using this definition, the following theorem gives an interpretation of the linear relaxations of (8.6) and (8.9).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.4.">Strength of the relaxations and their interpretation in terms of graph</head><formula xml:id="formula_267">s i o i u i a i v i r i s i +1 G s i o i u i a i v i r i s i +1 G ⊥ ⊥ s i o i u i a i v i r i s i +1 G Figure 8</formula><p>.4 -Graph relaxations corresponding to linear relaxations for the chess game example.</p><p>Theorem 8.12. We have</p><formula xml:id="formula_268">P = M(G) and max µ∈P v∈V r 〈r v , µ v 〉 = MEU(G, ρ),</formula><p>and</p><formula xml:id="formula_269">P ⊥ ⊥ = M(G ⊥ ⊥ ) and max µ∈P ⊥ ⊥ v∈V r 〈r v , µ v 〉 = MEU(G ⊥ ⊥ , ρ).</formula><p>Hence, if (µ, δ) is a solution of the linear relaxation of (8.6), then δ is a strategy on G, while if (µ, δ) is a solution of the linear relaxation of (8.9), then δ is a strategy on G ⊥ ⊥ . To prove Theorem 8.12, we will need the following lemma.</p><p>Lemma 8.13. Let v be a vertex in V a . Then</p><formula xml:id="formula_270">x C v → p C ⊥ ⊥ v |C ⊥ ⊥ v (x C ⊥ ⊥ v |x C ⊥ ⊥ v \v , x v ) is a function of (x C ⊥ ⊥ v , x C ⊥ ⊥ v \v ) only. Hence, if a distribution µ C v satisfies µ C v = µ C ⊥ ⊥ v p C ⊥ ⊥ v |C ⊥ ⊥ v , then C ⊥ ⊥ v ⊥ v | C ⊥ ⊥ v \{v} according to µ C v .</formula><p>Proof. Consider the augmented model P G † . Let P be a C ⊥ ⊥ v -v trail. Let Q be the trail P followed by the arc (v, ϑ v ). Given that v has no descendants in C v (because we consider the case of a gradual RJT so that offspring</p><formula xml:id="formula_271">(C v ) = {v}), the vertex v is a v-structure of Q. As v ∈ C ⊥ ⊥ v , if P is active given C ⊥ ⊥ v \{v}, then P is active given C ⊥ ⊥ v , which contradicts the definition of C ⊥ ⊥ v . Hence, C ⊥ ⊥ v ⊥ v | C ⊥ ⊥</formula><p>v \{v} according to P G † , and</p><formula xml:id="formula_272">x C v → p C ⊥ ⊥ v |C ⊥ ⊥ v (x C ⊥ ⊥ v |x C ⊥ ⊥ v \v , x v ) is a function of (x C ⊥ ⊥ v , x C ⊥ ⊥ v \v ) only.</formula><p>The second part of the lemma is an immediate corollary.</p><p>We can now prove Theorem 8.12.</p><p>Proof of Theorem 8.12. First, remark that, once we have proved P = M(G) and P ⊥ ⊥ = M(G ⊥ ⊥ ), the result follows from Theorem 6.1.</p><p>We now prove P = M(G). Let µ be in P. Then µ is a vector of moments in the marginal polytope of the RJT G on G. We now prove by disjunction of cases that, according to</p><formula xml:id="formula_273">µ C v , X v is independent from its non-descendants in G that are in C v given pa G (v). If v ∈ V a , we have fa G (v) = C v and the result is immediate. If v ∈ V s , we have µ C v = µ Čv p v|pa G (v) , for v ∈ V s ,</formula><p>and pa G (v) = pa G (v) for v in V s then gives the result. Theorem 7.3 then ensures that µ is a vector of moments of a distribution that factorizes on G, which yields P ⊆ M(G). The inclusion M(G) ⊆ P is immediate. Consider now a vector of moments µ in P ⊥ ⊥ . Given v ∈ V a , Lemma 8.13 and the definition of G ⊥ ⊥ ensure that, according to µ</p><formula xml:id="formula_274">C v , variable X v is independent from its non- descendants in G ⊥ ⊥ in C v , i.e., C ⊥ ⊥ v \{v}, given its parents in G ⊥ ⊥ , i.e., C ⊥ ⊥ v \v. If v ∈ V s , constraints µ C v = µ Čv p v|pa(v) still implies that X v is independent from its non-descendants in C v given its parents according to µ C v , because by definition of G ⊥ ⊥ , for v ∈ V s , we have pa G ⊥ ⊥ (v) = pa G (v). Theorem 7.3 again enables to conclude that P ⊥ ⊥ ⊆ M(G ⊥ ⊥ ). Inclusion M(G ⊥ ⊥ ) ⊆ P ⊥ ⊥ is immedi- ate.</formula><p>Remark furthermore that M(G ) is generally not a polytope. Indeed, when G = G, this is the reason why (8.2) is not a linear program. An important result of the theorem is that M(G) and M(G ⊥ ⊥ ) are polytopes, and MEU(G, ρ) and MEU(G ⊥ ⊥ , ρ) can therefore be solved using the linear programs max µ∈P v∈V r 〈r v , µ v 〉 and max µ∈P ⊥ ⊥ v∈V r 〈r v , µ v 〉 respectively. In Chapter 9, we will characterize the influence diagrams for which the set of achievable moments is a polytope.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.5">Integer programming using value functions</head><p>The aim of this section is to show the formulation that models MEU(G, ρ) and involving the value functions introduced in Section 6.2.3. While the approach of Section 8.1 is based on a representation of the set of achievable moments M(G) using linear programs, we propose in this section a representation of the set of achievable value functions F(G) using linear programs.</p><p>We start by proving how NLP (6.8) models MEU(G, ρ) in Section 8.5.1, and then linearize it into MILP (6.9) in Section 8.5.2. This formulation has been introduced for the POMDP example in Section 4.4. The proofs are simpler than the ones using the vector of moments. However, the numerical experiments in Section 4.5 show that, in general, it takes longer to solve MILP (6.9) than MILP (6.6). These formulations using value functions will play a key role in Chapter 9.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.5.1">An exact nonlinear formulation</head><p>Consider an influence diagram G = (V s ,V a , A). Let ρ = (X V , p,r) be a parametrization such that X V = v∈V X v the support of the vector of random variables attached to all vertices of G, p = {p v|pa(v) } v∈V s is the collection of fixed and assumed known conditional probabilities, and r = {r v } v∈V r is the collection of reward functions r v : X v → R. We recall that unless G has two disjoint connected components, without loss of generality we can assume that G has a single</p><formula xml:id="formula_275">root vertex C v 0 with v 0 ∈ V s .</formula><p>We recall NLP (6.8)</p><formula xml:id="formula_276">max λ,δ x 0 ∈X 0 p 0 (x 0 )λ C 0 (x 0 ) s.t. λ C v = r C v + u∈V s : C u ∈V s ∩ch(C v ) x u λ C u p u|pa(u) + u∈V a : C u ∈V s ∩ch(C v ) x u λ C u δ u|pa(u) , ∀v ∈ V.</formula><p>We have Theorem 6.5 which we recall here with the notation introduced above: Theorem 6.5. Let (λ, δ) be a feasible solution of NLP <ref type="bibr">(6.8)</ref>. Then λ is the vector of value functions of the probability distribution P δ induced by δ. Furthermore, (λ, δ) is an optimal solu-tion of NLP (6.8) if and only if δ is an optimal policy of MEU(G, ρ). In particular, NLP (6.8) and MEU(G, ρ) have the same optimal value. Theorem 6.5 gives the following corollary: Corollary 8.14. Given any gradual RJT G and any parametrization ρ on G, the set of achievable value functions F G (G, ρ) can be written:</p><formula xml:id="formula_277">F G (G, ρ) =            λs.t. ∃δ ∈ ∆ : λ C v = r C v + u∈V s : C u ∈ch(C v ) x u λ C u p u|pa(u) + u∈V a : C u ∈ch(C v ) x u λ C u δ u|pa(u) , ∀x C v ∈ X C v , ∀v ∈ V           </formula><p>Corollary 8.14 enables to write NLP (6.8) as max λ∈F (G) v∈V r 〈r 0 , λ 0 〉, and it will be useful in the proofs Chapter 9 because it characterizes the set of achievable value functions by the set of constraints of (6.8).</p><p>Proof of Corollary 8.14. Let λ be a vector such that there exists δ ∈ ∆ and</p><formula xml:id="formula_278">λ C v = r C v + u∈V s : C u ∈ch(C v ) x u λ C u p u|pa(u) + u∈V a : C u ∈ch(C v ) x u λ C u δ u|pa(u) .</formula><p>Hence, (λ, δ) is a feasible solution of NLP (6.8). Theorem 6.5 ensures that λ is the vector of value functions of P δ . Therefore, λ ∈ F(G).</p><p>Let λ ∈ F(G). Hence, there exists δ ∈ ∆ such that for any v ∈ V and x C v ∈ X C v we have:</p><formula xml:id="formula_279">λ C v (x C v ) = E δ C ∈des(C v ) r C (X C )|X C v = x C v = r C v (x C v ) + E δ C ∈des(C v ) r C (X C )|X C v = x C v = r C v (x C v ) + u∈V : C u ∈ch(C v ) E δ C ∈des(C u ) r C (X C )|X C v = x C v = r C v (x C v ) + u∈V : C u ∈ch(C v ) x u ∈X u P δ (X u = x u |X C v = x C v )E δ C ∈des(C u ) r C (X C )|X C v = x C v , X u = x u = r C v (x C v ) + u∈V : C u ∈ch(C v ) x u ∈X u P δ (X u = x u |X pa(u) = x pa(u) )E δ C ∈des(C u ) r C (X C )|X C u = x C u</formula><p>where the last equality comes from the fact that</p><formula xml:id="formula_280">X u ⊥ ⊥ X V \des(u) |X pa(u) P δ and X des(C u ) ⊥ ⊥ X C v |X C u P δ . Since P δ X v |X pa(v) = p v|pa(v) if v ∈ V s and P δ X v |X pa(v) = δ v|pa(v) if v ∈ V a , we obtain that (λ, δ) satisfies λ C v = r C v + u∈V s : C u ∈ch(C v ) x u λ C u p u|pa(u) + u∈V a : C u ∈ch(C v ) x u λ C u δ u|pa(u) ,</formula><p>which achieves the proof.</p><p>Proof of Theorem 6.5. Let now (λ, δ) be a feasible solution of 6.8. Then δ is a feasible solution of MEU(G, ρ). We now prove that λ corresponds to the vector of value functions of the distribution P δ induced by δ, from which we can deduce that</p><formula xml:id="formula_281">E δ v∈V r r v (X v ) = x v 0 p(x v 0 )λ C 0 (x v 0 ).</formula><p>We prove by induction that for each vertex</p><formula xml:id="formula_282">C ∈ V, λ C (x C ) = E δ   C ∈des(C ) x C \C r C (X C )|X C = x C   .</formula><p>Let G be a topological ordering on the RJT G. We denote the set of vertices by V = {C 1 , . . . ,C n } where C i C j if, and only if, i j . We want to prove the induction hypothesis</p><formula xml:id="formula_283">λ C i (x C i ) = E δ C ∈des(C i ) x C \C i r C (X C )|X C i = x C i for i ∈ {1, . . . , n}. If i = n, then C n is a leaf of G,<label>and</label></formula><formula xml:id="formula_284">des(C n ) = . Hence, constraints (6.8b) ensure that λ C n (x C n ) = r C n (x C n ). Now we assume the induction hypothesis is true until i + 1 for 1 &lt; i &lt; n. By definition of a topological ordering if C k ∈ des(C i ), then k i . Constraints (6.8b) ensure that λ C i = r C i (x C i ) + k i : C k ∈ch(C i ) x k f k (x k |x pa(k) )λ C k (x C k ), where f k (x k |x pa(k) ) = δ k (x k |x pa(k) ) when k ∈ V a and f k (x k |x pa(k) ) = p k (x k |x pa(k) ) otherwise.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The induction hypothesis ensures that λ</head><formula xml:id="formula_285">C k (x C k ) = E δ   C ∈des(C k ) r C (x C )|X C k = x C k   for all k i .</formula><p>Therefore, we obtain</p><formula xml:id="formula_286">λ C i (x C i ) = r C i (x C i ) + k&gt;i : C k ∈ch(C i ) x k f k (x k |x pa(k) )E δ   C ∈des(C k ) r C (X C )|X C k = x C k   = r C i (x C i ) + k&gt;i : C k ∈ch(C i ) x k f k (x k |x pa(k) ) x des(C k ) P δ X des(C k ) = x des(C k ) |X C k = x C k C ∈des(C k ) r C (x C ).</formula><p>We compute the conditional probabilities</p><formula xml:id="formula_287">P δ X des(C k ) = x des(C k ) |X C k = x C k as follows P δ X des(C k ) = x des(C k ) |X C k = x C k = P δ X des(C k ) = x des(C k ) P δ X C k = x C k = j k:C j ∈des(C k ) f j (x j |x pa( j ) )P δ X Čk = x Čk f k (x k |x pa(k) )P δ X Čk = x Čk = j &gt;k:C j ∈des(C k ) f j (x j |x pa( j ) )</formula><p>Therefore, we have</p><formula xml:id="formula_288">λ C i (x C i ) = r C i (x C i ) + k&gt;i : C k ∈ch(C i ) x k f k (x k |x pa(k) ) x des(C k ) j &gt;k:C j ∈des(C k ) f j (x j |x pa( j ) ) C ∈des(C k ) r C (x C ) = r C i (x C i ) + k&gt;i : C k ∈ch(C i ) x des(C k )\C i j k:C j ∈des(C k ) f j (x j |x pa( j ) ) C ∈des(C k ) r C (x C ) = r C i (x C i ) + k&gt;i : C k ∈ch(C i ) x des(C k )\C i P δ X des(C k ) = x des(C k ) |X Čk = x Čk C ∈des(C k ) r C (x C ) = r C i (x C i ) + k&gt;i : C k ∈ch(C i ) E δ   C ∈des(C k ) r C (x C )|X Čk = x Čk   = r C i (x C i ) + k&gt;i : C k ∈ch(C i ) E δ   C ∈des(C k ) r C (x C )|X C i = x C i   = E δ   C ∈des(C i ) r C (x C )|X C i = x C i  </formula><p>where the fifth equality comes from the fact that X des(C k ) ⊥ ⊥ X C i |X Čk for any distribution P δ factorizing on G. The last equality proves the result for C i . Consequently, λ is the vector of value functions of P δ .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.5.2">Turning the NLP into an MILP</head><p>Like NLP (8.2), NLP (6.8) is hard to solve due to the nonlinear terms λ C v δ v|pa(v) in the constraints. We recall that there always exists at least one optimal strategy which is deterministic (and therefore integral) for MEU(G, ρ), that is a strategy δ satisfying (6.3). Like the approach in Section 8.1, we can therefore add integrality constraints (6.3) to (6.8). Then we replace the term</p><formula xml:id="formula_289">λ C v (x C v )δ v|pa(v) (x fa(v) ) by a variable α C v (x C v ) satisfying α C v (x C v ) = λ C v (x C v )δ v|pa(v) (x fa(v) ), ∀x C v ∈ X C v , ∀v ∈ V. (8.22)</formula><p>We assume that we have access to a vector of upper bound b ub and a vector of lower bound b lb of λ which do not depend on λ. In Section 8.5.3, we will explain how to compute b ub and b lb Now we linearize constraints (8.22) using the following McCormick inequalities</p><formula xml:id="formula_290">α C v (x C v ) λ C v (x C v ), ∀x C v ∈ X C v , ∀v ∈ V α C v (x C v ) λ C v (x C v ) -b ub C v (x C v )(1 -δ v|pa(v) (x fa(v) )), ∀x C v ∈ X C v , ∀v ∈ V α C v (x C v ) b ub C v (x C v )δ v|pa(v) (x fa(v) ), ∀x C v ∈ X C v , ∀v ∈ V α C v (x C v ) b lb C v (x C v )δ v|pa(v) (x fa(v) ), ∀x C v ∈ X C v , ∀v ∈ V (8.23)</formula><p>As explained in Section 8.3, these inequalities are equivalent to the bilinear constraints <ref type="bibr">(8.22)</ref>.</p><p>Hence, the problem we obtain the following MILP max</p><formula xml:id="formula_291">λ,δ x v 0 ∈X v 0 p v 0 (x v 0 )λ C 0 (x v 0 ) s.t. λ C v (x C v ) = r C v (x C v ) + u∈V s : C u ∈ch(C v ) x u λ C u (x C u )p u|pa(u) (x u |x pa(u) ) + u∈V s : C u ∈ch(C v ) x u α C u (x C u ), ∀x C v ∈ X C v , ∀v ∈ V α C v , λ C v , δ v|pa(v) satisfies (8.23) (8.24)</formula><p>The tightness of the linear relaxation of MILP (6.9) depends on the quality of the bounds b ub</p><formula xml:id="formula_292">C v (x C v ) and b lb C v (x C v ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.5.3">Algorithm to compute good quality bounds</head><p>This section provides a dynamic programming equation to compute the vector of upper bounds b ub and lower bounds b ub of λ. Let G = (V, A) be a gradual RJT on G = (V, A). We denote by L G be the set of leafs of G, i.e., L G = {C ∈ V, ch(C ) = }. We define inductively on v the functions</p><formula xml:id="formula_293">b ub C v , b lb C v : X C v → R as follows.                                  b ub C (x C ) = b lb C (x C ) = r C (x C ), ∀x C ∈ X C , ∀C ∈ L G b ub C v (x C v ) = r C v (x C v ) + u∈V s : C u ∈ch(C v ) x u b ub C u (x C u )p u|pa(u) (x u |x pa(u) ) + u∈V a : C u ∈ch(C v ) max x u ∈X u b ub C u (x C u ), ∀x C v ∈ X C v , ∀v ∈ V b lb C v (x C v ) = r C v (x C v ) + u∈V s : C u ∈ch(C v ) x u b lb C u (x C u )p u|pa(u) (x u |x pa(u) ) + u∈V a : C u ∈ch(C v ) min x u ∈X u b lb C u (x C u ), ∀x C v ∈ X C v , ∀v ∈ V Proposition 8.15. If λ belongs to F(G), then b lb λ b ub .</formula><p>Thanks to Theorem 6.5 for any feasible solution (λ, δ) of MILP <ref type="bibr">(8.24)</ref>, λ belongs to F d (G) ⊆ F(G). Hence, we can incorporate these bounds in MILP <ref type="bibr">(8.24)</ref>.</p><p>Proof of Proposition 8.15. We prove the result by induction. We use the notation of the proof of Theorem 6.5.</p><formula xml:id="formula_294">If i = n, then b ub C n (x C n ) = b ub C n (x C n ) = r C n (x C n ) = λ C n (x C n ) and the result is true. We assume that for k &gt; i , b lb C k (x C k ) λ C k (x C k ) b ub C k (x C k ) for all x C k in X C k . Since λ ∈ R(G), we</formula><p>have the following recursion equation</p><formula xml:id="formula_295">λ C v (x C v ) = r C v (x C v ) + u∈V s : C u ∈ch(C v ) x u λ C u (x C u )p u|pa(u) (x u |x pa(u) ) + u∈V a : C u ∈ch(C v ) x u λ C u (x C u )δ u|pa(u) (x u |x pa(u) ) r C v (x C v ) + u∈V s : C u ∈ch(C v ) x u b ub C u (x C u )p u|pa(u) (x u |x pa(u) ) + u∈V a : C u ∈ch(C v ) x u b ub C u (x C u )δ u|pa(u) (x u |x pa(u) ) max δ u ∈∆ u :u∈V a C u ∈ch(C v ) r C v (x C v ) + u∈V s : C u ∈ch(C v ) x u b ub C u (x C u )p u|pa(u) (x u |x pa(u) ) + u∈V a : C u ∈ch(C v ) max δ u|pa(u) ∈∆ u x u b ub C u (x C u )δ u|pa(u) (x u |x pa(u) ) = r C v (x C v ) + u∈V s : C u ∈ch(C v ) x u b ub C u (x C u )p u|pa(u) (x u |x pa(u) ) + u∈V a : C u ∈ch(C v ) max x u x u b ub C u (x u , x Ču )</formula><p>By replacing the max operator with min operator and by reversing the inequalities, we obtain that λ b lb .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.5.4">Strengthening the linear relaxation</head><p>The aim of this section is to introduce an MILP, which is similar to MILP (6.9), that uses the set C ⊥ ⊥ defined in Section 8.2 and gives tighter linear relaxation in practice.</p><p>In this section, we introduce an MILP that uses the conditional probabilities P(X</p><formula xml:id="formula_296">C ⊥ ⊥ v |X C ⊥ ⊥ v</formula><p>), which appear in the valid inequalities (6.7), and leads to a linear relaxation that is tighter in practice.</p><p>Like for MILP (6.6), since δ are continuous variables, the McCormick's inequalities <ref type="bibr">(8.23)</ref> do not longer ensure that the nonlinear constraints α C v = λ C v δ v|pa(v) are satisfied. Hence, the vector λ of a feasible solution (λ, δ) of the linear relaxation of MILP (6.9) is not necessary the vector of value function induced by P δ . Actually, like for MILP (6.6) we wish to reduce the feasible set of the linear relaxation of MILP (6.9). However, unlike MILP (6.6) we are not able to derive valid inequalities for the variables of MILP (6.9). Instead of introducing valid inequalities for MILP (6.9), we introduce a formulation that uses the conditional independences of X C ⊥ ⊥ from θ V a given X C ⊥ ⊥ , and that gives McCormick's bound on the variables λ that are tighter.</p><p>We introduce this formulation in three steps to obtain another formulation that uses these independences. First, we introduce another NLP that gives an optimal strategy of MEU(G, ρ). Second, we turn this NLP into an MILP using the McCormick's inequalities. Third, we compute a vector of lower bounds b lb,c and upper bounds b ub,c of any feasible solution and we prove that these bounds are respectively greater than b lb and smaller than b ub . This approach generalizes the one we use for the POMDP with memoryless policies in Section 4.4.4.</p><p>A nonlinear formulation. We introduce the following NLP:</p><formula xml:id="formula_297">max λ,δ x v 0 ∈X v 0 p v 0 (x v 0 )λ C 0 (x v 0 ) s.t. λ C v = r C v + u∈V s : C u ∈ch(C v ) x u λ C u p u|pa(u) + u∈V a : C u ∈ch(C v ) x C ⊥ ⊥ u ,x u λ C u p C ⊥ ⊥ u |C ⊥ ⊥ u δ u|pa(u) , ∀x C v ∈ X C v , ∀v ∈ V δ ∈ ∆<label>(8.25a) (8.25b) (8.25c)</label></formula><p>The constraints of NLP <ref type="bibr">(8.25)</ref> are similar to the ones of NLP (6.8) except that Constraints (8.25b) differ from Constraints (6.8b). Indeed, we replaced λ C v (x C v ) by the expected value</p><formula xml:id="formula_298">x C ⊥ ⊥ v P δ (X C ⊥ ⊥ v = x C ⊥ ⊥ v |X C ⊥ ⊥ v = x C ⊥ ⊥ v )λ C v (x C ⊥ ⊥ v , x C ⊥ ⊥ v</formula><p>). It turns out a feasible solution of NLP (8.25) is not necessary a vector of value functions. Fortunately, Proposition 8.16 below ensures that despite the loss of the value function property of Theorem 6.5, any feasible strategy δ gives the same objective value for MEU(G, ρ) and NLP <ref type="bibr">(8.25)</ref>. Proposition 8.16. Let (λ, δ) be a feasible solution of NLP <ref type="bibr">(8.25)</ref>. Then, (λ, δ) satisfies</p><formula xml:id="formula_299">E δ C ∈des(C ) r C (X C ) = x C P δ (X C = x C )λ C (x C ),<label>(8.26)</label></formula><p>for any C in V. In particular, x 0 ∈X v 0 p(x 0 )λ v 0 (x 0 ) = E δ v∈V r r v (X v ) .</p><p>Proof. Let (λ, δ) be a feasible solution of NLP <ref type="bibr">(8.25)</ref>. Using the notation of the proof of Theorem 6.5, we prove that (8.26) holds by induction from i = n to i = 1. For i = n, we have</p><formula xml:id="formula_300">λ C n (x C n ) = r C n (x C n )</formula><p>. Therefore, we obtain</p><formula xml:id="formula_301">x Cn P δ (X C n = x C n )λ C n (x C n ) = x Cn P δ (X C n = x C n )r C n (x C n ) = E δ r C n (X C n ) ,</formula><p>which proves the case i = n. Now we assume that the induction hypothesis is true until i +1 for 1 &lt; i &lt; n. By definition of a topological ordering if C k ∈ des(C i ), then k &gt; i . We compute the right-hand side of Equation (8.26) using constraints (8.25b)</p><formula xml:id="formula_302">x Cv P δ (X C v = x C v )λ C v (x C v ) = x Cv P δ (X C v = x C v )r C v (x C v ) + u∈V s : C u ∈ch(C v ) x u ,x Cv P δ (X C v = x C v )λ C u p u|pa(u) + u∈V a : C u ∈ch(C v ) x Cv P δ (X C v = x C v ) x C ⊥ ⊥ u ,x u λ C u p C ⊥ ⊥ u |C ⊥ ⊥ u δ u|pa(u) = x Cv P δ (X C v = x C v )r C v (x C v ) + u∈V s : C u ∈ch(C v ) x Cu P δ (X Ču = x Ču )p u|pa(u) λ C u + u∈V a : C u ∈ch(C v ) x Cu P δ (X Ču = x Ču ) x C ⊥ ⊥ u ,x u λ C u p C ⊥ ⊥ u |C ⊥ ⊥ u δ u|pa(u) = x Cv P δ (X C v = x C v )r C v (x C v ) + u∈V s : C u ∈ch(C v ) z *</formula><p>vf c the optimal value of NLP (4.24).</p><p>Theorem 8.17. Let (λ, δ) be a feasible solution of NLP <ref type="bibr">(8.25)</ref>. Then, (λ, δ) is an optimal solution of NLP <ref type="bibr">(8.25)</ref> if and only if δ is an optimal strategy of MEU(G, ρ). In particular, the optimal values of MEU(G, ρ) and NLP <ref type="bibr">(8.25)</ref> are equal.</p><p>Proof. The proof is immediate from Proposition 8.16.</p><p>Turning NLP <ref type="bibr">(8.25</ref>) into an MILP. Again, we can linearize the constraints (8.25b) by introducing the variables α and the McCormick's inequalities <ref type="bibr">(8.23)</ref>. We obtain the following MILP:  As we showed in the numerical experiments in Section 4.5 for the POMDP example, the optimal value of the linear relaxation of MILP <ref type="bibr">(8.27)</ref> is not greater than the optimal value of the linear relaxation of MILP (6.9).</p><formula xml:id="formula_303">max λ,α,δ x v 0 ∈X v 0 p v 0 (x v 0 )λ C 0 (x v 0 ) s.t. λ C v = r C v + u∈V s : C u ∈ch(C v ) x u λ C u p u|pa(u) + u∈V a : C u ∈ch(C v ) x C ⊥ ⊥ u ,x u p C ⊥ ⊥ u |C ⊥ ⊥ u α C u , ∀x C v ∈ X C v , ∀v ∈ V McCormick(α, λ, δ) δ ∈ ∆ d</formula><formula xml:id="formula_304">C v : X C v → R as follows.                                      b lb,c C (x C ) = b lb C (x C ) = r C (x C ), ∀x C ∈ X C , ∀C ∈ L G b lb,c C v (x C v ) = r C v (x C v ) + u:C u ∈ch(C v ) u∈V s x u b ub,c C u (x C u )p u|pa(u) (x u |x pa(u) ) + u:C u ∈ch(C v ) u∈V a min x u ∈X u x C ⊥ ⊥ u p C ⊥ ⊥ u |C ⊥ ⊥ u (x Ču )b ub,c C u (x C u ), ∀x C v ∈ X C v , ∀v ∈ V b ub,c C v (x C v ) = r C v (x C v ) + u:C u ∈ch(C v ) u∈V s x u b ub,c C u (x C u )p u|pa(u) (x u |x pa(u) ) + u:C u ∈ch(C v ) u∈V a max x u ∈X u x C ⊥ ⊥ u p C ⊥ ⊥ u |C ⊥ ⊥ u (x Ču )b ub,c C u (x C u ), ∀x C v ∈ X C v ,</formula><p>Proof of Proposition 8.18. Let (λ, δ) be a feasible solution of NLP <ref type="bibr">(8.25)</ref>. We prove the result by induction. We use the notation of the proof of Theorem 6.5.</p><formula xml:id="formula_305">If i = n, then b lb,c C n (x C n ) = b ub,c C n (x C n ) = r C n (x C n ) = λ C n (x C n</formula><p>) and the result is true. We assume that for k &gt; i , b lb,c</p><formula xml:id="formula_306">C k (x C k ) λ C k (x C k ) b ub,c C k (x C k ) for all x C k in X C k .</formula><p>Since λ satisfies constraints (8.27b), we have the following recursion equation</p><formula xml:id="formula_307">λ C i (x C i ) = r C i (x C i ) + u∈V s : C u ∈ch(C i ) x u λ C u (x C u )p u|pa(u) (x fa(u) ) + u∈V a : C u ∈ch(C i ) x u ,x C ⊥ ⊥ u λ C u (x C u )δ u|pa(u) (x fa(u) )p C ⊥ ⊥ u |C ⊥ ⊥ u (x Ču ) max δ u : u∈V a C u ∈ch(C i ) r C i (x C i ) + u∈V s : C u ∈ch(C i ) x u b ub,c C u (x C u )p u|pa(u) (x fa(u) ) + u∈V a : C u ∈ch(C i ) x u ,x C ⊥ ⊥ u δ u|pa(u) (x fa(u) )p C ⊥ ⊥ u |C ⊥ ⊥ u (x Ču )b ub,c C u (x C u ) r C i (x C i ) + u∈V s : C u ∈ch(C i ) x u b ub,c C u (x C u )p u|pa(u) (x u |x pa(u) ) + u∈V a : C u ∈ch(C i ) max x u x C ⊥ ⊥ u p C ⊥ ⊥ u |C ⊥ ⊥ u (x Ču )b ub,c C u (x C u )</formula><p>The first inequality comes by using the induction hypothesis and the last inequality comes from the fact that max δ u x u δ u|pa(u) (x fa(u) )</p><formula xml:id="formula_308">x C ⊥ ⊥ u p C ⊥ ⊥ u |C ⊥ ⊥ u (x Ču )b ub,c C u (x C u ) = max x u x C ⊥ ⊥ u p C ⊥ ⊥ u |C ⊥ ⊥ u (x Ču )b ub,c C u (x C u ),</formula><p>because an optimum is reached on a vertex of the simplex ∆ u . By replacing the max operator with min operator and by reversing the inequalities, we obtain that λ b lb,c . Now we prove that b ub,c b ub . We prove the result by induction. It holds for i = n. We assume that the induction hypothesis holds for every k &gt; i . By definition of b lb,c , we have:</p><formula xml:id="formula_309">b ub,c C i (x C i ) = r C i (x C i ) + u∈V s : C u ∈ch(C i ) x u b ub,c C u (x C u )p u|pa(u) (x u |x pa(u) ) + u∈V a : C u ∈ch(C i ) max x u ∈X u x C ⊥ ⊥ u p C ⊥ ⊥ u |C ⊥ ⊥ u (x Ču )b ub,c C u (x C u ) r C i (x C i ) + u∈V s : C u ∈ch(C i ) x u b ub,c C u (x C u )p u|pa(u) (x u |x pa(u) ) + u∈V a : C u ∈ch(C i ) x C ⊥ ⊥ u p C ⊥ ⊥ u |C ⊥ ⊥ u (x Ču ) =1 max x u ∈X u b ub,c C u (x C u ) r C i (x C i ) + u∈V s : C u ∈ch(C i ) x u b ub,c C u (x C u )p u|pa(u) (x u |x pa(u) ) + u∈V a : C u ∈ch(C i ) max x u ∈X u b ub C u (x C u ) = b ub C i (x C i )</formula><p>Again, by replacing the max operator with min operator and by reversing the inequalities, we obtain that b lb,c b lb .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.6">Numerical Experiments</head><p>In this thesis, we have introduced the MILP formulation <ref type="bibr">(8.6)</ref> for MEU(G, ρ), and shown with Corollary 9.9 that, when strengthened with valid inequalities and well-chosen bounds in the McCormick constraints, the bounds provided by the linear relaxation of our formulations are better than the ones obtained by the soluble relaxations used in the literature. In this section, we study how these formulations with moment variables and value function variables behave numerically on instances of Examples 4 and 6.</p><p>Our formulations should not be seen as an alternative to SPU (introduced p. 140) since they have different objectives. SPU is a heuristic that enables to find quickly a good solution, which is generally a local optimum on our instances because these are not soluble influence diagrams.</p><p>Our exact approaches are order of magnitude slower than SPU but find better solutions than SPU and prove small optimality gaps. It is therefore natural to use the two approaches sequentially and warm start the MILP solver with the solution returned by SPU, which we do in all our numerical experiments.</p><p>Given their importance to reduce the optimality gaps, we study carefully the impact of our valid inequalities on the linear relaxation bound. For notational simplicity, and since it is unambiguous, in the rest of this section, we use the same notation to refer to a given vertex of the graph and to refer to the random variable associated with this vertex.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.6.1">Experimental settings</head><p>Experiments performed on each instance. We have introduced two elements to strengthen the linear relaxation of our MILP formulation. We remind the reader that we introduced in Equation ( <ref type="formula" target="#formula_225">8</ref> </p><formula xml:id="formula_310">v∈V r 〈r v , µ v 〉 | (µ, δ) ∈ Q,δ ∈ ∆ d with four different sets Q: Q 1 = P × ∆ d ∩Q 1 (no cuts), Q b = P × ∆ d ∩Q b (McCormick only), Q ⊥ ⊥,1 = P ⊥ ⊥ × ∆ d ∩ Q 1 (independence cuts only), and Q ⊥ ⊥,b = P ⊥ ⊥ × ∆ d ∩ Q b (McCormick</formula><p>and independence cuts). Also, we denote respectively by Q vf and Q vf,⊥⊥ the feasible set of MILP (6.9) and <ref type="bibr">(8.27)</ref>, where vf means "value functions." Instances considered. Examples 4 and 6 are multistage models. Let T denote the number of time steps of an instance. Once T has been chosen, the influence diagram G is known, and all that is left to do is to choose a parametrization ρ. We consider instances such that, for all v ∈ fa(V a ), X v has k a elements, and, for all v ∈ V \fa(V a ), X s has k s elements. As we explain in the next paragraph, k s , k a and T control how hard the problem is. To generate a PID instance, we start by choosing (k s , k a , T ), which also sets the influence diagram, and then we draw uniformly on [0, 1] the conditional probabilities p v|pa(v) for all v ∈ V \V a and x fa(v) ∈ X fa(v) and we normalize, and we draw uniformly on [0, 10] the rewards r v (x v ) for all v ∈ V r and all x v ∈ X v .</p><p>For our results to be representative of any instance with parameters (k s , k a , T ), we generate 50 instances for each triplet, and report averaged results over these 50 instances.</p><p>Intrinsic difficulty of the instances considered. Solving an influence diagram requires to find an optimal strategy, which is difficult because evaluating a given strategy is already difficult in the first place, and because optimizing on the set of strategies is then also difficult. The difficulty of evaluating a strategy is the difficulty of solving an inference problem on the underlying graph. A good indicator of this difficulty is therefore the treewidth of the graph. There is no measure that characterizes the intrinsic complexity of the problem of finding an optimal strategy, but the cost of the naive approach is the number of feasible deterministic strategies <ref type="bibr" target="#b97">[98]</ref>, i.e., ∆ d . Our instances have a moderate treewidth, 2 for Example 4 and 3 for Example 6, and are therefore not difficult from an inference point of view. But they could be a priori difficult from an optimization point of view, because</p><formula xml:id="formula_311">∆ d = v∈V a X v u∈pa(v) X u = k T k s a is large.</formula><p>Size of our MILP formulations on the instances considered. The number of constraints and variables in our MILP <ref type="foot" target="#foot_17">4</ref> is in O(|V |κ ω r +1 ), where ω r is the rooted treewidth of the influence diagram, and κ = max v∈V |X v |.. Our MILP formulations can therefore only deal with instances of moderate rooted treewidth, which can be arbitrarily larger than the treewidth. In our examples, the rooted treewidth is equal to the treewidth and no greater than 3, while κ = max(k s , k a ), and so the size of the MILPs remains tractable for instances with large |∆ d |.</p><p>Experimental settings. All MILPs have been written in Julia <ref type="bibr" target="#b17">[18]</ref> with the JuMP <ref type="bibr" target="#b40">[41]</ref> interface and solved using Gurobi 9.0 <ref type="bibr" target="#b51">[52]</ref> with the default settings. Experiments have been run on a server with 192Gb of RAM and 32 cores at 3.30GHz.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Reported results.</head><p>The numerical results obtained on Examples 4 and 6 are reported in Table 8.1. We denote by z, z LR , and z B the value of the best integer solution found, the optimal value of the linear relaxation, and the best upper bound found, respectively. We define the integrality gap g i as z LR -z z LR , and the final gap g f as z B -z z B . Let z SPU be the value obtained using SPU. We define the improvement with respect to SPU i SPU as i SPU = z-z SPU z SPU . Each line in Table <ref type="table" target="#tab_43">8</ref>.1 provides average values of different quantities on 20 instances with identical parameter (k s , k a , T ). The first column specifies the value of (k s , k a , T ) for the instances considered, the second the approximate number of admissible strategies. The third column indicates the cuts used. In the next three columns, we report the average value of g i , g f , i SPU on the 20 instances considered. Column "Opt" provides the percentage of instances solved to optimality, and column "Time" the average computing time. All gaps are given in percent, and computing times are given in seconds. Sometimes, the time limit is reached only for some of the 20 instances, and we end up with a non-zero average final gap together with an average computing time that is smaller than the time limit.</p><formula xml:id="formula_312">-s 1 s 1 -o 1 s 1 o 1 -u 1 s 1 o 1 u 1 -a 1 s 1 o 1 a 1 -v 1 v 1 -r 1 s 1 v 1 -s 2</formula><p>Figure <ref type="figure" target="#fig_53">8</ref>.5 -RJT for the chess game. The element to the right of -is the offspring.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.6.2">Bob and Alice daily chess game</head><p>We consider the chess game example represented in Figure <ref type="figure" target="#fig_44">6</ref>.2b. The beginning of the RJT built by Algorithm 4 for this example is represented in Figure <ref type="figure" target="#fig_53">8</ref>.5. The rooted treewidth of this problem is 3. Table <ref type="table" target="#tab_43">8</ref>.1 reports results on the generated instances. We can tackle large instances of this problem: We can reach optimality in less than one hour for a strategy set of size 10 171 , and find a small provable gap on even bigger instances. Moreover, we see that the independence cuts enables to strongly reduce the gaps and the computing time, while the improved McCormick bounds yield more minor improvements. However, on this problem, our MILP formulation only marginally improves the results returned by SPU, and its main value is the bound obtained. One can observe that the results obtained using the value function formulations produce in general poorer results. In particular, the solver does not improve the best bound computed during the resolution, which is outlined by the small difference between the integrality gap and the final gap values.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.6.3">Partially Observable Markov Decision Process with limited memory</head><p>We now consider our POMDP instances. Figure <ref type="figure" target="#fig_44">6</ref>.1b provides the graph representation of the POMDP with limited information. The rooted treewidth of this problem is 2. This influence diagram is not soluble<ref type="foot" target="#foot_18">foot_18</ref> . Figure <ref type="figure" target="#fig_53">8</ref>.6 represents the RJT built by Algorithm 4. On this RJT, G ⊥ ⊥ = G, and thus P ⊥ ⊥ = P , which is also the constraint set of the classical MDP relaxation of a POMDP, in which the decision maker knows the state s t when he makes the decision a t . This MDP relaxation leads to poor lower bounds. We therefore use instead the larger RJT represented in Figure <ref type="figure" target="#fig_53">8</ref>.7. In that RJT, C ⊥ ⊥ a t = s t , so that G ⊥ ⊥ is not anymore equal to G and the valid cuts enable to enforce the independence of s t and a t given (s t -1 , a t -1 , o t ) for t &gt; 1. Table <ref type="table" target="#tab_43">8</ref>.1 provides the numerical results on our instances. This example is harder to solve to optimality. SPU has worse performance as well on this example, and our formulations manage to improve the solution found by SPU. Once again the valid cuts significantly reduce the linear relaxation gap and the solving time, even on large instances. Again, one can observe that the value function formulations give poorer results in general. For large instances, we encourage to use the formulations with moment variables rather than the value function formulations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Polynomial cases of influence diagrams</head><p>This chapter is devoted to prove Theorems 6.7, 6.8 and 6.9. We recall the definition of a soluble influence diagram introduced in Section 6.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Definition 9.1. An influence diagram G is soluble if for any parametrization ρ of G, any local optimum is a global optimum.</head><p>Roughly speaking, a soluble influence diagram is an influence diagram that is "easy" to solve, where "easy" means that "if the inference problem is tractable, then solving the maximum expected utility problem is tractable". In the literature on influence diagrams, the soluble influence diagrams have received several characterizations. One of them is based on the notion of dseparation in directed graphical model introduced in Section 8.2.2, which enables to decide in polynomial time whether an influence diagram is soluble or not. Another characterization says that the standard SPU algorithm of Lauritzen and Nilsson <ref type="bibr" target="#b80">[81]</ref> converges to a global optimum in a finite and polynomial number of time steps, ensuring that given an oracle solving the inference problem in polynomial time, the SPU algorithm returns an optimal solution of MEU(G, ρ) in polynomial time. In this chapter, we provide a new characterization of soluble influence diagrams in terms of moments. The main result consists in saying that the convexity of the set of achievable moments is a necessary and sufficient condition of being soluble. Furthermore, we show that, for soluble influence diagrams, the linear relaxation of MILP (8.9), which is the linear relaxation of MILP (6.6) with valid cuts (6.7), gives an optimal solution of MEU(G, ρ). We have even more: there are IDs which are not soluble that can be solved using the linear relaxation of MILP (8.9). In addition, we propose a linear program formulated using value functions that gives an optimal solution of MEU(G, ρ) for any soluble influence diagrams G. Such a linear program turns out to be the dual of the linear relaxation of MILP (8.9).</p><p>Chapter 9 is organized as follows:</p><p>• Section 9.1 introduces several notions that are required to give the characterizations of the soluble IDs in the literature. • Section 9.2 is devoted to prove Theorem 6.7. First, it recalls the characterizations of the soluble influence diagrams. In particular, it describes how we use the relevance graph, which is a notion of Koller and Friedman <ref type="bibr" target="#b74">[75,</ref><ref type="bibr">Definition 23.9]</ref>, to model the strategic dependences between the decision variables. Second, leveraging this notion, we prove Theorems 6.7 and 6.8.</p><p>• Section 9.3 gives an example showing that there are influence diagrams that are not solubles and such that the linear relaxation of MILP (8.9) gives an optimal strategy for MEU(G, ρ). • Section 9.4 introduces the linear formulations (6.10), <ref type="bibr">(6.11)</ref> based on the value functions variables and it proves Theorem 6.9. • Section 9.5 presents some numerical experiments on an example of non-soluble influence diagrams that can be solved by the linear relaxation of MILP (8.9). In particular, the value obtained by running the SPU algorithm is significantly lower than the optimal value.</p><p>In this chapter, we make the assumption that influence diagrams are such that any vertex v ∈ V has a descendant in the set of utility vertices V r , i.e., V s ∪ V a = anc(V r ). The following remark explains why we can make this assumption without loss of generality.</p><p>Remark 13. Consider a PID (G, ρ) where G = (V, A) and V s is the union of chance vertices V c and utility vertices V r . Let (G , ρ ) be the influence diagram obtained by removing any vertex that is not in V r and has no descendant in V r and restrict ρ accordingly. If a random vector X V factorizes as a directed graphical model on (V, A) and V ⊆ V is such that anc(V ) = V , then X V factorizes as a directed graphical model on the subgraph induced by V with the same conditional probabilities p v|pa <ref type="bibr">(v)</ref> . Hence, given a strategy δ on (G, ρ) and its restriction δ to (G , ρ ), we have</p><formula xml:id="formula_313">E δ v∈V r r v (X v ) = E δ v∈V r r v (X v )</formula><p>where the first expectation is taken in (G, ρ) and the second in (G , ρ ), and the two influence diagrams model the same MEU(G, ρ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.1">Soluble Influence Diagrams</head><p>The aim of this section is to introduce some notions and results, whose proofs can be found in the book of Koller and Friedman <ref type="bibr" target="#b74">[75,</ref><ref type="bibr">Chapter 23.5]</ref>, and that are key in proving Theorems 6.7 and 6.8.</p><p>Consider an influence diagram G = (V, A) with V = V s ∪V a . Three notions, strategic relevance, sreachability and relevance graph, have been introduced in the literature to characterize when a local minimum is also global; see, e.g. <ref type="bibr" target="#b74">[75,</ref><ref type="bibr">Chapter 23.5]</ref>. A decision vertex v strategically relies on u if the choice of a locally optimal policy δ v given (δ w ) w =v depends on δ u for some parametrization ρ. A decision vertex u is s-reachable from a decision vertex v if ϑ u is not dseparated from des(v) given fa(v):</p><formula xml:id="formula_314">ϑ u ⊥ G † des(v) | fa(v),<label>(9.1)</label></formula><p>where G † is the augmented graph defined in Section 8.2.2 and d-separation is defined in the same section. The usual definition is ϑ u ⊥ G † des(v) ∩ V r | fa(v), but these definitions coincide in our setting, since we have assumed that des(v) ∩ V r = for any v ∈ V a . The relevance graph of G is the digraph H with vertex set V a , and whose arcs are the pairs (v, u) of decision vertices such that u is s-reachable from v. Finally, the single policy update algorithm (SPU) <ref type="bibr" target="#b80">[81]</ref> is the standard coordinate ascent heuristic for influence diagrams. It iteratively improves a strategy <ref type="bibr">Koller and Friedman [75,</ref><ref type="bibr">Proposition 23.3]</ref> ensure that this local optimum can be directly derived after performing inference on the influence diagram. In particular, given an oracle solving the inference problem in polynomial time, an iteration of the 9.1. Soluble Influence Diagrams SPU algorithm runs in polynomial time.</p><formula xml:id="formula_315">arg max δ v ∈∆ v E δ v ,δ -v u∈V r r u (X u ) .</formula><p>The following proposition states that the notions of strategy relevance and s-reachability coincide in a certain sense. Proposition 9.1. <ref type="bibr" target="#b74">[75,</ref><ref type="bibr">Theorems 23.2 and 23.3]</ref> Let G = (V, A) be an influence diagram with V = V s ∪ V a , and u and v be two decision vertices in V a . If u is not s-reachable from v, then v does not strategically rely on u, while if u is s-reachable from v, there exists a parametrization ρ such that v strategically relies on u. Proposition 9.1 ensures that the relevance graph fully represents the strategic dependencies between local policies on each decision vertex. This result is key in understanding the theorem that characterizes the soluble influence diagrams, which are easily solved, and provides several equivalent criteria to identify them. Theorem 9.2. <ref type="bibr" target="#b74">[75,</ref><ref type="bibr">Theorem 23.5]</ref> Given an influence diagram G, the following statements characterize a soluble influence diagram.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">For any parametrization ρ of G, any local optimum is a global optimum. 2. For any parametrization ρ of G, SPU converges to a global optimum in a finite and poly-</head><p>nomial number of steps. <ref type="foot" target="#foot_19">1</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The relevance graph of G is acyclic.</head><p>In the literature, the soluble influence diagrams are defined using the third characterization <ref type="bibr" target="#b80">[81]</ref>. Since at each iteration of SPU algorithm, the complexity of local maximization operation only depends on the complexity of solving the inference problem, Theorem 9.2 ensures that the difficulty of solving soluble influence diagrams reduces to the difficulty of solving the inference problem on the underlying directed graphical model. In addition, the characterizations in Theorem 9.2 are key in determining if an influence diagram is soluble. In particular, it gives the following proof of Proposition 6.6.</p><p>Proof of Proposition 6.6. Thanks to Theorem 9.2, determining if an influence diagram is soluble is equivalent to determine if the relevance graph is acyclic. Building the relevance graph consists in determining for each decision vertex u, the set {v ∈ V a : ϑ v ⊥des(u)|fa(u)}. The Bayes-Ball algorithm determines such a set in polynomial time O(|V | + |A|) <ref type="bibr" target="#b137">[138,</ref><ref type="bibr">Theorem 4]</ref>. Hence, we can build the relevance graph in O |V a |(|V | + |A|) . Checking if a directed graph is acyclic can be done in polynomial time using Kahn's algorithm <ref type="bibr" target="#b64">[65]</ref>. Therefore, the time complexity is polynomial, which achieves the proof.</p><p>Note that Proposition 6.6 is never mentioned or proved in the literature. However, it is common knowledge that the d-separation can be checked in polynomial time, an thus deciding if a graph is soluble can be done in polynomial time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.2">Linear program for soluble influence diagrams</head><p>The aim of this section is to prove Theorem 6.7 (Section 9.2.1) and Theorem 6.8 (Section 9.2.2). In addition, we show that the linear relaxation of MILP (8.9) is always better than a "soluble relaxation" on the RJT, a notion we introduce in Section 9.2.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.2.1">Linear relaxations</head><p>While Theorem 9.2 characterizes the soluble influence diagrams using the relevance graph, we relate this notion to the integer programs of Chapter 8 using Theorem 6.7 which we recall here: Theorem 6.7. If G is soluble, then there exists an RJT, such that, for every parametrization ρ, an optimal solution of the linear relaxation of MILP (6.6) with the valid inequalities (6.7) induces an optimal solution of MEU(G, ρ) and both problems have the same optimal values. Such an RJT can be computed in polynomial time. Theorems 6.7 and 8.1 imply that, if G is soluble, then MILP (8.9) reduces to the linear program max</p><formula xml:id="formula_316">µ∈P ⊥ ⊥ v∈V r 〈r v , µ v 〉.</formula><p>However, it is not a sufficient condition for being soluble. Indeed, we will show in Section 9.3 that there are some influence diagrams that are not soluble and such that the linear relaxation of MILP (8.9) induces an optimal strategy of MEU(G, ρ). Theorem 6.7 is a corollary of Theorem 8.12 and the following lemma.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 9.3.</head><p>There exists an RJT G such that G ⊥ ⊥ = G if and only if G is soluble. Such an RJT can be computed using Algorithm 5.</p><p>The proof of this lemma is postponed at the end of the section because of the length of its proof. Note that based on a topological order of the relevance graph, Algorithm 5 proceeds by computing a larger graph (satisfying perfect recall) that contains the graph G and that assigns the same parent sets to elements of V s as in G, then uses a topological order of this graph to order the vertices of G for the computation of a rooted junction tree.</p><p>Algorithm 5 Build a "good" RJT for a soluble graph G 1: Input: An ID G = (V, A). 2: Initialize: A = . 3: Compute the relevance graph H of G 4: Compute an arbitrary topological order H on V a for the relevance graph H</p><formula xml:id="formula_317">5: A ← A ∪ {(u, v) ∈ V a × V a : u H v} 6: G ← (V, A ) 7: A ← A ∪ {(u, v) ∈ V s × V a : u ∉ des G (v)} 8: A = (V, A ) 9:</formula><p>Compute an arbitrary topological order on G 10: Return the result of Algorithm 4 for (G, )</p><p>Proof of Theorem 6.7. Note that by definition for any feasible solution (µ, δ) of the linear relaxation of MILP (8.9), we have µ ∈ P ⊥ ⊥ . If G is a soluble influence diagram, then Lemma 9.3 ensures that there exists an RJT G such that G = G ⊥ ⊥ . Consider such an RJT G built by Algorithm 5. Since G = G ⊥ ⊥ , we obtain M G (G) = M G (G ⊥ ⊥ ). On the other hand, Theorem 8.12 ensures that for every parametrization ρ, we have M G (G ⊥ ⊥ , ρ) = P ⊥ ⊥ . We deduce that for such an RJT, we obtain M G (G, ρ) = P ⊥ ⊥ . Therefore, for any optimal (µ, δ) solution of the linear relaxation of MILP (8.9), there exists a strategy δ ∈ ∆ such that µ is the vector of moments of P δ and the optimal values are equal because max As an immediate consequence of Lemma 9.4, if G is soluble and is a topological order on G, then its restriction H to V a is a topological order on the relevance graph H .</p><formula xml:id="formula_318">µ∈P ⊥ ⊥ v∈V r 〈r v , µ v 〉 = max µ∈M G (G,ρ) v∈V r 〈r v , µ v 〉 = M EU (G, ρ).</formula><p>Proof. Assume that u is s-reachable from v, that is (v, u) is an arc in H . We first show that this implies that u and v have descendants in common. Indeed, by definition of s-reachability, this means that there exist w ∈ des(v) and an active trail T from ϑ u to w. Either, T is a directed path and w is also a descendant of u or T must have a -structure. In the latter case, let x be the vertex with the -structure closest to ϑ u on T ; since the trail is active, we must have x ∈ fa(v) but since x is a descendant of u, in that case, v must be a descendant of u. In both cases considered u and v have descendants in common. Now, if u is not a descendant of v, then v is s-reachable from u, which is not possible as H is acyclic. Hence u ∈ des(v).</p><p>Proof of Lemma 9.3. Let G be a soluble influence diagram. We start by proving that Algorithm 5 with G as an input returns an RJT G. It suffices to show that it is possible to compute topological orderings in step 9, that is, to prove that H , G and G , defined in Algorithm 5, are acyclic. H is acyclic because the influence diagram is soluble. We now prove that G is acyclic. As G is acyclic and by definition of G , a cycle in G contains necessarily two vertices of V a . Let u and v thus be two distinct elements of V a . Remark that, if there exists a path from u to v in G, then v is strategically reachable from u, and u H v. Hence, by definition of G , the indexes of vertices in V a for H can only increase along a path in G . There is therefore no cycle in G containing two vertices in V a , and thus no cycle in G . We now prove that G is acyclic. Suppose that there is a cycle in G . Let G be a topological order on G , and let v h be the smallest vertex v for that v l ∈ V a . Arc (u l , v l ) is possibly identical to (u h , v h ). By definition of G , given two disjoint vertices u and v in V a , either (u, v) ∈ A or (v, u) ∈ A . Since v h G v l by definition of v h , we have either v h = v l or (v h , v l ) ∈ A . And since all the arcs in the v l -u h subpath of the cycle are in A , we have u h ∈ des G (v l ). Hence u h ∈ des G (v h ), which contradicts the definition of E in</p><p>Step 7. Hence, Algorithm 5 always returns an RJT, which we denote by G.</p><p>It remains to prove that G is such that C ⊥ ⊥ v ⊆ fa(v) for each decision vertex v ∈ V a . We start with two preliminary results. Remark that A ⊆ A implies that is a topological order on G. Let H denotes its restriction to V a . Lemma 9.4 ensures that H is a topological order on H . Hence, we have</p><formula xml:id="formula_319">ϑ V a ≺v ⊥ des(v) | fa(v), for all v ∈ V a . (9.2)</formula><p>Furthermore, if u ∈ V a and v ∈ V s u , the definition of G implies the existence of a path from u to v in G, and hence the existence of a path from V a u to v in G. We now prove C ⊥ ⊥ v ⊆ fa(v) for each v ∈ V a . This part of the proof is illustrated on Figure <ref type="figure" target="#fig_66">9</ref>.1.a. Let v be a vertex in V a , let u ∈ C v \fa(v), and let b ∈ V a ≺u . We only have to prove that u is d-separated from ϑ b given fa(v). We start by proving that u and v have common descendants. Proposition 7.9 guarantees that (7.7b) is an equivalence. Hence, there exists a u-v trail in V v . Consider such a u-v trail Q with a minimum number of -structures. Suppose for a contradiction that Q has more than one -structure. Starting from v, let w 1 be the first -structure of Q and u 1 bet its first vertex with diverging arcs u 1 . Using the result at the end of the previous paragraph, we have u 1 ∈ des(V a v ). Since Q has been chosen with a minimal number of v-structures, we obtain u 1 ∈ des(V a v ). Let a 1 denote an ancestor of u 1 in V a v . Since w 1 ∈ des(v), Equation (9.2) ensures that w 1 ⊥ ϑ v | fa(a 1 ). Hence, the v-w i path is not active given fa(a 1 ), and it therefore necessarily intersects pa(a i ). Hence, u 1 ∈ des(v), and Q there exists a u-v trail Q with fewer -structures than Q, which gives a contradiction. Trail Q therefore has a unique v-structure, and u and v have a common descendant w. Hence, if ϑ b -u trail P is active given fa(v), then P followed by a u-w path is active given fa(v). The fact that des(v) ⊥ ϑ b |fa(v) ensures that there is no-such path P , and we have proved that u is d-separated from ϑ b given fa(v).</p><p>Conversely, let G be a non-soluble influence diagram, and G an RJT on G. Let u and v be two vertices in V a such that des(v) ⊥ϑ u |pa(v) and des(u) ⊥ϑ v |pa(u). Without loss of generality, we assume that if there is a path between C u and C v , it is from C v to C u . To prove the converse, we prove that C ⊥ ⊥ u = fa(u). This part of the proof is illustrated on Figure <ref type="figure" target="#fig_66">9</ref>.1.b. There exists an active trail Q from w ∈ des(u) to ϑ v given pa(u). Starting from w, let x be the first vertex with diverging arcs of Q if Q contains such a structure, and be equal to v otherwise. And let P be the w-x subtrail of Q. Remark that P must be an x-w path in G, because any passing -structure on P cannot be at a descendant of w, for it would then be a descendant of u which could not have any descendant in fa(u) as G is acyclic. The path P contains no -structure, and is active given fa(u). Hence, it does not intersect fa(u). Since x and u have w as common descendant, Proposition 7.4 ensures that C x and C u are on the same branch of G. If v = x, x ∈ anc(w) and there is a path in G from C x to C w , moreover, since we assumed C u is a descendant of C v in G, and since u ∈ anc(w), then the path from C x to C w contains C u and all the vertices of P . Now, if x = v, then x is the first vertex with diverging arcs, and in that case it belongs to anc(u), because Q \ P must contain at least one -structure and any such -structure can only be at a vertex in anc(u). So, again, there is a path in G from C x to C w which contains C u and all the a) vertices of P . Starting from x, let y be the last vertex of P such that C y is above C u in G, and z be the child of y in P . But since Q is active, the y-ϑ v subtrail of Q is active given fa(u), and we therefore have C ⊥ ⊥ u = fa(u).</p><formula xml:id="formula_320">V v b u v w u 1 a i w 1 Q pa(v) P b) v x pa(u) y u z w C w C z C u C y C x</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.2.2">Characterization using the set of achievable moments</head><p>In this section, we give another characterization of soluble influence diagrams using Theorem 6.8 which we recall here: Theorem 6.8. An influence diagram G is soluble if and only if there exists an RJT G such that for every parametrization ρ on G, the set of achievable moments M G G, ρ is a polytope. In this case, such an rooted junction tree G can be computed in polynomial time.</p><p>In fact, Theorem 6.8 is a corollary of the following stronger result: Theorem 9.5. If G is not soluble then there exists a parametrization ρ such that, for every rooted junction tree G, the set of achievable moments M G (G, ρ) is not convex.</p><p>If G is soluble, then there exists an rooted junction tree G such that for every parametrization ρ, the set of achievable moments M G (G, ρ) coincides with P ⊥ ⊥ . Such an rooted junction tree can be computed using Algorithm 5.</p><p>The set of achievable moments fully characterizes the soluble influence diagrams. To visualize the form of the set of achievable moments, we introduce the following the lemma: </p><formula xml:id="formula_321">∈ ∆ such that µ C v = δ v|pa(v) µ C v \{v} for all v ∈ V a . Since δ ∈ ∆ and by definition ∆ = v∈V a x pa(v) ∈X pa(v) δ v|pa(v) (.|x pa(v) ) ∈ R X v + : x v δ v|pa(v) (x v |x pa(v) ) = 1 ∆ v (x pa(v) )</formula><p>.</p><p>In addition, for any v ∈ V a and x pa(v) ∈ X pa(v) , we have:</p><formula xml:id="formula_322">∆ v (x pa(v) ) = Conv δ v|pa(v) (.|x pa(v) ) ∈ {0, 1} X v : x v δ v|pa(v) (x v |x pa(v) ) = 1 ∆ d v (x pa(v) )</formula><p>.</p><p>Therefore, we obtain that ∆ = v∈V a x pa(v) ∈X pa(v) Conv(∆ d v (x pa(v) )). It is known that the convex hull of a Cartesian product is the Cartesian product of the convex hulls. Therefore,</p><formula xml:id="formula_323">∆ = Conv v∈V a x pa(v) ∈X pa(v) ∆ d v (x pa(v) ) .</formula><p>On the other hand, we have</p><formula xml:id="formula_324">v∈V a x pa(v) ∈X pa(v) ∆ d v (x pa(v) ) = ∆ d . We deduce that ∆ = Conv(∆ d ).</formula><p>Hence, there exists a finite set I of deterministic strategies δ i ∈ ∆ d for any i ∈ I and non-negative scalars (λ i ) i ∈I such that i ∈I λ i = 1 and δ = i ∈I λ i δ i . Therefore,</p><formula xml:id="formula_325">µ C v = i ∈I λ i δ i v|pa(v) µ C v \{v} . Let μ C v i = δ i v|pa(v) µ C v \{v} for all i ∈ I . Then for all v ∈ V a , µ C v = i ∈I λ i μ C v i . For all v ∈ V s , we set μ C v i = µ C v . We deduce that µ = i ∈I λ i μi and μi ∈ M d (G). Therefore µ ∈ Conv M d (G) ,</formula><p>which achieves the proof.</p><p>Since Conv M d (G) is a polytope, Lemma 9.6 gives the following corollary:</p><p>Corollary 9.7. The set of achievable moments is convex if and only if it is a polytope.</p><p>Although we do not use this corollary in the remainder of this thesis, it enables to provide an abstract representation of the set of achievable moments.  is the unique vertex in the intersection of P and Q. Let u and v be the parents of w in P and Q respectively. Consider a parametrization where all the variables that are not in P or Q are unary, all the variables in P and Q are binary, all the variables in the a-u subpath of P are equal to X a , all the variables in the b-v subpath of P are equal to X b , and p w|pa(w) is defined arbitrarily. Let G be an arbitrary junction tree, C be its cluster containing fa(w). Then choosing a distribution µ a as policy δ a and a distribution µ b as policy δ b implies that the restriction of µ C to X uv is µ uv = µ a µ b . Hence, the marginalization on X uv of the set of distributions µ C that can be reached for different policy is the set of independent distributions, which is not convex.</p><formula xml:id="formula_326">ϑ a a s k-1 s 1 s 0 a s k-1 s 1 s k-1 s 1 s 0 t k t 2 t 1 p k p 2 p 1 b w b w s 0 w X s 0 = X s 0 = X w s 0 = {1, 2, e} X s = X s = X s = {1, 2}, ∀ &gt; 0 X t = X p = {0, 1}, ∀ &gt; 0 X b = X w b = {1, 2, j} X w = {-10, 0, 1, 2} P Q p s 0 (x) = 1/3 for x in {1, 2, e} p s (x) = 1/2 for &gt; 0 and x ∈ {1, 2} X t = 1(X s k-1 = X s ), ∀ X s = X s , X s = X s i , ∀ X p = X t , X w b = X b , ∀ X w s 0 = X s 0 X w =    0 if X w b = j i if X w b = X w s 0 = i for i ∈ {1, 2} -10 otherwise.</formula><p>Hence, M(G) is not convex.</p><p>We now consider the case where a ∈ anc(b) or b ∈ anc(a). W.l.o.g., we suppose a ∈ anc(b). There exists a trail from ϑ a to w in des(b) that is active given pa(b). Let Q be such a trail with a minimum number of v-structures. And let P be a b-w path. W.l.o.g., we suppose that w is the only vertex in both P and Q. Let w b be the parent of w on P and w s 0 its parent in Q. Starting from w, let s 0 , . . . , s k-1 denote the vertices with divergent arcs in Q, let t 1 , . . . , t k the vstructures, and p denote the parent of b that is below t . Finally let s (resp. s ) denote the parent of t (resp t +1 ) on the s -t subpath (resp. s -t i +1 subpath) of Q. The structures that we have just exhibited entail that G contains a subgraph of the form represented on Figure <ref type="figure" target="#fig_66">9</ref>.3. Each dashed arrows correspond to a path whose length may be equal to 0, in which case the vertices connected by the path are the same.</p><p>We now introduce a game that we will be able to encode on the graph of Figure <ref type="figure" target="#fig_66">9</ref>.3 and hence on G. This game is a dice game with two players a and b. Before rolling a uniform die with three faces, player a chooses to play 1 or 2, where "playing i " means observing if the die is equal to i , and passing this information to player b. The die s 0 is rolled. If a has played 1 (resp. 2), he passes the information true to b if the die s 0 is equal to 1 (resp. 2), and false if it is equal to 2 (resp. 1), or something else e. Player b does not know what a has played. Based on the information he receives, player b decides to play 1, 2, or joker, that we denote j. If he plays j, then none of the player either earns or loses money. If he plays i in {1, 2}, then both players earn i euros if die s 0 is equal to i , and lose 10 euros otherwise. The goal is maximize the expected payoff. This game has two locally optimal strategies δ 1 and δ 2 . In strategy δ i , player a plays i and b plays i if he receives true and j otherwise. Both strategies are locally optimal: each players decision is the best possible given the other ones. But only strategy δ 2 is globally optimal.</p><p>It changes nothing to the game if we add k -1 coin tosses X s 1 , . . . , X s k-1 , and player b observes the k equality tests X t 1 , . . . , X t k-1 , where X t = 1(X s -1 = X s ). Indeed, player b can compute k =1 x p and knows that X a = X s 0 if and only if this sum is even. The parameterization of the influence diagram that enables to encode this game is specified on the right part of Figure <ref type="figure" target="#fig_66">9</ref>.3.</p><p>For any x, the mapping 1 x (•) is the indicator function of x. All the variables that are not on </p><formula xml:id="formula_327">δ i a = 1 i and δ i b (x p 1 , . . . , x p k ) = i if k =1 x p = 0 mod 2, 0 otherwise.</formula><p>where 1 i is the Dirac in i . A technical case to handle is the one where a = a = t k . In that case, we define X a = {0, 1} and δ i a = 1 i (X s k-1 ). ations can therefore be used in branch-and-bound schemes for influence diagrams, as proposed in Khaled et al. <ref type="bibr" target="#b67">[68]</ref>. To compare the relevance of such a scheme to our MILP approach we need to compare the quality of the soluble graph relaxation and linear relaxation bounds. Corollary 9.9. Let G be a soluble graph relaxation of G, and G the RJT obtained by running Algorithm 5 on G . Then the linear relaxation of (8.9) applied to G with RJT G provides a bound at least as good as the one provided by the soluble relaxation G .</p><p>Note that this bound can sometimes be strictly better thanks to constraints (µ, δ) ∈ Q b . Remark 14. In the literature, soluble relaxations have already been used to obtain bounds in different settings. For example Yuan et al. <ref type="bibr" target="#b165">[166]</ref> used them in a branch and bound scheme. Their bounds rely on the notion of sufficient information set (SIS) for a decision vertex v <ref type="bibr" target="#b109">[110]</ref>. <ref type="foot" target="#foot_21">2</ref>SIS are set of vertices that have the following property: If, given an influence diagram G, we have a SIS D v for each decision vertex v, then the influence diagram we obtain when we add arcs u, v for each u in D v is a soluble relaxation of G. Different SIS may be available for a given decision vertex. Poh and Horvitz <ref type="bibr" target="#b120">[121,</ref><ref type="bibr">Theorem 2]</ref> show that, the closer the SIS is to the descendant of the vertex, the worse the bound is; but the easier the inference is. Yuan et al. <ref type="bibr" target="#b165">[166]</ref> make the choice of an easy inference and propose to use a SIS of minimum cardinality. An alternative option would be to add the perfect recall arcs, which would lead to a much harder inference problem but to better bounds. Using our G ⊥ ⊥ corresponds the following choice: among the SIS that enable to use our RJT for inference, use the one that leads to the best relaxation.</p><p>Proof of Corollary 9.9. Let G be the RJT obtained by running Algorithm 5 on G . By Lemma 9.3,</p><formula xml:id="formula_328">v is d-separated from C v \fa G (v) given pa G (v) in G .</formula><p>Since G is obtained by adding arcs to G, vertex v is therefore also d-separated from C v \fa G (v) given pa G (v) in G. We therefore obtain implies A ⊥ ⊥ ⊆ A . Thus, by Theorem 8.12, the bound provided by the linear relaxation of the MILP (8.9) is at least as good as the soluble graph relaxation bound.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.3">Examples of non-soluble IDs solved by linear programs</head><p>As mentioned in Section 9.1, the fact that the linear relaxation of MILP (8.9) gives an optimal solution of MEU(G, ρ) is not a sufficient condition to be a soluble ID. In this section, we introduce some examples that can be solved by an optimal solution of the linear relaxation of MILP (8.9) and that are not soluble. In fact, we are able to characterize this class of influence diagrams, which is slightly larger than the soluble ones. However, describing this class of influence diagrams is beyond the scope of this thesis. In this section, we introduce an example of influence diagram that is not soluble and such that for every parametrization ρ, the linear relaxation of MILP (8.9) gives an optimal solution of MEU(G, ρ). The numerical experiments in Section 9.5 illustrate that the SPU algorithm, whose the exactness characterizes the soluble IDs, badly performs on these examples.</p><p>Example 7. Consider a game with M players. The game is cooperative in the sense that all the players share the same goal. At each time t , all the players have access to the state of a game, and this transition leads to an immediate reward r (s, a, s ). The choice of player m is modeled using individual policies δ m in ∆ m where ∆ m is the set of strategies of player m. A strategy is the vector of player policies, i.e., δ = (δ m ) m∈ <ref type="bibr">[M ]</ref> . We want to find the best policies for each player. Given a horizon T , the goal is to find a strategy δ maximizing the total expected reward over the horizon T : max δ∈∆ E T t =1 r (S t , A t , S t +1 ) . Such a problem can be represented by the influence diagram in Figure <ref type="figure" target="#fig_66">9</ref>.4. It can be checked that this influence diagram is non-soluble.</p><formula xml:id="formula_329">S 1 S 2 S 3 S 4 A 1 1 A 2 1 . . . A M 1 A 1 2 A 2 2 . . . A M 2 A 1 3 A 2 3 . . . A M 3 Figure 9.</formula><p>The reason why the SPU algorithm does not provide a strategy performing well on Example 7 is not clear. However, we believe that SPU algorithm fails when there are "parallel" decision vertices, i.e., pair of decision vertices such that there is no path on to each other. Indeed, the "parallel" decisions in Example 7 correspond to simultaneous decisions and the SPU algorithm optimizes the strategy by considering it sequentially.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.4">Dual formulation for the soluble influence diagrams</head><p>The aim of this section is to prove Theorem 6.9, which we recall here: Theorem 6.9. The following properties hold:</p><p>(i) Linear program (6.10) is the dual of the linear relaxation of MILP (6.6) where variable δ has been removed. (ii) Linear program <ref type="bibr">(6.11)</ref> is the dual of the linear relaxation of MILP (6.6) with valid inequalities (6.7) where variable δ has been removed.</p><p>Furthermore, the strong duality holds in both cases.</p><p>This theorem helps to understand how the vector of moments and the vector of value functions are related. While Theorem 6.7 ensures that there exists an RJT such that the linear relaxation of MILP (8.9) gives an optimal solution of MEU(G, ρ) for a soluble influence diagram, the following corollary ensures that this result can be extended to formulation <ref type="bibr">(6.11)</ref>.</p><p>Corollary 9.10. If G is soluble, then there exists an RJT such that Linear program (6.11) induces an optimal solution of MEU(G, ρ) and both problems have the same optimal values. Such an RJT can be computed in polynomial time.</p><p>Proof. Since G is soluble, there exists an RJT G such that G ⊥ ⊥ = G. Theorem 6.9 ensures that the strong duality holds with the linear relaxation (8.9). Hence, by Theorem 6.7 the optimal value of the linear program (6.11) is equal to the optimal value of MEU(G, ρ). Now we build a feasible strategy of MEU(G, ρ) from an optimal solution of Linear program (6.11). We define δ such that</p><formula xml:id="formula_330">δ v (x v |x pa(v) ) = 1 when x v belongs to arg max x v x C ⊥ ⊥ v p C ⊥ ⊥ v |C ⊥ ⊥ v (x C v )λ C v (x v , x Čv ) for any x C ⊥ ⊥ v and v ∈ V a . Since C ⊥ ⊥ v = fa(v)</formula><p>, it follows that the δ v depends on x fa(v) , ensuring that δ is a feasible strategy of MEU(G, ρ). Since we consider an minimization problem, an optimal solution of Linear Program (6.11) satisfies for any vertex v ∈ V,</p><formula xml:id="formula_331">λ C v = max x u :u∈V a , C u ∈ch(C v ) r C v + u∈V s : C u ∈ch(C v ) x u p u|pa(u) λ C u + u∈V a : C u ∈ch(C v ) x C ⊥ ⊥ u p C ⊥ ⊥ u |fa(C u ) λ C u = r C v + u∈V s : C u ∈ch(C v ) x u p u|pa(u) λ C u + max x u :u∈V a , C u ∈ch(C v ) u∈V a : C u ∈ch(C v ) x C ⊥ ⊥ u p C ⊥ ⊥ u |fa(u) λ C u = r C v + u∈V s : C u ∈ch(C v ) x u p u|pa(u) λ C u + u∈V a : C u ∈ch(C v ) max x u x C ⊥ ⊥ u p C ⊥ ⊥ u |fa(u) λ C u</formula><p>ensuring that δ reaches the optimal value of (6.11), which is the optimal value of MEU(G, ρ).</p><p>Remark 15. Note that linear program (6.11) can be read as a linear formulation of the SPU algorithm on the corresponding RJT. If G is soluble, then the SPU algorithm builds an optimal strategy of MEU(G, ρ) by sequentially optimizing the individual policies on each decision vertex. The constraints of linear program (6.11) can be read as an iteration of the SPU algorithm by maximizing locally on each decision vertex u ∈ V a as max</p><formula xml:id="formula_332">x u x C ⊥ ⊥ u p C ⊥ ⊥ u |fa(u) λ C u .</formula><p>Proof of Theorem 6.9. We can remove the variables δ from the linear relaxation of MILP (8.6) or MILP (8.9) because it does not play a role. Let G = (V, A) be an gradual RJT of G. We prove (i).</p><p>We first reformulate the linear relaxation of MILP (8.6):</p><formula xml:id="formula_333">max µ v∈V 〈r C v , µ C v 〉 s.t. µ C v = p v|pa(v) x pa(Cv )\Cv µ pa(C v ) ∀v ∈ V s x v µ C v = x pa(Cv )\Cv µ pa(C v ) ∀v ∈ V a µ 0 (9.3a) (9.3b) (9.3c) (9.3d)</formula><p>where the reward function r C v is defined in <ref type="bibr">(6.4)</ref>. This reformulation comes from the fact that the consistency constraints (6.5c) and the normalization constraints (6.5b) are induced by the constraints of (9.3). Now it remains to prove that the linear program (6.10) is the dual of the linear program (9.3). To do so, we compute the Lagrangian relaxation of Linear program <ref type="bibr">(9.3)</ref>.</p><formula xml:id="formula_334">Let λ = (λ C v (x C v ))</formula><p>x Cv ,v∈V be the dual variables associated to the constraints (9.3b). Let π =</p><formula xml:id="formula_335">(π C u ∩C v (x C u ∩C v ))</formula><p>x Cu ∩Cv ,(C u ,C v )∈A be the dual variables associated to the constraints (9.3c). Then, the Lagrangian is</p><formula xml:id="formula_336">L(µ,λ,π) = v∈V x Cv r C v (x C v )µ C v (x C v ) + v∈V s x Cv λ C v (x C v ) µ C v (x C v ) -p v|pa(v) (x fa(v) )</formula><p>x pa(Cv )\Cv</p><formula xml:id="formula_337">µ pa(C v ) (x pa(C v ) ) + v∈V a x Čv π C v ∩pa(C v ) (x C v ∩pa(C v ) ) x v µ C v (x C v ) - x pa(Cv )\Cv µ C v (x C v ) = v∈V s x Cv µ C v (x C v ) r C v (x C v ) + λ C v (x C v ) - v∈V s x Cv µ C v (x C v ) u∈V s : C u ∈ch(C v ) x u p u|pa(u) (x fa(u) )λ C u (x C u ) + u∈V a : C u ∈ch(C v ) π C v ∩C u (x C u ∩C v ) + v∈V a x Cv µ C v (x C v ) r C v (x C v ) - u∈V s : C u ∈ch(C v ) x u p u|pa(u) (x fa(u) )λ C u (x C u ) - u∈V a : C u ∈ch(C v ) π C v ∩C u (x C u ∩C v ) + π pa(C v )∩C v (x pa(C v )∩C v )</formula><p>The dual problem of the linear program (9.3) is min λ,π max µ 0 L(µ,λ,π). Hence, the dual problem can be written as follows:</p><formula xml:id="formula_338">min λ -〈λ C 0 , p 0 〉 s.t. r C v + λ C v - u∈V s : C u ∈ch(C v ) x u p u|pa(u) λ C u - u∈V a : C u ∈ch(C v ) π C v ∩C u 0 ∀v ∈ V s r C v - u∈V s : C u ∈ch(C v ) x u p u|pa(u) λ C u + π pa(C v )∩C v - u∈V a : C u ∈ch(C v ) π C v ∩C u 0 ∀v ∈ V a</formula><p>This linear program can be reformulated as follows:</p><formula xml:id="formula_339">min λ 〈λ C 0 , p 0 〉 s.t. λ C v r C v + u∈V s : C u ∈ch(C v ) x u p u|pa(u) λ C u + u∈V a : C u ∈ch(C v ) π C v ∩C u ∀v ∈ V s π pa(C v )∩C v r C v + u∈V s : C u ∈ch(C v ) x u p u|pa(u) λ C u + u∈V a : C u ∈ch(C v ) π C v ∩C u ∀v ∈ V a By introducing variables λ C v for v ∈ V a such that π pa(C v )∩C v λ C v r C v + u∈V s : C u ∈ch(C v ) x u p u|pa(u) λ C u + u∈V a : C u ∈ch(C v ) π C v ∩C u ,</formula><p>we finally obtain the following formulation:</p><formula xml:id="formula_340">min λ 〈λ C 0 , p 0 〉 s.t. λ C v r C v + u∈V s : C u ∈ch(C v ) x u p u|pa(u) λ C u + u∈V a : C u ∈ch(C v ) λ C u ∀v ∈ V</formula><p>It achieves the proof since the last formulation corresponds exactly Linear Program (6.10). Now we prove (ii). Like for the proof of (i), we reformulate the linear relaxation of MILP (8.9): </p><formula xml:id="formula_341">max µ v∈V 〈r C v , µ C v 〉 s.t. µ C v = p v|pa(v) x pa(Cv )\Cv µ pa(C v ) ∀v ∈ V s µ C v = p C ⊥ ⊥ v |C ⊥ ⊥ v x C ⊥ ⊥ v µ C v ∀v ∈ V a x v µ C v = x pa(Cv )\Cv µ pa(C v ) ∀v ∈ V a µ 0 (<label>9</label></formula><formula xml:id="formula_342">= (π C u ∩C v (x C u ∩C v ))</formula><p>x Cu ∩Cv ,(C u ,C v )∈A be the dual variables associated to the constraints (9.7d).</p><p>Using the previous calculus, the Lagrangian L can be written:</p><formula xml:id="formula_343">L(µ,λ,π) = v∈V s x Cv µ C v (x C v ) r C v (x C v ) + λ C v (x C v ) - u∈V s : C u ∈ch(C v ) x u p u|pa(u) (x fa(u) )λ C u (x C u ) - v∈V s x Cv µ C v (x C v ) u∈V a : C u ∈ch(C v ) π C v ∩C u (x C u ∩C v ) + v∈V a x Cv µ C v (x C v ) r C v (x C v ) - u∈V s : C u ∈ch(C v ) x u p u|pa(u) (x fa(u) )λ C u (x C u ) - u∈V a : C u ∈ch(C v ) π C v ∩C u (x C u ∩C v ) + π pa(C v )∩C v (x pa(C v )∩C v ) + λ C v (x C v ) - x C ⊥ ⊥ v p C ⊥ ⊥ v |C ⊥ ⊥ v (x Čv )λ C v (x C v )</formula><p>Hence, the dual problem is the following:</p><formula xml:id="formula_344">min λ -〈λ C 0 , p 0 〉 s.t. r C v + λ C v - u∈V s : C u ∈ch(C v ) x u p u|pa(u) λ C u - u∈V a : C u ∈ch(C v ) π C v ∩C u 0 ∀v ∈ V s r C v + λ C v - x C ⊥ ⊥ v p C ⊥ ⊥ v |C ⊥ ⊥ v λ C v - u∈V s : C u ∈ch(C v ) x u p u|pa(u) λ C u + π pa(C v )∩C v - u∈V a : C u ∈ch(C v ) π C v ∩C u 0∀v ∈ V a</formula><p>This linear program leads to the following one:</p><formula xml:id="formula_345">min λ 〈λ C 0 , p 0 〉 s.t. λ C v r C v + u∈V s : C u ∈ch(C v ) x u p u|pa(u) λ C u + u∈V a : C u ∈ch(C v ) π C v ∩C u ∀v ∈ V s π pa(C v )∩C v + λ C v - x C ⊥ ⊥ v p C ⊥ ⊥ v |C ⊥ ⊥ v λ C v r C v + u∈V s : C u ∈ch(C v ) x u p u|pa(u) λ C u + u∈V a : C u ∈ch(C v ) π C v ∩C u ∀v ∈ V a</formula><p>For any optimal solution of Linear Program (9.10), we have:</p><formula xml:id="formula_346">π pa(C v )∩C v x C ⊥ ⊥ v p C ⊥ ⊥ v |C ⊥ ⊥ v λ C v λ C v r C v + u∈V s : C u ∈ch(C v ) x u p u|pa(u) λ C u + u∈V a : C u ∈ch(C v ) π C v ∩C u</formula><p>Indeed, otherwise it can be showed that we can obtain an optimal solution with a lower value. Hence it follows that linear program (9.10) becomes</p><formula xml:id="formula_347">min λ 〈λ C 0 , p 0 〉 s.t. λ C v r C v + u∈V s : C u ∈ch(C v ) x u p u|pa(u) λ C u + u∈V a : C u ∈ch(C v ) x C ⊥ ⊥ u p C ⊥ ⊥ u |C ⊥ ⊥ u λ C u ∀v ∈ V</formula><p>It proves (ii). The strong duality holds in both case because there always exist an optimal solution of the primal problems (e.g. <ref type="bibr" target="#b92">[93]</ref>). It achieves the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.5">Numerical experiments</head><p>In this section, we give numerical results on random instances of Example 7. As mentioned in section 9.3, this example is represented by an influence diagram that is non-soluble and such that an optimal solution of MEU(G, ρ) can be found by solving the linear relaxation of MILP (8.9). All linear programs have been implemented in Julia with JuMP interface <ref type="bibr" target="#b40">[41]</ref> and solved using Gurobi 9.0 <ref type="bibr" target="#b51">[52]</ref>. Experiments have been run on a server with 192Gb of RAM and 32 cores at 3.30GHz. Metrics. We denote respectively by z * and z * R the optimal values of MEU(G, ρ), which is computed by solving MILP (8.9), and the optimal value of the linear relaxation of MILP (8.9). We compare these values with the value z SPU obtained by running the SPU algorithm. To do so, for each value z ∈ {z SPU , z * R } we compute the relative gaps with respect to the value of optimal value g (z) = z-z * z * and we report it as well as the computation time of each mathematical program in Table <ref type="table" target="#tab_47">9</ref>.1. All the mathematical programs have been solved optimally. The results are averaged over the set 50 generated instances and reported in Table <ref type="table" target="#tab_47">9</ref>.1 The first column indicates the value of the triplet (M , k s , k a ). The second column indicates the value of the finite horizon T. The third column indicates the size of the set of deterministic strategies ∆ d ml . Finally, the last three columns indicate the algorithms used (Alg.), the average gap value over the 50 instances, and the averaged computation time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The instances are generated by first choosing k</head><formula xml:id="formula_348">s = X S and k a = X 1 A = • • • = X M A .</formula><p>The results in Table <ref type="table" target="#tab_47">9</ref>.1 show that the linear relaxation of MILP (8.9) gives the same optimal value as that of MILP (8.9). It validates the fact that there are non-soluble influence diagrams for which the linear relaxation of MILP (8.9) gives an optimal strategy. Since the SPU strategy is a local optimum, the value of the gap z SPU -z * z * is non-positive as shown in Table <ref type="table" target="#tab_47">9</ref>.1. One can also observe that, as we expected, the SPU algorithm gives a strategy with an objective value which can be far from the optimal value. In particular these gaps are significantly larger than those of the numerical experiments in Section 8.6. Note that when the number of players increases, the gap g (z SPU ) decreases also. It supports our intuition that when the influence diagram has "parallel" decision vertices, the SPU algorithm fails to give a good strategy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Part III Maintenance problem at Air France 10 Data-driven maintenance optimization</head><p>In this chapter, we focus on the airplane maintenance problem at Air France, which we recall here. Given an airplane planning with scheduled maintenance slots, the decision maker receives sensor data at each maintenance slot on M equipments of an airplane and chooses at most K equipments that should be maintained during this maintenance slot. The objective is to choose a maintenance policy maximizing the expected costs, which correspond to the sum of the maintenance costs and the failure costs.</p><p>The sensor data correspond to a collection of time series recorded at 1Hz during flights. In Chapter 3, we introduced a generic predictive maintenance problem and we formalized it as a weakly coupled POMDP problem to build a policy for the maintenance of the components of a system. We wish to use such a maintenance policy in the airplane maintenance problem at Air France. This requires to cast the airplane maintenance problem as an instance of the generic predictive maintenance problem described in Chapter 3. However, this raises three practical issues. First, in the airplane maintenance problem, at each maintenance slot the decision maker has access to sensor data, which are continuous and high dimensional, instead of discrete observations. Consequently, it requires to create a discretization method that transforms the sensor data recorded during a flight into a discrete observation for each equipment. Second, Air France's requirement is that the resulting discrete observations be interpretable. Third, the parameters of the POMDPs, which compose the weakly coupled POMDP, are not available in practice. This requires to estimate these parameters. In this chapter, we describe a statistical methodology that addresses these three issues and that casts the airplane maintenance problem as a weakly coupled POMDP problem. This chapter is organized as follows:</p><p>• Section 10.1 describes the current Air France's approach and the main steps of our approach to address the three issues mentioned above. In particular, we argue about our choices of statistical tools. • Section 10.2 formally introduces the problem of finding a maintenance policy given sensor data. • Section 10.3 describes how we transform such a maintenance problem into a weakly coupled POMDP. In particular, we detail how we estimate the parameters of the weakly coupled POMDP. • Section 10.4 provides numerical experiments on a simulated system on which we apply the policy of Chapter 5, which is a feasible policy of the weakly coupled POMDP. We com- pare the results of this policy against a maintenance policy reproducing Air France's one on the simulated system. Although we are not able to simulate the airplane's sensor data, we also provide numerical experiments on Air France's dataset by comparing the past failures and what our maintenance policy would have suggested. • Section 10.5 contains bibliographical remarks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.1">About the airplane maintenance problem at Air France</head><p>In this section, we describe the current approach at Air France to address the airplane maintenance problem and the main steps of our approach. We recall that we have access to a dataset containing sensor data that correspond to a collection of time series over several years on the whole fleet of airplanes.</p><p>Current approach at Air France and its limits. Air France already uses predictive maintenance for a few failure prone equipments. In their current practice, the maintenance engineers use fault trees to support their decisions. Using machine learning terminology, fault trees are equivalent to decision trees, generally hand-designed and of small size. In this dissertation, we will always designate the tool used in Air France as decision tree. A decision tree is illustrated on Figure <ref type="figure" target="#fig_68">10</ref>.1. It takes in input a vector of features extracting relevant information from the time series. Using a succession of binary rules splitting the features space in two, it partitions this feature space into a small number of clusters. Each of the clusters is informative from an engineering point of view: some correspond to normal behavior (blue labels in Figure <ref type="figure" target="#fig_68">10</ref>.1), some to high failure risk (red labels in Figure <ref type="figure" target="#fig_68">10</ref>.1). The engineers then maintain the equipment if the decision tree returns a label corresponding to a high failure risk cluster. This approach is a diagnosis-based <ref type="bibr" target="#b2">[3]</ref> heuristic and is blind about the failure risk for each equipment over the remaining of the horizon. It is easy to understand why this approach enabled to drastically reduce the failures and the maintenance costs of the small number of failure-prone equipments considered. Indeed, when only very few equipments are considered, prioritizing 10.1. About the airplane maintenance problem at Air France between equipments is not an issue, and this heuristic makes perfect sense. In addition, even if the decision trees usually provide a diagnosis, Air France designs their decision tree in such a way that it detects the first symptoms of deterioration. This characteristic ensures that the heuristic is more preventive than corrective, i.e., an equipment is maintained before it fails. Furthermore, the decision tree used can make these diagnosis very accurately because it leverages a physical understanding of the equipment. However, such an approach cannot be extended to a large number of equipments. The first reason is mathematical. When there are many equipments and scarce maintenance resources, anticipating the future on several decision steps becomes crucial, and a diagnosis-based heuristic cannot work anymore. We therefore need a richer multistage stochastic optimization approach, which itself requires a richer prediction model. The second reason is industrial: Building manually a good decision tree requires several months of work of an expert. Given that experts are also a scarce resource, such an investment cannot be scaled to dozens of equipments, and we therefore need to use prediction models that require less expert time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>How to build a prediction model trusted by maintenance engineers.</head><p>There are two ways of building trust in models: experimental testing, or validation by experts who understand the model. A specific difficulty comes from the fact that the data is censored. Indeed, taking maintenance decisions requires to be able to predict the behavior of the system just before it fails. But since failures are costly, airlines try to avoid them as much as possible. If we have much data on the system when it works well, we have a small amount of data on the behavior of the system right before it fails: On the whole history, the number of failures observed on one equipment never exceeds 10. Given the small number of failures in our dataset, and that we do not have access to a simulator, experimental validation is not possible. We must therefore propose a model that maintenance experts can validate, but whose design and validation do not require too much of their time. We solve that conundrum as follows: our model makes decisions using only information that maintenance experts can easily understand. Since decision trees form the standard method used by the engineers of Air France to take maintenance decisions, we interpret "information that maintenance experts can easily understand" as "the result of a small size decision tree." <ref type="foot" target="#foot_22">1</ref> To reduce our need of expert time, the decision trees are learned from historical data, and experts only check that the classification they produce makes sense from an engineering point of view. An additional difficulty to build a decision tree given our dataset is that the data are not labeled. Since we do not know when an equipment exactly fails, we are not able to assign a label on each flight indicating if the equipment has an abnormal behavior due to a failure. Main steps of our approach. The goal of our approach is to cast the airplane maintenance problem as a weakly coupled POMDP and the decision maker chooses which equipments to maintain at each maintenance slot according to a feasible policy of this weakly coupled POMDP. To do so, we proceed in several steps which are summarized in Figure <ref type="figure" target="#fig_68">10</ref>.2. We first extract for each monitored equipment a vector of features in R d from the sensor data. We then use a de-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.2.">Formalizing the airplane maintenance problem</head><p>quires to evaluate the policy it returns on the true system, and not only according to the model we learned. Since we do not have a simulator for the sensor data of the airplanes equipments, we evaluate the performance of our approach using a simple simulator of a system with multiple deteriorating components. To evaluate our maintenance policy on the airplane maintenance problem, we compare the maintenance times in the historical dataset against what our maintenance policy would have done.</p><p>We emphasize that all the steps of our approach are relatively easy to implement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.2">Formalizing the airplane maintenance problem</head><p>We consider a system on a horizon T ∈ Z + . This system is composed of M equipments indexed by m ∈ [M ]. These equipments are subject to failures, and for τ ∈ [T ], we denote by F m τ the binary random variable equal to 1 if a failure happens on equipment m at date τ. The sensor signals are recorded, and we denote by Z m τ the signals in R k ×{0, 1} k with k, k ∈ N, recorded on equipment m at date τ ∈ [T ]. There are T maintenance slots scheduled on given (deterministic) dates τ t , with τ t &lt; τ t +1 for every t ∈ [T ]. On each maintenance slot, the decision maker chooses to maintain at most K equipments. We denote by A m t the binary random variable equal to 1 if equipment m is maintained on slot t . At each maintenance slot, at most K equipments can be maintained. Hence, the action A m t m∈[M ] on slot t belongs to the action space X A defines as follows The approach is data-driven: we do not have access to any model of the system, nor to any simulator of the component, and we do not know if sensor signals provide enough information to model the dynamic of the system. In particular, we do not have access to the probability distribution on (Z m τ ) m∈[M ] , (F m τ ) m∈[M ] , and (A m t ) m∈[M ] , and therefore have no way to evaluate the objective function of (10.2). We only have access to historical values taken by these random variables on a previous horizon. As mentioned in Section 10.1, the historical dataset contains a small number of failures. Hence, the probability distributions cannot be learned precisely. Furthermore, since we cannot evaluate a policy using a simulator, such a policy must be interpretable. Since Problem (10.2) is hard to solve, we propose to restrict the set of policies. Comp. 1 </p><formula xml:id="formula_349">X A = a = (a m ) m∈[M ] ∈ {0, 1} M : m∈[M ]</formula><formula xml:id="formula_350">X 1 1 A 1 X 1 2 A 2 X 1 3 X 1 1 A 1 X 1 1 A 2 X 1 1 X 1 1 A 1 X 1 2 A 2</formula><formula xml:id="formula_351">S 1 1 X 1 1 O 1 1 A 1 Z 1 τ τ1 S 1 2 X 1 2 O 1 2 A 2 Z 1 τ τ2 S 1 3 X 1 3 O 1 3 A 3</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.3">Modeling as a weakly coupled POMDP</head><p>In this section, we explain how to model the general maintenance problem (10.2) as a weakly coupled POMDP and how to learn the weakly coupled POMDP parameters. Hence, using notation of 3, we have to set the value of X m S , X m O , X m A , p m , r m for m ∈ [M ], and the value of X A . (R</p><formula xml:id="formula_352">k m × {0, 1} k m ) [0,τ t ] φ m -→ R d m f m -→ X m O (z m τ ) τ τ t -→ x m t -→ o m t</formula><p>First, we extract features X m t in X m X := R d m from the times series (Z m τ ) τ τ t using a manually defined φ (represented with dashed purple arcs in Figure <ref type="figure" target="#fig_68">10</ref>.3). Second, we turn these features into observations in a finite set X m O using a decision tree f m (represented with dashed green arcs on Figure <ref type="figure" target="#fig_68">10</ref>.3) that we learn from the data. We will explain later how we define where f = ( f 1 , . . . , f M ) and φ = (φ 1 , . . . , φ M ). In the remaining of this section, we explain how to choose φ m , and how to learn f m , and finally how to set the weakly coupled POMDP parameters (10.3).</p><p>Choice of features vector φ m . We take in input the sensor signals time series that have been selected by the maintenance experts as the most relevant ones. We then compute a moderate number of features standardly used in predictive maintenance such as peak values, mean, standard deviation, etc <ref type="bibr" target="#b62">[63,</ref><ref type="bibr" target="#b152">153]</ref>. We then select the most relevant features using the maintenance expert knowledge. We end up with 10 to 50 features per equipment. Note that due to the large amount of historical sensor data manipulated (typically for one flight and one equipment there are 20 time series each containing 20000 data points), big data technologies must be used. We use Spark <ref type="bibr" target="#b166">[167]</ref>.</p><p>Learning f and the POMDP is more challenging, as we now detail.</p><p>Learning the decision tree f m . We introduce a methodology to learn f m , in such a way that the partition of the feature space R d m it realizes is informative about the dynamic of the system. Our methodology is in two steps.</p><p>1. Learning HMM to predict the evolution of the feature vector X m t . We compute X m t = φ m (R m τ ) τ t on our learning dataset to obtain the trajectories followed by the different features, and then we learn a Gaussian HMM (represented with blue arcs in Figure <ref type="figure" target="#fig_68">10</ref>.2) with n m S hidden states to predict the evolution of the features X m t using standard algorithms. The number of states n m S is chosen carefully. 2. Learning the decision tree f m . On our learning dataset, we recompute the most probable hidden state Ŝt m according to the HMM learned at the previous step, and we choose f m as follows: We train a decision tree to predict the most probable state Ŝm t given X m t . Since we have a dataset of labeled data (X m t , Ŝm t ), we do this by learning a decision tree using standard CART algorithms <ref type="bibr" target="#b18">[19]</ref>.</p><p>We now provide details on each of these steps.</p><p>1. Learning the Gaussian HMM that models the features evolution. Following well-established practice in the maintenance literature <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b81">82]</ref> we use a Gaussian left-right HMM. Kim et al. <ref type="bibr" target="#b72">[73]</ref> show that Gaussian models are appropriate to predictive maintenance. Denote by pm the transition probability distribution of the Gaussian HMM of equipment m. We choose a number of hidden states n m S , and we set X  <ref type="figure" target="#fig_44">6</ref>] enable to model deteriorating systems, since the assumptions on the transition probability distribution enable to model the fact that a equipment cannot repair itself. We assume that the failure state s m F is the maximal element with respect to order ≺. Following the literature, we use the Baum-Welch Algorithm <ref type="bibr" target="#b10">[11]</ref> to learn the HMM parameters.</p><p>Remark 16. The number of hidden states n m S is also a parameter. Like Le et al. <ref type="bibr" target="#b81">[82]</ref>, we choose the value of n m S that leads to the minimal value of the Bayes Information Criterion (BIC) [53, Sec 7.7].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2.</head><p>Learning the decision tree f m . Learning algorithms for decision trees require labeled data input. We describe here how we label a sequence of features in R d m , x 1 , x 2 , . . . , x N . We use our Gaussian HMM to predict the most probable sequence of hidden states ŝ1 , ŝ2 , . . . , ŝT using the Viterbi algorithm <ref type="bibr" target="#b154">[155]</ref>. This gives us a label ŝt to data point x t , for all t in [T ]. Finally, we learn a decision tree that predicts the value of the hidden state ŝt given the observation x t . We choose to learn such a decision tree because each state in X m S corresponds to a degradation state of the equipment. Hence, our decision tree predicts the deterioration of the equipment. Let f m be the learned decision tree. It maps any continuous features to discrete states, i.e., f m : R d m → X m O where X m O := X m S is a finite space. We choose to use the CART (Classification And Regression Trees) algorithm <ref type="bibr" target="#b18">[19]</ref> to learn f m . Numerical experiments show that the learned decision tree has a satisfying accuracy.</p><p>Having estimated a Gaussian HMM and a decision tree for each equipment, we finally describe how we set the weakly coupled POMDP parameters <ref type="bibr">(10.3)</ref>.</p><p>Setting the weakly coupled POMDP parameters <ref type="bibr">(10.3)</ref>. We already have defined X m Now we set the parameters p m for any equipment m. From step 1, we already have the probability distributions P(S 1 ), P(S t +1 |S t ) and the Gaussian law parameters of the emission probability distributions. From step 2, we already have the learned decision tree f . We set the following . Note that we assumed in (10.5) that when a equipment has been maintained, the equipment is new. The right-hand side of (10.6) is an integral over R d m , which is difficult to compute since the vector of features is not independent. Therefore, we compute it using Monte-Carlo simulation. Discussion. When we cast Problem (10.2) as a weakly coupled POMDP we restrict ourselves to maintenance policies of the form <ref type="bibr">(10.4)</ref>. This restriction enables to address several challenges.</p><p>First, due to the large dimension of the sensor data recorded on each flight we are not able to manipulate it in machine learning algorithms. Hence, it requires the use of a feature function φ to aggregate the collected time series Z m τ into a vector of features X m τ . Second, while modeling a maintenance problem using a POMDP with discrete observations is a common practice in the literature, a major difficulty of our problem lies in the choice of f m to make the observations interpretable and discrete. Indeed, since the policy of our weakly coupled POMDP uses only the information in O m t = f m • φ m (Z m τ ) τ t , this policy can be relevant only if f m is chosen in such a way that O m t provides relevant information on how the system evolves. But since the partition of R d m into n m O subspaces realized by f m does not correspond to a groundtruth, there is no data from which it can be learned in a supervised learning way. Indeed, our dataset is not labeled. One way to learn f m would therefore be to use unsupervised learning algorithms, but doing so, we have no way to indicate to the unsupervised learning algorithm that f m should partition R d m in such a way that the different clusters are informative on the dynamic of the system. Therefore, we use the left-right HMM's predictions that assign an informative label to each feature vector X m τ . Learning a decision tree based on this labeling then make our approach combines interpretability and clustering.</p><p>An commonly used metric in predictive maintenance is the Remaining Useful Life (RUL), i.e., the time left before the next failure. We can evaluate the efficiency of our statistical model by measuring the accuracy of the predictions of the RUL within a validation methodology, which is commonly used in machine learning. However, our statistical approach is not a contribution to machine learning. What matters is the efficiency of the maintenance policy in terms of resulting saving costs and computation time. Nevertheless, as mentioned in the introduction of this chapter, in addition to the Gaussian HMM predictions building automatically a decision tree which gives an indicative label for each equipment corresponding to its degradation state is a useful contribution for Air France.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.4">Numerical results</head><p>At Air France, the maintenance decision leverages a manually designed decision tree, that takes a continuous observation in input and returns a binary output {0, 1}. A failure is diagnosed on an equipment when the decision tree returns 1 and then a maintenance of the equipment is suggested. We would like to compare our approach against this current practice. However, we cannot evaluate it on real airline data for two reasons. First, we do not have an airplane's equipment simulator. Second, the historical dataset is censored, i.e., many equipments have been maintained before failing <ref type="bibr" target="#b131">[132]</ref>. Since we do not have an airplane's component simulator, we cannot evaluate the performance of our policy on real data. We evaluate our methodology on a simulator of a deteriorating system with multiple components. Therefore, we construct a simulator of a deteriorating system based on the predictive maintenance literature and we reproduce the current practice on such a system. Then, we present the benefits of using our policy over the current practice on such system.</p><p>We also give several results on the real dataset of Air France. We compare the past decisions made by the current approach at Air France and what our policy would have done on the historical dataset. Even if the results have to be analyzed cautiously, one can observe a significant improvement over the current practice.</p><p>We use the library scikitlearn <ref type="bibr" target="#b117">[118]</ref> for all machine learning algorithms. All linear programs have been implemented in Julia with package JuMP <ref type="bibr" target="#b40">[41]</ref> and solved using Gurobi 9.0 <ref type="bibr" target="#b51">[52]</ref>.</p><p>Experiments have been run on a server with 192GB of RAM and 32 cores at 3.30GHz. The code used to perform the numerical results on simulated data can be found at the following link <ref type="url" target="https://github.com/Victor2175/maintenance_system">https://github.com/Victor2175/maintenance_system</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.4.1">Evaluating the policy using a simulator</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>System's description</head><p>We want to simulate a mechanical system composed of M deteriorating components. Several cracks are present in each component of the system. The deterioration of the component corresponds to the propagation of the cracks. Denote by Z m τ ∈ R d m the noisy observation of the crack depth in component m at time τ. We suppose that for each component m, the dimension d m has a moderate value (typically 3 d m 5) such that it does not require to perform feature extraction or feature selection. It means that our feature vector is equal to the sensor data at any time, i.e., X m t := Z m τ t . We assume that the crack depth in each component evolves independently. A complete description of how we simulate the crack depth propagations in the M components of a system is available in Appendix C. Maintain all components in M r .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>9: end if</head><p>In Appendix C, we explain how we reproduce the airline's binary decision trees g m for all m in [M ], using our simulator.</p><p>Numerical results. We apply our approach on the simulated system and we evaluate our policy against the current practice corresponding to Algorithm 6. We consider:</p><p>• our implicit policy (5.14) embed in the rolling horizon heuristic detailed in Algorithm 3 (Alg. 3) for different rolling horizons T r = 1, 2, 5 • the current practice in industry (Alg. 6) Note that when T r = 1, Algorithm 3 becomes a greedy heuristic because the decision maker takes the action minimizing the expected costs over one time step. We evaluate each policy on different number of components M ∈ {3, 5, 10, 15, 20}. For each value of M , we set the maintenance capacity K := M +1 3 . This choice is arbitrary but it enables to keep a fixed proportion regarding to the number of components M . We set the interval maintenance time between the maintenance slots h = 30∆t , where ∆t = 1 is the discretization time step of our simulator. We also set the number of scheduled maintenance slots T sim = 200. For each value of M , we randomly draw 10 instances as described in Appendix C, where an instance corresponds to a set of parameters that fully describe a system. For each instance, we evaluate a policy 100 times, each time over the 200 time steps. In total, for each value of M , a policy is evaluated 1000 times. For each policy evaluation, we count the total cost and the number of failures at the end of the period. In addition, we calculate the mean time to take a decision during the evaluation, i.e., the mean computation time over the 200 time steps. As mentioned in Remark 16, the number of states |X m S | of the learned Gaussian HMMs is carefully chosen using the Bayes Information Criterion. For every instances, we obtain 3 |X m S | 10 for each component m. Table <ref type="table" target="#tab_49">10</ref>.1 summarizes the results obtained. The first column indicates the number of components M . The second column indicates the policies used. Finally, the last three columns provide the policy computation time (Time), the value of objective function (Obj.) expressed as the percentage of cost saving over the airline's policy, and the number of failures (Fail.). All these quantities are averaged over the 1000 policy evaluations.</p><p>The results in Table <ref type="table" target="#tab_49">10</ref>.1 show that the maintenance policy of Algorithm 3 outperforms the airline's maintenance policy in terms of costs or failures, which is what we expected. In addition, we observe that using Algorithm 3 with T r = 1 already strongly outperforms the Air France's  maintenance policy, and using a larger horizon gives almost the same total costs as Algorithm 3 with T r = 5. Unfortunately, we are not able to explain precisely this phenomenon. But we try to give some explanations.</p><p>This phenomenon can be due to the fact that the weakly coupled POMDP parameters can be not well estimated for some instances. Indeed, the Gaussian HMM parameters are obtained by running the Baum-Welch algorithm, which usually reaches an local optimum of the likelihood instead of an global optimum. Depending on the initial conditions of the algorithm, this local optimum may lead to bad predictions. When we use Algorithm 3, the larger the rolling horizon T r , the worse the predictions. Informally, it means that "if the predictions are not good for the next time step, then they will be worse over 2 or more time steps." However, one can observe that the number of failures is significantly lower when we use a larger rolling horizon T r , which means that the resulting maintenance policy is more preventive than with T r = 1. These results emphasize that our maintenance policy strongly depend of the quality of the estimation of the HMM parameters. When the number of components grows, if more than one component has HMM parameters giving poor predictions, the maintenance policy can be worse.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.4.2">Evaluating the policy on Air France real data</head><p>We present here some numerical results on data of Air France's maintenance problem. Since we cannot simulate the equipment's evolution over time, we propose an alternative method to evaluate our maintenance policy <ref type="bibr">(5.14)</ref>. We compare the past maintenance decisions against what our policy would have done. In our case, we have access to the sensor data of two equipments (M = 2) and the corresponding maintenance dates.  Note that these results have to be considered carefully because we do not know when the failure happens exactly, which means that our maintenance policy could have suggested to maintain an equipment that has already failed. Table <ref type="table" target="#tab_49">10</ref>.2 shows that our policy would have avoided several failures and outperforms Air France's policy. Since the results obtained are at least as good as Air France's results, we can reasonably say that the learned decision tree that predicts the state of the equipment is efficient. It is also a contribution for Air France. In addition, one can observe that the computation time is always below several seconds, which is an advantage for Air France.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.5">Bibliographical remarks</head><p>A data-driven maintenance optimization approach learns a statistical model from the available data, and, leverages it to derive an optimized maintenance decision when new data becomes available. The tools we used to build the statistical model, feature extraction, HMMs and decision trees, are known in the predictive maintenance literature. Kim et al. <ref type="bibr" target="#b71">[72]</ref> proposed a similar data-driven methodology on the maintenance of heavy hauler truck used in mining industry and they showed that their approach enables to save 34% of the cost with respect to the current practice.</p><p>Since our approach uses some tools from different research areas, we divide the literature review in two sections. First, we review the current practice on the feature extraction, the HMMs and the decision trees in predictive maintenance. Second, we review the use of POMDP in maintenance. We add here bibliographical remarks about the three steps to build the statistical model described in this chapter: features extraction, Hidden Markov Model and decision tree.</p><p>Features extraction is a common technique in machine learning to reduce the dimension of data by aggregating the input data in a finite set of features. The resulting dimension is the number of features. For an overview of feature extractions and signal processing with maintenance sensor data, see for instance  <ref type="bibr" target="#b60">[61]</ref> propose an approach based on the notion of feature predictability, i.e., the ability of a feature to be predicted by a state-of-the-art time series prediction algorithm. In particular they show that the more predictable the features, the better the failure predictions. We choose to stick to the predictive maintenance literature. We compute a moderate number of simple features for time series <ref type="bibr">[153,</ref> Table <ref type="table">1</ref>] such that we obtain observations in R d , where d is between 10 and 50.</p><p>Hidden Markov Model (HMM) is one of the main tools to model a component's evolution with observations in R d . The component's degradation is modeled as a hidden Markov chain in a finite state space, and we assume that at each time the continuous observation in R d depends on the current hidden state. An HMM is parameterized by its conditional probability distributions. This modeling has been widely used in maintenance industry <ref type="bibr" target="#b81">[82,</ref><ref type="bibr" target="#b141">142,</ref><ref type="bibr" target="#b150">151,</ref><ref type="bibr" target="#b159">160]</ref>.</p><p>In particular, HMMs are appreciated in such a context due to their ability to predict the Remaining Useful Life (RUL) <ref type="bibr" target="#b160">[161]</ref>. It is for this reason that HMMs lie in the broad class of prognostic methods <ref type="bibr" target="#b59">[60]</ref>. Like Le et al. <ref type="bibr" target="#b81">[82]</ref>, we model the evolution of the continuous observations in R d using a Gaussian HMM. However, an expert cannot interpret the RUL's predictions made by the HMM. Hence, it requires to make the HMM's observations interpretable by a maintenance expert using a decision tree.</p><p>Decision trees are widely used in the industry to detect failures. Given a vector of features in R d in input, it returns a discrete label, which corresponds to a failure risk cluster of the system. As mentioned before, the decision trees are appreciated in the industry because they are interpretable <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b46">47]</ref>. While in the airline industry the decision trees are hand designed by maintenance experts, machine learning algorithms allow to learn decision trees that can be interpreted as decision trees <ref type="bibr" target="#b133">[134]</ref>. Decision tree learning approaches have been used in various maintenance applications <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b130">131,</ref><ref type="bibr" target="#b149">150]</ref>. In all of them, the decision trees are learned using a training dataset with labeled data, and algorithm C4.5 or CART <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b52">53]</ref>. For each observation in the dataset, a label indicates the presence of a failure and the decision tree aims at predicting this label.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11">Conclusion</head><p>As a conclusion, we summarize the main contributions of this thesis, and outline some research directions suggested by our results. Our work was applied to the case of Air France, which was our partner during the thesis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.1">Main contributions</head><p>In this dissertation, we have developed algorithms for stochastic optimization problems including the Partially Observable Markov Decision Process problem and the maximum expected utility problem in influence diagrams. The first and second parts are mainly theoretical and introduce mathematical programming formulations to solve these stochastic optimization problems. The third part applies one of these algorithms to solve the airplane maintenance problem at Air France.</p><p>Part I introduces a generic predictive maintenance problem with capacity constraints. We consider a system with several components, each of them evolving independently over time.</p><p>At each maintenance slot, based on an observation of each component the decision maker chooses which components to maintain. The actions taken on each component are linked by the capacity constraints. This choice is modeled using a memoryless policy that maps a vector of observations to an action. The objective is to choose a memoryless policy minimizing the expected failure costs and maintenance costs over a finite horizon. This problem lies in the broad class of weakly coupled dynamic programs <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b54">55]</ref>. An additional feature of our problem comes from the fact that the components are partially observable. While modeling the deterioration of a component using a POMDP is standard in the literature <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b90">91]</ref>, we introduce the weakly coupled POMDP that models a system with several components, each of them evolving independently as a POMDP. We emphasize the modeling power of the weakly coupled POMDP on several practical problems.</p><p>To provide a good policy for the weakly coupled POMDP, we made several contributions for the POMDP problem with memoryless policies, which is NP-hard. First, we proposed an mixedinteger linear program that gives an optimal memoryless policy for POMDP. The variables of this MILP are the marginal probability distributions of the random variables, and the constraints are the ones satisfied by a joint probability distribution over the random variables. Second, based on a probabilistic interpretation of the dependences between the random variables, we introduced valid inequalities for our MILP that improve its resolution. The numerical results in Chapter 4 show that these valid inequalities tighten significantly the linear relaxation. Third, we show how to relate the value of our MILP to the value of the usual POMDP where the actions are taken based on the history of actions and observations. We evaluate the performances of memoryless policies on several POMDP instances from the literature over finite horizon. The numerical results show that the memoryless policies perform well on a large part of the instances, including those corresponding to maintenance problems.</p><p>Like weakly coupled dynamic programs, the weakly coupled POMDPs suffer from the curse of dimensionality because the sizes of the state space and the observation space of the full system are exponential in the number of components. Even encoding a memoryless policy is intractable. To overcome this issue, we introduce an MILP containing a polynomial number of variables and constraints, which breaks the curse of dimensionality, and we give theoretical guarantees on its optimal value by playing with different "probabilistic" approximations. In particular, like <ref type="bibr" target="#b1">[2]</ref> for the weakly coupled dynamic programs (or weakly coupled MDPs) we use the Lagrangian relaxation approach to derive a tractable upper bound. In Chapter 5, we illustrate the quality of our approximation on instances of multi-armed bandit problems. Leveraging this MILP, we define a feasible policy of the weakly coupled POMDP that provides satisfying results for the predictive maintenance problem with capacity constraints in Chapter 10.</p><p>Part II focuses on stochastic optimization problems including MDPs and POMDPs where the uncertainty satisfies some structure of influence diagram. Chapter 6 describes our main results for influence diagrams, which generalize the integer programs of Part I on POMDPs to influence diagrams. We introduce linear programming and MILP approaches for the maximum expected utility problem in influence diagrams. The variables of the programs correspond to the collection of vector of moments of the distribution on subsets of the variables that are associated to vertices of a new kind of junction tree, which we call an rooted junction tree. We have also proposed algorithms to build rooted junction trees tailored to our linear and integer programs. In Chapter 8 we proposed an MILP approach to solve the maximum expected utility problem on influence diagrams together with valid cuts. Again, these valid cuts are based on a probabilistic interpretation and the numerical experiments in Chapter 8 show that the bound obtained with this linear relaxation is indeed better in practice than without these inequalities.</p><p>In Chapter 9, we study soluble influence diagrams <ref type="bibr" target="#b80">[81]</ref>, which are influence diagrams whose maximum expected utility problem is easy, in the sense that it can be solved by the algorithm "single policy update"(SPU). We show that for soluble influence diagrams the maximum expected utility problem can also be solved exactly via our linear programs. Furthermore, we characterized soluble influence diagrams as the influence diagrams for which there exists a junction tree such that the set of possible vectors of moments on the vertices of the tree is convex for any parameterization of the influence diagram. The bound provided by the linear relaxation is better than the bound that could be obtained using SPU on a soluble relaxation.</p><p>Part III focuses on the airplane maintenance problem at Air France. While the predictive maintenance problem with capacity constraints formalized in Part I considers discrete observations, the decision maker of the airplane maintenance problem has access to sensor data recorded during flights. We describe a statistical methodology to cast this airplane maintenance problem as a weakly coupled POMDP problem. In particular, based on a dataset of sensor data we</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Part Appendix</head><p>A Examples where z IP &lt; v * ml or z IP &gt; v * ml In this section, we describe two instances showing respectively that MILP (5.1) is neither an upper bound nor a lower bound. We denote by z IP the optimal value of MILP (5.1).</p><p>A.1 The inequality z IP v * ml does not hold in general  Solving P wc ml with T = 4 using MILP (4.7) on X S , X O and X A , we obtain an optimal value of v * ml = 47.3693, while the optimal value of our MILP (5.1) is z IP = 47.7356. Hence, we obtain v * ml &lt; z IP . Therefore, v * ml z IP does not hold in general.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>2. 2 Lof Figures 2 . 4 21 5. 1</head><label>224211</label><figDesc>'évolution de l'état de dégradation d'un composant avec quatre états. Le composant débute dans son état le plus sain et évolue de manière stochastique (flèches bleues) vers un état plus dégradé. Pour chaque état de dégradation, le décideur a accès à une observation bruitée qui est émise de manière aléatoire(flèches bleues en zigzag). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.3 Modélisation du problème de maintenance prédictive avec contraintes de capacité pour un système de M composants. Les flèches bleues et rouges représentent respectivement la détérioration stochastique des composants et la politique de maintenance. À chaque plage de maintenance, chaque composant est dans un état de dégradation et le décideur a accès à une observation partielle pour chaque composant. Puis le décideur applique une politique de maintenance (flèches rouges) qui retourne une décision de maintenance qui impacte l'état de dégradation de chaque composant. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xi List Les deux diagrammes d'influence qui modélisent l'exemple de la décision médicale et le POMDP avec politique sans mémoire. Les sommets en forme de cercles, carrés et losanges représentent respectivement les sommets chances, décisions et utilités. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . The relaxations of Section 5.4. An arrow from Problem X to a Problem Y indicates that Y is a relaxation of X in the sense we defined at the beginning of this section. In each block, we indicate which assumptions (see p. 61) we use to obtain the formulations. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 70 5.2 Scheme of the evaluation of our implicit policy</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>6. 1 90 7. 1 of Figures 9 . 2</head><label>190192</label><figDesc>Influence diagram examples, where we represent chance vertices (V s ) in circles, decision vertices (V a ) in rectangles, and utility vertices (V r ) in diamonds. . . . . . 89 6.2 influence diagrams of Examples 5 and 6. . . . . . . . . . . . . . . . . . . . . . . . . 89 6.3 a) A directed graph G, b) a junction tree on G, and c) a gradual rooted junction tree on G, where, for each cluster C , we indicate on the left part of the labels the vertices of C \offspring(C ), and on the right part the vertices of offspring(C ). . . . Example where satisfying (7.4) on junction tree b) is not sufficient to ensure factorization on graph a). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.2 Example of influence diagram whose rooted treewidth is larger than its pathwidth. a. Influence diagram. b. Path decomposition with minimum width. c. RJT with minimum width. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.3 Illustration of the proof of Proposition 7.9. Plain arcs represent arcs, dashed line trails. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.4 Rooted junction tree produced by Algorithm 4 on the example of Figure 6.2b. The offspring of a vertex is to the right of symbol -. . . . . . . . . . . . . . . . . . . . . . 8.1 influence diagram and its RJT with a non valid cut (6.7) for C = {a, u, v, b} with C ⊥ ⊥ = {u}. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8.2 Example of augmented graph G † on a POMDP. . . . . . . . . . . . . . . . . . . . . . 8.3 influence diagrams with useful McCormick inequalities. . . . . . . . . . . . . . . . 8.4 Graph relaxations corresponding to linear relaxations for the chess game example. 8.5 RJT for the chess game. The element to the right of -is the offspring. . . . . . . . 8.6 Rooted Junction Tree built by Algorithm (4) for a POMDP with limited memory. . 8.7 A bigger Rooted Junction Tree for a POMDP with limited memory. . . . . . . . . . 9.1 Illustration of the proof of Lemma 9.3. a) Direct statement, with j = i -1. Trail P is illustrated by dashed line, trail Q by a dash-dotted line, and other paths by dotted lines. b) Converse statement, with path Q in plain line, and other paths dotted lines, and paths in G in dashed lines. . . . . . . . . . . . . . . . . . . . . . . . . . . . xii List Planar representation of the set of achievable moments M(G), the set of achievable deterministic moments M d (G) and the convex hull of the set of achievable moments Conv(M(G)) = Conv M d (G) . . . . . . . . . . . . . . . . . . . . . . . . . 9.3 Influence diagram and parametrization used in the proof of Theorem 9.5 . . . . . 9.4 Influence diagram of Example 7 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 . 1 -</head><label>11</label><figDesc>Figure1.1 -The airplane maintenance problem: we want to find a maintenance policy (red arrows) that takes sensor data from M equipments in input and returns a maintenance decision in output.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 1 . 2 -</head><label>12</label><figDesc>Figure 1.2 -Degradation state evolution of a component with four states. The component starts in its most healthy state and evolves stochastically (blue arrows) toward a more degraded state.At each degradation state, the decision maker has access to a noisy observation that is randomly emitted (squiggly blue arrows).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 1 . 4 -</head><label>14</label><figDesc>Figure 1.4 -The two influence diagrams modeling the medical example and the POMDP example. The circle, square and diamond vertices respectively represent the chance, decision and utility vertices.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>x</head><label></label><figDesc>∈ Z p × R n-p , où b et c sont des vecteurs réels et A une matrice réelle.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>) et o = (o 1 , . . . , o M ) représentent respectivement l'action et l'observation du système. Pour chaque composant m, une transition (s, a, s ) amène une récompense immédiate r m (s, a, s ) = -Coût m (maintenance)1 réparer (a) -Coût m (panne)1 panne (s ), où 1 x (y) vaut 1 si x = y et 0 sinon.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>Un POMDP avec politique sans mémoire.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 2 . 4 -</head><label>24</label><figDesc>Figure 2.4 -Les deux diagrammes d'influence qui modélisent l'exemple de la décision médicale et le POMDP avec politique sans mémoire. Les sommets en forme de cercles, carrés et losanges représentent respectivement les sommets chances, décisions et utilités.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>s m ) and P(S t +1 = s |S t = s, A t = a) = M m=1 p m (s m |s m , a m ), (3.4) for all t in [T ]. In addition, the reward is assumed to decompose additively r (s, a, s ) = M m=1 r m (s m , a m , s m ). (3.5)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head></head><label></label><figDesc>the observation space X O = 2 It suffices to set D m := D mk M , b := bk where k = min i ∈[q] b i and b i indicates the i -th coordinate of vector b, and XA</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Remark 4 .</head><label>4</label><figDesc>Suppose that the decision maker has access to an initial observation o in X O . Hence, for any policy δ in ∆ ml we have P δ (O 1 = o) = 1. Taking into account the initial observation requires to slightly modify the constraints of NLP (4.1): We replace constraints (4.1e) and (4.1g) in NLP (4.1) at time t = 1 by</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>s</head><label></label><figDesc>∈X S p(o|s )µ t -1 s a s = p(o|s)p(s|s , a ) s µ t -1 s a s s ∈X S p(o|s )p(s |s , a ) s µ t -1 s a s = p(o|s)p(s|s , a ) s ∈X S p(o|s )p(s |s , a ) By setting p(s|s , a , o) = p(o|s)p(s|s , a ) s∈X S p(o|s)p(s|s , a ) , equality (4.8c) holds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>(4. 16 )</head><label>16</label><figDesc>First, we prove Inequality(4.15). By definition, we have ∆ ml ⊆ ∆ his . Hence, we obtain v * ml v * his . Using Theorem 4.1, we deduce that z * v * his . Therefore the inequality v * ml v * his z * R holds. Now we prove Inequality (4.16</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>(4. 23 ) 4 . 6 .</head><label>2346</label><figDesc>Proposition Let λ be the vector of value functions of the probability distribution P δ induced by a policy δ. Then, λ satisfies b lb λ b ub .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Proposition 4 . 9 .</head><label>49</label><figDesc>Let (λ, δ) be a feasible solution of NLP (4.24). Then, λ satisfies b lb,c λ b ub,c . In addition, the bounds obtained are tighter than b lb and b ub , i.e., b lb b lb,c and b ub b ub,c .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>2 .</head><label>2</label><figDesc>The first column indicates the instance considered. The three next columns indicate respectively the cardinality of X S , X O and X A of the instance. The fourth column indicates the algorithm used. The last six columns indicate the total expected reward (Obj.) and the gap values for different finite horizon T ∈ {5, 10, 20}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head></head><label></label><figDesc>and, X A , p , and r are respectively defined by (3.2)-(3.5</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_20"><head></head><label></label><figDesc>4.1, constraints (5.1b) and (5.1c) ensure that the variables τ m can still be interpreted as the vector of moments of the probability distribution P δ m induced by the deterministic policy δ m for each component m in [M ]. Constraint (5.1d) is a surrogate modeling of the linking constraints in the definition of X A (3.2). However, given a feasible solution (τ m , δ m ) m∈[M ]    </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_21"><head></head><label></label><figDesc>Second, the number of binary variables in MILP (5.1) is linear in M . Indeed, the number of binary variables of MILP (5.1) is T M m=1 |X m O ||X m A |. Furthermore, using the definition of the "local" policy set and the fact that X m A = {0, 1} in the predictive maintenance problem with capacity constraints, a "local" policy δ m can be encoded using only the binary variables δ t ,m 1|o for o ∈ X m O , m ∈ [M ] and t ∈ [T ]. Hence the number of binary variables is T M m=1 |X m O | in that content.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_22"><head>Theorem 5 . 3 .</head><label>53</label><figDesc>(P wc ml ) and MILP (5.1) are both relaxations of MILP (5.3). In particular, z LB v * ml and z LB z IP .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_23"><head>Proof of Theorem 5 . 3 .</head><label>53</label><figDesc>Let (τ m , δ m ) m∈[M ] be a feasible solution of MILP (5.3). We prove that (τ m , δ m ) m∈[M ] is a feasible solution of MILP (5.1) and (P wc ml ). First, we show that (τ m , δ m ) m∈[M ] is a feasible solution of MILP (5.1). We define the variables τ t ,m a for any a ∈ X m A , m ∈ [M ], and t ∈ [T ] such that s∈X m S ,o∈X m O τ t ,m soa = τ t ,m a . In addition, we introduce the variables τ t ,m o = s∈X m S ,o∈X m O τ t ,m soa for any o ∈ X m O , any m ∈ [M ] and t ∈ [T ]. It suffices to show that inequality (5.1d) holds. We compute the left-hand side of (5.1d).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_24"><head>Figure 5 . 1 illustrates</head><label>51</label><figDesc>Theorems 5.3, 5.4 and the Lagrangian relaxation property.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_25"><head>Algorithm 1 1 : 2 : 3 : 4 : 5 :</head><label>112345</label><figDesc>Column generation to compute z LR . Input: T , weakly coupled POMDP (X m S , X m O , X m A , p m , r m , D m ) m∈[M ] , b Output: An optimal solution of the master problem (5.12) and the value z LR Initialize z ← -∞, Q m ← for m = 1, . . . , M do Define δ m such that δ t ,m a|o = 1 a m e (a) for every a ∈ X m A , o ∈ X m O and t ∈ [T ] 6:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_26"><head>13 : 15 :Figure 5 . 1 - 5 . 4 . 5 Figure 5 . 1 1 -</head><label>131551545511</label><figDesc>Figure 5.1 -The relaxations of Section 5.4. An arrow from Problem X to a Problem Y indicates that Y is a relaxation of X in the sense we defined at the beginning of this section. In each block, we indicate which assumptions (see p. 61) we use to obtain the formulations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_27"><head>5 :</head><label>5</label><figDesc>Return a = (a 1 , . . . , a M ) for which δ t ,m a m |o m = 1 for every m in[M ]    </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_28"><head>a∈</head><label></label><figDesc>{0, 1} for any a ∈ X m A and m ∈ [M ]. Let a * be the action taken at step 5. Therefore, τ t ,m a = 1 when a = a m * and 0 otherwise. Now we compute the linking constraint of (3.2).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_29"><head>D</head><label></label><figDesc>m (a m * ) = M m=1 D m (a m * )τ t ,m a m * = M m=1 a∈X m A D m (a)τ t ,m a b</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_30"><head>. 4 TFigure 5 . 2 -</head><label>452</label><figDesc>Figure5.2 -Scheme of the evaluation of our implicit policy(5.14)  in Algorithm 3 at time t = 3 and t = 4. The decision maker observes h 3 and takes action Act3  8 (h 3 ). Then, the decision maker observes h 4 and takes action Act4  9 (h 4 ). The black points indicate the time steps and the red point corresponds to the time when the decision is taken. The black hatched lines represent the past at the current time (red). The red square indicates the horizon taken into account in the optimization problem.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_31"><head></head><label></label><figDesc>|X m S | = 5, |X m O | = 5 and |X m A | = 2. Each bridge starts almost surely in its most healthy state. We add noises to the transition probabilities and emission probabilities of the bridges to ensure that they have slightly different parameters p m for all m in [M ]. For every bridge m in [M ], we set C m F = 1000 and C m R = 100. The bridges are inspected every months and evolve until the horizon of H = 24 months. One instance consists in the value Instance set</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_32"><head>Figure 6 . 1 -</head><label>61</label><figDesc>Figure 6.1 -Influence diagram examples, where we represent chance vertices (V s ) in circles, decision vertices (V a ) in rectangles, and utility vertices (V r ) in diamonds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_33"><head>r 3 s 4 (</head><label>4</label><figDesc>b) Bob and Alice chess game</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_34"><head>Figure 6 . 2 -</head><label>62</label><figDesc>Figure 6.2 -influence diagrams of Examples 5 and 6.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_35"><head>Figure 6 . 3 -</head><label>63</label><figDesc>Figure 6.3 -a) A directed graph G, b) a junction tree on G, and c) a gradual rooted junction tree on G, where, for each cluster C , we indicate on the left part of the labels the vertices of C \offspring(C ), and on the right part the vertices of offspring(C ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_36"><head></head><label></label><figDesc>where C v is the above-defined root-cluster of v, and let Č denote C \offspring(C ). We say that an RJT is a gradual RJT if for all v in V , offspring(C v ) = {v}. Note that by adding vertices to an RJT, we can always turn it into a gradual RJT. Indeed, suppose that offspring(C ) = {v 1 , . . . , v k }, where v 1 , . . . , v k are listed in a topological order. It suffices to replace the vertex C byC 1 → C 2 → • • • → C k where C i = C \{v i +1 , . . . , v k },with an arc from the parent of C to C 1 and arcs from C k to the children of C . All the results in this thesis are more simply written with gradual RJTs even though they could have been written with RJTs. See Figure 6.3 for an example of junction tree and RJT. Given a vertex C in V, we refer as G C = (V C , A C ) the directed subtree of G rooted at vertex C .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_37"><head>Lemma 6 . 2 .Proposition 6 . 3 .</head><label>6263</label><figDesc>Let D, D ⊆ C . If D and D are strategy independent in C , then D ∪ D is strategy independent in C . Lemma 6.2 ensures the existence and uniqueness of the largest inclusion-wise strategy independent set. We denote by C ⊥ ⊥ such a subset of C . We denote by C ⊥ ⊥ the set C \C ⊥ ⊥ . The set C ⊥ ⊥ can be empty. In Section 8.2.2, we give a full characterization of the set C ⊥ ⊥ from which we obtain the following Proposition: Given a set C in V , C ⊥ ⊥ can be computed in O |C |(|V | + |A|) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_38"><head>2 Proposition 7 . 2 .</head><label>272</label><figDesc>the definition M 0 (T ) are usually called local consistency constraints<ref type="bibr" target="#b155">[156]</ref>. Proposition 7.2 below states that the marginal polytope of a junction tree characterizes the vector of moments deriving from a probability distribution on X V . For convenience, we introduce the set of separatorsS = C 1 ∩C 2 | (C 1 ,C 2 ) ∈ E . [156,Proposition 2.1]  Let G = (V, A) be a directed graphical model and T = (V, E) a junction tree of G. A vector of moments µ belongs to M 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_39"><head>Figure 7 . 1 -</head><label>71</label><figDesc>Figure 7.1 -Example where satisfying (7.4) on junction tree b) is not sufficient to ensure factorization on graph a).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_40"><head>. 4 ) 7 . 3 .</head><label>473</label><figDesc>Theorem Let µ be a vector of moments in the marginal polytope of an RJT G = (V, A) of G = (V, A). If µ satisfies (7.4), then the unique probability distribution P µ on X V factorizes according to G.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_41"><head>Figure 7 .</head><label>7</label><figDesc>1.b, Equation(7.4)  does not enforce the independence of u and v, which is required on the graph of Figure7.1.a. But Theorem 7.3 ensures that property (7.4) becomes a sufficient condition under the additional assumption that G = (V, A) is an RJT.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_42"><head>7 . 4 . 3 . 7 . 5 .</head><label>74375</label><figDesc>then either there is a unique path from C u to C v or from C v to C u in G. Proof. Let G be an RJT on G. Consider a vertex v of G and a vertex C of G containing v. Recall that G v is the subtree of G induced by the vertices containing v. Since C is a vertex of G v , and by definition of C v , there exists a C v -C path in G. Now consider u ∈ pa(v). Since fa(v) ⊆ C v , we Moments on rooted junction trees have u ∈ C v . Thus there exists a C u -C v path in G. The first statement follows by induction along a u-v path in G.We now show the second statement. Let w be a vertex in des G (u) ∩ des G (v), then by the first statement there exists both a C u -C w and a C v -C w path in G. As G is a rooted tree, this implies the existence of either aC u -C v path or of a C v -C u path in G.The following lemma is key in proving Theorem 7.Lemma Let C , D be subsets of V such that fa(D) ⊆ C and des(D) ∩ C = D. Consider a distribution µ C on C . Suppose that for each v in D, X v is independent from its non-descendants given its parents according to µ C . Then, µ C factorizes as µ C = µ C \D v∈D q v|pa(v) where µ C \D =x D</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_43"><head>7 . 4 Algorithm 4 2 :</head><label>7442</label><figDesc>It turns out that the RJT produced by Algorithm 4 has been considered in the literature under the name bucket tree [66, Definition 5.2]. Build a minimal gradual RJT given a topological order 1: Input G = (V, A) and a topological order on G Initialize C v = for all v ∈ V and A = 3: for each vertex v of V taken in reverse topological order do 4:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_44"><head>Figure 6 .</head><label>6</label><figDesc>2b, Algorithm 4 produces the RJT represented on Figure 7.4. Algorithm 4 runs in polynomial time since there are |V | iterations, whose individually have a time complexity in the worst case of |V |.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_45"><head>Proposition 7 . 8 . 7 . 5 .</head><label>7875</label><figDesc>Let G = (V, A) be an RJT satisfying offspring(C v ) = {v}, and be a topological Building a gradual RJT order on G. Then induces a topological order on G andw ∈ T v =⇒ C v C w , ch(u) ∩ T v = and u v =⇒ u ∈ C v .Proof of Proposition 7.8. Let G = (V, A) be an RJT satisfying offspring(C v ) = {v}, and be a topological order on G. Property 1 in Proposition 7.4 ensures that induces a topological order on G.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_46"><head>Proposition 7 . 9 .</head><label>79</label><figDesc>The graph G = (V, A) produced by Algorithm 4 is the unique RJT satisfying offspring(C v ) = {v} such that the topological order on G taken as input of Algorithm 4 induces a topological order on G and the implications in (7.7) are equivalences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_47"><head>Figure 7 . 2 -Figure 7 . 3 -</head><label>7273</label><figDesc>Figure 7.2 -Example of influence diagram whose rooted treewidth is larger than its pathwidth. a. Influence diagram. b. Path decomposition with minimum width. c. RJT with minimum width.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_48"><head>Figure 7 . 4 -</head><label>74</label><figDesc>Figure 7.4 -Rooted junction tree produced by Algorithm 4 on the example of Figure 6.2b. The offspring of a vertex is to the right of symbol -.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_49"><head>Figure 8 . 1 -</head><label>81</label><figDesc>Figure 8.1 -influence diagram and its RJT with a non valid cut (6.7) for C = {a, u, v, b} with C ⊥ ⊥ = {u}.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_50"><head></head><label></label><figDesc>for any probability distribution factorizing on G if and only if B 1 and B 2 are d-separated by D [75, Theorem 3.4].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_51"><head>Lemma 6 . 2 .</head><label>62</label><figDesc>for any probability distribution factorizing on G † . Using (8.10), we get that P δ (X D |X C \D ) = P G † (X D |X C \D ) for any parametrization ρ and any strategy δ, which concludes the proof. The characterization of Corollary 8.6 allows us to define the strategy independent sets in C using the notion of d-separation, which is a pure graphical property. Now we can prove Lemma 6.2 which we recall here: Let D, D ⊆ C . If D and D are strategy independent in C , then D ∪ D is strategy independent in C .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_52"><head>4 Figure 8 . 2 -Lemma 8 . 8 .</head><label>48288</label><figDesc>Figure 8.2 -Example of augmented graph G † on a POMDP.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_53"><head>Lemma 8 .</head><label>8</label><figDesc>8 has been recently proven by two of the authors [27, Theorem 1]. Using their terminology, M * is the Markov blanket of B in C . Note that if C = V this is the usual Markov blanket.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_54"><head>.</head><label></label><figDesc>Consider x ∈ [0, 1], y ∈ [0, b] and z ∈ [0, 1], such that z = x y. Noting that (1x)(by) 0 we obtain Constraint (8.13a). Constraints (8.13b) and (8.13c) are obtained by upper bounding by bounding one variable. Now assume that x ∈ {0, 1}, y ∈ [0, b] and z ∈ [0, 1] satisfy Equation (8.13). Then, if x = 1, constraints (8.13a) and (8.13b) yield z = y. Otherwise, as z 0, we have z = 0 by (8.13c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_55"><head>Chapter 8 .Figure 8 . 3 -</head><label>883</label><figDesc>Figure 8.3 -influence diagrams with useful McCormick inequalities.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_56"><head></head><label></label><figDesc>section provides examples of influence diagrams where inequalities McCormick(v, b) improves the linear relaxation of MILPs (8.6) and (8.9). Consider the influence diagram on Figure 8.3.a, and assume that we have a bound µ st b st . Then, the McCormick relaxation of µ st a = µ st δ a|t reads µ st a µ st + b st (δ a|t -1) µ st a b st δ a|t</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_57"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8.4 illustrates G and G ⊥ ⊥ on the influence diagram of Figure 6.2b. Note that A ⊆ A ⊥ ⊥ ⊆ A, and remark the three following facts on G and G ⊥ ⊥ . First, the definition of both influence diagrams depends on G and G. Second, G is still an RJT on G and G ⊥ ⊥ . And third, any parametriza- tion (X V , p,r) of G is also a parametrization of G and of G ⊥ ⊥ . The second and third results are satisfied by any influence diagram G = (V, A ∪ A ), where A contains only arcs of the form (u, v) with v ∈ V a and u ∈ C v . Denoted by ∆ G the set of feasible strategies for (G , X V , p,r), we can extend the definition of M G (G, ρ) in Corollary 8.2 to such G with</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_58"><head></head><label></label><figDesc>Computing the boundsUsing the McCormick's inequalities requires to compute a vector of lower bound b lb,c and upper bound b ub,c of the feasible vectors λ of NLP (8.25). Let G = (V, A) be a gradual RJT on G = (V, A). We denote by L G be the set of leafs of G, i.e., L G = {C ∈ V, ch(C ) = }. We define inductively on v the functions b lb,c C v , b ub,c</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_59"><head>∀v ∈ V Proposition 8 . 18 .</head><label>818</label><figDesc>Let (λ, δ) be a feasible solution of NLP (8.25). Then, λ satisfies b lb,c λ b ub,c . In addition, the bounds obtained are tighter than b lb and b ub , i.e., b lb b lb,c and b ub b ub,c .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_60"><head>. 5 ) 1 obtained with b = 1 ,</head><label>511</label><figDesc>on page 112 the polytope Q b obtained using the vector of bounds b in Mc-Cormick constraints. Also, recall from Remark 11 that, in the special case of the polytope Q McCormick constraints are loose. To study the impact of McCormick constraints and valid inequalities, we solve the problems max</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_61"><head>Algorithm 5 Lemma 9 . 4 .</head><label>594</label><figDesc>runs in polynomial time because each step is polynomial in the size of the graph. It achieves the proof.For any set C and binary relation R, we denote by C Rv the set of vertices u in C such that u R v.The following lemma will be useful in the proof of Lemma 9.3. Let H denote the relevance graph of G. In general, if u ∈ des(v), then (v, u) ∈ H . If G is soluble, then it becomes an equivalence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_62"><head>Figure 9 . 1 -</head><label>91</label><figDesc>Figure 9.1 -Illustration of the proof of Lemma 9.3. a) Direct statement, with j = i -1. Trail P is illustrated by dashed line, trail Q by a dash-dotted line, and other paths by dotted lines. b) Converse statement, with path Q in plain line, and other paths dotted lines, and paths in G in dashed lines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_63"><head>Lemma 9 . 6 .</head><label>96</label><figDesc>For every parametrization ρ and every rooted junction tree G, the convex hull of the set of achievable moments coincides with the convex hull of the set of achievable deterministic moments, i.e., Conv M G (G, ρ) = Conv M d G (G, ρ) , where the notation Conv(A) denotes the convex hull of a set A. Proof. Since M d (G) ⊆ M(G), we have Conv M d (G) ⊆ Conv(M(G)). Now it suffices to prove that M(G) ⊆ Conv M d (G) . Let µ ∈ M(G). Corollary 8.2 ensures that there exists δ</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_64"><head>Figure 9 . 2 illustratesFigure 9 . 2 -</head><label>9292</label><figDesc>Figure 9.2 -Planar representation of the set of achievable moments M(G), the set of achievable deterministic moments M d (G) and the convex hull of the set of achievable moments Conv(M(G)) = Conv M d (G) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_65"><head>Figure 9 . 3 -</head><label>93</label><figDesc>Figure 9.3 -Influence diagram and parametrization used in the proof of Theorem 9.5</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_66"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9.3 or on the paths on Figure 9.3 are unary. All the variables along paths represented by dashed arrows are equal. Strategies δ i can therefore be defined as</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_67"><head>Figure 10 . 1 -</head><label>101</label><figDesc>Figure 10.1 -An example of a decision tree that takes as inputs the average (avg) pressure and the standard deviation (std) of the temperature. It returns a discrete label in {1, 2, 3}. Each label corresponds to a cluster. Clusters 1 and 2 correspond to normal behavior (blue), and cluster 3 corresponds to high-failure risk (red).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_68"><head>1 )</head><label>1</label><figDesc>Maintaining equipment m costs c m m , while a failure on this equipment costs c m f . A maintenancepolicy d is a conditional distribution of A m t m∈[M ] given (Z m τ , F m τ ) τ τ t ,m∈[M ] , (A m t ) t &lt;t ,m∈[M ] .We denote by D the set of maintenance policies. The objective is to find a policy that minimizes</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_70"><head>Figure 10 . 3 -</head><label>103</label><figDesc>Figure10.3 -A graphical representation of our approach using the notation introduced in Section 10.2. First, the feature extraction φ from sensor data is represented by purple dashed arcs. Second, the probabilistic dependences of our Gaussian HMMs are represented by blue plain arcs for each equipment. Third, the mapping of our decision tree f is represented by dotted green arcs. Fourth, the policy of the decision maker is represented by red plain arcs. Dashed arcs indicate a mapping. Plain arcs indicate the probabilistic dependences as for a Bayesian network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_71"><head>(10. 3 ) 3 .</head><label>33</label><figDesc>In the usual POMDP methodology, the observations O m t are the outputs of the system directly observed by the decision maker on the system, and the POMDP parameters p are learned from a history of trajectories on O m t , A m t on each equipment m. On the contrary, in the content of Problem (10.2), the decision maker observes time seriesZ m τ in R k m × {0, 1}k m and not observations in a finite set X m O . In order to turn the sensor data into observations in a finite set X m O , we 10.Modeling as a weakly coupled POMDP use two successive mappings φ m and f m as follows.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_72"><head></head><label></label><figDesc>define the observation at time t asO m t := f m • φ (Z m τ ) τ τ t .To obtain a policy d for Problem (10.2), we then proceed as follows. We learn the parameters θ m of a weakly coupled POMDP based on the history of the observations, and we compute an approximate policy δ for the weakly coupled POMDP (see Chapter 5), and then retrieve a maintenance policy d for the initial problem (10.2) using d := δ • f • φ(10.4)    </figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_73"><head></head><label></label><figDesc>m S := [n m S ]. The hidden state corresponds to the degradation level. A left-right HMM is a HMM such that there exists an total ordering ≺ on X m S and for all s, s ∈ X m S , pm (s |s) = 0 when s ≺ s. Left-right HMMs [125, Fig.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_74"><head></head><label></label><figDesc>equipment m. We use the action space X A ⊆ {0, 1} M defined in (10.1), which has the form (3.2) by setting D m (a) = a for all a in {0, 1} and b = 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_75"><head>10. 3 .</head><label>3</label><figDesc>Modeling as a weakly coupled POMDP probability distributions. p m (s |s, a) := P S m t +1 = s |S m t = s, A m t = a = pm (s) if a = 1 pm (s |s) otherwise p m (o|s) := P O m t = p|S m t = s = E 1 { f (X m t )=o} |S m t = s (10.5) (10.6) for all s, s ∈ X m S , o ∈ X m O , a ∈ {0, 1} and m ∈ [M ]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_76"><head></head><label></label><figDesc>Now we set the reward function. We associate a maintenance cost c m m and a failure cost c m f to each equipment m. The individual immediate reward function can be writtenr m (s, a, s ) = -1 s =s m F c m f -1 a=1 c m m , for all s, s ∈ X m S , a ∈ {0, 1} and m ∈ [M ].We solve Problem (10.2) using a maintenance policy d := δ • f • φ, where δ is a solution of P wc ml (see Section 3.2).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_77"><head>Algorithm 6 1 : 5 :</head><label>615</label><figDesc>Airline's maintenance policy at a given time t Input Decision trees g m , observations x m for all m in [M ]. 2: Compute vector g m (x m ) m∈[M ] . 3: Compute M r = {m ∈ [M ], s.t. g m (x m ) = 1}. 4: if |M r | K then Sort the components m 1 , . . . , m |M r | such that c</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_78"><head></head><label></label><figDesc>(a) Algorithm 3 with T r = 2. (b) Algorithm 3 with T r = 2. (c) Algorithm 3 with T r = 5. (d) Algorithm 3 with T r = 5. (e) Algorithm 3 with T r = 10. (f) Algorithm 3 with T r = 10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_79"><head>Figure 10 . 5 - 2 -</head><label>1052</label><figDesc>Figure 10.5 -An example of the application of our maintenance policy on two airplanes (one for each column) of Air France's dataset. The horizontal axis represents the flights. The blue (resp. red) plain vertical lines indicate the flights after which a maintenance has been performed on equipment 1 (resp. equipment 2). The blue (resp. red) dashed lines represent the maintenance suggestion of our maintenance policy for equipment 1 (resp. equipment 2). The rising edges of the dashed lines indicate when our maintenance policy would have suggested to maintain the equipment.</figDesc><graphic coords="196,118.77,399.72,170.08,127.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_80"><head>1 O = X 2 O</head><label>12</label><figDesc>Consider a weakly coupled POMDP withM = 2, K = 1, X 1 S = X 2 S = {1, 2, 3}, and X = {1, 2}. We set the following initial probability data, p 1 (•) = 0.0286 0.4429 0.5284 , p 2 (•) = 0.5328 0.2202 0.2469 , the following transition probability data,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>1.1 The airplane maintenance problem: we want to find a maintenance policy (red arrows) that takes sensor data from M equipments in input and returns a maintenance decision in output. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 1.2 Degradation state evolution of a component with four states. The component starts in its most healthy state and evolves stochastically (blue arrows) toward a more degraded state. At each degradation state, the decision maker has access to a noisy observation that is randomly emitted (squiggly blue arrows). . . . . . . . 4</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>(S t = s t ,O t = o t , A t = a t ) 1 t T , S T +1 = s T +1 = p(s 1 )</figDesc><table><row><cell>T t =1</cell><cell>p(o t |s t )p(s t +1 |s t , a t )δ t a t |o t</cell><cell>(1.2)</cell></row><row><cell>for every (s</cell><cell></cell><cell></cell></row></table><note><p><p><p>.</p><ref type="bibr" target="#b0">1)</ref> </p>where S t , O t and A t are random variables representing the state, the observation and the action at time t . The expectation in (1.1) over δ is taken according to the probability distribution P δ Chapter 1. Introduction induced by the policy δ chosen in ∆, and defined as follows P δ t , o t , a t ) 1 t T</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>2.2. Processus de décisions avec observations partielles manière</head><label></label><figDesc>efficace avec des solveurs commerciaux. Comme ces solveurs utilisent l'algorithme de séparation et évaluation 2 , il est usuel d'introduire des inégalités valides qui améliorent la qualité des bornes obtenues lorsqu'on résout le problème sans les contraintes d'intégrité. Une seconde contribution de cette thèse est un ensemble d'inégalités valides qui aident la résolution de notre PLNE. Plusieurs expérimentations numériques montrent l'efficacité de ces inégalités valides.</figDesc><table><row><cell>Le</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>cadre du POMDP pour le problème de maintenance prédictive avec contraintes de ca- pacité.</head><label></label><figDesc>Il est naturel de supposer que chaque composant se comporte comme un POMDP. Pour chaque composant l'état de dégradation, l'observation partielle et la décision de maintenance (voir figure 2.3) sont respectivement représentés par l'état, l'observation et l'action d'un POMDP. Pour chaque composant, toutes les probabilités conditionnelles p(o|s) et p(s |s, a)</figDesc><table /><note><p>sont calculées à partir des paramètres de la chaîne de Markov cachée. Pour un système de M composants, le problème se divise en M sous-problèmes qui sont faiblement couplés par les contraintes de capacité qui affectent les actions prises sur chaque composant. Par exemple, la contrainte de capacité "le nombre de composants qu'on peut réparer durant une plage de maintenance ne peut dépasser K ," est modélisée par la contrainte M m=1 a m K , où a m représente l'action prise sur le composant m et vaut 1 si le composant m est réparé et 0 sinon. Comme les actions prises sur chaque composant sont couplées, la politique de maintenance doit être considérée sur l'ensemble du système. À chaque pas de temps t , la politique de maintenance est de la forme δ t a|o , où a = (a 1 , . . . , a M</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>.4. Une méthodologie statistique pour le problème de maintenance des avions correspondent</head><label></label><figDesc>à des données brutes, qui sont continues et en grande dimension. Il est donc nécessaire de développer une méthodologie qui permette de transformer les valeurs des signaux en indicateurs discrets.</figDesc><table><row><cell>maintenance prédictive avec contraintes de capacité. Les expérimentations numériques mon-trent que notre politique de maintenance donne de "bons" résultats numériques comparés à 2La deuxième question concerne la phase d'apprentissage. Sur la base des données historiques, ceux obtenus en utilisant la politique de maintenance d'Air France.</cell></row><row><cell>l'objectif est d'estimer les paramètres qui définissent complètement le modèle de Markov caché</cell></row><row><cell>pour chaque équipement, et qui sont nécessaires comme entrées du problème de maintenance</cell></row><row><cell>prédictive avec les contraintes de capacité.</cell></row><row><cell>La troisième question concerne l'interprétabilité de l'approche. En effet, même si elles n'ont</cell></row><row><cell>pas d'impact immédiat sur la sécurité des vols 3 , les décisions de maintenance que nous sug-</cell></row><row><cell>gérons ne sont pas bénignes. Les opérations de maintenance telles que le démontage du train</cell></row><row><cell>d'atterrissage d'un avion long-courrier sont coûteuses et prennent du temps. Et si une panne</cell></row><row><cell>se produit, Air France devra annuler les prochains vols effectués par l'avion, ce qui est encore</cell></row><row><cell>plus coûteux. Alors, si nous voulons que notre modèle soit utilisé dans la pratique, les prédic-</cell></row><row><cell>tions du modèle statistique, le modèle de Markov caché, ainsi que les décisions de maintenance</cell></row><row><cell>résultant d'une politique de maintenance, doivent être fiables du point de vue des ingénieurs</cell></row><row><cell>de maintenance. Une difficulté supplémentaire sur ce point est le fait que la base données que</cell></row><row><cell>nous considérons contient un faible nombre de pannes du fait qu'Air France essaie de les éviter</cell></row><row><cell>autant que possible et que nous disposons d'un faible nombre de données brutes correspon-</cell></row><row><cell>dant à un comportement défaillant d'un équipement avant la panne. Par conséquent, nous ne</cell></row><row><cell>pouvons pas valider le modèle statistique à l'aide de validations expérimentales.</cell></row><row><cell>Notre approche est décrite dans le chapitre 10 et aborde ces questions en combinant quatre</cell></row><row><cell>étapes qui utilisent plusieurs outils statistiques. Premièrement, comme les données brutes que</cell></row><row><cell>nous traitons sont en grande dimension et bruitées, nous transformons les séries temporelles</cell></row><row><cell>enregistrées pendant un vol par un nombre raisonnable d'attributs pertinents. Par exemple,</cell></row><row><cell>si un capteur enregistre l'évolution de la pression dans un équipement pendant un vol, un ex-</cell></row><row><cell>emple d'attribut est la moyenne ou l'écart-type de la pression sur la durée du vol. Ensuite, 2, pour nôtre problème appliqué. Cependant, modéliser un problème de mainte-nous apprenons les paramètres d'un modèle de Markov caché gaussien qui prédit l'évolution nance d'un système réel comme un problème de maintenance prédictive avec contraintes de du vecteur des attributs. Un tel modèle est un modèle de Markov caché où les observations capacité comme dans la section 2.1 n'est pas immédiat. En particulier, c'est le cas de notre sont continues et sont émises selon une distribution de probabilité gaussienne. Troisième-problème de maintenance des avions chez Air France. En effet, les données disponibles à ment, nous transformons ce modèle de Markov caché gaussien en un modèle de Markov caché chaque plage de maintenance sont des données brutes enregistrées par les capteurs sur les avec observations discrètes en utilisant un arbre de décision. Il prend un vecteur d'observations équipements des avions et correspondent aux valeurs de signaux enregistrés durant les vols. continues en entrée et renvoie un indicateur discret. Le mécanisme au sein d'un arbre de dé-Un exemple de signal enregistré par un capteur est l'évolution de la pression au sein d'un cision est une combinaison de règles binaires telles que "est-ce que la pression moyenne est équipement échantillonnée à 1Hz durant un vol. C'est pourquoi notre base de données con-supérieure à 20 bars ?", qui conduit à un indicateur discret, dont les valeurs correspondent aux tient plusieurs années de données brutes et quelques dates de panne. Par conséquent, ex-primer le problème de maintenance des avions comme un problème de maintenance pré-différents niveaux de risques de panne.</cell></row><row><cell>dictive avec contraintes de capacité nécessite le calcul des paramètres des chaînes de Markov Ces règles binaires peuvent être facilement comprises et validées par les ingénieurs de la main-</cell></row><row><cell>cachées qui modélisent les processus de détérioration des équipements à partir de cette base tenance, ce qui répond au troisième problème. Comme cet indicateur est discret, il répond</cell></row><row><cell>de données. Nous proposons une méthodologie statistique préliminaire qui permet de trans-également à la première question. Enfin, à partir du modèle de Markov caché gaussien et de</cell></row><row><cell>former le problème de maintenance des avions (figure 2.1) en un problème de maintenance l'arbre de décision, nous calculons les paramètres du modèle de Markov caché avec indica-</cell></row><row><cell>prédictive avec contraintes de capacité (figure 2.3). teurs discrets, ce qui répond au deuxième problème. L'utilisation de cette approche permet de</cell></row><row><cell>Cela soulève trois questions pratiques. Premièrement, dans le problème de la maintenance considérer le problème de maintenance des avions chez Air France comme un problème de</cell></row><row><cell>prédictive avec contraintes de capacité, le décideur a accès à un indicateur discret comme</cell></row><row><cell>observation partielle, alors que les observations du problème de la maintenance des avions</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>A POMDP is a multi-stage stochastic optimization problem defined as follows. It models on a horizon T in Z + the evolution of a system. At each time t in [T ], the system is in a random state S t , which belongs to a finite state space X S . The system starts in state s in X S with probability p(s) := P(S 1 = s). At time t , the decision maker does not have access to S t , but observes O t , whose value belongs to a finite state X O . When the system is in state S t = s, it emits an observation O t = o with probability P(O t = o|S t = s) := p(o|s). Then, the decision maker takes an action A t , which belongs to a finite space X A .Given an action A t = a, the system transits from state S t = s to state S t +1 = s with probability P S t +1 = s |S t = s, A t = a := p(s |s, a), and the decision maker receives the immediate reward r (s, a, s ), where the reward function is defined as a real valued function r :</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>.3 POMDP problem with memoryless policies Let</head><label></label><figDesc>s 1 ) (X S , X O , X A , p,r) be a POMDP. Given a finite horizon T ∈ Z + , a memoryless policy is a vectorδ = (δ 1 , . . . , δ T ),where δ t is the conditional probability distribution at time t of action A t given observation O t , i.e., δ t a|o := P(A t = a|O t = o) for any a in X A and o in X O . Such policies are said memoryless because the choice of A t only depends on the current observation O t , in contrast with the history of observations and actions H t . We denote by ∆ ml the set of memoryless policies</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">3.2. Weakly coupled POMDP</cell></row><row><cell cols="5">finite horizon T . The POMDP problem is exactly the following:</cell></row><row><cell></cell><cell></cell><cell>T</cell><cell></cell><cell></cell></row><row><cell>max δ∈∆ his</cell><cell>E δ</cell><cell>t =1</cell><cell cols="2">r (S t , A t , S t +1 )</cell><cell>(P his )</cell></row><row><cell cols="3">It is known that (P his ) is PSPACE-hard [113].</cell><cell></cell><cell></cell></row><row><cell>3.1</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell>T t =1</cell><cell>p(o t |s t )p(s t +1 |s t , a t )δ t a t |h t ,</cell></row><row><cell cols="5">where h t = (s 1 , o 1 , . . . , a t -1 , o t ). We denote by E δ the expectation according to P δ . The goal of</cell></row><row><cell cols="5">the decision maker is to find a policy δ in ∆ his maximizing the expected total reward over the</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_13"><head>Chapter 3. Predictive maintenance with capacity constraints a</head><label></label><figDesc>2) by setting D m (a m ) = m for every a m ∈ {0, 1} and m ∈ [M ], and b = K . We assume that each component m evolves independently from state S m t = s to state S m t +1 = s with probability p m (s |s, a), and the decision maker receives reward r m (s, a, s ). In addition, we assume that when a component is maintained, it behaves like a new one, i.e.,</figDesc><table><row><cell>p m (s |s, 1) = p m (s ),</cell><cell>(3.7)</cell></row><row><cell cols="2">for any s, s in X m S , and the conditional probabilities factorize as (3.3) and (3.4). Each compo-nent has a maintenance cost C m R and a failure cost C m F at each component m. The individual</cell></row><row><cell>immediate reward function can be written</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_18"><head></head><label></label><figDesc>4.1e)-(4.1f) are satisfied. It remains to prove that constraints (4.6a), (4.6b), (4.6c) are satisfied. First, (4.6a) holds because</figDesc><table><row><cell>µ t soa</cell><cell>max 0, p(o|s)</cell><cell>µ t -1 s a s</cell><cell>p(o|s)</cell><cell>µ t -1 s a s ,</cell></row><row><cell></cell><cell cols="2">s ∈X S ,a ∈X A</cell><cell>s ∈X S ,a ∈X A</cell><cell></cell></row><row><cell cols="2">Second, (4.6b) holds because</cell><cell></cell><cell></cell><cell></cell></row><row><cell>µ t soa</cell><cell>µ t s oa = δ t a|o</cell><cell>p(o|s</cell><cell></cell><cell></cell></row><row><cell></cell><cell>s ∈X S</cell><cell>s ∈X S</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_19"><head></head><label></label><figDesc>.7). Constraints (4.1b)-(4.1f) ensure that (µ 1 s , µ t sas ) t ∈[T ] satisfies constraints (4.13d)-(4.13e), which implies that it is a feasible solution of Linear program (4.13). Let µ be a feasible solution of Linear program (4.13). It suffices to define variables δ t a|o and µ t</figDesc><table /><note><p>soa for all a in X A , o in X O , s in X S , and t in [T ]. We define these variables using (4.9) and (4.10). In the proof of Proposition 4.3, we proved that (µ, δ) is a feasible solution of the linear relaxation of MILP (4.7). Consequently, the equivalence holds and z * R = v * MDP . Now we prove that inequalities (4.14) hold. Note that Proposition 4.3 ensures that</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_22"><head></head><label></label><figDesc>which appear in the valid inequalities (4.8), and leads to a linear relaxation that is tighter in practice.</figDesc><table /><note><p>Like for MILP (4.7), when δ is a vector of continuous variables, the McCormick's constraints (4.20) do not ensure that the constraints α t soa = λ t soa δ t a|o are satisfied for any s in X S , o in X O , a in X A and t in [T ]. Hence, the constraints (4.18d) are not necessary satisfied in the linear relaxation of MILP (4.21). Intuitively, it means that the feasible set of the linear relaxation of MILP (4.21) is too large. Like for MILP (4.7) we wish to reduce the feasible set of the linear relaxation of MILP (4.21). However, unlike MILP (4.7) we are not able to derive valid inequalities for the variables of MILP (4.21). Instead of introducing valid inequalities for MILP (4.21), we introduce a formulation that uses the conditional independences (4.12) and that gives McCormick's bound on the variables λ that are tighter.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_24"><head></head><label></label><figDesc>Suppose that the induction hypothesis holds until t + 1. We prove the result for t . By definition we have:</figDesc><table><row><cell>b ub,c,t soa =</cell></row></table><note><p><p>s |s, a) r (s, a, s ) + o p(o |s ) s ,a p(s |s, a, o )δ t a |o λ t +1 s o a s p(s |s, a) r (s, a, s ) + o p(o |s ) a δ t a |o s p(s |s, a, o )b ub,t +1 s o a s p(s |s, a) r (s, a, s ) + o p(o |s ) max</p>a s p(s |s, a, o )b ub,t +1 s o a = b ub,c,t soa Now we prove that b ub,c,t soa b ub,t soa . Again, we prove the result by backward induction. It is immediate for t = T. s p(s |s, a)r (s, a, s ) + s ,o p(s |s, a)p(o |s ) max a s p(s |s, a, o )b ub,c,t +1 s o a s p(s |s, a)r (s, a, s ) + s ,s ,o p(s |s, a)p(o |s )p(s |s, a, o ) max a b ub,c,t +1 s o a = s p(s |s, a)r (s, a, s ) + s ,o p(s |s, a)p(o |s ) s p(s |s, a, o ) =1 max a b ub,t +1 s o a = b ub,t soa , The first inequality comes from the induction hypothesis and by decomposing the maximum over the sum. The second equality come from the fact that p(s |s, a, o )p(s |s, a)p(o |s ) = p(s |s, a, o )p(s |s, a)p(o |s ). It achieves the proof.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_25"><head></head><label></label><figDesc>instances are generated by first choosing k s = |X S | = |X O | and k a = |X A |. We then randomly generate the initial probability p(s) s∈X S , the transition probability p(s |s, a) s,s ∈X S S , X O , X A , p,r). A way to measure the difficulty of solving an MILP (4.7) for POMDP (X S , X O , X A , p,r) with horizon T can be characterized by the size of the set of deterministic policies |∆ d ml | = |X A | T |X O | , which only depends on the size of the observation space X O and the action space X A . We generate instances for different values of the pair (k s , k a ).</figDesc><table><row><cell></cell><cell></cell><cell>, the</cell></row><row><cell></cell><cell cols="2">a∈X A</cell></row><row><cell>emission probability p(o|s) s∈X S o∈X O</cell><cell>a∈X A and the immediate reward function r (s, a, s ) s,s ∈X S</cell><cell>. An in-</cell></row><row><cell>stance is the tuple (X</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_26"><head>Table 4 .</head><label>4</label><figDesc>1 -POMDP results using MILP (4.7) with and without (4.8), with a time limit of 3600s simulations to compute the expectation. By definition, the objective value z SARSOP obtained by using this policy is a lower bound of v * his . We run the SARSOP algorithm using the Julia library</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">MILP (4.7)</cell><cell></cell><cell></cell><cell>MILP (4.21)</cell></row><row><cell cols="2">(k s , k a ) T</cell><cell>∆ d ml</cell><cell>Formulation</cell><cell>g i</cell><cell>g f</cell><cell cols="2">Opt Time</cell><cell>g i</cell><cell>g f</cell><cell>Opt Time</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>(%)</cell><cell cols="2">(%) (%)</cell><cell>(s)</cell><cell>(%)</cell><cell>(%) (%)</cell><cell>(s)</cell></row><row><cell>(3, 3)</cell><cell cols="2">10 10 14</cell><cell>Basic</cell><cell cols="3">6.02 Opt 100</cell><cell cols="3">1.49 5.52 Opt 100</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Strengthened</cell><cell cols="3">1.70 Opt 100</cell><cell cols="3">0.62 0.22 Opt 100</cell></row><row><cell></cell><cell cols="2">20 10 28</cell><cell>Basic</cell><cell cols="3">6.04 Opt 100</cell><cell cols="3">664 5.80 Opt 100</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Strengthened</cell><cell cols="3">1.52 Opt 100</cell><cell cols="3">466 1.28 Opt 100</cell></row><row><cell>(3, 4)</cell><cell cols="2">10 10 24</cell><cell>Basic</cell><cell cols="2">9.51 0.34</cell><cell>87</cell><cell cols="3">512 8.69 3.06</cell><cell>43</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Strengthened</cell><cell cols="2">3.16 0.18</cell><cell cols="4">87 514.4 2.16 0.79</cell><cell>61</cell></row><row><cell></cell><cell cols="2">20 10 48</cell><cell>Basic</cell><cell cols="2">9.64 1.96</cell><cell cols="4">43 2221 9.25 8.34</cell><cell>10</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Strengthened</cell><cell cols="2">2.86 1.13</cell><cell cols="4">61 1731 2.38 2.03</cell><cell>21</cell></row><row><cell>(3, 5)</cell><cell cols="2">10 10 34</cell><cell>Basic</cell><cell cols="2">9.33 0.83</cell><cell cols="4">57 1591 8.65 5.49</cell><cell>17</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Strengthened</cell><cell cols="2">2.35 0.38</cell><cell cols="4">70 1113 1.64 1.03</cell><cell>48</cell></row><row><cell></cell><cell cols="2">20 10 69</cell><cell>Basic</cell><cell cols="2">9.60 3.30</cell><cell cols="4">26 2702 9.23 8.62</cell><cell>9</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Strengthened</cell><cell cols="2">2.14 1.14</cell><cell cols="4">52 1879 1.99 1.80</cell><cell>26</cell></row><row><cell>(4, 3)</cell><cell cols="2">10 10 14</cell><cell>Basic</cell><cell cols="3">7.39 Opt 100</cell><cell cols="3">26 5.63 0.39</cell><cell>90</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Strengthened</cell><cell cols="3">2.28 Opt 100</cell><cell cols="3">9.16 1.32 0.18</cell><cell>90</cell></row><row><cell></cell><cell cols="2">20 10 28</cell><cell>Basic</cell><cell cols="2">6.01 1.01</cell><cell cols="4">60 1598 6.02 4.22</cell><cell>35</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Strengthened</cell><cell cols="2">2.03 0.32</cell><cell>80</cell><cell cols="3">987 1.44 1.12</cell><cell>45</cell></row><row><cell>(4, 4)</cell><cell cols="2">10 10 24</cell><cell>Basic</cell><cell cols="2">12.19 0.98</cell><cell cols="4">65 1477 8.80 4.81</cell><cell>40</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Strengthened</cell><cell cols="2">3.44 0.27</cell><cell>80</cell><cell cols="3">967 1.76 1.02</cell><cell>50</cell></row><row><cell></cell><cell cols="2">20 10 48</cell><cell>Basic</cell><cell cols="2">12.29 4.66</cell><cell cols="4">20 2900 9.26 8.59</cell><cell>20</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Strengthened</cell><cell cols="2">3.05 1.48</cell><cell cols="4">30 2651 1.96 1.80</cell><cell>40</cell></row><row><cell>(4, 5)</cell><cell cols="2">10 10 34</cell><cell>Basic</cell><cell cols="2">11.64 1.76</cell><cell cols="4">35 2427 8.36 5.32</cell><cell>30</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Strengthened</cell><cell cols="2">3.09 0.62</cell><cell cols="4">65 1345 1.74 1.32</cell><cell>55</cell></row><row><cell></cell><cell cols="2">20 10 69</cell><cell>Basic</cell><cell cols="2">12.04 5.46</cell><cell cols="4">5 3413 9.48 8.81</cell><cell>16</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Strengthened</cell><cell cols="2">3.20 1.67</cell><cell cols="4">32 2490 2.15 2.08</cell><cell>21</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_27"><head>Table 4</head><label>4</label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell>Size</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Horizon</cell><cell></cell><cell></cell></row><row><cell>Instances</cell><cell cols="3">|X S | |X O | |X A |</cell><cell>Algorithms</cell><cell cols="2">T = 5</cell><cell cols="2">T = 10</cell><cell cols="2">T = 20</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Obj.</cell><cell>Gap(%)</cell><cell>Obj.</cell><cell>Gap(%)</cell><cell>Obj.</cell><cell>Gap(%)</cell></row><row><cell>1d.noisy</cell><cell>4</cell><cell>2</cell><cell>2</cell><cell>MILP</cell><cell>1.56</cell><cell>18.73</cell><cell>2.97</cell><cell>19.18</cell><cell>5.82</cell><cell>18.73</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>SARSOP</cell><cell>0.57</cell><cell>70.12</cell><cell>0.67</cell><cell>81.76</cell><cell>0.81</cell><cell>88.71</cell></row><row><cell>4x5x2  *</cell><cell>39</cell><cell>4</cell><cell>4</cell><cell>MILP</cell><cell>0.37</cell><cell>58.13</cell><cell>0.75</cell><cell>57.45</cell><cell>1.86</cell><cell>47.58</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>SARSOP</cell><cell>0.08</cell><cell>90.87</cell><cell>0.08</cell><cell>95.28</cell><cell>0.08</cell><cell>97.50</cell></row><row><cell>aircraftID</cell><cell>12</cell><cell>5</cell><cell>6</cell><cell>MILP</cell><cell>5.69</cell><cell>0.00</cell><cell>10.10</cell><cell>0.00</cell><cell>19.76</cell><cell>0.00</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>SARSOP</cell><cell>3.39</cell><cell>40.46</cell><cell>7.63</cell><cell>24.46</cell><cell>17.32</cell><cell>12.41</cell></row><row><cell>aloha.10</cell><cell>30</cell><cell>3</cell><cell>9</cell><cell>MILP</cell><cell>38.04</cell><cell>0.56</cell><cell>62.74</cell><cell>1.66</cell><cell>84.92</cell><cell>13.84</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>SARSOP</cell><cell>38.15</cell><cell>0.25</cell><cell>63.74</cell><cell>0.20</cell><cell>89.09</cell><cell>9.61</cell></row><row><cell>cheng.D3-1</cell><cell>3</cell><cell>3</cell><cell>3</cell><cell>MILP</cell><cell>32.29</cell><cell>1.87</cell><cell>64.38</cell><cell>1.11</cell><cell>128.55</cell><cell>0.72</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>SARSOP</cell><cell>32.04</cell><cell>2.65</cell><cell>64.16</cell><cell>1.45</cell><cell>128.28</cell><cell>0.93</cell></row><row><cell>cheng.D4-1</cell><cell>4</cell><cell>4</cell><cell>4</cell><cell>MILP</cell><cell>33.83</cell><cell>5.20</cell><cell>67.37</cell><cell>4.10</cell><cell>134.45</cell><cell>3.54</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>SARSOP</cell><cell>32.40</cell><cell>9.1</cell><cell>65.90</cell><cell>6.19</cell><cell>133.05</cell><cell>4.54</cell></row><row><cell>cheng.D5-1</cell><cell>5</cell><cell>5</cell><cell>5</cell><cell>MILP</cell><cell>32.89</cell><cell>3.28</cell><cell>65.64</cell><cell>2.25</cell><cell>131.12</cell><cell>1.73</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>SARSOP</cell><cell>32.47</cell><cell>4.50</cell><cell>65.23</cell><cell>2.86</cell><cell>130.81</cell><cell>1.96</cell></row><row><cell>learning.c3</cell><cell>24</cell><cell>3</cell><cell>12</cell><cell>MILP</cell><cell>1.63</cell><cell>45.3</cell><cell>2.20</cell><cell>26.76</cell><cell>2.33</cell><cell>22.48</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>SARSOP</cell><cell>0.33</cell><cell>88.89</cell><cell>0.33</cell><cell>89.00</cell><cell>0.34</cell><cell>88.67</cell></row><row><cell>milos-aaai97  *</cell><cell>20</cell><cell>8</cell><cell>6</cell><cell>MILP</cell><cell>26.83</cell><cell>10.28</cell><cell>53.41</cell><cell>36.06</cell><cell>92.09</cell><cell>55.06</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>SARSOP</cell><cell>12.62</cell><cell>57.79</cell><cell>39.52</cell><cell>52.69</cell><cell>97.73</cell><cell>52.31</cell></row><row><cell>network  *</cell><cell>7</cell><cell>2</cell><cell>4</cell><cell>MILP</cell><cell>20.30</cell><cell>2.49</cell><cell>95.06</cell><cell>22.85</cell><cell>203.87</cell><cell>36.02</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>SARSOP</cell><cell>20.88</cell><cell>0.00</cell><cell>95.78</cell><cell>22.26</cell><cell>245.88</cell><cell>22.98</cell></row><row><cell>bridge-repair a</cell><cell>5</cell><cell>5</cell><cell>10</cell><cell>MILP</cell><cell>1992.77</cell><cell cols="2">0.15 7801.56</cell><cell cols="2">0.44 27937.93</cell><cell>0.13</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>SARSOP</cell><cell>1514.15</cell><cell cols="2">24.13 6832.99</cell><cell cols="2">12.80 26568.42</cell><cell>5.03</cell></row><row><cell>query.s2</cell><cell>9</cell><cell>3</cell><cell>2</cell><cell>MILP</cell><cell>21.54</cell><cell>0.95</cell><cell>46.25</cell><cell>0.10</cell><cell>96.50</cell><cell>0.11</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>SARSOP</cell><cell>15.77</cell><cell>27.50</cell><cell>31.68</cell><cell>31.56</cell><cell>64.91</cell><cell>30.66</cell></row><row><cell>machine a</cell><cell>256</cell><cell>16</cell><cell>4</cell><cell>MILP</cell><cell>4.90</cell><cell>0.00</cell><cell>9.50</cell><cell>0.81</cell><cell>17.98</cell><cell>0.05</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>SARSOP</cell><cell>4.90</cell><cell>0.00</cell><cell>9.35</cell><cell>2.38</cell><cell>15.69</cell><cell>12.79</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p>.2 support the remark of Walraven and Spaan [158, Section 3] * Instances of navigation problem a Instances of maintenance problem</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_28"><head>Table 4 .</head><label>4</label><figDesc>2 -Numerical results on benchmark instances. The results written in bold indicate the best value obtained for each instance.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_29"><head></head><label></label><figDesc>in operations research (see e.g. Geoffrion<ref type="bibr" target="#b47">[48,</ref> Theorem 1]) states that the bound of the Lagrangian relaxation of an integer program is not worse than the bound of the linear relaxation. It shows that z LR z R .</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_30"><head></head><label></label><figDesc>In the POMDP literature, the probability distribution P δ S m t |H m t is the belief state of component m. We can use the belief state update (seeRemark 7)  on each of the components to compute the belief state p m (s m |h m ). We introduce the following algorithm: Input An history of observations and actions h ∈ (X O × X A ) t -1 × X O 2: Output An action a ∈ X A 3: Compute the belief state p m (s|h m ) according to the belief state update (see remark below) for every state s in X m S and every component m in [M ] 4: Remove constraints and variables indexed by t &lt; t in MILP (5.1) and solve the resulting problem with horizon Tt , initial probability distributions p m (s|h m ) s∈X m S for every component m in [M ] and initial observation o (see Remark 4) to obtain an optimal solution (τ m , δ m ) m∈[M ]</figDesc><table><row><cell>Algorithm 2 History-dependent policy Act t T (h)</cell></row></table><note><p>b) and a history h = (o 1 , a 1 , . . . , o t -1 , a t -1 , o t ) available at time t . Conditionally to h, the vectors of state component S m t 1 t t for all m in [M ] become independent, i.e., P δ (S t = s|H t = h) = M m=1 p m (s m |h m ) P δ S m t = s m |H m t = h m . 1:</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_31"><head></head><label></label><figDesc>r), at each time t , the belief state (p(s|H t )) s∈X S is a sufficient statistic of the history of actions and observations H t [42, Theorem 4]. Given the action a t taken at time t an the observation o t +1 received at time t + 1, the belief state can be easily updated over time according to the belief state update [86, Eq. (1)]: p(s t +1 |H t +1 ) = p(s t +1 |H t , a t , o t +1 ) =</figDesc><table><row><cell>Hence, we obtain</cell><cell>τ t ,m a =</cell><cell>s∈X m S ,o∈X m O</cell><cell>τ t ,m soa =</cell><cell>S s∈X m</cell><cell>τ t ,m so m t a = δ t ,m a|o m t</cell></row><row><cell>It ensures that τ t ,m</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">p(o t +1 |s)p(s|s t , a t )</cell></row><row><cell></cell><cell></cell><cell cols="2">s∈X S</cell><cell></cell><cell></cell></row></table><note><p>s ∈X S p(o t +1 |s )p(s |s t , a t ) p(s|H t ) Proof of Theorem 5.7. It suffices to prove that at each time t ∈ [T ], for every observation h ∈ X t H the element Act t T (h) belongs to X A , i.e., M m=1 D m Act t,m T (h) b. Let (τ m , δ m ) m∈[M ] be a feasible solution of MILP (5.1) at step 4. Since O m t = o m t almost surely, τ t ,m soa is equal to 0 when o = o m t .</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_32"><head>Table 5 .</head><label>5</label><figDesc>Time(s) 2 -The values of G mean (g), G 95 (g), and G max (g) obtained on the small-scale and medium-scale instances with M ∈ {2, 3, 5}, n = 4 and solved with different finite horizon T ∈ {2, 5, 10}.</figDesc><table><row><cell>REG.SAR</cell><cell>2</cell><cell>g LB</cell><cell>9.91</cell><cell>15.13</cell><cell>15.62</cell><cell>0.03</cell><cell>15.72</cell><cell>21.51</cell><cell>22.87</cell><cell>0.08</cell><cell>17.42</cell><cell>25.08</cell><cell>26.53</cell><cell>0.21</cell></row><row><cell></cell><cell></cell><cell>g IP</cell><cell>9.91</cell><cell>15.13</cell><cell>15.62</cell><cell>0.14</cell><cell>15.72</cell><cell>21.51</cell><cell>22.87</cell><cell>0.64</cell><cell>17.42</cell><cell>25.08</cell><cell>26.53</cell><cell>2.57</cell></row><row><cell></cell><cell></cell><cell>g UB</cell><cell>7.54</cell><cell>10.54</cell><cell>10.70</cell><cell>0.16</cell><cell>11.18</cell><cell>17.89</cell><cell>19.40</cell><cell>0.36</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>g LR</cell><cell>7.02</cell><cell>10.21</cell><cell>10.33</cell><cell>9.55</cell><cell>10.86</cell><cell>16.97</cell><cell>18.73</cell><cell>14.08</cell><cell>13.00</cell><cell>18.41</cell><cell>18.64</cell><cell>15.52</cell></row><row><cell></cell><cell>5</cell><cell>g LB</cell><cell>6.06</cell><cell>9.23</cell><cell>9.57</cell><cell>0.39</cell><cell>10.34</cell><cell>13.01</cell><cell>13.26</cell><cell>1.32</cell><cell>14.51</cell><cell>18.45</cell><cell>18.48</cell><cell>3.43</cell></row><row><cell></cell><cell></cell><cell>g IP</cell><cell>6.06</cell><cell>9.23</cell><cell>9.57</cell><cell>17.14</cell><cell>10.34</cell><cell>13.01</cell><cell cols="2">13.26 1525.63</cell><cell>17.10</cell><cell>27.03</cell><cell cols="2">27.04 2907.11</cell></row><row><cell></cell><cell></cell><cell>g UB</cell><cell>4.59</cell><cell>6.07</cell><cell cols="2">6.19 1260.58</cell><cell>7.18</cell><cell>9.89</cell><cell cols="2">9.99 3247.98</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>g LR</cell><cell>4.04</cell><cell>5.89</cell><cell>5.95</cell><cell>43.00</cell><cell>6.79</cell><cell>9.36</cell><cell>9.42</cell><cell>54.66</cell><cell>10.46</cell><cell>13.15</cell><cell>13.61</cell><cell>51.17</cell></row><row><cell></cell><cell cols="2">10 g LB</cell><cell>3.36</cell><cell>5.45</cell><cell>5.85</cell><cell>2.86</cell><cell>6.27</cell><cell>8.91</cell><cell>9.15</cell><cell>39.76</cell><cell>8.04</cell><cell>11.14</cell><cell>11.55</cell><cell>349.00</cell></row><row><cell></cell><cell></cell><cell>g IP</cell><cell>3.38</cell><cell>5.55</cell><cell cols="2">6.04 1283.79</cell><cell>10.08</cell><cell>18.86</cell><cell cols="2">19.93 3205.30</cell><cell>15.53</cell><cell>23.63</cell><cell>24.08</cell><cell>&gt; 3600</cell></row><row><cell></cell><cell></cell><cell>g UB</cell><cell>5.08</cell><cell>9.41</cell><cell cols="2">9.51 3248.01</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>g LR</cell><cell>2.13</cell><cell>3.27</cell><cell>3.46</cell><cell>946.01</cell><cell>3.90</cell><cell>5.74</cell><cell cols="2">5.84 1536.79</cell><cell>5.98</cell><cell>7.52</cell><cell>7.67</cell><cell>661.04</cell></row><row><cell>RSTLS.SAR</cell><cell>2</cell><cell>g LB</cell><cell>12.73</cell><cell>16.84</cell><cell>17.04</cell><cell>0.03</cell><cell>17.96</cell><cell>22.67</cell><cell>22.77</cell><cell>0.07</cell><cell>15.13</cell><cell>16.83</cell><cell>17.03</cell><cell>0.17</cell></row><row><cell></cell><cell></cell><cell>g IP</cell><cell>12.73</cell><cell>16.84</cell><cell>17.04</cell><cell>0.14</cell><cell>17.96</cell><cell>22.67</cell><cell>22.77</cell><cell>0.62</cell><cell>15.13</cell><cell>16.83</cell><cell>17.03</cell><cell>2.28</cell></row><row><cell></cell><cell></cell><cell>g UB</cell><cell>8.33</cell><cell>13.09</cell><cell>15.56</cell><cell>0.07</cell><cell>12.01</cell><cell>18.18</cell><cell>18.93</cell><cell>0.06</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>g LR</cell><cell>8.16</cell><cell>12.71</cell><cell>14.99</cell><cell>9.22</cell><cell>11.99</cell><cell>18.15</cell><cell>18.91</cell><cell>14.44</cell><cell>9.94</cell><cell>11.20</cell><cell>11.28</cell><cell>15.79</cell></row><row><cell></cell><cell>5</cell><cell>g LB</cell><cell>10.77</cell><cell>13.61</cell><cell>14.22</cell><cell>0.44</cell><cell>14.54</cell><cell>18.64</cell><cell>18.86</cell><cell>1.40</cell><cell>13.85</cell><cell>16.46</cell><cell>17.00</cell><cell>3.91</cell></row><row><cell></cell><cell></cell><cell>g IP</cell><cell>10.77</cell><cell>13.61</cell><cell>14.22</cell><cell>12.50</cell><cell>14.54</cell><cell>18.64</cell><cell>18.86</cell><cell>358.36</cell><cell>18.31</cell><cell>21.51</cell><cell cols="2">21.66 3030.09</cell></row><row><cell></cell><cell></cell><cell>g UB</cell><cell>6.55</cell><cell>8.51</cell><cell>8.73</cell><cell>171.29</cell><cell>8.14</cell><cell>12.43</cell><cell cols="2">13.86 1716.52</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>g LR</cell><cell>6.32</cell><cell>8.16</cell><cell>8.35</cell><cell>41.99</cell><cell>7.92</cell><cell>12.01</cell><cell>13.67</cell><cell>36.89</cell><cell>7.92</cell><cell>9.86</cell><cell>10.19</cell><cell>59.57</cell></row><row><cell></cell><cell cols="2">10 g LB</cell><cell>10.39</cell><cell>13.73</cell><cell>14.32</cell><cell>3.20</cell><cell>13.18</cell><cell>16.36</cell><cell>16.83</cell><cell>29.58</cell><cell>14.55</cell><cell>17.85</cell><cell>18.18</cell><cell>312.76</cell></row><row><cell></cell><cell></cell><cell>g IP</cell><cell>10.86</cell><cell>15.35</cell><cell cols="2">17.28 1896.65</cell><cell>14.42</cell><cell>17.97</cell><cell>18.07</cell><cell>&gt; 3600</cell><cell>18.83</cell><cell>26.05</cell><cell>27.44</cell><cell>&gt; 3600</cell></row><row><cell></cell><cell></cell><cell>g UB</cell><cell>5.99</cell><cell>8.44</cell><cell>9.28</cell><cell>&gt; 3600</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>g LR</cell><cell>5.44</cell><cell>7.29</cell><cell>7.83</cell><cell>&gt; 3600</cell><cell>5.74</cell><cell>6.93</cell><cell cols="2">7.05 1669.80</cell><cell>7.34</cell><cell>9.42</cell><cell>9.68</cell><cell>&gt; 3600</cell></row><row><cell>RSTLS.SBR</cell><cell>2</cell><cell>g LB</cell><cell>6.14</cell><cell>8.70</cell><cell>9.06</cell><cell>0.02</cell><cell>8.50</cell><cell>11.51</cell><cell>11.67</cell><cell>0.03</cell><cell>9.07</cell><cell>10.98</cell><cell>11.54</cell><cell>0.07</cell></row><row><cell></cell><cell></cell><cell>g IP</cell><cell>6.14</cell><cell>8.70</cell><cell>9.06</cell><cell>0.04</cell><cell>8.50</cell><cell>11.51</cell><cell>11.67</cell><cell>0.14</cell><cell>9.07</cell><cell>10.98</cell><cell>11.54</cell><cell>0.72</cell></row><row><cell></cell><cell></cell><cell>g UB</cell><cell>3.54</cell><cell>5.63</cell><cell>6.00</cell><cell>0.02</cell><cell>5.39</cell><cell>7.60</cell><cell>7.75</cell><cell>0.02</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>g LR</cell><cell>3.47</cell><cell>5.49</cell><cell>5.75</cell><cell>6.72</cell><cell>5.32</cell><cell>7.56</cell><cell>7.71</cell><cell>7.74</cell><cell>6.39</cell><cell>7.63</cell><cell>7.99</cell><cell>7.90</cell></row><row><cell></cell><cell>5</cell><cell>g LB</cell><cell>4.65</cell><cell>7.58</cell><cell>7.84</cell><cell>0.18</cell><cell>6.77</cell><cell>9.13</cell><cell>9.24</cell><cell>0.66</cell><cell>8.61</cell><cell>10.38</cell><cell>10.51</cell><cell>1.72</cell></row><row><cell></cell><cell></cell><cell>g IP</cell><cell>4.65</cell><cell>7.58</cell><cell>7.84</cell><cell>4.27</cell><cell>6.77</cell><cell>9.13</cell><cell>9.24</cell><cell>311.29</cell><cell>10.84</cell><cell>14.81</cell><cell cols="2">16.37 3291.89</cell></row><row><cell></cell><cell></cell><cell>g UB</cell><cell>2.61</cell><cell>4.80</cell><cell>5.78</cell><cell>18.55</cell><cell>3.61</cell><cell>5.26</cell><cell>5.43</cell><cell>500.43</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>g LR</cell><cell>2.33</cell><cell>4.16</cell><cell>4.85</cell><cell>14.43</cell><cell>3.43</cell><cell>5.14</cell><cell>5.27</cell><cell>21.78</cell><cell>5.10</cell><cell>5.89</cell><cell>5.91</cell><cell>19.59</cell></row><row><cell></cell><cell cols="2">10 g LB</cell><cell>4.27</cell><cell>7.82</cell><cell>8.52</cell><cell>1.29</cell><cell>6.00</cell><cell>8.86</cell><cell>9.20</cell><cell>16.71</cell><cell>7.99</cell><cell>10.75</cell><cell>11.28</cell><cell>127.66</cell></row><row><cell></cell><cell></cell><cell>g IP</cell><cell>4.37</cell><cell>7.90</cell><cell>8.52</cell><cell>890.65</cell><cell>9.87</cell><cell>15.37</cell><cell cols="2">15.70 3265.16</cell><cell>15.84</cell><cell>17.97</cell><cell>18.01</cell><cell>&gt; 3600</cell></row><row><cell></cell><cell></cell><cell>g UB</cell><cell>2.31</cell><cell>4.96</cell><cell cols="2">6.48 1836.06</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>g LR</cell><cell>1.86</cell><cell>3.42</cell><cell cols="2">3.86 3168.63</cell><cell>2.77</cell><cell>4.69</cell><cell cols="2">4.76 3494.95</cell><cell>4.73</cell><cell>5.65</cell><cell cols="2">5.80 1443.62</cell></row><row><cell>RSTLS.DET.SBR</cell><cell>2</cell><cell>g LB</cell><cell>6.08</cell><cell>11.58</cell><cell>13.78</cell><cell>0.02</cell><cell>8.52</cell><cell>13.94</cell><cell>14.43</cell><cell>0.03</cell><cell>9.74</cell><cell>13.54</cell><cell>13.57</cell><cell>0.11</cell></row><row><cell></cell><cell></cell><cell>g IP</cell><cell>6.08</cell><cell>11.58</cell><cell>13.78</cell><cell>0.05</cell><cell>8.52</cell><cell>13.94</cell><cell>14.43</cell><cell>0.26</cell><cell>9.74</cell><cell>13.54</cell><cell>13.57</cell><cell>1.76</cell></row><row><cell></cell><cell></cell><cell>g UB</cell><cell>4.70</cell><cell>8.33</cell><cell>9.45</cell><cell>0.02</cell><cell>6.54</cell><cell>9.24</cell><cell>9.59</cell><cell>0.03</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>g LR</cell><cell>4.67</cell><cell>8.31</cell><cell>9.42</cell><cell>12.44</cell><cell>6.35</cell><cell>9.22</cell><cell>9.56</cell><cell>14.00</cell><cell>7.43</cell><cell>10.17</cell><cell>10.60</cell><cell>13.76</cell></row><row><cell></cell><cell>5</cell><cell>g LB</cell><cell>2.99</cell><cell>7.26</cell><cell>8.87</cell><cell>0.12</cell><cell>6.15</cell><cell>10.06</cell><cell>10.98</cell><cell>0.68</cell><cell>5.88</cell><cell>8.21</cell><cell>8.95</cell><cell>1.76</cell></row><row><cell></cell><cell></cell><cell>g IP</cell><cell>2.99</cell><cell>7.26</cell><cell>8.87</cell><cell>0.17</cell><cell>6.24</cell><cell>10.06</cell><cell>10.98</cell><cell>6.47</cell><cell>5.88</cell><cell>8.21</cell><cell>8.95</cell><cell>32.56</cell></row><row><cell></cell><cell></cell><cell>g UB</cell><cell>2.72</cell><cell>6.86</cell><cell>8.66</cell><cell>2.97</cell><cell>5.01</cell><cell>8.11</cell><cell>9.52</cell><cell>77.43</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>g LR</cell><cell>2.45</cell><cell>6.31</cell><cell>8.15</cell><cell>17.40</cell><cell>4.58</cell><cell>7.26</cell><cell>8.62</cell><cell>21.58</cell><cell>4.95</cell><cell>7.41</cell><cell>7.58</cell><cell>26.48</cell></row><row><cell></cell><cell cols="2">10 g LB</cell><cell>2.39</cell><cell>6.94</cell><cell>7.50</cell><cell>0.59</cell><cell>4.29</cell><cell>8.36</cell><cell>10.01</cell><cell>3.90</cell><cell>4.70</cell><cell>7.54</cell><cell>8.25</cell><cell>43.95</cell></row><row><cell></cell><cell></cell><cell>g IP</cell><cell>2.39</cell><cell>6.94</cell><cell>7.50</cell><cell>4.79</cell><cell>4.29</cell><cell>8.36</cell><cell>10.01</cell><cell>514.32</cell><cell>6.38</cell><cell>13.10</cell><cell cols="2">13.87 1148.11</cell></row><row><cell></cell><cell></cell><cell>g UB</cell><cell>2.23</cell><cell>7.04</cell><cell>7.60</cell><cell>816.32</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell></cell><cell></cell><cell>g LR</cell><cell>1.89</cell><cell>6.12</cell><cell>7.22</cell><cell>76.55</cell><cell>3.10</cell><cell>5.92</cell><cell>6.70</cell><cell>197.00</cell><cell>3.98</cell><cell>6.68</cell><cell>7.02</cell><cell>116.91</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_33"><head>24.14 10.43 0.591</head><label></label><figDesc></figDesc><table><row><cell>M</cell><cell>ω T r</cell><cell cols="2">|ν IP | (×10 3 ) (×10 3 ) Std. err.</cell><cell>f IP</cell><cell>Std. err.</cell><cell>G LR IP (%)</cell><cell>G R c IP (%)</cell><cell>Time (s)</cell></row><row><cell cols="2">3 0.2 2</cell><cell>5.71</cell><cell>2.32</cell><cell>4.6</cell><cell>2.2</cell><cell cols="3">11.60 14.09 0.004</cell></row><row><cell></cell><cell>5</cell><cell>5.45</cell><cell>2.16</cell><cell>4.0</cell><cell>2.1</cell><cell>6.51</cell><cell cols="2">8.88 0.055</cell></row><row><cell></cell><cell>0.4 2</cell><cell>5.71</cell><cell>2.32</cell><cell>4.6</cell><cell>2.2</cell><cell cols="3">11.60 14.09 0.005</cell></row><row><cell></cell><cell>5</cell><cell>5.45</cell><cell>2.16</cell><cell>4.0</cell><cell>2.1</cell><cell>6.51</cell><cell cols="2">8.88 0.062</cell></row><row><cell></cell><cell>0.6 2</cell><cell>5.71</cell><cell>2.32</cell><cell>4.6</cell><cell>2.2</cell><cell cols="3">11.60 14.09 0.006</cell></row><row><cell></cell><cell>5</cell><cell>5.45</cell><cell>2.16</cell><cell>4.0</cell><cell>2.1</cell><cell>6.51</cell><cell cols="2">8.88 0.064</cell></row><row><cell></cell><cell>0.8 2</cell><cell>5.17</cell><cell>1.90</cell><cell>4.0</cell><cell>1.8</cell><cell>0.95</cell><cell cols="2">3.21 0.005</cell></row><row><cell></cell><cell>5</cell><cell>5.11</cell><cell>1.86</cell><cell>3.5</cell><cell>1.8</cell><cell>-0.12</cell><cell cols="2">2.12 0.056</cell></row><row><cell cols="2">4 0.2 2</cell><cell>8.81</cell><cell>2.87</cell><cell>6.5</cell><cell>2.9</cell><cell cols="3">-1.77 13.79 0.007</cell></row><row><cell></cell><cell>5</cell><cell>8.64</cell><cell>2.92</cell><cell>6.3</cell><cell>2.9</cell><cell cols="3">-3.64 11.62 0.116</cell></row><row><cell></cell><cell>0.4 2</cell><cell>8.81</cell><cell>2.87</cell><cell>6.5</cell><cell>2.9</cell><cell cols="3">-1.77 13.79 0.007</cell></row><row><cell></cell><cell>5</cell><cell>8.64</cell><cell>2.92</cell><cell>6.3</cell><cell>2.9</cell><cell cols="3">-3.64 11.62 0.118</cell></row><row><cell></cell><cell>0.6 2</cell><cell>7.70</cell><cell>1.96</cell><cell>4.3</cell><cell>1.9</cell><cell>-0.94</cell><cell cols="2">0.46 0.006</cell></row><row><cell></cell><cell>5</cell><cell>7.62</cell><cell>1.89</cell><cell>4.0</cell><cell>1.9</cell><cell cols="3">-2.05 -0.66 0.067</cell></row><row><cell></cell><cell>0.8 2</cell><cell>7.70</cell><cell>1.91</cell><cell>4.1</cell><cell>1.8</cell><cell>-0.94</cell><cell cols="2">0.45 0.006</cell></row><row><cell></cell><cell>5</cell><cell>7.61</cell><cell>1.81</cell><cell>3.6</cell><cell>1.7</cell><cell cols="3">-2.16 -0.79 0.060</cell></row><row><cell cols="2">5 0.2 2</cell><cell>13.78</cell><cell cols="2">4.77 11.5</cell><cell cols="4">4.8 -41.47 30.24 0.008</cell></row><row><cell></cell><cell>5</cell><cell>13.38</cell><cell cols="2">4.58 11.1</cell><cell cols="4">4.6 -43.18 26.44 0.250</cell></row><row><cell></cell><cell>0.4 2</cell><cell>10.43</cell><cell>2.50</cell><cell>6.4</cell><cell>2.5</cell><cell>0.76</cell><cell cols="2">2.13 0.007</cell></row><row><cell></cell><cell>5</cell><cell>10.27</cell><cell>2.39</cell><cell>5.7</cell><cell>2.4</cell><cell>-0.77</cell><cell cols="2">0.58 0.124</cell></row><row><cell></cell><cell>0.6 2</cell><cell>10.26</cell><cell>2.31</cell><cell>5.7</cell><cell>2.2</cell><cell>0.00</cell><cell cols="2">0.80 0.006</cell></row><row><cell></cell><cell>5</cell><cell>10.22</cell><cell>2.26</cell><cell>5.2</cell><cell>2.2</cell><cell>-0.44</cell><cell cols="2">0.35 0.071</cell></row><row><cell></cell><cell>0.8 2</cell><cell>10.24</cell><cell>2.28</cell><cell>5.6</cell><cell>2.2</cell><cell>-0.27</cell><cell cols="2">0.58 0.006</cell></row><row><cell></cell><cell>5</cell><cell>10.20</cell><cell>2.22</cell><cell>5.1</cell><cell>2.2</cell><cell>-0.65</cell><cell cols="2">0.20 0.070</cell></row><row><cell></cell><cell>0.2 2</cell><cell>22.63</cell><cell cols="2">5.45 18.0</cell><cell cols="4">5.5 -34.20 17.71 0.012</cell></row><row><cell></cell><cell>5</cell><cell>22.06</cell><cell cols="2">5.24 17.5</cell><cell cols="4">5.2 -35.85 14.74 0.384</cell></row><row><cell></cell><cell>0.4 2</cell><cell>19.19</cell><cell cols="2">3.38 11.5</cell><cell>3.3</cell><cell>1.32</cell><cell cols="2">3.07 0.012</cell></row><row><cell></cell><cell>5</cell><cell>18.91</cell><cell cols="2">3.26 10.6</cell><cell>3.2</cell><cell>-0.16</cell><cell cols="2">1.57 0.196</cell></row><row><cell></cell><cell>0.6 2</cell><cell>19.10</cell><cell cols="2">3.28 10.8</cell><cell>3.1</cell><cell>0.92</cell><cell cols="2">2.60 0.011</cell></row><row><cell></cell><cell>5</cell><cell>18.81</cell><cell>3.06</cell><cell>9.7</cell><cell>2.9</cell><cell>-0.62</cell><cell cols="2">1.03 0.138</cell></row><row><cell></cell><cell>0.8 2</cell><cell>19.09</cell><cell cols="2">3.27 10.8</cell><cell>3.1</cell><cell>0.86</cell><cell cols="2">2.53 0.011</cell></row><row><cell></cell><cell>5</cell><cell>18.82</cell><cell>3.08</cell><cell>9.6</cell><cell>2.9</cell><cell>-0.56</cell><cell cols="2">1.09 0.137</cell></row><row><cell></cell><cell>0.2 2</cell><cell>31.54</cell><cell cols="2">6.03 25.0</cell><cell cols="4">6.0 -22.56 12.73 0.017</cell></row><row><cell></cell><cell cols="5">5 5.9 -0.4 2 30.89 5.88 24.0 28.18 4.22 19.1 4.1</cell><cell>0.45</cell><cell cols="2">1.93 0.016</cell></row><row><cell></cell><cell>5</cell><cell>27.67</cell><cell cols="2">3.97 16.8</cell><cell>3.9</cell><cell>-1.39</cell><cell cols="2">0.06 0.232</cell></row><row><cell></cell><cell>0.6 2</cell><cell>28.12</cell><cell cols="2">4.16 18.8</cell><cell>4.0</cell><cell>0.10</cell><cell cols="2">1.70 0.016</cell></row><row><cell></cell><cell>5</cell><cell>27.67</cell><cell cols="2">3.84 16.3</cell><cell>3.7</cell><cell>-1.51</cell><cell cols="2">0.05 0.225</cell></row><row><cell></cell><cell>0.8 2</cell><cell>28.12</cell><cell cols="2">4.16 18.8</cell><cell>4.0</cell><cell>0.11</cell><cell cols="2">1.71 0.015</cell></row><row><cell></cell><cell>5</cell><cell>27.65</cell><cell cols="2">3.86 16.2</cell><cell>3.7</cell><cell cols="3">-1.57 -0.01 0.226</cell></row><row><cell></cell><cell>0.2 2</cell><cell>45.06</cell><cell cols="2">7.01 35.9</cell><cell cols="2">7.1 -20.37</cell><cell cols="2">8.67 0.022</cell></row><row><cell></cell><cell>5</cell><cell>44.28</cell><cell cols="2">6.90 35.1</cell><cell cols="2">6.9 -21.74</cell><cell cols="2">6.80 0.660</cell></row><row><cell></cell><cell>0.4 2</cell><cell>41.18</cell><cell cols="2">4.72 23.0</cell><cell>4.7</cell><cell>0.35</cell><cell cols="2">1.50 0.020</cell></row><row><cell></cell><cell>5</cell><cell>40.83</cell><cell cols="2">4.66 22.7</cell><cell>4.7</cell><cell>-0.49</cell><cell cols="2">0.66 0.469</cell></row><row><cell></cell><cell>0.6 2</cell><cell>40.96</cell><cell cols="2">4.25 18.4</cell><cell>4.1</cell><cell>0.32</cell><cell cols="2">1.22 0.020</cell></row><row><cell></cell><cell>5</cell><cell>40.72</cell><cell cols="2">4.19 17.9</cell><cell>4.0</cell><cell>-0.28</cell><cell cols="2">0.62 0.316</cell></row><row><cell></cell><cell>0.8 2</cell><cell>40.96</cell><cell cols="2">4.24 18.3</cell><cell>4.0</cell><cell>0.29</cell><cell cols="2">1.21 0.019</cell></row><row><cell></cell><cell>5</cell><cell>40.76</cell><cell cols="2">4.09 17.8</cell><cell>3.9</cell><cell>-0.19</cell><cell cols="2">0.73 0.313</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_34"><head>Table 5 .</head><label>5</label><figDesc>3 -Performances of the matheuritic on different rolling horizon T r ∈ {2, 5}: Numerical values of |ν IP |, f IP (and their corresponding standard errors), G LR IP and G R c IP obtained on an instance (M , ω) with M ∈ {3, 4, 5, 10, 15, 20} and ω ∈ {0.2, 0.4, 0.6, 0.8}. The values written in bold indicate the best performances of policy δ IP regarding optimality and scalability (computation time).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_36"><head></head><label></label><figDesc>and t 1 , . . . , t k the vertices with diverging arcs in Q. Note that, since the trail is included in V v , the first vertices of the trail have to be immediate descendants of v in G so that the trail takes the form v s 0 t 1 s 1 . . . t k s k w, where possibly s k = w and the last arc is not present. Then, given that v ≺ s 0 , and that is topological for G, Proposition 7.4.2 implies that C v C s 0 . But by the same argument, Property 2 in Proposition 7.4 implies C t 1 C s 0 , but since G is a tree and v ≺ t 1 , we</figDesc><table><row><cell>must have C v</cell><cell>C t 1</cell><cell>C s 1 . By induction on i , we have C v</cell><cell>C s i and thus C v</cell><cell>C w , which</cell></row><row><cell cols="2">shows Equation (7.7a).</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_39"><head></head><label></label><figDesc>• Section 8.3 details basic knowledge about the McCormick's relaxation. It introduces a dynamic programming like algorithm that computes "good" bounds on the moments of any probabilistic distribution of a PID. Incorporating these new bounds in the McCormick linear inequalities tighten the linear relaxation of MILP (6.6).</figDesc><table><row><cell>• Section 8.4 provides an interpretation of the linear relaxation of MILP (6.6) and the valid</cell></row><row><cell>inequalities (6.7) in terms of graphs.</cell></row><row><cell>• Section 8.5 proves that NLP (6.8) that models MEU(G, ρ). Such NLP is based on the value</cell></row><row><cell>functions. Using McCormick's linearization technique, this NLP can be turned into an</cell></row><row><cell>MILP.</cell></row><row><cell>• Section 8.6 illustrates the mathematical programs of this chapter and their properties on</cell></row><row><cell>some simple numerical examples.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_40"><head></head><label></label><figDesc>3) to(8.2). With this integrality constraint, Equation (8.2d) becomes a logical constraint, i.e., a constraint of the form λy = z with λ binary and continuous y and z. Note that constraints can be handled by modern MILP solvers such as CPLEX or Gurobi, that can solve NLP (8.2) with integrality constraints (6.3). Čv (x Čv ) x Čv ∈X Čv ,v∈V a be a vector of upper bounds satisfying</figDesc><table><row><cell>Alternatively, by a classical result in integer programming, we can turn NLP (8.2) into an equiv-</cell></row><row><cell>alent MILP by replacing constraint (8.2d) by its McCormick relaxation [100]. For a given p, let</cell></row><row><cell>b = b</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_41"><head>. 6 )</head><label>6</label><figDesc>Remark 11. The strength of the McCormick constraints (McCormick(v, b)) depends on the quality of the bounds b Čv on µ Čv . As for a solution µ of MILP (8.6), µ Čv corresponds to a probability distribution, the simplest admissible bound over µ Čv is just b = 1, leading to the polytope Q 1 . Unfortunately, McCormick's constraints are loose in this case: we show in Section 8.3 that, for any µ in P, there exists δ in ∆ such that (µ, δ) satisfies those McCormick constraints. Section 8.3.2 gives an example showing that McCormick constraints do retain information about the conditional independence if bounds b Čv smaller than 1 are used. Finally, Section 8.3.3 provides a dynamic programming algorithm that efficiently computes such a b.</figDesc><table /><note><p>Hence, when b = 1, McCormick constraints fail to retain any information about the conditional independence statements encoded in the associated nonlinear constraints. Since δ does not appear outside of the McCormick constraints, their sole interest in that case is to enable the branching decisions on δ to have an impact on µ.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_43"><head>Table 8 .</head><label>8</label><figDesc>1 -Mean results on 20 randomly generated instances with a time limit of 3600s.</figDesc><table><row><cell cols="2">(k s , k a , T )</cell><cell>|∆ d |</cell><cell cols="3">Polytope</cell><cell>g i (%)</cell><cell cols="4">Example 6: Chess game g f i SPU Opt (%) (%) (%)</cell><cell>Time (s)</cell><cell>g i (%)</cell><cell>Example 4: POMDP g f Opt i SPU (%) (%) (%)</cell><cell>Time (s)</cell></row><row><cell cols="2">(3, 5, 20)</cell><cell>10 69</cell><cell cols="3">Q vf</cell><cell cols="2">4.81 4.80</cell><cell>0.07</cell><cell cols="2">5</cell><cell>3421</cell><cell>9.25</cell><cell>8.61</cell><cell>0.66</cell><cell>3061</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Q vf,⊥⊥</cell><cell cols="2">1.00 0.91</cell><cell>0.07</cell><cell cols="2">20</cell><cell>2895</cell><cell>1.88</cell><cell>1.77</cell><cell>0.56</cell><cell>2558</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Q</cell><cell>1</cell><cell cols="2">5.02 0.40</cell><cell>0.07</cell><cell cols="2">65</cell><cell>1353</cell><cell>8.33</cell><cell>4.31</cell><cell>0.69</cell><cell>2852</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Q</cell><cell cols="2">b</cell><cell cols="2">4.60 0.42</cell><cell>0.07</cell><cell cols="2">65</cell><cell>1415</cell><cell>7.82</cell><cell>4.03</cell><cell>0.64</cell><cell>2788</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Q ⊥ ⊥,1</cell><cell cols="2">1.05 0.21</cell><cell>0.07</cell><cell cols="2">70</cell><cell>1109</cell><cell>2.02</cell><cell>1.08</cell><cell>0.67</cell><cell>1679</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Q ⊥ ⊥,b</cell><cell cols="2">0.97 0.19</cell><cell>0.07</cell><cell cols="2">70</cell><cell>1110</cell><cell>1.68</cell><cell>1.09</cell><cell>0.67</cell><cell>1568</cell></row><row><cell cols="2">(3, 6, 20)</cell><cell>10 93</cell><cell cols="3">Q vf</cell><cell cols="2">4.36 4.32</cell><cell>0.04</cell><cell cols="3">0 &gt;3600 10.64 10.06</cell><cell>2.22</cell><cell>3420</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Q vf,⊥⊥</cell><cell cols="2">0.72 0.65</cell><cell>0.04</cell><cell cols="2">15</cell><cell>3094</cell><cell>2.21</cell><cell>2.15</cell><cell>2.11</cell><cell>3422</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Q</cell><cell>1</cell><cell cols="2">4.55 0.42</cell><cell>0.04</cell><cell cols="2">25</cell><cell>2731</cell><cell>9.68</cell><cell>6.13</cell><cell>2.30</cell><cell>3420</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Q</cell><cell cols="2">b</cell><cell cols="2">4.21 0.38</cell><cell>0.02</cell><cell cols="2">35</cell><cell>2508</cell><cell>9.13</cell><cell>5.80</cell><cell>2.32</cell><cell>3420</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Q ⊥ ⊥,1</cell><cell cols="2">0.75 0.22</cell><cell>0.04</cell><cell cols="2">50</cell><cell>1886</cell><cell>2.35</cell><cell>1.48</cell><cell>2.32</cell><cell>3005</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Q ⊥ ⊥,b</cell><cell cols="2">0.70 0.22</cell><cell>0.02</cell><cell cols="2">50</cell><cell>2122</cell><cell>1.93</cell><cell>1.45</cell><cell>2.32</cell><cell>2792</cell></row><row><cell cols="2">(3, 9, 20)</cell><cell>10 171</cell><cell cols="3">Q vf</cell><cell cols="2">7.20 7.20</cell><cell>0.11</cell><cell cols="3">0 &gt;3600</cell><cell>8.85</cell><cell>8.36</cell><cell>1.96</cell><cell>&gt;3600</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Q vf,⊥⊥</cell><cell cols="2">1.92 1.92</cell><cell>0.12</cell><cell cols="2">10</cell><cell>3249</cell><cell>3.55</cell><cell>3.52</cell><cell>1.96</cell><cell>3426</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Q</cell><cell>1</cell><cell cols="2">7.52 2.29</cell><cell>0.12</cell><cell cols="2">5</cell><cell>3441</cell><cell>8.26</cell><cell>6.19</cell><cell>2.00</cell><cell>&gt;3600</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Q</cell><cell cols="2">b</cell><cell cols="2">6.91 2.41</cell><cell>0.11</cell><cell cols="2">5</cell><cell>3470</cell><cell>7.80</cell><cell>5.87</cell><cell>2.03</cell><cell>&gt;3600</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Q ⊥ ⊥,1</cell><cell cols="2">1.99 1.13</cell><cell>0.12</cell><cell cols="2">15</cell><cell>3061</cell><cell>2.28</cell><cell>1.75</cell><cell>1.96</cell><cell>3258</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Q ⊥ ⊥,b</cell><cell cols="2">1.89 1.13</cell><cell>0.11</cell><cell cols="2">15</cell><cell>3061</cell><cell>1.93</cell><cell>1.64</cell><cell>1.95</cell><cell>3161</cell></row><row><cell cols="2">(3, 10, 20)</cell><cell>10 200</cell><cell cols="3">Q vf</cell><cell cols="2">5.93 5.93</cell><cell>0.02</cell><cell cols="3">0 &gt;3600 12.75 12.20</cell><cell>1.89</cell><cell>&gt;3600</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Q vf,⊥⊥</cell><cell cols="2">1.23 1.21</cell><cell>0.02</cell><cell cols="2">10</cell><cell>3276</cell><cell>4.56</cell><cell>4.55</cell><cell>1.89</cell><cell>3440</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Q</cell><cell>1</cell><cell cols="2">6.21 1.83</cell><cell>0.02</cell><cell cols="2">5</cell><cell>3449 11.56</cell><cell>9.26</cell><cell>1.71</cell><cell>&gt;3600</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Q</cell><cell cols="2">b</cell><cell cols="2">5.78 1.73</cell><cell>0.02</cell><cell cols="2">15</cell><cell>3151 19.95</cell><cell>9.14</cell><cell>1.72</cell><cell>&gt;3600</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Q ⊥ ⊥,1</cell><cell cols="2">1.27 0.79</cell><cell>0.02</cell><cell cols="2">15</cell><cell>2882</cell><cell>3.48</cell><cell>2.91</cell><cell>1.64</cell><cell>3243</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Q ⊥ ⊥,b</cell><cell cols="2">1.21 0.78</cell><cell>0.02</cell><cell cols="2">20</cell><cell>2882</cell><cell>2.97</cell><cell>2.88</cell><cell>1.68</cell><cell>3243</cell></row><row><cell cols="2">(4, 9, 20)</cell><cell>10 171</cell><cell cols="3">Q vf</cell><cell cols="2">7.12 7.12</cell><cell>0.04</cell><cell cols="3">0 &gt;3600 11.18 10.72</cell><cell>0.81</cell><cell>&gt;3600</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Q vf,⊥⊥</cell><cell cols="2">1.67 1.66</cell><cell>0.04</cell><cell cols="2">10</cell><cell>3289</cell><cell>3.31</cell><cell>3.31</cell><cell>0.81</cell><cell>&gt;3600</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Q</cell><cell>1</cell><cell cols="2">7.44 4.04</cell><cell>0.04</cell><cell cols="3">0 &gt;3600 10.27</cell><cell>7.95</cell><cell>0.98</cell><cell>&gt;3600</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Q</cell><cell cols="2">b</cell><cell cols="2">6.99 4.15</cell><cell>0.04</cell><cell cols="3">0 &gt;3600</cell><cell>9.57</cell><cell>7.78</cell><cell>1.04</cell><cell>&gt;3600</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Q ⊥ ⊥,1</cell><cell cols="2">1.74 1.15</cell><cell>0.04</cell><cell cols="2">10</cell><cell>3240</cell><cell>2.85</cell><cell>2.18</cell><cell>0.94</cell><cell>3274</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Q ⊥ ⊥,b</cell><cell cols="2">1.64 1.15</cell><cell>0.04</cell><cell cols="2">10</cell><cell>3240</cell><cell>2.27</cell><cell>2.14</cell><cell>0.97</cell><cell>3263</cell></row><row><cell cols="2">(4, 10, 20)</cell><cell>10 200</cell><cell cols="3">Q vf</cell><cell cols="2">7.50 7.50</cell><cell>0.08</cell><cell cols="3">0 &gt;3600 14.78 14.33</cell><cell>0.55</cell><cell>&gt;3600</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Q vf,⊥⊥</cell><cell cols="2">1.76 1.76</cell><cell>0.08</cell><cell cols="3">0 &gt;3600</cell><cell>4.16</cell><cell>4.16</cell><cell>0.55</cell><cell>&gt;3600</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="2">Q</cell><cell>1</cell><cell cols="2">8.07 4.20</cell><cell>0.08</cell><cell cols="3">0 &gt;3600 12.80 10.52</cell><cell>0.81</cell><cell>&gt;3600</cell></row><row><cell></cell><cell></cell><cell></cell><cell>Q</cell><cell cols="2">b</cell><cell cols="2">7.62 4.65</cell><cell>0.04</cell><cell cols="3">0 &gt;3600 12.05 10.41</cell><cell>0.86</cell><cell>&gt;3600</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Q ⊥ ⊥,1</cell><cell cols="2">1.88 1.31</cell><cell>0.07</cell><cell cols="2">5</cell><cell>3411</cell><cell>4.07</cell><cell>3.51</cell><cell>0.41</cell><cell>&gt;3600</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">Q ⊥ ⊥,b</cell><cell cols="2">1.76 1.29</cell><cell>0.08</cell><cell cols="2">5</cell><cell>3410</cell><cell>3.44</cell><cell>3.38</cell><cell>0.51</cell><cell>&gt;3600</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">s 1 a 1 s 2 -r 1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>s 1 a 1 s 2 -r 1</cell></row><row><cell>-s 1</cell><cell>s 1 -o 1</cell><cell cols="2">s 1 o 1 -a 1</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">-s 1</cell><cell>s 1 -o 1</cell><cell>s 1 o 1 -a 1</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="3">s 1 a 1 -s 2</cell><cell></cell><cell>s 2 -o 2</cell><cell cols="2">s 2 o 2 -a 2</cell><cell></cell><cell>s 1 a 1 -s 2</cell><cell>s 1 a 1 s 2 -o 2</cell><cell>s 1 a 1 s 2 o 2 -a 2</cell></row><row><cell cols="10">Figure 8.6 -Rooted Junction Tree built by Algo-</cell><cell cols="2">Figure 8.7 -A bigger Rooted Junction Tree for a</cell></row><row><cell cols="9">rithm (4) for a POMDP with limited memory.</cell><cell></cell><cell cols="2">POMDP with limited memory.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_44"><head></head><label></label><figDesc>4 -Influence diagram of Example 7 denoted by S t . Based on a state S t = s, each player m in [M ] plays simultaneously an individual decision (or action) A m t = a m and the game's state transits randomly to state S t +1 = s according to transition probability p(s |s, a) where a = (a m ) m∈[M ]</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_46"><head></head><label></label><figDesc>We then randomly generate the initial probability distributions p(s) s∈X S , the transition probability distributions p(s |s, a 1 , . . . , a M )</figDesc><table><row><cell></cell><cell></cell><cell>s∈X S</cell><cell>, and the immediate reward functions</cell></row><row><cell>r (s, a 1 , . . . , a M , s )</cell><cell>s∈X S</cell><cell cols="2">a 1 ∈X 1 A ,...,a M ∈X M A . An instance is the tuple (M , k s , k a , p,r). We choose (M , k s , k a ) ∈</cell></row><row><cell cols="4">a 1 ∈X 1 A ,...,a M ∈X M A {3, 5} 3 and we generate 50 instances (M , k s , k a , p,r).</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_47"><head>Table 9 .</head><label>9</label><figDesc>(M , k s , k a ) T</figDesc><table><row><cell>(3,3,3)</cell><cell>∆ d ml 10 10 42</cell><cell>Alg. MILP (8.9)</cell><cell cols="2">Gap (%) Time (s) Opt. 0.05</cell></row><row><cell></cell><cell></cell><cell>SPU</cell><cell>-5.08</cell><cell>7.96</cell></row><row><cell></cell><cell></cell><cell>Lin. Relax. MILP (8.9)</cell><cell>0.00</cell><cell>0.05</cell></row><row><cell></cell><cell>20 10 84</cell><cell>MILP (8.9)</cell><cell>Opt.</cell><cell>0.12</cell></row><row><cell></cell><cell></cell><cell>SPU</cell><cell>-5.09</cell><cell>15.92</cell></row><row><cell></cell><cell></cell><cell>Lin. Relax. MILP (8.9)</cell><cell>0.00</cell><cell>0.10</cell></row><row><cell>(3,5,5)</cell><cell>10 10 84</cell><cell>MILP (8.9)</cell><cell>Opt.</cell><cell>0.24</cell></row><row><cell></cell><cell></cell><cell>SPU</cell><cell>-7.06</cell><cell>8.42</cell></row><row><cell></cell><cell></cell><cell>Lin. Relax. MILP (8.9)</cell><cell>0.00</cell><cell>0.28</cell></row><row><cell></cell><cell>20 10 168</cell><cell>MILP (8.9)</cell><cell>Opt.</cell><cell>0.62</cell></row><row><cell></cell><cell></cell><cell>SPU</cell><cell>-7.06</cell><cell>16.84</cell></row><row><cell></cell><cell></cell><cell>Lin. Relax. MILP (8.9)</cell><cell>0.00</cell><cell>0.57</cell></row><row><cell>(5,3,3)</cell><cell>10 10 71</cell><cell>MILP (8.9)</cell><cell>Opt.</cell><cell>2.25</cell></row><row><cell></cell><cell></cell><cell>SPU</cell><cell>-11.38</cell><cell>7.59</cell></row><row><cell></cell><cell></cell><cell>Lin. Relax. MILP (8.9)</cell><cell>0.00</cell><cell>1.07</cell></row><row><cell></cell><cell>20 10 142</cell><cell>MILP (8.9)</cell><cell>Opt.</cell><cell>7.52</cell></row><row><cell></cell><cell></cell><cell>SPU</cell><cell>-11.37</cell><cell>15.17</cell></row><row><cell></cell><cell></cell><cell>Lin. Relax. MILP (8.9)</cell><cell>0.00</cell><cell>2.92</cell></row><row><cell>(5,5,5)</cell><cell>10 10 174</cell><cell>MILP (8.9)</cell><cell>Opt.</cell><cell>29.67</cell></row><row><cell></cell><cell></cell><cell>SPU</cell><cell>-13.71</cell><cell>7.85</cell></row><row><cell></cell><cell></cell><cell>Lin. Relax. MILP (8.9)</cell><cell>0.00</cell><cell>16.89</cell></row><row><cell></cell><cell>20 10 348</cell><cell>MILP (8.9)</cell><cell cols="2">Opt. 1374.61</cell></row><row><cell></cell><cell></cell><cell>SPU</cell><cell>-13.75</cell><cell>15.68</cell></row><row><cell></cell><cell></cell><cell>Lin. Relax. MILP (8.9)</cell><cell cols="2">0.00 1311.62</cell></row></table><note><p>1 -Average results on 50 instances of the cooperative game.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_48"><head>10.4. Numerical results</head><label></label><figDesc></figDesc><table><row><cell cols="2">M K</cell><cell>Policy</cell><cell cols="3">Time (s) Obj (%) Fail.</cell></row><row><cell>3</cell><cell>1</cell><cell>Airline</cell><cell>-</cell><cell cols="2">-10.0</cell></row><row><cell></cell><cell></cell><cell>Alg. 3, T r = 1</cell><cell>0.001</cell><cell>54.2</cell><cell>2.3</cell></row><row><cell></cell><cell></cell><cell>Alg. 3, T r = 2</cell><cell>0.010</cell><cell>54.8</cell><cell>1.6</cell></row><row><cell></cell><cell></cell><cell>Alg. 3, T r = 5</cell><cell>0.100</cell><cell>57.9</cell><cell>1.0</cell></row><row><cell>5</cell><cell>2</cell><cell>Airline</cell><cell>-</cell><cell cols="2">-19.6</cell></row><row><cell></cell><cell></cell><cell>Alg. 3, T r = 1</cell><cell>0.003</cell><cell>55.7</cell><cell>4.0</cell></row><row><cell></cell><cell></cell><cell>Alg. 3, T r = 2</cell><cell>0.020</cell><cell>52.8</cell><cell>2.5</cell></row><row><cell></cell><cell></cell><cell>Alg. 3, T r = 5</cell><cell>0.290</cell><cell>57.3</cell><cell>1.6</cell></row><row><cell cols="2">10 3</cell><cell>Airline</cell><cell>-</cell><cell cols="2">-35.2</cell></row><row><cell></cell><cell></cell><cell>Alg. 3, T r = 1</cell><cell>0.004</cell><cell>53.5</cell><cell>7.9</cell></row><row><cell></cell><cell></cell><cell>Alg. 3, T r = 2</cell><cell>0.030</cell><cell>50.6</cell><cell>5.4</cell></row><row><cell></cell><cell></cell><cell>Alg. 3, T r = 5</cell><cell>1.200</cell><cell>53.3</cell><cell>4.3</cell></row><row><cell cols="2">15 5</cell><cell>Airline</cell><cell>-</cell><cell cols="2">-49.7</cell></row><row><cell></cell><cell></cell><cell>Alg. 3, T r = 1</cell><cell>0.008</cell><cell cols="2">49.2 12.6</cell></row><row><cell></cell><cell></cell><cell>Alg. 3, T r = 2</cell><cell>0.050</cell><cell>42.8</cell><cell>8.7</cell></row><row><cell></cell><cell></cell><cell>Alg. 3, T r = 5</cell><cell>2.000</cell><cell>47.5</cell><cell>6.1</cell></row><row><cell cols="2">20 7</cell><cell>Airline</cell><cell>-</cell><cell cols="2">-69.4</cell></row><row><cell></cell><cell></cell><cell>Alg. 3, T r = 1</cell><cell>0.006</cell><cell cols="2">55.1 14.4</cell></row><row><cell></cell><cell></cell><cell>Alg. 3, T r = 2</cell><cell>0.040</cell><cell>47.2</cell><cell>9.3</cell></row><row><cell></cell><cell></cell><cell>Alg. 3, T r = 5</cell><cell>4.700</cell><cell>49.7</cell><cell>5.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_49"><head>Table 10 .</head><label>10</label><figDesc>1 -Numerical results on the simulated system averaged over the 1000 policy evaluations. The figures in bold indicate the best performances.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_50"><head></head><label></label><figDesc>Tsui et al. [153, Sec 2.1], Jouin et al. [63, Sec 4.1.2] or Gouriveau and Zerhouni [51, Chapter 3]. In order to keep the most relevant features, Javed et al.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_51"><head></head><label></label><figDesc>wc ml with T = 4 using MILP (4.7) on X S , X O and X A , we obtain an optimal value of v * ml = 44.7122, while the optimal value of our MILP (5.1) is z IP = 44.2834. Hence, we obtain z IP &lt; v * ml . Therefore, v * ml z IP does not hold in general. The inequality z IP v * ml does not hold in generalConsider a weakly coupled POMDP with M = 2, K = 1, X 1 S = X 2 S = {1, 2, 3}, andX 1 O = X 2</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>O = {1, 2}.</cell></row><row><cell cols="7">We set the following initial probability data,</cell></row><row><cell cols="7">p 1 (•) = 0.4311 0.5255 0.0434 ,</cell><cell>p 2 (•) = 0.4835 0.1745 0.3421 ,</cell></row><row><cell cols="5">the following transition probability data,</cell><cell></cell></row><row><cell></cell><cell cols="5"> 0.1517 0.3481 0.5002</cell><cell></cell><cell> 0.3435 0.3291 0.3274</cell><cell></cell></row><row><cell>p 1 (•|•, 0) =</cell><cell cols="5">  0.1639 0.0922 0.7439</cell><cell> ,</cell><cell>p 2 (•|•, 0) =</cell><cell>  0.5964 0.1653 0.2383</cell><cell> ,</cell></row><row><cell></cell><cell cols="5">0.3395 0.2385 0.4220</cell><cell>0.3968 0.2626 0.3406</cell></row><row><cell></cell><cell cols="5"> 0.3467 0.2733 0.3800</cell><cell></cell><cell> 0.3160 0.4210 0.2630</cell><cell></cell></row><row><cell>p 1 (•|•, 1) =</cell><cell cols="5">  0.5027 0.3548 0.1425</cell><cell> ,</cell><cell>p 2 (•|•, 1) =</cell><cell>  0.3583 0.3882 0.2535</cell><cell> ,</cell></row><row><cell></cell><cell cols="5">0.2530 0.5466 0.2003</cell><cell>0.3611 0.4308 0.2081</cell></row><row><cell cols="5">the following emission probability data,</cell><cell></cell></row><row><cell></cell><cell></cell><cell cols="2"> 0.2052 0.7948</cell><cell></cell><cell></cell><cell> 0.6273 0.3727</cell><cell></cell></row><row><cell cols="2">p 1 (•|•) =</cell><cell cols="2">  0.8296 0.1704</cell><cell> ,</cell><cell></cell><cell>p 2 (•|•) =</cell><cell>  0.0392 0.9608</cell><cell> ,</cell></row><row><cell></cell><cell></cell><cell></cell><cell>0.5330 0.4670</cell><cell></cell><cell></cell><cell>0.4024 0.5976</cell></row><row><cell cols="4">and the following reward data</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="4"> 7.0075 6.2135 8.4122</cell><cell cols="2"></cell><cell> 8.7418 2.6682 2.5227</cell><cell></cell></row><row><cell>r 1 (•, 0, •) =</cell><cell cols="4">  9.7198 9.5152 2.6182</cell><cell cols="2"> ,</cell><cell>r 2 (•, 0, •) =</cell><cell>  8.7673 6.1198 6.4814</cell><cell> ,</cell></row><row><cell></cell><cell cols="4">1.8522 7.4390 4.9132</cell><cell></cell><cell>6.4971 3.8810 0.3476</cell></row><row><cell></cell><cell cols="4"> 2.8154 7.0215 1.6752</cell><cell cols="2"></cell><cell> 7.4528 8.5013 9.1925</cell><cell></cell></row><row><cell>r 1 (•, 1, •) =</cell><cell cols="4">  7.8149 0.7849 4.3722</cell><cell cols="2"> ,</cell><cell>r 2 (•, 1, •) =</cell><cell>  4.3003 2.0946 4.2973</cell><cell> .</cell></row><row><cell></cell><cell cols="4">5.9378 9.1273 1.1657</cell><cell></cell><cell>4.2865 0.8470 9.5848</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">3.3101 7.8198 6.9773</cell><cell></cell><cell> 2.9600 8.1503 4.5911</cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">2.0722 2.6782 3.5715</cell><cell> ,</cell><cell>r 2 (•, 0, •) =</cell><cell>  2.2638 6.0290 2.5511</cell><cell> ,</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">8.4428 2.6010 3.2765</cell><cell>8.0789 7.9927 5.0259</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4"> 1.9315 9.3614 2.8927</cell><cell></cell><cell> 6.2647 6.6832 1.1263</cell><cell></cell></row><row><cell cols="3">r 1 (•, 1, •) =</cell><cell cols="4">  4.8769 5.3131 7.3626</cell><cell> ,</cell><cell>r 2 (•, 1, •) =</cell><cell>  9.9182 9.0278 5.9492</cell><cell> .</cell></row><row><cell></cell><cell></cell><cell></cell><cell cols="4">3.7944 4.5557 8.6462</cell><cell>9.8333 0.4466 4.3798</cell></row></table><note><p>ml Solving P A.2</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Markov Decision Process (MDP) et Partially Observable Markov Decision Process (POMDP) en anglais v</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_1"><p>A critical equipment fails when it reaches a conservative threshold set by regulation agencies, and beyond which the airplanes are not allowed to take off again.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_2"><p>POMDP pour Partially Observable Markov Decision Process en anglais.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_3"><p>Branch-and-Bound en anglais</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_4"><p>Un équipement critique tombe en panne lorsqu'il atteint un seuil conservatif fixé par les agences de régulation, et au-delà duquel les avions ne sont plus autorisés à décoller.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_5"><p>Weakly coupled dynamic programs are also called weakly coupled Markov decision processes in<ref type="bibr" target="#b14">[15]</ref>.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_6"><p>http://pomdp.org/examples/index.html</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_7"><p>While δ is called (memoryless) policy in the literature on POMDPs, δ is called strategy in the literature on influence diagrams (see e.g.<ref type="bibr" target="#b74">[75,</ref> Chap.  </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_8"><p>23])</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_9"><p>If Alice did not want to play every day, we would also need to model her decisions. In that case, Bob and Alice would have different objectives and we would need to use a Multi Agent Influence Diagram<ref type="bibr" target="#b75">[76]</ref>. However, since Alice wants to play chess every day, her decisions do not need to be taken into account, and we can model the problem as an influence diagram.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_10"><p>The probabilistic graphical model community sometimes calls a directed tree what we call here a rooted tree, and a polytree what we call here a directed tree.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_11"><p>The separators are often included in the definition of the junction tree and their associated moments τ S in the definition of the marginal polytope. We do not include them in this work, because we do not need them in our mathematical programming formulations. Adding them would increase the size of the mathematical program and downgrade the performance of the solver.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_12"><p>The concept of strong junction tree relies on the notion of elimination ordering for a given influence diagram with perfect recall. The main difference is that a strong junction tree is a notion on an influence diagram, where the set of decision vertices and their orders play a role, while RJTs rely on the underlying digraph. The notion of strong junction tree is obtained by replacing (ii) in the definition of an RJT by: "given an elimination ordering, if (C u ,C v ) is an arc, there exists an ordering of C v that respects the elimination ordering such that C u ∩ C v is before C v \C u in that ordering." An RJT is a strong junction tree. Conversely, a strong junction tree is not necessary an RJT. Indeed, Jensen et al.<ref type="bibr" target="#b61">[62,</ref> Figure  </p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_13"><p><ref type="bibr" target="#b3">4]</ref> shows an example of strong junction where there is v ∈ V such that fa(v) C v . As strong junction trees is a notion on influence diagram and not on graphs, Theorem 7.3 has no natural generalization for strong junction trees.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_14"><p>Although the particular RJT obtained by Algorithm 4 is a bucket tree, considering RJTs that are not bucket trees is also useful. Figure8.7 shows an application where it is interesting to use an RJT that is not a bucket tree.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_15"><p>We say that there is a v-structure at v i on a trailQ = (v 1 , . . . , v k ) if 1 &lt; i &lt; k and v i -1 → v i ← v i +1 .</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_16"><p>This type of property is well known in the literature on causality in graphical models, where the policies are viewed as interventions and some conditional probabilities are shown to be invariant under interventions; see, e.g., Koller and Friedman [75, Definition 21.3, p. 1019] or Peters et al. [119, Remark 6.40, p. 113].</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_17"><p>The number of constraints defining polytopes Q1 and Q b is v∈V s |X C v |+3 v∈V a |X C v |+ (C u ,C v )∈A |X C u ∩C v |, where |X C v | = u∈C v |X u | for all v in V . If we use valid cuts, then the number of constraints of polytopes Q ⊥ ⊥,1 and Q ⊥ ⊥,b is 2 v∈V s |X C v | + 4 v∈V a |X C v | + (C u ,C v )∈A |X C u ∩C v |.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_18"><p>This follows from the characterization of soluble influence diagrams in Section 9.1 and the fact thatϑ a t -1 ⊥ G † des(a t )|pa(a t ) for all t ∈ [T ]</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_19"><p>In fact, if the graph is soluble, and if the decision vertices are ordered in reverse topological order for the relevance graph, then SPU converges after exactly one pass over the vertices.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_20"><p>G in the cycle such that there is an arc (u, v) in E \E in the cycle. And let (u h , v h ) be the corresponding arc in the cycle. Let (u l , v l ) be the arc of A right before (u h , v h ) in the cycle such</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_21"><p>When Nilsson and Höhle<ref type="bibr" target="#b109">[110]</ref> wrote their paper, the notion of soluble influence diagram was still not known, and they used a weaker version. Their terminology is also different.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_22"><p>Since decision trees are widely used for maintenance across several industries<ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b130">131,</ref><ref type="bibr" target="#b149">150]</ref>, this assumption is not specific to Air France.</p></note>
		</body>
		<back>

			
			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Remerciements</head></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>component.</head><p>In practice, inequalities (5.2) help the resolution of MILP <ref type="bibr">(5.1)</ref>. However, since the extended formulation obtained by adding inequalities (5.2) in MILP (5.1) has a large number of variables and constraints when the number of components is large (M 15), the linear relaxation takes longer to solve.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Strengths of the linear relaxation</head><p>While in Section 4.3 we showed that the linear relaxation of MILP (4.7) is equivalent to the MDP approximation, one may ask the question: How do we relate the linear relaxation of MILP (5.1) with the MDP approximation of a weakly coupled POMDP? As stated the theorem below, we are able to link the value of the linear relaxation of MILP (5.1) (with and without valid inequalities (5.2)) with the optimal value v * ml and v * his . We denote respectively by z R c and z R the optimal values of the linear relaxations of MILP (5.1) with and without valid inequalities (5.2). It turns out the linear relaxation of MILP (5.1) is equivalent to the fluid formulation of Bertsimas and Mišić <ref type="bibr" target="#b14">[15]</ref>, which is a relaxation of the MDP approximation of a weakly coupled POMDP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">An upper bound and a lower bound</head><p>We show that (P wc ml ) and MILP (5.1) share a lower bound z LB and an upper bound z UB :</p><p>The aim of this section is to introduce the mathematical programs that give z LB and z UB by playing with the approximations (A), (B) and (C). In Section 5.4.4, we propose an interpretation of the bounds obtained. The upper bound z UB is difficult to compute in practice. Instead of computing z UB , we propose to compute another upper bound which is based on the Lagrangian relaxation of the linking constraints <ref type="bibr">(5.1d</ref>). This upper bound is much easier to compute for larger instances.</p><p>In this section, we need to compare MILP formulations that do not share the same set of variables. We therefore say that a problem P is a relaxation of problem P' when given a feasible solution of P' we can build a feasible solution of P with the same value.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.1">The lower bound from an MILP with an exponential number of constraints</head><p>Using the same notation as in MILP (5.1), we introduce the following MILP.</p><p>of the tuple (M , K , (p m ) m∈ <ref type="bibr">[M ]</ref> ). We build an instance as follows: first we choose a value of M in {3, 4, 5, 10, 15, 20}, second we build the probabilities p m by adding a random real in [0, 0.1] to each non-zero value of the probabilities of Ellis et al. <ref type="bibr" target="#b43">[44]</ref>, and finally we choose K = max( ω × M , 1), where ω is a scalar belonging {0.2, 0.4, 0.6, 0.8} (when M = 3, then K belongs to {1, 2}).</p><p>The range of values of K is chosen in such a way that it goes from highly restrictive constraints (smallest values of ω) to more flexible constraints (largest values of ω), with respect to the value of M . When K M , the decision maker can consider the subproblems separately, which is much easier. We enforce K to be non smaller than 1 because if K = 0, the authority cannot maintain the bridges.</p><p>We evaluate the performances of matheuristic 3 for rolling horizon T r in {2, 5}. For each instance (M , K , (p m ) m∈[M ] ), we perform 10 3 runs of matheuristic 3. We compute the average total cost |ν IP |, the average number of failures f IP over the 10 3 simulations. We compare ν IP with the upper bound z R c and the Lagrangian bound z LR by evaluating the average gap G</p><p>The lower the value of G R c IP , the better is the performance of policy δ IP . In addition, for each simulation, we compute the average computation time in seconds of the underlying formulation over all steps of the simulation. We then consider the average value over all the N simulations. For the quantities |ν IP | and f IP we also report the standard deviations over all simulations. </p><p>Corollary 8.2 enables to write Problems (6.5) and (8.2) as max µ∈M(G) v∈V r 〈r v , µ v 〉, and it will be useful in the proofs Chapter 9 because it characterizes the set of achievable moments by the set of constraints of (8.2).</p><p>Proof of Corollary 8.2. We denote the set µ ∈</p><p>Let µ ∈ S(G). Then, there exists a strategy δ such that (µ, δ) is a feasible solution of NLP (8.2). Theorem 8.1 ensures that µ is the vector of moments of the probability distribution P δ . Therefore, µ ∈ M(G).</p><p>Let µ ∈ M(G). By definition, there exists δ ∈ ∆ such that µ C (x C ) = P δ (X C = x C ) for any x C ∈ X C and any C ∈ V. First, Proposition 7.2 ensures that µ belongs to M(G). Second, since P δ factorizes according to G, Corollary 7.6 ensures that µ C v = µ v|pa(v) µ Čv for any v ∈ V , where</p><p>. By definition of P δ in (6.2), we deduce that µ v|pa(v) = δ v|pa(v) if v ∈ V a and µ v|pa(v) = p v|pa(v) otherwise. Therefore, µ ∈ S G (G). It achieves the proof.</p><p>Proof of Theorems 6.1 and 8.1. Let (µ, δ) be an admissible solution of NLP (8.2). Then δ is an admissible solution of MEU(G, ρ). We now prove that µ corresponds to the vector of moments of the distribution P δ induced by δ, from which we can deduce that</p><p>The following remark is the a key argument. Let A, P , and D be three disjoint subsets of V , P a distribution on X V , µ A∪P ∪D the distribution induced by P on X A∪P ∪D , and</p><p>By (8.3), we have that the vector µ satisfies the conditions of Theorem 7.3, and hence it derives from a distribution P µ that factorizes on G. Furthermore, constraints in the definition (8.1) of</p><p>for all v ∈ V s , which yields the result.</p><p>Conversely, let δ be an admissible solution of MEU(G, ρ), and µ be the vector of moments induced by P δ . We have <ref type="figure"></ref>and<ref type="figure">(µ, δ</ref>) is a solution of (8.2). Furthermore,  In the experiments of Section 8.6, the time to compute all the C ⊥ ⊥ and all p C ⊥ ⊥ |C ⊥ ⊥ was negligible compared to the time needed to solve an LP or the MILP.</p><p>Finding the largest inclusion-wise strategy independent set is motivated by the fact that the large D the stronger the valid cuts (8.7) in the following sense: Given a strategy independent set D in C , for any set D ⊆ D, the equalities µ C = µ C \D p D |C \D are valid for MILP (6.6) and are linear in µ. This fact comes from the following stability property of the strategy independent sets. Lemma 8.4. Let D be a strategy independent set in C , and consider D ⊆ D. Then, D is a strategy independent set in C .</p><p>Proof. It suffices to prove that P δ X D = x D |X C \D = x C \D does not depend on δ. We compute the conditional probability using Bayes law:</p><p>The numerator and the denominator in the last fraction above do not depend on δ because D is strategy independent set in C , which achieves the proof.</p><p>Consider a PID (G, ρ) with G = (V s ,V a , A) and ρ = (X V , p,r). Let G be a gradual RJT on G. We</p><p>Now we compute separately the last term above</p><p>The last equality comes from the induction hypothesis. Therefore, we deduce that</p><p>It follows that given a strategy δ, for C = C 0 , the equality (8.26) says that the objective values of MEU(G, ρ) and NLP (8.25) are equal. Now, we are able to write a theorem for NLP <ref type="bibr">(8.25)</ref> that is similar to Theorem 6. . Denoting again 1 x the Dirac at</p><p>x, we have</p><p>We claim that there is no policy δ that induces distribution µ bs 0 on X bs 0 . Indeed, in a distribution induced by a policy δ, it follows from the parametrization that if P(X a = 1) &lt; 1 and</p><p>when a = t k ). As µ ub is such that P(X b = 1) &gt; 0, P(X b = 2) &gt; 0, and P(X b = 1, X u = e) = 0, it cannot be induced by a policy. Hence, M G (G, ρ) is not convex. Therefore, M G (G, ρ) is not a polytope.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.2.3">Comparison of soluble and linear relaxations</head><p>MILP solvers are based on (much improved) branch-and-bound algorithms that use the linear relaxation to obtain bounds. Their ability to solve formulation (8.9) therefore depends on the quality of the bound provided by the linear relaxation. The following lemma ensures that adding arcs with head in the decision vertices gives an upper bound.</p><p>) is such that A is the union of A and a set of arcs with head in V a , then for every parametrization ρ, M EU (G , ρ) is a relaxation of M EU (G, ρ).</p><p>Since SPU solves efficiently soluble influence diagrams, we could imagine alternative branchand-bound schemes that use bounds computed using SPU on an influence diagram larger than G which is soluble. To formalize this idea, we introduce the following notion:</p><p>where A is the union of A and a set of arcs with head in V a . In this section, we show that the linear relaxation of MILP (8.9) is always at least as good as the ones obtained from soluble relaxations on our RJT.</p><p>Note that Theorem 8.12 can be reinterpreted as the link between soluble graph relaxation and linear relaxations. And since M(G) = P and M(G ⊥ ⊥ ) = P ⊥ ⊥ , by Theorem 9. Figure <ref type="figure">10</ref>.2 -The four elements of our approach: the feature extraction (in purple), the prediction model (in blue), the decision tree (in green) and the policy (in red). cision tree to turn this vector of features into a label that can be interpreted by experts. Finally, based on the labels of all equipments, our policy chooses which equipments should be maintained in such a way that the number of equipments to maintain does not exceed the maintenance capacity. This maintenance is then performed, and the airplane operates flights until the next maintenance slot. Using the discrete labels as observations at each maintenance slot, the resulting problem is a predictive maintenance problem as the one described in Chapter 3.</p><p>Learning to predict the evolution of sensor data would make little sense since it is very noisy and high dimensional. We therefore learn a Gaussian Hidden Markov model (HMM) that predicts the evolution of the vector of features. Learning such a statistic model consists in estimating its parameters, which are the transition probability distributions and the Gaussian law parameters. We must specify how we extract the features, how we learn the Gaussian HMM parameters and the decision tree that transforms the Gaussian HMM into a HMM with discrete observations. If the way we extract features and learn the Gaussian HMM parameters is relatively standard, the way we learn the decision tree is less so. Indeed, in predictive maintenance the decision trees are generally hand-designed for maintenance diagnosis, and not automatically learned to provide the input of a policy. The Gaussian HMM parameters together with the learned decision tree lead to the parameters of a HMM with discrete observations, which are the outputs of the decision tree. These HMM parameters are learned for each equipment, which is sufficient to define the weakly coupled POMDP parameters.</p><p>Once the weakly coupled POMDP parameter have been set, we can use the maintenance policy proposed in Part I. It raises a practical issue: Evaluating the performance of our approach re-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="10.4.">Numerical results</head><p>Evaluation of a policy on the simulated system. We simulate the system over a time-period T with T sim scheduled maintenance slots. We assume that the T sim maintenance slots are periodically scheduled with interval time h, i.e., τ t = t × h is the time of maintenance slot t and T = T sim × h. Each component starts at time τ = 0. At each maintenance slot t ∈ [T sim ], the decision maker receives the observation Z m</p><p>. According to a policy, if the decision maker maintains component m, then he pays the maintenance cost c m m . Otherwise, we let the system evolve until the next maintenance slot. If a failure happens between two maintenance slots, we suppose that the failure is observed. Then, the decision maker pays a failure cost c m f . We assume that when a component fails, the decision maker pays a failure cost and the component has been maintained.</p><p>Links with the Air France's maintenance problem. The simulated system aims at reproducing the airplane's maintenance problem. Indeed, the monitored equipments evolve independently. Between two maintenance slots the airplane is used to operate flights as shown in Figure <ref type="figure">1</ref>.1. At each flight τ, the airline has access to some sensor data corresponding to Z m τ ∈ R k m for each equipment m ∈ [M ]. These continuous observations are in practice very noisy. We reproduce it in our simulator by adding a large noise to the crack depth measurements; see Appendix C. When the airplane goes into maintenance slot t , the airline decides to maintain at most K equipments. When a failure happens on an equipment between two maintenance slots, the equipment has to be maintained. Otherwise, the airplane is not able to take off again, which is what we mentioned in the last paragraph.</p><p>The system we simulate is a mechanical system. In the literature, the predictive maintenance is much more used on mechanical systems than electrical systems. Most of the equipments tracked by the airline are mechanical. It seems that there is no apparent reason that prevents the methodology of working in practice on electrical components of airplane. However, it requires a deeper work on the feature extraction step.</p><p>Reproducing the airline's maintenance policy. The airline's practice in industry is based on a decision tree with a binary output {0, 1}. A component is maintained when the output of the decision tree is 1. We reproduce here such a policy for our system. Let g m be a decision tree that takes as input a continuous observation x m in R d m for all component m in [M ], and returns a binary output in {0, 1}. The decision maker computes the vector (g m (x m )) m∈ <ref type="bibr">[M ]</ref> . We maintain at most K components satisfying g m (x m ) = 1. Thus, we introduce the set of components that should be maintained M r ,</p><p>If |M r | &lt; K , then we maintain all component m in M r . Otherwise, we select the component m in M r with the K highest failure cost c m f . Algorithm (6) describes how we reproduce the airline's current practice. Handling the dataset, which contains all the sensor data, requires to use a Big Data processing engine. We choose to use Spark <ref type="bibr" target="#b166">[167]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Maintenance date</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Apply our policy</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation of the maintenance policy.</head><p>We compare what our maintenance policy would have done on the historical dataset against the past maintenance decisions of Air France. The available information are the maintenance dates of each equipment. When an equipment has been maintained, we know if the equipment has failed or not. However, as mentioned before, the data do not contain the exact failure dates.</p><p>At each flight τ, we compute what our policy would have suggested on (Z m τ ) m∈[M ] . We compare the maintenance dates and the first flight when our policy would have suggested a maintenance. Figure <ref type="figure">10</ref>.4 shows the scheme of the evaluation of our policy on the dataset. If the maintenance is a consequence of a failure, then we expect that the first maintenance suggestion would have appeared before the maintenance slot.</p><p>Numerical results on the AirFrance's dataset. By applying the policy evaluation scheme 10.4, we compare the maintenance dates and the dates where our maintenance policy would have suggested to maintain the equipment. Figure <ref type="figure">10</ref>.5 illustrates our maintenance evaluation scheme on two different airplanes. After each flight, we apply our maintenance policy and the rising edges of the blue (resp. red) dashed line indicate when it suggests to maintain the equipment 1 (resp. 2). The sizes of the state space and the observation space we obtain by using the methodology of Section 10. A maintenance date of an equipment (vertical plain line in Figure <ref type="figure">10</ref>.5) does not necessary indicate that the equipment failed. Indeed, in most of the cases the maintenances are preventive and the equipment has not failed when it it maintained. When a failure has been diagnosed in maintenance, it means that Air France's approach did not predict it in advance. To evaluate our maintenance policy, we count the percentage of failures such that our maintenance policy would have suggested to maintain the equipment before the corresponding maintenance dates, among those that have been diagnosed in maintenance. We denote by f m T r such a percentage for equipment m and obtained by using Algorithm 3 with rolling horizon T r . On the whole fleet, we count 10 diagnosed failures for each equipment. Table <ref type="table">10</ref>.2 reports all the values of f m T r obtained on the whole fleet. The first column indicates the rolling horizon T r we use in our policy. The second and third columns indicate respectively the values of f 1 T r and f 2 T r . Finally, the last column indicates the averaged computation time (Comp. Time) to take a decision at each flight.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.2.">Research directions</head><p>explain how we estimate the POMDP parameters of each equipment of the airplane. One of the main advantages of our approach is that all its steps are interpretable by maintenance experts, which means that the observations of the POMDPs are the output of a decision tree. Since we do not have access to a simulator of the sensor data, we evaluate our weakly coupled POMDP policy using a simulator of a system with several deteriorating components, which is built from the literature. In addition, we compare what our maintenance policy would have suggested in the historical dataset against Air France's decisions. In both cases, the numerical experiments show that our weakly coupled POMDP policy outperforms the one used by Air France.</p><p>Along this thesis, we developed the ideas on influence diagrams and detailed them in the following published paper. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="11.2">Research directions</head><p>We now highlight some research directions raised by our work on POMDPs and the influence diagrams.</p><p>In Chapter 5, we introduced different integer programs with theoretical guarantees for the weakly coupled POMDP. In particular, most of these approximations are based on a surrogate modeling of the linking capacity constraints. However, as we observe in the numerical experiments of multi-armed bandit instances, the integer programs become difficult to solve when the size of the state spaces and observation spaces increase. We could investigate a Branch-and-price algorithm based on the Dantzig-Wolfe decomposition of the integer programs. Leveraging the column generation approach, this algorithm would give an optimal solution of MILP (5.1) and could be efficient on large scale instances.</p><p>We are currently working on an approximate integer formulation for (P wc ml ) using the value function variables. The resulting formulation could be a generalization of the ones of Adelman and Mersereau <ref type="bibr" target="#b1">[2]</ref> to weakly coupled POMDPs.</p><p>In Part II, we proposed mathematical programming approaches for solving the maximum expected utility problem in influence diagrams. Two elements limit the scale of the problems that can be dealt with using our approach. First, we use exact inference, which limits the applicability to models with small treewidth. Second, rooted junction trees may contain clusters larger than those of standard junction trees. A possible way to overcome these limitations in future works would be to develop mathematical programming heuristics for influence diagrams that use variational inference instead of exact inference. Several works in graphical models that propose linear programming approaches for variational inference could be investigated <ref type="bibr" target="#b147">[148,</ref><ref type="bibr" target="#b148">149]</ref>. We are working on a method to use inference techniques such as Bethe entropy approximation <ref type="bibr" target="#b155">[156,</ref><ref type="bibr">Chapter 4]</ref> for influence diagrams.</p><p>We are also working on a distributionally robust version of the maximum expected utility problem in influence diagrams. In Part II, we assumed that the conditional probability distributions of the chance vertices p = (p v|pa(v) ) v∈V s are known by the decision maker. However, in practice these parameters are not always available. Indeed, the decision maker has usually access to N realizations of the random variables of the influence diagrams (X i V ) i =1,...,N . Based on this dataset, the usual approach we used for the airplane maintenance problem at Air France is to compute approximate parameters p and then to find a feasible strategy δ of the maximum expected utility problem. However, as we observe in the numerical experiments of Section 10, such an approach may give misleading strategies with poor results. To overcome this issue, we suggest to study the following distributionally robust optimization problem max</p><p>where the expectation is taken according to the probability distribution P p,δ and N (p) defines a neighborhood around the estimate p. This type of distributionally robust optimization problems have received interest in the last decades <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b125">126]</ref> for problem with exogenous noises. In influence diagram problems, the decisions and the "nature" are not independent. Recently, several works have proposed theoretical studies for robust MDPs <ref type="bibr" target="#b162">[163]</ref> and robust POMDPs <ref type="bibr" target="#b162">[163]</ref>. We wish to extend our mathematical programming approaches for this robust version of the maximum expected utility problem with theoretical guarantees.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Algorithm to build a small RJT</head><p>In this appendix we present an algorithm to build a RJT without considering a topological ordering on the initial graph G = (V, A).</p><p>The only difference between Algorithms 4 and 7 is that the for loop along a reverse topological ordering of Algorithm 4 is replaced in Algorithm 7 by a breadth first search that computes online this reverse topological ordering. Hence, if we denote this ordering, Algorithm 7 builds the same RJT as the one we obtain when we use Algorithm 4 with in input. Therefore, the RJT built by Algorithm 7 satisfies 7.6, and is such that the implications in (7.7) are equivalence. Furthermore, Steps 5 and 6 enable to ensure that, when there is no path between a vertex u ∈ V a and a vertex v ∈ V s , then u is placed before v in the reverse topological ordering computed by the breadth first search. Therefore, is a topological ordering on the graph G used as Step 9 of Algorithm 5. Hence, if G is soluble, Algorithm 7 builds a RJT such that G ⊥ ⊥ = G.</p><p>Remark that on non-soluble IDs, Steps 5 and 6 are a heuristic aimed at minimizing the size of C ⊥ ⊥ v for each v in V s . Such a heuristic is not relevant if valid cuts (6.7) are not used. In that case, an alternative approach could be to add as few variable as possible to C v for v in V a to improve the quality of the soluble relaxation G. This could be done by putting vertices u in V s unrelated to v ∈ V a after in this topological order, i.e., by replacing V a by V s in Steps 5 and 6. </p><p>Remove C x from C Remove v from G and recompute L 18: end while</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Deteriorating system's simulator and decision trees</head><p>In Section C.1 of this appendix we describe the system we simulate to evaluate the performances of our maintenance policy in Chapter 10. In Section C.2, we explain how we reproduce the Air France's practice for the system we simulate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.1 Simulator's description</head><p>Fatigue Crack Growth (FCG) models the appearance and propagation of a crack within a bearing. It is a widely used model in the predictive maintenance literature <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b81">82,</ref><ref type="bibr" target="#b108">109,</ref><ref type="bibr" target="#b160">161]</ref> to simulate a deteriorating system. Each coordinate k ∈ [d ] of the crack depth y k evolves according to the following Paris-Erdogan differential equation <ref type="bibr" target="#b114">[115]</ref> dy</p><p>where C and n are parameters depending on the material property, β is the base stress level of the component and γ determines the extra stress level of the component. This latter parameter depends on the environment state. The greater γ is, the faster the crack propagates.</p><p>Simulation We simulate the cracks in a component with FCG model where the extra stress level is a random variable. Let Γ i be such random variable at time i . The system's extra stress level transits randomly from Γ i = γ to Γ i +1 = γ with probability p Γ (γ |γ) for all γ, γ ∈ X Γ . We assume that the crack propagation rate stagnates or increases:</p><p>We combine this model with the discretization scheme detailed in Le et al. <ref type="bibr" target="#b81">[82]</ref> of Equation (C.1). Let γ i and (y k i ) 1 k d be respectively the extra stress level and the crack length at time i : where W i ∼ N (0, Σ w ), Σ w ∈ R d ×d , and ∆t represents the discretization time step of the scheme.</p><p>Note that the stochasticity of scheme (C.3) comes from W i and Γ i . We assume that the component fails when the crack length reaches a critical threshold y c . Hence, a failure happens at i when there is at least one coordinate k ∈</p><p>Since we have noisy measurements X i of the crack depth Y i , we add a white noise:</p><p>where ξ ∼ N (0, Σ ξ ) with Σ ξ ∈ R d ×d is a measurement error. Labeling the data. Let y 1 , . . . , y T ∈ R d be the crack depth evolution generated according to (C.3) until a failure. Therefore, y i T y cr i t i c for at least one i in <ref type="bibr">[d ]</ref>. Let q i be the 90-th percentile of the i-th coordinate of the crack depth. For each data point y t , we affect a binary label z t ∈ {0, 1} as follows</p><p>Hence, z t indicates the early stages of a failure.</p><p>Learning the decision tree. Given a sequence y 1 , . . . , y T in R d generated according to (C.3), we compute the corresponding sequence of labels z 1 , . . . , z T in {0, 1} using (C.4). We generate a large number of sequences and we compute the corresponding sequence of labels. Then, we use the CART Algorithm <ref type="bibr" target="#b18">[19]</ref> to learn a decision tree g : R d → {0, 1} that takes in input the observation x t ∈ R d and in output the label z t ∈ {0, 1}.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Group maintenance: A restless bandits approach</title>
		<author>
			<persName><forename type="first">Abderrahmane</forename><surname>Abbou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Viliam</forename><surname>Makis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal on Computing</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="719" to="731" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Relaxations of weakly coupled stochastic dynamic programs</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Adelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><forename type="middle">J</forename><surname>Mersereau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Operations Research</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="712" to="727" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A review on condition-based maintenance optimization models for stochastically deteriorating system</title>
		<author>
			<persName><forename type="first">Suzan</forename><surname>Alaswad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yisha</forename><surname>Xiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Reliability Engineering &amp; System Safety</title>
		<imprint>
			<biblScope unit="volume">157</biblScope>
			<biblScope unit="page" from="54" to="63" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Decision-theoretic specification of credal networks: A unified language for uncertain modeling with sets of Bayesian networks</title>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Antonucci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Zaffalon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Approximate Reasoning</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="345" to="361" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">An investigation into mathematical programming for finite horizon decentralized POMDPs</title>
		<author>
			<persName><forename type="first">Raghav</forename><surname>Aras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alain</forename><surname>Dutech</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="329" to="396" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Mixed integer linear programming for exact finite-horizon planning in decentralized POMDPs</title>
		<author>
			<persName><forename type="first">Raghav</forename><surname>Aras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alain</forename><surname>Dutech</surname></persName>
		</author>
		<author>
			<persName><forename type="first">François</forename><surname>Charpillet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventeenth International Conference on International Conference on Automated Planning and Scheduling</title>
		<meeting>the Seventeenth International Conference on International Conference on Automated Planning and Scheduling</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="18" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Open problem: Approximate planning of POMDPs in the class of memoryless policies</title>
		<author>
			<persName><forename type="first">Kamyar</forename><surname>Azizzadenesheli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Lazaric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Animashree</forename><surname>Anandkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning Research</title>
		<meeting>Machine Learning Research</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="1639" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Reinforcement learning of POMDPs using spectral methods</title>
		<author>
			<persName><forename type="first">Kamyar</forename><surname>Azizzadenesheli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Lazaric</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Animashree</forename><surname>Anandkumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Machine Learning Research</title>
		<meeting>Machine Learning Research</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="193" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Policy search by dynamic programming</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">Andrew</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><forename type="middle">G</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="831" to="838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Neuronlike adaptive elements that can solve difficult learning control problems</title>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><forename type="middle">W</forename><surname>Anderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="834" to="846" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Statistical inference for probabilistic functions of finite state Markov chains</title>
		<author>
			<persName><forename type="first">Leonard</forename><forename type="middle">E</forename><surname>Baum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ted</forename><surname>Petrie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Mathematical Statistics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1554" to="1563" />
			<date type="published" when="1966">1966</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Adaptive control processes-a guided tour</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Bellman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1961">1961</date>
			<publisher>Princeton University Press</publisher>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="315" to="316" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Nonlinear Programming</title>
		<author>
			<persName><forename type="first">Dimitri</forename><forename type="middle">P</forename><surname>Bertsekas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Athena Scientific</title>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dynamic Programming and Optimal Control</title>
		<author>
			<persName><forename type="first">Dimitri</forename><forename type="middle">P</forename><surname>Bertsekas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Athena Scientific</title>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
	<note>3rd edition</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Decomposable Markov decision processes: A fluid optimization approach</title>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Bertsimas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Velibor</surname></persName>
		</author>
		<author>
			<persName><surname>Mišić</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Operations Research</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="1537" to="1555" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Conservation laws, extended polymatroids and multiarmed bandit problems; a polyhedral approach to indexable systems</title>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Bertsimas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">José</forename><surname>Niño</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mora</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics of Operations Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="257" to="306" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Restless bandits, linear programming relaxations, and a primal-dual index heuristic</title>
		<author>
			<persName><forename type="first">Dimitris</forename><surname>Bertsimas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">José</forename><surname>Niño</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mora</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Operations Research</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="80" to="90" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Julia: A fresh approach to numerical computing</title>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Bezanson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><surname>Edelman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Karpinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Viral</surname></persName>
		</author>
		<author>
			<persName><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM review</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="65" to="98" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Classification and Regression Trees</title>
		<author>
			<persName><forename type="first">Leo</forename><surname>Breiman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerome</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">A</forename><surname>Olshen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Charles</forename><forename type="middle">J</forename><surname>Stone</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984">1984</date>
			<publisher>Wadsworth and Brooks</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Information relaxations and duality in stochastic dynamic programs</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">B</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">E</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Operations Research</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="785" to="801" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Convex sets of probabilities propagation by simulated annealing</title>
		<author>
			<persName><forename type="first">Andrés</forename><surname>Cano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">José</forename><forename type="middle">E</forename><surname>Cano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serafín</forename><surname>Moral</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifth International Conference on Information Processing and Management of Uncertainty in Knowledge-Based Systems</title>
		<meeting>the Fifth International Conference on Information Processing and Management of Uncertainty in Knowledge-Based Systems</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="4" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Incremental pruning: A simple, fast, exact method for partially observable Markov decision processes</title>
		<author>
			<persName><forename type="first">Anthony</forename><surname>Cassandra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nevin</forename><forename type="middle">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the Thirteenth Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="54" to="61" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Exact and Approximate Algorithms for Partially Observable Markov Decision Processes</title>
		<author>
			<persName><forename type="first">Anthony</forename><forename type="middle">R</forename><surname>Cassandra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998">1998</date>
		</imprint>
		<respStmt>
			<orgName>Brown University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A survey of POMDP applications</title>
		<author>
			<persName><forename type="first">Anthony</forename><forename type="middle">R</forename><surname>Cassandra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Working note of AAAI 1998 Fall Symposium on Planning with Partially Observable Markov Decision Processes</title>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Complexity of inference in graphical models</title>
		<author>
			<persName><forename type="first">Venkat</forename><surname>Chandrasekaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Srebro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prahladh</forename><surname>Harsha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Fourth Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the Twenty-Fourth Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="70" to="78" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Failure diagnosis using decision trees</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alice</forename><forename type="middle">X</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jim</forename><surname>Lloyd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Brewer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First International Conference on Autonomic Computing</title>
		<meeting>the First International Conference on Autonomic Computing</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Two generalizations of Markov blankets</title>
		<author>
			<persName><forename type="first">Victor</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Axel</forename><surname>Parmentier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1903.03538</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">J</forename><surname>Conejo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Minguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Garcia-Bertrand</surname></persName>
		</author>
		<title level="m">Decomposition Techniques in Mathematical Programming: Engineering and Science Applications</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Integer Programming</title>
		<author>
			<persName><forename type="first">Michele</forename><surname>Conforti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerard</forename><surname>Cornuejols</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Giacomo</forename><surname>Zambelli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Springer Publishing Company</publisher>
		</imprint>
	</monogr>
	<note>Incorporated</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">The computational complexity of probabilistic inference using Bayesian belief networks</title>
		<author>
			<persName><forename type="first">Gregory</forename><forename type="middle">F</forename><surname>Cooper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="page" from="393" to="405" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Using a simple crack growth model in predicting remaining useful life</title>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Coppe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><forename type="middle">J</forename><surname>Pais</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raphael</forename><forename type="middle">T</forename><surname>Haftka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nam</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Aircraft</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="page" from="1965" to="1973" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Inference in credal networks through integer programming</title>
		<author>
			<persName><forename type="first">P</forename><surname>Cassio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>De Campos</surname></persName>
		</author>
		<author>
			<persName><surname>Gagliardi Cozman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 5th International Symposium on Imprecise Probability: Theories and Applications</title>
		<meeting>the 5th International Symposium on Imprecise Probability: Theories and Applications</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="145" to="154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Strategy selection in influence diagrams using imprecise probabilities</title>
		<author>
			<persName><forename type="first">P</forename><surname>Cassio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>De Campos</surname></persName>
		</author>
		<author>
			<persName><surname>Ji</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1206.3246</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The linear programming approach to approximate dynamic programming</title>
		<author>
			<persName><forename type="first">Daniela</forename><surname>Pucci</surname></persName>
		</author>
		<author>
			<persName><forename type="first">De</forename><surname>Farias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Van</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Operations research</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="page" from="850" to="865" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Reasoning with Probabilistic and Deterministic Graphical Models: Exact Algorithms</title>
		<author>
			<persName><forename type="first">Rina</forename><surname>Dechter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>Morgan &amp; Claypool Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Distributionally robust optimization under moment uncertainty with application to data-driven problems</title>
		<author>
			<persName><forename type="first">Erick</forename><surname>Delage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinyu</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Operations Research</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="page" from="595" to="612" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Optimization of Sequential Decision Making for Chronic Diseases: From Data to Decisions, chapter 13</title>
		<author>
			<persName><forename type="first">Brian</forename><forename type="middle">T</forename><surname>Denton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="316" to="348" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A probabilistic production and inventory problem</title>
		<author>
			<persName><forename type="first">F</forename><surname>Epenoux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Management Science</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="98" to="108" />
			<date type="published" when="1963">1963</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Techniques for interpretable machine learning</title>
		<author>
			<persName><forename type="first">Mengnan</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ninghao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="68" to="77" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">α-min: A compact approximate solver for finite-horizon POMDPs</title>
		<author>
			<persName><forename type="first">Yann</forename><surname>Dujardin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tom</forename><surname>Dietterich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iadine</forename><surname>Chades</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conferences on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="2582" to="2588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">JuMP: A modeling language for mathematical optimization</title>
		<author>
			<persName><forename type="first">Iain</forename><surname>Dunning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joey</forename><surname>Huchette</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miles</forename><surname>Lubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Review</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="295" to="320" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Optimum maintenance with incomplete information</title>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">E</forename><surname>Eckles</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Operations Research</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="1058" to="1067" />
			<date type="published" when="1968">1968</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">POMDPs.jl: A framework for sequential decision making under uncertainty</title>
		<author>
			<persName><forename type="first">Maxim</forename><surname>Egorov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><forename type="middle">N</forename><surname>Sunberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Balaban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><forename type="middle">A</forename><surname>Wheeler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jayesh</forename><forename type="middle">K</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mykel</forename><forename type="middle">J</forename><surname>Kochenderfer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1" to="5" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Inspection, maintenance, and repair with partial observability</title>
		<author>
			<persName><forename type="first">Hugh</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingxiang</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ross</forename><forename type="middle">B</forename><surname>Corotis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Infrastructure Systems</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="92" to="99" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Data-driven distributionally robust optimization using the wasserstein metric: performance guarantees and tractable reformulations</title>
		<author>
			<persName><forename type="first">Peyman</forename><surname>Mohajerin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Esfahani</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Kuhn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<biblScope unit="volume">171</biblScope>
			<biblScope unit="page" from="115" to="166" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">The linear programming approach to approximate dynamic programming</title>
		<author>
			<persName><forename type="first">Daniela</forename><forename type="middle">P De</forename><surname>Farias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><forename type="middle">V</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Operations Research</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<date type="published" when="2001">2003. 2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Comprehensible classification models: A position paper</title>
		<author>
			<persName><forename type="first">Alex</forename><forename type="middle">A</forename><surname>Freitas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGKDD Exploration Newsletter</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1" to="10" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Lagrangian relaxation for integer programming</title>
		<author>
			<persName><forename type="first">M</forename><surname>Arthur</surname></persName>
		</author>
		<author>
			<persName><surname>Geoffrion</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming Study</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<date type="published" when="1974">1974</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A dynamic allocation index for the discounted multiarmed bandit problem</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Gittins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">M</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="page" from="561" to="565" />
			<date type="published" when="1979">1979</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Multi-armed Bandit Allocation Indices</title>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">C</forename><surname>Gittins</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989">1989</date>
			<publisher>Wiley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><surname>Medjaher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Gouriveau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Zerhouni</surname></persName>
		</author>
		<title level="m">PHM and Predictive Maintenance</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<monogr>
		<title level="m">LLC Gurobi Optimization. Gurobi optimizer reference manual</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<monogr>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Tibshirani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jerome</forename><surname>Friedman</surname></persName>
		</author>
		<title level="m">The Elements of Statistical Learning</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Value-function approximations for partially observable Markov decision processes</title>
		<author>
			<persName><forename type="first">Milos</forename><surname>Hauskrecht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">A Lagrangian decomposition approach to weakly coupled dynamic optimization problems and its applications</title>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hawkins</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
		<respStmt>
			<orgName>Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Influence diagrams</title>
		<author>
			<persName><forename type="first">Ronald</forename><forename type="middle">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">E</forename><surname>Matheson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Readings on the Principles and Applications of Decision Analysis</title>
		<imprint>
			<date type="published" when="1984">1984</date>
			<biblScope unit="volume">II</biblScope>
			<biblScope unit="page" from="721" to="762" />
		</imprint>
		<respStmt>
			<orgName>Strategic Decisions Group</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">A</forename><surname>Ronald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><surname>Matheson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Influence diagrams. Decision Analysis</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="127" to="143" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Multi-language evaluation of exact solvers in graphical model discrete optimization</title>
		<author>
			<persName><forename type="first">Barry</forename><surname>Hurley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O'</forename><surname>Barry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Allouche</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Katsirelos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Schiex</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Zytnicki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Givry</forename><surname>De</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Constraints</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="413" to="434" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Reinforcement learning algorithm for partially observable Markov decision problems</title>
		<author>
			<persName><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Satinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Seventh International Conference on Neural Information Processing Systems</title>
		<meeting>the Seventh International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="345" to="352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">A review on machinery diagnostics and prognostics implementing condition-based maintenance</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">S</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daming</forename><surname>Jardine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><surname>Banjevic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mechanical Systems and Signal Processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="1483" to="1510" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Improving data-driven prognostics by assessing predictability of features</title>
		<author>
			<persName><forename type="first">Kamran</forename><surname>Javed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafael</forename><surname>Gouriveau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ryad</forename><surname>Zemouri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noureddine</forename><surname>Zerhouni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Annual Conference of the Prognostics and Health Management Society</title>
		<meeting>the Annual Conference of the Prognostics and Health Management Society</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="555" to="560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">From influence diagrams to junction trees</title>
		<author>
			<persName><forename type="first">Frank</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Søren</forename><forename type="middle">L</forename><surname>Finn V Jensen</surname></persName>
		</author>
		<author>
			<persName><surname>Dittmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth international conference on Uncertainty in artificial intelligence</title>
		<meeting>the Tenth international conference on Uncertainty in artificial intelligence</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="367" to="373" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Prognostics and Health Management of PEMFC -state of the art and remaining challenges</title>
		<author>
			<persName><forename type="first">Marine</forename><surname>Jouin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafael</forename><surname>Gouriveau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Hissel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Cécile</forename><surname>Péra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noureddine</forename><surname>Zerhouni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Hydrogen Energy</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="15307" to="15317" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Planning and acting in partially observable stochastic domains</title>
		<author>
			<persName><forename type="first">Leslie</forename><surname>Pack Kaelbling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">L</forename><surname>Littman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><forename type="middle">R</forename><surname>Cassandra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page" from="99" to="134" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Topological sorting of large networks</title>
		<author>
			<persName><forename type="first">Arthur</forename><forename type="middle">B</forename><surname>Kahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="558" to="562" />
			<date type="published" when="1962">1962</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Unifying tree decompositions for reasoning in graphical models</title>
		<author>
			<persName><forename type="first">Kalev</forename><surname>Kask</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rina</forename><surname>Dechter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Javier</forename><surname>Larrosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Avi</forename><surname>Dechter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">166</biblScope>
			<biblScope unit="page" from="165" to="193" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Sequential decision making with limited observation capability: Application to wireless networks</title>
		<author>
			<persName><forename type="first">Kesav</forename><surname>Kaza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Meshram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Varun</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shabbir</forename><forename type="middle">N</forename><surname>Merchant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cognitive Communications and Networking</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="237" to="251" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Solving limited-memory influence diagrams using branch-and-bound search</title>
		<author>
			<persName><forename type="first">Arindam</forename><surname>Khaled</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">A</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changhe</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the Twenty-Ninth Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">A POMDP framework for integrated scheduling of infrastructure maintenance and inspection</title>
		<author>
			<persName><forename type="first">Jong</forename><forename type="middle">Woo</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Go</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Bong</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Chemical Engineering</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="page" from="239" to="252" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">Optimal control of a partially observable failing system with costly multivariate observations</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Jong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kim</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Viliam</forename><surname>Makis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Stochastic Models</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="584" to="608" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Joint optimization of sampling and control of partially observable failing systems</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Jong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kim</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Viliam</forename><surname>Makis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Operations Research</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="777" to="790" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Optimal Bayesian fault prediction scheme for a partially observable system subject to random failure</title>
		<author>
			<persName><forename type="first">Jong</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Viliam</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chi-Guhn</forename><surname>Makis</surname></persName>
		</author>
		<author>
			<persName><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European Journal of Operational Research</title>
		<imprint>
			<biblScope unit="volume">214</biblScope>
			<biblScope unit="page" from="331" to="339" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Parameter estimation for partially observable systems subject to random failure</title>
		<author>
			<persName><forename type="first">Jong</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Viliam</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Makis</surname></persName>
		</author>
		<author>
			<persName><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Stochastic Models in Business and Industry</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="279" to="294" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">The stochastic inventory routing problem with direct deliveries</title>
		<author>
			<persName><forename type="first">Anton</forename><forename type="middle">J</forename><surname>Kleywegt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><forename type="middle">S</forename><surname>Nori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><forename type="middle">W P</forename><surname>Savelsbergh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transportation Science</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="94" to="118" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<monogr>
		<title level="m" type="main">Probabilistic graphical models: principles and techniques</title>
		<author>
			<persName><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nir</forename><surname>Friedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">Multi-agent influence diagrams for representing and solving games</title>
		<author>
			<persName><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><surname>Milch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Games and economic behavior</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="page" from="181" to="221" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<analytic>
		<title level="a" type="main">PAC reinforcement learning with rich observations</title>
		<author>
			<persName><forename type="first">Akshay</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alekh</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Langford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth Advances in Neural Information Processing Systems</title>
		<meeting>the Twenty-Ninth Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Partially observed Markov decision processes: From filtering to controlled sensing</title>
		<author>
			<persName><forename type="first">Vikram</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">Partially observed Markov decision process multiarmed bandits-structural results</title>
		<author>
			<persName><forename type="first">Vikram</forename><surname>Krishnamurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Wahlberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics of Operations Research</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="287" to="302" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<analytic>
		<title level="a" type="main">SARSOP: Efficient point-based POMDP planning by approximating optimally reachable belief spaces</title>
		<author>
			<persName><forename type="first">Hanna</forename><surname>Kurniawati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wee</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourth Conference on Robotics: Science and Systems</title>
		<meeting>the Fourth Conference on Robotics: Science and Systems</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Representing and solving decision problems with limited information</title>
		<author>
			<persName><forename type="first">L</forename><surname>Steffen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dennis</forename><surname>Lauritzen</surname></persName>
		</author>
		<author>
			<persName><surname>Nilsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Management Science</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="page" from="1235" to="1251" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">Hidden Markov Models for diagnostics and prognostics of systems under multiple deterioration modes</title>
		<author>
			<persName><forename type="first">Thanh</forename><surname>Trung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Florent</forename><surname>Chatelain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christophe</forename><surname>Bérenguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-fourth Conference on European Safety and Reliability</title>
		<meeting>the Twenty-fourth Conference on European Safety and Reliability</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Join graph decomposition bounds for influence diagrams</title>
		<author>
			<persName><forename type="first">Junkyu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">T</forename><surname>Ihler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rina</forename><surname>Dechter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Fourth Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the Thirty-Fourth Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1053" to="1062" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Finding optimal memoryless policies of POMDPs under the expected average reward criterion</title>
		<author>
			<persName><forename type="first">Yanjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baoqun</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongsheng</forename><surname>Xi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European Journal of Operational Research</title>
		<imprint>
			<biblScope unit="volume">211</biblScope>
			<biblScope unit="page" from="556" to="567" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<monogr>
		<title level="m" type="main">Introduction to global optimization</title>
		<author>
			<persName><forename type="first">Leo</forename><surname>Liberti</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b85">
	<monogr>
		<title level="m" type="main">The witness algorithm: Solving partially observable Markov decision processes</title>
		<author>
			<persName><forename type="first">L</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><surname>Littman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Memoryless policies: Theoretical limitations and practical results</title>
		<author>
			<persName><forename type="first">L</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><surname>Littman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Third International Conference on Simulation of Adaptive Behavior: From Animals to Animats 3: From Animals to Animats 3</title>
		<meeting>the Third International Conference on Simulation of Adaptive Behavior: From Animals to Animats 3: From Animals to Animats 3</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="238" to="245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<monogr>
		<title level="m" type="main">Reasoning and Decisions in Probabilistic Graphical Models-A Unified Framework</title>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
		<respStmt>
			<orgName>University of California, Irvine</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Belief propagation for structured decision making</title>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Ihler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Uncertainty in Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="523" to="532" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Multivariate Bayesian control chart</title>
		<author>
			<persName><forename type="first">Viliam</forename><surname>Makis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Operations Research</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="page" from="487" to="496" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Optimal replacement under partial observations</title>
		<author>
			<persName><forename type="first">Viliam</forename><surname>Makis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiamei</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mathematics of Operations Research</title>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="382" to="394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">A benders based rolling horizon algorithm for a dynamic facility location problem</title>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Marufuzzaman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ridvan</forename><surname>Gedik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><forename type="middle">S</forename><surname>Roni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; Industrial Engineering</title>
		<imprint>
			<biblScope unit="volume">98</biblScope>
			<biblScope unit="page" from="462" to="469" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<monogr>
		<title level="m" type="main">Understanding and Using Linear Programming</title>
		<author>
			<persName><forename type="first">Jirí</forename><surname>Matouek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernd</forename><surname>Gärtner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Equivalences between maximum a posteriori inference in Bayesian networks and maximum expected utility computation in influence diagrams</title>
		<author>
			<persName><forename type="first">Denis</forename><forename type="middle">D</forename><surname>Maua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Approximate Reasoning</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="211" to="229" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Solving decision problems with limited information</title>
		<author>
			<persName><forename type="first">Denis</forename><forename type="middle">D</forename><surname>Mauá</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cassio</forename><forename type="middle">P</forename><surname>Campos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-fifth Conference on Advances in Neural Information Processing Systems</title>
		<meeting>the Twenty-fifth Conference on Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="603" to="611" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">Fast local search methods for solving limited memory influence diagrams</title>
		<author>
			<persName><forename type="first">Denis</forename><forename type="middle">D</forename><surname>Mauá</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabio</forename><surname>Gagliardi Cozman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Approximate Reasoning</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="230" to="245" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<analytic>
		<title level="a" type="main">The complexity of approximately solving influence diagrams</title>
		<author>
			<persName><forename type="first">Denis</forename><forename type="middle">D</forename><surname>Mauá</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cassio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>De Campos</surname></persName>
		</author>
		<author>
			<persName><surname>Zaffalon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Eighth Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the Twenty-Eighth Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="604" to="613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Solving limited memory influence diagrams</title>
		<author>
			<persName><forename type="first">Denis</forename><forename type="middle">D</forename><surname>Mauá</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Cassio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>De Campos</surname></persName>
		</author>
		<author>
			<persName><surname>Zaffalon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="97" to="140" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">On the complexity of solving polytree-shaped limited memory influence diagrams with binary variables</title>
		<author>
			<persName><forename type="first">Denis</forename><forename type="middle">D</forename><surname>Mauá</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">De</forename><surname>Cassio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marco</forename><surname>Campos</surname></persName>
		</author>
		<author>
			<persName><surname>Zaffalon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">205</biblScope>
			<biblScope unit="page" from="30" to="38" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<analytic>
		<title level="a" type="main">Computability of global solutions to factorable nonconvex programs: Part i -convex underestimating problems</title>
		<author>
			<persName><forename type="first">Garth</forename><forename type="middle">P</forename><surname>Mccormick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="147" to="175" />
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<monogr>
		<title level="m" type="main">Multi-armed bandits with constrained arms and hidden states</title>
		<author>
			<persName><forename type="first">Varun</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Meshram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kesav</forename><surname>Kaza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shabbir</forename><forename type="middle">N</forename><surname>Merchant</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">Sequential decision making with limited observation capability: Application to wireless networks</title>
		<author>
			<persName><forename type="first">Varun</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Meshram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kesav</forename><surname>Kaza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shabbir</forename><forename type="middle">N</forename><surname>Merchant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cognitive Communications and Networking</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="237" to="251" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<monogr>
		<title level="m" type="main">Information-sensitive replenishment when inventory records are inaccurate</title>
		<author>
			<persName><forename type="first">Adam</forename><forename type="middle">J</forename><surname>Mersereau</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013">2013</date>
			<publisher>Production and Operations Management</publisher>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="792" to="810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<analytic>
		<title level="a" type="main">On the whittle index for restless multiarmed hidden Markov bandits</title>
		<author>
			<persName><forename type="first">D</forename><surname>Rahul Meshram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Manjunath</surname></persName>
		</author>
		<author>
			<persName><surname>Gopalan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Automatic Control</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="3046" to="3053" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<analytic>
		<title level="a" type="main">Solving very large weakly coupled Markov decision processes</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Meuleau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Milos</forename><surname>Hauskrecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kee-Eung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leonid</forename><surname>Peshkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leslie</forename><forename type="middle">Pack</forename><surname>Kaelbling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Craig</forename><surname>Boutilier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth National/Tenth Conference on Artificial Intelligence/Innovative Applications of Artificial Intelligence</title>
		<meeting>the Fifteenth National/Tenth Conference on Artificial Intelligence/Innovative Applications of Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="165" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">Solving POMDPs by searching the space of finite policies</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Meuleau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kee-Eung</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Leslie</forename><surname>Pack Kaelbling</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><forename type="middle">R</forename><surname>Cassandra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fifteenth Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the Fifteenth Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="417" to="426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<analytic>
		<title level="a" type="main">A survey of partially observable Markov decision processes: Theory, models, and algorithms</title>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Monahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Management Science</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1" to="16" />
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b107">
	<monogr>
		<title level="m" type="main">Geometry and determinism of optimal stationary control in partially observable Markov decision processes</title>
		<author>
			<persName><forename type="first">Guido</forename><surname>Montufar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keyan</forename><surname>Ghazi-Zahedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nihat</forename><surname>Ay</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.07206</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">Application of stochastic filtering for lifetime prediction</title>
		<author>
			<persName><forename type="first">Eija</forename><surname>Myötyri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Urho</forename><surname>Pulkkinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaisa</forename><surname>Simola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Reliability Engineering &amp; System Safety</title>
		<imprint>
			<biblScope unit="volume">91</biblScope>
			<biblScope unit="page" from="200" to="208" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
	<note>Selected Papers Presented at QUALITA 2003</note>
</biblStruct>

<biblStruct xml:id="b109">
	<analytic>
		<title level="a" type="main">Computing bounds on expected utilities for optimal policies based on limited information</title>
		<author>
			<persName><forename type="first">Dennis</forename><surname>Nilsson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Höhle</surname></persName>
		</author>
		<idno>94</idno>
	</analytic>
	<monogr>
		<title level="j">Danish Informatics Network in the Agriculture Sciences</title>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Dynamic allocation indices for restless projects and queueing admission control: a polyhedral approach</title>
		<author>
			<persName><forename type="first">José</forename><surname>Niño-Mora</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="page" from="361" to="413" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">An adaptive dynamic programming algorithm for a stochastic multiproduct batch dispatch problem</title>
		<author>
			<persName><forename type="first">Katerina</forename><forename type="middle">P</forename><surname>Papadaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Warren</forename><forename type="middle">B</forename><surname>Powell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Naval Research Logistics</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="742" to="769" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">The complexity of Markov decision processes</title>
		<author>
			<persName><forename type="first">Christos</forename><forename type="middle">H</forename><surname>Papadimitriou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">N</forename><surname>Tsitsiklis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics of Operations Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="441" to="450" />
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">The complexity of optimal queuing network control</title>
		<author>
			<persName><forename type="first">Christos</forename><forename type="middle">H</forename><surname>Papadimitriou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">N</forename><surname>Tsitsiklis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics of Operations Research</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="293" to="305" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">A Critical Analysis of Crack Propagation Laws</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fazil</forename><surname>Erdogan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Basic Engineering</title>
		<imprint>
			<biblScope unit="volume">85</biblScope>
			<biblScope unit="page" from="528" to="533" />
			<date type="published" when="1963">1963</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<analytic>
		<title level="a" type="main">Weakly coupled Markov decision processes with imperfect information</title>
		<author>
			<persName><forename type="first">Mahshid</forename><surname>Salemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parizi</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Archis</forename><surname>Ghate</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Winter Simulation Conference</title>
		<meeting>the Winter Simulation Conference</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3609" to="3602" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<analytic>
		<title level="a" type="main">Integer programming on the junction tree polytope for influence diagrams</title>
		<author>
			<persName><forename type="first">Axel</forename><surname>Parmentier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Leclère</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillaume</forename><surname>Obozinski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Salmon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IN-FORMS Journal on Optimization</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="209" to="228" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName><forename type="first">Fabian</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaël</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bertrand</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathieu</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ron</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jake</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Édouard</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<monogr>
		<title level="m" type="main">Elements of causal inference: foundations and learning algorithms</title>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schölkopf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Point-based value iteration: An anytime algorithm for POMDPs</title>
		<author>
			<persName><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoff</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth International Joint Conference on Artificial Intelligence</title>
		<meeting>the Eighteenth International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="1025" to="1030" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<analytic>
		<title level="a" type="main">A graph-theoretic analysis of information value</title>
		<author>
			<persName><forename type="first">Kim</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Poh</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Horvitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twelfth International Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the Twelfth International Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="427" to="435" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<monogr>
		<title level="m" type="main">Clearing the Jungle of Stochastic Optimization</title>
		<author>
			<persName><forename type="first">Warren</forename><forename type="middle">B</forename><surname>Powell</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="109" to="137" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<monogr>
		<title level="m" type="main">Approximate Dynamic Programming: Solving the Curses of Dimensionality</title>
		<author>
			<persName><forename type="first">Warren</forename><forename type="middle">B</forename><surname>Powell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>Wiley</publisher>
		</imprint>
	</monogr>
	<note>2nd edition</note>
</biblStruct>

<biblStruct xml:id="b123">
	<monogr>
		<title level="m" type="main">Markov Decision Processes: Discrete Stochastic Dynamic Programming</title>
		<author>
			<persName><forename type="first">Martin</forename><forename type="middle">L</forename><surname>Puterman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<publisher>John Wiley &amp; Sons, Inc</publisher>
		</imprint>
	</monogr>
	<note>1st edition</note>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">An introduction to hidden Markov models</title>
		<author>
			<persName><forename type="first">Lawrence</forename><forename type="middle">R</forename><surname>Rabiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Biing</forename><forename type="middle">H</forename><surname>Juang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE ASSP Magazine</title>
		<imprint>
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<monogr>
		<author>
			<persName><forename type="first">Hamed</forename><surname>Rahimian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Mehrotra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1908.05659</idno>
		<title level="m">Distributionally robust optimization: A review</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">A rolling horizon heuristic for creating a liquefied natural gas annual delivery program</title>
		<author>
			<persName><forename type="first">Jørgen</forename><surname>Glomvik Rakke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Magnus</forename><surname>Stålhane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Rørholt Moe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marielle</forename><surname>Christiansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Henrik</forename><surname>Andersson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kjetil</forename><surname>Fagerholt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Inge</forename><surname>Norstad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transportation Research Part C: Emerging Technologies</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="896" to="911" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">Rolling horizon heuristics for production planning and set-up scheduling with backlogs and error-prone demand forecasts</title>
		<author>
			<persName><forename type="first">R</forename><surname>Alistair</surname></persName>
		</author>
		<author>
			<persName><surname>Clark</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Production Planning &amp; Control</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="81" to="97" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">Graph minors. V. excluding a planar graph</title>
		<author>
			<persName><forename type="first">Neil</forename><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><forename type="middle">D</forename><surname>Seymour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Combinatorial Theory, Series B</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="92" to="114" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">Sébastien Paquet, and Brahim Chaib-draa</title>
		<author>
			<persName><forename type="first">Stéphane</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="663" to="704" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
	<note>Online planning algorithms for POMDPs</note>
</biblStruct>

<biblStruct xml:id="b130">
	<monogr>
		<title level="m" type="main">Vibration based fault diagnosis of monoblock centrifugal pump using decision tree. Expert Systems with Applications</title>
		<author>
			<persName><forename type="first">Natarajan</forename><surname>Sakthivel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Sugumaran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Babudevasenapati</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="4040" to="4049" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">Metrics for evaluating performance of prognostic techniques</title>
		<author>
			<persName><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Celaya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Balaban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Goebel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Saha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Schwabacher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2008 International Conference on Prognostics and Health Management</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b132">
	<analytic>
		<title level="a" type="main">A linear algorithm for the pathwidth of trees</title>
		<author>
			<persName><forename type="first">Petra</forename><surname>Scheffler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Topics in combinatorics and graph theory</title>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="613" to="620" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b133">
	<analytic>
		<title level="a" type="main">Fault-tree analysis using a binary decision tree</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">G</forename><surname>Schneeweiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Reliability</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="453" to="457" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b134">
	<monogr>
		<title level="m" type="main">Combinatorial optimization: polyhedra and efficiency</title>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Schrijver</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003">2003</date>
			<publisher>Springer</publisher>
			<biblScope unit="volume">24</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b135">
	<analytic>
		<title level="a" type="main">A theory of rolling horizon decision making</title>
		<author>
			<persName><forename type="first">Suresh</forename><surname>Sethi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerhard</forename><surname>Sorger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Operations Research</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="387" to="416" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b136">
	<analytic>
		<title level="a" type="main">Evaluating influence diagrams</title>
		<author>
			<persName><forename type="first">Ross</forename><forename type="middle">D</forename><surname>Shachter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Operations Research</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="871" to="882" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b137">
	<analytic>
		<title level="a" type="main">Bayes-ball: Rational pastime (for determining irrelevance and requisite information in belief networks and influence diagrams)</title>
		<author>
			<persName><forename type="first">Ross</forename><forename type="middle">D</forename><surname>Shachter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the Fourteenth Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="1998">1998</date>
			<biblScope unit="page" from="480" to="487" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b138">
	<analytic>
		<title level="a" type="main">Probability propagation</title>
		<author>
			<persName><forename type="first">Glenn</forename><surname>Shafer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName><surname>Shenoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of Mathematics and Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="327" to="351" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b139">
	<analytic>
		<title level="a" type="main">A survey of point-based POMDP solvers</title>
		<author>
			<persName><forename type="first">Guy</forename><surname>Shani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Kaplow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Autonomous Agents and Multi-Agent Systems</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1" to="51" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b140">
	<analytic>
		<title level="a" type="main">Valuation-based systems for Bayesian decision analysis</title>
		<author>
			<persName><forename type="first">P</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName><surname>Shenoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Operations research</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="463" to="484" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b141">
	<analytic>
		<title level="a" type="main">Remaining useful life estimation -a review on the statistical data driven approaches</title>
		<author>
			<persName><forename type="first">Xiao-Sheng</forename><surname>Si</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chang-Hua</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dong-Hua</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European Journal of Operational Research</title>
		<imprint>
			<biblScope unit="volume">213</biblScope>
			<biblScope unit="page" from="1" to="14" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b142">
	<analytic>
		<title level="a" type="main">Learning without stateestimation in partially observable Markovian decision processes</title>
		<author>
			<persName><forename type="first">P</forename><surname>Satinder</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommi</forename><forename type="middle">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh International Conference on International Conference on Machine Learning</title>
		<meeting>the Eleventh International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="284" to="292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b143">
	<analytic>
		<title level="a" type="main">The optimal control of partially observable Markov processes over a finite horizon</title>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">D</forename><surname>Smallwood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><forename type="middle">J</forename><surname>Sondik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Operations Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1071" to="1088" />
			<date type="published" when="1973">1973</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b144">
	<analytic>
		<title level="a" type="main">Heuristic search value iteration for POMDPs</title>
		<author>
			<persName><forename type="first">Trey</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reid</forename><surname>Simmons</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twentieth Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the Twentieth Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2004">2004</date>
			<biblScope unit="page" from="520" to="527" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b145">
	<analytic>
		<title level="a" type="main">Point-based POMDP algorithms: Improved analysis and implementation</title>
		<author>
			<persName><forename type="first">Trey</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reid</forename><surname>Simmons</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-First Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the Twenty-First Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="542" to="549" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b146">
	<analytic>
		<title level="a" type="main">The optimal control of partially observable Markov processes over the infinite horizon: Discounted costs</title>
		<author>
			<persName><forename type="first">Edward</forename><forename type="middle">J</forename><surname>Sondik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Operations Research</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="282" to="304" />
			<date type="published" when="1978">1978</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b147">
	<analytic>
		<title level="a" type="main">Introduction to dual decomposition for inference</title>
		<author>
			<persName><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommi</forename><surname>Amir Globerson</surname></persName>
		</author>
		<author>
			<persName><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Optimization for Machine Learning</title>
		<imprint>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b148">
	<analytic>
		<title level="a" type="main">Efficiently searching for frustrated cycles in MAP inference</title>
		<author>
			<persName><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Do</forename><surname>Kook Choe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yitao</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Eighth Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the Twenty-Eighth Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="795" to="804" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b149">
	<analytic>
		<title level="a" type="main">Decision tree and PCA-based fault diagnosis of rotating machinery</title>
		<author>
			<persName><forename type="first">Weixiang</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaqing</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mechanical Systems and Signal Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="page" from="1300" to="1317" />
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b150">
	<analytic>
		<title level="a" type="main">A data-driven failure prognostics method based on mixture of gaussians hidden Markov models</title>
		<author>
			<persName><forename type="first">Diego</forename><surname>Alejandro Tobon-Mejia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kamal</forename><surname>Medjaher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noureddine</forename><surname>Zerhouni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerard</forename><surname>Tripot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Reliability</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="page" from="491" to="503" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b151">
	<analytic>
		<title level="a" type="main">Using Lagrangian relaxation to compute capacity-dependent bid prices in network revenue management</title>
		<author>
			<persName><forename type="first">Huseyin</forename><surname>Topaloglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Operations Research</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="637" to="649" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b152">
	<analytic>
		<title level="a" type="main">Prognostics and health management: A review on data driven approaches</title>
		<author>
			<persName><forename type="first">Kwok-Leung</forename><surname>Tsui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhen</forename><surname>Hai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbin</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Problems in Engineering</title>
		<imprint>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b153">
	<analytic>
		<title level="a" type="main">A survey of preventive maintenance models for stochastically deteriorating single-unit systems</title>
		<author>
			<persName><forename type="first">Ciriaco</forename><surname>Valdez-Flores</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">M</forename><surname>Feldman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Naval Research Logistics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="419" to="446" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b154">
	<analytic>
		<title level="a" type="main">Error bounds for convolutional codes and an asymptotically optimum decoding algorithm</title>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">J</forename><surname>Viterbi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="260" to="269" />
			<date type="published" when="1967">1967</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b155">
	<analytic>
		<title level="a" type="main">Graphical models, exponential families, and variational inference</title>
		<author>
			<persName><forename type="first">Martin</forename><forename type="middle">J</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends in Machine Learning</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="305" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b156">
	<analytic>
		<title level="a" type="main">Column generation algorithms for constrained POMDPs</title>
		<author>
			<persName><forename type="first">Erwin</forename><surname>Walraven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthijs</forename><surname>Spaan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="page" from="489" to="533" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b157">
	<analytic>
		<title level="a" type="main">Point-based value iteration for finite-horizon POMDPs</title>
		<author>
			<persName><forename type="first">Erwin</forename><surname>Walraven</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename><surname>Matthijs</surname></persName>
		</author>
		<author>
			<persName><surname>Spaan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="page" from="307" to="341" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b158">
	<analytic>
		<title level="a" type="main">A survey of maintenance policies of deteriorating systems</title>
		<author>
			<persName><forename type="first">Hongzhou</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European Journal of Operational Research</title>
		<imprint>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="469" to="489" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b159">
	<analytic>
		<title level="a" type="main">Chmm for tool condition monitoring and remaining useful life prediction</title>
		<author>
			<persName><forename type="first">Mei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Advanced Manufacturing Technology</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="463" to="471" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b160">
	<analytic>
		<title level="a" type="main">A model-based prognostics method for fatigue crack growth in fuselage panels</title>
		<author>
			<persName><forename type="first">Yiwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Gogu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Binaud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Bes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chinese Journal of Aeronautics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="396" to="408" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b161">
	<analytic>
		<title level="a" type="main">Restless bandits: Activity allocation in a changing world</title>
		<author>
			<persName><forename type="first">P</forename><surname>Whittle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Applied Probability</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="287" to="298" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b162">
	<analytic>
		<title level="a" type="main">Robust Markov decision processes</title>
		<author>
			<persName><forename type="first">Wolfram</forename><surname>Wiesemann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Kuhn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Berç</forename><surname>Rustem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics of Operations Research</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="page" from="153" to="183" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b163">
	<analytic>
		<title level="a" type="main">Experimental results on learning stochastic memoryless policies for partially observable Markov decision processes</title>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Satinder</surname></persName>
		</author>
		<author>
			<persName><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eleventh Conference on Advances in Neural Information Processing Systems</title>
		<meeting>the Eleventh Conference on Advances in Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="1073" to="1080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b164">
	<analytic>
		<title level="a" type="main">Weakly coupled dynamic program: Information and Lagrangian relaxations</title>
		<author>
			<persName><forename type="first">Fan</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Helin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Enlu</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Automatic Control</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="698" to="713" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b165">
	<analytic>
		<title level="a" type="main">Solving multistage influence diagrams using branch-and-bound search</title>
		<author>
			<persName><forename type="first">Changhe</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojian</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Hansen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-sixth Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the Twenty-sixth Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="691" to="700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b166">
	<analytic>
		<title level="a" type="main">Apache spark: A unified engine for big data processing</title>
		<author>
			<persName><forename type="first">Matei</forename><surname>Zaharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reynold</forename><forename type="middle">S</forename><surname>Xin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Wendell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tathagata</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Armbrust</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ankur</forename><surname>Dave</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangrui</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Rosen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shivaram</forename><surname>Venkataraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Franklin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="56" to="65" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b167">
	<monogr>
		<title level="m" type="main">Planning in stochastic domains: Problem characteristics and approximation</title>
		<author>
			<persName><forename type="first">Nevin</forename><forename type="middle">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenju</forename><surname>Liu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
		<respStmt>
			<orgName>The Hong Kong University of Science and Technology</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
