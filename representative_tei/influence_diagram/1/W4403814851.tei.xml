<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">VARIATIONAL AUTO-ENCODER BASED SOLUTIONS TO INTERACTIVE DYNAMIC INFLUENCE DIAGRAMS</title>
				<funder ref="#_4e23rd2">
					<orgName type="full">Natural Science Foundation of Fujian Province, China</orgName>
				</funder>
				<funder ref="#_Cehq3B4">
					<orgName type="full">Guangdong Province, China</orgName>
				</funder>
				<funder ref="#_Eu24Pk2 #_nBSuPQH #_6PuFxNY">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-10-01">October 1, 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">A</forename><surname>Preprint</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Biyang</forename><surname>Ma</surname></persName>
						</author>
						<author role="corresp">
							<persName><forename type="first">Yifeng</forename><surname>Zeng</surname></persName>
							<email>yifeng.zeng@northumbria.ac.uk</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Yinghui Pan National Engineering Laboratory for Big Data System Computing Technology Shenzhen University Shenzhen</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Minnan Normal University Zhangzhou</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer and Information Sciences</orgName>
								<orgName type="institution">Hanyi Zhang National Engineering Laboratory for Big Data System Computing Technology Shenzhen University Shenzhen</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution">Northumbria University</orgName>
								<address>
									<settlement>Newcastle</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">VARIATIONAL AUTO-ENCODER BASED SOLUTIONS TO INTERACTIVE DYNAMIC INFLUENCE DIAGRAMS</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-10-01">October 1, 2024</date>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:52+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Decision-making</term>
					<term>Multi-agent Systerm</term>
					<term>Variational Auto-encoder</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Addressing multiagent decision problems in AI, especially those involving collaborative or competitive agents acting concurrently in a partially observable and stochastic environment, remains a formidable challenge. While Interactive Dynamic Influence Diagrams (I-DIDs) have offered a promising decision framework for such problems, they encounter limitations when the subject agent encounters unknown behaviors exhibited by other agents that are not explicitly modeled within the I-DID. This can lead to sub-optimal responses from the subject agent. In this paper, we propose a novel data-driven approach that utilizes an encoder-decoder architecture, particularly a variational autoencoder, to enhance I-DID solutions. By integrating a perplexity-based tree loss function into the optimization algorithm of the variational autoencoder, coupled with the advantages of Zig-Zag One-Hot encoding and decoding, we generate potential behaviors of other agents within the I-DID that are more likely to contain their true behaviors, even from limited interactions. This new approach enables the subject agent to respond more appropriately to unknown behaviors, thus improving its decision quality. We empirically demonstrate the effectiveness of the proposed approach in two wellestablished problem domains, highlighting its potential for handling multi-agent decision problems with unknown behaviors. This work is the first time of using neural networks based approaches to deal with the I-DID challenge in agent planning and learning problems.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Interactions between intelligent agents operating in a shared and uncertain environment amplify the overall system's uncertainty, posing formidable challenges for efficient modeling and decision-making in multi-agent systems. Consequently, the actions of these agents mutually influence each other, necessitating the comprehensive consideration of both environmental dynamics and potential action sequences of other agents in order to make optimal decisions <ref type="bibr" target="#b38">[39]</ref>. This issue is commonly referred to as multi-agent sequential decision making under uncertainty <ref type="bibr" target="#b27">[28]</ref>.</p><p>Over the years, various decision models have been proposed to tackle multi-agent sequential decision making (MSDM) problems, including decentralized partially observable Markov decision processes (POMDPs) <ref type="bibr" target="#b35">[36]</ref>, interactive POMDPs <ref type="bibr" target="#b34">[35]</ref>, and interactive dynamic influence diagrams (I-DIDs) <ref type="bibr" target="#b33">[34]</ref>. Among these, I-DIDs stand out for their ability to model both collaborative and competitive agents, as well as their computational advantages <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33]</ref>. Furthermore, their probabilistic graphical model structure makes them inherently explainable in reasoning about the behaviors of other agents.</p><p>To solve I-DIDs, knowledge-based approaches have been explored to provide the subject agent with more information about other agents <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b23">24]</ref>. However, these approaches may be limited by environmental complexity, incomplete data, and uncertainty, resulting in inaccurate or limited information. One approach to address this is by increasing the number of candidate models for other agents, but this can lead to a prohibitively large model set, increasing computational complexity <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b28">29]</ref>. As an alternative, data-driven methods have been proposed that leverage historical behavioral data of other agents to learn their potential decision models, enhancing the adaptability and interpret-ability of the decision models <ref type="bibr" target="#b6">[7]</ref>.</p><p>However, data-driven decision modeling confronts two significant challenges: scarcity or constraints in data availability and inadequacy or bias in data quality <ref type="bibr" target="#b42">[43]</ref>. Limited or biased historical data originating from other agents frequently fails to accurately capture their genuine models. Moreover, inadequate or restricted data sampling hinders the development of comprehensive intent models, ultimately leading to a loss of information from the original historical decision sequences. Consequently, these factors can contribute to sub-optimal or non-generalizable decision models for the subject agent.</p><p>In this paper, we propose a novel data-driven framework to learn the true policy model of other agents from limited historical interaction data. We extract historical behavior characteristics to learn a new set of incomplete behavior models from interaction sequences. To enhance modeling accuracy, we adapt the Variational Autoencoder (VAE) <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b17">18]</ref> to generate a collection of models that can encompass the true models of other agents from an incomplete set of policy trees. This new approach leverages information from incomplete policy trees that are often discarded or approximated by traditional methods. We develop a perplexity-based metric to quantify the likelihood of including true behavior models and select optimized top-K behaviors from the large candidate set. Our contributions are summarized as follows:</p><p>â€¢ We propose the VAE-based algorithm to address challenges in data-driven I-DIDs, resulting in VAE-enabled behaviors. This paves the way for future research on neural networks in MSDM problems.</p><p>â€¢ We develop the Zig-Zag One-Hot (ZZOH) encoding and decoding technique, tailored specifically for policy trees, which empowers the VAE to efficiently handle both complete and incomplete policy trees as input data, thus enhancing its versatility.</p><p>â€¢ We analyze the quality of I-DID solutions using a novel perplexity-based metric, providing insights for algorithm fine-tuning and confidence in the performance.</p><p>â€¢ We conduct empirical comparisons to state-of-the-art I-DID solutions in two problem domains and investigate the potential of the novel algorithms.</p><p>The remainder of this paper is organized as follows. Section 2 reviews related works on solving I-DIDs. Section 3 provides background knowledge on I-DIDs and VAE. Section 4 presents our approach to generating behaviors of other agents in I-DIDs. Section 5 shows the experimental results by comparing various I-DID solutions. Finally, we conclude the research and discuss future work in Section 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>This section reviews the current research and emerging trends in three key areas: neural computing-based decisionmaking modeling, modeling of unknown agent behaviors, and I-DID models, along with related data-driven decisionmaking approaches.</p><p>Neural computing techniques, particularly deep reinforcement learning (DRL), have attained significant progress in addressing multi-agent decision-making challenges <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15]</ref>. DRL, which combines deep learning and reinforcement learning, facilitates end-to-end learning control but encounters limitations in sparse rewards, limited samples, and multiagent environments. Recent advancements in hierarchical, multi-agent, and imitation learning, along with maximum entropy-based methods, offer promising research directions <ref type="bibr" target="#b16">[17]</ref>. Investigations such as Kononov and Maslennikov's recurrent neural networks trained through reinforcement learning <ref type="bibr" target="#b15">[16]</ref> and Zhang et al.'s method for generating natural language explanations for intelligent agents' behavior <ref type="bibr" target="#b10">[11]</ref>, demonstrate the potential of DRL. Additionally, MO-MIX and other DRL approaches enable multi-agent cooperation across diverse domains <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b39">40]</ref>.</p><p>To emulate human-level intelligence and handle unexpected agent behaviors, neuro-symbolic multi-agent systems are emerging. These systems leverage neural networks' ability to extract symbolic features from raw data, combined with sophisticated symbolic reasoning mechanisms. Techniques like agent-based models (ABMs) <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b11">12]</ref>, datadriven decision-making <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b2">3]</ref>, and large language models (LLMs) <ref type="bibr" target="#b9">[10]</ref> are being explored. An et al. <ref type="bibr" target="#b4">[5]</ref> proposed a reinforcement learning and CNN-based approach for agents to self-learn behavior rules from data, promoting the integration of ABMs with data science and AI. Wang et al. <ref type="bibr" target="#b40">[41]</ref> presented optimal decision-making and path planning for multi-agent systems in complex environments using mean-field modeling and reinforcement learning for unmanned aerial vehicles. Wason et al. <ref type="bibr" target="#b8">[9]</ref> and Nascimento et al. <ref type="bibr" target="#b7">[8]</ref> explored integrating large language models into multi-agent systems, highlighting their human-like capabilities but also their distinctiveness, agent autonomy, and decision-making in dynamic environments.</p><p>Similarly, several I-DID solutions have relied solely on behavioral and value equivalences to constrain the model space for other agents, presuming that the true behaviors of those agents fall within the subject agent's modeling capabilities <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b32">33]</ref>. However, this approach falls short in fully accounting for unexpected or novel behaviors. Pan et al. <ref type="bibr" target="#b5">[6]</ref> proposed a genetic algorithm-based framework that incorporates randomness into opponent modeling, thus generating novel behaviors for agents, thereby demonstrating the significant potential of evolutionary approaches.</p><p>In the context of data-driven I-DIDs <ref type="bibr" target="#b6">[7]</ref>, the objective is to optimize multi-agent decision-making in uncertain environments, a significant challenge in AI research. Our work in this article pioneers a novel approach to modeling other agents in I-DIDs, exploring neural computing-based data-driven methods to approximate real agent behavior models from historical data in agent planning research.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Preliminary Knowledge</head><p>As we will adapt the VAE-based data generation methods to augment the I-DID model with novel metrics for other agents and generate novel behaviors, we provide background knowledge on I-DID and VAE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Background knowledge on I-DIDs</head><p>The traditional influence diagram, designed for single-agent decisions, has evolved into the I-DID framework. This framework is a probabilistic graph tailored for interactive multi-agent decision-making under partial observability. From the agent i's perspective, the I-DID predicts agent j's actions, aiding agent i in optimizing its decisions. We focus on agent i and integrate agent j's potential behaviors into the decision framework.</p><p>We introduce the dynamic influence diagram (DID) for a single agent in Fig. <ref type="figure" target="#fig_0">1</ref>(a). This DID represents the agent's decision process over three time steps, with solutions shown in Fig. <ref type="figure" target="#fig_0">1(b</ref>). In the DID, ovals represent either chance nodes for environmental states (S) or observations (O); rectangles are decision nodes for the agent's actions (A); diamonds denote utility nodes capturing rewards (R). At time t, the agent's decision (A t ) is influenced by the current observation (O t ) and the previous decision (A t-1 ). The observation (O t ) depends on the previous decision (A t-1 ) and the current state (S t ). The states (S t ) are partially observable and influenced by the previous states (S t-1 ) and decision (A t-1 ). Rewards (R t-1 ) are determined by a utility function considering both the states and decisions. Arcs model conditional probabilities among the connected nodes. For example, if O t is influenced by A t-1 and S t , the arrows from A t-1 and S t to O t indicate this dependency. Similarly, if S t is affected by A t-1 and transitions from S t-1 , the arrows from A t-1 and S t-1 to S t show the state transition. The DID demonstrates how the agent optimizes its decisions in the response to the changing environment. After defining the transition, observation, and utility functions, we use traditional inference algorithms to solve the model and get the agent's optimal policy. This policy is represented by a policy tree (see Fig. <ref type="figure" target="#fig_0">1(b)</ref>). At time t = 1, the agent takes action a 1 and the subsequent actions based on observations. The paths in the tree correspond to the observations, while nodes represent the agent's chosen actions. In different domains, the number of the paths and node types may vary. The policy tree, as a DID solution, encapsulates the model's optimal policy (behavioral model). It's a full k-ary tree, where k is the number of possible observations.</p><p>In the I-DID model, a hexagonal node, known as the model node M j , dynamically extends the behavioral models of other agents in the influence diagram. This node contains potential behavioral models of agent j, which are then provided to the subject agent i, transforming the complex I-DID problem into a conventional DID. However, as the number of time slices increases, managing an extensive set of candidate models becomes intractable. Compression and pruning of model nodes are often necessary before integrating them into the DID, but this can result in nodes with similar behavioral models, leading to a limited set of optimal decisions for agent i.</p><p>In this article, to address the challenges posed by a large number of time slices and selecting vast candidate models, we use a variational autoencoder (VAE) to learn from agent j's historical trajectory. The new method generates a set  Figure <ref type="figure">2</ref>: By extending DID model (the blue part), the agent i optimises its decision in the I-DID models with the blue part which models agent j's decision making process.</p><p>of highly reliable policy trees, capturing key features of the trajectory. By embedding the potentially true behavioral model of agent j into the I-DID, we enable agent i's optimal policy to consider the most probable behaviors of agent j.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Background knowledge on VAE</head><p>Variational Autoencoders (VAEs) <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b37">38]</ref> are a type of unsupervised learning model that combines the capabilities of autoencoders with probabilistic generative models. Unlike traditional autoencoders <ref type="bibr" target="#b29">[30]</ref>, which encode inputs into a fixed latent representation, VAEs map the inputs to a probability distribution in the latent space. The key idea behind VAEs is to encode input data into the parameters of a latent distribution, typically a Gaussian distribution. The encoder network predicts the mean (Âµ Ï• ) and variance (Ïƒ 2 Ï• ) of this distribution, while the decoder network reconstructs the original input from latent samples. To enable differentiable sampling from the latent distribution, VAEs employ the reparameterization trick. This involves sampling random noise from a standard normal distribution and scaling it according to the predicted mean and variance to obtain a latent sample (z âˆˆ R l ). The VAE's objective function consists of two parts: a reconstruction loss that measures the similarity between the original input and the reconstructed output, and a Kullback-Leibler (KL) divergence that regularizes the latent distribution to be close to a prior distribution (e.g., a standard normal distribution). Optimizing this objective function leads to encoder and decoder networks that can encode meaningful representations of the input data into the latent space. Formally, for a given input x from dataset D, the VAE loss function is defined as:</p><formula xml:id="formula_0">L vae Ï•,Î¸ (x) = L KL Ï•,Î¸ (P Ï• (z | x), P Î¸ (z)) + L recon Ï•,Î¸ (x)<label>(1)</label></formula><p>where L KL Ï•,Î¸ is the KL divergence between the posterior distribution P Ï• (z | x) and the prior distribution P Î¸ (z), and L recon Ï•,Î¸ is the reconstruction loss. Over the entire dataset D, the VAE loss function is given by:</p><formula xml:id="formula_1">L vae Ï•,Î¸ (D) = xâˆˆD L vae Ï•,Î¸ (x)<label>(2)</label></formula><p>By minimizing the loss function through the techniques like a stochastic gradient descent, the VAE learns to encode inputs into a latent space that captures their essential features while enabling the generation of new data samples that follow the distribution of the original input data, where we can apply stochastic gradient descent (SGD) <ref type="bibr" target="#b37">[38]</ref> to find the optimized parameters Ï• * and Î¸ * .</p><p>Policy tree generation, often referred to as learning agents' behavioral models, involves the automated construction of a behavioral representation in the form of a policy tree from extensive multi-agent interaction data. The behavioral model aims to predict the actions an agent will take given observations from the environment, which is typically represented as a complete multi-fork tree, as depicted in Fig. <ref type="figure" target="#fig_0">1</ref>.</p><p>From the perspective of agent i, it is not necessary to understand how agent j optimizes its behavior but rather to predict how agent j will act. In the I-DID model, agent i is provided with m policy trees representing agent j's behavior, where these trees shall be complete. However, two challenges arise:</p><p>â€¢ Many policy trees generated directly from historical behavior sequences are not fully developed -they are incomplete. â€¢ The generated policy trees may not accurately reflect the true behavioral model of agent j.</p><p>Addressing these issues is crucial for the effective utilization of the policy trees in predicting and understanding multi-agent interactions. To address the first issue, there are two traditional methods, but both have their limitations <ref type="bibr" target="#b1">[2]</ref>. Discarding policy trees with missing nodes is problematic because it risks excluding significant policy sequences that may contain valuable information from the original historical data. This approach is only feasible when data is abundant, but in most cases, data is limited. The alternative method, based on statistical analysis, estimates the probability of each action in the current time slice using the actions from the previous time slice and the current observations. Then, a roulette wheel selection process is employed to randomly fill in the missing actions in the policy tree. However, this approach has its shortcomings as well. The actions at different levels in the policy tree represent distinct contexts. For instance, the action a 4 at time slice t = 4 is influenced not only by the action a 3 and its corresponding observation, but also by the actions and observations at earlier time slices such as t = 1 and t = 2. Consequently, the high confidence in taking action a 2 at t = 2 does not necessarily translate to the high probability of taking the same action at t = 4.</p><p>To tackle the second issue of potential incompleteness in modeling agent j's behaviors, we endeavor to gather a broader and more diverse collection of behavioral models. The greater the diversity within this set, the higher the probability that it will capture the full spectrum of agent j's actual behavior patterns. This approach ensures that our models are comprehensive and representative, increasing their reliability and adaptability.</p><p>To that end, we develop a new framework capable of generating a comprehensive ensemble of policy trees from agents' interaction data. This framework not only reconstructs policy trees accurately but also guarantees the diversity within the resulting set. To tackle these challenges, we develop a policy generation method leveraging variational autoencoder techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Reconstructing an Incomplete Policy Tree</head><p>Given the limitation that agents often cannot store a large number of policy trees to navigate complex multi-agent interactive environments, the agent must repeatedly traverse the policy from the root down during interactions. In a simple term, from the subject agent's perspective, we can construct potentially incomplete policy trees for other agents solely based on a single sequence of interaction data.</p><p>To obtain a long sequence of agent j's action-observations, we control the interaction between agent i and the environment. This sequence is denoted as</p><formula xml:id="formula_2">h L = (a 1 o 1 , a 2 o 2 , . . . , a t o t , . . . , a L o L )</formula><p>, where a t âˆˆ A and o t âˆˆ â„¦ represent an alternating series of actions and observations at time slice t. The parameter L is the total number of time slices in this interaction.</p><p>Given the interaction data h L , the depth of the policy tree (denoted as T ), and the number of agent j's models (denoted as m), we can reconstruct a set of possible incomplete policy trees H T of the fixed depth T (T â‰ª L) using four operators: split, union, roulette, and graphing. As illustrated in Fig. <ref type="figure" target="#fig_2">3</ref>, these operators enable us to generate the desired set of the policy trees, and the implementation of these operators is described in Alg. 3 of the Appendix.</p><p>Utilizing the split operator (denoted as S in Fig. <ref type="figure" target="#fig_2">3</ref> 1 âƒ), we begin at the starting point and extract sequences of actions with a fixed length T , thereby forming policy paths that commence with a specific action a. We refer to such a path as h T a . If this policy path is aligned with a sequence of observations o, it can be denoted as h T ao . Concurrently, we compute the probability of each policy path, denoted as</p><formula xml:id="formula_3">P(h T a ) â† #(h T a ) â€¢ T /L.</formula><p>Here, #(h T a ) representing the number of times h T a appears in the entire sequence h L . We repeat this operation until the end of the sequence, ultimately forming a set of policy paths H T denoted as H T â† S T (h L ).</p><p>To obtain a set of policy trees from the set of policy paths H T , we use the union operator (denoted as U in Fig. <ref type="figure" target="#fig_2">3</ref> 2 âƒ). First, we define a subset of policy paths that begin with the same action a and observations o as</p><formula xml:id="formula_4">H T ao = (h T ao ,P(h T ao ))âˆˆH T (h T ao , P(h T ao )).</formula><p>This subset contains all the policy paths that start with the same action a and follow a specific sequence of observations o. We then compute the probability of this subset by summing up the probabilities of all the policy paths within it: P(H T ao ) = (h T ao ,P(h T ao ))âˆˆH T P(h T ao ). After collecting all the subsets H T ao for every possible sequence of the observations o âˆˆ â„¦ T -1 following action a, we construct a larger set H T a that contains all the policy paths beginning with action a. This set is defined as H T a = oâˆˆâ„¦ T-1 (H T ao , P(H T ao )) and its overall probability is computed as P(H T a ) = oâˆˆâ„¦ T-1 P(H T ao ). Finally, for every action a âˆˆ A, we combine the sets H T a to form the complete set of policy paths H T , which is denoted as H T = aâˆˆA (H T a , P(H T a )). This set represents all possible policy trees of depth T that can be constructed from the original interaction sequence h L .</p><p>To generate a set of incomplete policy trees D T containing m samples, we first apply the roulette operator (denoted as R, Fig. <ref type="figure" target="#fig_2">3</ref> 3 âƒ) on the set of policy tree sets H T . This random selection process yields a single policy tree set H T a for a particular action a, represented as H T a â† RH T . Next, for each possible sequence of observations o âˆˆ â„¦ T -1 following action a, we apply the roulette operator again to select a subset of policy paths from the corresponding subset H T ao . This results in a collection of subsets, which is a possible policy tree (denoted as H T a ), defined as H T a â† oâˆˆâ„¦ T -1 RH T ao . By repeatedly applying the roulette operator in this manner, we generate m incomplete policy tree collections, {H T } m . The set is denoted as D T and represents a diverse set of partially constructed policy trees, each of which starts with a randomly selected action a and contains randomly sampled policy paths for each possible sequence of observations. When applying the graphing operator (denoted as G, Fig. <ref type="figure" target="#fig_2">3 4</ref> âƒ) to the set of policy paths H T a for the purpose of visualizing the corresponding policy tree, we represent this transformation as H T a â† GH T a . Note that, although we use the same notation H T a to refer to both the policy tree and the set of policy paths rooted at a with a length of time-slice T , the visualization process actually creates a representation of the policy tree in a graphical form. To clarify this distinction, we can explicitly state that H T a represents both the abstract policy tree structure and the concrete set of policy paths, while GH T a represents the same policy tree but in a graphical representation generated by the graphing operator G.</p><p>As depicted in Fig. <ref type="figure" target="#fig_2">3</ref>, the incomplete policy tree H T a1 is reconstructed from the historical behavior sequence h L through the application of the four operators: split, union, roulette, and graphing (Fig. <ref type="figure" target="#fig_2">3</ref> 1 âƒ-4 âƒ). Additionally, Fig. <ref type="figure" target="#fig_0">1</ref> illustrates a scenario where, after agent j observes o 2 in the first time slice and takes action a 1 , it is unable to select a subsequent action if it encounters the observation o 1 . Notably, some incomplete policy trees are inevitably created during the generation of agent j's policy trees from historical data. These incomplete policy trees, though a special type of behavior model, differ from complete policy trees in that they lack specified actions for certain observations in certain time slices. However, the I-DID model requires a complete policy tree model for its solution methodology. Therefore, an incomplete policy tree cannot be directly utilized in I-DID. Thus, we propose a VAE-based method to convert incomplete policy trees into their complete counterparts, thereby enabling their utilization within the I-DID.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Standard Policy Tree Generalization</head><p>As mentioned earlier, discarding or randomly processing incomplete policy trees can result in a significant loss of crucial information from the original historical decision sequences. To address this, we develop a VAE-based approach for generating policy trees. This method effectively leverages both complete and incomplete policy trees, enabling the production of numerous new policy trees from just a few examples. By doing so, it maximizes the coverage of agent j's true behaviors.</p><p>In this manner, our approach solves two key problems faced by traditional methods in modeling agent j's behaviors. As illustrated in Fig. <ref type="figure">4</ref>, after training the VAE with a selection of incomplete policy trees, the model is capable of generating policy trees with varying degrees of deviation, adhering closely to the distribution of the historical data. This approach significantly enhances the diversity of the overall policy tree collection, leading to a more comprehensive representation of agent j's behaviors.</p><p>Here, we introduce the Zig-Zag One-Hot (ZZOH) encoding and decoding technique specifically designed for policy trees. This technique enables the VAE to effectively handle both the complete and incomplete policy trees as input and output data. Since ZZOH encoding prioritizes serializing the policy tree structure, nodes closer to the root have a greater impact on the overall decision-making effectiveness of the entire policy tree. Therefore, we adapt the loss function of the VAE network to prioritize learning behaviors from nodes associated with earlier time slices. This approach helps capture the structural information embedded in the original policy tree. After reconstructing and filtering a large number of policy trees using the VAE, we utilize the Measurement of diversity with frames (MDF) and information confusion degree (IDF) to select the top-K policy trees from the generated set.   </p><formula xml:id="formula_5">ğ‘ ! ğ‘ ! ğ‘ " ğ‘ # ğ‘ # ğ‘œ ! ğ‘œ " ğ‘œ ! ğ‘œ " ğ‘ " ğ‘œ ! ğ‘œ " ğ‘ ! ğ‘ ! ğ‘ ! ğ‘ # ğ‘ # ğ‘ ! ğ‘œ ! ğ‘œ " ğ‘œ ! ğ‘œ " ğ‘ ! ğ‘œ ! ğ‘œ " ğ‘ " ğ‘ ! ğ‘ " ğ‘ # ğ‘ # ğ‘ " ğ‘œ ! ğ‘œ " ğ‘œ ! ğ‘œ " ğ‘ ! ğ‘œ ! ğ‘œ " ğ‘ ! ğ‘ ! ğ‘ # ğ‘ # ğ‘ # ğ‘ " ğ‘œ ! ğ‘œ " ğ‘œ ! ğ‘œ " ğ‘ " ğ‘œ ! ğ‘œ " ğ‘ # ğ‘ ! ğ‘ " ğ‘ # ğ‘ # ğ‘ " ğ‘œ ! ğ‘œ " ğ‘œ ! ğ‘œ " ğ‘ " ğ‘œ ! ğ‘œ " ğœº~ğ’©(ğŸ, ğˆ) noise compressed space</formula><p>Incomplete policy trees</p><formula xml:id="formula_6">ğ‘ ! ğ‘ ! ğ‘ # ğ‘ # ğ‘ " ğ‘œ ! ğ‘œ " ğ‘œ ! ğ‘œ " ğ‘ ! ğ‘œ ! ğ‘œ " Decoder Encoder Figure 4:</formula><p>The principle of a VAE-based approach that leverages incomplete policy trees to generate diverse new trees, maximizing coverage of agent j's true behaviors and enhancing the diversity of the overall policy tree collection.</p><p>Empirically, we verify that the behavior model of agent j generated using the VAE closely aligns with the distribution of agent j's historical data. Furthermore, we compare the diversity and credibility of the top-K policy trees. We have described the detailed process of generating more diverse policy trees using the VAE model in Fig. <ref type="figure" target="#fig_3">5</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>VAE</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Zig-Zag One-Hot encoding &amp; decoding</head><p>To serialize the policy tree and make it compatible with the VAE network, we introduce the Zig-Zag One-Hot (ZZOH) encoding and decoding technique. This approach effectively transforms both complete and incomplete policy trees into column vectors that can be utilized by the VAE network.</p><p>Considering some empty nodes in incomplete policy tree, we enlarge the action space A j = {a 1 , a 2 , ...a |Aj | } of agent j with empty action a 0 , denoted as Ãƒj = {a 0 , a 1 , a 2 , ...a |Aj | }. Then, we use one-hot encoding to encode each action of the enlarged action space Ãƒj into a binary code with the length | Ãƒj | and generate an encoding set of action space, denoted as Ãƒc j . For example, given the action space A j = {a 1 , a 2 , a 3 }, we have enlarged the action space Ãƒj = {a 0 , a 1 , a 2 , a 3 }, and the one-hot encoding set of enlarged action space</p><formula xml:id="formula_7">Ãƒc j = a 0 a 1 a 2 a 3 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1</formula><p>, and we have Ãƒc j [a 0 ] = [1; 0; 0; 0]. By encoding the action space, we define an operator Z for encoding/decoding policy trees to/from binary codes. This lossless transformation into a vector representation preserves the tree's structural information, enabling efficient processing and analysis.</p><p>Thus, for given a policy tree H T , we encode each action node into a binary code using the encoding set of enlarged action space, and then concatenate the binary codes into a column vector x in a zigzag order, denoted as ZH T . Wherein the numerical information indicates the node action information, and the node position information indicates the sequence information in front of the node and the current observation result. We repeat this operation to encode the policy tree in the given policy tree set D T and form the dataset X for training and testing the VAE network, which is denoted as ZD T , as shown in Fig. <ref type="figure" target="#fig_3">5 1</ref> âƒ. We also present the implementation of the operators in Alg. 5 of the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Tree Loss Function in VAE</head><p>The intricate structure of policy trees encapsulates critical information, including action values, sequential order, and their relative importance. Action nodes closer to the root exercise greater influence on the overall decision-making effectiveness of the tree, shaping the trajectory to all the leaf nodes. However, this structural information is not fully captured in serialized tree representations.</p><p>To leverage serialized policy tree data for VAE training and better capture structural nuances, we propose a modified VAE loss function. This refinement prioritizes the behavior learning from nodes with smaller time slices, emphasizing the structural information embedded in the original policy tree. Effectively, this approach assigns higher weights to action nodes closer to the root, ensuring that their significance is reflected in the VAE learning.</p><p>Given the prior distribution over latent variables as a centered isotropic multivariate Gaussian: P Î¸ (z) = N (z; 0, I), we define the likelihood function P Î¸ (x | z) as a multivariate Bernoulli distribution (for binary data) where the distribution parameters are determined from z using a fully-connected neural network with a single hidden layer. Since the true posterior P Î¸ (z | x) is often intractable, we employ a variational approximate posterior P Ï• (z | x) to approximate it. Specifically, we let the variational approximate posterior be a multivariate Gaussian with a diagonal covariance structure:</p><formula xml:id="formula_8">P Ï• (z | x) = N (z; Âµ Ï• , Ïƒ 2 Ï• I).</formula><p>where Âµ Ï• and Ïƒ 2 Ï• are the mean and variance vectors predicted by the encoder network parameterized by Ï•. To enable differentiable sampling from this variational posterior, we utilize the reparameterization trick. Specifically, we sample a random noise vector Îµ from a standard normal distribution N (0, I) and then transform it according to the predicted mean and variance: z = Âµ Ï• + Ïƒ Ï• âŠ™ Îµ. This re-parameterization allows gradients to flow through the sampling process, enabling the use of gradient-based optimization techniques to train the VAE.</p><p>Âµ Ï• , Ïƒ Ï• = Encoder Ï• (x)</p><formula xml:id="formula_9">Îµ âˆ¼ P(Îµ) = N (Îµ; 0, I) z = Âµ Ï• + Ïƒ Ï• âŠ™ Îµ<label>(3)</label></formula><p>We have a the decoder of VAE for Bernoulli data:</p><formula xml:id="formula_10">x = Decoder Î¸ (z) log P Î¸ (x | z) = |x| k=1 log P Î¸ (x k | z) = |x| k=1 Bernoulli(x k ; xk ) = |x| k=1 x k log xk + (1 -x k ) log(1 -xk ) = â„“(x, x)<label>(4)</label></formula><p>where â„“(x, x) denotes the Binary Cross-Entropy Loss (BCELoss) <ref type="bibr" target="#b37">[38]</ref> due to the VAE decoder for Bernoulli data, aka the basic construction error.</p><p>To utilize serialized data from the policy tree to train a VAE network and better represent the structural information and data distribution characteristics of the original policy tree, we propose the decoder of VAE for Bernoulli data with tree weights.</p><formula xml:id="formula_11">â„“ tree (x, x) = |x| k=1 w(k)(x k log xk + (1 -x k ) log(1 -xk ))<label>(5)</label></formula><p>where</p><formula xml:id="formula_12">w(k) = log (1 + h(âŒŠ k-1 N âŒ‹ + 1)), N = |x|/(|A j | + 1),âˆ€x i âˆˆ x : 0 â‰¤ xi â‰¤ 1. h(n)</formula><p>is the height of a node in the policy tree, which can be calculated as:</p><formula xml:id="formula_13">h(n) = T -c + 1, if n âˆˆ [ |â„¦| c-1 -1 |â„¦| -1 + 1, |â„¦| c -1 |â„¦| -1 ], c âˆˆ R +<label>(6)</label></formula><p>Then we have the loss function of VAE:</p><formula xml:id="formula_14">L recon Î¸,Ï• (x) = -E zâˆ¼P Ï• (z|x) log(P Î¸ (x | z)) = -E Îµâˆ¼N (Îµ;0,I) log(P Î¸ (x | Âµ Ï• + Ïƒ Ï• âŠ™ Îµ)) = - 1 2n s ns l=1 â„“ tree (x, x(l) )<label>(7)</label></formula><formula xml:id="formula_15">L KL Î¸,Ï• (x; Ï•) = L KL (P Ï• (z | x), P Î¸ (z)) = D KL (N (z; Âµ Ï• , Ïƒ 2 Ï• I) || N (z; 0, I)) = 1 2 |Âµ Ï• | k=1 1 + log Ïƒ 2 Ï•,k -Âµ 2 Ï•,k -Ïƒ 2 Ï•,k = L KL Ï• (x; Ï•)<label>(8)</label></formula><formula xml:id="formula_16">L Î¸,Ï• (x) = L recon Î¸,Ï• (x) + L K L Ï• (x)<label>(9)</label></formula><p>where n s is the batch learning sampling size, x = f Î¸ (g Ï• (x)) is the output vector of VAE network, the operator âŠ™ is a</p><p>Algorithm 1: Learning a tree-loss based VAE network through SGD (@VAE) Data: Dataset X = {x} m , learning rate Î±, batch size n b , sampling size n s .</p><p>Result: Learned network Net vae Î¸,Ï• (â€¢) with parameters Î¸ and Ï• Initialize: Variational network parameters Î¸ and Ï• randomly repeat</p><formula xml:id="formula_17">Sample a batch of n b data X = {x} n b from X for x âˆˆ X do Âµ, Ïƒ â† Encoder Ï• (x) for l âˆˆ {1, 2, . . . , n s } do Generate noise Îµ âˆ¼ N (0, I) 8 z â† Âµ + Ïƒ âŠ™ Îµ 9 x(l) â† Decoder Î¸ (z)</formula><p>end Compute the loss function:</p><formula xml:id="formula_18">L recon Î¸,Ï• (x) â† -1 2ns ns l=1 â„“ tree (x, x(l) ) L KL Ï• (x) â† 1 2 |Âµ Ï• | k=1 1 + log Ïƒ 2 Ï•,k -Âµ 2 Ï•,k -Ïƒ 2 Ï•,k<label>end</label></formula><p>Compute the gradients of the loss function with respect to the network parameters: âˆ‡ Î¸ x L recon Î¸,Ï• (x)</p><formula xml:id="formula_19">âˆ‡ Ï• x L recon Î¸,Ï• (x) + L KL Ï• (x)</formula><p>Update parameters using gradient descent:</p><formula xml:id="formula_20">Î¸ â† Î¸ -Î±âˆ‡ Î¸ x L recon Î¸,Ï• (x) Ï• â† Ï• -Î±âˆ‡ Ï• x L recon Î¸,Ï• (x) + L K L Ï• (x) until Convergence;</formula><p>element-wise product of two vector and removes all the zero-elements.</p><p>Subsequently we apply stochastic gradient descent algorithm (SGD) with the batch learning to train the VAE network, minimizing the modified reconstruction loss. First, we compute the gradients of the loss function with respect to the network parameters: âˆ‡ Î¸ x L recon Î¸,Ï• (x) and âˆ‡ Ï• x L recon Î¸,Ï• (x) + L KL Ï• (x) . Then, we update parameters using gradient descent:Î¸ â† Î¸ -Î±âˆ‡ Î¸ x L recon Î¸,Ï• (x) and Ï• â† Ï• -Î±âˆ‡ Ï• x L recon Î¸,Ï• (x) + L K L Ï• (x) . After the training process is completed, the original policy tree is reconstructed to expand and obtain the complete policy tree, as shown in Fig. <ref type="figure" target="#fig_3">5</ref>. During the training phase of VAE, we will train the VAE network parameters to learn the distribution of the behaviors from historical policy trees that may have some incomplete policy trees. In the testing, we reconstruct the input policy tree to obtain a complete policy tree, including the selection probabilities for each node.</p><p>We outline the VAE training in Alg. 1. We initialize the VAE parameters Î¸ and Ï• and choose a data batch of size n b (lines 1-2). Then we encode each datum to get the compressed variable statistics and decode with the noise to reconstruct (lines 5-10). Subsequently we adjust and compute the VAE loss (lines 11-13), calculate the gradients, update the parameters, and repeat the procedures until the parameter values become stable (lines 15-20). After the network parameters are finalized, we input the given policy trees H T into the trained VAE network. This generates a new vector x representing a policy tree as outputs. Applying the One-hot encoding operator I(x) transforms a probability vector to one-hot binary vector for each action node, which means selecting the action with the highest probability in the corresponding chosen vector for the node, presented in Alg. 4. Then, we apply the ZOOH decoding operator to produce a policy tree Z x, ensuring that the resulting policy tree is complete and valid. We repeat this process until we obtain M complete policy trees (Fig. <ref type="figure" target="#fig_3">5</ref> 3 âƒ), denoted as ÄT = {Z x} M .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Evaluation Index</head><p>After reconstructing a significant number of policy trees, we need to filter out those that do not accurately represent agent j's true behavior. To achieve this, we leverage the measurement of diversity with frames (MDF) <ref type="bibr" target="#b5">[6]</ref> and introduce a novel metric called information confusion degree (ICD). These metrics are to select the top-K policy trees from the generated set.</p><p>MDF evaluates the diversity across both the vertical and horizontal dimensions: the vertical refers the variations in behavior sequences within each policy tree, and the horizontal explores the sequence differences across all potential observations at a time step.</p><formula xml:id="formula_21">d { Ä¤T } K MDF = T t=1 Diff (h t ) + Diff (H t ) |â„¦ j | t-1<label>(10)</label></formula><p>where Dif f (h t ) and Dif f (H t ) count the different sequences h t and sub-trees (frames) H t respectively in D T K = { Ä¤T } K . |â„¦ j | denotes the number of agent j's observations. Additionally, we introduce ICD to evaluate the reliability of the policy tree data. This metric guarantees a high confidence for each node in the reconstructed tree. In the VAE-generated tree, nodes act as classifiers, choosing the most probable action. A small margin between the top action's probability and the rest leads to high information confusion, indicating unclear or unreliable decisions. We want the ICD value of the policy tree to reach the maximum value, and select the top-K tree as the output.</p><formula xml:id="formula_22">d { Ä¤T } K ICD = - Ä¤T âˆˆ{ Ä¤T } K N n=1 log(1 + h(n)) * p n log(p n )<label>(11)</label></formula><p>where p n is the probability of action under corresponding observation and N is the number of action nodes.</p><p>By selecting the top-K policy trees, we aim for the maximum diversity or minimal information confusion among the generated trees. Since MDF captures the diversity of the overall behaviors and the ICD of these trees reflects their true distribution in the historical data, we conduct the top-K selection using both MDF and ICD.</p><formula xml:id="formula_23">{ Ä¤T } K = argmax { Ä¤T } K âŠ‚ ÄT d { Ä¤T } K<label>(12)</label></formula><p>where d (â€¢) is calculate by either Eq. 11 or Eq. 10.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">The Framework of the VAE-based Policy Tree Generation</head><p>We elaborate the generation of the diverse policy trees using the VAE in Alg. 2. As depicted in Fig. <ref type="figure" target="#fig_2">3</ref> and Fig. <ref type="figure" target="#fig_3">5</ref>, the key steps are as follows.</p><p>Initially, the process begins by reconstructing a set of incomplete policy trees, denoted as D T , from an agent j's interactive history h L . This reconstruction is achieved through a series of operations including split, union, roulette, and graphying, which serve to extract and organize policy trees from the historical data (lines 1-9, Fig. <ref type="figure" target="#fig_2">3</ref> </p><formula xml:id="formula_24">1 âƒ-4 âƒ).</formula><p>Subsequently, a ZZOH encoding operator is applied to each policy tree in D T , converting them into a binary vector representation. This encoded dataset, denoted as X, is then used to train and test the VAE network (line 10, Fig. <ref type="figure" target="#fig_3">5</ref> 1 âƒ). Utilizing stochastic gradient descent (SGD) and a tree-specific loss function, the VAE network learns to capture the latent representations of the policy trees (line 11, Fig. <ref type="figure" target="#fig_3">5 2</ref> âƒ).</p><p>Once the VAE network has been trained, it is ready to generate more complete policy trees (lines 12-18, Fig. <ref type="figure" target="#fig_3">5</ref> 3 âƒ). This is achieved by randomly selecting a binary vector from the original dataset X and feeding it into the VAE network. The network then generates a new binary vector, which is decoded using One-hot encoding and the ZZOH decoding operator to reconstruct a complete policy tree (lines <ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref>. This process is repeated until a desired number of complete policy trees are obtained (lines <ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref>.</p><p>Finally, from the generated set of policy trees ÄT , the most diverse or reliable behaviors are selected using metrics such as the MDF and ICD. These metrics allow us to identify the K policy trees that exhibit the highest degree of diversity or reliability, enabling the selection of behaviors that are the best suited for a given task or scenario (line 19, Fig. <ref type="figure" target="#fig_3">5 4</ref> âƒ).</p><p>Algorithm 2: VAE-Enabled behaviors (@V EB)</p><p>Data: agent j's interactive history h L , learning rate Î±, action set A and observation set â„¦, the number of policy trees m, the number of generated policy trees M, the evaluation index function d(â€¢), the parameter of top-K selection function K Result: New K policy trees { Ä¤T } K H T â† S T h L . â—Applying split operator; H T â† UH T . â—Applying union operator;</p><formula xml:id="formula_25">D T â† âˆ… for k âˆˆ {1, 2, â€¢ â€¢ â€¢ m} do H T a â† RH T . â—Applying roulette operator; H T a â† âˆª oâˆˆâ„¦ T -1 RH T ao . â—Applying roulette operator;</formula><p>H T a â† GH T a . â—Applying graphing operator;</p><formula xml:id="formula_26">D T â† D T âˆª H T a end X â† âˆª H T âˆˆD T ZH T . â—Applying ZZOH operator; Net vae Î¸,Ï• (â€¢) â† VAE(X ). â—Learning VAE with tree loss; ÄT â† âˆ… for k âˆˆ {1, 2, â€¢ â€¢ â€¢ M} do x â† RX . â—Applying roulette operator;</formula><p>x â† Net vae Î¸,Ï• (x). â—Generate data via VAE;</p><p>x â† I(x). â—Applying One-hot encoding;</p><formula xml:id="formula_27">ÄT â† ÄT âˆª Zx â—Applying ZZOH operator; end { Ä¤T } K â† argmax { Ä¤T } K âŠ‚ ÄT d { Ä¤T } K â—Applying top-K operator;</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Results</head><p>We improve the I-DID by incorporating the VAE-based method to generate and select the diverse and representative behaviors for agent j. We conduct the experiments in two well-studied multi-agent problem domains: the multi-agent tiger problem (Tiger) and the multi-agent unmanned aerial vehicle (UAV) problem <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b28">29]</ref>. All the experiments are performed on a Windows 10 system with an 11-th Gen Intel Core i7-6700 CPU @ 3.40GHz (4 cores) and 24GB RAM.</p><p>In the multi-agent context, we focus on a general scenario with two agents, considering agent i as the subject and constructing an I-DID model for the problem domains. The M j nodes in this model represent the behavioral models of agent j, as shown in Fig. <ref type="figure">2</ref>. Our proposed approach optimizes the I-DID model by providing agent i with a set of agent j's representative behaviors. We compare seven algorithms for solving the I-DID model in the experiments:</p><p>â€¢ The classic I-DID algorithm, which relies solely on known models M to expand the model nodes, assuming the true behavior of agent j lies within these nodes <ref type="bibr" target="#b46">[47]</ref>. â€¢ The genetic algorithm based algorithm (IDID-GA) generates agent j's behaviors through genetic operations <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b5">6]</ref> . â€¢ The IDID-MDF <ref type="bibr" target="#b5">[6]</ref> and IDID-VAE-MDF algorithms, which utilize VAE methods to generate new data from agent j's historical data, and then employ the MDF metric to select the top-k behaviors of agent j. â€¢ The IDID-Random, IDID-VAE-MDF, and IDID-VAE-ICD algorithms, which differ in how they select the top-K behaviors from the policy trees generated by VAE. IDID-Random selects the behaviors randomly, while IDID-VAE-MDF and IDID-VAE-ICD utilize MDF and ICD respectively and IDID-VAE-BCELoss replaces the proposed tree loss with the traditional BCELoss.</p><p>All seven algorithms (IDID, IDID-MDF, IDID-VAE-MDF, IDID-GA,IDID-Random, IDID-VAE-ICD and IDID-VAE-BCELoss) use the same underlying algorithm to solve the I-DID model, differing only in how they expand the model nodes with candidate models for agent j.</p><p>To evaluate the algorithms, we consider the average rewards received by agent i during interactions with agent j. We randomly select one behavior model of agent j as its true model from the set of M j nodes. Agent i then executes its optimal I-DID policy, while agent j follows the selected behavior. In each interaction, agent i accumulates rewards based on the action results at each time step over the entire planning horizon T . We repeat the interaction 50 times and compute the average reward for agent i.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Multi-agent tiger problems</head><p>The two-agent tiger problem serves as a canonical benchmark for assessing the performance of multi-agent planning models. In Fig. <ref type="figure" target="#fig_4">6</ref>, agent i and agent j must make a choice between opening the right/left door (OR/OL) or listening (L) to determine the tiger's position, given limited visibility. If an agent opens the door with gold behind it, it will take the gold; however, if the tiger is behind the door, the agent will be eaten. Meanwhile, if both agents simultaneously open the door with the gold, they will share the reward equally. The agents' decisions are influenced by their observations, which may not always be accurate. For instance, a squeak emanating from a door could be mistaken for another agent's voice, the tiger's growl, or a misinterpretation of the sound.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Diversity and Measurements</head><p>We first explore how the top-K selection algorithm impacts the diversity of the policy tree set and investigate the relationship between the selection criteria of the policy tree. To evaluate the diversity of the top-K policy tree set for agent j generated by IDID-VAE-MDF and IDID-VAE-ICD in the Tiger problem, we conducted the experiments on the correlation between the K values and the diversity (MDF evaluation metric). As shown in the Fig. <ref type="figure">7</ref>, there is a small diversity gap between the top-K policy trees selected using the ICD index and those selected using the diversity index MDF. This verifies that the policy trees generated by VAE generally have good diversity, which makes it difficult for MDF to distinguish between policy trees and select possibly true behaviors. To explore the correlation between the average reward obtained by the subject agent i and the corresponding metrics (MDF and ICD), we select the obtained policy tree set using various evaluation metrics to decide the top-K set. The selected metrics are then normalized. Finally, we measure the average reward that agent i can achieve using this policy tree set. As shown in Fig. <ref type="figure" target="#fig_7">8</ref>, the rewards obtained by agent i increase as the corresponding metrics rise. The x-axis with the label d represents the normalized values of ICD and MDF metrics, processed using min-max normalization. Under the policy trees selected by MDF, the average rewards of agent i increase with MDF, but there seems to be an upper limit. The policy trees selected by ICD appear to be less stable. At time slice T = 4, there is an overall upward trend.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Comparison Results of Multiple I-DID Algorithms</head><p>We investigate the quality of the model, specifically the average rewards, and compare multiple I-DID algorithms. To optimize its decision-making, agent i must anticipate agent j's actions concurrently. We construct an I-DID model tailored for agent i and devise the model space M j for agent j through multiple I-DID algorithms. In Figs. 9 and 10, the results show the average rewards for agent i using various I-DID methods for T = 3 and T = 4. The original I-DID gives agent j six historical models (M = 6). We compared IDID-MDF, IDID-Random, IDID-VAE-MDF, IDID-GA, IDID-VAE-ICD, and IDID-VAE-BCELoss, picking the top 10 models for agent j. We find that IDID-MDF, IDID-VAE-MDF, IDID-GA, and IDID-VAE-ICD all beat the original I-DID. Notably, IDID-VAE-ICD and IDID-MDF do similarly well, probably because the tiger problem isn't too complex. These improved methods are better at modeling and predicting agent j's true behavior, aiding agent i's decisions. In the tests, IDID-VAE-ICD outperformed IDID-VAE-ICD-BCELoss. This is because IDID-VAE-ICD uses a specialized loss based on policy tree nodes' importance, whereas IDID-VAE-ICD-BCELoss relies on the standard binary cross-entropy loss. In addition, the VAE-based I-DID solutions outperform the IDID-GA and show their potential in generating agent j's true behaviors from the known models. The multi-agent Unmanned Aerial Vehicle (UAV) problem poses a significant challenge in the realm of multi-agent planning. As depicted in Fig. <ref type="figure" target="#fig_10">11</ref>, both UAVs, referred to as agents, have the option to move in four different directions or remain stationary. In line with realistic scenarios, the UAVs are unable to ascertain the precise positions of other agents and can only receive signals relative to each other. Here, we designate agent i as the chaser, tasked with intercepting the fleeing agent j, while agent j aims to reach a safe house. Since both agents operate concurrently, agent i requires an accurate estimation of agent j's behavior in order to successfully achieve its goal. Agent i will be rewarded if it successfully intercepts agent j before it reaches the safe house. We let chaser i use an I-DID model, providing it with potential true behavior models of agent j.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Multi-agent UAV problems</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Diversity and Measurements</head><p>Initially, we explore how the top-K selection algorithm affects the diversity of the policy tree set, delving into the correlation between policy tree selection criteria. As illustrated in Fig. <ref type="figure" target="#fig_12">12</ref>, both curves display notable fluctuations at small values of K, but they gradually rise and stabilize as K increases, ultimately resulting in a slight divergence. By examining the top-K selection function alongside experimental data from two distinct problem domains, it becomes evident that a small K value corresponds to a wide range of potential policy tree set combinations, thereby augmenting the diversity within these sets. This effect may not be immediately obvious in less complex problem domains. Nonetheless, as the complexity of the domain increases, so does the potential decision tree space. Hence, when selecting only a few policy trees from this expanded space, there is a substantial chance of encountering significant disparities among them. According to the diversity function, smaller K values are associated with a limited number of sets, restricting the potential policy tree paths and sub-trees that can be uncovered. This limitation suggests that the diversity value linked to smaller K will not surpass that of larger K values. As K increases, the final convergence value of IDID-VAE-ICD is lower than that of IDID-VAE-MDF, indicating its superiority in facilitating the selection of an appropriate policy tree set. In essence, continuously augmenting the number of potential policy trees does not inherently lead to greater diversity, nor does it ensure a higher likelihood of discovering the true behavior of agent j.  We study the correlation between agent i's average reward and two metrics: MDF and ICD. After carefully selecting the top-K policy trees from the available set using various metrics and normalizing them via min-max normalization ( d), we discover a direct correlation between these metrics and agent i's average reward, as illustrated in Fig. <ref type="figure" target="#fig_15">13</ref>. In the Tiger problem (refer to Fig. <ref type="figure" target="#fig_7">8</ref>), while the ICD metric does not strongly correlate with the average reward compared to the MDF metric, both still ensure respectable average rewards for the chosen policy trees. However, in the UAV problem, the ICD metric demonstrates a stronger correlation with average rewards than the MDF metric. This suggests that the MDF metric may not consistently identify the true policy tree, particularly given the variety of trees generated by VAE. In contrast, the ICD metric ensures both diversity and a closer alignment with the actual behavior model. This reinforces our observation that VAE-generated policy trees exhibit diversity, posing a challenge for MDF in recognizing and selecting potential real behavior models. When presented with a broad array of policy trees, the MDF metric finds it difficult to discern which ones closely align with the true distribution of agent j's behaviors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Comparison Results of Multiple I-DID Algorithms</head><p>We assess model performance by comparing the average rewards across multiple models. For agent i to improve its decisions, it must predict agent j's actions. We are tailoring an I-DID model for agent i and building a model space M j for agent j using various I-DID techniques. The objective is to measure the accuracy of these methods in modeling and predicting agent j's behavior, thereby influencing agent i's target interception success. This success is partly attributed to the loss function we proposed, which assigns greater weight to the early nodes of the policy tree in long-term decision-making, and partly attributed to the ICD, the construction index of the policy tree set. The VAE model, combined with the ICD index and our policy tree loss function, effectively constructs the authentic subspace within the vast policy tree space, demonstrating its ability to accurately generate agent j's true behaviors from historical interactive data.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Experimental Summary and Discussions</head><p>After comparing IDID-VAE-ICD and IDID-VAE-MDF, we find that within the VAE framework, evaluating policy tree diversity using MDF is less effective. This is because VAE can generate more novel paths, maintaining high diversity among new trees, which is a feature absent in the previous methods. ICD leverages the information from the VAE-based policy tree generation process, allowing for an intuitive comparison of different policy trees' ability to represent historical behavior characteristics.</p><p>Our experiments reveal that the VAE framework offers a linear relationship between tree generation speed and planning range, providing the speed increase over the previous methods, as depicted in Table <ref type="table" target="#tab_2">1</ref>. Although IDID-MDF, IDID-VAE-ICD, and IDID-GA take longer than the basic IDID due to the iterations for the new model generation, IDID-VAE-ICD proves more efficient than IDID-GA. However, since all these algorithms operate offline, their efficiency doesn't hinder the overall performance. Additionally, IDID-VAE-ICD outperforms IDID-VAE-ICD-BCELoss thanks to its customized loss function designed for policy tree node importance distribution.</p><p>Comprehensive experiments show that both IDID-MDF and IDID-VAE-ICD outperform IDID-GA and IDID in most cases. IDID-VAE-ICD consistently performs well due to the VAE's ability to balance coherence and individuality in behaviors, while IDID-MDF tends to produce monotonic behaviors. Although finding optimal parameter values remains crucial, the VAE-based I-DID solutions have demonstrated potential in modeling unexpected behaviors that classical AI approaches can't achieve. Despite the randomness in the VAE network, experimental results show stable performance in terms of average rewards, supported by our analysis. Overall, IDID-VAE-ICD exhibits strong adaptability to complex behavioral patterns, maintaining stable performance in uncertain environments, paving the way for future intelligent decision-making systems. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We explored neural computing-based I-DID methods to address unpredictable agent behaviors in planning research. Using a VAE approach, we overcame the challenge of reusing incomplete policy trees from interactive data. The VAE model not only generates numerous new policy trees, both complete and incomplete, from a limited set of initial examples, but also offers flexibility in producing varying degrees of deviation. This allows for the creation of a broader range of policy trees. Furthermore, the integration of a novel perplexity-based metric within our VAE-based approach has significantly enhanced the diversity of the overall policy tree ensemble. By maximizing the coverage of agent j's true policy models, the new method demonstrates promising potential for accurately approximating real agent behavior models from historical data. Following the VAE training with a selection of incomplete policy trees, this data-driven approach has the capability to be applied to increasingly diverse and complex domains, highlighting its adaptability and robustness.</p><p>This work represents a significant contribution to the field of agent planning research, providing a valuable framework for handling the unpredictable behaviors of other agents in multi-agent systems. It also opens the door to exploit neural networks based approaches for addressing the I-DID challenge, which has been addressed through traditional Bayesian approach in the past decade. In the future, we will improve the efficiency, accuracy, and interpret-ability of our models, thereby increasing their adaptability to the uncertainties of online interactive environments. This will be achieved through advancements in deep learning architectures and the incorporation of explainable AI techniques.</p><p>â€¢ Split Operator: The split operator S divides a sequence into multiple sub-sequences based on a fixed length, and stores them along with their probabilities in the form of a set.</p><p>-Initially, an empty set H T is initialized to store policy paths and their associated probabilities (line 4).</p><p>-The given action-observation sequence is then segmented into multiple sub-sequences of fixed length (line 5). -These sub-sequences, along with their probabilities, are stored in the set H T (lines 6-9), providing a structured representation of policy paths. â€¢ Union Operator: The union operator U combines the elements of the given set of policy paths into a set of sets of policy trees based on the grouping of their root node actions and observation pairs.</p><p>-Policy path subsets H T ao are defined and their probabilities computed, where each subset begins with a specific action a and observation sequence o (lines 15-16).</p><p>-For every possible observation sequence o that follows action a, the corresponding subsets H T ao are collected (lines 14-16).</p><p>-Sets H T a are then constructed, encompassing all policy paths that start with a particular action a (line 17). -The overall probability P(H T a ) of each H T a is determined by summing the probabilities of its constituent subsets H T ao (line 18). -Combining the sets H T a and their probabilities for all actions a âˆˆ A yields the comprehensive set of policy paths H T (lines <ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref>).</p><p>-The comprehensive set H T represents all potential policy trees of depth T that can be derived from the original interaction sequence h L (line 20). â€¢ Roulette Operator: The roulette operator R randomly selects an element from a given set.</p><p>-Given a set A of elements (a, p) where p is the probability associated with a, a random number p r between 0 and 1 is generated (line 24). -A running sum p c is maintained to track the cumulative probabilities encountered while iterating through the set A (line 25). -As soon as p c exceeds or equals p r , the corresponding element a is returned, effectively selecting an element based on its associated probability (lines 26-31). â€¢ Graphing Operator:The graphing operator G converts the set of paths from the policy tree into a tree structure within a graph model.</p><p>-An edge set E is constructed to store the sub-paths within the policy tree (lines 35-41).</p><p>-Duplicates are removed from E, ensuring that edges with the same time slice, node value (action), and edge weight (observation) are not represented multiple times (line 42). -Unique nodes from the deduplicated edge set E are then extracted and compiled into a node set V (line 44). -Finally, a directed graph G(E, V ) is generated and plotted, representing the policy tree constructed from the edge and node sets (line 45).</p><p>B: [The Operators in encoding and decoding an Incomplete Policy Tree] The ZZOH encoder and decoder operators are described in Section 4.2.1 and in Alg. 2. The operators are used to perform a zigzag transformation on the actions of a given policy tree, encoding them into a binary format using one-hot encoding to form a column vector. Conversely, it can also decode a given column vector from one-hot encoding back into actions, subsequently reconstructing the policy tree. As depicted in Alg. 5, the key steps of each operator are as follows: -Initially, we extract the action sequence from the given policy tree H T (lines 3-12).</p><p>-Subsequently, for each action in the sequence, we generate a corresponding one-hot binary code representation p (lines <ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref><ref type="bibr" target="#b15">[16]</ref>. This one-hot encoding is designed specifically for policy trees, utilizing the Zig-Zag encoding technique. â€¢ ZZOH Decoder Operator:</p><p>-Initially, given a one-hot binary code representation x, we decode it back into an action sequence p (lines 20-26). The decoding process reverses the Zig-Zag encoding, accurately reconstructing the original action sequence. -Subsequently, using the decoded action sequence p, we reconstruct the policy tree H T (lines <ref type="bibr" target="#b26">[27]</ref><ref type="bibr" target="#b27">[28]</ref><ref type="bibr" target="#b28">[29]</ref><ref type="bibr" target="#b29">[30]</ref><ref type="bibr" target="#b30">[31]</ref><ref type="bibr" target="#b31">[32]</ref><ref type="bibr" target="#b32">[33]</ref><ref type="bibr" target="#b33">[34]</ref><ref type="bibr" target="#b34">[35]</ref><ref type="bibr" target="#b35">[36]</ref><ref type="bibr" target="#b36">[37]</ref><ref type="bibr" target="#b37">[38]</ref><ref type="bibr" target="#b38">[39]</ref><ref type="bibr" target="#b39">[40]</ref>. This reconstruction ensures that the generated tree closely matches the original tree, capturing its structure and behavior.</p><p>The One-hot encoding operator is described in Section 4.3.1 and in Alg. 2. The one-hot operator I used to convert the summary vector output by the VAE network into a binary encoding format that matches the one-hot encoding of the policy tree. As depicted in Alg. 4, the key steps of each operator are as follows: Initialize a vector x to represent the one-hot encoding of the policy tree (line 3). Then, for each node in the policy tree (lines 4-8).Locate the index with the maximum value in the corresponding sub-binary representation (line 6). Represent that sub-binary encoding using the enlarged action space Ãƒj (line 7).  </p><formula xml:id="formula_28">+ âŒŠ i-</formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A dynamic influence diagram and its solutions: (a)the left is the dynamic influence diagram with three time steps and (b) the right is its solution represented as a policy tree. The blocks with the same color in (a) and (b) belong to the same time slice.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>ğ» â† ğ’® â„ : ğ‘ ğ‘œ , ğ‘ ğ‘œ , ğ‘ ğ‘œ , 0.03 ğ‘ ğ‘œ , ğ‘ ğ‘œ , ğ‘ ğ‘œ , 0.03 â‹¯ graphing ğ’¢ 1 4 â„ : ğ‘ ğ‘œ , ğ‘ ğ‘œ , ğ‘ ğ‘œ , ğ‘ ğ‘œ , ğ‘ ğ‘œ , ğ‘ ğ‘œ , â‹¯ , ğ‘ ğ‘œ , ğ‘ ğ‘œ , ğ‘ ğ‘œ split ğ’®</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Reconstructing m incomplete policy trees via four operators (split,union,roulette and graphying) from behavior sequences.</figDesc><graphic coords="7,189.65,240.90,232.74,76.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>IFigure 5 :</head><label>5</label><figDesc>Figure 5: The VAE network creates new policy trees. With the Zig-Zag One-Hot encoding-decoding method designed for policy trees, VAE can handle both complete and incomplete trees, emphasizing learning from earlier nodes. By using MDF and IDF, we pick the most diverse and reliable top-K trees, matching historical agent patterns closely.</figDesc><graphic coords="8,121.52,259.38,236.80,101.26" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: A multi-agent tiger problem, being simplified to the two-agent version, where modeling agent j's behavior is to optimize agent i's decision. The problem specification follows: |S| = 2,|A i | = |A j | = 3,|â„¦ i | = 6, and |â„¦ j | = 2.</figDesc><graphic coords="13,307.17,71.91,51.25,51.29" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>4 Figure 7 :</head><label>47</label><figDesc>Figure 7: For (a) T = 3 and (b) T = 4, given different K values, the diversity of the top-K policy trees generated by IDID-VAE-MDF and IDID-VAE-ICD respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: For (a) T = 3 and (b) T = 4, the correlation between the average rewards obtained by agent i and the corresponding metrics (MDF and ICD) refers to the relationship between the rewards obtained using different models, namely IDID-VAE-MDF and IDID-VAE-ICD.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>4 Figure 9 :</head><label>49</label><figDesc>Figure 9: The average rewards are received by the subject agent i with agent j's behaviors models (generated by IDID, IDID-MDF, IDID-VAE-MDF and IDID-Random) for (a) T = 3 and (b) T = 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>4 Figure 10 :</head><label>410</label><figDesc>Figure 10: The average rewards are received by the subject agent i with agent j's behaviors models (generated by IDID-VAE-MDF, IDID-VAE-ICD, IDID-GA and IDID-VAE-BCELoss) for (a) T = 3 and (b) T = 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: In a two-agent UAV problem, agent i aims to capture agent j before agent j reaches the safe house. The problem specification follows: |S| = 81, |A i | = |A j | = 5, and |â„¦ i | = |â„¦ j | = 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: For (a) T = 3 and (b) T = 4, given different K values, the diversity of the top-K policy trees generated by IDID-VAE-MDF and IDID-VAE-ICD respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figs. 14</head><label>14</label><figDesc>Figs. 14 and 15 compare the average rewards in a multi-agent UAV setting. Among the various methods tested, including multiple IDID techniques, IDID-VAE-ICD stands out, outperforming the original I-DID and other approaches. In Figs.14 (T =3) and 15 (T = 3), IDID-G, IDID-VAE-BCELoss, and IDID-VAE-ICD surpass other models due to their capability to generate diverse policy trees, comprehensively covering the true model. Notably, in Figs.14 (T = 4) and 15 (T = 4), unlike IDID-GA and IDID-VAE-BCELoss, IDID-VAE-ICD shines in policy tree generation and selection. This success is partly attributed to the loss function we proposed, which assigns greater weight to the early nodes of the policy tree in long-term decision-making, and partly attributed to the ICD, the construction index of the policy tree set. The VAE model, combined with the ICD index and our policy tree loss function, effectively constructs the authentic subspace within the vast policy tree space, demonstrating its ability to accurately generate agent j's true behaviors from historical interactive data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: For (a) T = 3 and (b) T = 4, the correlation between the average rewards obtained by agent i and the corresponding metrics (MDF and ICD) refers to the relationship between the rewards obtained using different models, namely IDID-VAE-MDF and IDID-VAE-ICD.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>4 Figure 14 :</head><label>414</label><figDesc>Figure 14: The average rewards are received by the subject agent i with agent j's behaviors models (generated by IDID, IDID-MDF, IDID-VAE-MDF and IDID-Random) for (a) T = 3 and (b) T = 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>4 Figure 15 :</head><label>415</label><figDesc>Figure 15: The average rewards are received by the subject agent i with agent j's behaviors models (generated by IDID-VAE-MDF, IDID-VAE-ICD, IDID-GA and IDID-VAE-BCELoss) for (a) T = 3 and (b) T = 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Algorithm 3 : 39 E</head><label>339</label><figDesc>The Four Operators in Reconstructing an Incomplete Policy Tree â— split operator Function S( h L , T) â— H T â† S T h L :a 1 o 1 , a 2 o 2 ...a t , o t , ...a L o L â† h L H T â† âˆ… {h T } â† {a (l-1)T+1 o (l-1)T+1 , . . . , a lT o lT } âŒŠ L T âŒ‹ l=1 for h T âˆˆ {h T } do P(h T ) â† #(h T a )T/L â— #(h T a )indicates the number of times h T appears in the sequence count the times ofh L H T â† H T (h T , P h T ) end return H T â— union operator Function U( H T ) â— H T â† UH T : for a âˆˆ A do for o âˆˆ â„¦ T-1 do H T ao â† (h T ao ,P(h T ao ))âˆˆH T (h T ao , P(h T ao )) P(H T ao ) â† (h T ao ,P(h T ao ))âˆˆH T P(h T ao ) end H T a â† oâˆˆâ„¦ T-1 (H T ao , P(H T ao )) P(H T a ) â† oâˆˆâ„¦ T-1 P(H T ao ) end H T â† aâˆˆA (H T a , P(H T a )) return H T â— roulette operator Function R( A) â— a â† RA: p r â† random() p c â† 0 for (a, p) âˆˆ A do p c â† p c + p if p r &lt;= p c then 29 return a end end return a â— Graphing operator Function G( H T a ) â— T ree â† GH T a : E â† âˆ… for (h T , P(h T )) âˆˆ H T do a 1 o 1 , a 2 o 2 ...a t , o t , ...a T o T â† h T for t âˆˆ {1, 2, â€¢ â€¢ â€¢ T -1} do â† E (o t : a t , a t+1 ) end end E â† Deduplicate(E)â— deduplicate the edge with same time slice, same node value(action) and edge weight (observation)V â† N ode(E))T ree â† G(E, V ) return T ree Algorithm 4: One-Hot Encoding Operator â— One-hot encoding Function I( x) â— x â† I(x):x âˆˆ R (|Aj |+1) |â„¦| T -1 |â„¦|-1 for l âˆˆ {1, 2, â€¢ â€¢ â€¢ |â„¦| T -1 |â„¦|-1 } do v â† x[(l -1)(|A j | + 1) + 1 : (|A j | + 1)l] k â† argmax kâˆˆ{1,2,â€¢â€¢â€¢|Aj |+1} v[k]x[(l -1)(|A j | + 1) + 1 : (|A j | + 1)l] â† Ãƒj [k] end return xâ€¢ ZZOH Encoder Operator:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>Algorithm 5 : 1 |â„¦|- 1 p 1 |â„¦|- 1 + âŒŠ i- 1 |â„¦| (t- 1 ) âŒ‹ + 1 10p 1 |â„¦|- 1</head><label>5111111111</label><figDesc>ZZOH Encoder &amp; Decoder Operatorsâ— ZZOH encoder operator Function Z( H T ) â— x â† ZH T :â— generate action sequence of tree H T x âˆˆ R (|Aj |+1) |â„¦| T -â† {a l |a l â† 0, âˆ€l âˆˆ {1, 2, â€¢ â€¢ â€¢ |â„¦| T -1 |â„¦|-1 }} for (h T , i) âˆˆ H T do a 1 o 1 , a 2 o 2 ...a t , o t , ...a T o T â† h T for t âˆˆ {1, 2, â€¢ â€¢ â€¢ T } do l â† |â„¦| (t-1) -[l] â† a tend end â— generate one-hot binary code of action sequence pfor l âˆˆ {1, 2, â€¢ â€¢ â€¢ |â„¦| T -1 |â„¦|-1 } do x[(l -1)(|A j | + 1) + 1 : (|A j | + 1)l] â† Ãƒc j [p[l]] end return x â— ZZOH decoder operator Function Z( x) â— H T â† Zx: â— generate action sequence p of one-hot binary code x p â† {a l |a l â† 0, âˆ€l âˆˆ {1, 2, â€¢ â€¢ â€¢ |â„¦| T -1 |â„¦|-1 }} for l âˆˆ {1, 2, â€¢ â€¢ â€¢ |â„¦| T -1 |â„¦|-1 } do v â† Ãƒc j â€¢ x[(l -1)(|A j | + 1) + 1 : (|A j | + 1)l] k â† argmax kâˆˆ{1,2,â€¢â€¢â€¢|Aj |+1} v[k] p[l] â† Ãƒj [k]end â— generate policy tree from action sequence pH T â† h T for (h T , i) âˆˆ H T do a 1 o 1 ,a 2 o 2 ...a t , o t , ...a T o T â† h T for t âˆˆ {1, 2, â€¢ â€¢ â€¢ T } do l â† |â„¦| (t-1) -</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>ğŸ , ğ‘¨ ğŸ ğ‘¹ ğ‘º ğŸ‘ , ğ‘¨ ğŸ‘ Pr ğ‘¶ ğŸ ğ‘º ğŸ Pr ğ‘¶ ğŸ ğ‘º ğŸ , ğ‘¨ ğŸ Pr ğ‘¶ ğŸ‘ ğ‘º ğŸ‘ , ğ‘¨ ğŸ ğ‘·ğ’“ ğ‘º ğŸ Pr ğ‘º ğŸ ğ‘º ğŸ , ğ‘¨ ğŸ Pr ğ‘º ğŸ‘ ğ‘º ğŸ , ğ‘¨ ğŸ</figDesc><table><row><cell cols="2">ğ’• = ğŸ</cell><cell>ğ’• = ğŸ</cell><cell>ğ’• = ğŸ‘</cell><cell cols="2">ğ’• = ğŸ</cell><cell></cell><cell></cell><cell></cell></row><row><cell>ğ‘¹ ğ‘º ğŸ , ğ‘¨ ğŸ</cell><cell>ğ‘¹ ğŸ</cell><cell cols="6">ğ‘¹ ğŸ ğ‘¹ ğ‘º ğ’‚ ğŸ ğ‘¹ ğŸ‘ ğ’ ğŸ</cell><cell cols="2">ğ’ ğŸ</cell></row><row><cell>ğ‘¨ ğŸ</cell><cell></cell><cell>ğ‘¨ ğŸ</cell><cell>ğ‘¨ ğŸ‘</cell><cell cols="2">ğ’• = ğŸ</cell><cell></cell><cell></cell><cell></cell></row><row><cell>ğ‘º ğŸ</cell><cell></cell><cell>ğ‘º ğŸ</cell><cell>ğ‘º ğŸ‘</cell><cell>ğ’ ğŸ</cell><cell>ğ’‚ ğŸ‘</cell><cell>ğ’ ğŸ</cell><cell cols="2">ğ’ ğŸ</cell><cell>ğ’‚ ğŸ</cell><cell>ğ’ ğŸ</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">ğ’‚ ğŸ</cell><cell>ğ’‚ ğŸ‘</cell><cell></cell><cell></cell><cell>ğ’‚ ğŸ</cell></row><row><cell>ğ‘¶ ğŸ</cell><cell></cell><cell>ğ‘¶ ğŸ</cell><cell>ğ‘¶ ğŸ‘</cell><cell cols="2">ğ’• = ğŸ‘</cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>ğ’‚</cell><cell></cell><cell></cell><cell></cell><cell>ğ‘</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Running timesâˆ¼(sec) for all the four methods in the two problem domains generating, specifically focusing on generating M = 10 policy trees.</figDesc><table><row><cell>Domain</cell><cell cols="2">Tiger</cell><cell></cell><cell>UAV</cell></row><row><cell>T</cell><cell>3</cell><cell>4</cell><cell>3</cell><cell>4</cell></row><row><cell>IDID</cell><cell>0.26</cell><cell>32.41</cell><cell cols="2">8.16 406.57</cell></row><row><cell>IDID-GA</cell><cell>8.8</cell><cell>45.1</cell><cell>27</cell><cell>637.18</cell></row><row><cell>IDID-MDF</cell><cell cols="4">442.94 573.85 81.96 97.40</cell></row><row><cell>IDID-VAE-ICD</cell><cell>2.09</cell><cell>5.36</cell><cell>9.23</cell><cell>41.84</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>1 |â„¦| (t-1) âŒ‹ + 1 T â† a 1 o 1 , a 2 o 2 ...a t , o t , ...a T o T end return H T</figDesc><table /><note><p>33 a t â† p[l] 34 if t Ì¸ = T then 35 k â† âŒŠ (i-1)|â„¦| t |â„¦| (T -1) âŒ‹ + 1 end end h</p></note></figure>
		</body>
		<back>

			<div type="funding">
<div><head n="7">Acknowledgments</head><p>This work is supported in part by the <rs type="funder">National Natural Science Foundation of China</rs> (Grants No.<rs type="grantNumber">62176225</rs>, <rs type="grantNumber">62276168</rs> and <rs type="grantNumber">61836005</rs>) and the <rs type="funder">Natural Science Foundation of Fujian Province, China</rs>(Grant No. <rs type="grantNumber">2022J05176</rs>) and <rs type="funder">Guangdong Province, China</rs>(Grant No. <rs type="grantNumber">2023A1515010869</rs>).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_Eu24Pk2">
					<idno type="grant-number">62176225</idno>
				</org>
				<org type="funding" xml:id="_nBSuPQH">
					<idno type="grant-number">62276168</idno>
				</org>
				<org type="funding" xml:id="_6PuFxNY">
					<idno type="grant-number">61836005</idno>
				</org>
				<org type="funding" xml:id="_4e23rd2">
					<idno type="grant-number">2022J05176</idno>
				</org>
				<org type="funding" xml:id="_Cehq3B4">
					<idno type="grant-number">2023A1515010869</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>A: [The Four Operators in Reconstructing an Incomplete Policy Tree] We present the pseudo-code to implement the four operators in reconstructing an incomplete policy tree. The operators are described in Section 4.1 and in Alg. 2. As depicted in Alg. 3, the key steps of each operator are as follows:</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Diversifying Agent&apos;s Behaviors in Interactive Decision Models</title>
		<author>
			<persName><forename type="first">Yinghui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><surname>Hanyi</surname></persName>
		</author>
		<author>
			<persName><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><surname>Yifeng</surname></persName>
		</author>
		<author>
			<persName><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><surname>Biyang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Intell. Syst</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="12035" to="12056" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning Behaviors in Agents Systems with Interactive Dynamic Influence Diagrams</title>
		<author>
			<persName><forename type="first">Ross</forename><surname>Conroy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifeng</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Cavazza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingke</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCAI</title>
		<imprint>
			<biblScope unit="page" from="39" to="45" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Data-Driven Decision Making and Near-Optimal Path Planning for Multiagent System in Games</title>
		<author>
			<persName><forename type="first">Xindi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE-J-MASS</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="320" to="328" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Data-Driven Decision-Making</title>
		<author>
			<persName><forename type="first">Ronald</forename><surname>Buijsse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martijn</forename><surname>Willemsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Snijders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data Science for Entrepreneurship: Principles and Methods for Data Engineering, Analytics, Entrepreneurship, and the Society</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="239" to="277" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Modeling agent decision and behavior in the light of data science and artificial intelligence</title>
		<author>
			<persName><forename type="first">Li</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Volker</forename><surname>Grimm</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abigail</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">L</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Malleson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alison</forename><surname>Heppenstall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Vincenot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><surname>Robinson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyue</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianguo</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emilie</forename><surname>Lindkvist</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Environ. Modell. Softw</title>
		<imprint>
			<biblScope unit="volume">166</biblScope>
			<biblScope unit="page">105713</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An Evolutionary Framework for Modelling Unknown Behaviours of Other Agents</title>
		<author>
			<persName><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><surname>Yinghui</surname></persName>
		</author>
		<author>
			<persName><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><surname>Biyang</surname></persName>
		</author>
		<author>
			<persName><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><surname>Yifeng</surname></persName>
		</author>
		<author>
			<persName><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Buxin</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Emerging Top. Comput. Intell</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1276" to="1289" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Toward data-driven solutions to interactive dynamic influence diagrams</title>
		<author>
			<persName><forename type="first">Yinghui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Biyang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi-Feng</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhong</forename><surname>Ming</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2431" to="2453" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Self-Adaptive Large Language Model (LLM)-Based Multiagent Systems</title>
		<author>
			<persName><forename type="first">Nathalia</forename><surname>Nascimento</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paulo</forename><surname>Alencar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Donald</forename><surname>Cowan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE ACSOS-C</title>
		<imprint>
			<biblScope unit="page" from="104" to="109" />
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Appraising Success of LLM-based Dialogue Agents</title>
		<author>
			<persName><forename type="first">Ritika</forename><surname>Wason</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Parul</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Devansh</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jasleen</forename><surname>Kaur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sunil Pratap</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">N</forename><surname>Hoda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE INDIACom</title>
		<imprint>
			<biblScope unit="page" from="1570" to="1573" />
			<date type="published" when="2024">2024. 2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">AgentLens: Visual Analysis for Agent Behaviors in LLM-based Autonomous Systems</title>
		<author>
			<persName><forename type="first">Jiaying</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jieyi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingchaojie</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingyuan</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchen</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Visual Comput. Graphics</title>
		<imprint>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Explaining Agent Behavior with Large Language Models</title>
		<author>
			<persName><forename type="first">Xijia</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yue</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Stepputtis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Katia</forename><surname>Sycara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Campbell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">arXiv</title>
		<imprint>
			<date type="published" when="2023">2023, eprint 2309.10346</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Towards reusable building blocks for agent-based modelling and theory development</title>
		<author>
			<persName><forename type="first">Uta</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Michael Barton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emile</forename><surname>Chappin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gunnar</forename><surname>DreÃŸler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tatiana</forename><surname>Filatova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thibault</forename><surname>Fronville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Allen</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Iris</forename><surname>Emiel Van Loon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Lorscheid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Birgit</forename><surname>Meyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cyril</forename><surname>MÃ¼ller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Viktoriia</forename><surname>Piou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Radchuk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lennart</forename><surname>Roxburgh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>SchÃ¼ler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nanda</forename><surname>Troost</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><forename type="middle">G</forename><surname>Wijermans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marie-Christin</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Volker</forename><surname>Wimmler</surname></persName>
		</author>
		<author>
			<persName><surname>Grimm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Environ. Modell. Softw</title>
		<imprint>
			<biblScope unit="volume">175</biblScope>
			<biblScope unit="page">106003</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Synergistic Integration Between Machine Learning and Agent-Based Modeling: A Multidisciplinary Review</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Valencia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ni-Bin</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2170" to="2190" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep Explainable Relational Reinforcement Learning: A Neuro-Symbolic Approach</title>
		<author>
			<persName><forename type="first">Rishi</forename><surname>Hazra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luc</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raedt</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning and Knowledge Discovery in Databases: Research Track</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="213" to="229" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<author>
			<persName><surname>Belle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Vaishak</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandra</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ekaterina</forename><surname>Russo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alistair</forename><surname>Komendantskaya</surname></persName>
		</author>
		<author>
			<persName><surname>Nottle</surname></persName>
		</author>
		<title level="m">Neuro-Symbolic AI + Agent Systems: A First Reflection on Trends, Opportunities and Challenges</title>
		<meeting><address><addrLine>Cham; Nature Switzerland</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<biblScope unit="page" from="180" to="200" />
		</imprint>
	</monogr>
	<note>AAMAS Workshops, 2024</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Performing decision-making tasks through dynamics of recurrent neural networks trained with reinforcement learning</title>
		<author>
			<persName><forename type="first">Roman</forename><surname>Kononov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oleg</forename><surname>Maslennikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2023 DCNA</title>
		<meeting>2023 DCNA</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="144" to="147" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep Reinforcement Learning: A Survey</title>
		<author>
			<persName><forename type="first">Xu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingxing</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dawei</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jincai</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bin</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiguang</forename><surname>Miao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="5064" to="5078" />
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Variational Autoencoder Inverse Mapper: An End-to-End Deep Learning Framework for Inverse Problems</title>
		<author>
			<persName><forename type="first">Manal</forename><surname>Almaeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasir</forename><surname>Alanazi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nobuo</forename><surname>Sato</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Melnitchouk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michelle</forename><forename type="middle">P</forename><surname>Kuchera</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yaohang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2021 IJCNN</title>
		<meeting>2021 IJCNN</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">MO-MIX: Multi-Objective Multi-Agent Cooperative Decision-Making With Deep Reinforcement Learning</title>
		<author>
			<persName><forename type="first">Tianmeng</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Biao</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunhua</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tingwen</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="12098" to="12112" />
			<date type="published" when="2023-10">Oct. 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multi-agent deep reinforcement learning based decision support model for resilient community post-hazard recovery</title>
		<author>
			<persName><forename type="first">Sen</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinzheng</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Huiquan</forename><surname>Miao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Reliab. Eng. Syst. Safe</title>
		<imprint>
			<biblScope unit="volume">242</biblScope>
			<biblScope unit="page">109754</biblScope>
			<date type="published" when="2024">2024</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning in computer vision: a comprehensive survey</title>
		<author>
			<persName><forename type="first">Ngan</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vidhiwar</forename><surname>Singh Rathour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kashu</forename><surname>Yamazaki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Khoa</forename><surname>Luu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marios</forename><surname>Savvides</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell. Rev</title>
		<imprint>
			<biblScope unit="page" from="1" to="87" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning based mobile robot navigation: A review</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tsinghua Science and Technology</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="674" to="691" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">SCC: An efficient deep reinforcement learning agent mastering the game of StarCraft II</title>
		<author>
			<persName><forename type="first">Xiangjun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junxiao</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Penghui</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenkun</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weimin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiongjun</forename><surname>Pi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jujie</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><surname>Others</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2021 ICML</title>
		<meeting>2021 ICML</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10905" to="10915" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Exploiting Model Equivalences for Solving Interactive Dynamic Influence Diagrams</title>
		<author>
			<persName><forename type="first">Yifeng</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prashant</forename><surname>Doshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artif. Int. Res</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="211" to="255" />
			<date type="published" when="2012-01">Jan. 2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Time-Critical Interactive Dynamic Influence Diagram</title>
		<author>
			<persName><forename type="first">Yinghui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifeng</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanping</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuefeng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Approx. Reasoning</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">C</biblScope>
			<biblScope unit="page" from="44" to="63" />
			<date type="published" when="2015-02">Feb. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning Agents&apos; Relations in Interactive Multiagent Dynamic Influence Diagrams</title>
		<author>
			<persName><forename type="first">Yinghui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifeng</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Mao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ADMI 2014</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Autonomous agents modelling other agents: A comprehensive survey and open problems</title>
		<author>
			<persName><forename type="first">Stefano</forename><forename type="middle">V</forename><surname>Albrecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Stone</surname></persName>
		</author>
		<idno>abs/1709.08071</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Sequential Decision Making Under Uncertainty</title>
		<author>
			<persName><forename type="first">Masoumeh</forename><surname>Tabaeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Izadi</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Abstraction, Reformulation and Approximation</title>
		<meeting><address><addrLine>Berlin, Heidelberg; Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="360" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Approximating Behavioral Equivalence for Scaling Solutions of I-DIDs</title>
		<author>
			<persName><forename type="first">Yifeng</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prashant</forename><surname>Doshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingke</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinghui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muthukumaran</forename><surname>Chandrasekaran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="511" to="552" />
			<date type="published" when="2016-11">Nov. 2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Reducing the Dimensionality of Data with Neural Networks</title>
		<author>
			<persName><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="issue">5786</biblScope>
			<biblScope unit="page" from="504" to="507" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Auto-Encoding Variational Bayes</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno>CoRR, abs/1312.6114</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Exploiting Model Equivalences for Solving Interactive Dynamic Influence Diagrams</title>
		<author>
			<persName><forename type="first">Yifeng</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prashant</forename><surname>Doshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artif. Intell. Res</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="211" to="255" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Recursively modeling other agents for decision making: A research perspective</title>
		<author>
			<persName><forename type="first">Prashant</forename><surname>Doshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Piotr</forename><forename type="middle">J</forename><surname>Gmytrasiewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edmund</forename><forename type="middle">H</forename><surname>Durfee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif. Intell</title>
		<imprint>
			<biblScope unit="volume">279</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Graphical Models for Interactive POMDPs: Representations and Solutions</title>
		<author>
			<persName><forename type="first">Prashant</forename><surname>Doshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifeng</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiongyu</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Auton. Agent Multi-ag.</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="376" to="416" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A Framework for Sequential Planning in Multiagent Settings</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Gmytrasiewicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Doshi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Artif. INntell. Res</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="49" to="79" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Formal models and algorithms for decentralized decision making under uncertainty</title>
		<author>
			<persName><forename type="first">S</forename><surname>Seuken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zilberstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="s">Auton. Agent Multi-ag.</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="190" to="250" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Iterative Online Planning in Multiagent Settings with Limited Model Spaces and PAC Guarantees</title>
		<author>
			<persName><forename type="first">Yingke</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prashant</forename><surname>Doshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifeng</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAMAS &apos;15</title>
		<meeting><address><addrLine>Richland, SC</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1161" to="1169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">The road from MLE to EM to VAE: A brief tutorial</title>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI Open</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="29" to="34" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Decision Making in Multiagent Systems: A Survey</title>
		<author>
			<persName><forename type="first">Yara</forename><surname>Rizk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mariette</forename><surname>Awad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><forename type="middle">W</forename><surname>Tunstel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Cognit. Dev. Syst</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="514" to="529" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Decision Making for Overtaking of Unmanned Vehicle Based on Deep Q-learning</title>
		<author>
			<persName><forename type="first">Wenbin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guo</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenjiang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rong</forename><surname>Fei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinhong</forename><surname>Hei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jialin</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE DDCLS</title>
		<imprint>
			<biblScope unit="page" from="350" to="353" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Data-Driven Decision Making and Near-Optimal Path Planning for Multiagent System in Games</title>
		<author>
			<persName><forename type="first">Xindi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE J. Miniaturization Air Space Syst</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="320" to="328" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Reinforcement Learning With Task Decomposition for Cooperative Multiagent Systems</title>
		<author>
			<persName><forename type="first">Changyin</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenzhang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Dong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2054" to="2065" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Data-Driven Modeling: Concept, Techniques, Challenges and a Case Study</title>
		<author>
			<persName><forename type="first">Maki</forename><forename type="middle">K</forename><surname>Habib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fusaomi</forename><surname>Ayankoso</surname></persName>
		</author>
		<author>
			<persName><surname>Nagata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE ICMA</title>
		<imprint>
			<biblScope unit="page" from="1000" to="1007" />
			<date type="published" when="2021">2021. 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Conservative Policy Construction Using Variational Autoencoders for Logged Data With Missing Values</title>
		<author>
			<persName><forename type="first">Mahed</forename><surname>Abroshan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Hou Yip</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cem</forename><surname>Tekin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mihaela</forename><surname>Van Der Schaar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks Learn. Syst</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="6368" to="6378" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning Behaviors in Agents Systems with Interactive Dynamic Influence Diagrams</title>
		<author>
			<persName><forename type="first">Ross</forename><surname>Conroy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifeng</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc</forename><surname>Cavazza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingke</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2015 IJCAI</title>
		<meeting>2015 IJCAI<address><addrLine>Buenos Aires, Argentina</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="39" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Improved Use of Partial Policies for Identifying Behavioral Equivalence</title>
		<author>
			<persName><forename type="first">Yifeng</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinghui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 2012 AAMAS</title>
		<meeting>2012 AAMAS<address><addrLine>Richland, SC</addrLine></address></meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1015" to="1022" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Graphical models for interactive POMDPs: representations and solutions</title>
		<author>
			<persName><forename type="first">Prashant</forename><surname>Doshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifeng</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiongyu</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Autonomous Agents and Multi-Agent Systems</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="376" to="416" />
			<date type="published" when="2009-06">June 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Modelling other agents through evolutionary behaviours</title>
		<author>
			<persName><forename type="first">Yifeng</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><surname>Ran</surname></persName>
		</author>
		<author>
			<persName><surname>Qiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Biyang</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinghui</forename><surname>Pan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Memet. Comput</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="30" />
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
