<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Influence Diagram Bandits: Variational Thompson Sampling for Structured Bandit Problems</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2020-07-09">9 Jul 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Tong</forename><surname>Yu</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Branislav</forename><surname>Kveton</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Zheng</forename><surname>Wen</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ruiyi</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName><forename type="first">Ole</forename><forename type="middle">J</forename><surname>Mengshoel</surname></persName>
						</author>
						<title level="a" type="main">Influence Diagram Bandits: Variational Thompson Sampling for Structured Bandit Problems</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-07-09">9 Jul 2020</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2007.04915v1[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T18:58+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a novel framework for structured bandits, which we call an influence diagram bandit. Our framework captures complex statistical dependencies between actions, latent variables, and observations; and thus unifies and extends many existing models, such as combinatorial semi-bandits, cascading bandits, and low-rank bandits. We develop novel online learning algorithms that learn to act efficiently in our models.</p><p>The key idea is to track a structured posterior distribution of model parameters, either exactly or approximately. To act, we sample model parameters from their posterior and then use the structure of the influence diagram to find the most optimistic action under the sampled parameters. We empirically evaluate our algorithms in three structured bandit problems, and show that they perform as well as or better than problem-specific state-of-the-art baselines.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Structured multi-armed bandits, such as combinatorial semibandits <ref type="bibr" target="#b9">(Gai et al., 2012;</ref><ref type="bibr" target="#b5">Chen et al., 2013;</ref><ref type="bibr">Kveton et al., 2015b)</ref>, cascading bandits <ref type="bibr">(Kveton et al., 2015a;</ref><ref type="bibr" target="#b25">Li et al., 2016)</ref>, and low-rank bandits <ref type="bibr" target="#b15">(Katariya et al., 2017;</ref><ref type="bibr" target="#b2">Bhargava et al., 2017;</ref><ref type="bibr" target="#b14">Jun et al., 2019;</ref><ref type="bibr" target="#b28">Lu et al., 2018;</ref><ref type="bibr" target="#b41">Zimmert &amp; Seldin, 2018)</ref>, have been extensively studied. Various learning algorithms have been developed, and many of them have either provable regret bounds, good experimental results, or both. Despite such significant progress along this research line, the prior work still suffers from limitations.</p><p>A major limitation is that there is no unified framework or general learning algorithms for structured bandits. Most existing algorithms are tailored to a specific structured bandit, Proceedings of the 37 th International Conference on Machine <ref type="bibr">Learning, Vienna, Austria, PMLR 119, 2020.</ref> Copyright 2020 by the author(s). and new algorithms are necessary even when the modeling assumptions are slightly modified. For example, cascading bandits assume that if an item in a recommended list is examined but not clicked by a user, the user examines the next item in the list. CascadeKL-UCB algorithm for cascading bandits <ref type="bibr">(Kveton et al., 2015a)</ref> relies heavily on this assumption. In practice though, there might be a small probability that the user skips some items in the list when browsing. If we take this into consideration, CascadeKL-UCB is no longer guaranteed to be sound and needs to be redesigned.</p><p>To enable more general algorithms, we propose a novel online learning framework of influence diagram bandits. The influence diagram <ref type="bibr" target="#b13">(Howard &amp; Matheson, 1984</ref>) is a generalization of Bayesian networks. It can naturally represent structured stochastic decision problems, and elegantly model complex statistical dependencies between actions, latent variables, and observations. This paper presents a specific type of influence diagrams (Section 2), enabling many structured bandits, such as cascading bandits <ref type="bibr">(Kveton et al., 2015a)</ref>, combinatorial semi-bandits <ref type="bibr" target="#b9">(Gai et al., 2012;</ref><ref type="bibr" target="#b5">Chen et al., 2013;</ref><ref type="bibr">Kveton et al., 2015b)</ref>, and rank-1 bandits <ref type="bibr" target="#b15">(Katariya et al., 2017)</ref>, to be formulated as special cases of influence diagram bandits. However, it is still non-trivial to efficiently learn to make the best decisions online with convergence guarantee, in an influence diagram with (i) complex structure and (ii) exponentially many actions. In this paper, we develop a Thompson sampling algorithm idTS and its approximations for influence diagram bandits. The key idea is to represent the model in a compact way and track a structured posterior distribution of model parameters. We sample model parameters from their posterior and then use the structure of the influence diagram to find the most optimistic action under the sampled parameters. In complex influence diagrams, latent variables naturally occur. In this case, the model posterior is intractable and exact posterior sampling is infeasible. To handle such problems, we propose variational Thompson sampling algorithms, idTSvi and idTSinc. By compact model representation and efficient computation of the best action under a sampled model via dynamic programming, our algorithms are efficient both statistically and computationally. We further derive an upper bound on the regret of idTS under additional assumptions, and show that the regret only depends on the model parameterization and is independent of the number of actions.</p><p>This paper makes four major contributions. First, we propose a novel framework called influence diagram bandit, which unifies and extends many existing structured bandits. Second, we develop algorithms to efficiently learn to make the best decisions under the influence diagrams with complex structures and exponentially many actions. We further derive an upper bound on the regret of our idTS algorithm. Third, by tracking a structured posterior distribution of model parameters, our algorithms naturally handle complex problems with latent variables. Finally, we validate our algorithms on three structured bandit problems. Empirically, the average cumulative reward of our algorithms is up to three times the reward of the baseline algorithms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head><p>An influence diagram is a Bayesian network augmented with decision nodes and a reward function <ref type="bibr" target="#b13">(Howard &amp; Matheson, 1984)</ref>. The structure of an influence diagram is determined by a directed acyclic graph (DAG) G. In our problem, nodes in the influence diagram can be partially observed and classified into three categories:</p><formula xml:id="formula_0">decision nodes A = (A i ) i∈N , la- tent nodes Z = (Z i ) i∈N</formula><p>, and observed nodes X = (X i ) i∈N . Among these nodes, X are stochastic and observed, Z are stochastic and unobserved, and A are non-stochastic. Except for the decision nodes, each node corresponds to a random variable. Without loss of generality, we assume that all random variables are Bernoulli<ref type="foot" target="#foot_5">foot_5</ref> . The decision nodes represent decisions that are under the full control of the decision maker. We further assume that each decision node has a single child and that each child of a decision node has a single parent. The edges in the diagram represent probabilistic dependencies between variables. The reward function r(X, Z) is a deterministic function of the values of nodes X and Z. In an influence diagram, the solution to the decision problem is an instantiation of decision nodes that maximizes the reward.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Influence Diagrams for Bandit Problems</head><p>The decision nodes affect how random variables X and Z are realized, which in turn determine the reward r(X, Z).</p><p>To explain this process, we introduce some notation. Let V be a node, child(V ) be the set of its children, and par(V ) be the set of its parents. For simplicity of exposition, each decision node A i is a categorical variable that can take on L values A = {1, . . . L}. Each value a ∈ A corresponds to a decision and is associated with a fixed Bernoulli mean µ a ∈ [0, 1]. The number of decision nodes is K. The action a = (a 1 , . . . , a K ) ∈ A K is a tuple of all K decisions.</p><p>The nodes in the influence diagram are instantiated by the following stochastic process. First, all decision nodes A i are assigned decisions by the decision maker. Let</p><formula xml:id="formula_1">A i = a i for all i ∈ [K].</formula><p>Then the value of each child(A i ) is drawn according to a i . Specifically, it is sampled from a Bernoulli distribution with mean µ ai , child(A i ) ∼ Ber(µ ai ). All remaining nodes are set as follows. For any node V that has not been set yet, if all nodes in par(V ) have been instantiated, V ∼ Ber(µ), where µ ∈ [0, 1] depends only on the assigned values to par(V ). Since the influence diagram is a DAG, this process is well defined and guaranteed to instantiate all nodes. The corresponding reward is r(X, Z).</p><p>The model can be parameterized by a vector of Bernoulli means θ ∈ [0, 1] d+L . The last L entries of θ correspond to µ a , one for each decision a ∈ A. The first d entries parameterize the conditional distributions of all nodes that are not in ∪ i∈[K] child(A i ), directly affected by the decision nodes. We denote the joint probability distribution of X and Z conditioned on action a ∈ A K by P (X, Z | θ, a).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Simple Example</head><p>We show a simple influence diagram in Figure <ref type="figure" target="#fig_0">1d</ref>. The decisions nodes are A = (A 1 , A 2 ), the observed node is X, and the latent nodes are Z = (Z 1 , Z 2 ). After the decision node A i is set to a i , it determines the mean of Z i , µ ai . Then, after both Z i ∼ Ber(µ ai ) are drawn, X is drawn from a Bernoulli distribution with mean</p><formula xml:id="formula_2">P (X = 1 | Z 1 , Z 2 ).</formula><p>Since the number of decisions is L, this model has L + 4 parameters. The L parameters are (µ a ) a∈A . The remaining 4 parameters are</p><formula xml:id="formula_3">P (X = 1 | Z 1 , Z 2 ) for Z 1 , Z 2 ∈ {0, 1}.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Influence Diagram Bandits</head><p>Consider an influence diagram with observed nodes X, latent nodes Z, and decision nodes A in Section 2. Recall that all random variables associated with these nodes are binary, and the model is parameterized by a vector of d + L Bernoulli means θ * ∈ [0, 1] d+L . The learning agent knows the structure of the influence diagram, but does not know the marginal and conditional distributions in it. That is, the agent does not know θ * . Let r(x, z) be the reward associated with X = x and Z = z. To simplify exposition, let r(a, θ) be the expected reward of action a under model</p><formula xml:id="formula_4">parameters θ ∈ [0, 1] d+L , r(a, θ) = x,z r(x, z)P (x, z | θ, a).</formula><p>At time t, the agent adaptively chooses action a t based on the past observations. Then the binary values of the children of decision nodes A are generated from their respective Bernoulli distributions, which are specified by a t . Analogously, the values of all other nodes are generated based on their respective marginal and conditional distributions. Let x t and z t denote the values of X and Z, respectively, at time t. At the end of time t, the agent observes x t and receives stochastic reward r(x t , z t ).</p><p>The agent's policy is evaluated by its n-step expected cumulative regret</p><formula xml:id="formula_5">R(n, θ * ) = E [ n t=1 R(a t , θ * )|θ * ] ,</formula><p>where R(a t , θ * ) = r(a * , θ * )-r(a t , θ * ) is the instantaneous regret at time t, and</p><formula xml:id="formula_6">a * = arg max a∈A K r(a, θ * )</formula><p>is the optimal action under true model parameters θ * . For simplicity of exposition, we assume that a * is unique. When we have a prior over θ * , an alternative performance metric is the n-step Bayes regret, which is defined as</p><formula xml:id="formula_7">R B (n) = E [R(n, θ * )] ,</formula><p>where the expectation is over θ * under its prior.</p><p>We review several examples of prior works, which can be viewed as special cases of influence diagram bandits, below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Online Learning to Rank in Click Models</head><p>The cascade model is a popular model in learning to rank <ref type="bibr" target="#b6">(Chuklin et al., 2015)</ref>, which has been studied extensively in the bandit setting, starting with <ref type="bibr">Kveton et al. (2015a)</ref>.</p><p>The model describes how a user interacts with a list of items a = (a 1 , . . . , a K ) at K positions. We visualize it in Figure <ref type="figure" target="#fig_0">1a</ref>. </p><formula xml:id="formula_8">P (C k = 1 | W k , E k ) = W k E k , P (E k = 1 | C k-1 , E k-1 ) = (1 -C k-1 )E k-1 .</formula><p>The first position is always examined, and therefore we have</p><formula xml:id="formula_9">P (E 1 = 1) = 1. The reward is the total number of clicks, r(X, Z) = K k=1 C k .</formula><p>The position-based model <ref type="bibr" target="#b6">(Chuklin et al., 2015)</ref> in Figure <ref type="figure" target="#fig_0">1b</ref> is another popular click model, which was studied in the bandit setting by <ref type="bibr" target="#b23">Lagrée et al. (2016)</ref>. The difference from the cascade model is that the examination indicator of position k, E k , is an independent random variable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Combinatorial Semi-Bandits</head><p>The third example is a combinatorial semi-bandit <ref type="bibr" target="#b9">(Gai et al., 2012;</ref><ref type="bibr" target="#b5">Chen et al., 2013;</ref><ref type="bibr">Kveton et al., 2015b;</ref><ref type="bibr" target="#b38">Wen et al., 2015)</ref>. In this model, the agent chooses K items a = (a 1 , . . . , a K ) and observes their rewards. We visualize this model in Figure <ref type="figure" target="#fig_0">1c</ref>. For the k-th item, the model has two nodes: decision node A k , which is set to the k-th chosen item a k ; and observed reward node X k , which is the reward of item a k . If the reward of item a k is a Bernoulli random variable with mean</p><formula xml:id="formula_10">µ a k , P (X k = 1 | A k = a k ) = µ a k , as in our model. The reward is the sum of individual item rewards, r(X, Z) = K k=1 X k .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Bernoulli Rank-1 Bandits</head><p>The last example is a Bernoulli rank-1 bandit <ref type="bibr" target="#b15">(Katariya et al., 2017)</ref>. In this model, the agent selects the row and column of a rank-1 matrix, and observes a stochastic reward of the product of their latent factors. We visualize this model in Figure <ref type="figure" target="#fig_0">1d</ref> and can represent it as follows. We have two decision nodes, A 1 for rows and A 2 for columns. The values of these nodes, a 1 and a 2 , are the chosen rows and columns, respectively. We have two latent nodes, Z 1 for rows and Z 2 for columns. For each,</p><formula xml:id="formula_11">P (Z k = 1 | A k = a k ) = µ a k ,</formula><p>where µ a k is the latent factor corresponding to choice a k . Finally, we have one observed reward node X such that</p><formula xml:id="formula_12">P (X = 1 | Z 1 , Z 2 ) = Z 1 Z 2 . The reward is r(X, Z) = X.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Algorithm</head><p>There are two major challenges in developing efficient online learning algorithm for influence diagram bandits. First, with exponentially many actions, it is challenging to develop a sample efficient algorithm to learn a generalizable model statistically efficiently. Second, it is computationally expensive to compute the most valuable action when instantiating the decision nodes in each step, given exponentially many combinations of options.</p><p>Many exploration strategies exist in the online setting, such as the -greedy policy <ref type="bibr" target="#b33">(Sutton &amp; Barto, 2018)</ref>, UCB1 <ref type="bibr" target="#b1">(Auer et al., 2002)</ref>, and Thompson sampling <ref type="bibr" target="#b35">(Thompson, 1933)</ref>. While we do not preclude that UCB-like algorithms can be developed for our problem, we argue that they are unnatural. Roughly speaking, the upper confidence bound (UCB) is the highest value of any action under any plausible model parameters, given the history. It is unclear how to solve this problem efficiently in influence diagrams. In contrast, Thompson sampling is more natural. The model parameters can be sampled from their posterior, and the problem of finding the most valuable action given fixed model parameters can be solved using dynamic programming <ref type="bibr" target="#b34">(Tatman &amp; Shachter, 1990)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Thompson Sampling</head><p>Thompson sampling <ref type="bibr" target="#b35">(Thompson, 1933)</ref> is a popular online learning algorithm, which we adapt to our setting as follows. Let x = (x ) t-1 =1 , z = (z ) t-1 =1 , and ã = (a ) t-1 =1 be the values of observed nodes, latent nodes, and decision nodes, respectively, up to the end of time t -1. First, the algorithm samples model parameters θ t ∼ p t-1 , where p t-1 is the posterior of θ * at the end of time t -1. That is,</p><formula xml:id="formula_13">p t-1 (θ) = P (θ * = θ | x, ã)</formula><p>for all θ. Second, the action at time t is chosen greedily with respect to θ t ,</p><formula xml:id="formula_14">a t = arg max a∈A K r(a, θ t ).</formula><p>Finally, the agent observes x t and receives reward r(x t , z t ). We call this algorithm idTS. Note that idTS is generally computationally intractable when latent nodes are present, due to the need to sample from the exact posterior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Fully-Observable Case</head><p>In the fully-observable case, with no latent variables and Beta prior, idTS is computationally efficient. To see it, note that in this case p t-1 factors over model parameters, and is a product of Beta distributions. Based on the observed node values, we can update the Beta posterior for each model parameter in θ * individually and computationally efficiently, since the Beta distribution is the conjugate prior of the Bernoulli distribution. One example of this case is the combinatorial semi-bandit in Section 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Partially-Observable Case</head><p>In complex influence diagram bandits, such as rank-1 bandits, latent variables are typically present. In such cases, exact sampling from p t-1 is usually computationally intractable, which limits the use of idTS. However, there are many computationally tractable approximations to sampling from p t-1 , such as variational inference and particle filtering <ref type="bibr" target="#b3">(Bishop, 2006;</ref><ref type="bibr" target="#b0">Andrieu et al., 2003)</ref>. In practice, the performance of particle filtering heavily depends on different settings of the algorithm (e.g., number of particles and transition prior). Thus, we develop an approximate Thompson sampling algorithm, idTSvi, based on variational inference. We compare our algorithms to particle filtering later in Section 7.4.</p><p>To keep notation uncluttered, we omit ã below. Let q(z, θ) be a factored distribution over (z, θ). To approximate the posterior p t-1 , we approximate P (z, θ | x) by minimizing its difference to q(z, θ). To achieve this, we decompose the following log marginal probability using log P (x) = θ z q(z, θ) log P (x)dθ = θ z q(z, θ) log</p><formula xml:id="formula_15">P (z, θ | x)P (x)q(z, θ) P (z, θ | x)q(z, θ) dθ = θ z q(z, θ) log P (x, z, θ) q(z, θ) dθ + θ z q(z, θ) log q(z, θ) P (z, θ | x) dθ.</formula><p>We denote the first term by L(q). The second term is the KL divergence between P (z, θ | x) and q(z, θ). Note P (x) is fixed and the KL divergence is non-negative. Therefore, we can minimize the KL divergence by maximizing L(q) with respect to q, to achieve better posterior approximations.</p><p>We maximize L(q) as follows. By the mean field approximation, let the approximate posterior factor as</p><formula xml:id="formula_16">q(z, θ) = q(θ)Π t-1 =1 q (z ),<label>(1)</label></formula><p>where q(θ) is the probability of model parameters θ and q (z ) is the probability that the values of latent nodes at time are z . Since θ i is the mean of a Bernoulli variable, we factor q(θ) as Π d+L i=1 θ αi i (1 -θ i ) βi and represent it as d+L tuples {(α i , β i )} d+L i=1 . For any , q (z ) is a categorical distribution. From the definition of P (x, z, θ), we have</p><formula xml:id="formula_17">log P (x, z, θ) = log P (θ) + t-1 =1 log P (x , z | θ). (2)</formula><p>By combining (1) and ( <ref type="formula">2</ref>) with the definition of L(q), we get that</p><formula xml:id="formula_18">L(q) = θ z q(z, θ) log P (x, z, θ)dθ - θ z q(z, θ) log q(z, θ)dθ = t-1 =1 θ q(θ) z q (z ) log P (x , z | θ)dθ + θ q(θ) log P (θ)dθ - t-1 =1 z q (z ) log q (z ) - θ q(θ) log q(θ)dθ<label>(3)</label></formula><p>The above decomposition suggests the following EM-like algorithm <ref type="bibr" target="#b8">(Dempster et al., 1977)</ref>.</p><p>First, in the E-step, we fix q(θ). Then</p><formula xml:id="formula_19">L(q) = t-1 z q (z ) θ q(θ) log P (x , z | θ)dθ - t-1 z q (z ) log q (z ) + C</formula><p>for some constant C. By taking its derivative with respect to q (z ) and setting it to zero, L(q) is maximized by</p><formula xml:id="formula_20">q (z ) ∝ exp θ q(θ) log P (x , z | θ)dθ .<label>(4)</label></formula><p>Second, in the M-step, we fix all q (z ). Then, for some constant C,</p><formula xml:id="formula_21">L(q) = θ q(θ) t-1 =1 z q (z ) log P (x , z | θ)dθ + θ q(θ) log P (θ)dθ - θ q(θ) log q(θ)dθ + C.</formula><p>By taking its derivative with respect to q(θ) and setting it to zero, L(q) is maximized by</p><formula xml:id="formula_22">q(θ) ∝ exp log P (θ) + t-1 =1 z q (z ) log P (x , z | θ) .</formula><p>(5)</p><p>Algorithm 1 idTSvi: Influence diagram TS with variational inference. 1: Input: &gt; 0 2: Randomly initialize q 3: for t = 1, . . . , n do 4:</p><p>Sample θ t proportionally to q(θ t )</p><p>5:</p><p>Take action a t = arg max a∈A K r(a, θ t )</p><p>6:</p><p>Observes x t and receive reward r(x t , z t )</p><p>7:</p><p>Randomly initialize q 8:</p><p>Calculate L(q) using (3) and set L (q) = -∞ 9:</p><p>while L(q) -L (q) ≥ do 10:</p><p>Set L (q) = L(q)</p><p>11:</p><p>for = 1, . . . , t do 12:</p><p>Update q (z ) using ( <ref type="formula" target="#formula_20">4</ref>), for all z 13: end for 14:</p><p>Update q(θ) using ( <ref type="formula">5</ref>) 15:</p><p>Update L(q) using (3) 16:</p><p>end while 17: end for</p><p>The above two steps, the E-step and M-step, are alternated until convergence. This is guaranteed under our model assumption.</p><p>The pseudocode of idTSvi is in Algorithm 1. In line 4, θ t is sampled from its posterior. In line 5, the agent takes action a t that maximizes the expected reward under model parameters θ t . In line 6, the agent observes x t and receives reward. From line 7 to line 16, we update the posterior of θ, by alternating the estimations of q (z ) and q(θ) until convergence. In line 12, we update q (z ) by the E-step. In line 14, we update q(θ) by the M-step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Improving Computational Efficiency</head><p>To make idTSvi computationally efficient, we propose its incremental variant, idTSinc. The main problem in Algorithm 1 is that each q t (z t ) is re-estimated at all times from time t to time n. To reduce the computational complexity of Algorithm 1, we estimate q t (z t ) only at time t. That is, the "for" loop in line 11 is only run for = t; and q (z ) for &lt; t are reused from the past. The pseudocode of idTSinc is in Appendix C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Regret Bound</head><p>In this section, we derive an upper bound on the n-step Bayes regret R B (n) for idTS in influence diagram bandits.</p><p>We introduce notations before proceeding. We say a node in an influence diagram bandit is de facto observed at time t if its realization is either observed or can be exactly in-ferred<ref type="foot" target="#foot_6">foot_6</ref> at that time. Let θ (i) * denote the ith element of θ * , which parameterizes a (conditional) Bernoulli distribution at node j i in the influence diagram. Note that if node j i has parents par(j i ), then θ (i) * corresponds to a particular realization at nodes par(j i ). We define the event E (i) t as the event that (1) both node j i and its parents par(j i ) (if any) are de facto observed at time t, (2) the realization at par(j i ) corresponds to θ (i) * , and (3) the realization at j i is conditionally independently drawn from Ber θ (i) *</p><p>. Note that under event E (i) t , the agent observes and knows that it observes a Bernoulli sample corresponding to θ (i) * at time t.</p><p>To simplify the exposition, if clear from context, we omit the subscript t of E (i) t . Our assumptions are stated below.</p><p>Assumption 1 The parameter index set {1, . . . , d + L} is partitioned into two disjoint subsets I + and I -. For all i ∈ I + (or i ∈ I -), r(a, θ) is weakly increasing (or weakly decreasing) in θ (i) for any action a and probability measure θ ∈ [0, 1] d+L .</p><p>Assumption 2 For any action a and any Bernoulli probability measures θ 1 , θ 2 ∈ [0, 1] d+L , we have</p><formula xml:id="formula_23">|r(a, θ1) -r(a, θ2)| ≤ C d+L i=1 P (E (i) |θ2, a) θ (i) 1 -θ (i) 2 ,</formula><p>where C ≥ 0 is a constant.</p><p>Assumption 1 says that r(a, θ) is element-wise monotone (either weakly increasing or decreasing) in θ. Assumption 2 says that the expected reward difference is bounded by a weighted sum of the probability measure difference, with weights proportional to the observation probabilities. Note that the satisfaction of Assumption 2 depends on both the functional form of r(X, Z) and the information feedback structure in the influence diagram.</p><p>Intuitively, both combinatorial semi-bandits and cascading bandits discussed in Section 3 satisfy Assumptions 1 and 2. The expected reward increases with θ in both models. Thus Assumption 1 holds. Assumption 2 holds in combinatorial semi-bandits, since all nodes are observed and the expected reward is linear in θ. Assumption 2 holds in cascading bandits due to Lemma 1 in <ref type="bibr">Kveton et al. (2015a)</ref>. The proof is in Appendix A.</p><p>Before we present our regret bound, we define the metric of maximum expected observations</p><formula xml:id="formula_24">O max = max a E d+L i=1 1 E (i) a ,<label>(6)</label></formula><p>where the expectation is over both θ * under the prior, and the random samples in the influence diagram under θ * and a. Notice that for any action a, </p><formula xml:id="formula_25">E d+L i=1 1 E (i) a</formula><formula xml:id="formula_26">R B (n) ≤ O C (L + d)O max n log n ,</formula><p>where O max is defined in (6). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Please refer to</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Related Work</head><p>In general, it is challenging to calculate the exact posterior distribution in Thompson sampling in complex problems. <ref type="bibr" target="#b36">Urteaga &amp; Wiggins (2018)</ref> used variational inference to approximate the posterior of arms by a mixture of Gaussians.</p><p>The main difference in our work is that we focus on structured arms and rewards, where the rewards are correlated through latent variables. Theoretical analysis of approximate inference in Thompson sampling was done in <ref type="bibr" target="#b30">Phan et al. (2019)</ref>. By matching the minimal exploration rates of sub-optimal arms, Combes et al. ( <ref type="formula">2017</ref>) solved a different class of structured bandit problems. <ref type="bibr" target="#b10">Gopalan et al. (2014)</ref> and <ref type="bibr" target="#b16">Kawale et al. (2015)</ref> used particle filtering to approximate the posterior in Thompson sampling. Particle filtering is known to be consistent. However, in practice, its performance depends heavily on the number of particles. When the number of particles is small, particle filtering is computationally efficient but achieves poor approximation results. This issue can be alleviated by particle-based Bayesian sampling <ref type="bibr" target="#b39">(Zhang et al., 2020)</ref>.</p><p>On the other hand, when the number of particles is large, particle filtering performs well but is computationally demanding. <ref type="bibr" target="#b4">Blundell et al. (2015)</ref> used variational inference to approximate the posterior in neural networks and incorporated it in Thompson sampling <ref type="bibr" target="#b4">(Blundell et al., 2015)</ref>. <ref type="bibr" target="#b11">Haarnoja et al. (2017;</ref><ref type="bibr">2018</ref>) learned an energy-based policy determined by rewards, which is approximated by minimizing the KL divergence between the optimal posterior and variational distribution. The intractable posterior of Thompson sampling in neural networks can be approximated in the last layer, which is treated as features in Bayesian linear regression <ref type="bibr" target="#b31">(Riquelme et al., 2018;</ref><ref type="bibr" target="#b26">Liu et al., 2018)</ref>. However, the uncertainty may be underestimated in bandit problems <ref type="bibr" target="#b40">(Zhang et al., 2019)</ref> or zero uncertainty is propagated via Bellman error <ref type="bibr" target="#b29">(Osband et al., 2018)</ref>. This can be alleviated by particle-based Thompson sampling <ref type="bibr" target="#b27">(Lu &amp; Van Roy, 2017;</ref><ref type="bibr" target="#b40">Zhang et al., 2019)</ref>. Follow-the-perturbed-leader exploration <ref type="bibr">(Kveton et al., 2019c;</ref><ref type="bibr">a;</ref><ref type="bibr">b;</ref><ref type="bibr" target="#b37">Vaswani et al., 2020;</ref><ref type="bibr" target="#b22">Kveton et al., 2020)</ref> is an alternative to Thompson sampling that does not require posterior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Experiments</head><p>In this section, we show that our algorithms can be applied beyond traditional models and learn more general models. Specifically, we compare our approaches to traditional bandit algorithms for the cascade model, position-based model, and rank-1 bandit. The performance of the algorithms is measured by their average cumulative reward. The average cumulative reward at time n is 1 n n t=1 r(x t , z t ), where r(x t , z t ) is the reward received at time t. The rewards of various models are defined in Sections 3.1 to 3.3. We report the average results over 20 runs with standard errors.</p><p>Note that idTSvi and idTSinc are expected to perform worse than idTS since they are approximations. We now briefly discuss how to justify that idTSvi and idTSinc perform similarly to idTS. Recall that due to latent variables, it is computationally intractable to implement the exact Thompson sampling idTS in general influence diagram bandits. However, we can efficiently compute an upper bound on the average cumulative reward of idTS based on numerical experiments in a feedback-relaxed setting. Specifically, consider a feedback-relaxed setting where all latent nodes Z are observed. Note that idTS is computationally efficient in this new setting since it is fully observed. Moreover, due to more information feedback, idTS should perform better in this feedback-relaxed setting and hence provide an upper bound on the average cumulative reward of idTS in the original problem. In this section, we refer to idTS in this feedback-relaxed setting as idTSfull, to distinguish it from idTS in the original problem. Intuitively, if idTSfull and idTSvi perform similarly, we also expect idTSvi and idTS to perform similarly.</p><p>To speed up idTSvi and idTSinc in our experiments, we do not run the "while" loop in line 9 of Algorithm 1 until convergence. In idTSvi, we run it only once. In idTSinc, which is less stable in estimating q t (z t ) but more computationally efficient, we run it up to 30 times. We did not observe any significant drop in the quality of idTSvi and idTSinc policies when we used this approximation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Cascade Model</head><p>Algorithms for the cascade model, such as CascadeKL-UCB, make strong assumptions. On the other hand, our algorithms make fewer assumptions and allow us to learn more general models. First, we experiment with the cascade model (Section 3.1). The number of items is L = 20 and the length of the list is K = 2. The attraction probability of item i is θ i = i/20. We refer to this problem as cascade model 1. Second, we experiment with a variant of the problem where the cascade assumption is violated, which we call cascade model 2. In cascade model 2, we modify the conditional dependencies of C k and E k as</p><formula xml:id="formula_27">P (C k = 1 | W k , E k ) = (1 -W k )E k , P (E k = 1 | C k-1 , E k-1 ) = C k-1 E k-1 .</formula><p>This means that an item is clicked only if it is not attractive and its position is examined, and an item is examined only if the previous item is examined and clicked.</p><p>Our results are reported in Figures <ref type="figure" target="#fig_2">2a</ref> and<ref type="figure" target="#fig_2">2b</ref>. We observe several trends. First, among all our algorithms, idTSfull and idTSvi perform the best. idTSinc performs the worst and its runs have a lot of variance, which is caused by an inaccurate estimation of q(z). Second, idTSinc is much more computationally efficient than idTSvi. For each run in cascade model 1, idTSvi needs 1990.502 seconds on average, while its incremental version idTSinc only needs 212.772 seconds. Third, in Figure <ref type="figure" target="#fig_2">2b</ref>, we observe a clear gap between CascadeKL-UCB and our algorithms, idTSfull and idTSvi. This is because the cascade assumption is violated,</p><formula xml:id="formula_28">0 0.5 1 1.5 2</formula><p>Step n and thus CascadeKL-UCB cannot effectively leverage the problem structure. In contrast, our algorithms are general and still effectively identify the best action.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Position-Based Model</head><p>We also experiment with the position-based model, which is detailed in Section 3.1. The number of items is L = 20 and the length of the list is K = 2. The attraction probability of item i is θ i = i/20. We consider two variants of the problem.</p><p>In position-based model 1, the examination probabilities of both positions are 0.7. In position-based model 2, the examination probabilities are 0.8 and 0.2. The baseline is a state-of-the-art bandit algorithm for the position-based model, TopRank <ref type="bibr" target="#b24">(Lattimore et al., 2018)</ref>. Our results are reported in Figures <ref type="figure" target="#fig_2">2c</ref> and<ref type="figure" target="#fig_2">2d</ref>. We observe that idTSfull and idTSvi clearly outperform TopRank, while idTSinc achieves similar rewards compared to TopRank.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.">Rank-1 Bandit</head><p>We also evaluate our algorithms in a rank-1 bandit, which is detailed in Section 3.3. The underlying rank-1 matrix is U V T , where U ∈ [0, 1] 8 and V ∈ [0, 1] 10 . We consider two problems. In rank-1 bandit 1, U i = i/16 and V i = i/20. In rank-1 bandit 2, U i = i/8 and V i = i/10. The baseline is a state-of-the-art algorithm for the rank-1 bandit, Rank1Elim <ref type="bibr" target="#b15">(Katariya et al., 2017)</ref>. As shown in Figures <ref type="figure" target="#fig_2">2e</ref> and<ref type="figure" target="#fig_2">2f</ref>, our algorithms outperform Rank1Elim. In Figure <ref type="figure" target="#fig_2">2f</ref>, Rank1Elim improves significantly after 10000 steps. The reason is that</p><p>Rank1Elim is an elimination algorithm, and thus improves sharply at the end of each elimination stage. Nevertheless, a clear gap remains between our algorithms and Rank1Elim.</p><p>The average cumulative reward of our algorithms is up to three times higher than that of Rank1Elim.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.4.">Comparison to Particle Filtering</head><p>In this section, we compare variational inference to particle filtering for posterior sampling in influence diagram bandits. As discussed in Section 4.1, our algorithms are based on variational inference, considering that the performance of particle filtering depends on hard to tune parameters, such as the number of particles and transition prior. We validate the advantages of variational inference in cascade model 1, position-based model 1, and rank-1 bandit 1.</p><p>We implement particle filtering as in <ref type="bibr" target="#b0">Andrieu et al. (2003)</ref>.</p><p>Let m be the number of particles. At each time, the algorithm works as follows. First, we obtain m models (particles) by sampling them from their Gaussian transition prior. Second, we set their importance weights based on their likelihoods. Similarly to idTSfull, we assume that the latent variables are observed. This simplifies the computation of the likelihood, and gives particle filtering an unfair advantage over idTSvi and idTSinc. Third, we choose the model with the highest weight, take the best action under that model, and observe its reward. Finally, we resample m particles proportionally to their importance weights. The new particles serve as the mean values of the Gaussian tran-</p><formula xml:id="formula_29">0 0.5 1 1.5 2</formula><p>Step n sition prior at the next time.</p><p>Our results are reported in Figure <ref type="figure" target="#fig_3">3</ref>. First, we observe that variational inference based algorithms, idTSvi and idTSinc, perform clearly better than particle filtering. The performance of particle filtering is unstable and has a higher variance. Second, the performance of particle filtering is sensitive to the number of particles used. By increasing the number from 5 to 20, the performance of particle filtering improves. Third, the performance of particle filtering may drop over time, because sometimes particles with low likelihoods are sampled, although with a small probability.</p><p>To further show the advantage of our algorithms, we compare the computational cost of variational inference and particle filtering in Table <ref type="table" target="#tab_2">1</ref>. This experiment is in cascade model 1, and we report the run time (in seconds) and average cumulative reward at 20000 steps. The run time is measured on a computer with one 2.9 GHz Intel Core i7 processor and 16 GB memory. We observe that idTSvi has the longest run time, while idTSinc is much faster. As we increase the number of particles, the run time of particle filtering increases, since we need to evaluate the likelihood of more particles. At the same run time, idTSinc achieves a higher reward than particle filtering. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Run time</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusions</head><p>Existing algorithms for structured bandits are tailored to specific models, and thus hard to extend. This paper overcomes this limitation by proposing a novel online learning framework of influence diagram bandits, which unifies and extends most existing structured bandits. We also develop efficient algorithms that learn to make the best decisions under influence diagrams with complex structures, latent variables, and exponentially many actions. We further derive an upper bound on the regret of our algorithm idTS. Finally, we conduct experiments in various structured bandits: cascading bandits, online learning to rank in the position-based model, and rank-1 bandits. The experiments demonstrate that our algorithms perform well and are general.</p><p>As we have discussed, this paper focuses on influence diagram bandits with Bernoulli random variables. We believe that the Bernoulli assumption is without loss of generality in the sense that, by appropriately modifying some technical assumptions, our developed algorithms and analysis results can be extended to more general cases with categorical or continuous random variables. We leave the rigorous derivations in these more general cases to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A. Discussion of Assumptions</head><p>In this section, we prove that the combinatorial semi-bandit and the cascading bandit satisfy Assumptions 1 and 2 proposed in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1. Combinatorial Semi-Bandits</head><p>Notice that in a combinatorial semi-bandit, the action a = (a 1 , . . . , a K ), and</p><formula xml:id="formula_30">r(a, θ) = K k=1 θ (a k ) = L l=1 θ (l) 1 (l ∈ a) .</formula><p>Thus, for any l, r(a, θ) is weakly increasing in θ (l) . Hence Assumption 1 is satisfied. On the other hand, we have</p><formula xml:id="formula_31">|r(a, θ 1 ) -r(a, θ 2 )| = L l=1 θ (l) 1 -θ (l) 2 1 (l ∈ a) ≤ L l=1 θ (l) 1 -θ (l) 2 1 (l ∈ a) = L l=1 P E (l) θ 2 , a θ (l) 1 -θ (l) 2 , (<label>7</label></formula><formula xml:id="formula_32">)</formula><p>where the last quality follows from the fact that all nodes in a combinatorial semi-bandit is observed, and hence P E (l) θ, a = 1 (l ∈ a) for all θ. Thus, Assumption 2 is satisfied with C = 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2. Cascading Bandits</head><p>For a cascading bandit, the action is a = (a 1 , . . . , a K ), and</p><formula xml:id="formula_33">r(a, θ) = 1 - K k=1 (1 -θ (a k ) ) = 1 - l∈a (1 -θ (l) ).</formula><p>Thus, for any l, r(a, θ) is weakly increasing in θ (l) . Hence Assumption 1 is satisfied. On the other hand, from <ref type="bibr">Kveton et al. (2015a)</ref>, we have</p><formula xml:id="formula_34">r(a, θ 1 ) -r(a, θ 2 ) = K k=1 k-1 k1=1 1 -θ (ak 1 ) 2 θ (a k ) 1 -θ (a k ) 2 K k2=k+1 1 -θ (ak 2 ) 1 = K k=1 P E (a k ) θ 2 , a θ (a k ) 1 -θ (a k ) 2 K k2=k+1 1 -θ (ak 2 ) 1 ,</formula><p>where the second equality follows from</p><formula xml:id="formula_35">P E (a k ) θ 2 , a = k-1 k1=1 1 -θ (ak 1 )</formula><p>2</p><p>. Thus, we have</p><formula xml:id="formula_36">|r(a, θ 1 ) -r(a, θ 2 )| = K k=1 P E (a k ) θ 2 , a θ (a k ) 1 -θ (a k ) 2 K k2=k+1 1 -θ (ak 2 ) 1 ≤ K k=1 P E (a k ) θ 2 , a θ (a k ) 1 -θ (a k ) 2 K k2=k+1 1 -θ (ak 2 ) 1 ≤ K k=1 P E (a k ) θ 2 , a θ (a k ) 1 -θ (a k ) 2</formula><p>, where the last inequality follows from</p><formula xml:id="formula_37">K k2=k+1 1 -θ (ak 2 ) 1 ∈ [0, 1].</formula><p>Thus, Assumption 2 is satisfied with C = 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B. Proof for Theorem 1</head><p>Proof:</p><p>Recall that the stochastic instantaneous reward is r(x, z). Note that r(x, z) is bounded since its domain is finite. Without loss of generality, we assume that r(x, z) ∈ [0, B]. Thus, for any action a and probability measure θ ∈ [0, 1] d+L , we have r(a, θ) ∈ [0, B].</p><p>Define R t = r(a * , θ * ) -r(a t , θ * ), then by definition, we have</p><formula xml:id="formula_38">R B (n) = n t=1 E[R t ] = n t=1 E [E[R t |H t-1 ]] ,</formula><p>where H t-1 is the "history" by the end of time t -1, which includes all the actions and observations by that time<ref type="foot" target="#foot_10">foot_10</ref> . For any parameter index i = 1, . . . , d + L and any time t, we define N</p><formula xml:id="formula_39">(i) t = t τ =1 1 E (i) τ</formula><p>as the number of times that the samples corresponding to parameter θ (i) * have been observed by the end of time t, and θ(i) t as the empirical mean for θ (i) * based on these N (i) t observations. Then we define the upper confidence bound (UCB) U (i) t and the lower confidence bound (LCB)</p><formula xml:id="formula_40">L (i) t as U (i) t = min θ(i) t + c t, N (i) t , 1 if N (i) t &gt; 0 1 otherwise L (i) t = max θ(i) t -c t, N (i) t , 0 if N (i) t &gt; 0 0 otherwise where c (t, N ) = 1.5 log(t)</formula><p>N for any positive integer t and N . Moreover, we define a probability measure θt ∈ [0, 1] d+L as</p><formula xml:id="formula_41">ϑ (i) t = U (i) t if i ∈ I + L (i) t if i ∈ I - Since both N (i)</formula><p>t-1 and θ(i) t-1 are conditionally deterministic given H t-1 , and I + and I -are deterministic, by the definitions above, U t-1 , L t-1 and ϑ t-1 are also conditionally deterministic given H t-1 . Moreover, as is discussed in <ref type="bibr" target="#b32">Russo &amp; Van Roy (2014)</ref>, since we apply exact Thompson sampling idTS, θ * and θ t are conditionally i.i.d. given H t-1 , and a * = arg max a r(a, θ * ) and a t = arg max a r(a, θ t ). Thus, conditioning on H t-1 , r(a * , ϑ t-1 ) and r(a t , ϑ t-1 ) are i.i.d., consequently, we have</p><formula xml:id="formula_42">E[R t |H t-1 ] = E[r(a * , θ * ) -r(a t , θ * )|H t-1 ] = E[r(a * , θ * ) -r(a * , ϑ t-1 )|H t-1 ] + E[r(a t , ϑ t-1 ) -r(a t , θ * )|H t-1 ].<label>(8)</label></formula><p>To simplify the exposition, for any time t and i = 1, . . . , d + L, we define</p><formula xml:id="formula_43">G (i) t = θ (i) * - θ(i) t &gt; c t, N (i) t , N (i) t &gt; 0 = θ (i) * &gt; U (i) t or θ (i) * &lt; L (i) t .<label>(9)</label></formula><p>Notice that</p><formula xml:id="formula_44">d+L i=1 G (i) t = d+L i=1 G (i) t = {L t ≤ θ * ≤ U t }. Moreover, from Assumption 1, if L t ≤ θ * ≤ U t ,</formula><p>based on the definition of ϑ t , we have r(a, θ * ) ≤ r(a, ϑ t ) for all action a. Thus, we have</p><formula xml:id="formula_45">r(a * , θ * ) -r(a * , ϑ t-1 ) (a) = [r(a * , θ * ) -r(a * , ϑ t-1 )] 1 (L t-1 ≤ θ * ≤ U t-1 ) + [r(a * , θ * ) -r(a * , ϑ t-1 )] 1 d+L i=1 G (i) t-1 (b) ≤ [r(a * , θ * ) -r(a * , ϑ t-1 )] 1 d+L i=1 G (i) t-1 (c) ≤ B1 d+L i=1 G (i) t-1 (d) ≤ B d+L i=1 1 G (i) t-1 ,<label>(10)</label></formula><p>where equality (a) is simply a decomposition based on indicators, inequality (b) follows from the fact that r(a, θ * ) ≤ r(a, ϑ t-1 ) if L t-1 ≤ θ * ≤ U t-1 , inequality (c) follows from the fact that r(X, Z) ∈ [0, B] for all (X, Z) and hence r(a, θ) ∈ [0, B] for all a and θ, and inequality (d) trivially follows from the union bound of the indicators.</p><p>On the other hand, we have</p><formula xml:id="formula_46">r(a t , ϑ t-1 ) -r(a t , θ * ) = [r(a t , ϑ t-1 ) -r(a t , θ * )] 1 (L t-1 ≤ θ * ≤ U t-1 ) + [r(a t , ϑ t-1 ) -r(a t , θ * )] 1 d+L i=1 G (i) t-1 .</formula><p>Similarly as the above analysis, we have</p><formula xml:id="formula_47">[r(a t , ϑ t-1 ) -r(a t , θ * )] 1 d+L i=1 G (i) t-1 ≤ B d+L i=1 1 G (i) t-1 .<label>(11)</label></formula><p>On the other hand, we have</p><formula xml:id="formula_48">[r(a t , ϑ t-1 ) -r(a t , θ * )] 1 (L t-1 ≤ θ * ≤ U t-1 ) (a) ≤ C d+L i=1 P E (i) t θ * , a t ϑ (i) t-1 -θ (i) * 1 (L t-1 ≤ θ * ≤ U t-1 ) (b) ≤ C d+L i=1 P E (i) t θ * , a t U (i) t-1 -L (i) t-1 1 (L t-1 ≤ θ * ≤ U t-1 ) (c) ≤ C d+L i=1 P E (i) t θ * , a t U (i) t-1 -L (i) t-1 ,</formula><p>where inequality (a) follows from Assumption 2, inequality (b) follows trivially from L t-1 ≤ θ * ≤ U t-1 and the definition of ϑ t-1 , and inequality (c) follows from the fact that U</p><formula xml:id="formula_49">(i) t-1 &gt; L (i)</formula><p>t-1 always holds, no matter what θ * is. Combining the above results, we have</p><formula xml:id="formula_50">E[R t |H t-1 ] ≤ C d+L i=1 E P E (i) t θ * , a t U (i) t-1 -L (i) t-1 H t-1 + 2B d+L i=1 E 1 G (i) t-1 H t-1 (a) = C d+L i=1 E P E (i) t θ * , a t H t-1 U (i) t-1 -L (i) t-1 + 2B d+L i=1 E 1 G (i) t-1 H t-1 (b) = C d+L i=1 E E 1 E (i) t θ * , a t H t-1 U (i) t-1 -L (i) t-1 + 2B d+L i=1 E 1 G (i) t-1 H t-1 (c) = C d+L i=1 E E 1 E (i) t θ * , a t , H t-1 H t-1 U (i) t-1 -L (i) t-1 + 2B d+L i=1 E 1 G (i) t-1 H t-1 (d) = C d+L i=1 E 1 E (i) t U (i) t-1 -L (i) t-1 H t-1 + 2B d+L i=1 E 1 G (i) t-1 H t-1 ,</formula><p>where (a) follows from the fact that U t-1 and L t-1 are deterministic conditioning on H t-1 , (b) follows from the definition of P E (i) t θ * , a t , (c) follows from that fact that conditioning on θ * and a t , E (i) t is independent of H t-1 , and (d) follows from the tower property. Thus we have</p><formula xml:id="formula_51">R B (n) ≤ CE d+L i=1 n t=1 1 E (i) t U (i) t-1 -L (i) t-1 + 2B d+L i=1 n t=1 P G (i) t-1 . (<label>12</label></formula><formula xml:id="formula_52">)</formula><p>We first bound the second term. Notice that we have P G</p><formula xml:id="formula_53">(i) t-1 = E P G (i)</formula><p>t-1 θ * . For any θ * , we have P G</p><p>(i)</p><p>t-1 θ * = P θ (i) * -θ(i)</p><formula xml:id="formula_54">N (i) t-1 &gt; c t, N<label>(i)</label></formula><p>t-1 , N</p><p>t-1 &gt; 0 θ * , where we use subscript N (i)</p><p>t-1 for θ to emphasize it is an empirical mean over N (i)</p><p>t-1 samples. Following the union bound developed in <ref type="bibr" target="#b1">Auer et al. (2002)</ref>, we have</p><formula xml:id="formula_56">P G (i) t-1 θ * = P θ (i) * - θ(i) N (i) t-1 &gt; c t, N (i) t-1 , N (i) t-1 &gt; 0 θ * (a) ≤ t-1 N =1 P θ (i) * - θ(i) N &gt; c (t, N ) θ * (b) ≤ N -1 t=1 2 t 3 &lt; 2 t 2 ,</formula><p>where inequality (a) follows from the union bound over the realization of N (i)</p><p>t-1 , and inequality (b) follows from the Hoeffding's inequality. Since the above inequality holds for any θ * , we have P G We now try to bound the first term of equation 12. Notice that trivially, we have</p><formula xml:id="formula_57">U (i) t-1 -L (i) t-1 ≤ 2c t, N (i) t-1 1 N (i) t-1 &gt; 0 + 1 N (i)</formula><p>t-1 = 0 = 2 1.5 log(t)</p><formula xml:id="formula_58">N (i) t-1 1 N (i) t-1 &gt; 0 + 1 N (i) t-1 = 0 ≤ 6 log(n) 1 N (i) t-1 1 N (i) t-1 &gt; 0 + 1 N (i)</formula><p>t-1 = 0 .</p><p>Thus, we have</p><formula xml:id="formula_59">d+L i=1 n t=1 1 E (i) t U (i) t-1 -L (i) t-1 ≤ 6 log(n) d+L i=1 n t=1 1 N (i) t-1 1 E (i) t , N<label>(i)</label></formula><p>t-1 &gt; 0 + (d + L).</p><p>Notice that from the Cauchy-Schwarz inequality, we have </p><p>q.e.d.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C. Pseudocode of idTSinc</head><p>The pseudocode of idTSinc is summarized in Algorithm 2.</p><p>Algorithm 2 idTSinc: A computationally efficient variant of idTSvi.</p><p>1: Input: &gt; 0 2: Randomly initialize q 3: for t = 1, . . . , n do 4:</p><p>Sample θ t proportionally to q(θ t )</p><p>5:</p><p>Take action a t = arg max a∈A K r(a, θ t ) 6:</p><p>Observes x t and receive reward r(x t , z t ) 7:</p><p>Randomly initialize q 8:</p><p>Calculate L(q) using (3) and set L (q) = -∞ 9:</p><p>while L(q) -L (q) ≥ do 10:</p><p>Set L (q) = L(q)</p><p>11:</p><p>Update q t (z t ) using ( <ref type="formula" target="#formula_20">4</ref>), for all z t 12:</p><p>Update q(θ) using (5) 13:</p><p>Update L(q) using (3) 14:</p><p>end while 15: end for</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Examples of existing models, which are special cases of our framework: (a) cascade model, (b) position-based model, (c) semi-bandit, and (d) rank-1 bandit. See the details of the models in Section 3. The red nodes are decision nodes, the gray nodes are latent nodes, and the blue nodes are observed nodes. See formal definitions in Section 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Appendix B for the proof of Theorem 1. It is natural for the regret bound to be O( √ O max ). One can see this by considering special cases. For example, in combinatorial semi-bandits, O max = K and Kveton et al. (2015b) proved a O( √ K) regret bound. Intuitively, the O( √ K) term reflects the total reward magnitude in combinatorial semi-bandits. We conclude this section by comparing our regret bound with existing bounds in special cases. In cascading bandits, we have d = 0 4 and C = 1. Thus our regret bound is O( √ LO max n log n). On the other hand, the regret bound in Kveton et al. (2015a) is O( √ LKn log n). Since O max ≤ K, our regret bound is at most O( √ log n) larger. In combinatorial semi-bandits, we have d = 0, O max = K, and C = 1 (see Appendix A). Thus, our regret bound reduces to R B (n) ≤ O √ LKn log n . On the other hand, Theorem 6 of Kveton et al. (2015b) derives a O √ LKn log n worst-case regret bound for a UCB-like algorithm for combinatorial semi-bandits. Our regret bound is only O √ log n larger.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. A comparison between different bandit algorithms in various structured bandit problems.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. A comparison between variational inference and particle filtering (PF) with different numbers of particles, when approximating the posterior of Thompson sampling in influence diagram bandits.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>E</head><label></label><figDesc>[O max ] = nO max ,(14)where equality (a) follows from the tower property, and equality (b) follows from the definition of O max . Thus, we have ≤ 6(d + L)O max n log(n) (1 + log(n)) + (d + L)Putting everything together, we haveR B (n) ≤ C 6(d + L)O max n log(n) (1 + log(n)) + C + 2π 2 3 B (d + L) = O C (d + L)O max n log(n) .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>For each position k ∈ [K], the model has four nodes: decision node A k , which is set to the item at position k, a k ; latent attraction node W k , which is the attraction indicator of item a k ; latent examination node E k , which indicates that position k is examined; and observed click node C k , which indicates that item a k is clicked. The attraction of item a k is a Bernoulli random variable with mean µ a k . Therefore,P (W k = 1 | A k = a k ) = µ a k ,as in our model. The rest of the dynamics, that the item is clicked only if it is attractive and its position is examined, and that the examination of items stops upon a click, is encoded as</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>is the expected number of observations under a; hence, O max is the maximum expected number of observations over all actions. Let |X| and |Z| be the number of observed nodes and latent nodes, respectively. Notice that by definition, O max ≤ |X| + |Z|. Moreover, O max ≤ |X| if no latent variables are exactly inferred 3 ; and O max = |X| in the fully observable case. Our main result is stated below. Theorem 1 Under Assumptions 1 and 2, if we apply exact Thompson sampling idTS to influence diagram bandits,</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 .</head><label>1</label><figDesc>The comparison of variational inference and particle filtering (PF) in cascade model 1. We report the run time (in seconds) and average cumulative reward at 20000 steps. The results are averaged over 20 runs.</figDesc><table><row><cell></cell><cell>Reward</cell></row><row><cell>idTSvi</cell><cell>1990.502 ± 19.702 0.999 ± 0.001</cell></row><row><cell>idTSinc</cell><cell>212.772 ± 2.204 0.983 ± 0.006</cell></row><row><cell>PF (5 particles)</cell><cell>117.856 ± 1.049 0.835 ± 0.019</cell></row><row><cell>PF (10 particles)</cell><cell>246.291 ± 3.327 0.878 ± 0.015</cell></row><row><cell>PF (15 particles)</cell><cell>357.284 ± 2.529 0.926 ± 0.013</cell></row><row><cell>PF (20 particles)</cell><cell>454.844 ± 3.054 0.932 ± 0.015</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Carnegie Mellon University</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Google Research</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>DeepMind</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>Duke University</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_4"><p>Norwegian University of Science and Technology. Correspondence to: Tong Yu &lt;worktongyu@gmail.com&gt;.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_5"><p>To handle non-binary categorical variables, we can extend our algorithms (Section 4) by replacing Beta with Dirichlet.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_6"><p>Exact inference is often possible when some conditional distributions are known and deterministic. For instance, in the cascade model (Section</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_7"><p>3.1), the attractions of items above the click position can be exactly inferred.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_8"><p>Note that a parent of an observed node might not be observed, thus in general Omax ≤ |X|.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_9"><p>As is inKveton et al. (2015a), we assume that the deterministic conditional distributions in cascading bandits are known to the learning agent, thus d = 0.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_10"><p>Rigorously speaking, {Ht} n-1 t=0 is a filtration and Ht-1 is a σ-algebra.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">An introduction to MCMC for machine learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Andrieu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>De Freitas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Doucet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="5" to="43" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Finite-time analysis of the multiarmed bandit problem</title>
		<author>
			<persName><forename type="first">P</forename><surname>Auer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Cesa-Bianchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine Learning</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="235" to="256" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Active positive semidefinite matrix completion: Algorithms, theory and applications</title>
		<author>
			<persName><forename type="first">A</forename><surname>Bhargava</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Ganti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nowak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1349" to="1357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<title level="m">Pattern Recognition and Machine Learning</title>
		<imprint>
			<publisher>springer</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Weight uncertainty in neural network</title>
		<author>
			<persName><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Cornebise</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1613" to="1622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Combinatorial multiarmed bandit: General framework, results and applications</title>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning</title>
		<meeting>the 30th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="151" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Click models for web search</title>
		<author>
			<persName><forename type="first">A</forename><surname>Chuklin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Markov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Rijke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Synthesis Lectures on Information Concepts, Retrieval, and Services</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1" to="115" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Minimal exploration in structured stochastic bandits</title>
		<author>
			<persName><forename type="first">R</forename><surname>Combes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Magureanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Proutiere</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1763" to="1771" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Maximum likelihood from incomplete data via the em algorithm</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">P</forename><surname>Dempster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Laird</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Methodological)</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="22" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Combinatorial network optimization with unknown variables: Multiarmed bandits with linear rewards and individual observations</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Gai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Krishnamachari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE/ACM Transactions on Networking</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1466" to="1478" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Thompson sampling for complex online problems</title>
		<author>
			<persName><forename type="first">A</forename><surname>Gopalan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Mannor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Mansour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning</title>
		<meeting>the 31st International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="100" to="108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Reinforcement learning with deep energy-based policies</title>
		<author>
			<persName><forename type="first">T</forename><surname>Haarnoja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Soft actorcritic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor</title>
		<author>
			<persName><forename type="first">T</forename><surname>Haarnoja</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Influence diagrams. The Principles and Applications of Decision Analysis</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">E</forename><surname>Matheson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Bilinear bandits with low-rank structure</title>
		<author>
			<persName><forename type="first">K.-S</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Willett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Nowak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Stochastic rank-1 bandits</title>
		<author>
			<persName><forename type="first">S</forename><surname>Katariya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kveton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szepesvari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Vernade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="392" to="401" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Efficient Thompson sampling for online matrix-factorization recommendation</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kawale</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kveton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Tran-Thanh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chawla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="1297" to="1305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Cascading bandits: Learning to rank in the cascade model</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kveton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szepesvari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ashkan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="767" to="776" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Tight regret bounds for stochastic combinatorial semi-bandits</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kveton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ashkan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szepesvari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Eighteenth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Eighteenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="535" to="543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Perturbed-history exploration in stochastic multiarmed bandits</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kveton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szepesvari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ghavamzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Boutilier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Joint Conference on Artificial Intelligence</title>
		<meeting>the 28th International Joint Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Perturbed-history exploration in stochastic linear bandits</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kveton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szepesvari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ghavamzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Boutilier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the 35th Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Garbage in, reward out: Bootstrapping exploration in multi-armed bandits</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kveton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szepesvari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ghavamzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Lattimore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 36th International Conference on Machine Learning</title>
		<meeting>the 36th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3601" to="3610" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Randomized exploration in generalized linear bandits</title>
		<author>
			<persName><forename type="first">B</forename><surname>Kveton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szepesvari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Ghavamzadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Boutilier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the 23rd International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Multiple-play bandits in the position-based model</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lagrée</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Vernade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Cappe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1597" to="1605" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Toprank: A practical algorithm for online stochastic ranking</title>
		<author>
			<persName><forename type="first">T</forename><surname>Lattimore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kveton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Szepesvari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="3945" to="3954" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Contextual combinatorial cascading bandits</title>
		<author>
			<persName><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="1245" to="1253" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Customized nonlinear bandits for online response selection in neural conversation models</title>
		<author>
			<persName><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Lane</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Mengshoel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd AAAI Conference on Artificial Intelligence</title>
		<meeting>the 32nd AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="5245" to="5252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Ensemble sampling</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="3258" to="3266" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Efficient online recommendation via low-rank ensemble sampling</title>
		<author>
			<persName><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kveton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM Conference on Recommender Systems</title>
		<meeting>the 12th ACM Conference on Recommender Systems</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="460" to="464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Randomized prior functions for deep reinforcement learning</title>
		<author>
			<persName><forename type="first">I</forename><surname>Osband</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Aslanides</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Cassirer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Thompson sampling and approximate inference</title>
		<author>
			<persName><forename type="first">M</forename><surname>Phan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Abbasi-Yadkori</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Domke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep Bayesian bandits showdown: An empirical comparison of Bayesian deep networks for Thompson sampling</title>
		<author>
			<persName><forename type="first">C</forename><surname>Riquelme</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Snoek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 6th International Conference on Learning Representations</title>
		<meeting>the 6th International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning to optimize via posterior sampling</title>
		<author>
			<persName><forename type="first">D</forename><surname>Russo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics of Operations Research</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1221" to="1243" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Reinforcement learning: An introduction</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">S</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Dynamic programming and influence diagrams</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Tatman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">D</forename><surname>Shachter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man, and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="365" to="379" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">On the likelihood that one unknown probability exceeds another in view of the evidence of two samples</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">R</forename><surname>Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3/4</biblScope>
			<biblScope unit="page" from="285" to="294" />
			<date type="published" when="1933">1933</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Variational inference for the multi-armed contextual bandit</title>
		<author>
			<persName><forename type="first">I</forename><surname>Urteaga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wiggins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="698" to="706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Old dog learns new tricks: Randomized UCB for bandit problems</title>
		<author>
			<persName><forename type="first">S</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Mehrabian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kveton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the 23rd International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Efficient learning in large-scale combinatorial semi-bandits</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Kveton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ashkan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1113" to="1122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Stochastic particle-optimization sampling and the non-asymptotic convergence theory</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Carin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1877" to="1887" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Scalable Thompson sampling via optimal transport</title>
		<author>
			<persName><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carin</forename></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename></persName>
		</author>
		<editor>AISTATS</editor>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Factored bandits</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zimmert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Seldin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2835" to="2844" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
