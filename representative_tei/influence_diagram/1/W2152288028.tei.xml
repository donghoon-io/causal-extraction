<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Efficient non-myopic value-of-information computation for influence diagrams</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2008-04-29">29 April 2008</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName><forename type="first">Wenhui</forename><surname>Liao</surname></persName>
							<email>wenhui.liao@thomsonreuters.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Research &amp; Development</orgName>
								<orgName type="institution" key="instit2">Thomson-Reuters Corporation</orgName>
								<address>
									<addrLine>610 Opperman Drive</addrLine>
									<postCode>55123</postCode>
									<settlement>Eagan</settlement>
									<region>MN</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Qiang</forename><surname>Ji</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Electrical</orgName>
								<orgName type="department" key="dep2">Computer and Systems Engineering</orgName>
								<orgName type="institution">Rensselaer Polytechnic Institute</orgName>
								<address>
									<postCode>12180-3590</postCode>
									<settlement>Troy</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Efficient non-myopic value-of-information computation for influence diagrams</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2008-04-29">29 April 2008</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1016/j.ijar.2008.04.003</idno>
					<note type="submission">Received 4 January 2007 Received in revised form 17 April 2008 Accepted 21 April 2008</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-14T19:02+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Value-of-information Influence diagrams Decision making Central-limit theorem Stress modeling</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In an influence diagram (ID), value-of-information (VOI) is defined as the difference between the maximum expected utilities with and without knowing the outcome of an uncertainty variable prior to making a decision. It is widely used as a sensitivity analysis technique to rate the usefulness of various information sources, and to decide whether pieces of evidence are worth acquisition before actually using them. However, due to the exponential time complexity of exactly computing VOI of multiple information sources, decision analysts and expert-system designers focus on the myopic VOI, which assumes observing only one information source, even though several information sources are available. In this paper, we present an approximate algorithm to compute non-myopic VOI efficiently by utilizing the central-limit theorem. The proposed method overcomes several limitations in the existing work. In addition, a partitioning procedure based on the d-separation concept is proposed to further improve the computational complexity of the proposed algorithm. Both the experiments with synthetic data and the experiments with real data from a real-world application demonstrate that the proposed algorithm can approximate the true non-myopic VOI well even with a small number of observations. The accuracy and efficiency of the algorithm makes it feasible in various applications where efficiently evaluating a large amount of information sources is necessary.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In a wide range of decision-making problems, a common scenario is that a decision maker must decide whether some information is worth collecting, and what information should be acquired first given several information sources available. Each set of information sources is usually evaluated by value-of-information (VOI). VOI is a quantitative measure of the value of knowing the outcome of the information source(s) prior to making a decision. In other words, it is quantified as the difference in value achievable with or without knowing the information sources in a decision-making problem.</p><p>Generally, VOI analysis is one of the most useful sensitivity analysis techniques for decision analysis <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b24">25]</ref>. VOI analysis evaluates the benefit of collecting additional information in a specific decision-making context <ref type="bibr" target="#b26">[27]</ref>. General VOI analyses usually require three key elements: (1) A set of available actions and information collection strategies; (2) A model connecting the actions and the related uncertainty variables within the context of the decision; and (3) values for the decision outcomes. The methods of VOI analysis could be quite different when different models are used.</p><p>In this paper, we consider VOI analysis in decision problems modeled by influence diagrams. Influence diagrams were introduced by Howard and Matheson in 1981 <ref type="bibr" target="#b12">[13]</ref> and have been widely used as a knowledge representation framework to facilitate decision making and probability inference under uncertainty. An ID uses a graphical representation to capture the three diverse sources of knowledge in decision making: conditional relationships about how events influence each other in the decision domain; informational relationships about what action sequences are feasible in any given set of circumstances; and functional relationships about how desirable the consequences are <ref type="bibr" target="#b20">[21]</ref>. An ID can systematically model all the relevant random variables and decision variables in a compact graphical model.</p><p>In the past several years, a few methods have been proposed to compute VOI in IDs. Ezawa <ref type="bibr" target="#b7">[8]</ref> introduces some basic concepts about VOI and evidence propagation in IDs. Dittmer and Jensen <ref type="bibr" target="#b6">[7]</ref> present a method for calculating myopic VOI in IDs based on the strong junction tree framework <ref type="bibr" target="#b14">[15]</ref>. Shachter <ref type="bibr" target="#b24">[25]</ref> further improves this method by enhancing the strong junction tree as well as developing methods for reusing the original tree in order to perform multiple VOI calculations. Zhang et al. <ref type="bibr" target="#b27">[28]</ref> present an algorithm to speed up the VOI computation by making use of the intermediate computation results, which are obtained when computing the optimal expected value of the original ID without the observations from the information sources. Instead of computing VOI directly, <ref type="bibr" target="#b21">[22]</ref> describe a procedure to identify a partial order over variables in terms of their VOIs based on the topological relationships among variables in the ID. However, all these papers only focus on computing myopic VOI, which is based on two assumptions: (1) ''No competition:" each information source is evaluated in isolation, as if it were the only source available for the entire decision; (2) ''One-step horizon:" the decision maker will act immediately after consulting the source <ref type="bibr" target="#b20">[21]</ref>. These assumptions result in a myopic policy: every time, the decision maker evaluates the VOI of each information source one by one, and chooses the one with the largest VOI. Then the observations are collected from the selected information sources, the probabilities are updated, and all the remaining information sources are to be reevaluated again, and a similar procedure repeats.</p><p>Obviously, the assumptions are not always reasonable in some decision circumstances. Usually, the decision maker will not act after acquiring only one information source. Also, although a single information source may have low VOI and is not worth acquisition compared to its cost, several information sources used together may have high VOI compared to their combined cost. In this case, by only evaluating myopic VOI, the conclusion will be not to collect such information, which is not optimal since its usage together with other information sources can lead to high value for the decision maker. Therefore, given these limitations in myopic VOI, it is necessary to compute non-myopic VOI.</p><p>Non-myopic VOI respects the fact that the decision maker may observe more than one piece of information before acting, thus requires the consideration of any possible ordered sequence of observations given a set of information sources. Unfortunately, the number of the sequences grows exponentially as the number of available information sources increases, and thus it is usually too cumbersome to compute non-myopic VOI for any practical use, and this is why the before mentioned work only focuses on myopic VOI. Given these facts, an approximate computation of non-myopic VOI is necessary to make it feasible in practical applications. To the best of our knowledge, <ref type="bibr" target="#b10">[11]</ref> are the only ones who proposed a solution to this problem. In their approach, the central-limit theorem is applied to approximately compute non-myopic VOI in a special type of ID for the diagnosis problem, where only one decision node exists. Certain assumptions are required in their method: (1) all the random nodes and decision nodes in the ID are required to be binary; (2) the information sources are conditionally independent from each other given the hypothesis node, which is the node associated with the decision node and utility node.</p><p>Motivated by the method of Heckerman et al., we extend this method to more general cases<ref type="foot" target="#foot_0">foot_0</ref> : (1) all the random nodes can have multiple states and the decision node can have multiple rules (alternatives); (2) the information sources can be dependent given the hypothesis node; and (3) the ID can have a more general structure. But same as Heckerman et al.'s method, we only discuss the VOI computation in terms of IDs that have only one decision node. This decision node shares only one utility node with another chance node. With the proposed algorithm, non-myopic VOI can be efficiently approximated. In order to validate the performance of the proposed algorithm, we not only perform the experiments based on the synthetic data for various types of IDs, but also provide a real-world application with real data.</p><p>Because of the efficiency and accuracy of the proposed method, we believe that it can be widely used to choose the optimal set of available information sources for a wide range of applications. No matter what selection strategies people use to choose an optimal set, such as greedy approaches, heuristic searching algorithms, or brute-force methods, the proposed method can be utilized to evaluate any information set efficiently in order to speed up the selection procedure.</p><p>The following sections are organized as follows. Section 2 presents a brief introduction to influence diagrams. The detail of the algorithm is described in Section 3. Section 4 discusses the experimental results based on synthetic data. And a real application is demonstrated in Section 5. Finally, Section 6 gives the conclusion and some suggestions for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Influence diagrams</head><p>An influence diagram (ID) is a graphical representation of a decision-making problem under uncertainty. Its knowledge representation can be viewed through three hierarchical levels, namely, relational, functional, and numerical. At the relational level, an ID represents the relationships between different variables through an acyclic directed graph consisting of various node types and directed arcs. The functional level specifies the interrelationships between various node types and defines the corresponding conditional probability distributions. Finally, the numerical level specifies the actual numbers associated with the probability distributions and utility values <ref type="bibr" target="#b5">[6]</ref>.</p><p>Specifically, an ID includes three types of nodes: decision, chance (random), and value (utility) nodes. Decision nodes, usually drawn as rectangles, indicate the decisions to be made and their set of possible alternative values. Chance nodes, usually drawn as circles/ellipses, represent uncertain variables that are relevant to the decision problem. They are similar to the nodes in Bayesian networks <ref type="bibr" target="#b13">[14]</ref>, and are associated with conditional probability tables (CPTs). Value nodes, usually drawn as diamonds, are associated with utility functions to represent the utility of each possible combination of the outcomes of the parent node. The arcs connecting different types of nodes have different meanings. An arc between two chance nodes represents probabilistic dependence, while an arc from a decision node to a chance node represents functional dependence, which means the actions associated with the decision node affect the outcome of the chance node. An arc between two decision nodes implies time precedence, while an arc from a chance node to a decision node is informational, i.e., it shows which variable will be known to the decision maker before a decision is made <ref type="bibr" target="#b20">[21]</ref>. An arc pointing to a utility node represents value influence, which indicates that the parents of the utility node are those that directly affect its utility. Fig. <ref type="figure" target="#fig_0">1</ref> illustrates these arcs and gives corresponding interpretations.</p><p>Most IDs assume a precedence ordering of the decision nodes. A regular ID assumes that there is a directed path containing all decision nodes; a no-forgetting ID assumes that each decision node and its parents are also parents of the successive decision nodes; and a stepwise decomposable ID assumes that the parents of each decision node divide the ID into two separate fractions. In this paper, we consider IDs that have only one decision node, i.e., ignoring all previous decisions. The goal of ID modeling is to choose an optimal policy that maximizes the overall expected utility. A policy is a sequence of decision rules where each rule corresponds to one decision node. Mathematically, if there is only one decision node in an ID and assuming additive decomposition of the utility functions, the expected utility under a decision rule d given any available evidence e, denoted by EUðdjeÞ, can be defined as follows:</p><formula xml:id="formula_0">EUðdjeÞ ¼ X n i¼1 X X i pðX i je; dÞu i ðX i ; dÞ;<label>ð1Þ</label></formula><p>where u i is the utility function over the domain X i [ fDg. For example, X i could be the parents of the utility node that u i is associated with. To evaluate an ID is to find an optimal policy as well as to compute its optimal expected utility <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b25">26]</ref>. More detail about IDs can be found in <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b13">14]</ref>. Generally, the advantages of an ID can be summarized by its compact and intuitive formulation, its easy numerical assessment, and its effective graphical representation of dependence between variables for modeling decision making under uncertainty. These benefits make ID a widely used tool to model and solve complex decision problems in recent years.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approximate VOI computation</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Value of information</head><p>The VOI of a set of information sources is defined as the difference between the maximum expected utilities with and without the information sources <ref type="bibr" target="#b16">[17]</ref>. VOI can be used to rate the usefulness of various information sources and to decide whether pieces of evidence are worth acquisition before actually using the information sources <ref type="bibr" target="#b20">[21]</ref>.</p><p>We discuss the VOI computation in terms of IDs that have only one decision node. This decision node shares only one utility node with another chance node, as shown in Fig. <ref type="figure">2</ref>. And the decision node and the chance node are assumed to be independent. In the ID, the chance node H, named as hypothesis node, represents a mutually exclusive and exhaustive set of possible hypotheses h 1 ; h 2 ; . . . ; h h ; the decision node D represents a set of possible alternatives d 1 ; d 2 ; . . . ; d q ; the utility node U represents the utility of the decision maker, which depends on the outcome of H and D; and the chance nodes O 1 ; . . . ; O n represent possible observations from all kinds of information sources about the true state of H. And each O i may have multiple states. Let O ¼ fO 1 ; . . . ; O n g, the VOI of O, VOIðOÞ, w.r.t. the decision node D, can be defined as follows: where uðÞ denotes the utility function associated with the utility node U, EUðOÞ denotes the expected utility to the decision maker if O were observed, while EUðOÞ denotes the expected utility to the decision maker without observing O. Here the cost of collecting information from the information sources is not included; thus, the VOI can also be called perfect VOI <ref type="bibr" target="#b10">[11]</ref>. The net VOI is the difference between the perfect VOI and the cost of collecting information <ref type="bibr" target="#b11">[12]</ref>. Since after calculating the perfect VOI, the computation of the net VOI is just a subtraction of cost, we focus on the perfect VOI in the subsequent sections.</p><formula xml:id="formula_1">VOIðOÞ ¼ EUðOÞ À EUðOÞ;<label>ð2Þ</label></formula><formula xml:id="formula_2">EUðOÞ ¼ X o2O pðoÞ max d j 2D X h i 2H pðh i joÞuðh i ; d j Þ;<label>ð3Þ</label></formula><formula xml:id="formula_3">EUðOÞ ¼ max d j 2D X h i 2H pðh i Þuðh i ; d j Þ;<label>ð4Þ</label></formula><p>As shown in Eq. ( <ref type="formula" target="#formula_1">2</ref>), to compute VOIðOÞ, it is necessary to compute EUðOÞ and EUðOÞ respectively. Obviously, EUðOÞ is easier to compute, whereas directly computing EUðOÞ could be cumbersome. If the decision maker has the option to observe a subset of observations fO 1 ; . . . ; O n g and each O i has m possible values, then there are m n possible instantiations of the observations in this set. Thus, to compute EUðOÞ, there are m n inferences to be performed. In other words, the time complexity of computing VOI is exponential. It becomes infeasible to compute VOI(O) when n is not small.</p><p>The key to computing VOIðOÞ efficiently is to compute EUðOÞ, which can be rewritten as follows:</p><formula xml:id="formula_4">EUðOÞ ¼ X o2O pðoÞ max d j 2D X h i 2H pðh i joÞuðh i ; d j Þ ¼ X o2O max d j 2D X h i 2H pðoÞpðh i joÞuðh i ; d j Þ ¼ X o2O max d j 2D X h i 2H pðh i Þpðojh i Þuðh i ; d j Þ:<label>ð5Þ</label></formula><p>It is noticed that each instantiation of O corresponds to a specific optimal action for the decision node D. We define the decision function d : O ! D, which maps an instantiation of O into a decision in D. For example, dðoÞ ¼ d k indicates when the observation is o, the corresponding optimal decision is</p><formula xml:id="formula_5">d k , d k ¼ arg max d j 2D P h i 2H pðh i joÞuðh i ; d j Þ.</formula><p>Therefore we can divide all the instantiations of O into several subsets, where the optimal action is the same for those instantiations in the same subset. Specifically, if D has q decision rules, fd 1 ; . . . ; d q g, all the instantiations of O can be divided into q subsets, o d 1 ; o d 2 ; . . . ; o dq , where o d k ¼ fo 2 OjdðoÞ ¼ d k g. Fig. <ref type="figure">3</ref> illustrates the relationships between each instantiation and the q subsets. Thus, from Eq. ( <ref type="formula" target="#formula_4">5</ref>), EU(O) can be further derived as follows:</p><formula xml:id="formula_6">EUðOÞ ¼ X h i 2H pðh i Þ X q k¼1 X o2o d k pðojh i Þuðh i ; d k Þ:<label>ð6Þ</label></formula><p>In the next several sections, we show how to compute EUðOÞ efficiently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Decision boundaries</head><p>In Eq. ( <ref type="formula" target="#formula_6">6</ref>), the difficult part is to compute P o2o d k pðojh i Þ because the size of the set o d k could be very large based on the previous analysis. In order to compute it efficiently, it is necessary to know how to divide all the instantiations of O into the q subsets. We first focus on the case that H has only two states, h 1 , h 2 , and then extend it to the general case in Section 3.4.</p><p>Based on the definition, the expected utility of taking the action</p><formula xml:id="formula_7">d k is EUðd k Þ ¼ pðh 1 Þ Ã u 1k þ pðh 2 Þ Ã u 2k , where u 1k ¼ uðh 1 ; d k Þ, and u 2k ¼ uðh 2 ; d k Þ.</formula><p>We can sort the index of all the decision rules based on the utility functions, such that u 1k &gt; u 1j and u 2k &lt; u 2j for k &lt; j. Fig. <ref type="figure" target="#fig_1">4</ref> gives an example of the utility function uðH; DÞ. As shown in the figure, as k increases, u 1k decreases and u 2k increases. If there is an action d i that cannot be sorted according to this criterion, it is either </p><formula xml:id="formula_8">D U O n-1 O 1 O i O j ... ... O 2 O n θ Fig. 2.</formula><p>Proof. see Appendix. h</p><p>The Proof of Proposition 2 establishes Eq. ( <ref type="formula" target="#formula_9">7</ref>) by showing that both sides of this equation express the probability that d k is the optimal decision for h 1 . Based on Proposition 2, we can get the following corollary.</p><formula xml:id="formula_10">Corollary 1 X o2o d k pðojh 1 Þ ¼ pðp Ã kl 6 pðh 1 joÞ 6 p Ã ku jh 1 Þ;<label>ð8Þ</label></formula><formula xml:id="formula_11">X o2o d k pðojh 2 Þ ¼ pðp Ã kl 6 pðh 1 joÞ 6 p Ã ku jh 2 Þ:<label>ð9Þ</label></formula><p>The equations in Corollary 1 indicate the probability that the decision maker will take the optimal decision d k after observing new evidence, given the situation that the state of H is h i before collecting the evidence.</p><p>Based on Corollary 1, the problem of computing P Let us take a closer look at pðh 1 joÞ pðh 2 joÞ because it is critical in the approximate algorithm. If all the O i nodes are conditionally independent from each other given H, based on the chain rule:</p><formula xml:id="formula_12">o2o d k pðojh i Þ; i ¼ 1; 2; (from Eq. (<label>6</label></formula><formula xml:id="formula_13">pðh 1 jOÞ pðh 2 jOÞ ¼ pðO 1 jh 1 Þ pðO 1 jh 2 Þ Á Á Á pðO n jh 1 Þ pðO n jh 2 Þ pðh 1 Þ pðh 2 Þ :<label>ð10Þ</label></formula><p>Usually some O i s may not be conditionally independent given H. We will show that pðh 1 joÞ pðh 2 joÞ is approximately distributed as a log-normal random variable. However, in order to prove it, it is necessary to obtain a format similar to Eq. ( <ref type="formula" target="#formula_13">10</ref>) even when O i s are not conditionally independent. We thus propose a partitioning procedure to partition O into several groups based on the principle of d-separation <ref type="bibr" target="#b20">[21]</ref>, where the nodes in one group are conditionally independent from the nodes in other groups. This procedure consists of three steps.</p><p>( (2) Build an undirected graph to model the relationships between the nodes. In such a graph, each vertex represents an O i node, and each edge between two vertices indicates that the two corresponding nodes are dependent according to the rules in Step 1.</p><p>(3) Partition the graph into disjoint connected subgraphs. A depth first search (DFS) algorithm <ref type="bibr" target="#b3">[4]</ref> is used to partition the graph into several connected components (disjoint connected subgraphs) so that each component is disconnected from other components. The nodes in each connected component are conditionally independent from the nodes in any other connected components. Therefore, each connected component corresponds to one group.</p><p>For example, for the ID in Fig. <ref type="figure" target="#fig_2">5a</ref>, with the partitioning procedure, the O i nodes can be divided into five groups, fO 1 ; O 2 g, fO 3 ; O 4 ; O 5 g, fO 6 g, fO 7 g, and fO 8 ; O 9 g. Fig. <ref type="figure" target="#fig_2">5b</ref> shows the graph built by the partitioning procedure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2.">Central-limit theorem</head><p>Generally, with the partition procedure presented in the previous subsection, O can be automatically divided into several sets, named O s 1 ; O s 2 ; . . . ; O sg , where g is the overall number of the groups. Thus, Eq. ( <ref type="formula" target="#formula_13">10</ref>) can be modified as follows:</p><formula xml:id="formula_14">pðh 1 jOÞ pðh 2 jOÞ ¼ pðO s 1 jh 1 Þ pðO s 1 jh 2 Þ Á Á Á pðO sg jh 1 Þ pðO sg jh 2 Þ pðh 1 Þ pðh 2 Þ ) ln pðh 1 jOÞ pðh 2 jOÞ ¼ X g i¼1 ln pðO s i jh 1 Þ pðO s i jh 2 Þ þ ln pðh 1 Þ pðh 2 Þ ) ln / ¼ X g i¼1 w i þ c;</formula><p>where / ¼ pðh 1 jOÞ pðh 2 jOÞ</p><formula xml:id="formula_15">; w i ¼ ln pðO s i jh 1 Þ pðO s i jh 2 Þ ; c ¼ ln pðh 1 Þ pðh 2 Þ :<label>ð11Þ</label></formula><p>In the above equation, c can be regarded as a constant reflecting the state of H before any new observation is obtained and any new decision is taken. Here, we assume pðh 2 jOÞ, pðO s i jh 2 Þ, and pðh 2 Þ are not equal to 0. Let W ¼ P g i¼1 w i be the sum of w i . Following <ref type="bibr" target="#b10">[11]</ref>, we use the cental-limit theorem to approximate W. The central-limit theorem <ref type="bibr" target="#b8">[9]</ref> states that the sum of independent variables approaches a Gaussian distribution when the number of variables becomes large. Also, the expectation and variance of the sum is the sum of the expectation and variance of each individual random variable. Thus, regarding each w i as an independent variable, W then follows a Gaussian distribution. Then, based on Eq. ( <ref type="formula" target="#formula_15">11</ref>), / will be a log-normal distribution. For a random variable X, if lnðXÞ has a Gaussian distribution, we say X has a lognormal distribution. The probability density function is:</p><formula xml:id="formula_16">pðxÞ ¼ 1 S ffiffiffiffi 2p p x e Àðln xÀMÞ 2 =ð2S 2 Þ , denoted as X $ LogNðM; S 2 Þ [5]</formula><p>, where M and S are the mean and standard deviation of the variable's logarithm <ref type="bibr" target="#b0">[1]</ref>. In order to assess the parameters (mean and var- iance) of the log-normal distribution, we need to compute the mean and the variance of each w i . The computational process is shown as follows.</p><formula xml:id="formula_17">D O 1 O 2 H 1 O 3 O 4 H 2 O 7 O 6 H 3 O 5 O 8 O 9 O 1 O 2 O 3 O 4 O 7 O 6 O 5</formula><p>Assume O s i has r i instantiations, fo s i 1 ; . . . ; o s i r i g, where r i is the product of the number of the states for each node in the group O s i , e.g., if O s i ¼ fO 1 ; O 2 g, and both O 1 and O 2 have three states, then r i ¼ 3 Ã 3 ¼ 9. Table <ref type="table">1</ref> gives the value and the probability distribution for each w i :</p><p>Based on the table, the expected value l, and the variance r 2 for each w i can be computed as follows:</p><formula xml:id="formula_18">lðw i jh 1 Þ ¼ X r i j¼1 pðo s i j jh 1 Þln pðo s i j jh 1 Þ pðo s i j jh 2 Þ ;<label>ð12Þ</label></formula><formula xml:id="formula_19">r 2 ðw i jh 1 Þ ¼ X r i j¼1 pðo s i j jh 1 Þln 2 pðo s i j jh 1 Þ pðo s i j jh 2 Þ À l 2 ðw i jh 1 Þ:<label>ð13Þ</label></formula><p>By the central-limit theorem, the expected value and the variance of W can be obtained by the following equations:</p><formula xml:id="formula_20">lðWjh 1 Þ ¼ X g i¼1 lðw i jh 1 Þ; ð14Þ r 2 ðWjh 1 Þ ¼ X g i¼1 r 2 ðw i jh 1 Þ:<label>ð15Þ</label></formula><p>Therefore, based on Eq. ( <ref type="formula" target="#formula_15">11</ref>), for W $ NðlðWjh 1 Þ; r 2 ðWjh 1 ÞÞ, we have / $ LogNðlðWjh 1 Þ þ c; r 2 ðWjh 1 ÞÞ, where LogN denotes the log-normal distribution. After getting the probability distribution function and the function parameters for / in Eq. ( <ref type="formula" target="#formula_15">11</ref>), we are ready to assess the non-myopic VOI.</p><p>Before we go to the next section, we first analyze the computational steps involved in computing the parameters for the log-normal distribution, which is the most time-consuming part in the algorithm. Based on Eqs. ( <ref type="formula" target="#formula_18">12</ref>) and ( <ref type="formula">14</ref>), the overall number of the computational steps is 4 P g i¼1 r i þ 2g. We will show that this number is much smaller than the overall number of the computational steps in the exact computational method during the algorithm analysis in Section 3.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.3.">Approximate non-myopic value-of-information</head><p>Based on Proposition 1 in Section 3. </p><formula xml:id="formula_21">X o2o d k pðojh 1 Þ ¼ pð/ Ã kl 6 / 6 / Ã ku jh 1 Þ:<label>ð16Þ</label></formula><p>Furthermore, from Section 3.3.2, we know that</p><formula xml:id="formula_22">/ $ LogNðlðWjh 1 Þ þ c; r 2 ðWjh 1 ÞÞ, thus, pð/ Ã kl 6 / 6 / Ã ku jh 1 Þ ¼ 1 rðWjh 1 Þ ffiffiffiffiffiffi ffi 2p p x Z / Ã ku / Ã kl e Àðln xÀlðWjh 1 ÞÀcÞ 2 2r 2 ðWjh 1 Þ dx;<label>ð17Þ</label></formula><p>pð/ Ã kl 6 / 6 / Ã ku jh 2 Þ can be computed in the same way by replacing h 1 with h 2 in the previous equations. Therefore, VOI can be approximated by combining Eqs. ( <ref type="formula" target="#formula_1">2</ref>), ( <ref type="formula" target="#formula_6">6</ref>), <ref type="bibr" target="#b15">(16)</ref>, and <ref type="bibr" target="#b16">(17)</ref>. Fig. <ref type="figure" target="#fig_3">6</ref> shows the key equations of the algorithm when H has only two states. In summary, to approximate VOIðOÞ efficiently, the key is to compute EUðOÞ, which leads to an approximation of P o2o d k pðojh 1 Þ with the log-normal distribution by exploiting the central-limit theorem and the decision boundaries.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Generalization</head><p>In the previous algorithm, the node H only allows two states, although the other random nodes and the decision node can be multiple states. However, in real-world applications, H may have more than two states. In this section, we extend the algorithm to the case that H can have several states too. Assume H has h states, h 1 ; . . . ; h h , and still, d has q rules, d 1 ; . . . ; d q , similarly to Eq. ( <ref type="formula" target="#formula_15">11</ref>), we have the following equations:</p><p>Table <ref type="table">1</ref> The probability distribution of wi</p><formula xml:id="formula_23">w i pðw i jh 1 Þ pðw i jh 2 Þ ln pðo s i 1 jh1 Þ pðo s i 1 jh2 Þ pðo si 1 jh 1 Þ pðo si 1 jh 2 Þ . . . . . . . . . ln pðo s i r i jh1 Þ pðo s i r i jh2 Þ pðo si ri jh 1 Þ pðo si ri jh 2 Þ pðh i jOÞ pðh h jOÞ ¼ pðO s 1 jh i Þ pðO s 1 jh h Þ Á Á Á pðO sg jh i Þ pðO sg jh h Þ pðh i Þ pðh h Þ ; i 6 ¼ h ) ln pðh i jOÞ pðh h jOÞ ¼ X g k¼1 ln pðO s k jh i Þ pðO s k jh h Þ þ ln pðh i Þ pðh h Þ ) ln / i ¼ X g k¼1 w i k þ c i ; where / i ¼ pðh i jOÞ pðh h jOÞ ; w i k ¼ ln pðO s k jh i Þ pðO s k jh h Þ ; c i ¼ ln pðh i Þ pðh h Þ :<label>ð18Þ</label></formula><formula xml:id="formula_24">Let W i ¼ P g k¼1 w i k , i 6 ¼ h, W i still has a Gaussian distribution.</formula><p>Here, we assume pðh h jOÞ, pðO s k jh h Þ, and pðh h Þ are not equal to 0. The similar method in Section 3.3 can be used to compute the variance and the mean. Specifically, for the new defined w i k in the above equation, Table <ref type="table">1</ref> can be modified as follows (see Table <ref type="table">2</ref>).</p><p>Thus, we get the following equations:</p><formula xml:id="formula_25">lðw i k jh j Þ ¼ X r k l¼1 pðo s k l jh j Þ ln pðo s k l jh i Þ pðo s k l jh h Þ ; 1 6 i &lt; h; 1 6 j 6 h; 1 6 k 6 g;<label>ð19Þ</label></formula><formula xml:id="formula_26">r 2 ðw i k jh j Þ ¼ X r k l¼1 pðo s k l jh j Þln 2 pðo s k l jh i Þ pðo s k l jh h Þ À l 2 ðw i k jh j Þ:<label>ð20Þ</label></formula><p>Similar to Eq. ( <ref type="formula">14</ref>), the expected value and the variance of W i can be obtained as we see here:</p><formula xml:id="formula_27">lðW i jh j Þ ¼ X g k¼1 lðw i k jh j Þ; 1 6 i &lt; h; 1 6 j 6 h;<label>ð21Þ</label></formula><formula xml:id="formula_28">r 2 ðW i jh j Þ ¼ X g k¼1 r 2 ðw i k jh j Þ:<label>ð22Þ</label></formula><p>Accordingly, / i follows the log-normal distribution with</p><formula xml:id="formula_29">S ij ¼ rðW i jh j Þ and M ij ¼ lðW i jh j Þ þ c i .</formula><p>We denote the probability density function of / i given h j as f h j ð/ i Þ. Eqs. ( <ref type="formula" target="#formula_25">19</ref>) and <ref type="bibr" target="#b20">(21)</ref> show that the overall number of the computational steps to assess the parameters for the log-normal distributions is 4h</p><formula xml:id="formula_30">P g k¼1 r k þ 2hðh À 1Þg when h &gt; 2. Even though f h j ð/ i Þ</formula><p>can be easily obtained, it is still necessary to get the decision boundaries for each optimal decision in order to efficiently compute P</p><formula xml:id="formula_31">o2o d k pðojh j Þ.</formula><p>Therefore, a set of linear inequality functions need to be solved when H has more than two states. For example, if d k is the optimal action, EUðd k Þ must be larger than the expected utility of taking any other action. Based on this, a set of linear inequality functions can be obtained: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Table 2</head><p>The probability distribution of</p><formula xml:id="formula_32">w i k w i k pðw i k jh 1 Þ . . . pðw i k jh h Þ ln pðo s k 1 jhi Þ pðo s k 1 jhh Þ pðo sk 1 jh 1 Þ . . . pðo sk 1 jh h Þ . . . . . . . . . . . . ln pðo s k r k jhi Þ pðo s k r k jhh Þ pðo sk rk jh 1 Þ . . . pðo sk rk jh h Þ pðh 1 Þu 1k þ pðh 2 Þu 2k þ Á Á Á þ pðh h Þu hk P pðh 1 Þu 1j þ Á Á Á þ pðh h Þu hj ) u 1k À u 1j þ u hj À u hk u hj À u hk Á pðh 1 Þ þ Á Á Á þ u ðhÀ1Þk À u ðhÀ1Þj þ u hj À u hk u hj À u hk Á pðh hÀ1 Þ P 1 ) u 1k À u 1j u hj À u hk Á pðh 1 Þ pðh h Þ þ Á Á Á þ u ðhÀ1Þk À u ðhÀ1Þj u hj À u hk Á pðh hÀ1 Þ pðh h Þ P 1:<label>ð23Þ</label></formula><p>We assume u hj À u hk &gt; 0; otherwise, ''P" is changed to ''6" in the last inequality. Let A k be the solution region of the above linear inequalities, then</p><formula xml:id="formula_33">X o2o d k pðojh j Þ ¼ Z A k f h j ð/ 1 Þ Á Á Á f h j ð/ hÀ1 Þ dA k ; 1 6 j 6 h; 1 6 k 6 q:<label>ð24Þ</label></formula><p>The right side of Eq. ( <ref type="formula" target="#formula_33">24</ref>) is an integral over the solution region A k decided by the linear inequalities. We first demonstrate how to solve the integral when H has three states, and then introduce the method for the case that H has more than three states.</p><p>When H has three states, Eq. ( <ref type="formula" target="#formula_32">23</ref>) can be simplified as follows:</p><formula xml:id="formula_34">pðh 1 Þu 1k þ pðh 2 Þu 2k þ pðh 3 Þu 3k P pðh 1 Þu 1j þ pðh 2 Þu 2j þ pðh 3 Þu 3j ) a 1kj Á pðh 1 Þ pðh 3 Þ þ a 2kj Á pðh 2 Þ pðh 3 Þ P 1;</formula><p>where a 1kj ¼</p><formula xml:id="formula_35">u 1k À u 1j u 3j À u 3k</formula><p>and a 2kj ¼</p><formula xml:id="formula_36">u 2k À u 2j u 3j À u 3k :<label>ð25Þ</label></formula><p>In the above, it is assumed that u 3j &gt; u 3k ; if u 3j &lt; u 3k , then ''P" is changed to ''6" in the last inequality. And Eq. ( <ref type="formula" target="#formula_33">24</ref>) can be simplified as follows:</p><formula xml:id="formula_37">X o2o d k pðojh j Þ ¼ Z A k f h j ð/ 1 Þf h j ð/ 2 ÞdA k ; 1 6 k 6 q; 1 6 j 6 3;<label>ð26Þ</label></formula><p>A k is decided by ðq À 1Þ linear inequalities and each inequality has two variables / 1 and / 2 as defined in Eq. ( <ref type="formula" target="#formula_36">25</ref>). We use the following steps to solve this integral when A k is a finite region.</p><p>1. Identify all the lines that define the inequalities and find all the intersection points between any two lines as well as the intersection points between any line and the x (or y) axis. 2. Choose the intersection points that satisfy all the linear inequalities, and use them as vertices to form a polygon. 3. Divide the polygon into several simple regions:Specifically, for each vertex, we generate a line crossing this vertex and parallel to the y-axis. The lines then divide the polygon into several simple regions. 4. Evaluate the integral in each simple region and sum the values together.</p><p>An example of the solution region is shown in Fig. <ref type="figure">7</ref>. In this example, if a 1kj &gt; a 1kj ði 6 ¼ jÞ, then a 2kj &gt; a 2kj too. Therefore, the solution region can be decided by the intersection points of the lines that are defined by the linear inequalities and the axes. For example, in Fig. <ref type="figure">7</ref>, A k is decided by a-d, which are selected from the intersection points fð1=a 1kj ; 0Þ; ð0; 1=a 2kj Þ; j ¼ 1; . . . ; q; j 6 ¼ kg. Based on <ref type="bibr" target="#b2">[3]</ref>, the time complexity of solving m linear inequalities with n variables (each inequality only has two variables) is Oðmn log m þ mn 2 log 2 nÞ. In this case, n is 2 and m is q À 1.</p><p>Fig. <ref type="figure">7</ref>. A solution region of a group of linear inequalities.</p><p>When H has more than three states, the integral needs to be performed in a high-dimension space (dimension is larger than 2). Therefore, we solve it with Quasi-Monte Carlo integration <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b15">16]</ref>, which is a popular method to handle multiple integral. Quasi-Monte Carlo integration picks points based on sequences of quasirandom numbers over some simple domain A 0 k which is a superset of A k , checks whether each point is within A k , and estimates the area (n-dimensional content) of A k as the area of A 0 k multiplied by the fraction of points falling within A k . Such a method is implemented by Mathematica <ref type="bibr" target="#b19">[20]</ref>, which can automatically handle a multiple integral with a region implicitly defined by multiple inequality functions. Fig. <ref type="figure" target="#fig_4">8</ref> shows the key equations of the algorithm when H has multiple states. The main equations are similar to those in Fig. <ref type="figure" target="#fig_3">6</ref>. However, since H has multiple states, it becomes more complex to obtain the parameters of the log-normal distribution and perform the integration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Algorithm analysis</head><p>Now, we analyze the computational complexity of the proposed approximation algorithm compared to the exact computational method. For simplicity, assume that the number of the state of each O i node is m, and there are n nodes in the set O. Assume we only count the time used for computing expected utilities. Then the computational complexity of the exact VOI computational method is approximately hm n , where h is the number of the state of the H node. With the approximation algorithm, the computational complexity is reduced to hm k , where h is the number of the state of the H node, and k is the number of O i nodes in the maximum group among fO s 1 ; . . . ; O sg g. In the best case, if all the O i nodes are conditionally independent given H, the time complexity is about linear with respect to m. In the worst case, if all the O i nodes are dependent, the time complexity is approximately m n . However, usually, in most real-world applications, k is less than n, thus, the approximate algorithm is expected to be more efficient than the exact computational method, as will be shown in the experiments. For example, for the ID in Fig. <ref type="figure" target="#fig_2">5</ref>, n ¼ 9, m ¼ 4, h ¼ 3, and q ¼ 3. Then, for the exact computation, the number of computations is around 3 Ã 4 9 ¼ 786432, while using the approximate algorithm, the number of computations is only around 3 Ã 4 3 ¼ 192.</p><p>However, in addition to the cost of computing expected utilities, the approximation algorithm also includes some extra costs: sorting the utility functions (Section 3.2), partitioning the O set (Section 3.3.1), and deciding the decision boundaries (Section 3.2) when H has two states, or performing the integral when H has more than two states (Section 3.4). These costs are not included in the above analysis. In general, the extra time in these steps is much less than the time used for computing expected utilities. For example, the time complexity of sorting is Oðq logðqÞÞ, the time complexity of the partition procedure is OðjVj þ jEjÞ (V is the set of vertex, and E is the set of edges in an ID), and the time complexity in deciding the decision boundaries when h has two states is Oðq 2 Þ. When h has more than two states, deciding the decision boundaries needs additional time. Empirically, it does not affect the overall speed, as will be shown in the experiments. In addition, most steps in computing expected utilities involve performing inferences in an ID, which is usually NP-hard and thus consumes much more time than a step in the procedures of sorting, partitioning, and integrating. he/she is pursuing. This portion is called predictive portion. On the other hand, the lower portion of the diagram, from the ''stress" node to the leaf nodes, depicts the observable features that reveal stress. These features include the quantifiable measures on the user physical appearance, physiology, behaviors, and performance. This portion is called diagnostic portion. The hybrid structure enables the ID to combine the predictive factors and observable evidence in user stress inference. For more detail please refer to <ref type="bibr" target="#b18">[19]</ref>.</p><p>To provide timely and appropriate assistance to relieve stress, two types of decision nodes are embedded in the model to achieve this goal. The first type is the assistance node associated with the stress node, which includes three types of assistance that have different degrees of impact and intrusiveness to a user. Another type of decision nodes is the sensing action node (S i node in Fig. <ref type="figure" target="#fig_0">10</ref>). It decides whether to activate a sensor for collecting evidence or not. Through the ID, we decide the sensing actions and the assistance action sequentially. In order to first determine the sensing actions (which sensors should be turned on), VOI is computed for a set S consisting of S i . Using the notations defined before, we have VOIðSÞ ¼ VOIðEÞ À P S i 2S u i ðS i Þ, where E is the set of observations corresponding to S and VOIðEÞ ¼ EUðEÞ À EUðEÞ. Fig. <ref type="figure" target="#fig_5">11</ref> shows the experimental results for the stress model. We enumerate all the possible combinations of sensors and then compute the value-of-information for each combination. Chart (a) illustrates the average VOI errors for different sensor sets with the same size. And Chart (b) displays the Euclidean distance between the true and estimated probabilities P o2o d k pðojh i Þ (Eq. ( <ref type="formula" target="#formula_37">26</ref>)). Similarly to the simulation experiments, the error decreases as the size of O set increases, and the computational time increases almost linearly in the approximation algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions and future work</head><p>As a concept commonly used in influence diagrams, VOI is widely used as a criterion to rate the usefulness of various information sources, and to decide whether pieces of evidence are worth acquiring before actually using the information sources. Due to the exponential time complexity of computing non-myopic VOI for multiple information sources, most researchers focus on the myopic VOI, which requires the assumptions (''No competition" and ''One-step horizon") that may not meet the requirements of real-world applications.</p><p>We thus proposed an algorithm to approximately compute non-myopic VOI efficiently by utilizing the central-limit theorem. Although it is motivated by the method of <ref type="bibr" target="#b10">[11]</ref>, it overcomes the limitations of their method, and works for more general cases, specifically, no binary-state assumption for all the nodes and no conditional-independence assumption for the information sources. Table <ref type="table">5</ref> compares our method with the method in <ref type="bibr" target="#b10">[11]</ref>. Due to the benefits of our method, it can be applied to a much broader field. The experiments demonstrate that the proposed algorithm can approximate the true non-myopic VOI well, even with a small number of observations. The efficiency of the algorithm makes it a feasible solution in various applications when efficiently evaluating a lot of information sources is necessary. Nevertheless, the proposed algorithm focuses on the influence diagrams with one decision node under certain assumptions. For example, currently, we assume the hypothesis node H and the decision node d are independent. If D and H are dependent, but conditionally independent given the observation set O, Eqs. ( <ref type="formula" target="#formula_4">5</ref>) and ( <ref type="formula" target="#formula_6">6</ref>) will not be affected, so our algorithm can still apply. However, if D and H are dependent given O, it may be difficult to directly apply our algorithm. Another scenario is that when there are more than one hypothesis node and/or utility nodes. One possible solution is to group all these hypotheses nodes into one. We would like to study these issues in the future.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig.1. Interpretations of arcs in an ID, where circles represent chance (random) nodes, rectangles for decision nodes, and diamonds for value (utility) nodes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. An example of the utility function UðH; DÞ.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. (a) An ID example; (b) the graph built by the partitioning procedure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. The key equations to approximate VOI when H has only two states, D has multiple rules, and the other nodes have multiple states.</figDesc><graphic coords="8,107.72,54.71,322.59,187.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. The key equations to compute VOI when H has multiple states.</figDesc><graphic coords="10,107.72,467.60,322.56,205.51" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 11 .</head><label>11</label><figDesc>Fig. 11. Results for the stress modeling: (a) average errors with the approximation algorithm; (b) Euclidean distance between the true and approximated P o2o d k pðojhiÞ; (c) computational time (log(t), unit is second); (d) true VOI vs. approximated VOI.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>An ID example for non-myopic VOI computation. H is the hypothesis node, D is the decision node, and U is the utility node. Oi represents possible observations from an information source. There could be hidden nodes between H and Oi. by another action, or it dominates another action. (If uðd i ; HÞ is always larger than uðd j ; HÞ, no matter what state of H is, we say d i dominates d j ). Then the dominated action can be removed from the set of possible actions, without changing the optimal policy. Proposition 1. Let r jk ¼ u 2j Àu 2k u 1k Àu 1j þu 2j Àu 2k , p Ã kl ¼ max k&lt;j6q r jk , and p Ã ku ¼ min 16j&lt;k r jk , then d k is the optimal action if and only if p Ã</figDesc><table><row><cell>Proposition 2</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>X</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>o2o d k</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>o 1 o 2</cell><cell>...</cell><cell>o i</cell><cell>...</cell><cell>o x</cell><cell>o d1</cell><cell>o d2</cell><cell>...</cell><cell>o dq</cell></row><row><cell></cell><cell></cell><cell>a</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>b</cell><cell></cell></row></table><note><p><p>Fig. 3. (a) Each o i corresponds to an instantiation; (b) all the instantiations can be divided into q subsets, where each instantiation in the set o d i corresponds to the optimal decision d i .</p>dominated kl 6 pðh 1 Þ 6 p Ã ku . In addition, p Ã ql ¼ 0 and p Ã 1u ¼ 1. (Here k is the index of an action.) Proof. see Appendix. h Proposition 1 presents that if the probability of H being h 1 is between p Ã kl and p Ã ku , d k is the optimal decision. From this, we can further derive Proposition 2. pðoÞ ¼ pðp Ã kl 6 pðh 1 joÞ 6 p Ã ku Þ:</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>)) transfers to the problem of computing pðp Ã kl 6 pðh 1 joÞ 6 p Ã ku jh i Þ, which is the topic of the next section. We will focus on pðp Ã kl 6 pðh 1 joÞ 6 p Ã ku jh 1 Þ only because the procedure of computing pðp Ã kl 6 pðh 1 joÞ 6 p Ã ku jh 2 Þ is similar. To compute pðp Ã kl 6 pðh 1 joÞ 6 p Ã ku jh 1 Þ, one way is to treat pðh 1 joÞ as a random variable. If the probability density function of this variable is known, it will be easy to compute pðp Ã</figDesc><table><row><cell cols="5">3.3. Approximation with central-limit theorem</cell></row><row><cell cols="5">3.3.1. A partitioning procedure</cell></row><row><cell>p Ã kl 1Àp Ã kl</cell><cell>6 pðh 1 joÞ pðh 2 joÞ 6</cell><cell>p Ã ku ku 1Àp Ã</cell><cell>jh 1</cell><cell>.</cell></row></table><note><p>kl 6 pðh 1 joÞ 6 p Ã ku jh 1 Þ. However, it is hard to get such a probability density function directly. But we notice that pðp Ã kl 6 pðh 1 joÞ 6 p Ã ku jh 1 Þ ¼ p p Ã kl 1Àp Ã kl 6 pðh 1 joÞ pðh 2 joÞ 6 p Ã ku 1Àp Ã ku jh 1 . Based on the transformation property between a random variable and its function [2], it is straightforward that pðp Ã kl 6 pðh 1 joÞ 6 p Ã ku jh 1 Þ ¼ p</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>1) Decide whether two nodes, O i , O j , are conditionally independent given H by exploring the ID structure based on four rules: (i) if there is a directed path between O i and O j without passing H, O i and O j are dependent; (ii) if both O i and O j are the ancestors of H, O i and O j are dependent given H; (iii) after removing the links to and from H from the original ID, if O i and O j have common ancestors, or O i is O j 's ancestor, or vice versa, then O i and O j are dependent; and (iv) in all the other cases, O i and O j are conditionally independent given H.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head></head><label></label><figDesc>2, we know that d k is the optimal action with the probability pðp Ã kl 6 pðh 1 joÞ 6 p Ã</figDesc><table><row><cell>which is equivalent to p</cell><cell>p Ã kl 1Àp Ã kl</cell><cell>6 / 6</cell><cell>p Ã ku 1Àp Ã ku</cell><cell>as shown in Section 3.3.1. Let / Ã kl ¼</cell><cell>p Ã kl 1Àp Ã kl</cell><cell>, and / Ã ku ¼</cell><cell>p Ã ku 1Àp Ã</cell><cell>ku Þ,</cell></row></table><note><p><p>ku</p>, thus, d k is the optimal decision if and only if / Ã kl 6 / 6 / Ã ku . Then, based on Corollary 1 in Section 3.2, the following equation stands:</p></note></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>A brief version of this extension can be found in<ref type="bibr" target="#b17">[18]</ref>. W. Liao, Q. Ji / International Journal of Approximate Reasoning 49 (2008) 436-450</p></note>
		</body>
		<back>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>The experiments are designed to demonstrate the performance of the proposed algorithm compared to the exact VOI computation. We limit the ID test model with at most 5 layers 2 and up to 11 information sources due to the exponential computational time behind the exact computation. Ten different ID models are constructed, where in one of the IDs the O nodes are conditionally independent given the H node. Table <ref type="table">3</ref> describes the structures of these IDs. The IDs are parameterized with 150 sets of different conditional probability tables and utility functions, a process which yields 1500 test cases. In each the one-third of them, H node has 2, 3, and 4 states, respectively. Without loss of generality, all the other random nodes and the decision node have four states.</p><p>For each test case, the VOIs for different O subsets with the size from 3 to 11 are computed. The results from the approximation algorithm are compared to the exact computation implemented with the brute-forth method. Let VOIt be the ground-truth, and VOI be the value computed with the proposed algorithm. Assuming VOIt 6 ¼ 0, the error rate is defined as follows:</p><p>The  <ref type="table">4</ref> describes the six groups. Fig. <ref type="figure">9</ref> illustrates the results from the six groups of 1500 test cases. Chart (a) shows the average errors for each group, while Chart (b) shows the VOIs for one specific case, which is randomly chosen from the test cases from ID_dep: 3-state. As the set size of the O i nodes increases, the error rate decreases. When the state number of H is the same, the error rates of the dependent cases are larger than the error rates of the conditional independent cases. This can be explained by the reason that the IDs in the dependent cases have fewer independent O subsets than the ID in the independent groups. Since the central-limit theorem is the basis of our algorithm, it works better when the number of w i increases, which corresponds to the number of independent O subsets. Even when the size of O set is as small as 6, the average error is less than or around 0.1 for all the cases. We could run several larger IDs with much more O i nodes, and the error curve would be progressively decreasing. Here, we intend to show the trend and the capability of this algorithm.</p><p>Charts (c) and (d) show the average computational time with the exact computation and the approximation computation. When the set size of the O i nodes is small, the computational time is similar. However, as the size becomes larger, the computational time of the exact computation increases exponentially, while the computational time of the approximation algorithm increases much slower. Thus, the larger the O set size is, the more time the approximation algorithm can save. Likewise, as the number of the state of each O i node further increases, the computational saving would be more significant.</p><p>As the number of states of H increase, the computational time also slightly increases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">An illustrative application</head><p>We use a real-world application in human computer interaction to demonstrate the advantages of the proposed algorithm. Fig. <ref type="figure">10</ref> shows an ID for user stress recognition and user assistance. The diagram consists of two portions. The upper portion, from the top to the ''stress" node, depicts the elements that can alter human stress. These elements include the workload, the environmental context, specific character of the user such as his/her trait, and importance of the goal that k is the size of the biggest group after partitioning. 2 The length of the longest path starting from (or ending at) the hypothesis node is 5. </p><p>Proof of Proposition 1. ) In this direction, we prove that if d k is the optimal action, pðh 1 Þ P max k&lt;j6q r jk and pðh 1 Þ 6 min 16j&lt;k r jk . If d k is the optimal action, EUðd k Þ must be larger than or equal to the expected utility of any other action. Based on the definition, the expected utility of taking the action</p><p>and u 2k ¼ uðh 2 ; d k Þ. Therefore, we get the equations as follows:</p><p>Thus, based on the above equations, pðh 1 Þ P max k&lt;j6q r jk and pðh 1 Þ 6 min 16j&lt;k r jk .</p><p>( In this direction, we prove that if pðh 1 Þ P max k&lt;j6q r jk and pðh 1 Þ 6 min 16j&lt;k r jk , then d k is the optimal action.</p><p>If pðh 1 Þ P max k&lt;j6q r jk 8j; k &lt; j 6 q, we get</p><p>Similarly, for 8j;  </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Handbook of Tables for Order Statistics from Log Normal Distributions with Applications</title>
		<author>
			<persName><forename type="first">N</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">W S</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<pubPlace>Kluwer, Amsterdam, Netherlands</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">G</forename><surname>Casella</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Berger</surname></persName>
		</author>
		<title level="m">Statistical Inference</title>
		<meeting><address><addrLine>Brooks/Cole</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1990">1990</date>
			<biblScope unit="page" from="45" to="46" />
		</imprint>
	</monogr>
	<note>Chapter 2</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Improved algorithms for linear inequalities with two variables per inequality</title>
		<author>
			<persName><forename type="first">E</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Megiddo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Computing</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1313" to="1347" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">H</forename><surname>Cormen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">E</forename><surname>Leiserson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Rivest</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Stein</surname></persName>
		</author>
		<title level="m">Introduction to Algorithms</title>
		<imprint>
			<publisher>MIT Press and McGraw-Hill</publisher>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<author>
			<persName><forename type="first">E</forename><surname>Crow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Shimizu</surname></persName>
		</author>
		<title level="m">Lognormal Distributions: Theory and Applications</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Dekker</publisher>
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Influence diagrams with multiple objectives and tradeoff analysis</title>
		<author>
			<persName><forename type="first">M</forename><surname>Diehl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Haimes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Systems, Man and Cybernetics, Part A</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="293" to="304" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Myopic value of information in influence diagrams</title>
		<author>
			<persName><forename type="first">S</forename><surname>Dittmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the Thirteenth Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="142" to="149" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Evidence propagation and value of evidence on influence diagrams</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">J</forename><surname>Ezawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Operations Research</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="73" to="83" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">W</forename><surname>Feller</surname></persName>
		</author>
		<title level="m">An Introduction to Probability Theory and Its Applications</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="1971">1971</date>
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
	<note>third ed.</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Monte Carlo methods for solving multivariable problems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Hammersley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of the New York Academy Sciences</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="844" to="874" />
			<date type="published" when="1960">1960</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">An approximate nonmyopic computation for value of information</title>
		<author>
			<persName><forename type="first">D</forename><surname>Heckerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Horvitz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Middleton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="292" to="298" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Value of information lotteries</title>
		<author>
			<persName><forename type="first">R</forename><surname>Howard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions of Systems Science and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="54" to="60" />
			<date type="published" when="1967">1967</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Influence diagrams, Readings on the Principles and Applications of Decision Analysis</title>
		<author>
			<persName><forename type="first">R</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Matheson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1981">1981</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="721" to="762" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Bayesian Networks and Decision Graphs</title>
		<author>
			<persName><forename type="first">F</forename><surname>Jensen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001">2001</date>
			<publisher>Springer-Verlag</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">From influence diagrams to junction trees</title>
		<author>
			<persName><forename type="first">F</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">V</forename><surname>Jensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Dittmer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<biblScope unit="page" from="367" to="374" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">M</forename><surname>Kalos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Whitlock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Monte</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Carlo</forename><surname>Methods</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wiley</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1986">1986</date>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">B</forename><surname>Korb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">E</forename><surname>Nicholson</surname></persName>
		</author>
		<title level="m">Bayesian Artificial Intelligence</title>
		<imprint>
			<publisher>Chapman and Hall/CRC</publisher>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Efficient active fusion for decision-making via VOI approximation</title>
		<author>
			<persName><forename type="first">W</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twenty-first National Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A decision theoretic model for stress recognition and user assistance</title>
		<author>
			<persName><forename type="first">W</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Twentieth National Conference on Artificial Intelligence (AAAI)</title>
		<imprint>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="529" to="534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName><surname>Mathematica</surname></persName>
		</author>
		<ptr target="&lt;http://www.wolfram.com/products/mathematica/index.html&gt;" />
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Probabilistic Reasoning in Intelligent Systems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988">1988</date>
			<publisher>Morgan Kaufmann Publishers</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A graph-theoretic analysis of information value</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">L</forename><surname>Poh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Horvitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th Annual Conference on Uncertainty in Artificial Intelligence (UAI-96</title>
		<meeting>the 12th Annual Conference on Uncertainty in Artificial Intelligence (UAI-96</meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="427" to="435" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">H</forename><surname>Raiffa</surname></persName>
		</author>
		<title level="m">Decision Analysis</title>
		<imprint>
			<publisher>Addison-Wesley</publisher>
			<date type="published" when="1968">1968</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Evaluating influence diagrams</title>
		<author>
			<persName><forename type="first">R</forename><surname>Shachter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Operations Research</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="871" to="882" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Efficient value of information computation</title>
		<author>
			<persName><forename type="first">R</forename><surname>Shachter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Annual Conference on Uncertainty in Artificial Intelligence (UAI-99)</title>
		<meeting>the 15th Annual Conference on Uncertainty in Artificial Intelligence (UAI-99)</meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="594" to="601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Using potential influence diagrams for probabilistic inference and decision making</title>
		<author>
			<persName><forename type="first">R</forename><surname>Shachter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">M</forename><surname>Ndilikilikesha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth Annual Conference on Uncertainty in Artificial Intelligence (UAI-93</title>
		<meeting>the Ninth Annual Conference on Uncertainty in Artificial Intelligence (UAI-93</meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="383" to="390" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Value of information analysis in environmental health risk management decisions: past, present, and future</title>
		<author>
			<persName><forename type="first">F</forename><surname>Yokota</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Thompson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Risk Analysis</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="635" to="650" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Incremental computation of the value of perfect information in stepwise-decomposable influence diagrams</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the Ninth Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="400" to="410" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
