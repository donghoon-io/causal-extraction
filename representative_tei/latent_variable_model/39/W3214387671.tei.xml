<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Identification of Partially Observed Linear Causal Models: Graphical Conditions for the Non-Gaussian and Heterogeneous Cases</title>
				<funder ref="#_Z3Wnnkb">
					<orgName type="full">United States Air Force</orgName>
				</funder>
				<funder>
					<orgName type="full">Apple Inc.</orgName>
				</funder>
				<funder ref="#_6BkVExA">
					<orgName type="full">European Union</orgName>
				</funder>
				<funder ref="#_PcvaNyU">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
				<funder ref="#_jVT5EBS">
					<orgName type="full">Novo Nordisk Foundation</orgName>
				</funder>
				<funder ref="#_B5VqzBC">
					<orgName type="full">National Institutes of Health</orgName>
					<orgName type="abbreviated">NIH</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Jeffrey</forename><surname>Adams</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Mathematical Sciences</orgName>
								<orgName type="institution">University of Copenhagen</orgName>
								<address>
									<country key="DK">Denmark</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName><forename type="first">Niels</forename><forename type="middle">Richard</forename><surname>Hansen</surname></persName>
							<email>niels.r.hansen@math.ku.dk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Mathematical Sciences</orgName>
								<orgName type="institution">University of Copenhagen</orgName>
								<address>
									<country key="DK">Denmark</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Philosophy</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<settlement>Pittsburgh</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Identification of Partially Observed Linear Causal Models: Graphical Conditions for the Non-Gaussian and Heterogeneous Cases</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-28T22:43+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In causal discovery, linear non-Gaussian acyclic models (LiNGAMs) have been studied extensively. While the causally sufficient case is well understood, in many real applications the observed variables are not causally related. Rather, they are generated by latent variables, such as confounders and mediators, which may themselves be causally related. Existing results on the identification of the causal structure among the latent variables often require very strong graphical assumptions. In this paper, we consider partially observed linear models with either non-Gaussian or heterogeneous errors. In that case we give two graphical conditions which are necessary for identification of the causal structure. These conditions are closely related to sparsity of the causal edges. Together with one additional condition on the coefficients, which holds generically for any graph, the two graphical conditions are also sufficient for identifiability. These new conditions can be satisfied even when the number of latent variables is very large. We demonstrate the validity of our results on synthetic data. * The work presented in this article was started while JA was at CMU. 35th Conference on Neural Information Processing Systems (NeurIPS 2021).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In the standard causal discovery problem, we are given non-experimental data and aim to learn the direct causal relations between the observed variables <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. But in many applications, we do not believe that all causal variables relevant to the observed system have been measured. While some of the observed variables may interact directly, others might interact indirectly via latent mediators, and still others could be generated by latent common causes; indeed, any pair of observed variables may stand in all three relations at once. Further, the relevant latent variables may be causally related themselves. For example, responses to psychometric questionnaires are usually thought of as noisy views of various traits, and the researcher is predominately interested in the causal relations between these hidden traits and their hierarchical structure. Similarly, in financial markets, stock returns may be causally related, but may also be confounded or mediated by a complicated network of unmeasured economic and political factors.</p><p>It is natural to ask what conditions are both necessary and sufficient for the identification of such partially observed causal structures from observational data. Various sufficient conditions have been proposed; however, these conditions are rather restrictive, and are not in general necessary for identification of the full causal structure.</p><p>In this work, we consider the case of linear causal models in which the overcomplete mixing matrix from the noise terms to the measured variables is identifiable up to permutation and scaling of columns. This is possible, for example, in the case of independent non-Gaussian noise <ref type="bibr" target="#b2">[3]</ref>, or when given access to heterogeneous domains in which the variances of the noise terms change independently across domains but the causal graph and weights remain constant (see Theorem 1 of our paper). We provide necessary and sufficient conditions under which the latent causal structure can be uniquely identified up to trivial indeterminicies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Problem setup</head><p>Suppose some causal variables V = {V 1 , . . . , V p } follow a linear structural equation model (SEM)</p><formula xml:id="formula_0">V = FV + ε,<label>(1)</label></formula><p>where V := (V 1 , . . . , V p ) T is a vector of causal variables, F is a causal adjacency matrix that can be permuted (by simultaneous row and column permutations) to strictly lower-triangular form, and ε = (ε 1 , . . . , ε p ) T is a vector of independent noise variables. In this paper we consider two settings for ε i : 1) all ε i are mutually independent and non-Gaussian; or 2) there are multiple domains, ε i are uncorrelated within each domain, and their variances change independently across domains. We will make the second assumption technically precise Section 3.</p><p>We seek necessary and jointly sufficient conditions for identifiability of F (up to trivial indeterminacies) in the case where only some subset of V (which we call X ) is measured. Thus F may encode observed-observed interactions, latent confounding, latent-latent interactions, and latent mediation or intermediate confounding. Our results identify F from the equivalence class M, as defined in Section 2.2, of mixing matrices induced by <ref type="bibr" target="#b0">(1)</ref>. This equivalence class M is identifiable if, for example, the errors are non-Gaussian or if their distribution changes over time or between domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Notation</head><p>For any matrix A and index sets J and K, we write A J K to denote the submatrix of A with columns indexed by J and rows indexed by K. Observe that A J K = I K AI J . Thus, F J K describes the direct effect of {V j : j ∈ J} on {V k : k ∈ K}. (Remember: causes are up-stream of their effects.)</p><p>The graph induced by (1) has edges V i → V j whenever F i j = 0. We write Pa(V i ) := {V j : V i ← V j } and Ch(V i ) := {V j : V i → V j }, respectively, to denote the parents and children of V i . We say that</p><formula xml:id="formula_1">(V 1 , ..., V k ) constitutes a directed path from V 1 to V k if V i → V i+1 for every i ∈ {1, ..., k -1}.</formula><p>Trivially, for every V i , (V i ) is a directed path from V i to itself; we say that a directed graph is acyclic (a DAG) if (V i ) is the only such path. We write Anc(V i ) := {V j ∈ V : V j has a directed path to V i } and Desc(V i ) := {V j : V i has a directed path to V j }, respectively, to denote the ancestors and descendants of V i . For DAGs, notice that Anc(</p><formula xml:id="formula_2">V i ) ∩ Desc(V i ) = {V i }, but V i ∈ Pa(V i ) ∪ Ch(V i ).</formula><p>We assume that only some subset X ⊆ V is observed, with the remaining L = V -X being latent. We use V i to denote a generic variable, observed or latent, while X i ∈ X denotes an observed variable and L j ∈ L denotes a latent variable. When it is clear from context, we occasionally suppress the distinction between a variable V i and its index i.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Identification and minimality</head><p>Since F induces a DAG, we can always solve (1) to express the causal variables in terms of the independent noise terms:</p><formula xml:id="formula_3">V = Mε,<label>(2) where</label></formula><formula xml:id="formula_4">M := (I -F) -1</formula><p>(3) is the mixing matrix with M i j being the net effect of ε i on V j . This net effect is calculated by multiplying causal weights along paths and summing across paths. Notice that if M i j = 0, then</p><formula xml:id="formula_5">V j ∈ Desc(V i ).</formula><p>Because L is hidden, let us explicitly write X in terms of ε:</p><formula xml:id="formula_6">X = M X ε.<label>(4)</label></formula><p>In both the non-Gaussian and heterogeneous settings we consider in this paper, M X is identifiable up to permutation and scaling of columns; that is, we can identify the equivalence class</p><formula xml:id="formula_7">M = {M X DP : DP ∈ DP p },<label>(5)</label></formula><p>where DP p := {DP ∈ R p×p : D is full rank diagonal and P is a permutation matrix}.</p><p>We argue this for both settings individually in Section 3.</p><p>We say that an adjacency matrix F generates M if (I -F) -1 X ∈ M. Of course, in partially observed systems, the adjacency matrix that generates M is not unique. However, some of these matrices are sparser than others. In causal discovery, as in model selection more broadly, we tend to prefer the "simplest" model that adequately fits the data <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>. As a result, without prior knowledge, a partially observed linear causal model cannot be identified if the population distribution can be written in terms of an equally sparse or sparser model; after all, we would never select a complicated model if a simpler model fits just as well. It is therefore natural to recast the question of identifiability to a question of maximal sparsity.</p><p>Let the 0 "norm" of a matrix • 0 denote the number of non-zero entries in that matrix. Then we say that a causal adjacency matrix F is minimal with respect to M if F generates M and F 0 ≥ F 0 for any F = F that generates M.</p><p>Let F denote the class of minimal adjacency matrices that generate M. Clearly, since M is identifiable, so is F. We say that an adjacency matrix F is identified up to trivialities if</p><formula xml:id="formula_8">F = (DP) -1 FDP : DP ∈ DP p with (DP) X X = I .<label>(6)</label></formula><p>The only indeterminacy remaining in F amounts to re-indexing and re-scaling the latent factors.</p><p>A word of caution is in order. Because the adjacency matrix that generates M is not unique in the partially observed case, it is only possible to talk about identification with respect to some selection principle. Throughout this work we use minimality as such a selection principle -indeed we define identification in terms of it. As justification, in Section 5.1, we describe one class of non-minimal adjacency matrices which are pathological and whose exclusion is desirable; further, in Section 6, we show that existing works make assumptions even stronger than minimality; further still, in Section 7, we show that popular model selection criteria like BIC favor minimal graphs. Nevertheless, just as BIC is not always the most sensible criterion for model selection, so minimality is not always the most sensible principle for an identification theory. For example, Figure <ref type="figure" target="#fig_0">1</ref> shows a non-minimal graph which is not pathological. Thus, if a practitioner believes the true partially observed causal model to be non-minimal, they should content themselves with partial identification (c.f. <ref type="bibr" target="#b5">[6]</ref>).</p><p>In Sections 4 and 5, we express identification up to trivialities in terms of two local graphical conditions, which are much easier to check than <ref type="bibr" target="#b5">(6)</ref>. But first, we return to the identifiability of M.</p><p>3 Sufficient conditions for identification of M</p><p>The main results of Sections 4 and 5 rely on the identifiability of M, which is theoretically guaranteed in the two settings we consider in this paper. In the first setting, ε i are assumed to be independent and non-Gaussian. Then according to Theorem 3 by Eriksson and Koivunen <ref type="bibr" target="#b6">[7]</ref>, M is identifiable from the distribution of X. The task of estimating M from X is known as Overcomplete Independent Component Analysis (OICA) <ref type="bibr" target="#b2">[3]</ref>, and in practice this task is known to be computationally difficult <ref type="bibr" target="#b7">[8]</ref>.</p><p>In the second setting, ε i are uncorrelated from each other with changing variances across multiple domains (or over time) and M X has full row rank (which is always the case for acyclic models). Note that in this setting, while the components of ε are mutually independent within each domain, they are not necessarily mutually independent across domains because their variances may be dependent across domains. This setting is expected to apply to a number of nonstationary scenarios including brain signal analysis, and the following theorem establishes the corresponding identifibiality of M.</p><p>Besides complementing Theorem 3 of Eriksson and Koivunen <ref type="bibr" target="#b6">[7]</ref> as an alternative foundation for our identification work, the identifiability of M in this setting may be of independent interest in the fields of blind source separation and system identification.</p><p>Theorem 1. Suppose we have observed X generated according to the mixing procedure (4) in a number of domains, t = 1, 2, ..., T , where M X has full row rank. Assume that ε i are uncorrelated in each domain and that their variances in domain t, denoted by σ 2 ti , change independently across domains in the sense that S, whose (t, i)-th entry is σ 2 ti , has full column rank. Further assume that each |X | columns of M X are linearly independent and that p ≤ 2|X | -2. Then if X admits a model X = MX ε, where ε also follows the above assumption on ε, then every column of MX must be proportional to a column of M X and vice versa.</p><p>Note that this theorem gives sufficient conditions for the identifiability of M; our empirical results suggest that they are not necessary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Necessary conditions for identification of F up to trivialities</head><p>In this section, we introduce our identification conditions and show that they are necessary for F to be identified up to trivialities. The identification conditions are graphical conditions described in terms of "bottlenecks" and "redundancies."</p><p>Let J, K, and B be subsets of the nodes of a directed graph. Note that they need not be mutually disjoint. We say that B is a bottleneck from J to K if, for every j ∈ J and every k ∈ K, each directed path from j to k includes some b ∈ B. A bottleneck B from J to K will be called minimal if every bottleneck B from J to K has |B | ≥ |B|, and unique minimal if the inequality is strict for B = B. Note that bottlenecks do not in general d-separate J and K along all paths, but only directed paths from J to K.</p><p>It is clear from the definition that, for each V i , Ch(V i ) is a bottleneck from Ch(V i ) to X. However, for identification, we further require:</p><formula xml:id="formula_9">Condition 1 (Bottleneck). For every V i , Ch(V i ) is the unique minimal bottleneck from Ch(V i ) to X .</formula><p>As illustrated in Figure <ref type="figure" target="#fig_0">1</ref>, the bottleneck condition ensures that if we try to "explain" the net effect of V i on X by replacing Ch(V i ) with any subset of Desc(V i ), the result is a denser graph. As illustrated in Figure <ref type="figure" target="#fig_2">3</ref>, the strong non-redundancy condition will further ensure that we cannot "explain" the effect of V i on Ch(V i ) via some of its non-descendants:</p><formula xml:id="formula_10">Condition 2 (Strong Non-Redundancy). For all L i , V j , if Ch(L i ) ⊆ Ch(V j ) ∪ {V j } then L i = V j .</formula><p>Figure <ref type="figure" target="#fig_1">2</ref> shows a graph that satisfies both of these conditions. To build intuition, let us list some simple consequences of these conditions. By the bottleneck condition, each variable must have fewer than |X | children; but if a variable has no latent children, then the bottleneck condition is satisfied trivially for that variable. By strong non-redundancy, each latent variable must have at least two children. For any pair (L i , V j ), if L i is an ancestor but not a parent of V j , or has more than one directed path to V j , then strong non-redundancy is satisfied for that pair. If V j is a parent of L i and they violate strong non-redundancy, then the bottleneck condition is violated for V j . Theorem 2. If F is identified up to trivialities, then the graph induced by F satisfies the bottleneck and strong non-redundancy conditions.  Thus the bottleneck and strong non-redundancy conditions are necessary for identification of F up to trivialities. In Section 5, we further show that they (along with a very mild constraint on the causal weights) are also jointly sufficient conditions.</p><p>If Ch(V i ) is not at least a minimal bottleneck for every V i , then F ∈ F. Figure <ref type="figure" target="#fig_0">1</ref> shows one example of such a violation of the bottleneck condition. Otherwise, as long as bottleneck faithfulness is also satisfied, F is an equivalence class of equally sparse latent structures which all violate at least one of the bottleneck and strong non-redundancy conditions. The nature of these indeterminacies is depicted in Figure <ref type="figure" target="#fig_2">3</ref>. In Figure <ref type="figure" target="#fig_1">2</ref> we show a simple yet illustrative example in which both conditions are satisfied. 5 Sufficient conditions for identification of F up to trivialities</p><p>In the previous section, we introduced two structural conditions which must be satisfied for F to be identifiable up to trivialities. In this section, we prove that they (along with "bottleneck faithfulness," a very mild constraint on the causal weights) are also jointly sufficient. Throughout, we assume that X is generated according to <ref type="bibr" target="#b0">(1)</ref>. In particular, we assume that M is identifiable, for example due to Theorem 3 of Eriksson and Koivunen <ref type="bibr" target="#b6">[7]</ref> or Theorem 1 of the present work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Bottleneck faithfulness</head><p>First, we connect ranks of submatrices of M to minimal bottlenecks of its corresponding graph. Proposition 1. Let B be a minimal bottleneck from J to K. Then Rank(M J K ) ≤ |B|.</p><p>Strict inequality in Proposition 1 for some minimal bottleneck B from J to K can make F nonidentifiable -even if the bottleneck condition and strong non-redundancy hold. For instance, both graphical conditions hold for</p><formula xml:id="formula_11">X =    0 1 -1 2 2 0 3 3 0 4 0 4    L + ε X , L = ε L , but Rank(M L X ) = 2 while the minimal bottleneck from L to X is L with |L| = 3. The system X =    0 1 -1 0 2 0 0 3 0 0 0 4    L + ε X , L = 0 0 0 1 0 0 1 0 0 L + ε L</formula><p>generates the same mixing matrix, M X , but has a strictly sparser graph. Thus to ensure identifiability, we assume that the causal coefficients satisfy: Condition 3 (Bottleneck Faithfulness). For every</p><formula xml:id="formula_12">J ⊆ V, K ⊆ X , if B is a minimal bottleneck from J to K, then Rank M J K = |B|.</formula><p>In the supplementary material we characterize the set of adjacency matrices F that are bottleneck faithful for a given graph. In particular, we show that a generic F is bottleneck faithful.</p><p>Interestingly, in linear systems, classical faithfulness is a special case of bottleneck faithfulness. Rank(M J K ) = 0 is a violation of classical faithfulness if there is a minimal bottleneck B = ∅ from J to K. That is, if there is a path from J to K but the path coefficients cancel out so that the net effect of J on K is 0, the system is not faithful to the graph. Bottleneck faithfulness generalizes this so that the net effect of J on K must have maximal rank for the given graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Identifiability</head><p>In this subsection, we show that if the bottleneck condition, strong non-redundancy, and bottleneck faithfulness hold for F, then F is identifiable up to trivialities. Throughout, we assume the three conditions hold.</p><p>Our approach is illustrated by the following computation. For any V i ,</p><formula xml:id="formula_13">M X (I -F) i = I i X .<label>(7)</label></formula><p>Let J = Ch(V i ). Since the support of F i is J, the equation</p><formula xml:id="formula_14">(M -I) i X = M J X x.<label>(8)</label></formula><p>always has a solution at x = F i J . In fact, (under the three assumptions of this section) this solution is unique:</p><formula xml:id="formula_15">Lemma 1. Let J = Ch(V i ). Then the unique solution to (8) is given by x = F i Ch(Vi) .</formula><p>But there is a version of (8) for each J ⊆ V. For which other choices of J does (8) have a solution? Clearly a solution always exists if J ⊇ Ch(V i ). On the other hand, we can guarantee that a solution with |J| ≤ |Ch(V i )| is only possible if J contains an ancestor of V i :</p><formula xml:id="formula_16">Lemma 2. Suppose J ⊆ V -Anc(V i ). If (M -I) i X ∈ Range M J X , then |J| ≥ |Ch(V i )|, with equality if and only if J = Ch(V i ).</formula><p>By Lemma 2, if any superset of Ch(V i ) containing no ancestors of V i is identifiable, then Ch(V i ) is also identifiable. Next, we will show how such a superset of Ch(V i ) can be identified. Let V k ⊆ V denote the variables whose longest path to X has fewer than k nodes. More formally, we define recursively</p><formula xml:id="formula_17">V 0 := ∅,<label>(9)</label></formula><formula xml:id="formula_18">V k+1 := {V i ∈ V : Ch(V i ) ⊆ V k } , for k ≥ 0.<label>(10)</label></formula><p>Naturally, we define X k := V k ∩ X and L k := V k ∩ L. Notice that V k is strictly increasing, and is induced by the topological ordering on V.</p><formula xml:id="formula_19">Proposition 2. For all k, either V k ⊂ V k+1 , or V k = V.</formula><p>Further, for each k ≥ 0, define</p><formula xml:id="formula_20">J k+1 (V i ) := arg min J∈ J⊆V k :M i X k ∈Range M J X k |J|.<label>(11)</label></formula><p>Intuitively, this denotes the set of minimal choices for J ⊂ V k such that (8) has a solution. From Lemma 2, we know that</p><formula xml:id="formula_21">J k+1 (V i ) = {Ch(V i )} if V i ∈ V k+1 -V k .</formula><p>The construction of (11) allows us to generalize Lemma 2 to describe which versions of ( <ref type="formula" target="#formula_14">8</ref>) have solutions when we are not sure of the causal order.</p><p>Lemma 3. For every k ≥ 0, let V k and J k (V i ) be defined as above. Then V i ∈ V k+1 -V k if and only if all of the following hold:</p><formula xml:id="formula_22">1. V i ∈ V k ; 2. |Support(M i X ) -X k | ≤ 1; 3. |J k+1 (V i )| = 1; and 4. for all V j = V i satisfying points 1 and 2, M j X k ∈ Range M J k (Vi) X k . As a result of Lemma 3, each V k is identifiable. Clearly, if V i ∈ V k+1 , then Ch(V i ) ∈ V k and V k ∩ Anc(V i ) = ∅.</formula><p>Hence by Lemma 2, Ch(V i ) is also identifiable. Thus the full DAG is identifiable, and each column of M X can be associated with the corresponding node in the DAG. However, Lemmas 1, 2, and 3 are not enough to distinguish which nodes correspond to latent variables and which correspond to observed variables; we have yet to pair each X i with its net effects M i X . Resolving this final indeterminacy is not hard. Intuitively, the vector of M X corresponding to X i must have non-zero coefficients in the i-th slot while every vector corresponding to descendants of X i will not. Lemma 4 formalizes this observation. Lemma 4. X i ∈ X k+1 if and only if X i ∈ V k+1 and Support (M Xi X ) -X k = {i}.</p><p>Together, Lemmas 1, 2, 3, and 4 imply that F is identifiable if M X is identifiable. Of course, we do not know M X -only M. Nevertheless, Lemmas 2, 3, and 4 do not involve the scaling and permutation of M X -only the linear dependencies of its columns. Some simple calculation shows that Lemma 1 can be used to put any M X PD ∈ M in one-to-one correspondence with (PD) -1 F(PD).</p><p>Theorem 3. Suppose F satisfies generalized non-redundancy, bottleneck faithfulness, and the bottleneck condition. Then F is identifiable up to indeterminacies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Relation to existing work</head><p>Constraint-and score-based approaches to causal discovery based on conditional independence testing-such as SGS <ref type="bibr" target="#b0">[1]</ref>, IC <ref type="bibr" target="#b1">[2]</ref>, PC <ref type="bibr" target="#b8">[9]</ref>, GES <ref type="bibr" target="#b9">[10]</ref>, and FGS <ref type="bibr" target="#b10">[11]</ref>-generally focus on the causally sufficient case. These algorithms identify the Markov equivalence class of graphs which all encode the same set of conditional independence relations. While some methods based on conditional independence tests, such as FCI <ref type="bibr" target="#b11">[12]</ref> and RFCI <ref type="bibr" target="#b12">[13]</ref>, are able to relax the assumption of causal sufficiency, their focus is on learning the causal relations between observed variables and distinguishing them from spurious dependencies induced by shared latent ancestors. Such methods recover only limited information about the latent structure, as only the most basic information about latent structure is identifiable from conditional independence relations alone. For one review of causal discovery methods, see Spirtes and Zhang <ref type="bibr" target="#b13">[14]</ref>.</p><p>It is possible to go beyond the equivalence class with additional assumptions on causal mechanisms <ref type="bibr" target="#b13">[14]</ref>. In particular, linear non-Gaussian models have been studied extensively. In the causally sufficient case, Shimizu et al. <ref type="bibr" target="#b14">[15]</ref> leverages acyclicity of the causal relations and the identifiability of the square ICA problem <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref> to show how the causal adjacency matrix can be identified, while Lacerda et al. <ref type="bibr" target="#b17">[18]</ref> further estimate a subclass of cyclic causal models. In both cases, one may replace the non-Gaussian noise assumption with the heterogeneous noise assumption (in the formal sense of Theorem 1) and the identifiability results still hold <ref type="bibr" target="#b18">[19]</ref>.</p><p>By contrast, previous works on partially observed linear non-Gaussian models only study certain special cases in which the models are partially identifiable. Hoyer et al. <ref type="bibr" target="#b19">[20]</ref> describe a procedure to convert partially observed causal models to a canonical form in which no latent variable has any parents. They further provide an algorithm which recovers all canonical forms consistent with the observed overcomplete basis M, which is identifiable by OICA <ref type="bibr" target="#b2">[3]</ref>. This recovered equivalence class of observationally equivalent canonical forms can be huge, and by definition can neither identify causal relations among latent confounders nor distinguish latent confounders from latent mediators.</p><p>More recently, Lemma 5 of Salehkaleybar et al. <ref type="bibr" target="#b5">[6]</ref> states that if M is identifiable, then the causal order among observed variables is identifiable if classical faithfulness holds between all variables. Their condition is strictly weaker than ours; as we discuss in Section 5.1, classical faithfulness is entailed by bottleneck faithfulness and imposes no graphical conditions. That a weaker condition suffices for their task is not surprising, since their task is strictly easier than ours; if F is identified up to trivialities then the causal order among observed variables is also identified (while the causal order alone tells very little about F). Lemma 5 has a reassuring consequence for our work: even if the graphical conditions for total identification fail to apply, the causal order of X is still identified.</p><p>If the practitioner is further interested in the observed variables' net effects on one another, (M -I) X X , then additional graphical assumptions are needed. Theorem 16 of Salehkaleybar et al. <ref type="bibr" target="#b5">[6]</ref> provides one condition sufficient for this purpose: no latent variable L i has precisely the same observed descendants as any observed variable X j (formally, for all L i , X j , Desc(L i ) ∩ X = Desc(X j ) ∩ X ).</p><p>Again, this being a relatively easy subtask of the problem we consider, it is not surprising that our conditions are not strictly weaker. With that said, our conditions are not strictly stronger, either; the bottleneck and strong non-redundancy conditions can be satisfied even when L i and X j have the same observed descendants, as shown in Figure <ref type="figure">4</ref>.</p><p>Figure <ref type="figure">4</ref>: Examples of graphs identifiable from M. From left to right: a graph where Desc(L) ∩ X = Desc(X 1 ) ∩ X ; a widening hierarchical structure; a hierarchical structure with intra-layer relations.</p><p>To recover causal structures of the hidden variables, many results rely on strong assumptions about clusters of pure variables (sets of observed variables which each share a latent confounder and have no other parents). For example, Spearman's classical Tetrad condition <ref type="bibr" target="#b20">[21]</ref> identifies latent causes with four pure observed children from covariance information alone. In the linear non-Gaussian case, existing work reduces the number of pure observed children to three <ref type="bibr" target="#b21">[22]</ref>, and more recently to two <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24]</ref>. Clearly, these are all special cases of both the bottleneck and non-redundancy conditions. As such, our graphical assumptions are strictly weaker. Proposition 3. Suppose each L i in a partially observed DAG has at least two pure children (latent or observed). Then the DAG satisfies the bottleneck and non-redundancy conditions.</p><p>However, identification is possible even when no latent confounder has any pure children; for example, Anandkumar et al. <ref type="bibr" target="#b24">[25]</ref> present a model in which latent variables with no pure children are identifiable. Rather than purity, they require a graph expansion property-for all non-singleton S ⊆ L,</p><formula xml:id="formula_23">| Li∈S Ch(L i ) ∩ X | ≥ |S| + d max , where d max = max i |Ch(L i ) ∩ X | -</formula><p>as well as a rank condition on F L X which places hard-to-check graphical constraints on the model and bounds |L| ≤ 1 3 |X |. Nonredundancy among latent variables can be derived from the expansion property by considering the case where |S| = 2, and the bottleneck condition by considering S = {L i } ∪ Ch(L i ) ∩ L. Thus in one sense, our conditions can be seen as a refinement on the expansion property; however, we also remove the many hard-to-check graphical consequences of the rank condition, and further show that many graphs even with |L| |X | are identifiable. For example, Figure <ref type="figure">4</ref> shows an identifiable hierarchical model in which the number of latent variables increases with depth. Moreover, we show that our conditions are sufficient for identifying hierarchical structures in which variables in the same layer are causally related. Figure <ref type="figure">4</ref> shows one such system. Proposition 4. Suppose F satisfies the rank and graph expansion conditions of Anandkumar et al. <ref type="bibr" target="#b24">[25]</ref>. Then F also satisfies the bottleneck and strong non-redundancy conditions. Propositions 3 and 4 show that our conditions are indeed more general than previous identification conditions; not only do our conditions allow and identify causal relations among observed variables, they also identify latent structures which no previous works could. (See, for example, Figure <ref type="figure">4</ref>.) Furthermore, in light of Theorems 2 and 3, they show that many existing works implicitly took sparsity of causal edges as a useful primitive for what it means for a partially observed causal model to be identifiable. Such a primitive is widely used throughout causal discovery, even in the causally sufficient case <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>.</p><p>Although many of these conditions for latent structure identification rely on non-Gaussian independent noise, direct estimation of the mixing matrix is often avoided in practice, especially in the causally insufficient case, as estimation of the overcomplete mixing matrix is computationally challenging <ref type="bibr" target="#b7">[8]</ref>. Estimation of the mixing matrix can be avoided by directly using the independent additive noise assumption and exploiting graphical conditions such as causal sufficiency or purity. For example, in the causally sufficient case, Pa(X i ) is identifiable by regressing X i on Z ⊆ X and testing the independence of the regression residuals and Z <ref type="bibr" target="#b25">[26]</ref>. This approach may be adapted to the non-linear <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref> and post-nonlinear <ref type="bibr" target="#b28">[29]</ref> cases. Tashiro et al. <ref type="bibr" target="#b29">[30]</ref> extend this idea to identify causal relations among observed variables in the causally insufficient case, and a related condition is developed by Xie et al. <ref type="bibr" target="#b23">[24]</ref> to identify one special type of confounder. However, such methods owe their efficiency to the strong structural conditions under which they guarantee identifiability. As the bottleneck and non-redundancy conditions are much more general, this naturally complicates the question of estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Estimation</head><p>In Theorems 1 and 3, we have shown that a causal system which satisfies the conditions of Section 4 is uniquely identifiable whenever M X is identifiable. However, as indicated in Section 6, estimation of M X from homogeneous non-Gaussian data-for example, by overcomplete ICA-is computationally hard. Further, whereas the estimation algorithms presented in <ref type="bibr" target="#b14">[15]</ref>, <ref type="bibr" target="#b17">[18]</ref>, and <ref type="bibr" target="#b5">[6]</ref> require the practitioner to test which entries of M X are exact zeros, a naive algorithm inspired by Lemmas 1, 2, 3, and 4 would further require them to test which submatrices' singular values are exact zeros. Such an algorithm is not advisable.</p><p>As a proof of concept, we therefore focus our experiments on partially observed linear causal models in the heterogeneous case. In this setting, F can be learned directly by optimizing the regularized likelihood with respect to F, given the sample covariance matrices of X. We leave more efficient estimation in more general settings to future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Simulations</head><p>Suppose we have access to samples from T heterogeneous domains. The data in the t-th domain follow</p><formula xml:id="formula_24">V = FV + ε,<label>(12)</label></formula><p>where ε ∼ N (0, Σ t ) for diagonal Σ t . Then in the t-th domain,</p><formula xml:id="formula_25">X = M X Σ t M T X .<label>(13)</label></formula><p>The negative log likelihood is</p><formula xml:id="formula_26">-2 (F, Σ) = T t=1 n t |X | log(2π) + log det(S t ) + Tr S -1 t Ê x t x T t ,<label>(14)</label></formula><p>where x t,i is the i-th row of the design matrix for the t-th domain, Ê x t x T t is the empirical second moment of the d-th domain, S t = M X Σ t M T X , and n t is the sample size in the t-th domain. If X is generated according to <ref type="bibr" target="#b11">(12)</ref>, the independent change condition in Theorem 1 holds for the noise variances, and F satisfies the assumptions of Section 4, then by Theorems 1 and 3, F is identifiable up to trivialities. Hence we can in principle optimize the regularized log likelihood.</p><p>As a sanity check for our theoretical results, we simulate data according to <ref type="bibr" target="#b11">(12)</ref>; for every identifiable graph structure with three observed variables and at most five directed edges, we generated ten causal adjacency matrices with weights randomly drawn from (-0.9, -0.5) ∪ (0.5, 0.9). We estimate F and |L| by minimizing the BIC via exhaustive search. By enumerating candidate graphs from sparsest to densest, a single search could take anywhere from 10 to 60 minutes on an Intel core i7 processor.</p><p>To verify that our estimation method was actually leveraging the noise's heterogeneity, we ran the experiment with one domain and 5000 observations. Only 3% of graphs were identified. Not surprisingly, only 15% of learned graphs had any latent variables at all. Increasing the number of domains from 1 to 3 but keeping the total sample size at 5000 (i.e. 1666 per domain) improved the rate of structure identification to 50% of trials.</p><p>With 5 domains and 500 samples per domain, the correct graph is identified on 50% of trials; with 1000 samples per domain, this improves to 70%; and with 10 000, this further improves to 80%. In every case that the wrong graph was recovered, the equivalence class of mixing matrices M generated by F had incorrect support, perhaps due to insufficient domains or accidentally coupled changes in the noise variances. This supports the main theory of Theorem 3, which, in light of Theorem 1, claims that the structure of F is uniquely determined from a correctly identified M. We report detailed results in the supplement for all graphs studied.</p><p>To verify our claims in Theorem 2, we also tested simulated data from ten non-identifiable partially observed DAGs-six of which stand in the main equivalence class relations of Figure <ref type="figure" target="#fig_2">3</ref>, and four of which are not minimal. Not surprisingly, members of the same equivalence class were indistinguishable, each system achieving the same log likelihood up to eight significant digits.</p><p>Because exhaustive search over graphs is computationally expensive (even for this toy problem, there are 1759 graphs, and so 1759 non-convex optimization problems), it would be desirable to instead optimize the L1-penalized negative log likelihood:</p><formula xml:id="formula_27">L(F, Σ) = -2 (F, Σ) + λ |F i,j |. (<label>15</label></formula><formula xml:id="formula_28">)</formula><p>As before, we simulated data from <ref type="bibr" target="#b11">(12)</ref>. Numerical experiments verify that L(F 0 , Σ 0 ) is very near a local minimum for all practical λ &gt; 0, where (F 0 , Σ 0 ) denotes the true adjacency matrix and noise covariances. However, experiments also suggest that this local minimum is generally quite far from the global minimum, both in parameter space and in L1 loss. Moreover, while the L1 penalty successfully drives many parameters to zero (as we would expect), our experiments frequently converge to minima which are denser than the true system. Intuitively, the L1 penalty does not care about the density of F; a dense system with small coefficients may have a comparable L1 penalty as a sparse system with large coefficients. Naturally, denser systems are better equipped to fit the observed domain covariances. We summarize these experiments in the supplement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Conclusion and discussions</head><p>In many fields, we do not believe that all causally relevant variables have been measured. In such partially observed settings, beyond accurately estimating the causal relations among observed variables, practitioners may want to further identify the causal relations among the hidden variables which generate the observed data. Inspired by this issue, we have contributed to the identification theory of partially observed linear causal models by providing necessary and sufficient graphical conditions for the identification of the full causal graph. Throughout, we assume the additive noise terms in the structural equation model follow non-Gaussian distributions or have independently changing variances across time or between domains. Such assumptions, unlike the single-domain Gaussianity assumption, render the mixing procedure from the noise terms to the observed variables identifiable up to the permutation and scaling of columns, thereby facilitating our final identifiability results. These conditions are expected to be applicable to a wide variety of partially observed structures. To deal with real applications, efficient estimation methods are needed, and we hope our theoretical identifiability results will stimulate algorithmic development to finally solve this important causal discovery problem. As future work, we will focus on developing practical estimation methods and extending our results to nonlinear cases.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An egregious violation of the bottleneck condition. Left: {V 2 , V 3 } is a strictly smaller bottleneck from Ch(V 1 ) to X . Right: a sparser yet observationally equivalent graph. Although both graphs also violate strong non-redundancy, egregious bottleneck violations are not always redundant.</figDesc><graphic coords="4,108.00,567.24,109.33,80.33" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: A simple graph illustrating our structural conditions. L 1 satisfies the bottleneck condition. L 2 and L 3 are non-redundant as each has a child the other does not. X 1 and L 3 are nonredundant as L 3 → X 2 and X 1 ∈ L.</figDesc><graphic coords="4,371.97,567.82,109.62,79.75" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Two equivalence classes. (a) and (b) are equivalent, the former violating the bottleneck condition (X = Ch(L 1 ) is a minimal bottleneck from Ch(L 1 ) to X ) and the latter strong nonredundancy (Ch(L 2 ) ⊆ Ch(L 1 )). (c) and (d) are equivalent, both violating strong non-redundancy.</figDesc><graphic coords="5,114.06,186.05,91.89,51.00" type="bitmap" /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This project has received funding from the <rs type="funder">European Union</rs>'s <rs type="programName">Horizon 2020 research and innovation programme</rs> under the <rs type="grantName">Marie Skłodowska-Curie</rs> grant agreement No <rs type="grantNumber">801199</rs>. The work presented in this article was supported in part by <rs type="funder">Novo Nordisk Foundation</rs> Grant <rs type="grantNumber">NNF20OC0062897</rs>. This work was supported in part by the <rs type="funder">National Institutes of Health (NIH)</rs> under Contract <rs type="grantNumber">R01HL159805</rs>, by the <rs type="funder">NSF</rs>-Convergence Accelerator Track-D award #<rs type="grantNumber">2134901</rs>, by the <rs type="funder">United States Air Force</rs> under Contract No. <rs type="grantNumber">FA8650-17-C7715</rs>, and by a grant from <rs type="funder">Apple Inc.</rs> The <rs type="funder">NIH</rs> or NSF is not responsible for the views reported in this article.</p><p>We are grateful to the anonymous reviewers for their careful reading and helpful comments.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_6BkVExA">
					<idno type="grant-number">801199</idno>
					<orgName type="grant-name">Marie Skłodowska-Curie</orgName>
					<orgName type="program" subtype="full">Horizon 2020 research and innovation programme</orgName>
				</org>
				<org type="funding" xml:id="_jVT5EBS">
					<idno type="grant-number">NNF20OC0062897</idno>
				</org>
				<org type="funding" xml:id="_B5VqzBC">
					<idno type="grant-number">R01HL159805</idno>
				</org>
				<org type="funding" xml:id="_PcvaNyU">
					<idno type="grant-number">2134901</idno>
				</org>
				<org type="funding" xml:id="_Z3Wnnkb">
					<idno type="grant-number">FA8650-17-C7715</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Causation, Prediction, and Search</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clark</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><surname>Scheines</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Heckerman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Causality</title>
		<author>
			<persName><forename type="first">Judea</forename><surname>Pearl</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning overcomplete representations</title>
		<author>
			<persName><forename type="first">S</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Terrence</forename><forename type="middle">J</forename><surname>Lewicki</surname></persName>
		</author>
		<author>
			<persName><surname>Sejnowski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="337" to="365" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The frugal inference of causal relations</title>
		<author>
			<persName><forename type="first">Malcolm</forename><surname>Forster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Raskutti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reuben</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naftali</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The British Journal for the Philosophy of Science</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page" from="4" to="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Learning directed acyclic graphs based on sparsest permutations</title>
		<author>
			<persName><forename type="first">Garvesh</forename><surname>Raskutti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caroline</forename><surname>Uhler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stat</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning linear non-Gaussian causal models in the presence of latent variables</title>
		<author>
			<persName><forename type="first">Saber</forename><surname>Salehkaleybar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amiremad</forename><surname>Ghassami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Negar</forename><surname>Kiyavash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">39</biblScope>
			<biblScope unit="page" from="1" to="24" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Identifiability, separability, and uniqueness of linear ICA models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Eriksson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Koivunen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="601" to="604" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Likelihood-free overcomplete ICA and applicationsin causal discovery</title>
		<author>
			<persName><forename type="first">Chenwei</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mingming</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dacheng</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">An algorithm for fast recovery of sparse causal graphs</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clark</forename><surname>Glymour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Social Science Computer Review</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="62" to="72" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Optimal structure identification with greedy search</title>
		<author>
			<persName><forename type="first">David</forename><surname>Maxwell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chickering</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="507" to="554" />
			<date type="published" when="2002-11">Nov. 2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A million variables and more: the Fast Greedy Equivalence Search algorithm for learning high-dimensional graphical causal models, with an application to functional magnetic resonance images</title>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Ramsey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madelyn</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruben</forename><surname>Sanchez-Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clark</forename><surname>Glymour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Data Science and Analytics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="121" to="129" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">An algorithm for causal inference in the presence of latent variables and selection bias. Computation, Causation, and Discovery</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Meek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Richardson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning high-dimensional directed acyclic graphs with latent and selection variables</title>
		<author>
			<persName><forename type="first">Diego</forename><surname>Colombo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Marloes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Maathuis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Kalisch</surname></persName>
		</author>
		<author>
			<persName><surname>Richardson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="page" from="294" to="321" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Causal discovery and inference: Concepts and recent methodological advances</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Informatics</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A linear non-Gaussian acyclic model for causal discovery</title>
		<author>
			<persName><forename type="first">Shohei</forename><surname>Shimizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrik</forename><forename type="middle">O</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aapo</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antti</forename><surname>Kerminen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Independent Component Analysis -a new concept</title>
		<author>
			<persName><forename type="first">P</forename><surname>Comon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Signal Processing</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="287" to="314" />
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<author>
			<persName><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Karhunen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Oja</surname></persName>
		</author>
		<title level="m">Independent Component Analysis</title>
		<imprint>
			<publisher>John Wiley &amp; Sons, Inc</publisher>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Discovering cyclic causal models by Independent Components Analysis</title>
		<author>
			<persName><forename type="first">G</forename><surname>Lacerda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Spirtes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ramsey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">O</forename><surname>Hoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Uncertainty in Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A neural net for blind separation of nonstationary signals</title>
		<author>
			<persName><forename type="first">Kiyotoshi</forename><surname>Matsuoka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Masahiro</forename><surname>Ohoya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mitsuru</forename><surname>Kawamoto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="411" to="419" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Estimation of causal effects using linear non-Gaussian causal models with hidden variables</title>
		<author>
			<persName><forename type="first">Patrik</forename><forename type="middle">O</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shohei</forename><surname>Shimizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antti</forename><forename type="middle">J</forename><surname>Kerminen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Markus</forename><surname>Palviainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Approximate Reasoning</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="362" to="378" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Pearson&apos;s contribution to the theory of two factors</title>
		<author>
			<persName><forename type="first">Charles</forename><surname>Spearman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">British Journal of Psychology</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">95</biblScope>
			<date type="published" when="1928">1928</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Estimation of linear non-Gaussian acyclic models for latent factors</title>
		<author>
			<persName><forename type="first">Shohei</forename><surname>Shimizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrik</forename><forename type="middle">O</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aapo</forename><surname>Hyvärinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="issue">7-9</biblScope>
			<biblScope unit="page" from="2024" to="2027" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Triad constraints for learning causal structure of latent variables</title>
		<author>
			<persName><forename type="first">Ruichu</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feng</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clark</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Generalized independent noise condition for estimating linear non-Gaussian latent variable graphs</title>
		<author>
			<persName><forename type="first">Feng</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruichu</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Biwei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Clark</forename><surname>Glymour</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Hao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning linear Bayesian networks with latent variables</title>
		<author>
			<persName><forename type="first">Animashree</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adel</forename><surname>Javanmard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sham</forename><forename type="middle">M</forename><surname>Kakade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Machine Learning</title>
		<meeting>the 30th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">DirectLiNGAM: A direct method for learning a linear non-Gaussian structural equation model</title>
		<author>
			<persName><forename type="first">Shohei</forename><surname>Shimizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takanori</forename><surname>Inazumi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yasuhiro</forename><surname>Sogawa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aapo</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshinobu</forename><surname>Kawahara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takashi</forename><surname>Washio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrik</forename><forename type="middle">O</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenneth</forename><surname>Bollen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1225" to="1248" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Nonlinear causal discovery with additive noise models</title>
		<author>
			<persName><surname>Po</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Mooij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<meeting><address><addrLine>Red Hook, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Curran</publisher>
			<date type="published" when="2009-06">June 2009</date>
			<biblScope unit="page" from="689" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Causal discovery with continuous additive noise models</title>
		<author>
			<persName><forename type="first">Jonas</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Joris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dominik</forename><surname>Mooij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Janzing</surname></persName>
		</author>
		<author>
			<persName><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">On the identifiability of the post-nonlinear causal model</title>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aapo</forename><surname>Hyvärinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the 25th Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="647" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">ParceLiNGAM: A causal ordering method robust against latent confounders</title>
		<author>
			<persName><forename type="first">Tatsuya</forename><surname>Tashiro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shohei</forename><surname>Shimizu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aapo</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takashi</forename><surname>Washio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
