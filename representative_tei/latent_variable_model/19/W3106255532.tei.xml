<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Generative Adversarial Phonology: Modeling Unsupervised Phonetic and Phonological Learning With Neural Networks</title>
				<funder>
					<orgName type="full">Berkeley Research Impact Initative</orgName>
					<orgName type="abbreviated">BRII</orgName>
				</funder>
				<funder>
					<orgName type="full">UC Berkeley Library</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2020-07-08">08 July 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Robert</forename><surname>Malouf</surname></persName>
						</author>
						<author role="corresp">
							<persName><forename type="first">Gašper</forename><surname>Beguš</surname></persName>
							<email>begus@berkeley.edu</email>
							<affiliation key="aff4">
								<orgName type="department">Department of Linguistics</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley Berkeley</settlement>
									<region>CA</region>
									<country key="US">United States</country>
								</address>
							</affiliation>
							<affiliation key="aff5">
								<orgName type="department">Department of Linguistics</orgName>
								<orgName type="institution">University of Washington</orgName>
								<address>
									<settlement>Seattle</settlement>
									<region>WA</region>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Carnegie Mellon University in Qatar</orgName>
								<address>
									<country key="QA">Qatar</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution">Carnegie Mellon University</orgName>
								<address>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff2">
								<orgName type="institution">San Diego State University</orgName>
								<address>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff3">
								<orgName type="institution" key="instit1">Kevin Tang</orgName>
								<orgName type="institution" key="instit2">University of Florida</orgName>
								<address>
									<country key="US">United States</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Generative Adversarial Phonology: Modeling Unsupervised Phonetic and Phonological Learning With Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-07-08">08 July 2020</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.3389/frai.2020.00044</idno>
					<note type="submission">This article was submitted to Language and Computation, a section of the journal Frontiers in Artificial Intelligence Received: 28 January 2020 Accepted: 19 May 2020</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-21T19:13+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>generative adversarial networks</term>
					<term>deep neural network interpretability</term>
					<term>language acquisition</term>
					<term>speech</term>
					<term>voice onset time</term>
					<term>allophonic distribution</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Training deep neural networks on well-understood dependencies in speech data can provide new insights into how they learn internal representations. This paper argues that acquisition of speech can be modeled as a dependency between random space and generated speech data in the Generative Adversarial Network architecture and proposes a methodology to uncover the network's internal representations that correspond to phonetic and phonological properties. The Generative Adversarial architecture is uniquely appropriate for modeling phonetic and phonological learning because the network is trained on unannotated raw acoustic data and learning is unsupervised without any language-specific assumptions or pre-assumed levels of abstraction. A Generative Adversarial Network was trained on an allophonic distribution in English, in which voiceless stops surface as aspirated word-initially before stressed vowels, except if preceded by a sibilant [s]. The network successfully learns the allophonic alternation: the network's generated speech signal contains the conditional distribution of aspiration duration. The paper proposes a technique for establishing the network's internal representations that identifies latent variables that correspond to, for example, presence of [s] and its spectral properties. By manipulating these variables, we actively control the presence of [s] and its frication amplitude in the generated outputs. This suggests that the network learns to use latent variables as an approximation of phonetic and phonological representations. Crucially, we observe that the dependencies learned in training extend beyond the training interval, which allows for additional exploration of learning representations. The paper also discusses how the network's architecture and innovative outputs resemble and differ from linguistic behavior in language acquisition, speech disorders, and speech errors, and how well-understood dependencies in speech data can help us interpret how neural networks learn their representations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">INTRODUCTION</head><p>How to model language acquisition is among the central questions in linguistics and cognitive science in general. Acoustic speech signal is the main input for hearing infants acquiring language. By the time acquisition is complete, humans are able to decode and encode information from or to a continuous speech stream and construct a grammar that enables them to do so <ref type="bibr" target="#b110">(Saffran et al., 1996</ref><ref type="bibr" target="#b111">(Saffran et al., , 2007;;</ref><ref type="bibr" target="#b71">Kuhl, 2010)</ref>. In addition to syntactic, morphological, and semantic representations, the learner needs to learn phonetic representations and phonological grammar: to analyze and in turn produce speech as a continuous acoustic stream represented by mental units called phonemes. Phonological grammar manipulates these discrete units and derives surface forms from stored lexical representations. The goal of linguistics and more specifically, phonology, is to explain how language-acquiring children construct a phonological grammar, how the grammar derives surface outputs from inputs, and what aspects of the grammar are language-specific in order to tease them apart from those aspects that can be explained by general cognitive processes or historical developments <ref type="bibr" target="#b28">(de Lacy, 2006;</ref><ref type="bibr" target="#b88">Moreton, 2008;</ref><ref type="bibr">Moreton and Pater, 2012a,b;</ref><ref type="bibr" target="#b29">de Lacy and Kingston, 2013;</ref><ref type="bibr" target="#b13">Beguš, 2018b)</ref>.</p><p>Computational models have been invoked for the purpose of modeling language acquisition and phonological grammar ever since the rise of computational methods and computationally informed linguistics (for an overview of the literature, see <ref type="bibr">Alderete and Tupper, 2018a;</ref><ref type="bibr" target="#b33">Dupoux, 2018;</ref><ref type="bibr" target="#b61">Jarosz, 2019;</ref><ref type="bibr" target="#b99">Pater, 2019)</ref>. One of the major shortcomings of the majority of the existing proposals is that learning is modeled with an already assumed level of abstraction <ref type="bibr" target="#b33">(Dupoux, 2018)</ref>. In other words, most of the proposals model phonological learning as symbol manipulation of discrete units that already operates on the abstract, discrete phonological level. The models thus require strong assumptions that phonetic learning has already taken place, and that phonemes as discrete units have already been inferred from continuous speech data (for an overview of the literature, see <ref type="bibr" target="#b95">Oudeyer, 2005</ref><ref type="bibr" target="#b96">Oudeyer, , 2006;;</ref><ref type="bibr" target="#b33">Dupoux, 2018)</ref>.</p><p>This paper proposes that language acquisition can be modeled with Generative Adversarial Networks <ref type="bibr" target="#b49">(Goodfellow et al., 2014)</ref>. More specifically, phonetic and phonological computation is modeled as the mapping from random space to generated data of a Generative Adversarial Network <ref type="bibr" target="#b49">(Goodfellow et al., 2014)</ref> trained on raw unannotated acoustic speech data in an unsupervised manner <ref type="bibr" target="#b31">(Donahue et al., 2019)</ref>. To the author's knowledge, language acquisition has not been modeled within the GAN framework despite several advantages of this architecture. The characteristic feature of the GAN architecture is an interaction between the Generator network that outputs raw data and the Discriminator that distinguishes real data from Generator's outputs <ref type="bibr" target="#b49">(Goodfellow et al., 2014)</ref>. A major advantage of the GAN architecture is that learning is completely unsupervised, the networks include no languagespecific elements, and that, as is argued in Section 4 below, phonetic learning is modeled simultaneously with phonological learning. The discussion on the relationship between phonetics and phonology is highly complex <ref type="bibr" target="#b68">(Kingston and Diehl, 1994;</ref><ref type="bibr" target="#b25">Cohn, 2006;</ref><ref type="bibr" target="#b67">Keyser and Stevens, 2006)</ref>. Several opposing proposals, however, argue that the two interact at various different stages and are not dissociated from each other <ref type="bibr" target="#b54">(Hayes, 1999;</ref><ref type="bibr" target="#b100">Pierrehumbert, 2001;</ref><ref type="bibr" target="#b38">Fruehwald, 2016</ref><ref type="bibr" target="#b39">Fruehwald, , 2017))</ref>. A network that models learning of phonetics from raw data and shows signs of phonological learning is likely one step closer to reality than models that operate with symbolic computation and assume phonetic learning has already taken place independently of phonology <ref type="bibr">(and vice versa)</ref>.</p><p>We argue that the latent variables in the input of the Generator network can be modeled as approximates to phonetic or potentially phonological representations that the Generator learns to output into a speech signal by attempting to maximize the error rate of a Discriminator network that distinguishes between real data and generated outputs. The Discriminator network thus has a parallel in human speech: the imitation principle <ref type="bibr" target="#b91">(Nguyen and Delvaux, 2015)</ref>. The Discriminator's function is to enforce that the Generator's outputs resemble (but do not replicate) the inputs as closely as possible. The GAN network thus incorporates both the pre-articulatory production elements (the Generator) as well as the imitation principle (the Discriminator) in speech acquisition. While other neural network architectures might be appropriate for modeling phonetic and phonological learning as well, the GAN architecture is unique in that it combines a network that produces innovative data (the Generator) with a network that forces imitation in the Generator. Unlike, for example, autoencoder networks, the Generative Adversarial network lacks a direct connection between the input and output data and generates innovative data rather than data that resembles the input as closely as possible.</p><p>We train a Generative Adversarial Network architecture implemented for audio files in <ref type="bibr" target="#b31">Donahue et al. (2019)</ref>  <ref type="bibr">(WaveGAN)</ref> on raw speech data that contains information for an allophonic distribution: word-initial pre-vocalic aspiration of voiceless stops (["p h It] ∼ ["spIt]). The data is curated in order to control for nondesired effects, which is why only sequences of the shape #TV and #sTV<ref type="foot" target="#foot_0">foot_0</ref> are fed to the model. This allophonic distribution is appropriate for testing learnability in a GAN architecture, because the dependency between the presence of [s] and duration of VOT is not strictly local. To be sure, the dependency is local in phonological terms, as <ref type="bibr">[s]</ref> and T are two segments and immediate neighbors, but in phonetic terms, a period of closure intervenes between the aspiration and the period (or absence thereof) of frication noise of <ref type="bibr">[s]</ref>. It is not immediately clear whether a GAN model is capable of learning such non-local dependencies. To our knowledge, this is the first proposal that tests whether neural networks are able to learn an allophonic distribution based on raw acoustic data.</p><p>The hypothesis of the computational experiment presented in Section 4 is the following: if VOT duration is conditioned on the presence of <ref type="bibr">[s]</ref> in output data generated from noise by the Generator network, it means that the Generator network has successfully learned a phonetically non-local allophonic distribution. Because the allophonic distribution is not strictly local and has to be learned and actively controlled by speakers (i.e., is not automatic), evidence for this type of learning is considered phonological learning in the broadest sense. Conditioning the presence of a phonetic feature based on the presence or absence of a phoneme that is not automatic is, in most models, considered part of phonology and is derived with phonological computation. That the tested distribution is nonautomatic and has to be actively controlled by the speakers is evident from L1 acquisition: failure to learn the distribution results in longer VOT durations in the sT condition documented in L1 acquisition (see Section 5.1).</p><p>The results suggest that phonetic and phonological learning can be modeled simultaneously, without supervision, directly from what language-acquiring infants are exposed to: raw acoustic data. A GAN model trained on an allophonic distribution is successful in learning to generate acoustic outputs that contain this allophonic distribution (VOT duration). Additionally, the model outputs innovative data for which no evidence was available in the training data, allowing a direct comparison between human speech data and the GAN's generated output. As argued in Section 4.2, some outputs are consistent with human linguistic behavior and suggest that the model recombines individual sounds, resembling phonemic representations and productivity in human language acquisition (Section 5).</p><p>This paper also proposes a technique for establishing the Generator's internal representations. The inability to uncover networks' representations has been used as an argument against neural network approaches to linguistic data (among others in <ref type="bibr" target="#b108">Rawski and Heinz, 2019)</ref>. We argue that the internal representation of a network can be, at least partially, uncovered. By regressing annotated dependencies between the Generator's latent space and output data, we identify values in the latent space that correspond to linguistically meaningful features in generated outputs. This paper demonstrates that manipulating the chosen values in the latent space has phonetic effects in the generated outputs, such as the presence of [s] and the amplitude of its frication. In other words, the GAN learns to use random noise as an approximation of phonetic (and potentially phonological) representations. This paper proposes that dependencies, learned during training in a latent space that is limited by some interval, extend beyond this interval. This crucial step allows for the discovery of several phonetic properties that the model learns.</p><p>By modeling phonetic and phonological learning with neural networks without any language-specific assumptions, the paper also addresses a broader question of how many languagespecific elements are needed in models of grammar and language acquisition. Most of the existing models require at least some language-specific devices, such as rules in rule-based approaches or pre-determined constraints with features and feature matrices in connectionist approaches. The model proposed here lacks language-specific assumptions (similar to the exemplar-based models). Comparing the performance of substance-free models with competing proposals and human behavior should result in a better understanding of what aspects of phonological grammar and acquisition are domain-specific (Section 5).</p><p>In the following, we first survey existing theories of phonological grammar and literature on computational approaches to phonology (Section 2). In Section 3, we present the model in <ref type="bibr" target="#b31">Donahue et al. (2019)</ref> based on <ref type="bibr" target="#b106">Radford et al. (2015)</ref> and provide acoustic and statistical analysis of the training data. The network's outputs are first acoustically analyzed and described in Sections 4.1 and 4.2. In Section 4.3, we present a technique for establishing the network's internal representations and test it with two generative tests. In Section 4.5, we analyze phonetic properties of the network's internal representations. Section 5 compares the outputs of the model with L1 acquisition, speech impairments, and speech errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">PREVIOUS WORK</head><p>In the generative tradition, phonological grammar derives surface phonetic outputs from phonological inputs <ref type="bibr" target="#b23">(Chomsky and Halle, 1968)</ref>. For example, /p/ is an abstract unit that can surface (be realized) with variations at the phonetic level. English /p/ is realized as aspirated [p h ] (produced with a puff of air) word-initially before stressed vowels, but as unaspirated plain [p] (without the puff of air) if <ref type="bibr">[s]</ref> immediately precedes it. This distribution is completely predictable and derivable with a simple rule <ref type="bibr" target="#b60">(Iverson and Salmons, 1995)</ref>, which is why the English phoneme /p/ as an abstract mental unit is unspecified for aspiration (or absence thereof) in the underlying representation (/"pIt/ "pit" and /"spIt/ "spit"). The surface phonetic outputs after the phonological derivation had taken place are ["p h It] with the aspiration and ["spIt] without the aspiration.</p><p>One of the main objectives of phonological theory is to explain how the grammar derives surface outputs, i.e., phonetic signals, from inputs, i.e., phonemic representations. Two influential proposals have been in the center of this discussion, the rulebased approach and Optimality Theory. The first approach uses rewrite rules <ref type="bibr" target="#b23">(Chomsky and Halle, 1968)</ref> or finite state automata <ref type="bibr" target="#b57">(Heinz, 2010;</ref><ref type="bibr" target="#b22">Chandlee, 2014)</ref> to derive outputs from inputs through derivation. A connectionist approach called Optimality Theory <ref type="bibr" target="#b104">(Prince and Smolensky, 2004)</ref> and related proposals such as Harmonic Grammar and Maximum Entropy (MaxEnt) grammar <ref type="bibr" target="#b73">(Legendre et al., 1990</ref><ref type="bibr" target="#b74">(Legendre et al., , 2006;;</ref><ref type="bibr" target="#b48">Goldwater and Johnson, 2003;</ref><ref type="bibr" target="#b127">Wilson, 2006;</ref><ref type="bibr" target="#b56">Hayes and Wilson, 2008;</ref><ref type="bibr" target="#b98">Pater, 2009;</ref><ref type="bibr" target="#b55">Hayes and White, 2013;</ref><ref type="bibr" target="#b125">White, 2014</ref><ref type="bibr" target="#b126">White, , 2017))</ref>, on the other hand, model phonological grammar as input-output pairing: the grammar chooses the most optimal output given an input. These models were heavily influenced by the early advances in neural network research <ref type="bibr">(Alderete and Tupper, 2018a;</ref><ref type="bibr" target="#b99">Pater, 2019)</ref>. Modeling linguistic data with neural networks has seen a rapid increase in the past few years <ref type="bibr" target="#b5">(Alderete et al., 2013;</ref><ref type="bibr" target="#b8">Avcu et al., 2017;</ref><ref type="bibr" target="#b70">Kirov, 2017;</ref><ref type="bibr">Alderete and Tupper, 2018a;</ref><ref type="bibr" target="#b33">Dupoux, 2018;</ref><ref type="bibr" target="#b83">Mahalunkar and Kelleher, 2018;</ref><ref type="bibr" target="#b123">Weber et al., 2018;</ref><ref type="bibr" target="#b103">Prickett et al., 2019,</ref> for cautionary notes, see <ref type="bibr" target="#b108">Rawski and Heinz, 2019)</ref>. One of the promising implications of neural network modeling is the ability to test generalizations that the models produce without languagespecific assumptions <ref type="bibr" target="#b99">(Pater, 2019)</ref>.</p><p>In opposition to the generative approaches, there exists a long tradition of usage-based models in phonology <ref type="bibr" target="#b19">(Bybee, 1999;</ref><ref type="bibr" target="#b115">Silverman, 2017)</ref> which diverges from the generative approaches in some crucial aspects. Exemplar models <ref type="bibr" target="#b62">(Johnson, 1997</ref><ref type="bibr" target="#b63">(Johnson, , 2007;;</ref><ref type="bibr" target="#b100">Pierrehumbert, 2001;</ref><ref type="bibr" target="#b41">Gahl and Yu, 2006;</ref><ref type="bibr" target="#b65">Kaplan, 2017)</ref>, for example, assume that phonetic representations are stored as experiences or exemplars. Grammatical behavior emerges as a consequence of generalization (or computation) over a cloud of exemplars <ref type="bibr" target="#b63">(Johnson, 2007;</ref><ref type="bibr" target="#b65">Kaplan, 2017)</ref>.</p><p>In this framework, there is no direct need for a separate underlying representation from which the surface outputs are derived (or optimized). Several phenomena in phonetics and phonology have been successfully derived within this approach (for an overview, see <ref type="bibr" target="#b65">Kaplan, 2017)</ref>, and the framework allows phonology to be modeled computationally. The computational models often involve interacting agents learning some simplified phonetic properties (e.g., <ref type="bibr" target="#b27">de Boer, 2000;</ref><ref type="bibr" target="#b124">Wedel, 2006;</ref><ref type="bibr" target="#b69">Kirby and Sonderegger, 2015)</ref>.</p><p>The majority of existing computational models in phonology (including finite state automata, the MaxEnt model and the existing neural network methods) model learning as symbol manipulation and operate with discrete units-either with completely abstract made-up units or with discrete units that feature some phonetic properties that can be approximated as phonemes. This means that either phonetic and phonological learning are modeled separately or one is assumed to have already been completed <ref type="bibr" target="#b84">(Martin et al., 2013;</ref><ref type="bibr" target="#b33">Dupoux, 2018)</ref>. This is true for both proposals that model phonological distributions or derivations <ref type="bibr" target="#b5">(Alderete et al., 2013;</ref><ref type="bibr" target="#b40">Futrell et al., 2017;</ref><ref type="bibr" target="#b70">Kirov, 2017;</ref><ref type="bibr" target="#b103">Prickett et al., 2019)</ref> and featural organizations <ref type="bibr" target="#b36">(Faruqui et al., 2016;</ref><ref type="bibr" target="#b114">Silfverberg et al., 2018)</ref>. The existing models also require strong assumptions about learning: underlying representations, for example, are pre-assumed and not inferred from data <ref type="bibr" target="#b70">(Kirov, 2017;</ref><ref type="bibr" target="#b103">Prickett et al., 2019)</ref>.</p><p>Most models in the subset of the proposals that operate with continuous phonetic data assume at least some level of abstraction and operate with already extracted features (e.g., formant values) on limited "toy" data (e.g., <ref type="bibr" target="#b100">Pierrehumbert, 2001;</ref><ref type="bibr" target="#b69">Kirby and Sonderegger, 2015</ref>, for a discussion, see <ref type="bibr" target="#b33">Dupoux, 2018)</ref>. <ref type="bibr" target="#b51">Guenther and Vladusich (2012)</ref>, <ref type="bibr" target="#b50">Guenther (2016)</ref> and <ref type="bibr" target="#b93">Oudeyer (2001</ref><ref type="bibr" target="#b94">Oudeyer ( , 2002</ref><ref type="bibr" target="#b95">Oudeyer ( , 2005</ref><ref type="bibr" target="#b96">Oudeyer ( , 2006) )</ref> propose models that use simple neural maps that are based on actual correlates of neurons involved in speech production in the human brain (based on various brain imaging techniques). Their models, however, do not operate with raw acoustic data (or require extraction of features in a highly abstract model of articulators; <ref type="bibr" target="#b95">Oudeyer, 2005</ref><ref type="bibr" target="#b96">Oudeyer, , 2006))</ref>, require a level of abstraction in the input to the model, and do not model phonological processesi.e., allophonic distributions. Phonological learning in most of these proposals is thus modeled as if phonetic learning (or at least a subset of phonetic learning) has already taken place: the initial state already includes phonemic inventories, phonemes as discrete units, feature matrices that have already been learned, or extracted phonetic values.</p><p>Prominent among the few models that operate with raw phonetic data are Gaussian mixture models for category-learning or phoneme extraction <ref type="bibr" target="#b72">(Lee and Glass, 2012;</ref><ref type="bibr" target="#b112">Schatz et al., 2019)</ref>. <ref type="bibr" target="#b112">Schatz et al. (2019)</ref> propose a Dirichlet process Gaussian mixture model that learns categories from raw acoustic input in an unsupervised learning task. The model is trained on English and Japanese data and the authors show that the asymmetry in perceptual [l]∼[r] distinction between English and Japanese falls out automatically from their model. The primary purpose of the proposal in <ref type="bibr" target="#b112">Schatz et al. (2019)</ref> is modeling perception and categorization: they model how a learner is able to categorize raw acoustic data into sets of discrete categorical units that have phonetic values (i.e., phonemes). No phonological processes are modeled in the proposal.</p><p>A number of earlier works in the connectionist approach included basic neural network architectures to model mapping from some simplified phonetic space to the discrete phonological space <ref type="bibr" target="#b85">(McClelland and Elman, 1986;</ref><ref type="bibr" target="#b43">Gaskell et al., 1995;</ref><ref type="bibr" target="#b101">Plaut and Kello, 1999;</ref><ref type="bibr" target="#b66">Kello and Plaut, 2003)</ref>. Input to most of these models is not raw acoustic data (except in <ref type="bibr" target="#b66">Kello and Plaut, 2003)</ref>, but already extracted features. Learning in these models is also not unsupervised: the models come pre-specified with discretized phonetic or phonological units. None of the models are generative and do not model learning of phonological processes, but rather of classifying a simplified phonetic space with already available phonological elements.</p><p>Recently, neural network models for unsupervised feature extraction have seen success in modeling acquisition of phonetic features from raw acoustic data <ref type="bibr" target="#b107">(Räsänen et al., 2016;</ref><ref type="bibr" target="#b34">Eloff et al., 2019;</ref><ref type="bibr" target="#b113">Shain and Elsner, 2019)</ref>. The model in <ref type="bibr" target="#b113">Shain and Elsner (2019)</ref>, for example, is an autoencoder neural network that is trained on pre-segmented acoustic data. The model takes as input segmented acoustic data and outputs values that can be correlated to phonological features. Learning is, however, not completely unsupervised as the network is trained on presegmented phones. <ref type="bibr">Thiollière et al. (2015)</ref> similarly propose an architecture that extracts units from unsupervised speech data. Other proposals for unsupervised acoustic analysis with neural network architecture are similarly primarily concerned with unsupervised feature extraction <ref type="bibr" target="#b64">(Kamper et al., 2015)</ref>. These proposals, however, do not model learning of phonological distributions, but only of feature representations, do not show a direct relationship between individual variables in the latent space and acoustic outputs (as in Section 4.4 and Figure <ref type="figure" target="#fig_15">14</ref>), and crucially are not generative, meaning that the models do not output innovative data, but try to replicate the input as closely as possible (e.g., in the autoencoder architecture).</p><p>As argued below, the model based on a Generative Adversarial Network (GAN) learns not only to generate innovative data that closely resemble human speech, but also learns internal representations that resemble phonological learning with unsupervised phonetic learning from raw acoustic data. Additionally, the model is generative and outputs both the conditional allophonic distributions and innovative data that can be compared to productive outputs in human speech acquisition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">MATERIALS</head><p>3.1. The Model: <ref type="bibr" target="#b31">Donahue et al. (2019)</ref> Based on <ref type="bibr" target="#b106">Radford et al. (2015)</ref> Generative Adversarial Networks, proposed by <ref type="bibr" target="#b49">Goodfellow et al. (2014)</ref>, have seen a rapid expansion in a variety of tasks, including but not limited to computer vision and image generation <ref type="bibr" target="#b106">(Radford et al., 2015)</ref>. The main characteristic of GANs is the architecture that involves two networks: the Generator network and the Discriminator network <ref type="bibr" target="#b49">(Goodfellow et al., 2014)</ref>. The Generator network is trained to generate data from random noise, while the Discriminator is trained to distinguish real data from the outputs of the Generator network (Figure <ref type="figure" target="#fig_0">1</ref>). The Generator is trained to generate data that maximizes the error rate of the Discriminator network. The training results in a Generator (G) network that takes random noise as its input (e.g., multiple variables with uniform distributions) and outputs data such that the Discriminator is inaccurate in distinguishing the generated from the real data (Figure <ref type="figure" target="#fig_0">1</ref>).</p><p>Applying the GAN architecture to time-series data such as a continuous speech stream poses several challenges. Recently, <ref type="bibr" target="#b31">Donahue et al. (2019)</ref> proposed an implementation of a Deep Convolutional Generative Adversarial Network proposed by <ref type="bibr" target="#b106">Radford et al. (2015)</ref> for audio data (WaveGAN); the model along with the code in <ref type="bibr" target="#b31">Donahue et al. (2019)</ref> were used for training in this paper. The model takes 1 s long raw audio files as inputs, sampled at 16 kHz with 16-bit quantization. The audio files are converted into a vector and fed to the Discriminator network as real data. Instead of the two-dimensional 5 × 5 filters, the WaveGAN model uses one-dimensional 1 × 25 filters and larger upsampling. The main architecture is preserved as in DCGAN, except that an additional layer is introduced in order to generate longer samples <ref type="bibr" target="#b31">(Donahue et al., 2019)</ref>. The Generator network takes as input z, a vector of one hundred uniformly distributed variables (z ∼ U(-1, 1)) and outputs 16,384 data points, which constitutes the output audio signal. The network has five 1D convolutional layers <ref type="bibr" target="#b31">(Donahue et al., 2019)</ref>. The Discriminator network takes 16,384 data points (raw audio files) as its input and outputs a single value. The Discriminator's weights are updated five times per each update of the Generator. The initial GAN design as proposed by <ref type="bibr" target="#b49">Goodfellow et al. (2014)</ref> trained the Discriminator network to distinguish real from generated data. Training such models, however, posed substantial challenges <ref type="bibr" target="#b31">(Donahue et al., 2019)</ref>. <ref type="bibr" target="#b31">Donahue et al. (2019)</ref> implement the WGAN-GP strategy <ref type="bibr" target="#b6">(Arjovsky et al., 2017;</ref><ref type="bibr" target="#b52">Gulrajani et al., 2017)</ref>, which means that the Discriminator is trained "as a function that assists in computing the Wasserstein distance" <ref type="bibr" target="#b31">(Donahue et al., 2019)</ref>. The WaveGAN model <ref type="bibr" target="#b31">(Donahue et al., 2019)</ref> uses ReLU activation in all but the last layer for the Generator network, and Leaky ReLU in all layers in the Discriminator network (as recommended for DCGAN in <ref type="bibr" target="#b106">Radford et al., 2015)</ref>. For exact dimensions of each layer and other details of the model, see <ref type="bibr" target="#b31">Donahue et al. (2019)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Training Data</head><p>The model was trained on the allophonic distribution of voiceless stops in English. As already mentioned in Section 1, voiceless stops /p, t, k/ surface as aspirated (produced with a puff of air) [p h , t h , k h ] in English in word-initial position when immediately followed by a stressed vowel <ref type="bibr" target="#b78">(Lisker, 1984;</ref><ref type="bibr" target="#b60">Iverson and Salmons, 1995;</ref><ref type="bibr" target="#b120">Vaux, 2002;</ref><ref type="bibr" target="#b121">Vaux and Samuels, 2005;</ref><ref type="bibr" target="#b26">Davis and Cho, 2006)</ref>. If an alveolar sibilant [s] precedes the stop, however, the aspiration is blocked and the stop surfaces as unaspirated <ref type="bibr">[p, t, k]</ref>  <ref type="bibr" target="#b121">(Vaux and Samuels, 2005)</ref>. A minimal pair illustrating this allophonic distribution is ["p h It] "pit" vs. ["spIt] "spit." The most prominent phonetic correlate of this allophonic distribution is the difference in Voice Onset Time (VOT) duration <ref type="bibr" target="#b79">(Lisker and Abramson, 1964;</ref><ref type="bibr" target="#b0">Abramson and Whalen, 2017)</ref> between the aspirated and unaspirated voiceless stops. VOT is the duration between the release of the stop <ref type="bibr">([p, t, k]</ref>) and the onset of periodic vibration in the following vowel.</p><p>The model was trained on data from the TIMIT database <ref type="bibr" target="#b42">(Garofolo et al., 1993)</ref> <ref type="foot" target="#foot_3">foot_3</ref> . The corpus was chosen because it is one of the largest currently available hand-annotated speech corpora, the recording quality is relatively high, and the corpus features a relative high degree of variability. The database includes 6,300 sentences, 10 sentences per 630 speakers from 8 major dialectal areas in the US <ref type="bibr" target="#b42">(Garofolo et al., 1993)</ref>. The training data consist of 16-bit •wav files with 16 kHz sampling rate of word initial sequences of voiceless stops /p, t, k/ (= T) that were followed by a vowel (#TV) and word initial sequences of /s/ + /p, t, k/, followed   Both stressed and unstressed vowels are included in the training data. Including both stressed and unstressed vowels is desirable, as this condition crucially complicates learning and makes the task for the model more challenging as well as more realistic. Aspiration is less prominent in word-initial stops not followed by a stressed vowel. This means that in the condition #TV, the stop will be either fully aspirated (if followed by a stressed vowel) or unaspirated (if followed by an unstressed vowel). Violin plots in Figure <ref type="figure" target="#fig_4">3</ref> illustrate that aspiration of stops before an unstressed vowel can be as short as in the #sTV condition. In the #sTV condition, the stop is never aspirated. Learning of two conditions is more complex if the dependent variable in one condition can range across the variable in the other condition.</p><p>The training data is not completely naturalistic: #TV and #sTV sequences are sliced from continuous speech data. This, however, has a desirable effect. The primary purpose of this paper is to test whether a GAN model can learn an allophonic distribution from data that consists of raw acoustic inputs. If the entire lexicon was included in the training data, the distribution of VOT duration could be conditioned on some other distribution, not the one this paper is predominately interested in: the presence or absence of <ref type="bibr">[s]</ref>. It is thus less likely that the distribution of VOT duration across the main condition of interest, the presence of [s], is conditioned on some other unwanted factor in the model precisely because of the balanced design of the training data. The only condition that can potentially influence learning is the distribution of vowels across the two conditions. Figure <ref type="figure" target="#fig_5">4</ref>, however, shows that vowels are relatively equally distributed across the two conditions, which means that vowel identity likely does not influence the outcomes substantially. Finally, vowel duration (or the equivalent of speech rate in real data) and identity are not controlled for in the present experiment. To control for vowel duration, VOT duration would have to be modeled as a proportion of the following vowel duration. Several confounds that are not easy to address would be introduced, the main of which is that vowel identification is problematic for generated inputs with fewer training steps. Because the primary interest of the experiment is the difference in VOT durations between two groups (the presence and absence of [s]) and substantial differences in vowel durations (or speech rate) between the two groups are not expected, we do not anticipate the results to be substantially influenced by speech rate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">EXPERIMENT</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Training and Generation</head><p>The purpose of this paper is to model phonetic and phonological learning. For this reason, the data was generated and examined at different points as the Generator network was in the process of being trained. For the purpose of modeling learning, it is more informative to probe the networks with fewer training steps, which allows a comparison between the model's outputs and L1 acquisition (Section 5.1). Outputs of the network are analyzed after 12,255 steps (Section 4.2). The number of steps was chosen as a compromise between quality of output data and the number of epochs in the training. Establishing the number of training steps at which an effective acoustic analysis can be performed is at this point somewhat arbitrary. We generated outputs of the Generator model trained after 1, <ref type="bibr">474, 4,075, 6,759, 9,367, and 12,255</ref> steps and manually inspected them. The model trained after 12,255 steps was considered the first that allowed  a reliable acoustic analysis based on quality of the generated outputs. It would be informative to test how accuracy of labeled data improves with training steps, but this is left for future work. The model was trained on a single NVIDIA K80 GPU. The network was trained at an approximate pace of 40 steps per 300 s. In Section 4.2, we present measurements of VOT durations in the #sTV and #TV conditions in the generated outputs and discuss linguistically interpretable innovative outputs that violate the training data. In Section 4.3.1, we propose a technique for recovering the Generator network's internal representations; in Section 4.4 we illustrate that manipulating these variables has a phonetically meaningful effect in the output data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">VOT Duration</head><p>The Generator network after 12,255 steps (∼ 716 epochs) generates acoustic data that appear close to actual speech data. Figure <ref type="figure" target="#fig_6">5</ref> illustrates a typical generated sample of #TV (left) and #sTV (right) structures with a substantial difference in VOT durations.</p><p>To test whether the Generator learns the conditional distribution of VOT duration, 2,000 samples were generated and manually inspected. First, VOT duration was manually annotated in all #sTV sequences. There were altogether 156 such sequences. To perform significance testing on a similar sample size, the first 158 sequences of the #TV structure were also annotated for VOT duration. VOT was measured from the release of closure to the onset of periodic vibration with clear formant structure. Altogether 314 generated samples were thus annotated. Only samples with structure that resembles real acoustic outputs and for which VOT could be determined were annotated. The proportion of inputs for which a clear #sTV or #TV sequence was not recognizable is relatively small: in only 8 of the first 175 annotated outputs (4.6%) was it not possible to estimate the VOT duration or whether the sequence is of the #TV or #sTV structure. Figure <ref type="figure">6</ref> shows the raw distribution of VOT durations in the generated samples that closely resembles the distribution in the training data (Figure <ref type="figure" target="#fig_4">3</ref>).</p><p>The results suggest that the network does learn the allophonic distribution: VOT duration is significantly shorter in the #sTV condition (β = -2.79, t = -78.34, p &lt; 0.0001; for details of the statistical model, see Section 2, Supplementary Materials). Figure <ref type="figure">6</ref> illustrates estimates of VOT duration across the two conditions with 95% confidence intervals. The model, however, shows clear traces that the learning is imperfect and that the generator network fails to learn the distribution categorically. This is strongly suggested by the fact that VOT durations are substantially longer in the generated data compared to the training data. The difference in means between the #TV and #sTV conditions in the training data is 32.35 ms, while in the generated data the difference is 22.52 ms. The ratio between the two conditions in the training data is 2.34, while the generated data's ratio is 1.59.</p><p>Another aspect of generated data that also strongly suggests the learning is imperfect is the fact that the longest VOT durations in the #sTV condition in the generated data are substantially longer than the longest VOT durations in the training data, where the longest duration reaches 65 ms (see Table <ref type="table" target="#tab_0">1</ref> and Figure <ref type="figure" target="#fig_4">3</ref>). VOT in the generated data is in 19 out of 156 total #sTV sequences (or 12.2%) longer than 65.5 ms, the longest VOT in the training data. The longest three VOT durations in #sTV sequences are, for example, 109.35, 84.17, and 82.37 ms. This generalization holds also in proportional terms. To control for the overall duration of segments in the output, we measure ratio of VOT duration and duration of the preceding [s] (i.e., thus controlling for "speech rate"). The highest ratio between the VOT duration and the duration of preceding [s] ( VOT  [s] ) in the training data is 0.77<ref type="foot" target="#foot_4">foot_4</ref> , which appears in an acoustically very different token compared to the generated outputs. The ratio in all other tokens in the training data are even lower, below 0.69. Several values of the ratios between VOT and [s] duration in the generated data are substantially higher compared to the training data. In the three outputs with longest absolute duration of VOT, the ratios are 1.91, 1.40, and 0.89. Other high ratios measured include, for example, 1.79, 1.72, 1.60, 1.50, 1.46. Figure <ref type="figure" target="#fig_7">7</ref> shows two such cases. It is clear that the generator fails to reproduce the conditioned durational distribution from the training data in these particular cases. In other words, while the Generator learns to output significantly shorter VOT durations when [s] is present in the output, it occasionally (in approximately 12.2% of cases) fails to observe this generalization and outputs a long VOT in the #sTV condition which is longer than any VOT duration in the #sTV condition in the training data. As will be argued in Section 5.1, the outcomes of this imperfect learning closely resemble L1 acquisition.</p><p>Longer VOT duration in the #sTV condition in the generated data compared to training data is not the only violation of the training data that the Generator outputs and that resembles linguistic behavior in humans. Among approximately 3,000 generated samples analyzed, we observe generated outputs that feature only frication noise of [s] and periodic vibration of the following vowel, but lack stop elements completely (e.g., closure and release of the stop). In other words, the generator occasionally outputs a linguistically valid and innovative #sV sequence for which no evidence was available in the training data. Such innovative sequences in which the segments are omitted or inserted are rare compared to innovative outputs with longer VOT-approximately two per 3,000 inspected cases (but the overall rate of outputs that are acoustically difficult to analyze is also small: 4.6%). All sequences containing [s] from the training data were manually inspected by the author and none of them contain a #sV sequence without a period of closure and VOT.</p><p>The minimal duration of closure in #sTV sequences in the training data is 9.2 ms, and the minimal duration of VOT is 9.4 ms. Aspiration noise in stops that resembles frication of [s] and homorganic sequences of [s] followed by an alveolar stop [t] (#stV) are occasionally acoustically similar to the sequence without the stop (#sV) due to similar articulatory positions or because frication noise from [s] carries onto the homorganic alveolar closure which can be very short. Such data points in the training data can serve as the basis for the innovative output #sV. However, there is a clear fall and a second rise of noise amplitude after the release of the stop in #stV sequences. Figure <ref type="figure" target="#fig_8">8</ref> shows two cases of the Generator network outputting an innovative #sV sequence without any stop-like fall of the amplitude, for which no direct evidence exists in the training data.</p><p>Similarly, the Generator occasionally outputs a sequence with two stops and a vowel (#TTV). One potential source of such innovative sequences might be residual noise that is sometimes present during the period of closure in the TIMIT database. However, residual noise in the training data differs substantially from a clear aspiration noise in the generated #TTV sequences. Figure <ref type="figure" target="#fig_9">9</ref> illustrates two generated examples in which the vocalic period is preceded by two bursts, two periods of aspiration and a short period of silence between the aspiration noise of the first consonant and the burst of the second consonant that corresponds to closure of the second stop<ref type="foot" target="#foot_5">foot_5</ref> . Spectrograms show the distribution of energy differs across the two bursts and aspiration noises, suggesting that the output represents a heterogranic cluster [pt] followed by a vowel.</p><p>Measuring overfitting is a substantial problem for Generative Adversarial Networks with no consensus on the most appropriate quantitative approach to the problem <ref type="bibr" target="#b49">(Goodfellow et al., 2014;</ref><ref type="bibr" target="#b106">Radford et al., 2015)</ref>. The risk with overfitting in a GAN architecture is that the Generator network would learn to fully replicate the input<ref type="foot" target="#foot_6">foot_6</ref> . The best evidence against overfitting is precisely the fact that the Generator network outputs samples that substantially violate data distributions (Figures <ref type="figure" target="#fig_7">7</ref><ref type="figure" target="#fig_8">8</ref><ref type="figure" target="#fig_9">9</ref>)<ref type="foot" target="#foot_7">foot_7</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Establishing Internal Representations</head><p>Establishing what and how neural networks learn is a challenging task <ref type="bibr" target="#b76">(Lillicrap and Kording, 2019)</ref>. Exploration of latent space in the GAN architecture has been performed before <ref type="bibr" target="#b106">(Radford et al., 2015;</ref><ref type="bibr" target="#b30">Donahue et al., 2017;</ref><ref type="bibr" target="#b77">Lipton and Tripathi, 2017)</ref>, but to the author's knowledge, most previous work did not focus on discovering meaningful values (phonetic correlates) of each variable and has not fully used the potential to extend those variables to values well outside the training range (15 or 25). Below, we propose a technique for uncovering dependencies   between the network's latent space and generated data based on logistic regression. We first use regression estimates to identify variables with a desired effect on the output by correlating the outputs of the Generator with its corresponding input variables that are uniformly distributed with an interval (-1, 1) during training. We then Generate outputs by setting the identified latent variables to values well beyond the training range <ref type="bibr">(to 4.5, 15, or 25)</ref>. This method has the potential to reveal the underlying values of latent variables and shed light on the network's internal representations. Using the proposed technique, we can estimate how the network learns to map from latent space to phonetically and phonologically meaningful units in the generated data.</p><p>To identify dependencies between the latent space and generated data, we correlate annotations of the output data with the variables in the latent space (in Section 4.3.1). As a starting point, we choose to identify correlates of the most prominent feature in the training data: the presence or absence of <ref type="bibr">[s]</ref>. Any number of other phonetic features can be correlated with this approach (for future directions, see Section 6); applying this technique to other features and other alternations should yield a better understanding of the network's learning mechanisms. Focusing on more than the chosen feature, however, is beyond the scope of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1.">Regression</head><p>First, 3,800 outputs from the Generator network were generated and manually annotated for the presence or absence of <ref type="bibr">[s]</ref>. 271 outputs (7.13%) were annotated as involving a segment [s] which is similar to the percentage of data points with [s] in the training data (9.8%). Frication that resembled [s]-like aspiration noise after the alveolar stop and before high vowels was not annotated as including [s]<ref type="foot" target="#foot_8">foot_8</ref> . Innovative outputs such as an #[s] without the following vowel or #sV sequences were annotated as including an <ref type="bibr">[s]</ref>.</p><p>The annotated data together with values of latent variables for each generated sample (z) were fit to a logistic regression generalized additive model (using the mgcv package; Wood, 2011 in R Core Team, 2018) with the presence or absence of [s] as the dependent variable (binomial distribution of successes and failures) and smooth terms of latent variables (z) as predictors of interest (estimated as penalized thin plate regression splines; <ref type="bibr" target="#b128">Wood, 2011)</ref>. Generalized additive models were chosen in order to avoid assumptions of linearity: it is possible that latent variables are not linearly correlated with features of interest in the output of the Generator network. The initial full model (FULL) includes smooths for all 100 variables in the latent space that are uniformly distributed within the interval (-1, 1) as predictors.</p><p>The models explored here do not serve for hypothesis testing, but for exploratory purposes: to identify variables, the effects of which are tested with two independent generative tests (see Sections 4.3.2 and 4.3.3). For this reason, several strategies to reduce the number of variables in the model with different shrinkage techniques are explored and compared: the latent variables for further analysis are then chosen based on combined results of different exploratory models.</p><p>First, we refit the model with modified smoothing penalty (MODIFIED), which allows shrinkage of the whole term <ref type="bibr" target="#b128">(Wood, 2011)</ref>. Second, we refit the model with original smoothing penalty (SELECT), but with an additional penalty for each term if all smoothing parameters tend to infinity <ref type="bibr" target="#b128">(Wood, 2011)</ref>. Finally, we identify non-significant terms by Wald test for each term (using anova.gam() with α = 0.05) and manually remove them from the model (EXCLUDED). 38 predictors are thus removed.</p><p>The estimated smooths appear mostly linear (Figure <ref type="figure" target="#fig_12">11</ref>). We also fit the data to a linear logistic regression model (LINEAR) with all 100 predictors. To reduce the number of predictors, another model is fit (LINEAR EXCLUDED) with those predictors removed that do not improve fit (based on the AIC criterion when each predictor is removed from the full model). 23 predictors are thus removed. The advantage of the linear model is that predictors are parametrically estimated<ref type="foot" target="#foot_9">foot_9</ref> . While the number of predictors in the models is high even after shrinkage or exclusion, there is little multicollinearity in the data as the 100 variables are randomly sampled for each generation. The highest Variance Inflation Factor in the linear logistic regression models (LINEAR and LINEAR EXCLUDED) estimated with the vif() function (in the car package; Fox and Weisberg, 2019) is 1.287. All concurvity estimates in the nonlinear models are below 0.3 (using concurvity() in <ref type="bibr" target="#b128">Wood, 2011)</ref>. While the number of successes per predictor is relatively low, it is unlikely that more data would yield substantially different results (as will be shown in Sections 4.3.2 and 4.3.3, the model successfully identifies those values that have direct phonetic correlates in the generated data).</p><p>Six models are thus fit in an exploratory method to identify variables in the latent space that predict the presence of [s] in generated outputs. Table <ref type="table" target="#tab_1">2</ref> lists AIC for each model. The LINEAREXCLUDED model has the lowest AIC score. All six models, however, yield similar results. For further tests based on Lasso regression and Random Forest models that also yield similar results, see Section 3 (Supplementary Materials).</p><p>To identify the latent variables with the highest correlation with [s] in the output, we extract χ 2 estimates for each term from the generalized additive models and estimates of slopes (β) from the linear model. Figure <ref type="figure" target="#fig_11">10</ref> plots those values in a descending order. The plot points to a substantial difference between the highest seven predictors and the rest of the latent space. Seven latent variables are thus identified (z 5 , z 11 , z 49 , z 29 , z 74 , z 26 , z 14 ) as potentially having the largest effect on the presence or absence of [s] in output. Several methods for finding the features that predict the presence or absence of [s] are thus used. Logistic regression is presented here because it is the simplest and easiest to interpret.  In future work, a combination of techniques is recommended to be used for exploratory purposes in a similar way as proposed in this paper. Below, we conduct two independent generative tests to evaluate whether the proposed technique indeed identifies variables that correspond to presence of [s] in the output.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2.">Generative Test 1</head><p>To conduct an independent generative test of whether the chosen values correlate with [s] in the output data of the Generator network, we set values of the seven identified predictors (z 5 , z 11 , z 49 , z 29 , z 74 , z 26 , z 14 ) to the marginal value of 1 or -1 (depending on whether the correlation is positive or negative; see Figure <ref type="figure" target="#fig_12">11</ref>) and generated 100 outputs. Altogether seven values in the latent space were thus manipulated, which represents only 7% (7/100) of all latent variables. Of the 100 outputs with manipulated values, 73 outputs included an [s] or [s]-like element, either with the stop closure and vowel or without them. The rate of outputs that contain [s] is thus significantly higher when the seven values are manipulated to the marginal levels compared to randomly chosen latent space. In the output data without manipulated values, only 271 out of 3,800 generated outputs (or 7.13%) contained an [s]. The difference is significant [χ 2</p><p>(1) = 559.0, p &lt; 0.00001]. High proportions of [s] in the output can be achieved with manipulation of single latent variables, but the values need to be highly marginal, i.e., extend well beyond the training space. Setting the z 11 value outside the training interval to -15, for example, causes the Generator to output [s] in 87 out of 100 generated (87%) sequences, which is again significantly more than with randomly distributed input latent variables [χ 2</p><p>(1) = 792.7, p &lt; 0.0001]. When z 11 is -25, the rate goes up to 96 out of 100, also significantly different from random inputs [χ 2</p><p>(1) = 959.8, p &lt; 0.0001].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3.">Generative Test 2</head><p>To further confirm that the regression models identify the variables involved with the presence of [s] in generated outputs, another generative experiment was conducted. In addition to manipulating the seven identified variables, we test the effect of other variables in the latent space on the presence of [s] in the output. If the regression estimates provide reliable information, the variables with higher estimates should have more of an effect on the presence of [s] in the output and vice versa. Testing the entire latent space would be too expensive, which is why we limit our tests to 25 variables with highest estimates from the regression models (which includes the seven chosen variables) and six additional variables with descending regression estimates. Altogether 31/100 variables or 31% of the latent variables are thus analyzed. The variables were chosen in the following way: first, we manipulate values of the first 25 variables with the highest estimates based on regression models in Figure <ref type="figure" target="#fig_11">10</ref> (7 chosen variables plus additional 18 variables for a total of 25). Because we want to test the effects of the latent variables as evenly as possible and also to test the effects of variables with the lowest regression estimates, we picked 6 additional variables that are distanced from the 25th highest variable in increments of 5 (random choice of variables might miss the variables with lowest estimates).</p><p>To perform the generative test of the correlation between the latent space and the proportion of [s] in the output, we set each of the 31 variables at a time to a marginal level well beyond the training interval (to ±4.5), while keeping the rest of the latent space randomly sampled, but constant. In other words, all variables are sampled randomly and held constant across all samples, with the exception of the variable in question at a time that is set to ±4.5. The ±4.5 value was chosen based on manual inspection of generated samples: as is clear from Figure <ref type="figure" target="#fig_17">15</ref>, changes in amplitude of [s] become increasingly smaller when variables have a value greater than ±3.5. For effects of values beyond 4.5, see Figure <ref type="figure" target="#fig_13">12</ref>.</p><p>One hundred outputs are generated for each of the 31 manipulated latent variables. Altogether 31 × 100 (3,100) outputs were thus analyzed and annotated for the presence or absence of [s] in the output. For example, when the effect of latent variable z 11 on the proportion of [s] in the output is tested, we set its value to -4.5 while keeping other variables random. Onehundred samples are generated in which the other 99 variables are randomly distributed with the exception of the z 11 variable (which is set at the marginal level). Samples are annotated for the presence or absence of [s] and the proportion of [s] in the output is calculated from the number of samples with [s] divided by the number of all samples. The same procedure is applied to the other 30 variables examined. To control for the unwanted effects of the latent space on the output, all 99 other variables with the exception of the one manipulated are kept constant across all 31 samples. The 31 data points of this proportion are thus the dependent variable in regression models (Figure <ref type="figure" target="#fig_14">13</ref>) that test the correlation between the identified variables and [s] in the output.</p><p>A beta regression model with the proportion of [s] as the dependent variable and with estimates of the Linear model as the independent variable suggests that there exists a significant linear correlation between the estimates of the regression model and the actual proportion of generated outputs with [s]: β = 1.44, z = 5.07, p &lt; 0.0001 (for details on model selection, see Section 4, Supplementary Materials). In other words, the technique for identifying latent variables that correlate with the presence of [s] in the output based on regression models (in Figure <ref type="figure" target="#fig_11">10</ref>) successfully identifies such variables. This is confirmed independently: the proportion of generated outputs containing an [s] correlates significantly with its estimates from the regression models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.4.">Interpretation</head><p>The regression models in Figure <ref type="figure" target="#fig_11">10</ref> identify those z-variables in the latent space that have the largest effect on the presence of [s] in the output. Figure <ref type="figure" target="#fig_14">13</ref> confirms that the generator outputs significantly higher proportions of [s] for some variables, while other variables have no effect on the presence of <ref type="bibr">[s]</ref>. In other words, variables with lower regression estimates do not affect the proportion of [s] in the output. The proportion of [s] in the output when variables such as z 90 , z 9 , z 4 , z 7 are manipulated is very close to the 7.13% of [s] in the output when all z-variables in the latent space are random. It thus appears that the Generator uses portions of the latent space to encode the presence of [s] in the output.</p><p>Some latent variables cause a high proportion of [s] in the output despite the regression model estimating their contribution lower than the seven identified latent variables (Figure <ref type="figure" target="#fig_14">13</ref>) and vice versa. Outputs for variable z 14 contain frication noise that falls between [s] and [s]-like aspiration, which were difficult to classify (also, the target for [s]-like outputs in this variable is closer to 2.5). The two variables with the highest proportion of [s] in the output that are estimated substantially lower than the seven variables are z 41 and z 98 . There is a clear explanation for the discrepancy of the regression estimates and the rates of [s]outputs for such variables. While outputs at the marginal values of the two variables (at ±4.5) do indeed contain a high proportion of [s]-outputs, the frication noise ceases during the (-1, 1) interval on which the model is trained. Because the regression model only sees the training interval (-1, 1) (annotations fed to the regression models are performed on this interval) and does not access outputs with variables outside of this interval, the estimates are consequently lower than the outputs at the marginal levels for these variables. There are only a handful of such variables, and since we are primarily interested in those variables that correspond to [s] both within the training interval and outside of it, we focus our analysis below on the seven variables identified in Section 4.4. The problem with variables in which [s]-outputs are present predominantly outside of the training interval is the possibility that the [s]-output in these types of cases is secondary/conditioned on some other distribution, because it was likely not encoded in the training stage.</p><p>While there is a consistent drop in estimates of the regression models after the seven identified variables (Figure <ref type="figure" target="#fig_11">10</ref>) and while several independent generation tests confirm that the seven variables have the strongest effect on the presence of [s] in the output, the cutoff point between the seven variables and the rest of the latent space is still somewhat arbitrary. It is likely that other latent variables directly or indirectly influence the presence of [s] as well: the learning at this point is not yet categorical and several dependencies not discovered here likely affect the results. Nevertheless, further explorations of the latent space suggest the variables identified with the logistic regression (and other) models (Figure <ref type="figure" target="#fig_11">10</ref>) are indeed the main variables involved with the presence or absence of [s] in the output.</p><p>Additionally, if at the value of z that so substantially exceeds the training interval (±4.5) the latent variable does not influence the outcomes substantially and only marginally increases the proportion of [s]-outputs, as is the case for the majority of the latent variables outside of the seven chosen ones, it is likely that its correlation with [s] in the output is secondary and that the variable does not contribute crucially to the presence of [s].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Interpolation and Phonetic Features</head><p>Fitting the annotated data and corresponding latent variables from the Generator network to generalized additive and linear logistic regression models identifies values in the latent space that correspond to the presence of [s] in the output. As will be shown below, this is not where exploration of the Generator's internal representations should end. We explore whether the mapping between the uniformly distributed input z-variables and the Generator's output signal that resembles speech can be associated with specific phonetic features in that output. The crucial step in this direction is to explore values of the latent space and their phonetic correlates in the output beyond the training interval, i.e., beyond (-1, 1). We observe that the Generator network, while being trained on latent space limited to the interval (-1, 1), learns representations that extend this interval. Even if the input latent variables (z) exceed the training interval, the Generator network outputs samples that closely resemble human speech. Furthermore, the dependencies learned during training extend outside of the (-1, 1) interval. As is argued in Section 4.5, exploring phonetic properties at these marginal values has the potential to reveal the actual underlying function of each latent variable.</p><p>To explore phonetic correlates of the seven latent variables, we set each of the seven variables separately to the marginal value -4.5 and interpolate to its opposite marginal value 4.5 in 0.5 increments, while keeping randomly-sampled values of the other 99 latent variables z constant. Again, the ±4.5 value was chosen based on manual inspection of generated samples: amplitude of [s] ceases to change substantially past values around ±3.5 (Figure <ref type="figure" target="#fig_17">15</ref>). Seven sets of generated samples are thus created, one for each of the seven z values: z 5 , z 11 , z 14 , z 26 , z 29 , z 49 , and z 74 (with the other 99 z-values randomly sampled, but kept constant for all seven manipulated variables). Each set contains a subset of 19 generated outputs that correspond to the interpolated variables from -4.5 to 4.5 in 0.5 increments. Twenty-nine such sets that contained an [s] in at least one set are extracted for analysis (sets that lack an [s] were not analyzed).</p><p>A clear pattern emerges in the generated data: the latent variables identified as corresponding to the presence of [s] via regression (Figure <ref type="figure" target="#fig_11">10</ref>) have direct phonetic correlates and cause changes in amplitude and the presence/absence of frication noise of [s] when each of the seven values in the latent space are manipulated to the chosen values, including values that exceed the training interval. In other words, by manipulating the identified latent variables, we control the presence/absence of <ref type="bibr">[s]</ref> in the output as well as the amplitude of its frication noise. Regardless of these underlying phonetic effects, manipulating the chosen variables has a clear effect of causing <ref type="bibr">[s]</ref> to appear in the output and controlling its amplitude.</p><p>To test the significance of the effects of the seven identified features on the presence of [s] and the amplitude of its frication noise, the 29 generated sets of 19 outputs (with z-value from -4.5 to 4.5) for each of the seven variables were analyzed. The outputs were manually annotated for [s] and the following vowel. Outputs gradually change from #sTV to #TV. Only sequences containing an [s] were analyzed; as soon as [s] stops in the output, annotations were stopped and the outputs were not further analyzed. Altogether 161 trajectories were thus annotated; the total number of data points measured is 1,088 because each trajectory contains a number of measurements of the interpolated values of z. For each datapoint, maximum intensity of the fricative and maximum intensity of the vowel were extracted in Praat <ref type="bibr" target="#b15">(Boersma and Weenink, 2015)</ref> with a 13.3 ms window length (with parabolic interpolation)<ref type="foot" target="#foot_10">foot_10</ref> . Figure <ref type="figure" target="#fig_17">15</ref> illustrates how manipulating the values of z of the chosen variables from the marginal value ±4.5 decreases frication noise in the output until [s] is completely absent.</p><p>To test whether the decreased frication noise is not part of a general effect of decreased amplitude, we perform significance tests on the ratio of maximum intensity between the frication noise of [s] and the following vowel in the #sTV sequences. Figure <ref type="figure" target="#fig_18">16</ref> plots the ratio of maximum intensity of the fricative divided by the sum of two maximum intensities: of the fricative ([s]) and of the vowel (V). The manipulated z-values are additionally normalized to the interval [0,1], where 0 represents the most marginal value with [s] (usually ±4.5; referred to as STRONG henceforth) and 1 represents the last value before [s] disappears (WEAK). Note that the point at which [s] is not present in the output anymore, but the vowel still surfaces (which would yield the ratio at 0) is not included in the model. The data were fit to a beta regression generalized additive mixed model (in the mgcv package; <ref type="bibr" target="#b128">Wood, 2011)</ref> with the ratio as the dependent variable, the seven chosen variables as the parametric term, thin-plate smooths for each variable and random smooths (with first order of penalty; <ref type="bibr" target="#b9">Baayen et al., 2016;</ref><ref type="bibr" target="#b116">Sóskuthy, 2017)</ref> for (i) trajectory and for (ii) value of other variables in the latent space of the Generator network. Figure <ref type="figure" target="#fig_18">16</ref> plots the normalized trajectories of the ratio and predicted values based on the generalized additive model. All smooths (except for z 74 ) are significantly different from 0 (all coefficients in Table <ref type="table">S3</ref>, Supplementary Materials) and the plots show a clear negative trajectory. In other words, maximum intensity of [s] is increasingly attenuated compared to the intensity of the vowel as z approaches the opposite value from the one identified as predicting the presence of [s] until it completely disappears from the output.</p><p>The seven variables thus strongly correspond to the presence or absence of [s] in the output; by manipulating the chosen variables to the identified values we can attenuate frication noise of [s] and cause its presence or complete disappearance in the generated data. Again, the discovery of these features is possible because we extend the initial training interval and test predictions on marginal values. In Section 4.5, we analyze further phonetic correlates of each of the seven variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Phonetic Values of Latent Variables</head><p>Interpolation of latent variables reveals that the presence of [s] is not controlled by a single latent variable, but by at least seven of them. Additionally, there appears to be no categorical cut-off point in the magnitude of the effect between the variables, only a steep drop of regression estimates (Figure <ref type="figure" target="#fig_11">10</ref>) and a decline of outputs with <ref type="bibr">[s]</ref> in generated data (Figure <ref type="figure" target="#fig_14">13</ref>). This suggests that the learning at this stage is gradient and probabilistic rather than fully categorical.</p><p>The different latent variables that correspond to the presence of [s], however, are not phonetically vacuous: individually, they have distinct phonetic correspondences. The generated samples reveal that the variables' secondary effect (besides outputting [s] and controlling its intensity) are likely spectral properties of the frication noise. The seven variables are thus similar in the sense that manipulation of their values results in the presence of [s] by controlling its frication noise. They crucially differ, however, in the effects on the spectral properties of the outputs.</p><p>To test this prediction, spectral properties of the output fricatives are analyzed. The same 29 sets of generated samples are used in the analysis; one z-value is manipulated in each set while other variables are sampled randomly and held constant. The marginal values of the variables were chosen for this test: the values with the strongest presence of [s] (which in most cases is ±4.5; henceforth STRONG) and the value before which [s] ceases from the output (henceforth WEAK). Center of gravity (COG), kurtosis, and skew of the frication noise were analyzed with and extracted with a script from <ref type="bibr" target="#b109">Rentz (2017)</ref> in Praat <ref type="bibr" target="#b15">(Boersma and Weenink, 2015)</ref>. Period of frication is sliced into 10% intervals. The data includes 161 trajectories (from the 29 generated sets) and 161 × 10 = 1, 610 unique data points. COG, kurtosis, and skew based on power spectra are measured in each of these 1,610 intervals with 750-8,000 Hz Hann band pass filter (100 Hz smoothing). Results were fit to six generalized additive mixed models with COG, kurtosis, and skew as the dependent variables (3 for each of the levels STRONG and WEAK). The parametric terms included the seven latent variables z. The smoothing terms included smooths for latent variable z 11 and difference smooths for the other six variables z. The model also includes random smooths for each fricative (from 10 to 100% with 10 knots) and for each of the 29 generated sets with equal random values of other 99 z-variables (with 7 knots; random smooths are fitted with first order of penalty, see Baayen et al., 2016; Sóskuthy,  <ref type="table">S3</ref>, Supplementary Materials) across the several variables with normalized values. 2017). The models were fit with correction for autocorrelation with ρ-values ranging from 0.15 to 0.7.</p><p>Spectral properties of the generated fricatives are generally not significantly different at the value of z right before [s] disappears from the outputs (WEAK; left column in Figure <ref type="figure" target="#fig_19">17</ref>). As values of z increase toward the marginal levels (in most cases, ±4.5), however, clear differentiation in spectral properties emerge between some of the seven z-variables (STRONG; right column in Figure <ref type="figure" target="#fig_19">17</ref>). The trajectory for center of gravity, for example, significantly differs between z 11 and most of the other six variables. Overall kurtosis is significantly different when z 11 is manipulated, compared to, for example, z 26 and z 29 . Similarly, while z 74 does not significantly attenuate amplitude of [s], it significantly differs in skew trajectory of <ref type="bibr">[s]</ref>. The main function of z 74 is thus likely in its control of spectral properties of frication of [s] (e.g., skew). For all coefficients and significant and non-significant relationship of the six models, see Tables <ref type="table">S4-S9</ref>, Supplementary Materials.</p><p>In sum, manipulating the latent variables that correspond to [s] in the output not only attenuates frication noise (when vocalic amplitude is controlled for) and causes [s] to surface or disappear from the output, but the different z-variables likely correspond to different phonetic features of the frication noise. At the level before the frication noise ceases from the output, there are no differences in spectral moments between the latent variables. By setting the values to the marginal levels well beyond the training interval, however, significant differences emerge both in overall levels as well as in trajectories of COG, kurtosis, and skew. It is thus likely that the variables collectively control the presence or absence of [s], but that individually, they control various phonetic features -spectral properties of the frication noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">DISCUSSION</head><p>The Generator network trained after 12,255 steps learns to generate outputs that closely resemble human speech in the training data. The results of the experiment in Section 4.2 suggest that the generated outputs from the Generator network replicate the conditional distribution of VOT duration in the training data. The Generator network thus not only learns to output signal that resembles human speech from noise (input variables sampled from a uniform distribution), but also learns to output shorter VOT durations when [s] is present in the signal. While this distribution is phonologically local, it is non-local in phonetic terms as a period of closure necessarily intervenes between [s] and VOT. It is likely, however, that minor local dependencies (such as burst or vowel duration) also contribute to this distribution. While it is currently not possible to disambiguate between the local and non-local effects, it is desirable for a model to capture all possible dependencies, as speech production and perception often employ several cues as well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Parallels in Human Behavior</head><p>Several similarities emerge between the training of the Generative Adversarial networks and L1 acquisition. The training data in the GAN model is of course not completely naturalistic (even though the inputs are raw audio recordings of speech data): the network is trained on only a subset of sound sequences that a language learning infant is exposed to. The purpose of these comparisons is not to suggest the GAN model learns the data in exactly the same manner as human infants, but to suggest that clear similarities exist in behavior between the proposed model and human behavior in speech acquisition. Such comparisons have both the potential to inform computational models of human cognition and conversely, shed light on the question of how neural networks learn the data.</p><p>While the generated outputs contain evidence that the network learns the conditional distribution of VOT duration, a proportion of outputs violates this distribution. In fact, in approximately 12.2% of the #sTV sequences, the Generator outputs VOT durations that are longer than any VOT duration in the #sTV condition in the training data. This suggests that the model learns the conditional distribution, but that the learning is imperfect and the Generator occasionally violates the distribution. Crucially, these outputs that violate the training data closely resemble human behavior in L1 acquisition. Infants acquiring VOT in English undergo a period in which they produce VOT durations substantially longer compared to the adult input, not only categorically in all stops <ref type="bibr" target="#b81">(Macken and Barton, 1980;</ref><ref type="bibr" target="#b20">Catts and Jensen, 1983;</ref><ref type="bibr" target="#b80">Lowenstein and Nittrouer, 2008)</ref>, but also in the position after the sibilant [s]. <ref type="bibr" target="#b86">McLeod et al. (1996)</ref> studied acquisition of #sTV and #TV sequences in 2;0 to 2;11 year old children. Unlike the Generator network, children often simplify the initial clusters from #sTV to a single stop #TV. What is parallel to the outputs of the Generator, however, is that the VOT duration of the simplified stop is overall significantly shorter in underlying #sTV sequences, but there exists a substantial period of variation and occasionally the language-acquiring children output long-lag VOT durations there <ref type="bibr" target="#b86">(McLeod et al., 1996,</ref> for similar results in language-delayed children, see <ref type="bibr" target="#b16">Bond, 1981)</ref>. <ref type="bibr" target="#b17">Bond and Wilson (1980)</ref> present a similar study, but include older children that do not simplify the #sT cluster. This group behaves exactly parallel to the Generator's network: the overall duration of VOT in the #sTV sequences is shorter compared to the #TV sequences, but the longest duration of any VOT is attested once in the #sTV, not in the #TV condition <ref type="bibr" target="#b17">(Bond and Wilson, 1980)</ref>. The children thus learn both to articulate the full #sT cluster and to output a shorter VOT durations in the cluster condition. Occasionally, however, they output a long-lag VOT in the #sTV condition that violates the allophonic distribution and is longer than any VOT in the #TV condition.</p><p>Further parallels exist between the Generator's behavior and L2 acquisition and speech errors. Studies on L2 acquisition of VOT durations in #sTV and #TV sequences suggest that learners start with a smaller distinction between the two groups and acquire the non-aspiration rule after [s] only with more exposure <ref type="bibr" target="#b53">(Haraguchi, 2003)</ref>. A smaller initial difference between the two conditions in L2 acquisition, for example, increases from Japanese learners of English with little exposure when compared with learners with more exposure <ref type="bibr" target="#b53">(Haraguchi, 2003)</ref>. Saudi Arabic L2 learners of English produce substantially longer VOT durations in #sTV sequences compared to the native inputs <ref type="bibr" target="#b2">(Alanazi, 2018)</ref>, which resembles imperfect learning in the Generator's network. Speech errors also provide a parallel to the described behavior of the Generator network. German has a similar process of aspiration distribution as English. In an experiment of elicited speech errors, German speakers produced aspirated stops with longer VOT durations in erroneous sequences with inserted sibilant in 34% of cases <ref type="bibr" target="#b102">(Pouplier et al., 2014)</ref>. This suggests that the allophonic rule fails to apply in the speech errors, which is parallel to the Generator network outputting a long VOT in the #sTV condition that violate the training data distributions.</p><p>Finally, the Generator network violating the VOT distribution resembles the behavior of patients with speech impairments. <ref type="bibr" target="#b18">Buchwald and Miozzo (2012)</ref> analyzed VOT durations of two patients with apraxia of speech that present cluster production errors, i.e., clusters of the structure #sTV are simplified to #TV. One patient outputs long VOT durations in the #sTV condition (after the cluster is simplified). VOT durations in the #sTV clusters in this patient correspond to VOT durations of singleton stops (#TV). The other patient also simplifies the cluster, but outputs shorter VOT durations in the #sTV condition, maintaining the underlying distribution. It is hypothesized that the first patient (with long VOT durations in the #sTV condition) shows signs of impairment that operates on the phonological level: because phonological computation is impaired, the patient fails to output shorter VOT durations in the #sTV condition. In other words, there are no motor planning mechanisms that would prevent the patient from producing shorter VOT durations in the #sTV condition, which is why the error is assumed to operate on the phonological level -a phonological rule fails to apply, which results in long VOT in the #sTV condition. The second patient, on the other hand, is hypothesized to shows traces of phonetic execution impairment, while the phonological computation (short VOT in the #sTV condition) is intact. The outputs of the Generator network that violate the training data are parallel to the behavior of the patient with assumed phonological impairment: in 12.2% of cases, the network outputs long VOT duration in the #sTV condition that is longer than any VOT duration in the same condition in the training data. Since the network lacks any articulatory component (see also discussion below), motor planning factors cannot explain the Generator's violations of the distributions in the training data.</p><p>As indicated by examples in Figures <ref type="figure" target="#fig_8">8,</ref><ref type="figure" target="#fig_9">9</ref>, the network also generates segmentally innovative outputs for which no evidence was available in the training data. A subset of the innovative outputs, such as #sV and #TTV sequences, are consistent with linguistic behavior in humans. The Generator's innovative outputs thus closely resemble one of the main properties of human phonology: productivity. Human subjects are able to evaluate and produce nonce-words even if a string of phonemes violates language-specific phonotactics, as long as the basic universal phonotactic requirements that treat phones as atomic units are satisfied (for an overview of phonotactic judgments, see <ref type="bibr">Ernestus, 2011 and literature therein)</ref>. Deleting or inserting segments are also common patterns in both L1 acquisition <ref type="bibr" target="#b82">(Macken and Ferguson, 1981)</ref>, loanword phonology <ref type="bibr" target="#b129">(Yildiz, 2005)</ref>, in children with speech disorders <ref type="bibr" target="#b21">(Catts and Kamhi, 1984;</ref><ref type="bibr" target="#b10">Barlow, 2001)</ref>, as well as in speech errors <ref type="bibr" target="#b4">(Alderete and Tupper, 2018b)</ref>. For example, #sT clusters are often simplified in L1 acquisition <ref type="bibr" target="#b44">(Gerlach, 2010)</ref>. While the most common outcome is deletion of [s] (which results in the #TV sequence), deletion of the stop is robustly attested as well in L1 acquisition (resulting in #sV), both in the general population and in children with speech disorders <ref type="bibr" target="#b21">(Catts and Kamhi, 1984;</ref><ref type="bibr" target="#b92">Ohala, 1999;</ref><ref type="bibr" target="#b44">Gerlach, 2010;</ref><ref type="bibr" target="#b117">Syrika et al., 2011)</ref>. While this deletion likely involves articulatory factors that are lacking in our model, the fact that segmental units can be deleted from the output and recombined in L1 acquisition resembles the deletion in the Generator's innovative outputs, such as the #sV sequence.</p><p>These innovative outputs of the Generator's network have potential for contributing to our understanding of the evolution of phonology in language evolution in general (for an overview of the field, see <ref type="bibr" target="#b45">Gibson et al., 2012)</ref>. The main process that any model of the evolution of phonology needs to explain is the change from "holistic" acoustic signals in the proto-language to the "combinatorial" principle that operates with discrete unitsphonemes and their combinations <ref type="bibr" target="#b93">(Oudeyer, 2001</ref><ref type="bibr" target="#b94">(Oudeyer, , 2002</ref><ref type="bibr" target="#b95">(Oudeyer, , 2005</ref><ref type="bibr" target="#b96">(Oudeyer, , 2006;;</ref><ref type="bibr" target="#b131">Zuidema and de Boer, 2009)</ref>. The Generator network shows traces of this behavior: in addition to learning to reproduce the input, it learns to recombine segments into novel and unobserved sequences. The exact details of modeling phonological evolution with Generative Adversarial architecture is, however, beyond the scope of the present paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Latent Variables as Correlates of Features</head><p>In Section 4.3, we propose a technique for recovering internal representations of the Generator network. The first crucial observation is that the dependencies learned in the latent space limited by some interval extend beyond that interval. This allows for an in-depth analysis of phonetic effects of each latent variable in the generated data. Regression models identify those variables in the latent space that strongly correlate with the presence of [s] in the output. Manipulating values of the identified latent variables, both within the training interval and outside of it, results in significantly higher rates of [s] in the output. By interpolating values of individual latent variables outside of the training interval, we explore the exact phonetic correlates of each latent variable. The results suggest that the Generator network learns to use latent variables to encode imperfect equivalents of phonetic features. Since the features not only correspond to phonetic properties, but to the categorical presence or absence of [s] in the output, the network also uses latent space to encode what would be an approximate equivalent of phonological representations in the broadest sense-absence or presence of a segment.</p><p>While the presence of [s] in the output is controlled by multiple latent variables, each of the variables likely has an underlying phonetic function. While there are no significant differences in phonetic correlates of z-variables when their value is at the last point before [s] ceases from the output, a clear differentiation emerges when the values are set to the marginal level (Figure <ref type="figure" target="#fig_19">17</ref>). The seven variables thus likely have a phonetic function: controlling various spectral properties of the frication noise.</p><p>Features have long been in the center of phonetic and phonological literature <ref type="bibr" target="#b119">(Trubetzkoy, 1939;</ref><ref type="bibr" target="#b23">Chomsky and Halle, 1968;</ref><ref type="bibr" target="#b24">Clements, 1985;</ref><ref type="bibr" target="#b32">Dresher, 2015;</ref><ref type="bibr" target="#b113">Shain and Elsner, 2019)</ref>. Extracting features based on unsupervised learning of presegmented phones with neural networks has recently seen success in the autoencoder architecture <ref type="bibr" target="#b107">(Räsänen et al., 2016;</ref><ref type="bibr" target="#b34">Eloff et al., 2019;</ref><ref type="bibr" target="#b113">Shain and Elsner, 2019)</ref>. <ref type="bibr" target="#b113">Shain and Elsner (2019)</ref> train an autoencoder with binary stochastic neurons on presegmented speech data and argue that bits in the code of the autoencoder network imperfectly correspond to phonological features as posited by phonological theory. As was argued in Section 4.3, our model shows traces of imperfect selforganizing of phonetic features (e.g., spectral moments) and phonological representations (e.g., the presence of [s]) in the latent space, while learning allophonic distributions at the same time. Considerable differences between the theoretically assumed features and our results, of course, remain. Latent space encoding in our model resembles entire phonological feature matrices (such as the full presence of [s] in the output) and phonetic features (such as COG or kurtosis), but the relationships are gradient and not categorical. The current model also does not test whether higher order grouping of phonemes in accordance with actual phonological features such as [±sonorant] emerge in the training. This task is left for future work. Despite these differences, the fact that we can actively control the presence of [s] and its spectral properties in the generated data with a subset of latent variables suggest that the network learns to encode information in its latent space that resembles phonetic and phonological representations.</p><p>On a very speculative level, the latent space of the Generator's network might have a conceptual correlation in featural representation of speech production in human brain, where featural representations are also gradient and involve multiple correlates. <ref type="bibr" target="#b11">Bashivan et al. (2019)</ref> argue for the existence of direct correlations between the neural network architecture and vision in human brain. Similarly, <ref type="bibr" target="#b51">Guenther and Vladusich (2012)</ref>, <ref type="bibr" target="#b50">Guenther (2016), and</ref><ref type="bibr" target="#b95">Oudeyer (2005)</ref> propose models of simple neural maps that might have direct equivalents in neural computation of speech planning with some actual clinical applications that result from such models<ref type="foot" target="#foot_11">foot_11</ref> . Recently, highdensity direct cortical surface (electrocorticographic) recordings of the superior temporal gyrus during open brain surgery in <ref type="bibr" target="#b87">Mesgarani et al. (2014)</ref> suggest that recorded brain activity has direct correlates in encoding of phonetic features. Encoding for phonetic and phonological features in the latent space of the Generator's network can speculatively be compared to such brain recordings that serve as the basis for articulatory execution. The correspondences between the brain activity and phonetic and phonological features are multiple and gradual, not categorical, which bear resemblances to our model. To be sure, this comparison can only be indirect and speculative at this point.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Future Directions</head><p>Among the objections against modeling phonological learning with Generative Adversarial Networks might be that the model is too powerful and that it overgenerates. First, it has been shown in numerous examples that phonology, while being computationally limited <ref type="bibr" target="#b58">(Heinz, 2011;</ref><ref type="bibr" target="#b8">Avcu et al., 2017)</ref>, is more powerful than the attested phonological typology. Subjects in the artificial grammar learning paradigm are, for example, able to successfully learn alternations that never surface in natural languages <ref type="bibr" target="#b46">(Glewwe, 2017;</ref><ref type="bibr" target="#b47">Glewwe et al., 2017;</ref><ref type="bibr" target="#b7">Avcu, 2018;</ref><ref type="bibr">Beguš, 2018a,b)</ref>. Second, overgeneration is a less severe violation than undergeneration. Absence of unnattested patterns that are derivable within a theory can be explained with external factors, such as historical developments or articulatory limitations. Not generating attested patterns, however, is a more serious shortcoming: a model of phonology should at minimum derive the observed phonological processes. Finally, the main reason the proposed model overgenerates is because the current proposal involves no information about the articulatory mechanism in speech production. In other words, the GAN model is completely unconstrained for articulatory mechanisms.</p><p>This would be problematic if the goal of the current model were a network that models phonetic and phonological learning both on the articulatory and the cognitive levels. The aims of the current proposal, however, are more restricted. The network models learning without any articulatory information. Lack of articulatory information in the model (and consequently, the overgeneration problem) might in fact be an advantage for computational models of the cognitive basis of speech production and perception. It is likely that speech acquisition involves various different types of learning. Learning of motor-planning on the articulatory level is likely different from learning of articulatory targets based on perception, which is in turn likely controlled by other systems than learning of abstract symbol manipulation on the phonological level, even though these levels are interconnected in acquisition. Among the evidence that exemplifies the different levels of representation are aphasia patients with different production errors <ref type="bibr" target="#b18">(Buchwald and Miozzo, 2012)</ref>. If impairment targets the motor-planning unit, the phonological level is intact and the production error causes only deletion of [s] in #sTV target clusters with the stop being unaspirated, as predicted by phonology. If, on the other hand, phonological computation is impaired, the stop surfaces as aspirated, similar to the outputs of our GAN model. By excluding articulatory information, we model phonetic and phonological learning as if they were unconstrained by articulators and therefore only influenced by the neural network architecture. In other words, we model phonological computation on a cognitive level as if no articulatory constraints were present in human speech. This is highly desired for the task of distinguishing those aspects of phonology that are influenced by cognitive factors from those that are influenced by articulation, motor planning, or historical developments <ref type="bibr" target="#b12">(Beguš, 2018a)</ref>.</p><p>While the proposal in this paper does not directly address the discussion between generative and exemplar-based approaches to phonology, the GAN models have the potential to offer some insights into this discussion as well. The results of the computational experiments suggest that the network learns to output data consistent with the training data without grammarspecific assumptions, which would support the exemplar-based approaches to phonology. On the other hand, the Generator network does seem to compress phonological information in its latent space in a way that does not directly correspond to stored exemplars. Further explorations of the latent space should shed light on this long-standing discussion.</p><p>Several further explorations and improvements of the model are warranted. The acoustic speech data fed to the network is modeled as waveform data points, i.e., pressure points in a time continuum (as proposed for WaveGAN in <ref type="bibr" target="#b31">Donahue et al., 2019)</ref>. This has considerable advantages for exploring the properties of the network, because spectral analysis introduces significant losses in the signal. A GAN trained on spectral transformations would likely be closer to reality, as human auditory mechanisms resemble spectral information more closely than raw pressure points <ref type="bibr" target="#b130">(Young, 2008;</ref><ref type="bibr" target="#b97">Pasley et al., 2012;</ref><ref type="bibr" target="#b87">Mesgarani et al., 2014)</ref>. Adding an articulatory model would likewise yield novel information on the role of articulatory learning on phonetic and phonological computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">CONCLUSION</head><p>The results of this paper suggest that we can model phonology not only with rules (as in rule-based approaches; <ref type="bibr" target="#b23">Chomsky and Halle, 1968)</ref>, exemplars <ref type="bibr" target="#b100">(Pierrehumbert, 2001)</ref>, finite-state automata <ref type="bibr" target="#b57">(Heinz, 2010;</ref><ref type="bibr" target="#b22">Chandlee, 2014)</ref>, input-output optimization (as in Optimality Theory; <ref type="bibr" target="#b104">Prince and Smolensky, 2004)</ref>, or with neural network architecture that already assumes some level of abstraction (see Section 1), but as a mapping between random latent variables and output data in deep neural networks that are trained in an unsupervised manner from raw acoustic data. To the author's knowledge, this is the first paper testing learning of allophonic distributions in an unsupervised manner from raw acoustic data using neural networks and the first proposal to use GANs for modeling language acquisition. The Generative Adversarial model of phonology (trained on an implementation of DCGAN architecture for audio data in <ref type="bibr" target="#b31">Donahue et al., 2019)</ref> derives outputs that resemble speech from latent variables. The results of the computational experiment suggest that the network learns the conditional allophonic distribution of VOT duration. We propose a technique that identifies variables in the latent space that correspond to phonetic and phonological properties in the output, such as the presence of [s], and show that by manipulating these values, we can generate data with or without <ref type="bibr">[s]</ref> in the output as well as control its intensity and spectral properties of its frication noise. While at least seven latent variables control the presence of [s], each of them likely has a phonetic function that controls spectral properties of the frication noise. The proposed technique thus suggests that the Generator network learns to encode phonetic and phonological information in its latent space. Finally, the model generates innovative outputs, suggesting its productive nature. The behavior of the model is compared against speech acquisition, speech errors, and speech impairment; several parallels are identified.</p><p>The current proposal models one allophonic distribution in English. Training GAN networks on further processes and on languages other than English as well as probing the networks at different training steps should yield more information about learning representations of different features, phonetic and phonological processes, and about computational models of the cognitive aspects of human speech production and perception in general. This paper outlines a methodology for establishing internal representations and testing predictions against generated data, but represents just a first step in a broader task of modeling phonetic and phonological learning in a Generative Adversarial framework.</p><p>The proposed model also has implications beyond modeling the cognitive basis of human speech. The results of establishing internal representations of the Generator network have implications for more applicable tasks in natural language processing. Identifying latent variables that correspond to output sounds allows for a model that generates desired output strings with different output properties. Discussing the details of such models is beyond the scope of this paper.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>FIGURE 1 |</head><label>1</label><figDesc>FIGURE 1 | A diagram showing the Generative Adversarial architecture as proposed in Goodfellow et al. (2014) and Donahue et al. (2019), trained on data from the TIMIT database in this paper.</figDesc><graphic coords="5,99.14,69.19,396.96,168.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>FIGURE 2 |</head><label>2</label><figDesc>FIGURE 2 | Waveforms and spectrograms (0 -8, 000 Hz) of [p h ae] (left) and [spae] (right) illustrating typical training data with annotations from TIMIT. Only the raw audio data (in •wav format) were used in training. The annotation illustrates a substantially longer duration of VOT in word-initial stops when no [s] precedes.</figDesc><graphic coords="6,85.14,69.99,425.28,163.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Figure 2 illustrates typical training data: raw audio files with speech data, but limited to two types of sequences, #TV and #sTV.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2</head><label>2</label><figDesc>also illustrates that the duration of VOT depends on a condition that is not immediately adjacent in phonetic terms: the absence/presence of [s] is interrupted from the VOT duration by a period of closure in the training data. That VOT is significantly shorter if T is preceded by[s]  in the training data is confirmed by a Gamma regression model: β = -0.84, t = -49.69, p &lt; 0.0001 (for details, see Section 1, Supplementary Materials).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>FIGURE 3 |</head><label>3</label><figDesc>FIGURE 3 | (A) Violin plots with box-plots of durations in ms of VOT in the training data based on two conditions: when word-initial #TV sequence is not preceded by [s] (#TV) and when it is preceded by [s] (#sTV) across the three places of articulation: [p], [t], [k]. (B) Fitted values of VOT durations with 95% confidence intervals from the Gamma (with log-link) regression model in TableS1in Supplementary Materials.</figDesc><graphic coords="7,70.64,69.67,453.60,186.00" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>FIGURE 4 |</head><label>4</label><figDesc>FIGURE 4 | Distribution of training items according to vowel identity as described in TIMIT in ARPABET, where aa = A, ae = ae, ah = 2, ao = O, aw = aU, ax = @, ax-h = @ ˚, axr = Ä, ay = aI, eh = E, er = Ç, ey = eI, ih = I, ix = 1, iy = i, ow = oU, oy = oI, uh = U, uw = u, ux = 0 in the International Phonetic Alphabet.</figDesc><graphic coords="7,99.14,319.35,396.96,144.48" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>FIGURE 5 |</head><label>5</label><figDesc>FIGURE 5 | Waveforms and spectrograms (0-8,000 Hz) of typical generated samples of #TV (left) and #sTV (right) sequences from a Generator trained after 12,255 steps.</figDesc><graphic coords="8,127.13,69.23,341.28,145.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>FIGURE 7 |</head><label>7</label><figDesc>FIGURE 7 | Waveforms and spectrograms (0-8,000 Hz) of two generated outputs of #sTV sequences in which the stop has longer VOT than any VOT in #sTV condition in the training data.</figDesc><graphic coords="10,127.13,69.23,341.28,145.44" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>FIGURE 8 |</head><label>8</label><figDesc>FIGURE 8 | Waveforms and spectrograms (0-8,000 Hz) of two generated outputs of the shape #sV. The sample on the left was generated after 16,715 steps.</figDesc><graphic coords="10,127.13,269.65,341.28,152.16" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>FIGURE 9 |</head><label>9</label><figDesc>FIGURE 9 | Waveforms and spectrograms (0-8,000 Hz) of two generated outputs of the shape #TTV.</figDesc><graphic coords="10,127.13,469.81,341.28,108.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>Figure 11 plots smooths of the seven predictors (z 5 , z 11 , z 49 , z 29 , z 74 , z 26 , z 14 ) from a non-linear model SELECT. The smooths show a linear or near-linear relationship between values of the chosen seven variables and the probability of [s] in the output.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>FIGURE 10 |</head><label>10</label><figDesc>FIGURE 10 | Plot of χ 2 values (left scale) for the 100 predictors across the four generalized additive models. For the two linear models (LINEAR and LINEAR EXCLUDED), estimates of slopes in absolute values (|β|) are plotted (right scale). The blue vertical line indicates the division between the seven chosen predictors and the rest of the predictor space with a clear drop in estimates between the first seven values (z 5 , z 11 , z 49 , z 29 , z 74 , z 26 , z 14 ) and the rest of the space.</figDesc><graphic coords="12,99.14,69.55,396.96,213.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>FIGURE 11 |</head><label>11</label><figDesc>FIGURE 11 | Plots of seven smooth terms with highest χ 2 values in a generalized additive logistic regression model with all 100 latent variables (z) as predictors, estimated with penalty for each term (SELECT). Many of the predictors show linear correlation, which is why a linear logistic regression outputs similar estimates.</figDesc><graphic coords="12,59.14,349.30,477.36,229.92" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>FIGURE 12 |</head><label>12</label><figDesc>FIGURE 12 | Seven waveforms and spectrograms (0-8,000 Hz) of outputs from the Generator network with the value of z 11 set at -25. In 96 out of 100 generated samples, the network outputs a sequence containing an [s]. With such a low value of z 11 (that correlates with amplitude of frication noise), the amplitude of the frication noise reaches the maximum level of 1 in all outputs with [s].</figDesc><graphic coords="13,49.50,69.64,236.40,131.04" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>FIGURE 13 |</head><label>13</label><figDesc>FIGURE 13 | Plot of absolute values of estimates from the Linear model for 31 analyzed latent variables z (numbered on the plot) and the percent of outputs that contain an [s] based on 100 generated samples. The blue solid line represents predicted values based on the beta regression model with estimates of the Linear model as the predictor; the dashed lines represent 95% confidence intervals.</figDesc><graphic coords="14,49.50,69.92,236.40,155.76" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>FIGURE 14 |</head><label>14</label><figDesc>FIGURE 14 | Waveforms and two spectrograms (both 0 -8, 000 Hz) of generated data with z 11 variable manipulated and interpolated. The values on the left of waveforms indicate the value of z 11 . The two spectrograms represent the highest and the lowest value of z 11 . A clear attenuation of the frication noise is visible until complete disappearance.</figDesc><graphic coords="15,99.14,69.79,396.96,224.88" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head>Figure 14</head><label>14</label><figDesc>illustrates this effect. Frication noise of [s] gradually decreases by increasing the value of z 11 until it completely disappears from the output. The exact value of z 11 for which the [s] disappears differs across examples and likely interacts with other features. It is possible that frication noise in the training has a higher amplitude in some conditions, which is why such cases require a higher magnitude of manipulation of z 11 . The figure also shows that as the frication noise of [s] disappears, aspiration of a stop in what appear to be #TV sequences starts surfacing and replaces the frication noise of [s]. Occasionally, frication noise of [s] gradually transforms into aspiration noise. The exact transformation is likely dependent on the 99 other z-variables held constant and their underlying phonetic effects.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>FIGURE 15 |</head><label>15</label><figDesc>FIGURE 15 | Plots of maximum intensity (in dB) of the fricative part in #sTV sequences when values of the seven z-variables are interpolated from the marginal values ±4.5 in 0.5 increments. Each set of generated samples with the randomly sampled latent variables held constant is colored with the same color across the seven z-variables. Values of z 5 , z 14 , and z 26 are inverted for clarity purposes.</figDesc><graphic coords="16,59.14,70.12,477.36,202.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>FIGURE 16</head><label>16</label><figDesc>FIGURE 16 | (A) Plots of ratios of maximum intensity between the frication of [s] and phonation of the vowel in #sTV sequences across the seven variables. The interpolated values are normalized where 0 represents the most marginal value of z with [s] in the output and 1 represents the value of z right before [s] ceases from the output. Four marginal values are left out from the plot (but are included in the models). Each set of generated samples with the randomly sampled latent variables held constant is colored with the same color across the seven z-variables. (B) Predicted values with 95% CIs of the ratio based on beta regression generalized additive model (TableS3, Supplementary Materials) across the several variables with normalized values.</figDesc><graphic coords="17,59.14,69.87,477.36,412.80" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_19"><head>FIGURE 17 |</head><label>17</label><figDesc>FIGURE 17 | A subset of predicted values of COG, kurtosis, and skew with 95% CIs in two conditions: WEAK with z-variables at the value before [s] ceases from the output (left column) and STRONG (right column) with the most marginal value with [s]-output (±4.5 in most cases). Predicted values are based on generalized additive models in Tables S4-S9 (Supplementary Materials). The plots show a clear differentiation from no significant differences in COG, kurtosis, and skew, to clear significant overall differences and trajectory differences as the z-values move from WEAK toward the marginal (STRONG) values. Difference smooths for the presented variables are in Figure S1, Supplementary Materials.</figDesc><graphic coords="20,59.14,70.11,477.36,430.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>TABLE 1 |</head><label>1</label><figDesc>Raw VOT durations in ms for the training data with SD and Range.</figDesc><table><row><cell>Structure</cell><cell>Place</cell><cell>VOT</cell><cell>SD</cell><cell>Lowest</cell><cell>Highest</cell><cell>Count</cell></row><row><cell>#TV</cell><cell>p</cell><cell>49.6</cell><cell>18.0</cell><cell>7.3</cell><cell>115.5</cell><cell>1,018</cell></row><row><cell></cell><cell>t</cell><cell>55.2</cell><cell>20.7</cell><cell>9.8</cell><cell>130.0</cell><cell>1,799</cell></row><row><cell></cell><cell>k</cell><cell>67.5</cell><cell>19.5</cell><cell>12.5</cell><cell>153.1</cell><cell>2,112</cell></row><row><cell>#sTV</cell><cell>p</cell><cell>19.4</cell><cell>7.1</cell><cell>9.4</cell><cell>49.2</cell><cell>115</cell></row><row><cell></cell><cell>t</cell><cell>25.6</cell><cell>7.9</cell><cell>10.6</cell><cell>65.0</cell><cell>288</cell></row><row><cell></cell><cell>k</cell><cell>30.1</cell><cell>8.6</cell><cell>14.4</cell><cell>55.0</cell><cell>130</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>TABLE 2 |</head><label>2</label><figDesc>AIC values of five fitted models with corresponding degrees of freedom (df), fitted with Maximum Likelihood. Select is not listed because it was not fitted with ML; AIC of Select fitted with REML is, however, similar to Excluded (=1,008.46 vs. 1008.54).</figDesc><table><row><cell>df</cell><cell>AIC</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>T represents voiceless stops /p, t, k/, V represents vowels (see Figure4), and # represents a word boundary.Frontiers in Artificial Intelligence | www.frontiersin.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_1"><p>July 2020 | Volume 3 | Article 44</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" xml:id="foot_2"><p>Frontiers in Artificial Intelligence | www.frontiersin.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_3"><p><ref type="bibr" target="#b31">Donahue et al. (2019)</ref> trained the model on the SC09 and TIMIT databases, but the results are not useful for modeling phonological learning, because the model is trained on a continuous speech stream and the generated sample fails to produce analyzable results for phonological purposes.Frontiers in Artificial Intelligence | www.frontiersin.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_4"><p>The TIMIT annotations would yield a ratio of 1.17, but the token was annotated by the author and the ratio appears much smaller. In any case, even with TIMIT's annotation, the ratio with value of 1.91 in the generated data is still substantially higher than the 1.17.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_5"><p>For evidence that units smaller than segments are phonologically relevant, see<ref type="bibr" target="#b59">Inkelas and Shih (2017)</ref> and literature therein.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_6"><p>In general, GANs do not overfit<ref type="bibr" target="#b1">(Adlam et al., 2019;</ref><ref type="bibr" target="#b31">Donahue et al., 2019)</ref>, as is suggested by our data. Even if overfitting did occur, it would result from training a Generator without a direct access to the training data (unlike in the autoencoder models, where the input training data and outputs are directly connected).</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6" xml:id="foot_7"><p><ref type="bibr" target="#b31">Donahue et al. (2019)</ref> test overfitting on models trained with a substantially higher number of steps (200,000) compared to our model (12,255) and presents evidence that GAN models trained on audio data do not overfit even with substantially higher number of training steps.Frontiers in Artificial Intelligence | www.frontiersin.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_8"><p>It is possible that some outputs were mislabeled, but the probability is low and the magnitude of mislabeled data would be minimal enough not to influence the results. The author manually inspected spectrograms of all generated data.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="8" xml:id="foot_9"><p>It would be possible to estimate smooth terms for only a subset of predictors, but such a model is unlikely to yield different results.Frontiers in Artificial Intelligence | www.frontiersin.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="9" xml:id="foot_10"><p>The script used for this task was provided by<ref type="bibr" target="#b75">Lennes (2003)</ref>.Frontiers in Artificial Intelligence | www.frontiersin.org</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="10" xml:id="foot_11"><p><ref type="bibr" target="#b122">Warlaumont and Finnegan (2016)</ref> propose a model of infant babbling that involves spiking neural networks and speech synthesis. While the model does not take any speech as an input, babbling emerges even if the objective for the simulation is maximization of perceptual salience.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>I would like to thank <rs type="person">Sameer Arshad</rs> for slicing data from the TIMIT database and <rs type="person">Heather Morrison</rs> for annotating data. Parts of this research were published in <ref type="bibr" target="#b14">Beguš (2020)</ref>.</p></div>
			</div>
			<div type="funding">
<div><head>FUNDING</head><p>This research was funded by a grant to new faculty at the <rs type="institution">University of Washington</rs>. Publication made possible in part by support from the <rs type="funder">Berkeley Research Impact Initative (BRII)</rs> sponsored by the <rs type="funder">UC Berkeley Library</rs>.</p></div>
			</div>
			<listOrg type="funding">
			</listOrg>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>DATA AVAILABILITY STATEMENT</head><p>The datasets generated for this study are available on request to the corresponding author.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>AUTHOR CONTRIBUTIONS</head><p>The author confirms being the sole contributor of this work and has approved it for publication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SUPPLEMENTARY MATERIAL</head><p>The Supplementary Material for this article can be found online at: <ref type="url" target="https://www.frontiersin.org/articles/10.3389/frai.2020.00044/full#supplementary-material">https://www.frontiersin.org/articles/10.3389/frai.2020. 00044/full#supplementary-material</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conflict of Interest:</head><p>The author declares that the research was conducted in the absence of any commercial or financial relationships that could be construed as a potential conflict of interest.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Voice onset time (VOT) at 50: theoretical and practical issues in measuring voicing distinctions</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Abramson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Whalen</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.wocn.2017.05.002</idno>
	</analytic>
	<monogr>
		<title level="j">J. Phonet</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="page" from="75" to="86" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Investigating under and overfitting in wasserstein generative adversarial networks</title>
		<author>
			<persName><forename type="first">B</forename><surname>Adlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Weill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Kapoor</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1910.14137</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>Preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">S</forename><surname>Alanazi</surname></persName>
		</author>
		<title level="m">The acquisition of English stops by Saudi L2 learners</title>
		<meeting><address><addrLine>Essex, United Kingdom</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
		<respStmt>
			<orgName>University of Essex</orgName>
		</respStmt>
	</monogr>
	<note>Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Connectionist approaches to generative phonology</title>
		<author>
			<persName><forename type="first">J</forename><surname>Alderete</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tupper</surname></persName>
		</author>
		<idno type="DOI">10.4324/9781315675428-13</idno>
	</analytic>
	<monogr>
		<title level="m">The Routledge Handbook of Phonological Theory</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Bosch</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Hannahs</surname></persName>
		</editor>
		<meeting><address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>Routledge</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="360" to="390" />
		</imprint>
	</monogr>
	<note>a)</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Phonological regularity, perceptual biases, and the role of phonotactics in speech error analysis</title>
		<author>
			<persName><forename type="first">J</forename><surname>Alderete</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tupper</surname></persName>
		</author>
		<idno type="DOI">10.1002/wcs.1466</idno>
	</analytic>
	<monogr>
		<title level="j">Rev. Cogn. Sci</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">1466</biblScope>
			<date type="published" when="2018">2018b)</date>
			<publisher>Wiley Interdiscipl</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Phonological constraint induction in a connectionist network: learning ocp-place constraints from data</title>
		<author>
			<persName><forename type="first">J</forename><surname>Alderete</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Tupper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Frisch</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.langsci.2012.10.002</idno>
	</analytic>
	<monogr>
		<title level="j">Lang. Sci</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="52" to="69" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Wasserstein generative adversarial networks</title>
		<author>
			<persName><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<editor>
			<persName><forename type="first">D</forename><surname>Precup</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</editor>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, NSW</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="214" to="223" />
		</imprint>
	</monogr>
	<note>of Proceedings of Machine Learning Research</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Experimental investigation of the subregular hypothesis</title>
		<author>
			<persName><forename type="first">E</forename><surname>Avcu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 35th West Coast Conference on Formal Linguistics</title>
		<editor>
			<persName><forename type="first">W</forename><forename type="middle">G</forename><surname>Bennett</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">L</forename><surname>Hracs</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><forename type="middle">R</forename><surname>Storoshenko</surname></persName>
		</editor>
		<meeting>the 35th West Coast Conference on Formal Linguistics<address><addrLine>Somerville, MA</addrLine></address></meeting>
		<imprint>
			<publisher>Cascadilla</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="77" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Subregular complexity and deep learning</title>
		<author>
			<persName><forename type="first">E</forename><surname>Avcu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Shibata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heinz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CLASP Papers in Computational Linguistics: Proceedings of the Conference on Logic and Machine Learning in Natural Language</title>
		<editor>
			<persName><forename type="first">S</forename><surname>Dobnik</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">S</forename><surname>Lappin</surname></persName>
		</editor>
		<meeting><address><addrLine>Gothenburg</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017. LaML 2017</date>
			<biblScope unit="page" from="20" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Autocorrelated errors in experimental data in the language sciences: some solutions offered by Generalized Additive Mixed Models</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">H</forename><surname>Baayen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Van Rij</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>De Cat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">N</forename><surname>Wood</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.02043</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note>Preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Case study</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">A</forename><surname>Barlow</surname></persName>
		</author>
		<idno type="DOI">10.1044/0161-1461(2001/022)</idno>
	</analytic>
	<monogr>
		<title level="j">Lang. Speech Hear. Serv. Sch</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="242" to="256" />
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Neural population control via deep image synthesis</title>
		<author>
			<persName><forename type="first">P</forename><surname>Bashivan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Dicarlo</surname></persName>
		</author>
		<idno type="DOI">10.1126/science.aav9436</idno>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">364</biblScope>
			<biblScope unit="page">6439</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Post-nasal devoicing and the blurring process</title>
		<author>
			<persName><forename type="first">G</forename><surname>Beguš</surname></persName>
		</author>
		<idno type="DOI">10.1017/S002222671800049X</idno>
	</analytic>
	<monogr>
		<title level="j">J. Linguist</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="689" to="753" />
			<date type="published" when="2018">2018a</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Unnatural phonology: a synchrony-diachrony interface approach</title>
		<author>
			<persName><forename type="first">G</forename><surname>Beguš</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018b)</date>
			<pubPlace>Cambridge, MA, United States</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Harvard University</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Modeling unsupervised phonetic and phonological learning in Generative Adversarial Phonology</title>
		<author>
			<persName><forename type="first">G</forename><surname>Beguš</surname></persName>
		</author>
		<idno type="DOI">10.7275/nbrf-1a27</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Society for Computation in Linguistics</title>
		<meeting>the Society for Computation in Linguistics<address><addrLine>New Orleans, LA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">15</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">PRAAT: Doing Phonetics by Computer</title>
		<author>
			<persName><forename type="first">P</forename><surname>Boersma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Weenink</surname></persName>
		</author>
		<ptr target="http://www.praat.org/" />
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
		</imprint>
	</monogr>
	<note>Computer Program]. version 5.4.06. Available online at. Retrieved February 21</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">A note concerning /s/ plus stop clusters in the speech of language-delayed children</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">S</forename><surname>Bond</surname></persName>
		</author>
		<idno type="DOI">10.1017/S0142716400000655</idno>
	</analytic>
	<monogr>
		<title level="j">Appl. Psycholinguist</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="55" to="63" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">/s/ plus stop clusters in children&apos;s speech</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">S</forename><surname>Bond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">F</forename><surname>Wilson</surname></persName>
		</author>
		<idno type="DOI">10.1159/000259988</idno>
	</analytic>
	<monogr>
		<title level="j">Phonetica</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="149" to="158" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Phonological and motor errors in individuals with acquired sound production impairment</title>
		<author>
			<persName><forename type="first">A</forename><surname>Buchwald</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Miozzo</surname></persName>
		</author>
		<idno type="DOI">10.1044/1092-4388(2012/11-0200)</idno>
	</analytic>
	<monogr>
		<title level="j">J. Speech Lang. Hear. Res</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="1573" to="S1586" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Usage-based phonology</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bybee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Functionalism and Formalism in Linguistics</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Darnell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Moravcsik</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Newmeyer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Noonan</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Wheatley</surname></persName>
		</editor>
		<meeting><address><addrLine>Amsterdam</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="211" to="242" />
		</imprint>
		<respStmt>
			<orgName>John Benjamins</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Speech timing of phonologically disordered children</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">W</forename><surname>Catts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">J</forename><surname>Jensen</surname></persName>
		</author>
		<idno type="DOI">10.1044/jshr.2604.501</idno>
	</analytic>
	<monogr>
		<title level="j">J. Speech Lang. Hear. Res</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="501" to="510" />
			<date type="published" when="1983">1983</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Simplification of /s/ + stop consonant clusters</title>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">W</forename><surname>Catts</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">G</forename><surname>Kamhi</surname></persName>
		</author>
		<idno type="DOI">10.1044/jshr.2704.556</idno>
	</analytic>
	<monogr>
		<title level="j">J. Speech Lang. Hear. Res</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="556" to="561" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Chandlee</surname></persName>
		</author>
		<title level="m">Strictly local phonological processes</title>
		<meeting><address><addrLine>Newark, DE, United States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
		<respStmt>
			<orgName>University of Delaware</orgName>
		</respStmt>
	</monogr>
	<note>Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">N</forename><surname>Chomsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Halle</surname></persName>
		</author>
		<title level="m">The Sound Pattern of English</title>
		<meeting><address><addrLine>New York, NY</addrLine></address></meeting>
		<imprint>
			<publisher>Harper &amp; Row</publisher>
			<date type="published" when="1968">1968</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The geometry of phonological features</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">N</forename><surname>Clements</surname></persName>
		</author>
		<idno type="DOI">10.1017/S0952675700000440</idno>
	</analytic>
	<monogr>
		<title level="j">Phonol. Yearbook</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="225" to="252" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">in Gradience in Grammar: Generative Perspectives</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Cohn</surname></persName>
		</author>
		<editor>G. Fanselow, C. Féry, and M. Schlesewsky</editor>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Oxford University Press</publisher>
			<biblScope unit="page" from="25" to="44" />
			<pubPlace>Oxford</pubPlace>
		</imprint>
	</monogr>
	<note>Is there gradient phonology?</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The distribution of aspirated stops and /h/ in American English and Korean: an alignment approach with typological implications</title>
		<author>
			<persName><forename type="first">S</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M.-H</forename><surname>Cho</surname></persName>
		</author>
		<idno type="DOI">10.1515/ling.2003.020</idno>
	</analytic>
	<monogr>
		<title level="j">Linguistics</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="607" to="652" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Self-organization in vowel systems</title>
		<author>
			<persName><forename type="first">B</forename><surname>De Boer</surname></persName>
		</author>
		<idno type="DOI">10.1006/jpho.2000.0125</idno>
	</analytic>
	<monogr>
		<title level="j">J. Phonet</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="441" to="465" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Transmissibility and the role of the phonological component: a theoretical synopsis of evolutionary phonology</title>
		<author>
			<persName><forename type="first">P</forename><surname>De Lacy</surname></persName>
		</author>
		<idno type="DOI">10.1515/TL.2006.012</idno>
	</analytic>
	<monogr>
		<title level="j">Theor. Linguist</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="185" to="196" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Synchronic explanation</title>
		<author>
			<persName><forename type="first">P</forename><surname>De Lacy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Kingston</surname></persName>
		</author>
		<idno type="DOI">10.1007/s11049-013-9191-y</idno>
	</analytic>
	<monogr>
		<title level="j">Nat. Lang. Linguist. Theory</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page" from="287" to="355" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Semantically decomposing the latent spaces of generative adversarial networks</title>
		<author>
			<persName><forename type="first">C</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Balsubramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.07904</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>CoRR arXiv [preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Adversarial audio synthesis</title>
		<author>
			<persName><forename type="first">C</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">J</forename><surname>Mcauley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">S</forename><surname>Puckette</surname></persName>
		</author>
		<ptr target="https://arxiv.org/abs/1802.04208" />
	</analytic>
	<monogr>
		<title level="m">7th International Conference on Learning Representations, ICLR 2019</title>
		<meeting><address><addrLine>New Orleans, LA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The motivation for contrastive feature hierarchies in phonology</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">E</forename><surname>Dresher</surname></persName>
		</author>
		<idno type="DOI">10.1075/lv.15.1.01dre</idno>
	</analytic>
	<monogr>
		<title level="j">Linguist. Variat</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1" to="40" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Cognitive science in the era of artificial intelligence: a roadmap for reverse-engineering the infant language-learner</title>
		<author>
			<persName><forename type="first">E</forename><surname>Dupoux</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cognition.2017.11.008</idno>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">173</biblScope>
			<biblScope unit="page" from="43" to="59" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Unsupervised acoustic unit discovery for speech synthesis using discrete latent-variable neural networks</title>
		<author>
			<persName><forename type="first">R</forename><surname>Eloff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nortje</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Van Niekerk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Govender</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Nortje</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Pretorius</surname></persName>
		</author>
		<idno type="DOI">10.21437/Interspeech.2019-1518</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. Interspeech</title>
		<meeting>Interspeech<address><addrLine>Graz</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
			<biblScope unit="page" from="1103" to="1107" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Gradience and categoricality in phonological theory</title>
		<author>
			<persName><forename type="first">M</forename><surname>Ernestus</surname></persName>
		</author>
		<idno type="DOI">10.1002/9781444335262.wbctp0089</idno>
	</analytic>
	<monogr>
		<title level="m">The Blackwell Companion to Phonology</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Van Oostendorp</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><forename type="middle">J</forename><surname>Ewen</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Hume</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><surname>Rice</surname></persName>
		</editor>
		<meeting><address><addrLine>Malden, MA</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley Blackwell</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Morphological inflection generation using character sequence to sequence learning</title>
		<author>
			<persName><forename type="first">M</forename><surname>Faruqui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Neubig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N16-1077</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="634" to="643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title/>
		<author>
			<persName><forename type="first">J</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Weisberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">An R Companion to Applied Regression</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>Sage</publisher>
			<pubPlace>Thousand Oaks CA</pubPlace>
		</imprint>
	</monogr>
	<note>3rd Edn</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">The early influence of phonology on a phonetic change. Language 376-410</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fruehwald</surname></persName>
		</author>
		<idno type="DOI">10.1353/lan.2016.0041</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">The role of phonology in phonetic change</title>
		<author>
			<persName><forename type="first">J</forename><surname>Fruehwald</surname></persName>
		</author>
		<idno type="DOI">10.1146/annurev-linguistics-011516-034101</idno>
	</analytic>
	<monogr>
		<title level="j">Annu. Rev. Linguist</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="25" to="42" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A generative model of phonotactics</title>
		<author>
			<persName><forename type="first">R</forename><surname>Futrell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Albright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Graff</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O'</forename><surname>Donnell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">J</forename></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00047</idno>
	</analytic>
	<monogr>
		<title level="j">Trans. Assoc. Comput. Linguist</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="73" to="86" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Introduction to the special issue on exemplar-based models in linguistics</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C L</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.1515/TLR.2006.007</idno>
	</analytic>
	<monogr>
		<title level="j">Linguist. Rev</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="213" to="216" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">S</forename><surname>Garofolo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Lamel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Fiscus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">D</forename><surname>Pallett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">N</forename><surname>Dahlgren</surname></persName>
		</author>
		<title level="m">TIMIT Acoustic-Phonetic Continuous Speech Corpus LDC93S1. Web Download. Philadelphia: Linguistic Data Consortium</title>
		<imprint>
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A connectionist model of phonological representation in speech perception</title>
		<author>
			<persName><forename type="first">M</forename><surname>Gaskell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">D</forename><surname>Marslen-Wilson</surname></persName>
		</author>
		<idno type="DOI">10.1207/s15516709cog1904_1</idno>
	</analytic>
	<monogr>
		<title level="j">Cogn. Sci</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="page" from="407" to="439" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">The acquisition of consonant feature sequences: harmony, metathesis and deletion patterns in phonological development</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">R</forename><surname>Gerlach</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010">2010</date>
			<pubPlace>Minneapolis, MN, United States</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Minnesota</orgName>
		</respStmt>
	</monogr>
	<note>Ph.D. thesis</note>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">The evolution of phonology</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Gibson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Tallerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">F</forename><surname>Macneilage</surname></persName>
		</author>
		<idno type="DOI">10.1093/oxfordhb/9780199541119.001.0001</idno>
	</analytic>
	<monogr>
		<title level="m">The Oxford Handbook of Language Evolution</title>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">R</forename><surname>Gibson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Tallerman</surname></persName>
		</editor>
		<meeting><address><addrLine>Oxford</addrLine></address></meeting>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Substantive bias in phonotactic learning: Positional extension of an obstruent voicing contrast</title>
		<author>
			<persName><forename type="first">E</forename><surname>Glewwe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>Chicago Linguistic Society</publisher>
			<pubPlace>Chicago, IL</pubPlace>
		</imprint>
	</monogr>
	<note>Talk presented at the 53rd meeting of</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Substantive bias and word-final voiced obstruents: an artificial grammar learning study</title>
		<author>
			<persName><forename type="first">E</forename><surname>Glewwe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Zymet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Jacobson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Yates</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Talk presented at the 92nd Annual Meeting of the Linguistic Society of America</title>
		<meeting><address><addrLine>Salt Lake City, UT</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning OT constraint rankings using a maximum entropy model</title>
		<author>
			<persName><forename type="first">S</forename><surname>Goldwater</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Variation within Optimality Theory</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Spenader</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Eriksson</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">O</forename><surname>Dahl</surname></persName>
		</editor>
		<meeting>the Workshop on Variation within Optimality Theory<address><addrLine>Stockholm</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="111" to="120" />
		</imprint>
		<respStmt>
			<orgName>Stockholm University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Cortes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</editor>
		<meeting><address><addrLine>Red Hook, NY</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">H</forename><surname>Guenther</surname></persName>
		</author>
		<idno type="DOI">10.7551/mitpress/10471.001.0001</idno>
		<title level="m">Neural Control of Speech</title>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">A neural theory of speech acquisition and production</title>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">H</forename><surname>Guenther</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Vladusich</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jneuroling.2009.08.006</idno>
	</analytic>
	<monogr>
		<title level="j">J. Neurolinguist</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="408" to="422" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Improved training of wasserstein gans</title>
		<author>
			<persName><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><forename type="middle">V</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Vishwanathan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 30</title>
		<meeting><address><addrLine>Red Hook, NY</addrLine></address></meeting>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5767" to="5777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">The acquisition of aspiration of voiceless stops and intonation patterns of English learners: pilot study</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Haraguchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the 8th Conference of Pan-Pacific Association of Applied Linguistics</title>
		<meeting>eeding of the 8th Conference of Pan-Pacific Association of Applied Linguistics<address><addrLine>Okayama</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="83" to="91" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Phonetically-driven phonology: the role of optimality theory and inductive grounding</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hayes</surname></persName>
		</author>
		<idno type="DOI">10.1075/slcs.41.13hay</idno>
	</analytic>
	<monogr>
		<title level="m">Functionalism and Formalism in Linguistics</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Darnell</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Moravscik</surname></persName>
		</editor>
		<meeting><address><addrLine>Amsterdam</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="volume">I</biblScope>
			<biblScope unit="page" from="243" to="285" />
		</imprint>
		<respStmt>
			<orgName>John Benjamins</orgName>
		</respStmt>
	</monogr>
	<note>General Papers</note>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Phonological naturalness and phonotactic learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>White</surname></persName>
		</author>
		<idno type="DOI">10.1162/LING_a_00119</idno>
	</analytic>
	<monogr>
		<title level="j">Linguist. Inq</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="45" to="75" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">A maximum entropy model of phonotactics and phonotactic learning</title>
		<author>
			<persName><forename type="first">B</forename><surname>Hayes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Wilson</surname></persName>
		</author>
		<idno type="DOI">10.1162/ling.2008.39.3.379</idno>
	</analytic>
	<monogr>
		<title level="j">Linguist. Inq</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Learning long-distance phonotactics</title>
		<author>
			<persName><forename type="first">J</forename><surname>Heinz</surname></persName>
		</author>
		<idno type="DOI">10.1162/LING_a_00015</idno>
	</analytic>
	<monogr>
		<title level="j">Linguist. Inq</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="623" to="661" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Computational phonology-part II: grammars, learning, and the future</title>
		<author>
			<persName><forename type="first">J</forename><surname>Heinz</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.1749-818X.2011.00268.x</idno>
	</analytic>
	<monogr>
		<title level="j">Lang. Linguist. Compass</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="153" to="168" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Looking into segments</title>
		<author>
			<persName><forename type="first">S</forename><surname>Inkelas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Jesney</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>O'hara</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Walker</surname></persName>
		</author>
		<idno type="DOI">10.3765/amp.v4i0.3996</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Forty-Fifth Annual Meeting of the North East Linguistic Society</title>
		<meeting>the Forty-Fifth Annual Meeting of the North East Linguistic Society<address><addrLine>Washington, DC</addrLine></address></meeting>
		<imprint>
			<publisher>Linguistic Society of America</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Aspiration and laryngeal representation in Germanic</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">K</forename><surname>Iverson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Salmons</surname></persName>
		</author>
		<idno type="DOI">10.1017/S0952675700002566</idno>
	</analytic>
	<monogr>
		<title level="j">Phonology</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="369" to="396" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Computational modeling of phonological learning</title>
		<author>
			<persName><forename type="first">G</forename><surname>Jarosz</surname></persName>
		</author>
		<idno type="DOI">10.1146/annurev-linguistics-011718-011832</idno>
	</analytic>
	<monogr>
		<title level="j">Annu. Rev. Linguist</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="67" to="90" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Speech perception without speaker normalization: an exemplar model</title>
		<author>
			<persName><forename type="first">K</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Talker Variability in Speech Processing</title>
		<meeting><address><addrLine>San Diego, CA</addrLine></address></meeting>
		<imprint>
			<publisher>Academic Press</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="145" to="165" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Decisions and mechanisms in exemplar-based phonology</title>
		<author>
			<persName><forename type="first">K</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Experimental Approaches to Phonology</title>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Solé</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><forename type="middle">S</forename><surname>Beddor</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">M</forename><surname>Ohala</surname></persName>
		</editor>
		<meeting><address><addrLine>Oxford</addrLine></address></meeting>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="25" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Unsupervised neural network based feature extraction using weak top-down constraints</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kamper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elsner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Goldwater</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICASSP.2015.7179087</idno>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<meeting><address><addrLine>Brisbane, QLD</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="5818" to="5822" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Exemplar-based models in linguistics</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kaplan</surname></persName>
		</author>
		<idno type="DOI">10.1093/obo/9780199772810-0201</idno>
	</analytic>
	<monogr>
		<title level="m">Oxford Bibliographies in Linguistics</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Aronoff</surname></persName>
		</editor>
		<meeting><address><addrLine>Oxford</addrLine></address></meeting>
		<imprint>
			<publisher>Oxford University Press</publisher>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">The interplay of perception and production in phonological development: beginnings of a connectionist model trained on real speech</title>
		<author>
			<persName><forename type="first">C</forename><surname>Kello</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Plaut</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Congress of Phonetic Sciences</title>
		<editor>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Solé</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Recasens</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Romero</surname></persName>
		</editor>
		<meeting><address><addrLine>Barcelona</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003">2003</date>
			<biblScope unit="page" from="297" to="300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Enhancement and overlap in the speech chain</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">J</forename><surname>Keyser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">N</forename><surname>Stevens</surname></persName>
		</author>
		<idno type="DOI">10.1353/lan.2006.0051</idno>
	</analytic>
	<monogr>
		<title level="j">Language</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="page" from="33" to="63" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<monogr>
		<title level="m" type="main">Phonetic knowledge. Language 419-454</title>
		<author>
			<persName><forename type="first">J</forename><surname>Kingston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">L</forename><surname>Diehl</surname></persName>
		</author>
		<idno type="DOI">10.1353/lan.1994.0023</idno>
		<imprint>
			<date type="published" when="1994">1994</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<monogr>
		<author>
			<persName><forename type="first">J</forename><surname>Kirby</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sonderegger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.04420</idno>
		<title level="m">Bias and population structure in the actuation of sound change</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>arXiv [preprint</note>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Recurrent neural networks as a strong baseline for morphophonological learning</title>
		<author>
			<persName><forename type="first">C</forename><surname>Kirov</surname></persName>
		</author>
		<ptr target="https://ckirov.github.io/papers/lsa2017.pdf" />
	</analytic>
	<monogr>
		<title level="m">Poster Presented at 2017 Meeting of the Linguistic Society of America</title>
		<meeting><address><addrLine>Austin, TX</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-10-07">2017. accessed October 7, 2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Brain mechanisms in early language acquisition</title>
		<author>
			<persName><forename type="first">P</forename><forename type="middle">K</forename><surname>Kuhl</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.neuron.2010.08.038</idno>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">67</biblScope>
			<biblScope unit="page" from="713" to="727" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">A nonparametric Bayesian approach to acoustic model discovery</title>
		<author>
			<persName><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><surname>-Y</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Glass</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 50th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Jeju Island</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="40" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<monogr>
		<title level="m" type="main">Harmonic grammar: A formal multi-level connectionist theory of linguistic well-formedness: Theoretical Foundations</title>
		<author>
			<persName><forename type="first">G</forename><surname>Legendre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Miyata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Smolensky</surname></persName>
		</author>
		<idno>#90-5</idno>
		<imprint>
			<date type="published" when="1990">1990</date>
			<pubPlace>Boulder, CO</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Colorado</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">ICS Technical Report</note>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">The optimality theoryharmonic grammar connection</title>
		<author>
			<persName><forename type="first">G</forename><surname>Legendre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Sorace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Smolensky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Harmonic Mind: From Neural Computation to</title>
		<editor>
			<persName><forename type="first">P</forename><surname>Smolensky</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Legendre</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2006">2006</date>
			<biblScope unit="page" from="339" to="402" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<monogr>
		<title level="m" type="main">f0-f1-f2-Intensity_PRAAT_Script. PRAAT script</title>
		<author>
			<persName><forename type="first">M</forename><surname>Lennes</surname></persName>
		</author>
		<editor>Dan McCloy, Esther Le Grésauze, and Gašper Beguš</editor>
		<imprint>
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b76">
	<monogr>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">P</forename><surname>Kording</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1907.06374</idno>
		<title level="m">What does it mean to understand a neural network? arXiv</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>preprint</note>
</biblStruct>

<biblStruct xml:id="b77">
	<monogr>
		<title level="m" type="main">Precise recovery of latent vectors from generative adversarial networks</title>
		<author>
			<persName><forename type="first">Z</forename><forename type="middle">C</forename><surname>Lipton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Tripathi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.04782</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>CoRR arXiv [preprint</note>
</biblStruct>

<biblStruct xml:id="b78">
	<analytic>
		<title level="a" type="main">How is the aspiration of English /p, t, k</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lisker</surname></persName>
		</author>
		<idno type="DOI">10.1177/002383098402700409</idno>
	</analytic>
	<monogr>
		<title level="j">Lang. Speech</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="391" to="394" />
			<date type="published" when="1984">1984</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b79">
	<monogr>
		<title level="m" type="main">A cross-language study of voicing in initial stops: acoustical measurements</title>
		<author>
			<persName><forename type="first">L</forename><surname>Lisker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Abramson</surname></persName>
		</author>
		<idno type="DOI">10.1080/00437956.1964.11659830</idno>
		<imprint>
			<date type="published" when="1964">1964</date>
			<biblScope unit="page" from="384" to="422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b80">
	<analytic>
		<title level="a" type="main">Patterns of acquisition of native voice onset time in English-learning children</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Lowenstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nittrouer</surname></persName>
		</author>
		<idno type="DOI">10.1121/1.2945118</idno>
	</analytic>
	<monogr>
		<title level="j">J. Acous. Soc. Am</title>
		<imprint>
			<biblScope unit="volume">124</biblScope>
			<biblScope unit="page" from="1180" to="1191" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b81">
	<analytic>
		<title level="a" type="main">The acquisition of the voicing contrast in English: a study of voice onset time in word-initial stop consonants</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Macken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Barton</surname></persName>
		</author>
		<idno type="DOI">10.1017/S0305000900007029</idno>
	</analytic>
	<monogr>
		<title level="j">J. Child Lang</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="41" to="74" />
			<date type="published" when="1980">1980</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b82">
	<analytic>
		<title level="a" type="main">Phonological universals in language acquisition*</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Macken</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Ferguson</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.1749-6632.1981.tb42002.x</idno>
	</analytic>
	<monogr>
		<title level="j">Ann. N. Y. Acad. Sci</title>
		<imprint>
			<biblScope unit="volume">379</biblScope>
			<biblScope unit="page" from="110" to="129" />
			<date type="published" when="1981">1981</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b83">
	<analytic>
		<title level="a" type="main">Using regular languages to explore the representational capacity of recurrent neural architectures</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mahalunkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">D</forename><surname>Kelleher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Kůrková</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Manolopoulos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Hammer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Iliadis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Maglogiannis</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-030-01424-7_19</idno>
	</analytic>
	<monogr>
		<title level="m">Artificial Neural Networks and Machine Learning-ICANN 2018</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="189" to="198" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b84">
	<analytic>
		<title level="a" type="main">Learning phonemes with a proto-lexicon</title>
		<author>
			<persName><forename type="first">A</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Peperkamp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Dupoux</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.1551-6709.2012.01267.x</idno>
	</analytic>
	<monogr>
		<title level="j">Cogn. Sci</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="103" to="124" />
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b85">
	<analytic>
		<title level="a" type="main">The trace model of speech perception</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Mcclelland</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Elman</surname></persName>
		</author>
		<idno type="DOI">10.1016/0010-0285(86)90015-0</idno>
	</analytic>
	<monogr>
		<title level="j">Cogn. Psychol</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1" to="86" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b86">
	<analytic>
		<title level="a" type="main">Homonyms and cluster reduction in the normal development of children&apos;s speech</title>
		<author>
			<persName><forename type="first">S</forename><surname>Mcleod</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Van Doorn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Reed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Sixth Australian International Conference on Speech Science &amp; Technology</title>
		<meeting>the Sixth Australian International Conference on Speech Science &amp; Technology<address><addrLine>Adelaide</addrLine></address></meeting>
		<imprint>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="331" to="336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b87">
	<analytic>
		<title level="a" type="main">Phonetic feature encoding in human superior temporal gyrus</title>
		<author>
			<persName><forename type="first">N</forename><surname>Mesgarani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
		<idno type="DOI">10.1126/science.1245994</idno>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">343</biblScope>
			<biblScope unit="page" from="1006" to="1010" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b88">
	<analytic>
		<title level="a" type="main">Analytic bias and phonological typology</title>
		<author>
			<persName><forename type="first">E</forename><surname>Moreton</surname></persName>
		</author>
		<idno type="DOI">10.1017/S0952675708001413</idno>
	</analytic>
	<monogr>
		<title level="j">Phonology</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="83" to="127" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b89">
	<analytic>
		<title level="a" type="main">Structure and substance in artificialphonology learning. Part I</title>
		<author>
			<persName><forename type="first">E</forename><surname>Moreton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pater</surname></persName>
		</author>
		<idno type="DOI">10.1002/lnc3.363</idno>
	</analytic>
	<monogr>
		<title level="j">Structure. Lang. Linguist. Compass</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="686" to="701" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b90">
	<analytic>
		<title level="a" type="main">Structure and substance in artificialphonology learning</title>
		<author>
			<persName><forename type="first">E</forename><surname>Moreton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pater</surname></persName>
		</author>
		<idno type="DOI">10.1002/lnc3.366</idno>
	</analytic>
	<monogr>
		<title level="j">Substance. Lang. Linguist. Compass</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="702" to="718" />
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note>Part II</note>
</biblStruct>

<biblStruct xml:id="b91">
	<analytic>
		<title level="a" type="main">Role of imitation in the emergence of phonological systems</title>
		<author>
			<persName><forename type="first">N</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Delvaux</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.wocn.2015.08.004</idno>
	</analytic>
	<monogr>
		<title level="j">J. Phonet</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="page" from="46" to="54" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b92">
	<analytic>
		<title level="a" type="main">The influence of sonority on children&apos;s cluster reductions</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">K</forename><surname>Ohala</surname></persName>
		</author>
		<idno type="DOI">10.1016/S0021-9924(99)00018-0</idno>
	</analytic>
	<monogr>
		<title level="j">J. Commun. Disord</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="397" to="422" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b93">
	<analytic>
		<title level="a" type="main">Coupled neural maps for the origins of vowel systems</title>
		<author>
			<persName><forename type="first">P.-Y</forename><surname>Oudeyer</surname></persName>
		</author>
		<idno type="DOI">10.1007/3-540-44668-0_163</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Artificial Neural Networks</title>
		<title level="s">Lecture Notes in Computer Science</title>
		<meeting>the International Conference on Artificial Neural Networks<address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="1171" to="1176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b94">
	<analytic>
		<title level="a" type="main">Phonemic coding might result from sensory-motor coupling dynamics</title>
		<author>
			<persName><forename type="first">P.-Y</forename><surname>Oudeyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">From animals to animats 7: Proceedings of the Seventh International Conference on Simulation of Adaptive Behavior</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Hallam</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Floreano</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Hallam</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">G</forename><surname>Hayes</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J.-A</forename><forename type="middle">M</forename><surname>Hallam</surname></persName>
		</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2002">2002</date>
			<biblScope unit="page" from="406" to="416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b95">
	<analytic>
		<title level="a" type="main">The self-organization of speech sounds</title>
		<author>
			<persName><forename type="first">P.-Y</forename><surname>Oudeyer</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jtbi.2004.10.025</idno>
	</analytic>
	<monogr>
		<title level="j">J. Theor. Biol</title>
		<imprint>
			<biblScope unit="volume">233</biblScope>
			<biblScope unit="page" from="435" to="449" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b96">
	<monogr>
		<title level="m" type="main">Self-Organization in the Evolution of Speech</title>
		<author>
			<persName><forename type="first">P.-Y</forename><surname>Oudeyer</surname></persName>
		</author>
		<idno type="DOI">10.1093/acprof:oso/9780199289158.001.0001</idno>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Oxford University Press</publisher>
			<pubPlace>Oxford</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b97">
	<analytic>
		<title level="a" type="main">Reconstructing speech from human auditory cortex</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">N</forename><surname>Pasley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">V</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mesgarani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Flinker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">E</forename><surname>Crone</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pbio.1001251</idno>
	</analytic>
	<monogr>
		<title level="j">PLoS Biol</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">1001251</biblScope>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b98">
	<analytic>
		<title level="a" type="main">Weighted constraints in generative linguistics</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pater</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.1551-6709.2009.01047.x</idno>
	</analytic>
	<monogr>
		<title level="j">Cogn. Sci</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="999" to="1035" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b99">
	<monogr>
		<title level="m" type="main">Generative linguistics and neural networks at 60: foundation, friction, and fusion. Language</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pater</surname></persName>
		</author>
		<idno type="DOI">10.1353/lan.2019.0005</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page">95</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b100">
	<monogr>
		<title level="m" type="main">in Frequency Effects and the Emergence of Lexical Structure</title>
		<author>
			<persName><forename type="first">J</forename><surname>Pierrehumbert</surname></persName>
		</author>
		<idno type="DOI">10.1075/tsl.45.08pie</idno>
		<editor>J. L. Bybee and P. J. Hopper</editor>
		<imprint>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="137" to="157" />
			<pubPlace>Amsterdam</pubPlace>
		</imprint>
		<respStmt>
			<orgName>John Benjamins</orgName>
		</respStmt>
	</monogr>
	<note>Exemplar dynamics: word frequency, lenition, and contrast</note>
</biblStruct>

<biblStruct xml:id="b101">
	<analytic>
		<title level="a" type="main">The emergence of phonology from the interplay of speech comprehension and production: a distributed connectionist approach</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">C</forename><surname>Plaut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">T</forename><surname>Kello</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Emergence of Language</title>
		<editor>
			<persName><forename type="first">B</forename><surname>Macwhinney</surname></persName>
		</editor>
		<meeting><address><addrLine>Mahwah, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Lawrence Erlbaum Associates Publishers</publisher>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="381" to="415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b102">
	<analytic>
		<title level="a" type="main">Voice onset time in consonant cluster errors: can phonetic accommodation differentiate cognitive from motor errors?</title>
		<author>
			<persName><forename type="first">M</forename><surname>Pouplier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Marin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Waltl</surname></persName>
		</author>
		<idno type="DOI">10.1044/2014_JSLHR-S-12-0412</idno>
	</analytic>
	<monogr>
		<title level="j">J. Speech Lang. Hear. Res</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="1577" to="1588" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b103">
	<monogr>
		<title level="m" type="main">Learning reduplication with a variablefree neural network</title>
		<author>
			<persName><forename type="first">B</forename><surname>Prickett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Traylor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Pater</surname></persName>
		</author>
		<ptr target="http://works.bepress.com/joe_pater/38/" />
		<imprint>
			<date type="published" when="2019-05">2019. May 2019</date>
			<biblScope unit="volume">23</biblScope>
			<pubPlace>Ms; Amherst, MA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>University of Massachusetts</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b104">
	<monogr>
		<title level="m" type="main">Optimality Theory: Constraint Interaction in Generative Grammar</title>
		<author>
			<persName><forename type="first">A</forename><surname>Prince</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Smolensky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004">2004</date>
			<pubPlace>Blackwell, Malden, MA</pubPlace>
		</imprint>
		<respStmt>
			<orgName>Rutgers University Center for Cognitive Science</orgName>
		</respStmt>
	</monogr>
	<note>First published as Tech. Rep. 2</note>
</biblStruct>

<biblStruct xml:id="b105">
	<analytic>
		<title level="a" type="main">R: A Language and Environment for Statistical Computing</title>
		<author>
			<orgName type="collaboration">R Core Team</orgName>
		</author>
	</analytic>
	<monogr>
		<title level="m">R Foundation for Statistical Computing</title>
		<meeting><address><addrLine>Vienna</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b106">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note>arXiv [preprint</note>
</biblStruct>

<biblStruct xml:id="b107">
	<analytic>
		<title level="a" type="main">Analyzing distributional learning of phonemic categories in unsupervised deep neural networks</title>
		<author>
			<persName><forename type="first">O</forename><surname>Räsänen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nagamine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mesgarani</surname></persName>
		</author>
		<ptr target="https://cogsci.mindmodeling.org/2016/papers/0308/paper0308.pdf" />
	</analytic>
	<monogr>
		<title level="m">Procedings of the 38th Annual Conference of the Cognitive Science Society</title>
		<title level="s">Cognitive Science Society</title>
		<editor>
			<persName><forename type="first">A</forename><surname>Papafragou</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Grodner</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">D</forename><surname>Mirman</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><forename type="middle">C</forename><surname>Trueswell</surname></persName>
		</editor>
		<meeting>edings of the 38th Annual Conference of the Cognitive Science Society<address><addrLine>Austin, TX</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b108">
	<analytic>
		<title level="a" type="main">No free lunch in linguistics or machine learning: response to pater</title>
		<author>
			<persName><forename type="first">J</forename><surname>Rawski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Heinz</surname></persName>
		</author>
		<idno type="DOI">10.1353/lan.2019.0004</idno>
	</analytic>
	<monogr>
		<title level="j">Language</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="page" from="125" to="e135" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b109">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">B</forename><surname>Rentz</surname></persName>
		</author>
		<ptr target="https://github.com/rentzb/praat-scripts/blob/master/spectral_moments.praat" />
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note type="report_type">spectral_moments.praat. praat script</note>
</biblStruct>

<biblStruct xml:id="b110">
	<analytic>
		<title level="a" type="main">Statistical learning by 8month-old infants</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Saffran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">N</forename><surname>Aslin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">L</forename><surname>Newport</surname></persName>
		</author>
		<idno type="DOI">10.1126/science.274.5294.1926</idno>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">274</biblScope>
			<biblScope unit="page" from="1926" to="1928" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b111">
	<analytic>
		<title level="a" type="main">The infant&apos;s auditory world: hearing, speech, and the beginnings of language</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Saffran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">F</forename><surname>Werker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><forename type="middle">A</forename><surname>Werner</surname></persName>
		</author>
		<idno type="DOI">10.1002/9780470147658.chpsy0202</idno>
	</analytic>
	<monogr>
		<title level="m">Handbook of Child Psychology</title>
		<editor>
			<persName><forename type="first">W</forename><surname>Damon</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><forename type="middle">M</forename><surname>Lerner</surname></persName>
		</editor>
		<meeting><address><addrLine>Hoboken, NJ</addrLine></address></meeting>
		<imprint>
			<publisher>Wiley</publisher>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b112">
	<analytic>
		<title level="a" type="main">Early phonetic learning without phonetic categories -insights from machine learning</title>
		<author>
			<persName><forename type="first">T</forename><surname>Schatz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Feldman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Goldwater</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><forename type="middle">N</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Dupoux</surname></persName>
		</author>
		<idno type="DOI">10.31234/osf.io/fc4wh</idno>
	</analytic>
	<monogr>
		<title level="j">PsyArXiv</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b113">
	<analytic>
		<title level="a" type="main">Measuring the perceptual availability of phonological features during language acquisition using unsupervised binary stochastic autoencoders</title>
		<author>
			<persName><forename type="first">C</forename><surname>Shain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Elsner</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1007</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, MN</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="69" to="85" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b114">
	<analytic>
		<title level="a" type="main">Sound analogies with phoneme embeddings</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">P</forename><surname>Silfverberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hulden</surname></persName>
		</author>
		<idno type="DOI">10.7275/R5NZ85VD</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Society for Computation in Linguistics (SCiL)</title>
		<meeting>the Society for Computation in Linguistics (SCiL)<address><addrLine>Salt Lake City, UT</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
			<biblScope unit="page" from="136" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b115">
	<monogr>
		<title level="m" type="main">A Critical Introduction to Phonology: Functional and Usage-Based Perspectives</title>
		<author>
			<persName><forename type="first">D</forename><surname>Silverman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<publisher>Bloomsbury Publishing</publisher>
			<pubPlace>London</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b116">
	<monogr>
		<title level="m" type="main">Generalised additive mixed models for dynamic analysis in linguistics: a practical introduction</title>
		<author>
			<persName><forename type="first">M</forename><surname>Sóskuthy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.05339</idno>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
	<note>arXiv [preprint</note>
</biblStruct>

<biblStruct xml:id="b117">
	<analytic>
		<title level="a" type="main">Acquisition of initial /s/-stop and stop-/s/ sequences in Greek</title>
		<author>
			<persName><forename type="first">A</forename><surname>Syrika</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Nicolaidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Beckman</surname></persName>
		</author>
		<idno type="DOI">10.1177/0023830911402597</idno>
		<idno type="PMID">22070044</idno>
	</analytic>
	<monogr>
		<title level="j">Lang. Speech</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="page" from="361" to="386" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b118">
	<analytic>
		<title level="a" type="main">A hybrid dynamic time warping-deep neural network architecture for unsupervised acoustic modeling</title>
		<author>
			<persName><forename type="first">R</forename><surname>Thiolliére</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Dunbar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Versteegh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Dupoux</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Interspeech</title>
		<meeting>Interspeech<address><addrLine>Dresden)</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b119">
	<analytic>
		<title level="a" type="main">Grundzüge der Phonologie</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">S</forename><surname>Trubetzkoy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Travaux de Cercle linguistique de Prague</title>
		<imprint>
			<date type="published" when="1939">1939</date>
			<pubPlace>Prague</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b120">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Vaux</surname></persName>
		</author>
		<title level="m">Aspiration in English</title>
		<meeting><address><addrLine>Cambridge, MA, United Kingdom</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
		<respStmt>
			<orgName>Ms.). Harvard University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b121">
	<analytic>
		<title level="a" type="main">Laryngeal markedness and aspiration</title>
		<author>
			<persName><forename type="first">B</forename><surname>Vaux</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Samuels</surname></persName>
		</author>
		<idno type="DOI">10.1017/S0952675705000667</idno>
	</analytic>
	<monogr>
		<title level="j">Phonology</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="395" to="436" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b122">
	<analytic>
		<title level="a" type="main">Learning to produce syllabic speech sounds via reward-modulated neural plasticity</title>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Warlaumont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">K</forename><surname>Finnegan</surname></persName>
		</author>
		<idno type="DOI">10.1371/journal.pone.0145096</idno>
	</analytic>
	<monogr>
		<title level="j">PLoS ONE</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">145096</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b123">
	<analytic>
		<title level="a" type="main">The fine line between linguistic generalization and failure in Seq2Seq-attention models</title>
		<author>
			<persName><forename type="first">N</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Shekhar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Balasubramanian</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W18-1004</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Generalization in the Age of Deep Learning</title>
		<meeting>the Workshop on Generalization in the Age of Deep Learning<address><addrLine>New Orleans, LA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="24" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b124">
	<analytic>
		<title level="a" type="main">Exemplar models, evolution and language change</title>
		<author>
			<persName><forename type="first">A</forename><surname>Wedel</surname></persName>
		</author>
		<idno type="DOI">10.1515/TLR.2006.010</idno>
	</analytic>
	<monogr>
		<title level="j">Linguist. Rev</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="247" to="274" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b125">
	<analytic>
		<title level="a" type="main">Evidence for a learning bias against saltatory phonological alternations</title>
		<author>
			<persName><forename type="first">J</forename><surname>White</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.cognition.2013.09.008</idno>
	</analytic>
	<monogr>
		<title level="j">Cognition</title>
		<imprint>
			<biblScope unit="volume">130</biblScope>
			<biblScope unit="page" from="96" to="115" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b126">
	<analytic>
		<title level="a" type="main">Accounting for the learnability of saltation in phonological theory: a maximum entropy model with a P-map bias</title>
		<author>
			<persName><forename type="first">J</forename><surname>White</surname></persName>
		</author>
		<idno type="DOI">10.1353/lan.2017.0001</idno>
	</analytic>
	<monogr>
		<title level="j">Language</title>
		<imprint>
			<biblScope unit="volume">93</biblScope>
			<biblScope unit="page" from="1" to="36" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b127">
	<analytic>
		<title level="a" type="main">Learning phonology with substantive bias: an experimental and computational study of velar palatalization</title>
		<author>
			<persName><forename type="first">C</forename><surname>Wilson</surname></persName>
		</author>
		<idno type="DOI">10.1207/s15516709cog0000_89</idno>
	</analytic>
	<monogr>
		<title level="j">Cogn. Sci</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="945" to="982" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b128">
	<analytic>
		<title level="a" type="main">Fast stable restricted maximum likelihood and marginal likelihood estimation of semiparametric generalized linear models</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">N</forename><surname>Wood</surname></persName>
		</author>
		<idno type="DOI">10.1111/j.1467-9868.2010.00749.x</idno>
	</analytic>
	<monogr>
		<title level="j">J. R. Stat. Soc</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="page" from="3" to="36" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b129">
	<analytic>
		<title level="a" type="main">The structure of initial /s/-clusters: evidence from L1 and L2 acquisition</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Yildiz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Developmental Paths in Phonological Acquisition</title>
		<editor>
			<persName><forename type="first">M</forename><surname>Tzakosta</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">C</forename><surname>Levelt</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">J</forename><surname>Van Der Weijer</surname></persName>
		</editor>
		<meeting><address><addrLine>Leiden</addrLine></address></meeting>
		<imprint>
			<publisher>LUCL</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="163" to="187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b130">
	<analytic>
		<title level="a" type="main">Neural representation of spectral and temporal information in speech</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">D</forename><surname>Young</surname></persName>
		</author>
		<idno type="DOI">10.1098/rstb.2007.2151</idno>
	</analytic>
	<monogr>
		<title level="j">Philos. Trans. R. Soc. B: Biol. Sci</title>
		<imprint>
			<biblScope unit="volume">363</biblScope>
			<biblScope unit="page" from="923" to="945" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b131">
	<analytic>
		<title level="a" type="main">The evolution of combinatorial phonology</title>
		<author>
			<persName><forename type="first">W</forename><surname>Zuidema</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>De Boer</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.wocn.2008.10.003</idno>
	</analytic>
	<monogr>
		<title level="j">J. Phonet</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="125" to="144" />
			<date type="published" when="2009">2009</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
