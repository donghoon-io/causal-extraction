<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Bayesian latent class extension of naive Bayesian classifier and its application to the classification of gastric cancer patients</title>
				<funder>
					<orgName type="full">Tarbiat Modares University</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Kimiya</forename><surname>Gohari</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Biostatistics</orgName>
								<orgName type="department" key="dep2">Faculty of Medical Sciences</orgName>
								<orgName type="institution">Tarbiat Modares University</orgName>
								<address>
									<settlement>Tehran</settlement>
									<country key="IR">Iran</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Anoshirvan</forename><surname>Kazemnejad</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Biostatistics</orgName>
								<orgName type="department" key="dep2">Faculty of Medical Sciences</orgName>
								<orgName type="institution">Tarbiat Modares University</orgName>
								<address>
									<settlement>Tehran</settlement>
									<country key="IR">Iran</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Marjan</forename><surname>Mohammadi</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Medical Biotechnology, Biotechnology Research Center</orgName>
								<orgName type="laboratory">HPGC Research Group</orgName>
								<orgName type="institution">Pasteur Institute of Iran</orgName>
								<address>
									<settlement>Tehran</settlement>
									<country key="IR">Iran</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Farzad</forename><surname>Eskandari</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Department of Statistics</orgName>
								<orgName type="institution">Allameh Tabataba&apos;i University</orgName>
								<address>
									<settlement>Tehran</settlement>
									<country key="IR">Iran</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Samaneh</forename><surname>Saberi</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Medical Biotechnology, Biotechnology Research Center</orgName>
								<orgName type="laboratory">HPGC Research Group</orgName>
								<orgName type="institution">Pasteur Institute of Iran</orgName>
								<address>
									<settlement>Tehran</settlement>
									<country key="IR">Iran</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Maryam</forename><surname>Esmaieli</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Medical Biotechnology, Biotechnology Research Center</orgName>
								<orgName type="laboratory">HPGC Research Group</orgName>
								<orgName type="institution">Pasteur Institute of Iran</orgName>
								<address>
									<settlement>Tehran</settlement>
									<country key="IR">Iran</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ali</forename><surname>Sheidaei</surname></persName>
							<affiliation key="aff3">
								<orgName type="department" key="dep1">Department of Epidemiology and Biostatistics</orgName>
								<orgName type="department" key="dep2">School of Public Health</orgName>
								<orgName type="institution">Tehran University of Medical Sciences</orgName>
								<address>
									<settlement>Tehran</settlement>
									<country key="IR">Iran</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Bayesian latent class extension of naive Bayesian classifier and its application to the classification of gastric cancer patients</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<idno type="DOI">10.1186/s12874-023-02013-4</idno>
					<note type="submission">Received: 5 November 2022 Accepted: 8 August 2023</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-21T19:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Naïve Bayesian classifier</term>
					<term>Bayesian latent class analysis</term>
					<term>Gibbs sampling</term>
					<term>Expectation maximization algorithm</term>
					<term>Gastric cancer</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Background The Naive Bayes (NB) classifier is a powerful supervised algorithm widely used in Machine Learning (ML). However, its effectiveness relies on a strict assumption of conditional independence, which is often violated in real-world scenarios. To address this limitation, various studies have explored extensions of NB that tackle the issue of non-conditional independence in the data. These approaches can be broadly categorized into two main categories: feature selection and structure expansion.</p><p>In this particular study, we propose a novel approach to enhancing NB by introducing a latent variable as the parent of the attributes. We define this latent variable using a flexible technique called Bayesian Latent Class Analysis (BLCA). As a result, our final model combines the strengths of NB and BLCA, giving rise to what we refer to as NB-BLCA. By incorporating the latent variable, we aim to capture complex dependencies among the attributes and improve the overall performance of the classifier.</p><p>Methods Both Expectation-Maximization (EM) algorithm and the Gibbs sampling approach were offered for parameter learning. A simulation study was conducted to evaluate the classification of the model in comparison with the ordinary NB model. In addition, real-world data related to 976 Gastric Cancer (GC) and 1189 Non-ulcer dyspepsia (NUD) patients was used to show the model's performance in an actual application. The validity of models was evaluated using the 10-fold cross-validation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>The presented model was superior to ordinary NB in all the simulation scenarios according to higher classification sensitivity and specificity in test data. The NB-BLCA model using Gibbs sampling accuracy was 87.77 (95% CI: 84.87-90.29). This index was estimated at 77.22 (95% CI: 73.64-80.53) and 74.71 (95% CI: 71.02-78.15) for the NB-BLCA model using the EM algorithm and ordinary NB classifier, respectively. Conclusions When considering the modification of the NB classifier, incorporating a latent component into the model offers numerous advantages, particularly within medical and health-related contexts. By doing so, the researchers can bypass the extensive search algorithm and structure learning required in the local learning and structure extension approach. The inclusion of latent class variables allows for the integration of all attributes during model construction. Consequently, the NB-BLCA model serves as a suitable alternative to conventional NB classifiers when the assumption of independence is violated, especially in domains pertaining to health and medicine.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Background</head><p>The Naive Bayes (NB) classifier is a well-established supervised algorithm in the field of Machine Learning (ML). Its simplicity and effectiveness in classification tasks have made it widely adopted across various domains <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. However, the NB classifier is built upon a fundamental assumption of conditional independence, wherein all feature pairs are considered mutually independent given the class variable <ref type="bibr" target="#b2">[3]</ref>. In practical real-world scenarios, this assumption is frequently violated, resulting in a reduction in the algorithm's performance <ref type="bibr" target="#b3">[4]</ref>.</p><p>In the context of health and medical domains, the features employed in analysis often originate from diverse aspects related to the subjects under study <ref type="bibr" target="#b4">[5]</ref>. These features can encompass symptoms in diagnostic scenarios or risk factors in the context of risk assessment. Consequently, the dependence among these features, even within a specific class, becomes inevitable. This dependency violates the assumption of conditional independence and calls for alternative approaches to effectively model and classify the data.</p><p>The issue of non-conditional independence in data has been addressed by various studies, proposing extensions of the Naive Bayes (NB) classifier <ref type="bibr" target="#b5">[6]</ref>. These approaches can be classified into two major categories. Firstly, some studies focused on altering the features through subset selection or assigning weights to them <ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref>. These approaches involve a search strategy to identify the most relevant features that optimize the classification performance of NB. Feature selection methods aim to identify critical variables based on their contribution to classification and eliminate less influential ones <ref type="bibr" target="#b11">[12]</ref>. Alternatively, feature weighting algorithms retain all variables in the model while assigning them importance weights <ref type="bibr" target="#b12">[13]</ref><ref type="bibr" target="#b13">[14]</ref><ref type="bibr" target="#b14">[15]</ref>. However, these algorithms heavily rely on the characteristics of the observed data, and their results can vary accordingly. Moreover, the application of these methods is computationally demanding, as they pose NP-hard (NP-hard: Denoting a computational problem that is at least as difficult to solve as the hardest problems in the class of problems known as NP, which includes a wide range of challenging computational tasks) problems requiring extensive computational resources <ref type="bibr" target="#b12">[13]</ref>.</p><p>In an alternative approach, some studies have proposed expanding the structure of the Naive Bayes (NB) classifier to accommodate conditional independence.</p><p>Examples of such methods include the Augmented Naive Bayes (ANB) <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref>, Tree Augmented Naive Bayes (TAN) <ref type="bibr" target="#b17">[18]</ref>, extended Tree Augmented Naive Bayes (eTAN) <ref type="bibr" target="#b18">[19]</ref>, k-dependence Bayesian classifier <ref type="bibr" target="#b19">[20]</ref>, and Averaged One-Dependence Estimators (AODE) <ref type="bibr" target="#b20">[21]</ref>. These algorithms share a common feature of augmenting the relationship set by introducing additional arcs between features. However, as more relationships are added to the original NB structure, the computational complexity increases. Hence, the challenge lies in striking a balance between the trade-off of increased relationships and computational complexity. Consequently, the search algorithms employed in this context face the same issue of being NP-hard <ref type="bibr" target="#b21">[22]</ref>.</p><p>An appealing alternative approach in extending the structure involves incorporating a latent variable into the model. By introducing a latent variable, we can effectively capture the correlation between features and enforce conditional independence within the structure <ref type="bibr" target="#b22">[23]</ref><ref type="bibr" target="#b23">[24]</ref><ref type="bibr" target="#b24">[25]</ref>. The utilization of latent variables holds particular relevance in health and medical applications, especially in cases where the underlying causal mechanisms of diseases remain unknown. Additionally, latent variables find application in situations where the direct cause of a disease is not directly measurable, but certain observable variables can provide valuable insights into it <ref type="bibr" target="#b4">[5]</ref>. Real medical data often involves complex interactions and relationships among various factors that influence health outcomes. The inclusion of latent variables provides a mechanism to capture these hidden factors, which may not be directly observable or measured <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref>. By incorporating latent variables into our models, we can account for unobserved factors that impact the observed features, leading to a more comprehensive understanding of the underlying mechanisms and improved predictive accuracy.</p><p>Defining a latent variable in the context of Naive Bayes (NB) requires careful consideration. Firstly, the placement of the latent variable within the structure determines its relationship with the features and class. For example, <ref type="bibr" target="#b27">Langseth and Nielsen (2006)</ref> proposed a hierarchical NB model where class variables serve as the root, attributes act as leaf nodes, and multiple latent variables act as parents to the leaf nodes <ref type="bibr" target="#b27">[28]</ref>. <ref type="bibr" target="#b28">Calders and Verwer (2010)</ref> presented an NB model for discrimination-free classification, incorporating a single latent variable as the parent of the class variable <ref type="bibr" target="#b28">[29]</ref>.</p><p>Similarly, Alizadeh et al. (2021) introduced a multiindependent latent component extension of NB, featuring a latent variable as the parent of attributes and also linked to the class variable <ref type="bibr" target="#b22">[23]</ref>.</p><p>Additionally, defining the latent variable(s) requires careful consideration. The latent variable should encapsulate all relevant information from the attributes while assisting the NB structure in maintaining the assumption of conditional independence. Striking a balance between capturing the dependencies in the data and preserving the conditional independence assumption is essential in defining the latent variable(s).</p><p>This study introduces a novel approach by incorporating a latent variable as the parent of attributes, similar to the model proposed by Calders and Verwer. However, our proposed model offers reduced complexity compared to the previous approach. The latent variable is defined using Bayesian Latent Class Analysis (BLCA), providing flexibility in modeling. As a result, our final model combines elements of both Naive Bayes (NB) and BLCA, and we refer to it as NB-BLCA. To learn the model's parameters, we provide two options: the Expectation-Maximization (EM) algorithm and the Gibbs sampling approach. A comprehensive simulation study is conducted to assess the classification performance of the proposed model. Furthermore, we apply the model to real-world data, specifically in classifying patients as either GC or NUD based on their attributes. By employing the NB-BLCA model, we aim to enhance classification accuracy while effectively capturing latent dependencies within the data, contributing to improved decision-making in healthcare settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Material and methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Naïve Bayesian classifier</head><p>Suppose in a classification problem, the levels of target variable C indicate the different classes. For instance, C could be the disease status indicator. In this exam- ple, the C levels indicate the disease's presence or absence. Another example could be a physician's diagnosed stages of GC patients. In such examples, we are interested in exploring the prediction power of a set of attributes (X 1 , . . . , X m ) for accurately detecting C levels. In an NB classifier framework, we assume the attributes (X 1 , . . . , X m ) are conditionally independent given the information about class variable C . Therefore, we aim to find the level c of the class variable C which maxi- mizes the posterior probability of this variable given the observed values of attributes:</p><p>(1) arg max c∈C P(C|x 1 , . . . , x m ) Using the Bayes rule for this posterior probability, we have:</p><p>As we mentioned before, the primary assumption of NB is conditional independency between attributes given the class variable. Therefore equation ( <ref type="formula">2</ref>) could be rewritten as:</p><p>In equation ( <ref type="formula">3</ref>), the denominator is constant for all the possible values of class variable C . Hence we could eliminate it and find the best class according to the below formula:</p><p>Therefore we allocate the subjects to the class variable levels, which are maximized according to their attributes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Bayesian latent class analysis</head><p>BLCA is a model-based clustering that finds explicitly unobserved homogenous subgroups among the total population and uses the Bayesian paradigm in this manner <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31]</ref>. This study introduces a version of Bayesian Latent Class Analysis (BLCA) specifically tailored for binary attributes while accommodating a multinomial distributed class variable. While it is possible to generalize the method for multinomial attributes or predictors, it requires the use of binary indicator variables, which is a common practice in various statistical applications such as regression. By employing this approach, for a dependent factor variable with q levels, one can include q-1 binary indicators, with each indicator representing a specific level of the original dependent variable by taking the value 1 and 0 for the other levels. The elimination of the last level is necessary to avoid redundancy. However, it is important to note that the binary version of BLCA often suffices for many health and medical applications.</p><p>Suppose we express the attributes by an M-dimensional vector-valued X = (X 1 , . . . , X N ) , where these come from G sub-populations. The sub-populations are typically referred to as classes or components. Therefore, we have two sets of parameters. A G-dimensional vector τ = (τ 1 , . . . , τ G ) , including parameters for prior belief in the proportions of each class. In addition, a matrix θ with dimension G × M for item probability of all classes. In this way, all elements τ are equal or greater than 0 and G g=1 τ g = 1 and θ gm is the probability of X im = 1 given the information about membership of group g for any i ∈ 1, . . . , N of individuals in the study. Hence, we have</p><formula xml:id="formula_0">P X im |θ gm = θ Xim gm (1 -θ gm ) 1-Xim for X im ∈ [0, 1] , according to the definition of Bernoulli distribution.</formula><p>If we make a naïve Bayes assumption of conditional independence of observations given the group membership, we can express the P X i |θ g = M m=1 P(X im |θ gm ) and the distribution of all X i s are:</p><p>The actual values for parameters θ and τ are unknown, and we suppose prior information about them. Therefore, the direct calculation of equation 5 is not feasible. In application, we introduce a set Z = (Z 1 , . . . , Z N ) where each Z i = (Z i1 , . . . , Z iG ) is a vector representing the actual class membership of X i . In this manner, Z ig = 1 if individual i belongs to subgroup g and 0 for other- wise. The new task is to find the best values for Z, which maximize the posterior probability of class membership, including the Z parameters.</p><p>The complete density of observed variables X i and missing values Z i is:</p><p>Using the Bayes theorem leads to the posterior probability of Z i , class membership for observation i , as:</p><p>The drawback of unknown actual values for parameters θ and τ still exist. An iterative approach that updates the prior information of these parameters in each step according to the observed data is proposed to achieve the best posterior distribution. In this regard, we assume conjugate prior distribution Beta(α gm , β gm ) for binary variables θ , and Dirichlet(δ) for multinomial variables τ . Note that hyperparameters α gm and β gm for Beta prior distributions, specify the item response probabilities of attributes m in class g . In the same manner, ( <ref type="formula">5</ref>)</p><formula xml:id="formula_1">P(X i |θ, τ ) = G g=1 τ g P(X i |θ g ) P(X i , Z i |τ , θ ) = G g=1 [τ g P(X i |θ g )] Z ig P(Z i |X i , τ , θ) = G g=1 [ τ g P(X i |θ g ) G h=1 τ h P(X i |θ h ) ] Z ig</formula><p>hyperparameter δ = (δ 1 , . . . , δ G ) specify the share of each class from the total samples. Supposing these prior distributions for θ and τ we have:</p><formula xml:id="formula_2">For each g ∈ [1, . . . , G] and m ∈ [1, . . . , M]</formula><p>. These assumptions lead to the joint posterior distribution τ and θ as:</p><p>In the following parts, we present two well-known iterative approaches for parameter estimation. These are the EM algorithm and Gibbs sampling method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The EM algorithm for BLCA</head><p>This algorithm follows an iterative process that continues until convergence is achieved, iteratively refining the results. The algorithm consists of two steps that are repeated in each iteration. In the first step, the algorithm calculates the expectation of the logarithm posterior probability. This step involves estimating the probabilities associated with each parameter based on the available data. In the second step, the algorithm determines the parameter values that maximize the expectation function obtained in the previous step. This maximization step involves adjusting the parameter values to optimize the fit of the model to the data <ref type="bibr" target="#b31">[32]</ref>. To initiate the algorithm, an initial guess of the parameter values is required for the first iteration. However, regardless of the initial values chosen, the algorithm is guaranteed to converge to the actual values of the parameters. The number of iterations required for convergence may vary depending on the specific dataset and initial values chosen.</p><p>By iteratively performing these two steps, the algorithm refines the parameter estimates, improving the accuracy and performance of the model until a satisfactory level of convergence is achieved <ref type="bibr" target="#b32">[33]</ref>. If we show the values of the parameters τ and θ in steps k by τ (t) and θ (t) , respectively the expected function in E-step for a BLCA is:</p><p>In the M-step, we update the parameters as follows:</p><formula xml:id="formula_3">P(τ |δ) ∝ G g=1 τ δ g -1 g P(θ gm |α gm , β gm ) ∝ θ α gm -1 gm (1 -θ gm ) β gm -1 P(τ , θ) ∝ N i=1 P(X i , Z i |τ , θ )P(θ )P(τ ) = N i=1 G g=1 τ Z ig +δ g -1 g M m=1 θ X im Z ig +α gm -1 gm (1 -θ gm ) (1-X im )Z ig +β gm -1 Q(θ, τ |θ (t) , τ (t) ) := E[log P(θ , τ |X, Z)|X, θ (t) , τ (t) ]</formula><p>Here the and T are parameter space for θ and τ , respectively. For all item response probability and class proportions, we have</p><formula xml:id="formula_4">= [0, 1] G×M and T = [0, 1] G given G g=1 τ g = 1.</formula><p>It has been shown that the practical formulations for these steps are <ref type="bibr" target="#b33">[34]</ref>: E-step:</p><p>M-step:</p><p>The Gibbs sampling for BLCA</p><p>As we already mentioned, calculating the joint posterior distribution of parameters τ and θ and unobserved class membership Z is directly impossible. However, deter- mining the class membership of samples is possible in the case of knowing the parameter values. Gibbs sampling is a Markov Chain Monte Carlo (MCMC) method that simplifies such issues and, instead of using the joint distribution, iteratively draws samples from the conditional distributions using the Markov property. These samples reflect the properties of the accurate joint posterior distribution <ref type="bibr" target="#b34">[35]</ref>.</p><p>The following steps are the practical approach for handling a BLCA using the Gibbs sampling:</p><p>1-Set initial values for parameters τ and θ and randomly assign each observation to a class. Although this step plays a crucial role in determining the convergence speed of the algorithm, it is important to provide guidance on how users can specify the initial values effectively. In our proposed method, one approach for specifying initial values is to use random initialization, which allows for exploration of different parts of the parameter space. This can help avoid potential biases that may arise from using fixed initial values.</p><formula xml:id="formula_5">θ (t+1) = argmaxQ θ, τ |θ (t) , τ (t) θ ∈� τ (t+1) = argmaxQ θ, τ |θ (t) , τ (t) τ ∈T Z (t+1) ig = τ (t) g P(X i |θ (t) g ) G h=1 τ (t) h P(X i |θ (t) h ) θ (t+1) gm = N i=1 X im Z (t+1) ig + α gm -1 N i=1 Z (t+1) ig + α gm + β gm -2 τ (t+1) g = N i=1 Z (t+1) ig + δ g -1 N + G h=1 δ h -G</formula><p>Additionally, users may consider conducting sensitivity analyses by running the algorithm multiple times with different initializations to assess the stability of the results. 2-Considering the conjugate prior of Beta distribution, generate elements of θ (t) randomly from the follow- ing distribution:</p><p>3-Considering the conjugate prior of Dirichlet distribution, generate elements of τ (k+1) randomly from the following distribution:</p><p>4-Consider the generated values of parameters and assign the individuals to classes randomly from a multinomial distribution according to their observed attributes X i which specify the posterior probabilities of membership in the classes:</p><p>5-Repeat steps 2 to 4 until making sure about convergence.</p><p>After running the Gibbs sampling, like all other MCMC methods, it is essential to check if the chain converged using the statistical criteria and trace plots. In addition, burn-in and thinning are necessary <ref type="bibr" target="#b35">[36]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>NB-BLCA</head><p>In this study, we present an extension of the NB classifier that uses BLCA to impose conditional independence assumptions on the structure of the model. NB and BLCA assume the Naïve assumption of conditional independence assumption given the information of class variable. In contrast to NB, which only requires this assumption for efficient classification, The BLCA model estimates the parameter values considering this purpose. The presentation of the NB classifier and our proposed model are depicted in Fig. <ref type="figure">1</ref>, parts A and B, respectively. In this figure, the latent class of BLCA is shown by L i [i = 1, . . . , K ] to differentiate from classes of the primary outcome C . Remember that latent class L is unobserved, but the class variable C is observable.</p><p>In the NB-BLCA model, the only child node of class variable C is the latent class variables L i . Therefore the posterior density in equation 3 could be reformed to:</p><formula xml:id="formula_6">θ (t) gm ∼ Beta( N i=1 X im Z (t-1) ig + α gm , N i=1 Z (t-1) ig (1 -X im ) + β gm ) τ (t) ∼ Dirichlet( N i=1 Z (t-1) i1 + δ 1 , . . . , N i=1 Z (t-1) iG + δ G ) Z (t) i ∼ Multinomial(1, τ (t) 1 P Xi|θ (t) 1 G h=1 τ (t) h P Xi|θ (t) h , . . . , τ<label>(t)</label></formula><formula xml:id="formula_7">G P(Xi|θ (t) G ) G h=1 τ (t) h P(Xi|θ<label>(t) h )</label></formula><p>)</p><p>As the latent class variables L i come from a mixture distribution with parameters (τ , θ , Z ), the calculation of this posterior probability is not straightforward. However, the generalized forms of the EM algorithm and Gibbs sampling in the previous sections enable us to predict class membership C due to information about the latent class assignment L i concluded from the observed attributes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Adjusting EM algorithm for NB-BLCA</head><p>In order to explain the EM algorithm for an NB-BLCA, we should define the following parameters:</p><p>The parameter q(c) is the probability of seeing the level c of the class variable. Hence, it is subject to con- straints q(c) ≥ 0 and q(c) = 1 for all the possible lev- els of this variable.</p><p>The parameter q i (l|c) for any i = 1, . . . , K is the prob- ability of latent class i taking value l , conditioned on the class c . This parameter is subject to constraints q i (l|c) ≥ 0 and q i (l|c) = 1 for all levels of class and latent class variables.</p><p>The practical formulations of the EM algorithm are presented in Fig. <ref type="figure" target="#fig_0">2</ref>. The algorithm estimates latent class variables membership using the attributes and then estimate the posterior probability of class membership of the target variable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Adjusting Gibbs sampling for NB-BLCA</head><p>The Gibbs sampler simplifies a complex joint posterior distribution into a set of steps, including generating samples from the conditional distributions. We explained how to generate latent class membership samples for a BLCA problem in 5 steps. The added task of generating</p><formula xml:id="formula_8">P(C|L) = k i=1 P(L i |C)P(C) c k i=1 P(L i |C = c)P(C = c)</formula><p>samples for the NB part of NB-BLCA is quickly done by adding an extra step. The sample generation could be done from a multinomial (if the class variable has more than two categories) or binomial distribution (the class variable only includes two levels). The practical formulations of the Gibbs sampler are presented in Fig. <ref type="figure">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Simulation study</head><p>We conducted a simulation study to evaluate the predictive performance of our model compared to a simple NB model. Furthermore, we included two alternative approaches that have been suggested to improve the correct classification of NB when the conditional assumption is violated. These approaches are Averaged one-dependence estimators (AODE), proposed by Webb et al. <ref type="bibr" target="#b20">[21]</ref>, and Hill-climbing tree augmented naive Bayes (TAN-HC), proposed by Keogh and Pazzani <ref type="bibr" target="#b36">[37]</ref>.</p><p>To generate the datasets, we utilized the Iterative Proportional Fitting Procedure (IPFP), originally proposed by <ref type="bibr">Deming and</ref> Stephan in 1940 as an algorithm aimed at minimizing the Pearson chi-squared statistic <ref type="bibr" target="#b37">[38]</ref>. The details of this method, as described by Suesse et al. <ref type="bibr" target="#b38">[39]</ref>, can be found in the 'mipfp' R package developed by Barthélemy and Suesse <ref type="bibr" target="#b39">[40]</ref>. Using this method we were able to simulate multivariate Bernoulli distributions assuming the Hypothetical Marginal Probabilities (HMP) of each variable and a matrix that includes the Odds Ratio (OR) of all pairs of variables.</p><p>The elements of the HMP vector were randomly generated from a uniform distribution between 0 and 1 ( HMP i ∼ U (0, 1) ) for each iteration. Similarly, the ele- ments of the paired OR matrix were randomly generated from a uniform distribution within the range of 0.25 and 4 ( OR ij ∼ U (0.25, 4)fori � = j ). To reduce com- putational complexity, we generated the feature variables in batches of 5 dimensions. Consequently, for scenarios involving only 5 features, we generated a Fig. <ref type="figure">1</ref> The Naïve Bayesian classifier (A) and proposed model network (B) structures single batch. For scenarios with 10 features, we generated 2 batches, and so on.</p><p>The response class variable Z was generated using a logistic regression approach. We assumed a regression coefficient of 2 ( β = 2 ) for all feature variables and applied the inverse logit transformation to their linear combination to calculate the probability of belonging to class 1. Additionally, a random error term from a Gaussian distribution with mean parameter 0 and standard deviation parameter 4 was added to this linear combination. The intercept coefficient ( α ) of the logistic regression served as a tuning parameter for specifying the marginal probability of the class variable.</p><p>Finally, the values of the response variable were generated from a Binomial distribution, taking into account the calculated probabilities.</p><p>We assumed marginal probabilities of 0.3, 0.5, and 0.7 for the class variable to explore their effect on the</p><formula xml:id="formula_9">Z = α + β p i=1 X i + N (0, σ = 4) P = 1 1 + e Z Y ∼ Binomial(P)</formula><p>Fig. <ref type="figure" target="#fig_0">2</ref> The EM Algorithm for the NB-BLCA model model's performance. To assess the impact of sample size on the model's performance, we considered samples consisting of 500, 1000, and 2000 subjects. Furthermore, we generated scenarios with 5, 10, and 20 feature variables.</p><p>For all algorithms, we used 70% of the randomly selected data as a training dataset, while the remaining 30% was used to evaluate algorithm performance. The validity of the algorithms was measured by calculating the mean values of sensitivity (recall), specificity, positive predictive value (precision), negative predictive value, and precision across 1000 replicates."</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Real-world data application</head><p>In this section, we used multicenter hospital-based data to demonstrate the application of the model in a real-world example. This data was related to 976 GC and 1189 NUD patients referred to the national cancer institute of Iran (NCII) from July 2003 to Jan 2020. Trained technicians interviewed each participant at the time of recruitment using a structured questionnaire after accepting enrolment in the study. The questionnaire includes 64 attributes in the five subdomains, demographic variables, dietary habits, self-reported medical status, narcotics use, and SES indicators. All the predictors were recoded into binary variables, and the list, including their names and levels, is available in Supplementary Table <ref type="table" target="#tab_0">1</ref>.</p><p>We fitted the NB classifier, NB-BLCA using the EM algorithm, and NB-BLCA using Gibbs sampler to data. A random sample with a proportion of 70% sample size was selected to train the models. The model's validity and Fig. <ref type="figure">3</ref> The Gibbs sampler Algorithm for the NB-BLCA model prediction ability were explored using the other 30% of subjects. The identical measurements in the simulation section were calculated and reported.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Results</head><p>In the simulation study, we compared the sensitivity, specificity, positive predictive value, negative predictive value, and precision of the ordinary Naive Bayes (NB) classifier, NB-BLCA, and other alternative models. Tables 1, 2 and 3 present these performance metrics for different scenarios, considering varying marginal probabilities of the class variables (0.3, 0.5, and 0.7) and different numbers of predictors.</p><p>When the marginal probability of the class variable is set to 0.3 and the number of predictors is low (5 attributes), the sensitivity of all models is relatively lower, failing to exceed 50%. However, as the sample size increases, the sensitivity improves. Even in the scenario with the highest sample size of 2000, the sensitivity remains below 50%. This indicates that all algorithms are sensitive to the lower rate of events in the data. It is worth noting that both increasing the number of predictors and the marginal probability of the class variables enhance the sensitivity of the models. In all scenarios, except for the marginal probability of the class variable 0.7 when the number of predictors is 5, the precision of our proposed model (NB-BLCA) is higher compared to the other approaches. This indicates that our model performs better in terms of correctly identifying positive instances among the predicted ones.</p><p>When the marginal probability of the class variable is low (0.3) and the number of predictors is less than 20, the superiority of our model is based on higher specificity. Increasing the number of predictors also leads to a greater increase in the sensitivity of our model compared to the other approaches. This trend is observed consistently across the different scenarios (as shown in Tables <ref type="table" target="#tab_1">2</ref> and<ref type="table" target="#tab_2">3</ref>). Similar to many classification algorithms, the performance of NB, AODE, TAN, and our proposed model is influenced by the prevalence of the outcome, with a lower rate of events having a significant impact on the sensitivity of these models.</p><p>Overall, the results demonstrate that the performance of the models is affected by the marginal probability of the class variable, the number of predictors, and the prevalence of the outcome. Our proposed model (NB-BLCA) shows favorable precision and specificity, particularly in scenarios with low marginal probability and a smaller number of predictors.</p><p>These findings highlight the importance of considering these factors when applying classification algorithms and emphasize the potential benefits of our proposed model in handling such scenarios.</p><p>In Table <ref type="table" target="#tab_3">4</ref>, we present the results of comparing the models' predictions for real world data (classification of patients into GC or NUD groups). All models showed a significant improvement in prediction accuracy (P-value &lt; 0.001). Among the models, the NB-BLCA model utilizing the Gibbs sampler achieved the highest accuracy of 87.77 (84.87-90.29), according to the 95% confidence interval. Notably, this confidence interval did not overlap with the intervals of the other two models, indicating a statistically significant increase in prediction accuracy. Additionally, the Gibbs sampler-based NB-BLCA model demonstrated a higher Kappa value compared to the other approaches. This indicates that the model correctly classified patients with a 76% higher accuracy than random assignment. Furthermore, when performing McNemar's test for the NB classifier, the result was not significant (p-value = 0.74), suggesting that the NB approach did not yield a substantial improvement.</p><p>While the NB-BLCA model had a lower specificity (74.87) compared to NB (77.12), it exhibited a significantly higher sensitivity. The increased sensitivity indicates a better ability to correctly identify positive cases. Overall, the NB-BLCA model employing the Gibbs sampler outperformed the other two alternatives in terms of prediction accuracy and various performance metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion</head><p>We presented a modified version of the ordinary NB classifier called NB-BLCA, which can enhance the model's prediction performance. In addition, we suggested two methods, Gibbs sampling, and the EM algorithm, for parameter estimation. Our findings, based on realworld data examples of GC patients, demonstrate that the Gibbs sampler method yields significantly improved prediction accuracy compared to the EM algorithm. The application of Gibbs sampling in our study has shown superior performance in accurately predicting outcomes, indicating its effectiveness in modeling and analyzing the given dataset. These results underscore the value of incorporating Gibbs sampling as a powerful tool for enhancing prediction accuracy in real-world scenarios involving GC patients. On the other hand, the simulation study revealed that NB-BLCA based on the EM algorithm was superior to the ordinary NB classifier in all the predefined scenarios. However, we should admit that our model is more sophisticated than the standard NB classifier in structure. Therefore, the usual trade-off between complexity and accuracy matters here. However, attention to the properties of each algorithm facilitates the fitting procedure and leads to more accurate results.</p><p>In the context of adjusting the naive Bayesian classifier when the conditional assumptions are violated, latent variable models emerge as one of the optimal solutions <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b40">41]</ref>. This assumption often fails to capture complex relationships and dependencies among features, leading to suboptimal performance. To overcome these limitations, latent variable models offer a powerful framework. By introducing latent variables, these models can capture the hidden dependencies and relationships among features, even in cases where the conditional independence assumption is violated <ref type="bibr" target="#b2">[3]</ref>. The inclusion of latent variables allows for more flexible and expressive modeling, enabling the representation of intricate interactions among features <ref type="bibr" target="#b2">[3]</ref>.</p><p>One key advantage of latent variable models is their ability to handle missing data and incomplete feature sets <ref type="bibr" target="#b41">[42]</ref>. By incorporating latent variables, these models can effectively impute missing values, mitigating the impact of incomplete information on classification accuracy. This is particularly valuable in real-world scenarios where data may be incomplete or contain missing values <ref type="bibr" target="#b42">[43]</ref>. Furthermore, latent variable models provide a means to account for unobserved or latent factors that may influence the observed features <ref type="bibr" target="#b43">[44]</ref>. By capturing these latent factors, the models can better explain the underlying data distribution and improve classification performance. Another benefit of latent variable models is their ability to offer principled probabilistic inference <ref type="bibr" target="#b44">[45]</ref>. This allows for robust uncertainty quantification and provides richer insights into the model's predictions. By understanding the uncertainty associated with the predictions, decision-makers can make more informed choices based on the level of confidence or uncertainty in the classification results.</p><p>In summary, when the conditional assumptions of the naive Bayesian classifier are violated, latent variable models serve as an optimal solution. By incorporating latent variables, these models capture hidden dependencies, handle missing data, account for unobserved factors, and offer principled probabilistic inference. Their ability to address the limitations of the naive Bayesian classifier makes latent variable models a valuable tool for improving classification performance in scenarios where conditional assumptions are not met.</p><p>The Gibbs sampler is one of the most efficient and wellknown MCMC algorithms. This algorithm is a special case of Metropolis-Hasting sampling wherein the randomly generated values are always accepted. It works based on the Markov property and generates random samples from the univariate conditional posterior distributions instead of an expensive joint distribution <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b45">46]</ref>. Therefore, the Gibbs sampler leads to the answers more quickly and needs less computational complexity. However, the samples achieved from this approach still are highly correlated. In this situation, thinning the samples has been suggested to make samples independent. It means picking separated points from the generated chain systematically <ref type="bibr" target="#b46">[47]</ref>. Separating the samples from the Markov chain dilutes the dependency and makes them independent. Another drawback of MCMC methods is the impact of misspecification of the initial values on the convergence of the chain. Fortunately, in most cases, the chain corrects itself at each scan, and we ensure that the later samples reflect the actual posterior distribution <ref type="bibr" target="#b47">[48]</ref>. Therefore, the only task we need is to burn in the initial values of the chain. Typically references suggest a basic rule of the first 1000 to 5000 sample burn-in <ref type="bibr" target="#b48">[49]</ref>. The other proposes a more conservative approach to selecting the starting value close to the distribution mode achieved from a likelihood-based model <ref type="bibr" target="#b49">[50]</ref>. We can use all these considerations to ensure chain convergence by correctly tuning the parameters.</p><p>As we confronted here, the EM algorithm is widespread in the case of the mixture distribution <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b51">52]</ref>. However, such a method is not without drawbacks. For instance, there is no guarantee to achieve global optima. In addition, the real value near the boundary makes the estimations unstable. Using parametric bootstrap sampling and refitting the model could benefit these situations <ref type="bibr" target="#b29">[30]</ref>. Hence, we restarted all the processes in the EM algorithm ten times in the simulation study and real-world data example. This approach is not straightforward when we sample from low-probability groups.</p><p>To overcome this problem, using likelihood sampling and logic sampling methods have been proposed <ref type="bibr" target="#b52">[53]</ref>. Fortunately, due to appropriate prior distribution, Gibbs's sampler is not a case of this issue. In this study, Beta and Dirichlet priors are proper and conjugate for parameters of interest <ref type="bibr" target="#b53">[54]</ref>.</p><p>The NB-BLCA model needs to determine the number of latent class variables and the number of levels for each of them. Data gathering in many medical and health applications starts after determining risk factors, influential predictors, and related domains <ref type="bibr" target="#b4">[5]</ref>. Therefore, the specialist could supervise us in detecting the required latent variables. However, it is not a general rule, especially in data mining applications. More development seems necessary in this situation. On the other hand, the number of levels for each latent variable depends on the data. Like principal component analysis (PCA) and Explanatory Factor Analysis (EFA), the best choice of levels could be made using the scree plot <ref type="bibr" target="#b54">[55]</ref>. In this manner, AIC and BIC criteria for both Gibbs sampling and EM algorithm and DIC for Gibbs sampling could lead us to select the best choice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conclusion</head><p>The addition of a latent component to the NB classifier model offers numerous advantages when compared to other modification attempts. Firstly, it aligns well with the nature of the data, particularly within medical and health contexts. Furthermore, incorporating the latent component allows us to bypass the extensive search algorithm and structure learning required in the local learning and structure extension approach. By utilizing latent class variables, all attributes are incorporated into the model building process, unlike attribute selection approaches that may ignore certain variables and result in the loss of information. As a result, the NB-BLCA model emerges as a suitable alternative to ordinary NB classifiers, particularly when the assumption of independence is violated, especially in the domains of health and medicine.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>( 2 )</head><label>2</label><figDesc>P(C|x 1 , . . . , x m ) = P(C)P(x 1 , . . . , x m |C) P(x 1 , . . . , x m ) x i |C = c)P(C = c)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1</head><label>1</label><figDesc>Comparison of Naive Bayes classifier and alternative approach: marginal probability of class variable = 0.3</figDesc><table><row><cell>Model</cell><cell>Number of</cell><cell>Sample size</cell><cell>Sensitivity</cell><cell>Specificity</cell><cell>Positive</cell><cell>Negative</cell><cell>Precision</cell></row><row><cell></cell><cell>predictors</cell><cell></cell><cell></cell><cell></cell><cell>predictive value</cell><cell>predictive value</cell><cell></cell></row><row><cell>NB</cell><cell>5</cell><cell>500</cell><cell>39.21</cell><cell>86.81</cell><cell>58.13</cell><cell>76.05</cell><cell>58.13</cell></row><row><cell>AODE</cell><cell>5</cell><cell>500</cell><cell>36.78</cell><cell>88.82</cell><cell>60.64</cell><cell>75.71</cell><cell>60.64</cell></row><row><cell>TAN</cell><cell>5</cell><cell>500</cell><cell>38.17</cell><cell>87.81</cell><cell>59.31</cell><cell>75.93</cell><cell>59.31</cell></row><row><cell>NB-BLCA (EM)</cell><cell>5</cell><cell>500</cell><cell>35.46</cell><cell>90.69</cell><cell>64.13</cell><cell>75.69</cell><cell>64.13</cell></row><row><cell>NB</cell><cell>5</cell><cell>1000</cell><cell>38.98</cell><cell>86.7</cell><cell>57.76</cell><cell>75.96</cell><cell>57.76</cell></row><row><cell>AODE</cell><cell>5</cell><cell>1000</cell><cell>35.8</cell><cell>88.76</cell><cell>59.8</cell><cell>75.43</cell><cell>59.8</cell></row><row><cell>TAN</cell><cell>5</cell><cell>1000</cell><cell>37.61</cell><cell>87.61</cell><cell>58.56</cell><cell>75.73</cell><cell>58.56</cell></row><row><cell>NB-BLCA (EM)</cell><cell>5</cell><cell>1000</cell><cell>33.25</cell><cell>90.74</cell><cell>62.45</cell><cell>75.06</cell><cell>62.45</cell></row><row><cell>NB</cell><cell>5</cell><cell>2000</cell><cell>38.75</cell><cell>86.64</cell><cell>57.56</cell><cell>75.86</cell><cell>57.56</cell></row><row><cell>AODE</cell><cell>5</cell><cell>2000</cell><cell>35.71</cell><cell>88.51</cell><cell>59.23</cell><cell>75.32</cell><cell>59.23</cell></row><row><cell>TAN</cell><cell>5</cell><cell>2000</cell><cell>37.36</cell><cell>87.49</cell><cell>58.24</cell><cell>75.61</cell><cell>58.24</cell></row><row><cell>NB-BLCA (EM)</cell><cell>5</cell><cell>2000</cell><cell>31.74</cell><cell>90.81</cell><cell>61.35</cell><cell>74.6</cell><cell>61.35</cell></row><row><cell>NB</cell><cell>10</cell><cell>500</cell><cell>49.48</cell><cell>87.13</cell><cell>63.48</cell><cell>79.37</cell><cell>63.48</cell></row><row><cell>AODE</cell><cell>10</cell><cell>500</cell><cell>49.86</cell><cell>88.27</cell><cell>65.81</cell><cell>79.7</cell><cell>65.81</cell></row><row><cell>TAN</cell><cell>10</cell><cell>500</cell><cell>49.92</cell><cell>87.69</cell><cell>64.73</cell><cell>79.61</cell><cell>64.73</cell></row><row><cell>NB-BLCA (EM)</cell><cell>10</cell><cell>500</cell><cell>66.38</cell><cell>92.52</cell><cell>80.08</cell><cell>86.04</cell><cell>80.08</cell></row><row><cell>NB</cell><cell>10</cell><cell>1000</cell><cell>48.47</cell><cell>87.26</cell><cell>63.19</cell><cell>79.06</cell><cell>63.19</cell></row><row><cell>AODE</cell><cell>10</cell><cell>1000</cell><cell>48.34</cell><cell>88.1</cell><cell>64.71</cell><cell>79.18</cell><cell>64.71</cell></row><row><cell>TAN</cell><cell>10</cell><cell>1000</cell><cell>48.51</cell><cell>87.51</cell><cell>63.66</cell><cell>79.12</cell><cell>63.66</cell></row><row><cell>NB-BLCA (EM)</cell><cell>10</cell><cell>1000</cell><cell>59.43</cell><cell>91.62</cell><cell>76.19</cell><cell>83.46</cell><cell>76.19</cell></row><row><cell>NB</cell><cell>10</cell><cell>2000</cell><cell>48.49</cell><cell>87.1</cell><cell>62.94</cell><cell>79</cell><cell>62.94</cell></row><row><cell>AODE</cell><cell>10</cell><cell>2000</cell><cell>48.12</cell><cell>87.74</cell><cell>63.97</cell><cell>79.01</cell><cell>63.97</cell></row><row><cell>TAN</cell><cell>10</cell><cell>2000</cell><cell>48.41</cell><cell>87.2</cell><cell>63.08</cell><cell>79</cell><cell>63.08</cell></row><row><cell>NB-BLCA (EM)</cell><cell>10</cell><cell>2000</cell><cell>54.27</cell><cell>90.92</cell><cell>72.96</cell><cell>81.58</cell><cell>72.96</cell></row><row><cell>NB</cell><cell>20</cell><cell>500</cell><cell>59.13</cell><cell>89.33</cell><cell>71.18</cell><cell>83.12</cell><cell>71.18</cell></row><row><cell>AODE</cell><cell>20</cell><cell>500</cell><cell>62.15</cell><cell>90.31</cell><cell>74.12</cell><cell>84.32</cell><cell>74.12</cell></row><row><cell>TAN</cell><cell>20</cell><cell>500</cell><cell>60.33</cell><cell>89.8</cell><cell>72.52</cell><cell>83.61</cell><cell>72.52</cell></row><row><cell>NB-BLCA (EM)</cell><cell>20</cell><cell>500</cell><cell>98.73</cell><cell>99.69</cell><cell>99.31</cell><cell>99.43</cell><cell>99.31</cell></row><row><cell>NB</cell><cell>20</cell><cell>1000</cell><cell>57.66</cell><cell>89.26</cell><cell>70.54</cell><cell>82.57</cell><cell>70.54</cell></row><row><cell>AODE</cell><cell>20</cell><cell>1000</cell><cell>59.26</cell><cell>89.82</cell><cell>72.21</cell><cell>83.2</cell><cell>72.21</cell></row><row><cell>TAN</cell><cell>20</cell><cell>1000</cell><cell>57.91</cell><cell>89.4</cell><cell>70.91</cell><cell>82.67</cell><cell>70.91</cell></row><row><cell>NB-BLCA (EM)</cell><cell>20</cell><cell>1000</cell><cell>97.63</cell><cell>99.33</cell><cell>98.48</cell><cell>98.95</cell><cell>98.48</cell></row><row><cell>NB</cell><cell>20</cell><cell>2000</cell><cell>57.29</cell><cell>89.12</cell><cell>70.04</cell><cell>82.47</cell><cell>70.04</cell></row><row><cell>AODE</cell><cell>20</cell><cell>2000</cell><cell>58.14</cell><cell>89.44</cell><cell>70.96</cell><cell>82.81</cell><cell>70.96</cell></row><row><cell>TAN</cell><cell>20</cell><cell>2000</cell><cell>57.31</cell><cell>89.13</cell><cell>70.07</cell><cell>82.48</cell><cell>70.07</cell></row><row><cell>NB-BLCA (EM)</cell><cell>20</cell><cell>2000</cell><cell>95.86</cell><cell>98.8</cell><cell>97.24</cell><cell>98.18</cell><cell>97.24</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2</head><label>2</label><figDesc>Comparison of Naive Bayes classifier and alternative approach: marginal probability of class variable = 0.5</figDesc><table><row><cell>Model</cell><cell>Number of</cell><cell>Sample size</cell><cell>Sensitivity</cell><cell>Specificity</cell><cell>Positive</cell><cell>Negative</cell><cell>Precision</cell></row><row><cell></cell><cell>predictors</cell><cell></cell><cell></cell><cell></cell><cell>predictive value</cell><cell>predictive value</cell><cell></cell></row><row><cell>NB</cell><cell>5</cell><cell>500</cell><cell>64.5</cell><cell>67.25</cell><cell>66.69</cell><cell>65.72</cell><cell>66.69</cell></row><row><cell>AODE</cell><cell>5</cell><cell>500</cell><cell>65.28</cell><cell>67.51</cell><cell>67.16</cell><cell>66.33</cell><cell>67.16</cell></row><row><cell>TAN</cell><cell>5</cell><cell>500</cell><cell>64.96</cell><cell>67.43</cell><cell>66.97</cell><cell>66.08</cell><cell>66.97</cell></row><row><cell>NB-BLCA (EM)</cell><cell>5</cell><cell>500</cell><cell>65.75</cell><cell>68.97</cell><cell>68.32</cell><cell>67.07</cell><cell>68.32</cell></row><row><cell>NB</cell><cell>5</cell><cell>1000</cell><cell>64.36</cell><cell>67.06</cell><cell>66.39</cell><cell>65.71</cell><cell>66.39</cell></row><row><cell>AODE</cell><cell>5</cell><cell>1000</cell><cell>64.73</cell><cell>67.24</cell><cell>66.69</cell><cell>66.02</cell><cell>66.69</cell></row><row><cell>TAN</cell><cell>5</cell><cell>1000</cell><cell>64.63</cell><cell>67.07</cell><cell>66.51</cell><cell>65.87</cell><cell>66.51</cell></row><row><cell>NB-BLCA (EM)</cell><cell>5</cell><cell>1000</cell><cell>64.93</cell><cell>68.12</cell><cell>67.25</cell><cell>66.33</cell><cell>67.25</cell></row><row><cell>NB</cell><cell>5</cell><cell>2000</cell><cell>64.39</cell><cell>66.82</cell><cell>66.34</cell><cell>65.59</cell><cell>66.34</cell></row><row><cell>AODE</cell><cell>5</cell><cell>2000</cell><cell>64.43</cell><cell>67.1</cell><cell>66.57</cell><cell>65.73</cell><cell>66.57</cell></row><row><cell>TAN</cell><cell>5</cell><cell>2000</cell><cell>64.34</cell><cell>66.96</cell><cell>66.42</cell><cell>65.61</cell><cell>66.42</cell></row><row><cell>NB-BLCA (EM)</cell><cell>5</cell><cell>2000</cell><cell>64.39</cell><cell>67.69</cell><cell>66.77</cell><cell>65.75</cell><cell>66.77</cell></row><row><cell>NB</cell><cell>10</cell><cell>500</cell><cell>70.26</cell><cell>71.57</cell><cell>71.31</cell><cell>70.67</cell><cell>71.31</cell></row><row><cell>AODE</cell><cell>10</cell><cell>500</cell><cell>71.08</cell><cell>72.65</cell><cell>72.33</cell><cell>71.57</cell><cell>72.33</cell></row><row><cell>TAN</cell><cell>10</cell><cell>500</cell><cell>70.81</cell><cell>72.26</cell><cell>71.97</cell><cell>71.24</cell><cell>71.97</cell></row><row><cell>NB-BLCA (EM)</cell><cell>10</cell><cell>500</cell><cell>79.43</cell><cell>82.61</cell><cell>82.16</cell><cell>80.19</cell><cell>82.16</cell></row><row><cell>NB</cell><cell>10</cell><cell>1000</cell><cell>70.08</cell><cell>71.25</cell><cell>71</cell><cell>70.45</cell><cell>71</cell></row><row><cell>AODE</cell><cell>10</cell><cell>1000</cell><cell>70.58</cell><cell>71.84</cell><cell>71.57</cell><cell>70.98</cell><cell>71.57</cell></row><row><cell>TAN</cell><cell>10</cell><cell>1000</cell><cell>70.28</cell><cell>71.41</cell><cell>71.17</cell><cell>70.64</cell><cell>71.17</cell></row><row><cell>NB-BLCA (EM)</cell><cell>10</cell><cell>1000</cell><cell>76.36</cell><cell>79.27</cell><cell>78.75</cell><cell>77.12</cell><cell>78.75</cell></row><row><cell>NB</cell><cell>10</cell><cell>2000</cell><cell>69.95</cell><cell>70.95</cell><cell>70.67</cell><cell>70.34</cell><cell>70.67</cell></row><row><cell>AODE</cell><cell>10</cell><cell>2000</cell><cell>70.26</cell><cell>71.26</cell><cell>71</cell><cell>70.66</cell><cell>71</cell></row><row><cell>TAN</cell><cell>10</cell><cell>2000</cell><cell>69.98</cell><cell>71.01</cell><cell>70.73</cell><cell>70.38</cell><cell>70.73</cell></row><row><cell>NB-BLCA (EM)</cell><cell>10</cell><cell>2000</cell><cell>73.8</cell><cell>76.62</cell><cell>75.96</cell><cell>74.64</cell><cell>75.96</cell></row><row><cell>NB</cell><cell>20</cell><cell>500</cell><cell>75.97</cell><cell>76.79</cell><cell>76.62</cell><cell>76.22</cell><cell>76.62</cell></row><row><cell>AODE</cell><cell>20</cell><cell>500</cell><cell>77.43</cell><cell>78.88</cell><cell>78.6</cell><cell>77.82</cell><cell>78.6</cell></row><row><cell>TAN</cell><cell>20</cell><cell>500</cell><cell>76.77</cell><cell>77.59</cell><cell>77.43</cell><cell>77.02</cell><cell>77.43</cell></row><row><cell>NB-BLCA (EM)</cell><cell>20</cell><cell>500</cell><cell>99.01</cell><cell>99.36</cell><cell>99.36</cell><cell>99.01</cell><cell>99.36</cell></row><row><cell>NB</cell><cell>20</cell><cell>1000</cell><cell>75.43</cell><cell>76.19</cell><cell>76.03</cell><cell>75.63</cell><cell>76.03</cell></row><row><cell>AODE</cell><cell>20</cell><cell>1000</cell><cell>76.24</cell><cell>77.34</cell><cell>77.11</cell><cell>76.51</cell><cell>77.11</cell></row><row><cell>TAN</cell><cell>20</cell><cell>1000</cell><cell>75.7</cell><cell>76.42</cell><cell>76.27</cell><cell>75.89</cell><cell>76.27</cell></row><row><cell>NB-BLCA (EM)</cell><cell>20</cell><cell>1000</cell><cell>98.13</cell><cell>98.62</cell><cell>98.61</cell><cell>98.15</cell><cell>98.61</cell></row><row><cell>NB</cell><cell>20</cell><cell>2000</cell><cell>75.39</cell><cell>75.99</cell><cell>75.89</cell><cell>75.52</cell><cell>75.89</cell></row><row><cell>AODE</cell><cell>20</cell><cell>2000</cell><cell>75.83</cell><cell>76.64</cell><cell>76.49</cell><cell>76</cell><cell>76.49</cell></row><row><cell>TAN</cell><cell>20</cell><cell>2000</cell><cell>75.41</cell><cell>76.02</cell><cell>75.92</cell><cell>75.54</cell><cell>75.92</cell></row><row><cell>NB-BLCA (EM)</cell><cell>20</cell><cell>2000</cell><cell>96.92</cell><cell>97.58</cell><cell>97.56</cell><cell>96.94</cell><cell>97.56</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3</head><label>3</label><figDesc>Comparison of Naive Bayes classifier and alternative approach: marginal probability of class variable = 0.7</figDesc><table><row><cell>Model</cell><cell>Number of</cell><cell>Sample size</cell><cell>Sensitivity</cell><cell>Specificity</cell><cell>Positive</cell><cell>Negative</cell><cell>Precision</cell></row><row><cell></cell><cell>predictors</cell><cell></cell><cell></cell><cell></cell><cell>predictive value</cell><cell>predictive value</cell><cell></cell></row><row><cell>NB</cell><cell>5</cell><cell>500</cell><cell>85.99</cell><cell>38.59</cell><cell>75.81</cell><cell>56.38</cell><cell>75.81</cell></row><row><cell>AODE</cell><cell>5</cell><cell>500</cell><cell>88.17</cell><cell>35.32</cell><cell>75.27</cell><cell>58.62</cell><cell>75.27</cell></row><row><cell>TAN</cell><cell>5</cell><cell>500</cell><cell>87.08</cell><cell>37.15</cell><cell>75.6</cell><cell>57.46</cell><cell>75.6</cell></row><row><cell>NB-BLCA (EM)</cell><cell>5</cell><cell>500</cell><cell>89.84</cell><cell>33.82</cell><cell>75.17</cell><cell>60.56</cell><cell>75.17</cell></row><row><cell>NB</cell><cell>5</cell><cell>1000</cell><cell>85.99</cell><cell>38.65</cell><cell>75.8</cell><cell>56.26</cell><cell>75.8</cell></row><row><cell>AODE</cell><cell>5</cell><cell>1000</cell><cell>88.21</cell><cell>34.98</cell><cell>75.16</cell><cell>58.03</cell><cell>75.16</cell></row><row><cell>TAN</cell><cell>5</cell><cell>1000</cell><cell>87.06</cell><cell>36.96</cell><cell>75.51</cell><cell>57.09</cell><cell>75.51</cell></row><row><cell>NB-BLCA (EM)</cell><cell>5</cell><cell>1000</cell><cell>90.06</cell><cell>32.09</cell><cell>74.67</cell><cell>59.26</cell><cell>74.67</cell></row><row><cell>NB</cell><cell>5</cell><cell>2000</cell><cell>85.87</cell><cell>38.67</cell><cell>75.76</cell><cell>56.05</cell><cell>75.76</cell></row><row><cell>AODE</cell><cell>5</cell><cell>2000</cell><cell>88.22</cell><cell>34.61</cell><cell>75.04</cell><cell>57.96</cell><cell>75.04</cell></row><row><cell>TAN</cell><cell>5</cell><cell>2000</cell><cell>87.02</cell><cell>36.75</cell><cell>75.43</cell><cell>56.9</cell><cell>75.43</cell></row><row><cell>NB-BLCA (EM)</cell><cell>5</cell><cell>2000</cell><cell>90.41</cell><cell>30.56</cell><cell>74.27</cell><cell>58.84</cell><cell>74.27</cell></row><row><cell>NB</cell><cell>10</cell><cell>500</cell><cell>86.26</cell><cell>50.37</cell><cell>79.44</cell><cell>62.46</cell><cell>79.44</cell></row><row><cell>AODE</cell><cell>10</cell><cell>500</cell><cell>87.23</cell><cell>50.59</cell><cell>79.69</cell><cell>64.24</cell><cell>79.69</cell></row><row><cell>TAN</cell><cell>10</cell><cell>500</cell><cell>86.77</cell><cell>50.72</cell><cell>79.65</cell><cell>63.5</cell><cell>79.65</cell></row><row><cell>NB-BLCA (EM)</cell><cell>10</cell><cell>500</cell><cell>90.28</cell><cell>66.73</cell><cell>85.82</cell><cell>75.7</cell><cell>85.82</cell></row><row><cell>NB</cell><cell>10</cell><cell>1000</cell><cell>86.5</cell><cell>49.01</cell><cell>79.13</cell><cell>62.06</cell><cell>79.13</cell></row><row><cell>AODE</cell><cell>10</cell><cell>1000</cell><cell>87.23</cell><cell>48.7</cell><cell>79.16</cell><cell>63.2</cell><cell>79.16</cell></row><row><cell>TAN</cell><cell>10</cell><cell>1000</cell><cell>86.71</cell><cell>49.01</cell><cell>79.17</cell><cell>62.41</cell><cell>79.17</cell></row><row><cell>NB-BLCA (EM)</cell><cell>10</cell><cell>1000</cell><cell>89.47</cell><cell>59.65</cell><cell>83.24</cell><cell>71.79</cell><cell>83.24</cell></row><row><cell>NB</cell><cell>10</cell><cell>2000</cell><cell>86.39</cell><cell>48.7</cell><cell>78.99</cell><cell>61.76</cell><cell>78.99</cell></row><row><cell>AODE</cell><cell>10</cell><cell>2000</cell><cell>87.09</cell><cell>48.07</cell><cell>78.92</cell><cell>62.68</cell><cell>78.92</cell></row><row><cell>TAN</cell><cell>10</cell><cell>2000</cell><cell>86.56</cell><cell>48.51</cell><cell>78.96</cell><cell>61.96</cell><cell>78.96</cell></row><row><cell>NB-BLCA (EM)</cell><cell>10</cell><cell>2000</cell><cell>89.05</cell><cell>54.23</cell><cell>81.29</cell><cell>68.98</cell><cell>81.29</cell></row><row><cell>NB</cell><cell>20</cell><cell>500</cell><cell>88.79</cell><cell>59.25</cell><cell>83.13</cell><cell>70.11</cell><cell>83.13</cell></row><row><cell>AODE</cell><cell>20</cell><cell>500</cell><cell>89.44</cell><cell>62.7</cell><cell>84.44</cell><cell>72.5</cell><cell>84.44</cell></row><row><cell>TAN</cell><cell>20</cell><cell>500</cell><cell>89.25</cell><cell>60.34</cell><cell>83.58</cell><cell>71.37</cell><cell>83.58</cell></row><row><cell>NB-BLCA (EM)</cell><cell>20</cell><cell>500</cell><cell>99.4</cell><cell>98.97</cell><cell>99.54</cell><cell>98.67</cell><cell>99.54</cell></row><row><cell>NB</cell><cell>20</cell><cell>1000</cell><cell>88.62</cell><cell>57.86</cell><cell>82.58</cell><cell>69.35</cell><cell>82.58</cell></row><row><cell>AODE</cell><cell>20</cell><cell>1000</cell><cell>88.95</cell><cell>59.71</cell><cell>83.27</cell><cell>70.62</cell><cell>83.27</cell></row><row><cell>TAN</cell><cell>20</cell><cell>1000</cell><cell>88.74</cell><cell>58.05</cell><cell>82.66</cell><cell>69.64</cell><cell>82.66</cell></row><row><cell>NB-BLCA (EM)</cell><cell>20</cell><cell>1000</cell><cell>98.76</cell><cell>97.65</cell><cell>98.95</cell><cell>97.24</cell><cell>98.95</cell></row><row><cell>NB</cell><cell>20</cell><cell>2000</cell><cell>88.65</cell><cell>57.45</cell><cell>82.45</cell><cell>69.21</cell><cell>82.45</cell></row><row><cell>AODE</cell><cell>20</cell><cell>2000</cell><cell>88.82</cell><cell>58.48</cell><cell>82.83</cell><cell>69.91</cell><cell>82.83</cell></row><row><cell>TAN</cell><cell>20</cell><cell>2000</cell><cell>88.66</cell><cell>57.45</cell><cell>82.45</cell><cell>69.23</cell><cell>82.45</cell></row><row><cell>NB-BLCA (EM)</cell><cell>20</cell><cell>2000</cell><cell>97.94</cell><cell>95.63</cell><cell>98.06</cell><cell>95.38</cell><cell>98.06</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4</head><label>4</label><figDesc>Comparison between predictive indices of NB-BLCA models and ordinary NB in real-world data of GC patients</figDesc><table><row><cell></cell><cell>Model</cell><cell></cell><cell></cell></row><row><cell>Index</cell><cell>NB-BLCA</cell><cell>NB-BLCA</cell><cell>NB classifier</cell></row><row><cell></cell><cell>(EM algorithm)</cell><cell>(Gibbs sampling)</cell><cell></cell></row><row><cell>Accuracy (95% CI)</cell><cell>77.22 (73.64-80.53)</cell><cell>87.77 (84.87-90.29)</cell><cell>74.71 (71.02-78.15)</cell></row><row><cell>No information rate (NIR)</cell><cell>63.32</cell><cell>50.92</cell><cell>53.43</cell></row><row><cell>P-Value [Accuracy &gt; NIR]</cell><cell>&lt;0.001</cell><cell>&lt;0.001</cell><cell>&lt;0.001</cell></row><row><cell>Kappa</cell><cell>0.53</cell><cell>0.76</cell><cell>0.49</cell></row><row><cell>Mcnemar's Test P-Value</cell><cell>&lt;0.001</cell><cell>&lt;001</cell><cell>0.74</cell></row><row><cell>Sensitivity</cell><cell>81.28</cell><cell>82.89</cell><cell>71.94</cell></row><row><cell>Specificity</cell><cell>74.87</cell><cell>92.83</cell><cell>77.12</cell></row><row><cell>Pos Pred Value</cell><cell>65.2</cell><cell>92.31</cell><cell>73.26</cell></row><row><cell>Neg Pred Value</cell><cell>87.35</cell><cell>83.95</cell><cell>75.93</cell></row><row><cell>Balanced Accuracy</cell><cell>78.07</cell><cell>87.86</cell><cell>74.53</cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p>This study is a part of the research process supported by <rs type="funder">Tarbiat Modares University</rs> to achieve a Ph.D. degree.</p></div>
			</div>
			
			<div type="funding">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Funding</head><p>Not applicable.</p></div>
			</div>


			<listOrg type="funding">
			</listOrg>

			<div type="availability">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Availability of data and materials</head><p>The datasets used or analyzed during the current study are available from the corresponding author upon reasonable request.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abbreviations</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplementary Information</head><p>The online version contains supplementary material available at <ref type="url" target="https://doi">https:// doi</ref>. org/ 10. 1186/ s12874-023-02013-4.</p><p>Additional file 1: S-Table <ref type="table">1</ref>. List of questionnaire binary attributes with the categories used in the Real-world data example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Authors' contributions</head><p>AK and KG contributed to the study conception and design,AK, KG, and AS performed analysis. MM, ME, and SS collect data and describe the clinical result. KG wrote the first draft of the manuscript, and all authors commented on previous versions. All authors read and approved the final manuscript.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Declarations</head><p>Ethics approval and consent to participate All methods were carried out following relevant guidelines and regulations. This study was approved by the ethics committee of the school of medical sciences -Tarbiat Modares university under the approval ID IR.MODARES. REC.1399.154. All participants provided written informed consent that their data collected as part of the study could be used in research. All the patients were followed until the event or when they preferred to stop participation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Consent for publication</head><p>Not applicable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Competing interests</head><p>The authors declare no competing interests.</p><p>• fast, convenient online submission</p><p>• thorough peer review by experienced researchers in your field</p><p>• rapid publication on acceptance</p><p>• support for research data, including large and complex data types</p><p>• gold Open Access which fosters wider collaboration and increased citations maximum visibility for your research: over 100M website views per year</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>•</head><p>At BMC, research is always in progress.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Learn more biomedcentral.com/submissions</head><p>Ready to submit your research Ready to submit your research ? Choose BMC and benefit from: ? Choose BMC and benefit from:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Publisher's Note</head><p>Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Applying naive bayesian networks to disease prediction: a systematic review</title>
		<author>
			<persName><forename type="first">M</forename><surname>Langarizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Moghbeli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Informatica Medica</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">364</biblScope>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Sentiment analysis of user reviews on covid-19 information applications using naive bayes classifier, Support Vector Machine, and K-Nearest Neighbor</title>
		<author>
			<persName><forename type="first">A</forename><surname>Salma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Silfianti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int Res J Adv Eng Sci</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="158" to="162" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Pattern recognition and machine learning</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">M</forename><surname>Nasrabadi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Springer</publisher>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="738" to="838" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Investigating the statistical assumptions of Naïve Bayes classifiers</title>
		<author>
			<persName><forename type="first">A</forename><surname>Kelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">A</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2021 55th annual conference on information sciences and systems (CISS)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">2021</biblScope>
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Classical latent variable models for medical research</title>
		<author>
			<persName><forename type="first">S</forename><surname>Rabe-Hesketh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Skrondal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stat Methods Med Res</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="32" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Naive Bayes: applications, variations and vulnerabilities: a review of literature with code snippets for implementation</title>
		<author>
			<persName><forename type="first">I</forename><surname>Wickramasinghe</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kalutarage</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Soft Computing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="2277" to="2293" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Induction of selective Bayesian classifiers</title>
		<author>
			<persName><forename type="first">P</forename><surname>Langley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Sage</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1994">1994</date>
			<publisher>Elsevier</publisher>
			<biblScope unit="page" from="399" to="406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Medical datamining with a new algorithm for feature selection and naive Bayesian classifier</title>
		<author>
			<persName><forename type="first">R</forename><surname>Abraham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Simha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Iyengar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">10th International Conference on Information Technology (ICIT 2007)</title>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="44" to="49" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A novel feature selection technique for text classification using Naive Bayes</title>
		<author>
			<persName><forename type="first">Dey</forename><surname>Sarkar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Goswami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Aktar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int Sch Res Notices</title>
		<imprint>
			<biblScope unit="page">717092</biblScope>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A comparative study on feature selection methods for drug discovery</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Chem Inf Comp Sci</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1823" to="1828" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Feature selection for the naive bayesian classifier using decision trees</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">A</forename><surname>Ratanamahatana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gunopulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl Artif Intell</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">5-6</biblScope>
			<biblScope unit="page" from="475" to="487" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The impact of feature selection on the accuracy of naïve bayes classifier</title>
		<author>
			<persName><forename type="first">J</forename><surname>Novakovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">18th Telecommunications forum TELFOR</title>
		<imprint>
			<biblScope unit="page" from="1113" to="1116" />
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Automated feature weighting in naive bayes for high-dimensional data classification</title>
		<author>
			<persName><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st ACM international conference on Information and knowledge management</title>
		<meeting>the 21st ACM international conference on Information and knowledge management</meeting>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">2012</biblScope>
			<biblScope unit="page" from="1243" to="1252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Calculating feature weights in naive bayes with kullback-leibler measure</title>
		<author>
			<persName><forename type="first">C-H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Gutierrez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Dou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE 11th International Conference on data mining</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011. 2011</date>
			<biblScope unit="page" from="1146" to="1151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Feature weighting methods: A review</title>
		<author>
			<persName><forename type="first">I</forename><surname>Niño-Adan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Manjarres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Landa-Torres</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Portillo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Expert Syst Appl</title>
		<imprint>
			<biblScope unit="volume">184</biblScope>
			<biblScope unit="page">115424</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Efficient discriminative learning of bayesian network classifier via boosted augmented naive bayes</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Pavlović</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd international conference on Machine learning</title>
		<meeting>the 22nd international conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2005">2005. 2005</date>
			<biblScope unit="page" from="369" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">An improved learning algorithm for augmented naive Bayes</title>
		<author>
			<persName><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">X</forename><surname>Ling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pacific-Asia Conference on Knowledge Discovery and Data Mining</title>
		<meeting><address><addrLine>Heidelberg; Berlin Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="581" to="586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Structure extension of tree-augmented naive bayes</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Entropy</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">721</biblScope>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">European Workshop on Probabilistic Graphical Models</title>
		<author>
			<persName><forename type="first">Campos</forename><surname>Cpd</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Cuccu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Corani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Zaffalon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>Springer International Publishing</publisher>
			<biblScope unit="page" from="176" to="189" />
			<pubPlace>Utrecht</pubPlace>
		</imprint>
	</monogr>
	<note>Extended tree augmented naive classifier</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">K-dependence Bayesian classifier ensemble</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Entropy</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">651</biblScope>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Not so naive Bayes: aggregating onedependence estimators</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">I</forename><surname>Webb</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">R</forename><surname>Boughton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">58</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="24" />
			<date type="published" when="2005">2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Discrete Bayesian network classifiers: A survey</title>
		<author>
			<persName><forename type="first">C</forename><surname>Bielza</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Larranaga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Computing Surveys (CSUR)</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="43" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multi independent latent component extension of naive bayes classifier</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Alizadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hediehloo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">S</forename><surname>Harzevili</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowl Based Syst</title>
		<imprint>
			<biblScope unit="volume">213</biblScope>
			<biblScope unit="page">106646</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Latent Dirichlet conditional naive-Bayes models</title>
		<author>
			<persName><forename type="first">A</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Seventh IEEE International Conference on Data Mining (ICDM 2007)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007">2007. 2007</date>
			<biblScope unit="page" from="421" to="426" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Mixture of latent multinomial naive Bayes classifier</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">S</forename><surname>Harzevili</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">H</forename><surname>Alizadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Appl Soft Computing</title>
		<imprint>
			<biblScope unit="volume">69</biblScope>
			<biblScope unit="page" from="516" to="527" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Latent variable mixture modeling in psychiatric research-a review and application</title>
		<author>
			<persName><forename type="first">J</forename><surname>Miettunen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Nordström</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kaakinen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ahmed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychol Med</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="457" to="467" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Latent variable and clustering methods in intersectionality research: systematic review of methods applications</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">R</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mahendran</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Walwyn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shokoohi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Social psychiatry and psychiatric epidemiology</title>
		<imprint>
			<biblScope unit="volume">2022</biblScope>
			<biblScope unit="page" from="1" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Classification using hierarchical naive Bayes models</title>
		<author>
			<persName><forename type="first">H</forename><surname>Langseth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Nielsen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mach learn</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="135" to="159" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Three naive Bayes approaches for discrimination-free classification</title>
		<author>
			<persName><forename type="first">T</forename><surname>Calders</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Verwer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Min Knowl Discov</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="277" to="292" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Bayesian latent class analysis tutorial</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Lord-Bessen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shiyko</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Loeb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multivariate Behav Res</title>
		<imprint>
			<biblScope unit="volume">53</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="430" to="451" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Using Bayesian priors for more flexible latent class analysis</title>
		<author>
			<persName><forename type="first">T</forename><surname>Asparouhov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Muthén</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the 2011 joint statistical meeting</title>
		<meeting>the 2011 joint statistical meeting<address><addrLine>Miami Beach, FL; Alexandria, VA</addrLine></address></meeting>
		<imprint>
			<publisher>American Statistical Association</publisher>
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">The EM Algorithm and Extensions</title>
		<author>
			<persName><forename type="first">G</forename><surname>Mclachlan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Krishnan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>Wiley</publisher>
			<biblScope unit="page">382</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Theory and use of the EM algorithm</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">R</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends ® in Signal Processing</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="223" to="296" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">BayesLCA: An R package for Bayesian latent class analysis</title>
		<author>
			<persName><forename type="first">A</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">B</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Stat Softw</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">13</biblScope>
			<biblScope unit="page" from="1" to="28" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Markov chain monte carlo and gibbs sampling</title>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Carlo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lecture Notes EEB</title>
		<imprint>
			<biblScope unit="volume">581</biblScope>
			<biblScope unit="page">540</biblScope>
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Bayesian ideas and data analysis: an introduction for scientists and statisticians</title>
		<author>
			<persName><forename type="first">R</forename><surname>Christensen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Johnson</forename><forename type="middle">W</forename><surname>Branscum</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Hanson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">E</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
			<publisher>CRC Press, Taylor and Francis Group</publisher>
			<pubPlace>Boca Ranton</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning the structure of augmented Bayesian classifiers</title>
		<author>
			<persName><forename type="first">E</forename><forename type="middle">J</forename><surname>Keogh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Pazzani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int J Artif Intell Tools</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">04</biblScope>
			<biblScope unit="page" from="587" to="601" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">On a least squares adjustment of a sampled frequency table when the expected marginal totals are known</title>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">E</forename><surname>Deming</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">F</forename><surname>Stephan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann Math Stat</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="427" to="444" />
			<date type="published" when="1940">1940</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Estimating crossclassified population counts of multidimensional tables: an application to regional Australia to obtain pseudo-census counts</title>
		<author>
			<persName><forename type="first">T</forename><surname>Suesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M-R</forename><surname>Namazi-Rad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Mokhtarian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Barthelemy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">mipfp: An R package for multidimensional array fitting and simulating multivariate Bernoulli distributions</title>
		<author>
			<persName><forename type="first">J</forename><surname>Barthélemy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Suesse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J Stat Softw</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Latent variable discovery in classification models</title>
		<author>
			<persName><forename type="first">N</forename><forename type="middle">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">D</forename><surname>Nielsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">V</forename><surname>Jensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artif Intell Med</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="283" to="299" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Inference and missing data</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">63</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="581" to="592" />
			<date type="published" when="1976">1976</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Statistical analysis with missing data</title>
		<author>
			<persName><forename type="first">Rja</forename><surname>Little</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rubin</forename><surname>Db</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NJ</title>
		<imprint>
			<biblScope unit="page">793</biblScope>
			<date type="published" when="2020">2020</date>
			<publisher>Wiley</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Mixtures of probabilistic principal component analyzers</title>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">E</forename><surname>Tipping</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="443" to="482" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Propagation algorithms for variational Bayesian learning</title>
		<author>
			<persName><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Beal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in neural information processing systems</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">On particle Gibbs sampling</title>
		<author>
			<persName><forename type="first">N</forename><surname>Chopin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">S</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bernoulli</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1855" to="1883" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Bayesian computation and stochastic systems</title>
		<author>
			<persName><forename type="first">J</forename><surname>Besag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Higdon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Mengersen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistical science</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3" to="41" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Convergence analysis of a collapsed Gibbs sampler for Bayesian vector autoregressions</title>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">O</forename><surname>Ekvall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">L</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electron J Stat</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="691" to="721" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Sufficient burn-in for Gibbs samplers for a hierarchical random effects model</title>
		<author>
			<persName><forename type="first">G</forename><forename type="middle">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">P</forename><surname>Hobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann Stat</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="784" to="817" />
			<date type="published" when="2004">2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">An insight into the Gibbs sampler: keep the samples or drop them?</title>
		<author>
			<persName><forename type="first">J</forename><surname>Boissy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J-F</forename><surname>Giovannelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Minvielle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Process Lett</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2069" to="2073" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Finite mixture distributions, sequential likelihood and the EM algorithm</title>
		<author>
			<persName><forename type="first">P</forename><surname>Arcidiacono</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">B</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econometrica</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="933" to="946" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Another interpretation of the EM algorithm for mixture distributions</title>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Hathaway</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stat Probab Lett</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="53" to="56" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Latent class modeling with covariates: Two improved threestep approaches</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">K</forename><surname>Vermunt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Political Analy</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="450" to="469" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Gibbs sampling, conjugate priors and coupling</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diaconis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Khare</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Saloff-Coste</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sankhya A</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="136" to="169" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Automatic dimensionality selection from the scree plot via the use of profile likelihood</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Ghodsi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Comput Stat Data Anal</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="918" to="930" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
