<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Generative Model for Joint Natural Language Understanding and Generation</title>
				<funder>
					<orgName type="full">Cambridge Trust</orgName>
				</funder>
				<funder ref="#_bxDSQ59">
					<orgName type="full">EPSRC</orgName>
				</funder>
				<funder>
					<orgName type="full">Ministry of Education, Taiwan</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Bo-Hsiang</forename><surname>Tseng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Engineering Department</orgName>
								<orgName type="institution">University of Cambridge</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jianpeng</forename><surname>Cheng</surname></persName>
							<email>jianpeng.cheng@apple.com</email>
							<affiliation key="aff0">
								<orgName type="department">Engineering Department</orgName>
								<orgName type="institution">University of Cambridge</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yimai</forename><surname>Fang</surname></persName>
							<email>yimai_fang@apple.com</email>
							<affiliation key="aff0">
								<orgName type="department">Engineering Department</orgName>
								<orgName type="institution">University of Cambridge</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">David</forename><surname>Vandyke</surname></persName>
							<email>dvandyke@apple.com</email>
							<affiliation key="aff0">
								<orgName type="department">Engineering Department</orgName>
								<orgName type="institution">University of Cambridge</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Generative Model for Joint Natural Language Understanding and Generation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-21T19:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Natural language understanding (NLU) and natural language generation (NLG) are two fundamental and related tasks in building task-oriented dialogue systems with opposite objectives: NLU tackles the transformation from natural language to formal representations, whereas NLG does the reverse. A key to success in either task is parallel training data which is expensive to obtain at a large scale. In this work, we propose a generative model which couples NLU and NLG through a shared latent variable. This approach allows us to explore both spaces of natural language and formal representations, and facilitates information sharing through the latent space to eventually benefit NLU and NLG. Our model achieves state-of-the-art performance on two dialogue datasets with both flat and tree-structured formal representations. We also show that the model can be trained in a semi-supervised fashion by utilising unlabelled data to boost its performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Natural language understanding (NLU) and natural language generation (NLG) are two fundamental tasks in building task-oriented dialogue systems. In a modern dialogue system, an NLU module first converts a user utterance, provided by an automatic speech recognition model, into a formal representation. The representation is then consumed by a downstream dialogue state tracker to update a belief state which represents an aggregated user goal. Based on the current belief state, a policy network decides the formal representation of the system response. This is finally used by an NLG module to generate the system response <ref type="bibr" target="#b36">(Young et al., 2010)</ref>.</p><p>It can be observed that NLU and NLG have opposite goals: NLU aims to map natural language to formal representations, while NLG generates utterances from their semantics. In research literature, NLU and NLG are well-studied as separate problems. State-of-the-art NLU systems tackle the task as classification <ref type="bibr" target="#b40">(Zhang and Wang, 2016)</ref> or as structured prediction or generation <ref type="bibr" target="#b7">(Damonte et al., 2019)</ref>, depending on the formal representations which can be flat slot-value pairs <ref type="bibr" target="#b15">(Henderson et al., 2014)</ref>, first-order logical form <ref type="bibr" target="#b39">(Zettlemoyer and Collins, 2012)</ref>, or structured queries <ref type="bibr" target="#b37">(Yu et al., 2018;</ref><ref type="bibr" target="#b24">Pasupat et al., 2019)</ref>. On the other hand, approaches to NLG vary from pipelined approach subsuming content planning and surface realisation <ref type="bibr">(Stent et al., 2004)</ref> to more recent end-to-end sequence generation <ref type="bibr" target="#b32">(Wen et al., 2015;</ref><ref type="bibr" target="#b11">Dušek et al., 2020)</ref>.</p><p>However, the duality between NLU and NLG has been less explored. In fact, both tasks can be treated as a translation problem: NLU converts natural language to formal language while NLG does the reverse. Both tasks require a substantial amount of utterance and representation pairs to succeed, and such data is costly to collect due to the complexity of annotation involved. Although unannotated data for either natural language or formal representations can be easily obtained, it is less clear how they can be leveraged as the two languages stand in different space.</p><p>In this paper, we propose a generative model for Joint natural language Understanding and Generation (JUG), which couples NLU and NLG with a latent variable representing the shared intent between natural language and formal representations. We aim to learn the association between two discrete spaces through a continuous latent variable which facilitates information sharing between two tasks. Moreover, JUG can be trained in a semi-supervised fashion, which enables us to explore each space of natural language and formal representations when unlabelled data is accessible. We examine our model on two dialogue datasets with different formal representations: the E2E dataset <ref type="bibr" target="#b23">(Novikova et al., 2017)</ref> where the semantics are represented as a collection of slot-value pairs; and a more recent weather dataset <ref type="bibr" target="#b1">(Balakrishnan et al., 2019)</ref> where the formal representations are tree-structured. Experimental results show that our model improves over standalone NLU/NLG models and existing methods on both tasks; and the performance can be further boosted by utilising unlabelled data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Model</head><p>Our key assumption is that there exists an abstract latent variable z underlying a pair of utterance x and formal representation y. In our generative model, this abstract intent guides the standard conditional generation of either NLG or NLU (Figure <ref type="figure" target="#fig_0">1a</ref>). Meanwhile, z can be inferred from either utterance x, or formal representation y (Figure <ref type="figure" target="#fig_0">1b</ref>). That means performing NLU requires us to infer the z from x, after which the formal representation y is generated conditioning on both z and x (Figure <ref type="figure" target="#fig_0">1c</ref>), and vice-versa for NLG (Figure <ref type="figure" target="#fig_0">1d</ref>). In the following, we will explain the model details, starting with NLG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">NLG</head><p>As mentioned above, the task of NLG requires us to infer z from y, and then generate x using both z and y. We choose the posterior distribution q(z|y) to be Gaussian. The task of inferring z can then be recast to computing mean µ and standard deviation σ of the Gaussian distribution using an NLG encoder. To do this, we use a bi-directional LSTM <ref type="bibr">(Hochreiter and Schmidhuber, 1997)</ref> to encode formal representation y. which is linearised and represented as a sequence of symbols. After encoding, we obtain a list of hidden vectors H, with each representing the concatenation of forward and backward LSTM states. These hidden vectors are then average-pooled and passed through two feedforward neural networks to compute mean µ µ µ y,z and standard deviation σ σ σ y,z vectors of the posterior q(z|y).</p><formula xml:id="formula_0">H = Bi-LSTM(y) h = Pooling(H) µ µ µ y,z = W µ h + b µ σ σ σ y,z = W σ h + b σ (1)</formula><p>where W and b represent neural network weights and bias. Then the latent vector z can be sampled from the approximated posterior using the re-parameterisation trick of Kingma and Welling (2013):</p><formula xml:id="formula_1">∼ N (0, I) z = µ µ µ y,z + σ σ σ y,z<label>(2)</label></formula><p>The final step is to generate natural language x based on latent variable z and formal representation y. We use an LSTM decoder relying on both z and y via attention mechanism <ref type="bibr" target="#b0">(Bahdanau et al., 2014)</ref>.</p><p>At each time step, the decoder computes:</p><formula xml:id="formula_2">g x i = LSTM(g x i-1 , x i-1 ) c i = attention(g x i , H) p(x i ) = softmax(W v [c i ⊕g x i ⊕z] + b v )<label>(3)</label></formula><p>where ⊕ denotes concatenation. x i-1 is the word vector of input token; g x i is the corresponding decoder hidden state and p(x i ) is the output token distribution at time step i.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">NLU</head><p>NLU performs the reverse procedures of NLG. First, an NLU encoder infers the latent variable z from utterance x. The encoder uses a bi-directional LSTM to convert the utterance into a list of hidden states. These hidden states are pooled and passed through feed-forward neural networks to compute the mean µ µ µ x,z and standard deviation σ σ σ x,z of the posterior q(z|x). This procedure follows Equation 1 in NLG.</p><p>However, note that a subtle difference between natural language and formal language is that the former is ambiguous while the later is precisely defined. This makes NLU a many-to-one mapping problem but NLG is one-to-many. To better reflect the fact that the NLU output requires less variance, when decoding we choose the latent vector z in NLU to be the mean vector µ µ µ x,z , instead of sampling it from q(z|x) like Equation <ref type="formula" target="#formula_1">2</ref>. <ref type="foot" target="#foot_0">1</ref>After the latent vector is obtained, the formal representation y is predicted from both z and x using an NLU decoder. Since the space of y depends on the formal language construct, we consider two common scenarios in dialogue systems. In the first scenario, y is represented as a set of slot-value pairs, e.g., {food type=British, area=north} in restaurant search domain <ref type="bibr" target="#b22">(Mrkšić et al., 2017)</ref>. The decoder here consists of several classifiers, one for each slot, to predict the corresponding values.<ref type="foot" target="#foot_1">foot_1</ref> Each classifier is modelled by a 1-layer feed-forward neural network that takes z as input:</p><formula xml:id="formula_3">p(y s ) = softmax(W s z + b s )<label>(4)</label></formula><p>where p(y s ) is the predicted value distribution of slot s.</p><p>In the second scenario, y is a tree-structured formal representation <ref type="bibr" target="#b2">(Banarescu et al., 2013)</ref>. We then generate y as a linearised token sequence using an LSTM decoder relying on both z and x via the standard attention mechanism <ref type="bibr" target="#b0">(Bahdanau et al., 2014)</ref>. The decoding procedure follows exactly Equation 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Model Summary</head><p>One flexibility of the JUG model comes from the fact that it has two ways to infer the shared latent variable z through either x or y; and the inferred z can aid the generation of both x and y. In this next section, we show how this shared latent variable enables the JUG model to explore unlabelled x and y, while aligning the learned meanings inside the latent space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Optimisation</head><p>We now describe how JUG can be optimised with a pair of x and y ( §3.1), and also unpaired x or y ( §3.2). We specifically discuss the prior choice of JUG objectives in §3.3. A combined objective can be thus derived for semi-supervised learning: a practical scenario when we have a small set of labelled data but abundant unlabelled ones ( §3.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Optimising p(x, y)</head><p>Given a pair of utterance x and formal representation y, our objective is to maximise the loglikelihood of the joint probability p(x, y):</p><formula xml:id="formula_4">log p(x, y) = log z p(x, y, z)<label>(5)</label></formula><p>The optimisation task is not directly tractable since it requires us to marginalise out the latent variable z. However, it can be solved by following the standard practice of neural variational inference <ref type="bibr" target="#b19">(Kingma and Welling, 2013</ref>). An objective based on the variational lower bound can be derived as</p><formula xml:id="formula_5">L x,y = E q(z|x) log p(y|z, x) + E q(z|x) log p(x|z, y) -KL[q(z|x)||p(z)]<label>(6)</label></formula><p>where the first term on the right side is the NLU model; the second term is the reconstruction of x; and the last term denotes the Kullback-Leibler divergence between the approximate posterior q(z|x) with the prior p(z). We defer the discussion of prior to Section 3.3 and detailed derivations to Appendix.</p><p>The symmetry between utterance and semantics offers an alternative way of inferring the posterior through the approximation q(z|y). Analogously we can derive a variational optimisation objective:</p><formula xml:id="formula_6">L y,x = E q(z|y) log p(x|z, y) + E q(z|y) log p(y|z, x) -KL[q(z|y)||p(z)] (7)</formula><p>where the first term is the NLG model; the second term is the reconstruction of y; and the last term denotes the KL divergence. It can be observed that our model has two posterior inference paths from either x or y, and also two generation paths. All paths can be optimised.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Optimising p(x) or p(y)</head><p>Additionally, when we have access to unlabelled utterance x (or formal representation y), the optimisation objective of JUG is the marginal likelihood p(x) (or p(y)):</p><formula xml:id="formula_7">log p(x) = log y z p(x, y, z)<label>(8)</label></formula><p>Note that both z and y are unobserved in this case. We can develop an objective based on the variational lower bound for the marginal:</p><formula xml:id="formula_8">L x = E q(y|z,x) E q(z|x) log p(x|z, y) -KL[q(z|x)||p(z)] (9)</formula><p>where the first term is the auto-encoder reconstruction of x with a cascaded NLU-NLG path. The second term is the KL divergence which regularizes the approximated posterior distribution. Detailed derivations can be found in Appendix.</p><p>When computing the reconstruction term of x, it requires us to first run through the NLU model to obtain the prediction on y, from which we run through NLG to reconstruct x. The full information flow is (x → z → y → z → x). <ref type="foot" target="#foot_2">3</ref> Connections can be drawn with recent work which uses backtranslation to augment training data for machine translation <ref type="bibr" target="#b26">(Sennrich et al., 2016;</ref><ref type="bibr" target="#b13">He et al., 2016)</ref>. Unlike back-translation, the presence of latent variable in our model requires us to sample z along the NLU-NLG path. The introduced stochasticity allows the model to explore a larger area of the data manifold.</p><p>The above describes the objectives when we have unlabelled x. We can derive a similar objective for leveraging unlabelled y:</p><formula xml:id="formula_9">L y = E q(x|z,y) E q(z|y) log p(y|z, x) -KL[q(z|y)||p(z)] (<label>10</label></formula><formula xml:id="formula_10">)</formula><p>where the first term is the auto-encoder reconstruction of y with a cascaded NLG-NLU path. The full information flow here is (y→z→x→z→y).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Choice of Prior</head><p>The objectives described in 3.1 and 3.2 require us to match an approximated posterior (either q(z|x) or q(z|y)) to a prior p(z) that reflects our belief. A common choice of p(z) in the research literature is the Normal distribution (Kingma and Welling, 2013). However, it should be noted that even if we match both q(z|x) and q(z|y) to the same prior, it does not guarantee that the two inferred posteriors are close to each other; this is a desired property of the shared latent space.</p><p>To better address the property, we propose a novel prior choice: when the posterior is inferred from x (i.e., q(z|x)), we choose the parameterised distribution q(z|y) as our prior belief of p(z). Similarly, when the posterior is inferred from y (i.e., q(z|y)), we have the freedom of defining p(z) to be q(z|x). This approach directly pulls q(z|x) and q(z|y) closer to ensure a shared latent space.</p><p>Finally, note that it is straightforward to compute both q(z|x) and q(z|y) when we have parallel x and y. However when we have the access to unlabelled data, as described in Section 3.2, we can only use the pseudo x-y pairs that are generated by our NLU or NLG model, such that we can match an inferred posterior to a pre-defined prior reflecting our belief of the shared latent space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Training Summary</head><p>In general, JUG subsumes the following three training scenarios which we will experiment with.</p><p>When we have fully labelled x and y, the JUG jointly optimises NLU and NLG in a supervised fashion with the objective as follows:</p><formula xml:id="formula_11">L basic = (x,y)∼(X,Y ) (L x,y + L y,x )<label>(11)</label></formula><p>where (X, Y ) denotes the set of labelled examples. Additionally in the fully supervised setting, JUG can be trained to optimise both NLU, NLG and auto-encoding paths. This corresponds to the following objective:</p><formula xml:id="formula_12">L marginal = L basic + (x,y)∼(X,Y ) (L x + L y ) (12)</formula><p>Furthermore, when we have additional unlabelled x or y, we optimise a semi-supervised JUG objective as follows:</p><formula xml:id="formula_13">L semi = L basic + x∼X L x + y∼Y L y (13)</formula><p>where X denotes the set of utterances and Y denotes the set of formal representations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We experiment on two dialogue datasets with different formal representations to test the generality of our model. The first dataset is E2E <ref type="bibr" target="#b23">(Novikova et al., 2017)</ref>, which contains utterances annotated with flat slot-value pairs as their semantic representations. The second dataset is the recent weather dataset <ref type="bibr" target="#b1">(Balakrishnan et al., 2019)</ref>, where both utterances and semantics are represented in tree structures. Examples of the two datasets are provided in tables 1 and 2.</p><p>Natural Language "sousa offers british food in the low price range. it is family friendly with a 3 out of 5 star rating. you can find it near the sunshine vegetarian cafe."</p><p>Semantic Representation restaurant_name=sousa, food=english, price_range=cheap, customer_rating=average, family_friendly=yes, near=sunshine vegetarian cafe  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Training Scenarios</head><p>We primarily evaluated our models on the raw splits of the original datasets, which enables us to fairly compare fully-supervised JUG with existing work on both NLU and NLG. <ref type="foot" target="#foot_3">4</ref> Statistics of the two datasets can be found in Table <ref type="table" target="#tab_3">3</ref>.</p><p>In addition, we set up an experiment to evaluate semi-supervised JUG with a varying amount of labelled training data (5%, 10%, 25%, 50%, 100%, with the rest being unlabelled). Note that the original E2E test set is designed on purpose with unseen slot-values in the test set to make it difficult <ref type="bibr" target="#b10">(Dušek et al., 2018</ref><ref type="bibr" target="#b11">(Dušek et al., , 2020))</ref>; we remove the distribution bias by randomly re-splitting the E2E dataset. On the contrary, utterances in the weather dataset contains extra tree-structure annotations which make the NLU task a toy problem. We therefore remove these annotations to make NLU more realistic, as shown in the second row of Table <ref type="table" target="#tab_1">2</ref>.</p><p>As described in Section 3.4, we can optimise our proposed JUG model in various ways. We investigate the following approaches:</p><p>JUG basic : this model jointly optimises NLU   <ref type="bibr" target="#b9">(Dušek and Jurcicek, 2016)</ref> 0.6593 SLUG <ref type="bibr" target="#b17">(Juraska et al., 2018)</ref> 0.6619 Dual supervised learning <ref type="bibr" target="#b27">(Su et al., 2019)</ref>  and NLG with the objective in Equation <ref type="formula" target="#formula_11">11</ref>. This uses labelled data only. JUG marginal : jointly optimises NLU, NLG and auto-encoders with only labelled data, per Equation <ref type="formula">12</ref>.</p><p>JUG semi : jointly optimises NLU and NLG with labelled data and auto-encoders with unlabelled data, per Equation <ref type="formula">13</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baseline Systems</head><p>We compare our proposed model with some existing methods as shown in Table <ref type="table" target="#tab_4">4</ref> and two designed baselines as follows:</p><p>Decoupled: The NLU and NLG models are trained separately by supervised learning. Both of the individual models have the same encoderdecoder structure as JUG. However, the main difference is that there is no shared latent variable between the two individual NLU and NLG models.</p><p>Augmentation: We pre-train Decoupled models to generate pseudo label from the unlabelled corpus <ref type="bibr" target="#b20">(Lee, 2013)</ref> in a setup similar to backtranslation <ref type="bibr" target="#b26">(Sennrich et al., 2016)</ref>. The pseudo data and labelled data are then used together to fine-tune the pre-trained models.</p><p>Among all systems in our experiments, the number of units in LSTM encoder/decoder are set to {150, 300} and the dimension of latent space is 150. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Main Results</head><p>We start by comparing the JUG basic performance with existing work following the original split of the datasets. The results are shown in Table <ref type="table" target="#tab_4">4</ref>. On E2E dataset, we follow previous work to use F1 of slot-values as the measurement for NLU, and BLEU-4 for NLG. For weather dataset, there is only published results for NLG. It can be observed that the JUG basic model outperforms the previous state-of-the-art NLU and NLG systems on the E2E dataset, and also for NLG on the weather dataset.</p><p>The results prove the effectiveness of introducing the shared latent variable z for jointly training NLU and NLG. We will further study the impact of the shared z in Section 4.4.2. We also evaluated the three training scenarios of JUG in the semi-supervised setting, with different proportion of labelled and unlabelled data. The results for E2E is presented in Table <ref type="table">5</ref> and<ref type="table" target="#tab_5">6</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>With the varying proportion of unlabelled data in</head><p>Figure <ref type="figure">2</ref>: Visualisation of latent variable z. Given a pair of x and y, z can be sampled from the posterior q(z|x) or q(z|y), denoted by blue and orange dots respectively.</p><p>the training set, we see that unlabelled data is helpful in almost all cases. Moreover, the performance gain is the more significant when the labelled data is less. This indicates that the proposed model is especially helpful for low resource setups when there is a limited amount of labelled training examples but more available unlabelled ones.</p><p>The results for weather dataset are presented in Table <ref type="table" target="#tab_6">7</ref> and<ref type="table" target="#tab_7">8</ref>. In this dataset, NLU is more like a semantic parsing task <ref type="bibr" target="#b3">(Berant et al., 2013)</ref> and we use exact match accuracy as its measurement. Meanwhile, NLG is measured by BLEU. The results reveal a very similar trend to that in E2E. The generated examples can be found in Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Analysis</head><p>In this section we further analyse the impact of the shared latent variable and also the impact of utilising unlabelled data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.1">Visualisation of Latent Space</head><p>As mentioned in Section 2.1, the latent variable z can be sampled from either posterior approximation q(z|x) or q(z|y). We inspect the latent space in Figure <ref type="figure">2</ref> to find out how well the model learns intent sharing. We plot z with the E2E dataset on 2dimentional space using t-SNE projection <ref type="bibr" target="#b21">(Maaten and Hinton, 2008)</ref>.</p><p>We observe two interesting properties. First, for each data point (x, y), the z values sampled from q(z|x) and q(z|y) are close to each other. This reveals that the meanings of x and y are tied in the latent space. Second, there exists distinct clusters in the space of z. By further inspecting the actual examples within each cluster, we found that a cluster represents a similar meaning composition. For instance, the cluster cen-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>NLU NLG JUG basic 90.55 0.726 JUG basic (feed random z) 38.13 0.482  tered at (-20, -40) contains {name, foodtype, price, rating, area, near}, while the cluster centered at (45, 10) contains {name, eattype, foodtype, price}. This indicates that the shared latent serves as conclusive global feature representations for NLU and NLG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.2">Impact of the Latent Variable</head><p>One novelty of our model is the introduction of shared latent variable z for natural language x and formal representations y. A common problem in neural variational models is that when coupling a powerful autogressive decoder, the decoder tends to learn to ignore z and solely rely on itself to generate the data <ref type="bibr" target="#b4">(Bowman et al., 2016;</ref><ref type="bibr" target="#b6">Chen et al., 2017;</ref><ref type="bibr" target="#b12">Goyal et al., 2017)</ref>. In order to examine to what extent does our model actually rely on the shared variable in both NLU and NLG, we seek for an empirical answer by comparing the JUG basic model with a model variant which uses a random value of z sampled from a normal distribution N (0, 1) during testing. From Table <ref type="table" target="#tab_8">9</ref>, we can observe that there exists a large performance drop if z is assigned with random values. This suggests that JUG indeed relies greatly on the shared variable to produce good-quality x or y.</p><p>We further analyse the various sources of errors to understand the cases which z helps to improve. On E2E dataset, wrong prediction in NLU comes from either predicting not_mention label for certain slots in ground truth semantics; predicting arbitrary values on slots not present in the ground truth semantics; or predicting wrong values com- paring to ground truth. Three types of error are referred to Missing (Mi), Redundant (Re) and Wrong (Wr) in Table <ref type="table" target="#tab_9">10</ref>. For NLG, semantic errors can be either missing or generating wrong slot values in the given semantics <ref type="bibr" target="#b32">(Wen et al., 2015)</ref>. Our model makes fewer mistakes in all these error sources comparing to the baseline Decoupled. We believe this is because the clustering property learned in the latent space provides better feature representations at a global scale, eventually benefiting NLU and NLG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.3">Impact of Unlabelled Data Source</head><p>In Section 4.3, we found that the performance of our model can be further enhanced by leveraging unlabelled data. As we used both unlabelled utterances and unlabelled semantic representations together, it is unclear if both contributed to the performance gain. To answer this question, we start with the JUG basic model, and experimented with adding unlabelled data from 1) only unlabelled utterances x; 2) only semantic representations y; 3) both x and y. As shown in Table <ref type="table" target="#tab_10">11</ref>, when adding any uni-sourced unlabelled data (x or y), the model is able to improve to a certain extent. However, the performance can be maximised when both data sources are utilised. This strengthens the argument that our model can leverage bi-sourced unlabelled data more effectively via latent space sharing to improve NLU and NLG at the same time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related Work</head><p>Natural Language Understanding (NLU) refers to the general task of mapping natural language to formal representations. One line of research in the dialogue community aims at detecting slot-value pairs expressed in user utterances as a classification problem <ref type="bibr" target="#b14">(Henderson et al., 2012;</ref><ref type="bibr" target="#b28">Sun et al., 2014;</ref><ref type="bibr" target="#b22">Mrkšić et al., 2017;</ref><ref type="bibr" target="#b30">Vodolán et al., 2017)</ref>. Another line of work focuses on converting single-turn user utterances to more structured meaning representa-tions as a semantic parsing task <ref type="bibr" target="#b38">(Zettlemoyer and Collins, 2005;</ref><ref type="bibr" target="#b16">Jia and Liang, 2016;</ref><ref type="bibr" target="#b8">Dong and Lapata, 2018;</ref><ref type="bibr" target="#b7">Damonte et al., 2019)</ref>.</p><p>In comparison, Natural Language Generation (NLG) is scoped as the task of generating natural utterances from their formal representations. This is traditionally handled with a pipelined approach <ref type="bibr" target="#b25">(Reiter and Dale, 1997)</ref> with content planning and surface realisation <ref type="bibr" target="#b31">(Walker et al., 2001;</ref><ref type="bibr">Stent et al., 2004)</ref>. More recently, NLG has been formulated as an end-to-end learning problem where text strings are generated with recurrent neural networks conditioning on the formal representation <ref type="bibr" target="#b32">(Wen et al., 2015;</ref><ref type="bibr" target="#b9">Dušek and Jurcicek, 2016;</ref><ref type="bibr" target="#b11">Dušek et al., 2020;</ref><ref type="bibr" target="#b1">Balakrishnan et al., 2019;</ref><ref type="bibr" target="#b29">Tseng et al., 2019)</ref>.</p><p>There has been very recent work which does NLU and NLG jointly. Both <ref type="bibr" target="#b35">Ye et al. (2019)</ref> and <ref type="bibr" target="#b5">Cao et al. (2019)</ref> explore the duality of semantic parsing and NLG. The former optimises two sequence-to-sequence models using dual information maximisation, while the latter introduces a dual learning framework for semantic parsing. <ref type="bibr" target="#b27">Su et al. (2019)</ref> proposes a learning framework for dual supervised learning <ref type="bibr" target="#b34">(Xia et al., 2017)</ref> where both NLU and NLG models are optimised towards a joint objective. Their method brings benefits with annotated data in supervised learning, but does not allow semi-supervised learning with unlabelled data. In contrast to their work, we propose a generative model which couples NLU and NLG with a shared latent variable. We focus on exploring a coupled representation space between natural language and corresponding semantic annotations. As proved in experiments, the information sharing helps our model to leverage unlabelled data for semi-supervised learning, which eventually benefits both NLU and NLG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We proposed a generative model which couples natural language and formal representations via a shared latent variable. Since the two space is coupled, we gain the luxury of exploiting each unpaired data source and transfer the acquired knowledge to the shared meaning space. This eventually benefits both NLU and NLG, especially in a lowresource scenario. The proposed model is also suitable for other translation tasks between two modalities.</p><p>As a final remark, natural language is richer and more informal. NLU needs to handle ambiguous or erroneous user inputs. However, formal representations utilised by an NLG system are more precisely-defined. In future, we aim to refine our generative model to better emphasise this difference of the two tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendices</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.1 Derivation of Lower Bounds</head><p>We derive the lower bounds for log p(x, y) as follows: log p(x, y) = log z p(x, y, z) = log z p(x, y, z)q(z|x) q(z|x)</p><p>= log z p(x|z, y)p(y|z, x)p(z)q(z|x) q(z|x)</p><p>= log E q(z|x) p(x|z, y)p(y|z, x)p(z) q(z|x)</p><p>≥ E q(z|x) log p(x|z, y)p(y|z, x)p(z) q(z|x) = E q(z|x) [log p(x|z, y) + log p(y|z, x)]</p><p>-KL[q(z|x)||p(z)] ( <ref type="formula">14</ref>) where q(z|x) represents an approximated posterior. This derivation gives us the Equation 6 in the paper. Similarly we can derive an alternative lower bound in Equation 7 by introducing q(z|y) instead of q(z|x).</p><p>For marginal log-likelihood log p(x) or log p(y), its lower bound is derived as follows: log p(x) = log y z p(x, y, z) = log y z p(x|z, y)p(y)p(z)q(z|x)q(y|z, x) q(z|x)q(y|z, x)</p><p>= log E q(y|z,x) E q(z|x) p(x|z, y)p(y)p(z) q(z|x)q(y|z, x)</p><p>≥ E q(y|z,x) E q(z|x) log p(x|z, y)p(y)p(z) q(z|x)q(y|z, x) = E q(y|z,x) E q(z|x) log p(x|z, y)</p><p>-KL[q(z|x)||p(z)] -KL[q(y|x, z)||p(y)]</p><p>(15) Note that the resulting lower bound consists of three terms: a reconstruction of x, a KL divergence which regularises the space of z, and also a KL divergence which regularises the space of y. We have dropped the last term in our optimisation objective in Equation 9, since we do not impose any prior assumption on the output space of the NLU model.</p><p>Analogously we can derive the lower bound for log p(y). We also do not impose any prior assumption on the output space of the NLG model, which leads us to Equation <ref type="formula" target="#formula_9">10</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Generated Examples</head><p>Reference of example x: "for those prepared to pay over £30 , giraffe is a restaurant located near the six bells ." y: {name=giraffe, eat_type=restaurant, price_range=more than £30, near=the six bells} Prediction by Decoupled model x: "near the six bells , there is a restaurant called giraffe that is children friendly ." (miss price_range) y: {name=travellers rest beefeater, price_range=more than £30, near=the six bells} (wrong name, miss eat_type) Prediction by JUG semi model x: "giraffe is a restaurant near the six bells with a price range of more than £30 ." (semantically correct) y: {name=giraffe, eat_type=restaurant, price_range=more than £30, near=the six bells} (exact match) Table <ref type="table" target="#tab_1">12</ref>: An example of E2E dataset and predictions generated by the baseline model Decoupled and the proposed model JUG semi . x and y denotes natural language and the corresponding semantic representation. Errors are highlighted following predictions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Generation and inference process in our model, and how NLU and NLG are achieved. x and y denotes utterances and formal representations respectively; z represents the shared latent variable for x and y.</figDesc><graphic coords="1,307.28,232.88,218.27,172.68" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>An example in E2E dataset.</figDesc><table><row><cell>Natural Language (original)</cell></row><row><cell>"[__DG_YES__ Yes ] , [__DG_INFORM__</cell></row><row><cell>[__ARG_DATE_TIME__ [__ARG_COLLOQUIAL__ today's ] ]</cell></row><row><cell>forecast is [__ARG_CLOUD_COVERAGE__ mostly cloudy ]</cell></row><row><cell>with [__ARG_CONDITION__ light rain showers ] ] ."</cell></row><row><cell>Natural Language (processed by removing tree annotations)</cell></row><row><cell>"Yes, today's forecast is mostly cloudy with light rain showers."</cell></row><row><cell>Semantic Representation</cell></row><row><cell>[__DG_YES__ [__ARG_TASK__ get_weather_attribute ] ]</cell></row><row><cell>[__DG_INFORM__ [__ARG_TASK__ get_forecast ]</cell></row><row><cell>[__ARG_CONDITION__ light rain showers ]</cell></row><row><cell>[__ARG_CLOUD_COVERAGE__ mostly cloudy ]</cell></row><row><cell>[__ARG_DATE_TIME__ [__ARG_COLLOQUIAL__ today's ] ] ]</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>An example in weather dataset. The natural language in original dataset (first row) is used for training to have a fair comparison with existing methods. The processed utterances (second row) is used in our semi-supervised setting.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 3 :</head><label>3</label><figDesc>Number of examples in two datasets</figDesc><table><row><cell>E2E NLU</cell><cell>F1</cell></row><row><cell cols="2">Dual supervised learning (Su et al., 2019) 0.7232</cell></row><row><cell>JUG basic</cell><cell>0.7337</cell></row><row><cell>E2E NLG</cell><cell>BLEU</cell></row><row><cell>TGEN</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 4 :</head><label>4</label><figDesc>Comparison with previous systems on two datasets. Note that there is no previous system trained for NLU in weather dataset.</figDesc><table><row><cell>0.5716</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 6 :</head><label>6</label><figDesc>The optimiser Adam<ref type="bibr" target="#b18">(Kingma and Ba, 2014)</ref> is used with learning rate 1e-3. Batch size is set to {32, 64}. All the models are fully trained and the NLG results on E2E dataset. BLEU and semantic accuracy (%) (in bracket) are both reported with varying percentage of labelled training data. Models using unlabelled data are marked with *.</figDesc><table><row><cell cols="2">Model / Data</cell><cell>5%</cell><cell>10%</cell><cell>25%</cell><cell>50%</cell><cell>100%</cell></row><row><cell>Decoupled</cell><cell></cell><cell cols="5">52.77 (0.874) 62.32 (0.902) 69.37 (0.924) 73.68 (0.935) 76.12 (0.942)</cell></row><row><cell cols="6">Augmentation  *  54.71 (0.878) 62.54 (0.902) 68.91 (0.922) 73.84 (0.935)</cell><cell>-</cell></row><row><cell>JUG basic</cell><cell></cell><cell cols="5">60.30 (0.902) 67.08 (0.918) 72.49 (0.932) 74.74 (0.937) 78.05 (0.945)</cell></row><row><cell cols="2">JUG marginal</cell><cell cols="5">62.96 (0.907) 68.43 (0.920) 73.35 (0.933) 75.74 (0.939) 78.93 (0.948)</cell></row><row><cell>JUG  *  semi</cell><cell></cell><cell cols="4">68.09 (0.921) 70.33 (0.925) 73.79 (0.935) 75.46 (0.939)</cell><cell>-</cell></row><row><cell cols="7">Table 5: NLU results on E2E dataset. Joint accuracy (%) and F1 score (in bracket) are both reported with varying</cell></row><row><cell cols="6">percentage of labelled training data. Models using unlabelled data are marked with *.</cell></row><row><cell cols="2">Model / Data</cell><cell>5%</cell><cell>10%</cell><cell>25%</cell><cell>50%</cell><cell>100%</cell></row><row><cell cols="2">Decoupled</cell><cell cols="5">0.693 (83.47) 0.723 (87.33) 0.784 (92.52) 0.793 (94.91) 0.813 (96.98)</cell></row><row><cell cols="6">Augmentation  *  0.747 (84.79) 0.770 (90.13) 0.806 (94.06) 0.815 (96.04)</cell><cell>-</cell></row><row><cell>JUG basic</cell><cell></cell><cell cols="5">0.685 (84.20) 0.734 (88.68) 0.769 (93.83) 0.788 (95.11) 0.810 (95.07)</cell></row><row><cell cols="2">JUG marginal</cell><cell cols="5">0.724 (85.57) 0.775 (93.59) 0.803 (94.99) 0.817 (98.67) 0.830 (99.11)</cell></row><row><cell>JUG  *  semi</cell><cell></cell><cell cols="4">0.814 (90.47) 0.792 (94.76) 0.819 (95.59) 0.827 (98.42)</cell><cell>-</cell></row><row><cell>Model / Data</cell><cell cols="3">5% 10% 25% 50% 100%</cell><cell></cell><cell></cell></row><row><cell>Decoupled</cell><cell cols="3">73.46 80.85 86.00 88.45 90.68</cell><cell></cell><cell></cell></row><row><cell cols="3">Augmentation  *  74.77 79.84 86.24 88.69</cell><cell>-</cell><cell></cell><cell></cell></row><row><cell>JUG basic</cell><cell cols="3">73.62 80.13 86.15 87.94 90.55</cell><cell></cell><cell></cell></row><row><cell>JUG marginal JUG  *  semi</cell><cell cols="3">74.61 81.14 86.83 89.06 91.28 79.19 83.22 87.46 89.17 -</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 7 :</head><label>7</label><figDesc>NLU results with exact match accuracy (%) on weather dataset.</figDesc><table /><note><p>best model is picked by the average of NLU and NLG results on validation set during training.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 8 :</head><label>8</label><figDesc>NLG results with BLEU on weather dataset.</figDesc><table><row><cell>. We</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 9 :</head><label>9</label><figDesc>A comparative study to evaluate the contribution of the learned latent variable z in NLU/NLG decoding. Models are trained on the whole weather dataset.</figDesc><table><row><cell>Method</cell><cell>NLU Mi Re Wr Mi Wr NLG</cell></row><row><cell cols="2">Decoupled 714 256 2382 5714 2317</cell></row><row><cell>JUG basic</cell><cell>594 169 1884 4871 2102</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 10 :</head><label>10</label><figDesc>Error analysis on E2E dataset. Numbers of missing (Mi), redundant (Re) and wrong (Wr) predictions on slot-value pairs are reported for NLU; numbers of missing or wrong generated slot values are listed for NLG. Lower number indicates the better results. Both models are trained on 5% of the training data.</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 11 :</head><label>11</label><figDesc>Comparison on sources of unlabelled data for semi-supervised learning using only utterances (x), only semantic representations (y) or both (x and y). JUG basic model is trained on 5% of training data.</figDesc><table><row><cell></cell><cell>E2E</cell><cell>Weather</cell></row><row><cell>Method</cell><cell cols="2">NLU NLG NLU NLG</cell></row><row><cell>JUG basic</cell><cell cols="2">60.30 0.685 73.62 0.634</cell></row><row><cell>+unlabelled x</cell><cell cols="2">62.89 0.765 74.97 0.654</cell></row><row><cell>+unlabelled y</cell><cell cols="2">59.55 0.815 76.98 0.621</cell></row><row><cell cols="3">+unlabelled x and y 68.09 0.814 79.19 0.670</cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Note that it is still necessary to compute the standard deviation σ σ σx,z in NLU, since the term is needed for optimisation. See more details in Section 3.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>Each slot has a set of corresponding values plus a special one not_mention.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3" xml:id="foot_2"><p>This information flow requires us to sample both z and y in reconstructing x. Since y is a discrete sequence, we use REINFORCE<ref type="bibr" target="#b33">(Williams, 1992)</ref> to pass the gradient from NLG to NLU in the cascaded NLU-NLG path.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_3"><p>Following Balakrishnan et al. (2019), the evaluation code https://github.com/tuetschek/e2e-metrics provided by the E2E organizers is used here for calculating BLEU in NLG.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments</head><p><rs type="person">Bo-Hsiang Tseng</rs> is supported by <rs type="funder">Cambridge Trust</rs> and the <rs type="funder">Ministry of Education, Taiwan</rs>. This work has been performed using resources provided by the Cambridge Tier-2 system operated by the <rs type="institution">University of Cambridge Research Computing Service</rs> (<ref type="url" target="http://www.hpc.cam.ac.uk">http://www.hpc.cam.ac.uk</ref>) funded by <rs type="funder">EPSRC</rs> Tier-2 capital grant <rs type="grantNumber">EP/P020259/1</rs>..</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_bxDSQ59">
					<idno type="grant-number">EP/P020259/1</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<author>
			<persName><forename type="first">Anusha</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinfeng</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kartikeya</forename><surname>Upasani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>White</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajen</forename><surname>Subba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.07220</idno>
		<title level="m">Constrained decoding for neural nlg from compositional representations in task-oriented dialogue</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Abstract meaning representation for sembanking</title>
		<author>
			<persName><forename type="first">Laura</forename><surname>Banarescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Claire</forename><surname>Bonial</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shu</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Madalina</forename><surname>Georgescu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kira</forename><surname>Griffitt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ulf</forename><surname>Hermjakob</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Knight</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martha</forename><surname>Palmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Schneider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse</title>
		<meeting>the 7th Linguistic Annotation Workshop and Interoperability with Discourse<address><addrLine>Sofia, Bulgaria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="178" to="186" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semantic parsing on Freebase from question-answer pairs</title>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roy</forename><surname>Frostig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Seattle, Washington, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1533" to="1544" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Generating sentences from a continuous space</title>
		<author>
			<persName><forename type="first">Luke</forename><surname>Samuel R Bowman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafal</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 20th SIGNLL Conference on Computational Natural Language Learning</title>
		<meeting>The 20th SIGNLL Conference on Computational Natural Language Learning</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="10" to="21" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Semantic parsing with dual learning</title>
		<author>
			<persName><forename type="first">Ruisheng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Su</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jieyu</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Variational lossy autoencoder</title>
		<author>
			<persName><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Diederik</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Prafulla</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">5th International Conference on Learning Representations</title>
		<meeting><address><addrLine>Toulon, France</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-04-24">2017. 2017. April 24-26, 2017</date>
		</imprint>
	</monogr>
	<note>Track Proceedings</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Practical semantic parsing for spoken language understanding</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Damonte</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tagyoung</forename><surname>Chung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter</title>
		<meeting>the 2019 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="16" to="23" />
		</imprint>
	</monogr>
	<note>Industry Papers</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Coarse-to-fine decoding for neural semantic parsing</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mirella</forename><surname>Lapata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="731" to="742" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Sequence-tosequence generation for spoken dialogue via deep syntax trees and strings</title>
		<author>
			<persName><forename type="first">Ondřej</forename><surname>Dušek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Filip</forename><surname>Jurcicek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Short Papers</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="45" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Findings of the e2e nlg challenge</title>
		<author>
			<persName><forename type="first">Ondřej</forename><surname>Dušek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jekaterina</forename><surname>Novikova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Verena</forename><surname>Rieser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 11th International Conference on Natural Language Generation</title>
		<meeting>the 11th International Conference on Natural Language Generation</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="322" to="328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Evaluating the state-of-the-art of end-to-end natural language generation: The e2e nlg challenge</title>
		<author>
			<persName><forename type="first">Ondřej</forename><surname>Dušek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jekaterina</forename><surname>Novikova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Verena</forename><surname>Rieser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Speech &amp; Language</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="page" from="123" to="156" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Z-forcing: Training stochastic recurrent networks</title>
		<author>
			<persName><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alias Parth</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marc-Alexandre</forename><surname>Côté</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nan</forename><forename type="middle">Rosemary</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="6713" to="6723" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Dual learning for machine translation</title>
		<author>
			<persName><forename type="first">Di</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingce</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liwei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Ying</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th International Conference on Neural Information Processing Systems</title>
		<meeting>the 30th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<publisher>Curran Associates Inc</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="820" to="828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Discriminative spoken language understanding using word confusion networks</title>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Milica</forename><surname>Gašić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Blaise</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pirros</forename><surname>Tsiakoulis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steve</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Spoken Language Technology Workshop (SLT)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="176" to="181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Word-based dialog state tracking with recurrent neural networks</title>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Blaise</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steve</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL)</title>
		<meeting>the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL)</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
		</imprint>
	</monogr>
	<note>Sepp Hochreiter and Jürgen Schmidhuber. 1997. Long short-term memory</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Data recombination for neural semantic parsing</title>
		<author>
			<persName><forename type="first">Robin</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Long Papers</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="12" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A deep ensemble model with slot alignment for sequence-to-sequence natural language generation</title>
		<author>
			<persName><forename type="first">Juraj</forename><surname>Juraska</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Panagiotis</forename><surname>Karagiannis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Bowden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marilyn</forename><surname>Walker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter</title>
		<title level="s">Long Papers</title>
		<meeting>the 2018 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="152" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Autoencoding variational bayes</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks</title>
		<author>
			<persName><forename type="first">Dong-Hyun</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Challenges in Representation Learning, ICML</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11">2008. Nov</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Neural belief tracker: Data-driven dialogue state tracking</title>
		<author>
			<persName><forename type="first">Nikola</forename><surname>Mrkšić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ó</forename><surname>Diarmuid</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tsung-Hsien</forename><surname>Séaghdha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Blaise</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steve</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1777" to="1788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The E2E dataset: New challenges for endto-end generation</title>
		<author>
			<persName><forename type="first">Jekaterina</forename><surname>Novikova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ondřej</forename><surname>Dušek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Verena</forename><surname>Rieser</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W17-5525</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue</title>
		<meeting>the 18th Annual SIGdial Meeting on Discourse and Dialogue<address><addrLine>Saarbrücken, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="201" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Span-based hierarchical semantic parsing for task-oriented dialog</title>
		<author>
			<persName><forename type="first">Panupong</forename><surname>Pasupat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sonal</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karishma</forename><surname>Mandyam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rushin</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1520" to="1526" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Building applied natural language generation systems</title>
		<author>
			<persName><forename type="first">Ehud</forename><surname>Reiter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Dale</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Natural Language Engineering</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="57" to="87" />
			<date type="published" when="1997">1997</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Trainable sentence planning for complex information presentation in spoken dialog systems</title>
		<author>
			<persName><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics</title>
		<editor>
			<persName><forename type="first">Amanda</forename><surname>Stent</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Rashmi</forename><surname>Prasad</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Marilyn</forename><surname>Walker</surname></persName>
		</editor>
		<meeting>the 54th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2004">2016. 2004</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">79</biblScope>
		</imprint>
	</monogr>
	<note>Proceedings of the 42nd annual meeting on association for computational linguistics. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Dual supervised learning for natural language understanding and generation</title>
		<author>
			<persName><forename type="first">Shang-Yu</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chao-Wei</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yun-Nung</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>ACL</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">The sjtu system for dialog state tracking challenge 2</title>
		<author>
			<persName><forename type="first">Kai</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Su</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue</title>
		<meeting>the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue<address><addrLine>SIG-DIAL</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="318" to="326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Tree-structured semantic encoder with knowledge sharing for domain adaptation in natural language generation</title>
		<author>
			<persName><forename type="first">Bo-Hsiang</forename><surname>Tseng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paweł</forename><surname>Budzianowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yen-Chen</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Milica</forename><surname>Gasic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th Annual SIGdial Meeting on Discourse and Dialogue</title>
		<meeting>the 20th Annual SIGdial Meeting on Discourse and Dialogue</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="155" to="164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Hybrid dialog state tracker with asr features</title>
		<author>
			<persName><forename type="first">Miroslav</forename><surname>Vodolán</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rudolf</forename><surname>Kadlec</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Kleindienst</surname></persName>
		</author>
		<author>
			<persName><surname>Parku</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017. 2017</date>
			<biblScope unit="page">205</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Spot: A trainable sentence planner</title>
		<author>
			<persName><forename type="first">Marilyn</forename><forename type="middle">A</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Owen</forename><surname>Rambow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Monica</forename><surname>Rogati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the second meeting of the North American Chapter of the Association for Computational Linguistics on Language technologies</title>
		<meeting>the second meeting of the North American Chapter of the Association for Computational Linguistics on Language technologies</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2001">2001</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Semantically conditioned lstm-based natural language generation for spoken dialogue systems</title>
		<author>
			<persName><forename type="first">Tsung-Hsien</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Milica</forename><surname>Gasic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nikola</forename><surname>Mrkšić</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pei-Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Vandyke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steve</forename><surname>Young</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1711" to="1721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Simple statistical gradientfollowing algorithms for connectionist reinforcement learning</title>
		<author>
			<persName><forename type="first">Williams</forename><surname>Ronald</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Dual supervised learning</title>
		<author>
			<persName><forename type="first">Yingce</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiang</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<ptr target="JMLR.org" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="3789" to="3798" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Jointly learning semantic parser and natural language generator via dual information maximization</title>
		<author>
			<persName><forename type="first">Hai</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenjie</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1906.00575</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The hidden information state model: A practical framework for pomdp-based spoken dialogue management</title>
		<author>
			<persName><forename type="first">Steve</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Milica</forename><surname>Gaå Ąiä</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Simon</forename><surname>Keizer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franãgois</forename><surname>Mairesse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jost</forename><surname>Schatzmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Blaise</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Yu</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.csl.2009.04.001</idno>
	</analytic>
	<monogr>
		<title level="j">Computer Speech Language</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="150" to="174" />
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Typesql: Knowledge-based type-aware neural text-to-sql generation</title>
		<author>
			<persName><forename type="first">Tao</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zifan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zilin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dragomir</forename><surname>Radev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter</title>
		<meeting>the 2018 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>the Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="588" to="594" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning to map sentences to logical form: structured classification with probabilistic categorial grammars</title>
		<author>
			<persName><forename type="first">S</forename><surname>Luke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><surname>Collins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-First Conference on Uncertainty in Artificial Intelligence</title>
		<meeting>the Twenty-First Conference on Uncertainty in Artificial Intelligence</meeting>
		<imprint>
			<publisher>AUAI Press</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="658" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Learning to map sentences to logical form: Structured classification with probabilistic categorial grammars</title>
		<author>
			<persName><forename type="first">S</forename><surname>Luke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><surname>Collins</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.1420</idno>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Prediction by JUG_semi model x: &quot;the temperature will be around __arg_temp__ degrees __arg_colloquial__ between __arg_start_time__ and __arg_end_time</title>
		<author>
			<persName><forename type="first">Xiaodong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Houfeng</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Prediction by Decoupled model x: &quot;it will be __arg_temp__ degrees and __arg_cloud_coverage__ from __arg_start_time__ to __arg_end_time</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="2993" to="2999" />
		</imprint>
	</monogr>
	<note>Table 13: An example of weather dataset and predictions generated by the baseline model Decoupled and the proposed model JUG semi. x and y denotes natural language and the corresponding semantic representation. NLU result are highlighted following predictions</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
