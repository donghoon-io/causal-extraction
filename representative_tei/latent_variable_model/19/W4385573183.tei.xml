<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Sequential Topic Selection Model with Latent Variable for Topic-Grounded Dialogue</title>
				<funder ref="#_gM2JPDs #_2xQgbGs">
					<orgName type="full">National Natural Science Foundation of China</orgName>
				</funder>
				<funder>
					<orgName type="full">fund of Joint Laboratory of HUST</orgName>
				</funder>
				<funder ref="#_TqeHyys">
					<orgName type="full">CCF-AFSG Research Fund</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xiaofei</forename><surname>Wen</surname></persName>
							<email>xfwen@hust.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="laboratory">Cognitive Computing and Intelligent Information Processing Laboratory</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Joint Laboratory of HUST and Pingan Property &amp; Casualty Research (HPL)</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Wei</forename><surname>Wei</surname></persName>
							<email>weiw@hust.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="laboratory">Cognitive Computing and Intelligent Information Processing Laboratory</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="laboratory">Joint Laboratory of HUST and Pingan Property &amp; Casualty Research (HPL)</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ling</forename><surname>Mao</surname></persName>
							<email>maoxl@bit.edu.cn</email>
							<affiliation key="aff2">
								<orgName type="department">Department of Computer Science and Technology</orgName>
								<orgName type="institution">Beijing Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Sequential Topic Selection Model with Latent Variable for Topic-Grounded Dialogue</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-21T19:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Recently, topic-grounded dialogue system has attracted significant attention due to its effectiveness in predicting the next topic to yield better responses via the historical context and given topic sequence. However, almost all existing topic prediction solutions focus on only the current conversation and corresponding topic sequence to predict the next conversation topic, without exploiting other topic-guided conversations which may contain relevant topictransitions to current conversation. To address the problem, in this paper we propose a novel approach, named Sequential Global Topic Attention (SGTA) to exploit topic transition over all conversations in a subtle way for better modeling post-to-response topic-transition and guiding the response generation to the current conversation. Specifically, we introduce a latent space modeled as a Multivariate Skew-Normal distribution with hybrid kernel functions to flexibly integrate the global-level information with sequence-level information, and predict the topic based on the distribution sampling results. We also leverage a topic-aware prior-posterior approach for secondary selection of predicted topics, which is utilized to optimize the response generation task. Extensive experiments demonstrate that our model outperforms competitive baselines on prediction and generation tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Dialog systems have been widely used in a variety of applications. Recent efforts in dialogue systems aim at improving the diversity of agent responses <ref type="bibr" target="#b33">(Zhang et al., 2018;</ref><ref type="bibr" target="#b36">Zhou et al., 2021;</ref><ref type="bibr">Liu et al., 2022c)</ref> and endowing agents with the ability to exploit knowledge, express empathy and retain personality <ref type="bibr" target="#b0">(Adiwardana et al., 2020;</ref><ref type="bibr" target="#b22">Roller et al., 2021;</ref><ref type="bibr" target="#b30">Wei et al., 2021;</ref><ref type="bibr">Liu et al., 2022b)</ref>. However, in many real-world scenarios (e.g., conversa-tional recommendation, shopping guide and psychological counseling), conversational agents need to proactively steer the conversation by smoothly transforming the conversation topic into a specified one. Therefore, topic-grounded controllable conversation has recently attracted extensive attention.</p><p>Indeed, topic-grounded controllable dialogue is a task to obtain informative responses through a series of predicted topics and the given context. Recently, topic-grounded conversation works mainly focus on modeling the post-to-response topic transition for predicting the next topic to guide the response generation. Many deep learning based approaches are proposed for the task, which utilize topic similarity information as well as additional knowledge information to model dynamic topic transitions <ref type="bibr" target="#b25">(Tang et al., 2019;</ref><ref type="bibr" target="#b31">Wu et al., 2019;</ref><ref type="bibr" target="#b20">Qin et al., 2020;</ref><ref type="bibr">Zhou et al., 2020b;</ref><ref type="bibr">Zhong et al., 2021)</ref>. These approaches have achieved encouraging results, but they still face the issues as follows. First, some of these methods infer the next turn topic only with current turn topic embedding <ref type="bibr" target="#b25">(Tang et al., 2019;</ref><ref type="bibr" target="#b20">Qin et al., 2020;</ref><ref type="bibr">Zhong et al., 2021)</ref>. However, a topic may be inherently associated with several previous topics. Thus they may suffer from the inability of sufficiently modeling the sequential topic-transitions into the topic embeddings as ignoring the inherent topic dependencies over historical topic sequence. Second, almost all previous approaches model the topic transferring information only over the current topic sequence while neglecting the useful topic-transition patterns from other sequences, since frequently co-occurring topics over all topic-sequences are more likely to related to each other. Therefore, fully exploring such information is conceptually advantageous to accurately modeling the topic transition for better infer the next topic.</p><p>In this paper, we propose a Transformer-based sequential modeling approach, named Sequential Global Topic Attention (SGTA), to fully exploit topic-co-occurrences over all topic sequences for better modeling the post-to-response topic transitions and accurately predicting the next topic to the current conversation. Specifically, SGTA consists of three key elements: topic sequence s, global co-occurrence matrix c and latent variable z. The relationships among these elements are elaborated with the graphical model in Figure <ref type="figure" target="#fig_0">1</ref>. Specifically, we propose a new latent space based on the value variation of the Transformer and use it to model the contextual relation within current topic sequence. The latent space is modeled as a Multivariate Skew-Normal (MSN) distribution <ref type="bibr" target="#b2">(Azzalini and Valle, 1996)</ref> due to the flexibility of its covariance parameter to integrate multiple information and the specificity of its shape parameter to control the skewness of the distribution. The covariance parameter Σ is designed via task-specific kernel functions for measuring the similarity of pair-wise sequences based on the topic representations. The shape parameter α is used to represent the unique relative relationships of different topics in a sequence. These two parameters are constructed efficiently using current sequence information and other sequences' global information. By sampling the MSN distribution on Transformers, we provide a reparameterization of the MSN distribution to enable amortized inference over the latent space. Based on the sampling results, the model simulates the topic transition relationships in the sequence for predicting the next topic in current conversation.</p><p>To properly use predicted words and address fluency and controllability issues in generating responses, we leverage a response-based posterior topic distribution to instruct the model to generate responses based on the predicted topics. The prior and posterior ideas are widely used in response generation tasks <ref type="bibr" target="#b14">(Lian et al., 2019)</ref>, which is widely-adapted for avoiding to learn the same conversation patterns to each utterance. Concretely, we utilize the KL-Div Loss between the prior and posterior distributions as part of the overall loss, which allows the model to better adapt to response generation by converging the transition scope of the semantic space.</p><p>We summarize the contributions of this work as:</p><p>• To the best of our knowledge, this is the first work that exploits global-level topic transitions of all sequence information to learn contextual information for topic-related dialogue.</p><p>• We propose a unified model to improve the topic prediction performance of current sequence by constructing task-specific MSN distribution using sequential information and global information.</p><p>• We propose a method that subtly exploits predicted words for response generation and achieves the state-of-the-art on the benchmark dataset compared to competitive baseline methods 2 Related Work  dation to enhance the controllability of the conversation process, and <ref type="bibr">Zhou et al. (2020b)</ref> utilizes Pre-trained Language Model(PrLM) to capture the given current topic sequence to guide conversational recommendation. Our work follows the task definition in <ref type="bibr">(Zhou et al., 2020b)</ref>, mainly focusing on topic transition and next-turn topic prediction problem in multi-turn dialogues, given the topic sequence. However, all previous approaches only model the topic-transition information on the current sequence. In contrast, our work learns topic transition information across all sequences to enhance the topic transfer modeling of the current sequence.</p><p>Topic-Aware Response Generation Most of early studies on topic-related dialogue fall into two categories, i.e., implicit topic-based <ref type="bibr" target="#b24">(Shang et al., 2015;</ref><ref type="bibr" target="#b23">Serban et al., 2016;</ref><ref type="bibr" target="#b26">Tian et al., 2017)</ref> and explicit topic-based <ref type="bibr" target="#b32">(Xing et al., 2017;</ref><ref type="bibr" target="#b9">Wang et al., 2018;</ref><ref type="bibr" target="#b6">Dziri et al., 2019;</ref><ref type="bibr">Liu et al., 2022a)</ref>. The former aims to model contexts at multiple semantic levels (i.e., topic, style, etc) to capture the dynamic conversation topic flow while ignoring the informativeness of responses. The latter generates appropriate utterances on the conversation utterances and given topic information, which relies heavily on manually predefined topic sequences. Due to the excellent performance of the PrLMs on the dialog generation task, <ref type="bibr" target="#b18">Liu et al. (2021)</ref> and <ref type="bibr">Zhou et al. (2020b)</ref> choose to encode topic information and context by the PrLM to obtain informative and fluent responses. However, it may suffer from topic-noise problem (i.e., raw topic fitting and similar topic selection) when generating responses. In contrast, our work leverages several topics to a prior distribution, using posterior information on already known topic to guide the generation of prior distribution that affects the generation process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>We propose a novel model for Topic-guided Dialogue. Figure <ref type="figure" target="#fig_1">2</ref> presents the architecture of our model, which comprises three main components: 1) Global Sequential Topic Attention layer; 2) Point-Wise Feed-Forward Network Prediction layer; 3) Prior and Posterior Response Generation layer. We next present the three components and parameter modeling in detail.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Problem Statement</head><p>Let C = u 1 , u 2 , . . . , u |C| be a multi-turn conversation; let S = {t 1 , t 2 , . . . , t l } be the topic sequence of conversation C, and let T = t 1 , t 2 , . . . , t |T | be all of topics. Given the conversation C and topic sequence S, the task of topic guide conversation aims to predict the top-N topics (1 ≤ N ≤ |T |) from T at turn k and generate appropriate response u |C|+1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Global Sequential Topic Attention layer</head><p>In this section, we will describe how to perform serialized embedding of topics in Section 3.2.1 and how to construct MSN distribution based on sequential information and global information in Section 3.2.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Embedding layer</head><p>To obtain a better semantic representation of the topic words in the sequence, we encode the topic sequences using BERT <ref type="bibr" target="#b5">(Devlin et al., 2019)</ref>; also, to bring the model to aware previous topics information while addressing the sequence, we add position embedding for each topic. We use the latest n topics in S, where n ≥ l, and we pad constant zero vector to make each sequence length equal. Specifically, the topic embedding matrix is defined as E ∈ R |T |×d , where d denotes the topic embeddings' dimensionality and E is the estimated by BERT. We extract the input matrix E ∈ R n×d , where E s = E Ts . Inspired by <ref type="bibr" target="#b9">Kang and McAuley (2018)</ref>, we inject a learnable position embedding P ∈ R n×d into the input embedding:</p><formula xml:id="formula_0">X = [e i ∥ p l-i+1 ]<label>(1)</label></formula><p>where e i ∈ E s and p i ∈ P. We make the reverse location embedding injection due to the unfixed length of the topic sequence. Compared with the regular forward location, the distance between each topic and the predicted topic contains certain effective information <ref type="bibr" target="#b29">(Wang et al., 2020;</ref><ref type="bibr" target="#b37">Zou et al., 2022)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Multivariate Skew-Normal Distribution</head><p>The core of our model is constructed based on Transformer's Multi-Head Attention, we choose Transformer due to its excellent ability in sequence modeling problem <ref type="bibr" target="#b9">(Kang and McAuley, 2018;</ref><ref type="bibr" target="#b10">Kim et al., 2020;</ref><ref type="bibr" target="#b8">Ji et al., 2020)</ref>. For the Scaled-Dot Attention in Multi-Head Attention, given the Equation (2):</p><formula xml:id="formula_1">Attn (Q, K, V ) = softmax( QK T √ d k )V (2)</formula><p>Since the key to attention mechanism is querying via the alignment score, we turn the alignment score (i.e., scaled dot product of Q and K T ) into a latent variable z, which allows us to adapt the model to apply the mandatory information flexibly, including global-level transition and topic relativities. Despite the superior results demonstrated by multivariate normal distribution in modeling the covariance matrix (Fisher and Sun, 2011), which exhibits the intra-sequence topic correlation, due to its forced symmetric shape of the density curve (i.e., suffering from modeling skewness), we devise z to follow the Multivariate Skewed Normal (MSN) distribution <ref type="bibr" target="#b2">(Azzalini and Valle, 1996)</ref>. According to <ref type="bibr" target="#b1">Azzalini and Capitanio (1999)</ref>, z ∈ R k is continuous with density function as:</p><formula xml:id="formula_2">f (z) = 2ϕ k (z -ξ; Σ)Φ{α T ω -1 (z -ξ)} (3)</formula><p>where Σ = ωψω is the covariance matrix and ψ ∈ R k×k denotes correlation parameter, as well as ξ = (ξ 1 , ..., ξ k ) T , ω = diag(ω 1 , ..., ω k ) and α ∈ R k denote the location, scale and shape parameters respectively. Moreover, ϕ k is the kdimensional multivariate normal density with the mean ξ and the covariance Σ, and Φ(•) is the N (0, 1) distribution function. For clarity, we denote the above distribution obediently as Equation (4) and follow the original notation:</p><formula xml:id="formula_3">z ∼ M SN (Z | ξ, Σ, α)<label>(4)</label></formula><p>As we need to sample the softmax parameter values from MSN distribution by which to revise the alignment score, we adapt Equation (2) to derive the following equation:</p><formula xml:id="formula_4">H = SGTA(T seq , G b ) = softmax(Z)V (5)</formula><p>where T seq denotes the input topic sequence, G b denotes the global-level topic transition cooccurrence matrix, which is constructed by statistically calculating the topic co-occurrence in the whole dataset. Furthermore, H = {h 1 , ..., h n } is denoted as hidden layer output with h i ∈ R d . All parameters modeling will be explained in detail in subsequent sections.</p><p>Location The Location ξ represents the mean of the distribution. Considering that we demand the deterministic alignment scores with maximum likelihood to facilitate sampling the MSN distribution, we manipulate the alignment score as:</p><formula xml:id="formula_5">ξ = LeakyReLU( (T seq W Q l )(T seq W K l ) √ d )<label>(6)</label></formula><p>To keep the standard deviation positive, we choose LeakyReLU as the activation function.</p><p>Covariance The covariance Σ indicates the relation between two different topics, which is formed by two subparameters ω and ψ as we mentioned before. For scale ω, given a topic sequence S with topic position S i p and S j p , we normalize them to x i and x j and infer the variance ω 2 i , ω 2 j ∈ R + of z respectively, by amortization inference as in Equation <ref type="bibr">(7)</ref>.</p><formula xml:id="formula_6">ω m = LeakyReLU( (x n W Q ω )(x m W K ω ) √ d )<label>(7)</label></formula><p>As for correlation ψ, we adopt the kernel function approach for the mixing calculation, since it can efficiently and nonlinearly compute the inner product of samples in the feature space and calculate high-dimensional distance measures. It's well known that proximal distance metric illustrates proximate relationship. Especially, we comprise a hybrid kernel function for building task-aware metric. Different from <ref type="bibr" target="#b25">Tang et al. (2019)</ref> and <ref type="bibr" target="#b8">Ji et al. (2020)</ref>, we consider global co-occurrence level topic transition information to construct kernel functions for metrics, nevertheless they build only based on the embedding similarity, which is difficult to obtain sufficient information. We then describe the specific implementation of kernel function.</p><p>• Co-occurrence kernel is determined by the number of co-occurrence between pairwise topics, which is linearly dependent with the topic pairs. The co-occurrence kernel is designed as:</p><formula xml:id="formula_7">k co (x i , x j ) = ω i ω j log( P β ij p i p j )<label>(8)</label></formula><p>where P ij is the co-occurrence value of x i and x j in the common sequence and p i , p j are their individual occurrence values. β is the factor that determines the impact degree of the co-occurrence value P ij .</p><p>• Topic pair kernel is determined by topic transition pattern pair representation, which is strongly correlated with the topic pairs association. The topic pair kernel k tp (x i , x j ) is designed as:</p><formula xml:id="formula_8">k tp = ω i ω j (exp(-γ∥x i -x j ∥ 2 ) + x i x j ) (9)</formula><p>which combines both Gaussian kernel and . In particular, the Gaussian kernel is primarily used to characterize the similarity between samples, and γ &gt; 0 is the unique hyperparameter of Gaussian kernel function.</p><p>To make full use of the above two part information, we design the above two kernel functions to be summed as shown in Equation (10):</p><formula xml:id="formula_9">k(x i , x j ) = k co (x i , x j ) + ηrk tp (x i , x j ) (10) where r = softmax(xW x + b x ), W x ∈ R d×d , b x ∈ R d</formula><p>and η denote the learnable parameters. After relation modeling, we set the correlation matrix ψ ij = k(x i ,x j ) ω i ω j and substitute ψ into Σ = ωψω to infer the covariance Σ. Shape The shape parameter α reflects the relation of each topic in the sequence with the last topic. It contains the consideration for relative position information, while this correlation helps the model to learn the implicit transition pattern between topiclevel and position-level relations. Specifically, we let α i = s n-i α i max( α) represents correlation between the final topic t n and topic t i , where α is a ratio parameter which mirrors the estimation and consideration at topic-level, while s n-i is a relative scaling parameter with position-level information.</p><p>We use the co-occurrence matrix G b to calculate the ratio parameter α i , divided into intra-sequence level and global level parts, representing the sum of linear arrangement from t i to t n and influence factors summation of the top-m frequently cooccurring topics with t i , respectively. g i,j is the i-th row and j-th column value of G b (i.e., the value of t i and t j ). The detailed formula for α i is given below:</p><formula xml:id="formula_10">α i = n j=1 g i,j g j,n intra-sequence level + m l=1 g i,k l g k l ,n global level (11)</formula><p>Equation ( <ref type="formula">11</ref>) calculates α i by utilizing the sum of two dot products, which exhibits the correlation between t i and t n , where g i,k l stands for the l-th element after decreasing the order of all g i,k values with k ∈ |T |, and m is an adjustable variable. Inspired by <ref type="bibr" target="#b8">Ji et al. (2020)</ref>, we utilize the average of the pairwise dot product sums of the remaining topics in the sequence to replace g j,j since it is an invalid value in matrix G b , which can be expressed as:</p><formula xml:id="formula_11">g j,j ⇐= p∈{1,...,n}\j g j,p n -1 average of remaining co-occurrence<label>(12)</label></formula><p>Similar to ω in section Covariance, we define the scaling parameter s n-i as Eq. ( <ref type="formula">13</ref>):</p><formula xml:id="formula_12">s n-i = LeakyReLU( (x n W Q s )(x i W K s )|n -i| √ d )</formula><p>(13) Loss function Given the above construction of the distribution parameters, the latent variable z needs to be inferred according to MSN distribution. Following Kingma and Welling (2013), z can be inferred by optimizing the lower bound on the evidence of Jensen's inequality for the marginal logarithm p(y n ) while predicting the (n + 1)-th topic. We present the loss function as shown in Eq. ( <ref type="formula">14</ref>).</p><formula xml:id="formula_13">L z (θ) = E z [log p (y n | z)] ≤ log p (y n | z) p(z)dz = log p (y n ) (14)</formula><p>We also design the co-occurrence loss L rank by listwise ranking loss <ref type="bibr" target="#b3">(Cao et al., 2007)</ref> to match topic-co-occurrence relevance and ranking consistency. The total loss L is defined as the sum of the co-occurrence loss L rank and the latent variable loss L z :</p><formula xml:id="formula_14">L = L z + δL rank (<label>15</label></formula><formula xml:id="formula_15">)</formula><p>where δ is a hyperparameter. We also use the reparameterization trick <ref type="bibr">(Kingma et al., 2015)</ref> to ensure that the model is trainable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Point-Wise Feed-Forward Network Prediction layer</head><p>We apply the PointWise Feed-Forward network in Transformer to the output of the model and incorporate location dependent information. The pointwise feedforward network consists of two linear layers and the activation layer. The output F = {FFN(h 1 ), ..., FFN(h d )}. We also stacks b self-attentive blocks to adaptively and hierarchically extract previous consumed topic information and learn complex topic transition patterns.</p><p>After the above adapted query attention, we predict the next topic (given the first n topics) based on</p><formula xml:id="formula_16">F (b) n : r i,n = F (b) n E i (<label>16</label></formula><formula xml:id="formula_17">)</formula><p>where r i,n is the relevance of topic t i being the next topic given the first n topics (i.e., t 1 , ..., t n ), and E i is the BERT embedding of topic t i . Empirically, high relevance represents a compact transfer relation, so we use ranking r to make predictions for next topic t i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Prior and Posterior Response Generation layer</head><p>After obtaining the predicted topic words, it is essential to employ them flexibly to generate smooth response for completing exhaustive dialogue interaction. Most of the previous methods select topics for generating responses based on the similarity between the previous topic and next turn's topic or context, which can be regarded as the topic prior distribution, however there are actual conversation scenarios in which multiple candidate topics are pertinent for the previous topic transitions (e.g., movie→music and movie→friend). Both music and friend can be considered as the next topic word for movie. Nevertheless, with the posterior distribution constructed from query and response, model can be more empirical in selecting the appropriate topics for generation.</p><p>Particularly, during training, we design that the posterior distribution p(t = t i |q, r) containing the response information r is approximated by the prior distribution p(t = t i |q) containing only historical information q, which includes context c and history topic sequence t 1:n-1 (i.e., q = {c, t 1:n-1 }). We evaluate the approximation loss with KL-Div Loss, as shown in the following equation:</p><formula xml:id="formula_18">L KL (θ) = N i=1 p(t = t i |q, r) log p(t = t i |q, r) p(t = t i |q) (17)</formula><p>The posterior distribution can guide the model to generate natural responses according to secondary selected topic,and converging the semantic transition range while generation. Compared to the previous method, BOW Loss serves as a part of the overall loss for evaluating each word of the predicted responses, while allowing for a better fit to the true responses, and meanwhile ensure the controllability of the generated utterances. Following <ref type="bibr" target="#b14">Lian et al. (2019)</ref>, the BOW Loss is defined as:</p><formula xml:id="formula_19">L BOW (θ) = -E t i ∼p(t|c,r) m j=1 log p(r j |t i ) (18)</formula><p>where θ denotes the model parameters in Eq. ( <ref type="formula">17</ref>) &amp; Eq. ( <ref type="formula">18</ref>). Additionally, we apply Transformer <ref type="bibr" target="#b27">(Vaswani et al., 2017)</ref> as the decoder of our model to generate outstanding and convincing responses. For name consistency, we denote this generation method via SGTA in the whole paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head><p>4.1 Dataset TG-ReDial dataset The TG-ReDial dataset <ref type="bibr">(Zhou et al., 2020b</ref>) is composed of 10,000 conversations between seekers and recommenders. It contains a total of 129,392 utterances from 1,482 users, covering 2,571 topics. The dataset is constructed in a topic-guided manner, where each conversation includes a topic sequence as well as a conversation target, and both parties communicate sequentially according to the topic sequence to accomplish the target and thus achieve the recommendation. It is</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model with target without target</head><p>Hit@1 Hit@3 Hit@5 Hit@1 Hit@3 Hit@5 PMI notable that the conversation targets are optional in our task setting as additional information in the model input, and we will give a detailed experimental comparison later. On average, each conversation in the TG-ReDial dataset has 7.9 topics and a utterance contains 19 words.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation details</head><p>We implemented SGTA and related baseline experiments in PyTorch. The default parameters for all experiments are set as follows: we set the batch size to 16 and the embedding size is set to 768. We used two self-attention blocks and one head for sequence modeling following <ref type="bibr" target="#b9">Kang and McAuley (2018)</ref>. It is worth noting that we constructed the co-occurrence matrix only from the training set data. We used Adam optimizer (Kingma and Ba, 2015) with a learning rate initialized to 1e -5 and the dropout rate is set to 0.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Baselines</head><p>Our baselines for assess the performance of topic prediction and response generation come in two groups: Topic Guide Prediction: (1) PMI and Kernel <ref type="bibr" target="#b25">(Tang et al., 2019)</ref> employ the forcing strategy of selecting keywords with higher similarity to the target. (2) DKRN <ref type="bibr" target="#b20">(Qin et al., 2020)</ref> introduces a graph routing mechanism for keyword search based on <ref type="bibr" target="#b25">Tang et al. (2019)</ref>. ( <ref type="formula">3</ref>) MGCG <ref type="bibr" target="#b19">(Liu et al., 2020)</ref> adopt a completion judgment mechanism in the exploration of topic sequence. (4) CKC <ref type="bibr">(Zhong et al., 2021)</ref> follows the work of ( <ref type="formula" target="#formula_0">1</ref>) and ( <ref type="formula">2</ref>), introduces the commonsense graph and utilizes the GNN method to select keywords. (5) Conversation-BERT, Topic-BERT and TG-CRS <ref type="bibr">(Zhou et al., 2020b)</ref> respectively input the context, historical topic sequence and the concatenation of them to BERT for encoding and predicting.</p><p>The above baselines are briefly divided into two sub-categories, similarity-based (i.e., PMI, Kernel, DKRN and CKC) and sequence-based (i.e., MGCG, Conversation-BERT, Topic-BERT and TG-CRS). The former predicts based on the similarity of next turn topic and target while the latter predicts based on the topic sequence and context.</p><p>Topic Response Generation: (1) KBRD <ref type="bibr" target="#b4">(Chen et al., 2019)</ref> enhances the transformers' decoder with keywords searched on the knowledge graph.</p><p>(2) KGSF <ref type="bibr">(Zhou et al., 2020a)</ref> incorporates both word-oriented and entity-oriented knowledge graphs to enhance response. (3) MGCG adopts multi-task learning framework to predict topics and generates responses with posterior distributions. ( <ref type="formula" target="#formula_3">4</ref>) Transformer <ref type="bibr" target="#b27">(Vaswani et al., 2017)</ref> is a widely used multi-head attention encoder-decoder structure. ( <ref type="formula">5</ref>) TG-CRS utilizes the predicted word concating context input to GPT-2 <ref type="bibr">(Radford et al.)</ref> to generate responses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Evaluation metrics</head><p>Automatic evaluation To evaluate the topic prediction task, following the previous work <ref type="bibr" target="#b25">(Tang et al., 2019;</ref><ref type="bibr">Zhou et al., 2020b;</ref><ref type="bibr" target="#b19">Liu et al., 2020)</ref>, we adopt Hit@k(k = 1, 3, 5) as metric for ranking all the possible topics (topic keywords recall at position k in all keywords). To evaluate the response generation task, following <ref type="bibr">Zhou et al. (2020b)</ref>  adopt perplexity (PPL), BLEU-n(n = 1, 2) and Distinct-n(n = 1, 2) for examining the fluency and informativeness of the responses.</p><p>Human evaluation For generation tasks, it is necessary to employ manual evaluation to test the ability of the models to make topic-related informative responses. We randomly select 100 dialogues from our model and baseline, recruit four annotators to evaluate several models in two aspects, i.e., fluency and informativeness. The former measures whether the generated responses are fluent, while the latter measures whether the system introduces relevant topics in response. Score ranges from 0 to 2, and the final average score is calculated as the result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Performance on Topic Prediction</head><p>Table <ref type="table" target="#tab_1">1</ref> shows the experimental results of topic prediction task. PMI and Kernel do not perform well, since they cannot consider the topic sequence context. CKC performs significantly better than other similarity-based baselines as it exploits the external information in the commonsense knowledge graphs. We notice that TG-CRS outperforms the other baselines since it jointly models dialogue context, topic sequence, and user profile. SGTA gives an increase of 1.3%, 4.5%, and 4.4% over TG-CRS in terms of Hit@1/3/5 respectively with consideration of target information, as SGTA effectively leverages the topic-transition patterns in other topic sequences. It should be clarified that since userlevel information is not taken into account in our task, our experimental set-up that contains the target differs from the original set-up, where we concatenate the target word embedding as additional information with the topic sequence. It is easy to figure that conversation topic transfer tends to be more uncontrollable without considering target information, where only CKC can make predic-tions in the similarity-based approach due to its having sufficient external information as a prediction base. Moreover, in the sequence-based methods, Topic-BERT is fairly better than Conversation-BERT, which shows the effectiveness of topic sequence. SGTA benefits from better topic sequence modeling, improves over the best baseline by 3.1%, 5.4% and 4.2% on Hit@1/3/5, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Performance on Response Generation</head><p>Table 2 exhibits the generation performance of automatic and manual metrics on the TG-REDIAL dataset. For automatic evaluation, we find that TG-CRS performs best in the baselines, indicating the robustness of the pre-trained model for the generation task. KGSF outperforms KBRD and MGCG in terms of diversity because it combines KG-enhanced item and word embedding to generate the utterance. Transformer performs better on the BLEU metric due to its word-by-word attention mechanism. SGTA performs best on BLEU and Distinct using a prior and posterior approximation without depending on pre-trained language models, improving 7.5%, 23%, 9.5% and 32% over TG-CRS at BLEU-1/2 as well as Distinct-1/2 respectively, though PPL is slightly weaker than TG-CRS's effect. As for the manual results, SGTA improves 2.8% compared to TG-CRS in the informativeness dimension, owing to the fact that the posterior approach makes model learn reasonable topic choices easily, and also SGTA's performance is in the first tier in terms of fluency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7">Ablation study</head><p>We conduct ablation study to assess the importance of global co-occurrence (w/o global), intrasequence position (w/o intra-pos), topic sequence context (w/o sequen), as well as the MSN distri- bution (w/o MSN), and results are presented in Table 3. Concretely, after removing global topic cooccurrence information (w/o global), the average reduction in our model effect is about for 6.6%, which demonstrates the desirability of extracting global information. On this basis, the effect of the model is improved by only removing the intrasequence position information (w/o intra-pos), but it is still lower than our model by 4.2%. Moreover, the experiment to remove the MSN distribution (w/o MSN) show that the information selection with skewness is effective under the premise of reasonable modeling parameters. Finally, experiments with the removing of sequential transformer modeling (w/o sequen) show the key role of both in the model structure.</p><p>We also conduct ablation study to assess the performance of MSN distribution three parameters: Location ξ (w/o MSN-ξ), Covariance Σ (w/o MSN-Σ) and Shape α (w/o MSN-α) and Table <ref type="table" target="#tab_5">4</ref> shows the results. We use the experimental design with the parameter set to zero, and each ablation study represents the effect of removing only the individual parameter separately. Specifically, shape parameter α and the covariance parameter Σ perform more important effects in the distribution construction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper, we focus on topic-grounded controllable dialogue tasks. We propose a new approach named SGTA. It models the latent space as a MSN distribution utilizing global information, intra-sequence semantic and position information, which allows the model to better integrate the relationships between information and make topic predictions based on the results of distribution sampling. We also utilize a prior-posterior distribution approach to generate a new topic response. Extensive experiments on TG-ReDial dataset show that SGTA achieves state-of-the-art performance on prediction and generation tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Graphical illustration of topic prediction.Given the current topic sequence s a and other topic sequences (e.g., s x and s y ), the global co-occurrence matrix c is obtained with all topic sequences. Parameters of MSN distribution d are constructed from s a and c in the latent space, and the next turn topic t a l +1 will be predicted based on the latent variable z obtained after sampling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The overall structure of the proposed model SGTA, which consists of (1) a sequential modeling module, (2) a prediction module and (3) a generation module.</figDesc><graphic coords="3,77.41,76.21,190.68,144.74" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 1 :</head><label>1</label><figDesc>0.8523 * 0.8671 * 0.4487 * 0.6030 * 0.6546 * Automatic evaluation of topic predictions task. Bold text indicates the best result. Significant improvements compared to the best baseline are marked with * (t-test, p &lt; 0.05).</figDesc><table><row><cell></cell><cell>0.0349</cell><cell>0.0927</cell><cell>0.1290</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>Kernel</cell><cell>0.0418</cell><cell>0.0957</cell><cell>0.1125</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>DKRN</cell><cell>0.4015</cell><cell>0.4821</cell><cell>0.5068</cell><cell>-</cell><cell>-</cell><cell>-</cell></row><row><cell>CKC</cell><cell>0.5914</cell><cell>0.7857</cell><cell>0.8265</cell><cell>0.3269</cell><cell>0.4505</cell><cell>0.4961</cell></row><row><cell>MGCG</cell><cell>0.5861</cell><cell>0.7528</cell><cell>0.8094</cell><cell>0.3157</cell><cell>0.4386</cell><cell>0.4483</cell></row><row><cell>Conversation-BERT</cell><cell>0.6072</cell><cell>0.7893</cell><cell>0.8105</cell><cell>0.2952</cell><cell>0.4099</cell><cell>0.4585</cell></row><row><cell>Topic-BERT</cell><cell>0.6104</cell><cell>0.7966</cell><cell>0.8147</cell><cell>0.4185</cell><cell>0.5520</cell><cell>0.5971</cell></row><row><cell>TG-CRS</cell><cell>0.6128</cell><cell>0.8157</cell><cell>0.8302</cell><cell>0.4352</cell><cell>0.5726</cell><cell>0.6291</cell></row><row><cell>SGTA</cell><cell>0.6208</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p>*  </p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head></head><label></label><figDesc>, we</figDesc><table><row><cell>Model</cell><cell>PPL</cell><cell cols="6">Automatic BLEU-1 BLEU-2 Distinct-1 Distinct-2 Fluency Informativeness Human</cell></row><row><cell>KBRD</cell><cell>28.022</cell><cell>0.221</cell><cell>0.028</cell><cell>0.004</cell><cell>0.008</cell><cell>1.16</cell><cell>1.30</cell></row><row><cell>KGSF</cell><cell>40.758</cell><cell>0.239</cell><cell>0.042</cell><cell>0.015</cell><cell>0.064</cell><cell>1.35</cell><cell>1.35</cell></row><row><cell>MGCG</cell><cell>12.386</cell><cell>0.256</cell><cell>0.061</cell><cell>0.012</cell><cell>0.041</cell><cell>1.31</cell><cell>1.26</cell></row><row><cell cols="2">Transformer 32.856</cell><cell>0.287</cell><cell>0.071</cell><cell>0.013</cell><cell>0.083</cell><cell>1.35</cell><cell>1.31</cell></row><row><cell>TG-CRS</cell><cell>7.223</cell><cell>0.280</cell><cell>0.065</cell><cell>0.021</cell><cell>0.094</cell><cell>1.42</cell><cell>1.43</cell></row><row><cell>SGTA</cell><cell>8.539</cell><cell>0.301  *</cell><cell>0.080  *</cell><cell>0.023  *</cell><cell>0.124  *</cell><cell>1.41</cell><cell>1.47</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 2 :</head><label>2</label><figDesc>Automatic evaluation and human evaluation of response generation task. Bold text indicates the best result. Significant improvements compared to the best baseline are marked with * (t-test, p &lt; 0.05).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 3 :</head><label>3</label><figDesc>Results of the ablation study for topic prediction task.</figDesc><table><row><cell>Model</cell><cell>Hit@1</cell><cell>with target Hit@3</cell><cell>Hit@5</cell></row><row><cell>SGTA</cell><cell cols="3">0.6208  *  0.8523  *  0.8671  *</cell></row><row><cell>w/o global</cell><cell>0.5852</cell><cell>0.7863</cell><cell>0.8121</cell></row><row><cell>w/o intra-pos</cell><cell>0.6081</cell><cell>0.8034</cell><cell>0.8256</cell></row><row><cell>w/o sequen</cell><cell>0.5749</cell><cell>0.7687</cell><cell>0.7905</cell></row><row><cell>w/o MSN</cell><cell>0.5895</cell><cell>0.7993</cell><cell>0.8203</cell></row><row><cell>Model</cell><cell>Hit@1</cell><cell>with target Hit@3</cell><cell>Hit@5</cell></row><row><cell>SGTA</cell><cell cols="3">0.6208  *  0.8523  *  0.8671  *</cell></row><row><cell>w/o MSN</cell><cell>0.5895</cell><cell>0.7993</cell><cell>0.8203</cell></row><row><cell>w/o MSN-ξ</cell><cell>0.6097</cell><cell>0.8107</cell><cell>0.8343</cell></row><row><cell>w/o MSN-Σ</cell><cell>0.5964</cell><cell>0.8039</cell><cell>0.8267</cell></row><row><cell>w/o MSN-α</cell><cell>0.5982</cell><cell>0.8051</cell><cell>0.8255</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head>Table 4 :</head><label>4</label><figDesc>Results of the ablation study for MSN parameters.</figDesc><table /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgement</head><p>This work was supported in part by the <rs type="funder">National Natural Science Foundation of China</rs> under Grant No. <rs type="grantNumber">62276110</rs>, Grant No.<rs type="grantNumber">61772076</rs>, in part by <rs type="funder">CCF-AFSG Research Fund</rs> under Grant No.<rs type="grantNumber">RF20210005</rs>, and in part by the <rs type="funder">fund of Joint Laboratory of HUST</rs> and <rs type="institution">Pingan Property &amp; Casualty Research (HPL)</rs>. The authors would also like to thank the anonymous reviewers for their comments on improving the quality of this paper.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_gM2JPDs">
					<idno type="grant-number">62276110</idno>
				</org>
				<org type="funding" xml:id="_2xQgbGs">
					<idno type="grant-number">61772076</idno>
				</org>
				<org type="funding" xml:id="_TqeHyys">
					<idno type="grant-number">RF20210005</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations</head><p>Our work suffers from the following limitations:</p><p>(1) lack of experimental results on a more topic conversation dataset. Although there are few existing topic conversation datasets and TG-ReDial is an explicit dataset at the topic level, experiments on more datasets are necessary for the generalizability of the method. (2) Our topic prediction has yet to be improved in mining global information at the dataset level. After sampling and verifying the experimental results we believe that the extraction by topic word co-occurrence has some shortcomings, such as words with more co-occurrence may not appear under the current round, which requires the integration of context as well as semantics.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Adiwardana</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minh-Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">R</forename><surname>So</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jamie</forename><surname>Hall</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noah</forename><surname>Fiedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Romal</forename><surname>Thoppilan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Apoorv</forename><surname>Kulshreshtha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Nemade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yifeng</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2001.09977</idno>
		<title level="m">Towards a human-like open-domain chatbot</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Statistical applications of the multivariate skew normal distribution</title>
		<author>
			<persName><forename type="first">A</forename><surname>Azzalini</surname></persName>
		</author>
		<author>
			<persName><surname>Capitanio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Statistical Methodology</title>
		<imprint>
			<date type="published" when="1999">1999</date>
			<biblScope unit="page" from="579" to="602" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The multivariate skew-normal distribution</title>
		<author>
			<persName><forename type="first">Adelchi</forename><surname>Azzalini</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A Dalla</forename><surname>Valle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">83</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="715" to="726" />
			<date type="published" when="1996">1996</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning to rank: from pairwise approach to listwise approach</title>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tao</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Feng</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th international conference on Machine learning</title>
		<meeting>the 24th international conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
			<biblScope unit="page" from="129" to="136" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Towards knowledge-based recommender dialog system</title>
		<author>
			<persName><forename type="first">Qibin</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junyang</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yichang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yukuo</forename><surname>Cen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongxia</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1803" to="1813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">BERT: Pre-training of deep bidirectional transformers for language understanding</title>
		<author>
			<persName><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming-Wei</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kenton</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N19-1423</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Long and Short Papers</title>
		<meeting>the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Minneapolis, Minnesota</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="4171" to="4186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Augmenting neural response generation with context-aware topical attention</title>
		<author>
			<persName><forename type="first">Nouha</forename><surname>Dziri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ehsan</forename><surname>Kamalloo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kory</forename><surname>Mathewson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Osmar</forename><surname>Zaiane</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W19-4103</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Workshop on NLP for Conversational AI</title>
		<meeting>the First Workshop on NLP for Conversational AI<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="18" to="31" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Improved stein-type shrinkage estimators for the highdimensional multivariate normal covariance matrix</title>
		<author>
			<persName><forename type="first">J</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoqian</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Statistics &amp; Data Analysis</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1909" to="1918" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Sequential recommendation with relation-aware kernelized selfattention</title>
		<author>
			<persName><forename type="first">Mingi</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weonyoung</forename><surname>Joo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyungwoo</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoon-Yeong</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Il-Chul</forename><surname>Moon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI conference on artificial intelligence</title>
		<meeting>the AAAI conference on artificial intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="4304" to="4311" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Selfattentive sequential recommendation</title>
		<author>
			<persName><forename type="first">Wang-Cheng</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julian</forename><surname>Mcauley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2018 IEEE International Conference on Data Mining (ICDM)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="197" to="206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Sequential latent knowledge selection for knowledge-grounded dialogue</title>
		<author>
			<persName><forename type="first">Byeongchang</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaewoo</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">8th International Conference on Learning Representations, ICLR 2020</title>
		<meeting><address><addrLine>Addis Ababa, Ethiopia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020-04-26">2020. April 26-30, 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR (Poster)</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Autoencoding variational bayes</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Variational dropout and the local reparameterization trick</title>
		<author>
			<persName><forename type="first">Tim</forename><surname>Durk P Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">28</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning to select knowledge for response generation in dialog systems</title>
		<author>
			<persName><forename type="first">Rongzhong</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jinhua</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2019/706</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19</title>
		<meeting>the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5081" to="5087" />
		</imprint>
	</monogr>
	<note>International Joint Conferences on Artificial Intelligence Organization</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">2022a. Incorporating casual analysis into diversified and logical response generation</title>
		<author>
			<persName><forename type="first">Jiayi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhixuan</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xing</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulin</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 29th International Conference on Computational Linguistics</title>
		<meeting>the 29th International Conference on Computational Linguistics<address><addrLine>Gyeongju, Republic of Korea</addrLine></address></meeting>
		<imprint>
			<biblScope unit="page" from="378" to="388" />
		</imprint>
	</monogr>
	<note>International Committee on Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Rui Fang, and Dangyang Chen. 2022b. Improving personality consistency in conversation by persona extending</title>
		<author>
			<persName><forename type="first">Yifan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianling</forename><surname>Mao</surname></persName>
		</author>
		<idno type="DOI">10.1145/3511808.3557359</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st ACM International Conference on Information amp; Knowledge Management, CIKM &apos;22</title>
		<meeting>the 31st ACM International Conference on Information amp; Knowledge Management, CIKM &apos;22<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<biblScope unit="page" from="1350" to="1359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">2022c. Declaration-based prompt tuning for visual question answering</title>
		<author>
			<persName><forename type="first">Yuhang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daowan</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feida</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2022/453</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI-22</title>
		<meeting>the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI-22</meeting>
		<imprint>
			<biblScope unit="page" from="3264" to="3270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">DuRecDial 2.0: A bilingual parallel corpus for conversational recommendation</title>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng-Yu</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.356</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Dominican Republic</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="4335" to="4347" />
		</imprint>
		<respStmt>
			<orgName>Online and Punta Cana</orgName>
		</respStmt>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Towards conversational recommendation over multi-type dialogs</title>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng-Yu</forename><surname>Niu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wanxiang</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ting</forename><surname>Liu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.98</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1036" to="1049" />
		</imprint>
	</monogr>
	<note>Online. Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dynamic knowledge routing network for target-guided open-domain conversation</title>
		<author>
			<persName><forename type="first">Jinghui</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianheng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="8657" to="8664" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Language models are unsupervised multitask learners</title>
		<author>
			<persName><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rewon</forename><surname>Child</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Recipes for building an open-domain chatbot</title>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Roller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Dinan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Da</forename><surname>Ju</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mary</forename><surname>Williamson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">Michael</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y-Lan</forename><surname>Boureau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.eacl-main.24</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</title>
		<meeting>the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="300" to="325" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Building end-to-end dialogue systems using generative hierarchical neural network models</title>
		<author>
			<persName><forename type="first">Iulian</forename><surname>Serban</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Sordoni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Neural responding machine for short-text conversation</title>
		<author>
			<persName><forename type="first">Lifeng</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengdong</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hang</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/P15-1152</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1577" to="1586" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Targetguided open-domain conversation</title>
		<author>
			<persName><forename type="first">Jianheng</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tiancheng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenyan</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eric</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiting</forename><surname>Hu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1565</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="5624" to="5634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">How to make context more useful? an empirical study on contextaware neural conversational models</title>
		<author>
			<persName><forename type="first">Zhiliang</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lili</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiping</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yansong</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongyan</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P17-2036</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Vancouver, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="231" to="236" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Chat more: Deepening and widening the chatting topic via a deep model</title>
		<author>
			<persName><forename type="first">Wenjie</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin-Shun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fumin</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liqiang</forename><surname>Nie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 41st international acm sigir conference on research &amp; development in information retrieval</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="255" to="264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Global context enhanced graph neural networks for session-based recommendation</title>
		<author>
			<persName><forename type="first">Ziyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gao</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao-Li</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian-Ling</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minghui</forename><surname>Qiu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 43rd International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="169" to="178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Target-guided emotion-aware chat machine</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiayi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xianling</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guibing</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feida</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuchong</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shanshan</forename><surname>Feng</surname></persName>
		</author>
		<idno type="DOI">10.1145/3456414</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Inf. Syst</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">24</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Proactive human-machine conversation with explicit conversation goal</title>
		<author>
			<persName><forename type="first">Wenquan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiangyang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hua</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rongzhong</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Wang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1369</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3794" to="3804" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Topic aware neural response generation</title>
		<author>
			<persName><forename type="first">Chen</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yalou</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ming</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei-Ying</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Generating informative and diverse conversational responses via adversarial information maximization</title>
		<author>
			<persName><forename type="first">Yizhe</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michel</forename><surname>Galley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianfeng</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhe</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiujun</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Brockett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Dolan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">;</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chunyan</forename><surname>Miao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems, 31. Peixiang Zhong</title>
		<imprint>
			<date type="published" when="2018">2018. 2021</date>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="14568" to="14576" />
		</imprint>
	</monogr>
	<note>Proceedings of the AAAI Conference on Artificial Intelligence</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Improving conversational recommender systems via knowledge graph based semantic fusion</title>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shuqing</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanhang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingsong</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1006" to="1014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Towards topic-guided conversational recommender system</title>
		<author>
			<persName><forename type="first">Kun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuanhang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wayne</forename><forename type="middle">Xin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoke</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji-Rong</forename><surname>Wen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.coling-main.365</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Computational Linguistics</title>
		<meeting>the 28th International Conference on Computational Linguistics<address><addrLine>Barcelona, Spain (Online</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4128" to="4139" />
		</imprint>
	</monogr>
	<note>International Committee on Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning from perturbations: Diverse and informative dialogue generation with inverse adversarial training</title>
		<author>
			<persName><forename type="first">Wangchunshu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qifei</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenle</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.acl-long.57</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</title>
		<meeting>the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="694" to="703" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Multilevel cross-view contrastive learning for knowledgeaware recommender system</title>
		<author>
			<persName><forename type="first">Ding</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xian-Ling</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minghui</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Feida</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Cao</surname></persName>
		</author>
		<idno type="DOI">10.1145/3477495.3532025</idno>
	</analytic>
	<monogr>
		<title level="m">SIGIR &apos;22: The 45th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting><address><addrLine>Madrid, Spain</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2022-07-11">2022. July 11 -15, 2022</date>
			<biblScope unit="page" from="1358" to="1368" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
