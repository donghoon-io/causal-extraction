<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Length-Aware Motion Synthesis via Latent Diffusion</title>
				<funder ref="#_9MfR7qK">
					<orgName type="full">Sapienza</orgName>
				</funder>
				<funder ref="#_MXDvdxv">
					<orgName type="full">ItalAI S</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2024-07-16">16 Jul 2024</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Alessio</forename><surname>Sampieri</surname></persName>
							<idno type="ORCID">0000-0002-1432-7499</idno>
							<affiliation key="aff0">
								<orgName type="institution">Sapienza University of Rome</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alessio</forename><surname>Palma</surname></persName>
							<idno type="ORCID">0009-0008-4332-9179</idno>
							<affiliation key="aff0">
								<orgName type="institution">Sapienza University of Rome</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Indro</forename><surname>Spinelli</surname></persName>
							<idno type="ORCID">0000-0003-1963-3548</idno>
							<affiliation key="aff0">
								<orgName type="institution">Sapienza University of Rome</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Fabio</forename><surname>Galasso</surname></persName>
							<idno type="ORCID">0000-0003-1875-7813</idno>
							<affiliation key="aff0">
								<orgName type="institution">Sapienza University of Rome</orgName>
								<address>
									<country key="IT">Italy</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Length-Aware Motion Synthesis via Latent Diffusion</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2024-07-16">16 Jul 2024</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2407.11532v1[cs.CV]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-28T22:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Text-to-motion</term>
					<term>Length-aware video generation</term>
					<term>Human body kinematics</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The target duration of a synthesized human motion is a critical attribute that requires modeling control over the motion dynamics and style. Speeding up an action performance is not merely fast-forwarding it. However, state-of-the-art techniques for human behavior synthesis have limited control over the target sequence length. We introduce the problem of generating length-aware 3D human motion sequences from textual descriptors, and we propose a novel model to synthesize motions of variable target lengths, which we dub "Length-Aware Latent Diffusion" (LADiff ). LADiff consists of two new modules: 1) a length-aware variational auto-encoder to learn motion representations with length-dependent latent codes; 2) a length-conforming latent diffusion model to generate motions with a richness of details that increases with the required target sequence length. LADiff significantly improves over the state-of-the-art across most of the existing motion synthesis metrics on the two established benchmarks of HumanML3D and KIT-ML.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Synthesizing human behavior is essential for creating immersive virtual worlds, whether designed for our leisure or to train robots to interact with real people safely and on a large scale <ref type="bibr" target="#b19">[20]</ref>. SOTA techniques for generating human behavior condition the generation on textual inputs. However, they have limited control over influential attributes, such as the desired sequence length. For example, suppose we want to generate a short kick motion. It is not enough to sub-sample a lengthy kick sequence. Instead, we should consider shorter duration and faster dynamics while respecting the laws of physics.</p><p>Leading approaches in 3D human motion generation cannot control the synthesized motion's length or consider the target sequence's variable length as a stylizing attribute. GPT-based approaches <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b28">29]</ref> are an example of the former, which predict auto-regressively. They can hardly constrain the output length, even when conditioned on the target length. Other leading techniques are based on diffusion models (DDPM) <ref type="bibr" target="#b7">[8]</ref> and latent representations <ref type="bibr" target="#b22">[23]</ref>. One such approach is MLD <ref type="bibr" target="#b2">[3]</ref>, where both the Variational Autoencoder (VAE) <ref type="bibr" target="#b10">[11]</ref> representation is agnostic of the desired size of the output sequence and the diffusion-based synthesis does not account for mechanisms which affect the output sequence style, depending on the desired length. MotionDiffuse <ref type="bibr" target="#b29">[30]</ref> is the sole to add a length control over sampling the noise distribution in the denoising stage of the  <ref type="figure">1:</ref> A pictorial illustration of the proposed LADiff generative model. The green latent space, learned via VAE, is subdivided into subspaces that activate progressively for longer target human motion sequences, i.e., the shortest sequence latent vectors lie on the 1D line, those for longer sequences lie on planes, cubes, or higher dimensions. Correspondingly, during the sequence generation via a latent DDPM, longer sequences learn attention patterns made up of more subspaces, i.e., more columns in the rectangles of latent-to-frame attention vectors. See Secs. 1 and 5 for more details. As a result shorter sequences depict faster actions, adopting the styles of motion that take the fewest frames. Longer sequences accommodate more frames with the longer version of the actions in terms of dynamics and style. We provide videos associated with the images in the paper in the additional materials. diffusion process. However, it only adds ad-hoc mechanisms for ensuring smoothness across actions of multi-activity generations (e.g., "synthesize a character which walks and sits").</p><p>Our proposal, "Length-Aware Latent Diffusion" (LADiff ), introduces novel length-aware models both for the latent representation and for the diffusion-based synthesis to exploit the length of the target sequence as a given input. We argue that the VAE latent embedding space should encode longer sequences with larger capacity because the longer motions require more details to generate. Therefore, in LADiff, we organize the latent representation space into subspaces, which activate progressively with the increasing target sequence length. As depicted in Fig. <ref type="figure">1</ref>, shortest sequences only lie in one subspace (on the 1D green line). Instead, longer sequences have higher-dimensional latents, which activate more subspaces (the green plane, then the green cubic latent space).</p><p>To cope with latent vectors of different sizes, we introduce a novel latent-length-adaptive diffusion model, which we use in LADiff to synthesize the motions. At training time, the diffusion model attends to a latent vector of varying dimensionality. In Fig. <ref type="figure">1</ref>, next to each generated sequence, the yellow-to-blue columns represent the attention of each latent to each frame. We use masking (blanked columns) for the unattended latents. During inference, longer sequences activate more subspaces, thus higher-dimensional latent each specializing in a chunk of frames of the extended sequence. See also the discussion in Sec. <ref type="bibr" target="#b4">5</ref>.</p><p>LADiff is the first to bridge the noise gap between VAEs, used for the latent representation learning, and DDPMs, adopted in the generation phase. VAEs learn to encode and decode clean motions in the first stage of representation learning. By contrast, DDPMs generate latent vectors via denoising, and their output may carry residual noise and a certain degree of stochasticity, which does not match the clean latent expected by the VAEs' decoder. To cope with the intrinsic DDPMs residual noise, we propose denoising VAEs <ref type="bibr" target="#b8">[9]</ref> to learn from perturbed input signals, enhancing the model's robustness against latent vector perturbations and boosting the final decoding performances.</p><p>We test LADiff on two prominent and widely used datasets, HumanML3D <ref type="bibr" target="#b5">[6]</ref> and KIT-ML <ref type="bibr" target="#b18">[19]</ref>, where it surpasses the performance of current leading techniques across multiple metrics. Specifically, the model leverages a smaller subspace to generate shorter motion sequences, and despite this limitation, it achieves better or comparable scores compared to the SOTA. This success is attributed to utilizing all training sequences to learn the latent features within that subspace. On the contrary, for longer sequences, the model learns to correspond contiguous frames to specialized subspaces to synthesize longer sequences. Thanks to the additional capacity and this specialization, LADiff generates convincing and realistic sequences. See the in-depth analysis of Sec. 5 and Fig. <ref type="figure" target="#fig_6">6b</ref>.</p><p>We summarize our key contributions as follows:</p><p>-Introduction of a VAEs that generates a length-aware latent space; -Introduction of a length-aware DDPM and increased synergy between representation and synthesis with the introduction of DVAE for motion; -An in-depth analysis of length-aware latents and related motion dynamics, and a comprehensive evaluation of LADiff on the HumanML3D and KIT-ML datasets, where it sets a new SOTA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>In recent years, text-conditioned human motion synthesis has emerged as a prominent area of research. Learning a latent representation and the conditional generation are building blocks of this task, which we discuss as related work here. Latent representation learning. Sequence representation is a challenging and widespread task for representing human motion <ref type="bibr" target="#b1">[2]</ref>. This encoding can occur in either a discrete or continuous space. Regarding discrete representation, T2M-GPT <ref type="bibr" target="#b28">[29]</ref> employs Vector Quantized Variational Autoencoders (VQ-VAE) <ref type="bibr" target="#b15">[16]</ref> to build a motion "vocabulary" with codebooks for motion generation. Mo-tionGPT <ref type="bibr" target="#b9">[10]</ref> exploits VQ-VAE, combining text and motion vocabularies, jointly learning language and motion before generating sequences. Similarly, AttT2M <ref type="bibr" target="#b31">[32]</ref> utilizes word-level and sentencelevel features to learn the cross-modal relationship between text and motion. M2DM <ref type="bibr" target="#b11">[12]</ref> applies an orthogonal regularization that encourages diversity and increased usage of codebook entries to improve performance. Considering continuous latent representation, Variational Autoencoders (VAE) <ref type="bibr" target="#b10">[11]</ref> are the SOTA due to their ease of training <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b16">17]</ref>. TEMOS <ref type="bibr" target="#b17">[18]</ref> introduces a Transformer-based VAE model <ref type="bibr" target="#b25">[26]</ref> that leverages a sequence-level latent vector to generate text-conditioned motions. MLD <ref type="bibr" target="#b2">[3]</ref> employs a VAE to learn latent representations of motion sequences. HumanML3D <ref type="bibr" target="#b5">[6]</ref> uses a temporal VAE to enhance the quality of fixed-size latent representations and incorporates an MLP for predicting sequence length from textual descriptions.</p><p>Our approach builds upon the success of VAE for its representation performance and ease of training. However, we organize its latent space in different subspaces to encode sequences of different lengths. No prior work, neither continuous nor discrete, designs latent representations that are aware of target lengths.</p><p>Generation. In this fast-evolving field, first approaches such as TEMOS <ref type="bibr" target="#b17">[18]</ref> directly used VAEs to generate motion based on textual conditioning, where the number of positional encodings established during generation determines the duration of these motions. There are two leading generation strategies: Generative Pre-trained Transformers (GPT) <ref type="bibr" target="#b20">[21]</ref>, and Denoising Diffusion Probabilistic Models (DDPM) <ref type="bibr" target="#b7">[8]</ref>. The first, represented by approaches such as MotionGPT <ref type="bibr" target="#b28">[29]</ref> and T2M-GPT <ref type="bibr" target="#b9">[10]</ref> leverages GPT architectures for autoregressive generation, where the occurrence of a specific token marks the generation's end. MDM <ref type="bibr" target="#b24">[25]</ref> is the archetype of the latter family of models, where that apply DDPM directly to raw motion data to capture the motion-text relationship for a determined number of tokens. MotionDiffuse <ref type="bibr" target="#b29">[30]</ref> adds smoothing to body parts and frames during noise sampling, which can be resource-intensive. Leveraging latent DDPM <ref type="bibr" target="#b22">[23]</ref>, MLD <ref type="bibr" target="#b2">[3]</ref> achieves SOTA performances. It presents a two-step approach wherein they initially use VAEs to learn a compact and low-dimensional motion latent space, followed by a diffusion process in this space. M2DM <ref type="bibr" target="#b11">[12]</ref> mixes GPT and DDPM approaches by using the latter to synthesize motions starting from a discrete latent space built using a token priority mechanism and a transformerbased decoder. We build upon the success of MLD <ref type="bibr" target="#b2">[3]</ref>. Still, we organize the latent space into subspaces dedicated to motions of different lengths, adapting the conditional DDPM generation to the variability of the target length to have a time-dependent style and dynamics. Retrieveal Augmented Generation <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b30">31]</ref> has shown significant performance improvements when conditioning motion generation on a sample retrieved from the entire training set. However, this approach requires storing and searching through a large database. We demonstrate how the lengthaware generation of LADiff can also be conditioned on the same retrieved sample.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>In this Section, we introduce the length-aware VAE for learning length-aware latent representations and latent diffusion process. Also, here we detail the proposed Denoising VAE to add robustness during synthesis. Next, we describe background concepts on the latent space, generation, and conditioning. Problem formalization. The objective of motion synthesis is to generate a motion X based on textual input w. We define motion as a sequence of poses X = [X 1 , .., X F ] ∈ R F ×V , where F are the number of frames and V the parameters of the pose vector. Following literature <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b28">29]</ref>, our pose vector V contains pose, velocity and rotation. We also define the textual description as a vector w ∈ R 1×D . In this work, we also provide as input the target sequence length f * ∈ R.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Encoder</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Background</head><p>Our approach is based on the current best practice in motion synthesis, where motions are generated using DDPM <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b29">30]</ref>, often with VAEs trained for reconstruction tasks. In this context, textual inputs condition the synthesis. First, the model reconstructs the input motion sequence, crafting a meaningful latent space. Then, the model generates the output motion guided by textual input.</p><p>Latent representation learning. VAE approaches consist of an encoder-decoder generative architecture learned to minimize a reconstruction error. In this framework, latent representations Z are the lower-dimensional embeddings produced as output by the encoder network parametrized by ϕ from the input data X. Following VAE literature <ref type="bibr" target="#b10">[11]</ref>, q ϕ (z|x) approximates the true posterior distribution of the latent space with multivariate Gaussian with a diagonal covariance structure:</p><formula xml:id="formula_0">q ϕ (z|x) = N (z|µ ϕ (x), σ 2 ϕ (x)I),<label>(1)</label></formula><p>where µ ϕ (•) and σ 2 ϕ (•), are outputs of the encoder. We sample from the approximate posterior z (i) ∼ q ϕ (z|x (i) ) using:</p><formula xml:id="formula_1">z (i) = µ (i) + σ (i)2 ⊙ ρ,<label>(2)</label></formula><p>where z (i) ∈ R 1×D is the latent vector, ρ ∼ N (0,I) is additional noise, ⊙ represent pairwise multiplication and x (i) is a sample from the dataset X. The decoder p θ (x|z) is a network parametrized by θ that maps the sampled values back to the input space. The parameters of the networks are obtained by optimizing the ELBO objective as described in <ref type="bibr" target="#b10">[11]</ref>.</p><p>Generation. Latent DDPM have a different approximated posterior g(z t |z t-1 ), denoted diffusion process, which gradually gradually converts latent representations z 0 = z into random noise z T in T timesteps:</p><formula xml:id="formula_2">g(z t |z t-1 ) = N (z t ; √ ᾱt z t-1 , (1 -ᾱt )I) ,<label>(3)</label></formula><p>where ᾱt is a scaling factor specific to timestep t. Then, the reverse process dubbed denoising gradually refines the noised vector to a suitable latent representation z 0 . Following <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24]</ref>, we use the notation {z t } T t=0 to denote the sequence of noised latent vectors, with z t-1 = ϵ ψ (z t , t) representing the denoising operation at time step t with ϵ ψ being a denoising autoencoder trained to predict the denoised variant of its input.</p><p>Conditioning. Following literature <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b4">5]</ref>, we incorporate textual input for conditional synthesis, enhancing the overall control and expressiveness of the generated motion sequences. The objective is to align textual and motion representations:</p><formula xml:id="formula_3">L = E ϵ∼N (0,I),t,w [∥ϵ -ϵ ψ (z t ,t,γ(w))∥ 2 2 ] ,<label>(4)</label></formula><p>where we add the textual input encoded via CLIP-ViT-L-14 <ref type="bibr" target="#b21">[22]</ref> γ(w).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Length-Aware VAE</head><p>We propose a novel Length-Aware VAE which extends the VAE framework by organizing the latent space to characterize the motion's length constraint. We decompose the entire latent space into K subspaces. Therefore, the complete latent space has dimension R K×D , and the smallest dimensional subspace is 1 × D-dimensional. On the latter live the latent vectors associated with the shortest sequences. As the motion length grows, we stepwise unlock bigger subspaces following the activation rate k = ⌈ f r ⌉, where k is the number of activated subspaces, f represents the number of frames, and r is the number of frames assigned to each subspace. This results in latent vector dimensionality that grows with the sequence length f , exploiting k subspaces of the latent space, each encoding exactly r frames. In Fig. <ref type="figure">1</ref>, we visualize the shortest motions on a 1-dimensional subspace represented by a line. The 2-dimensional plane represents middle-sized sequences and comprises the previous subspace. Finally, the most extended sequences exploit the entire 3-dimensional space enclosing the previous subspaces. We train the encoder-decoder Length-Aware VAE by passing the motion sequence X through the encoder, obtaining the desired k means (µ) and variances (σ 2 ):</p><formula xml:id="formula_4">q ϕ (z|x) = N (z|µ 1 (x), σ 2 1 (x)I, . . . , µ k (x), σ 2 k (x)I)<label>(5)</label></formula><p>Then we sample k values according to the activated subspaces to build our latent representation z ∈ R k×D though concatenation:</p><formula xml:id="formula_5">z (i) = [µ (i) 1 + σ (i)2 1 ⊙ ρ 1 , . . . , µ (i) k + σ (i)2 k ⊙ ρ k ] .<label>(6)</label></formula><p>Finally, we use the decoder to reconstruct the input sequence X. The transformer-based decoder handles the varying dimensional space by using the attention mechanism. The training of the encoder and decoder represents the first stage of our training pipeline. From these procedures arises a structured latent space with subspaces specialized on sequences of different lengths. The first subspace, dedicated to the shortest sequences, has the smallest dimensional space. However, it is active and trained for every sequence in the dataset. The last subspaces are engaged only for longer sequences with two main consequences: first, longer sequences have a higher capacity space, and second, they specialize over a subset of the entire data distribution containing longer sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Length-Aware Latent Diffusion</head><p>We present the core concept of our Length-Aware Latent Diffusion process: the dynamic adaptation of latent dimensions for generating motion sequences. Specifically, each latent z t resides in R k×D , with k ∈ {1, . . . ,K}. During sampling (or at inference time), we initialize z T using our activation rate k and the desired motion length f * :</p><formula xml:id="formula_6">z T = N (0, I) ∈ R ⌈ f * r ⌉×D<label>(7)</label></formula><p>The VAE's decoder receives the denoised vector z 0 ∈ R k×D together with the desired motion length f * for the final synthesis. The transformer decoder uses attention to account for the generated latents of variable length. As a byproduct of our adaptive latent vectors, the inference time is reduced, since the decoder uses fewer terms in the attention mechanisms.The length-aware latent diffusion denoiser is trained in a second stage, during which the encoder and decoder are kept frozen.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Denoising VAE</head><p>We train VAE and latent-DDPM in cascaded stages. However, they assume distinct characteristics of the latent vectors. We train VAEs to reconstruct clean inputs as faithfully as possible. Conversely, the latent DDPMs generate latent vectors from pure Gaussian noise. These generated latent vectors may contain residual noise, which poses challenges for the VAE decoder, trained to reconstruct clean inputs and then frozen.</p><p>To establish a more coherent connection between these two phases, we propose employ Denoising VAE (DVAE) <ref type="bibr" target="#b8">[9]</ref> to perturb features of the input sequence to create a robust latent space. We randomly sample a percentage of frames from the input sequence and perturb them with controlled Gaussian noise ϵ ∼ N (0,1). Therefore, we introduce an augmentation strategy to align the latent variable distribution from the two phases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Implementation Details</head><p>The framework comprises three major blocks: the encoder E, decoder D and denoiser ϵ θ . While E and D constitute the reconstruction phase (24M parameters), the denoiser (9M parameters) together with the frozen decoder D form the generation phase. Each block consists of 9 layers and 4 heads, with an embedding dimension of D = 256. Both the encoder and decoder are constructed using standard transformer layers. The denoiser ϵ θ incorporates classical self-attention over the latent vector and linear cross-attention between latent vector and textual input, adopting also the Stylization block from <ref type="bibr" target="#b29">[30]</ref>. We use a batch size of 64 for the first phase and 128 for the second. The learning rate is set to 10 -4 , and the optimizer used is AdamW <ref type="bibr" target="#b14">[15]</ref>. During training, we performed 1000 diffusion steps. At inference, we use only 20 diffusion steps. We used 8 Tesla V100 GPUs, conducting 3k epochs for both phases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>We evaluate the reconstruction and generation capabilities of LADiff quantitatively and qualitatively. The analysis comprehends an ablation study of the proposed model components.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and Metrics</head><p>For this task, we evaluate our model on two large and established datasets, namely HumanML3D and KIT-ML. Both datasets utilize the SMPL <ref type="bibr" target="#b13">[14]</ref> representation with 22 and 21 joints respectively. HumanML3D <ref type="bibr" target="#b5">[6]</ref> with 14.6k motion sequences, each associated with three textual descriptions, provides an extensive range of movements, from martial arts to everyday actions. KIT-ML <ref type="bibr" target="#b18">[19]</ref> instead consists of 3.9k motions with a total of 6.2k textual descriptions. Following <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30]</ref>, we use motions of the same lengths as HumanML3D. Metrics. We evaluate the generated motion sequences employing all commonly adopted metrics. R-precision and MM-Dist. In the context of the text-to-motion task, <ref type="bibr" target="#b5">[6]</ref> offers motion and text feature extractors designed to generate geometrically consistent features for paired text-motion combinations and vice versa. In this feature space, we assess motion-retrieval precision (R-precision) by mixing the generated motion with 31 mismatched motions. We then calculate the text-motion top-1/2/3 matching accuracy. Additionally, we measure Multi-modal Distance (MM-Dist), which </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R Precision ↑ Methods</head><p>Top 1 Top 2 Top 3 FID ↓ MM-Dist ↓ Diversity → MModality ↑ Real 0.511 ±.003 0.703 ±.003 0.797 ±.002 0.002 ±.000 2.974 ±.008 9.503 ±.065 -MotionDiffuse <ref type="bibr" target="#b29">[30]</ref> 0.491 ±.001 0.681 ±.001 0.782 ±.001 0.630 ±.001 3.113 ±.001 9.410 ±.049 1.553 ±.042 MDM <ref type="bibr" target="#b24">[25]</ref> 0.320 ±.005 0.498 ±.004 0.611 ±.007 0.544 ±.044 5.566 ±.027 9.559 ±.086 2.799 ±.072 MLD <ref type="bibr" target="#b2">[3]</ref> 0.481 ±.003 0.673 ±.003 0.772 ±.002 0.473 ±.013 3.196 ±.010 9.724 ±.082 2.413 ±.079 T2M-GPT <ref type="bibr" target="#b28">[29]</ref> 0.491 ±.003 0.680 ±.003 0.775 ±.002 0.116 ±.004 3.118 ±.011 9.761 ±.081 1.856 ±.011 MotionGPT <ref type="bibr" target="#b9">[10]</ref> 0.492 ±.003 0.681 ±.003 0.778 ±.002 0.232 ±.008 3.096 ±.008 9.528 ±.071 2.008 ±.084 Fg-T2M <ref type="bibr" target="#b27">[28]</ref> 0.492 ±.002 0.683 ±.003 0.783 ±.002 0.243 ±.019 3.109 ±.007 9.278 ±.072 1.614 ±.049 M2DM <ref type="bibr" target="#b11">[12]</ref> 0.497 ±.003 0.682 ±.002 0.763 ±.003 0.352 ±.005 3.134 ±.010 9.926 ±.073 3.587 ±.072 AttT2M <ref type="bibr" target="#b31">[32]</ref> 0.499 ±.006 0.690 ±.002 0.786 ±.002 0.112 ±.006 3.038 ±.007 9.700 ±.090 2.452 ±.051 Ours (r =32) 0.493 ±.002 0.686 ±.002 0.784 ±.001 0.110 ±.004 3.077 ±.010 9.622 ±.071 2.095 ±.076 Ours (r =48) 0.503 ±.002 0.696 ±.003 0.792 ±.002 0.182 ±.004 3.054 ±.008 9.795 ±.076 2.115 ±.063 ReMoDiffuse <ref type="bibr" target="#b30">[31]</ref> 0.510 ±.005 0.698 ±.006 0.795 ±.004 0.103 ±.004 2.974 ±.016 9.081 ±.075 1.795 ±.028 Ours (r =48) RAG 0.494 ±.002 0.691 ±.003 0.786 ±.001 0.054 ±.002 3.112 ±.008 9.517 ±.077 2.453 ±.074   quantifies the distance between the generated motions and the associated text. The L2-loss is employed to determine both R-Precision and MM-Dist. FID. We assess Motion Quality using the Frechet Inception Distance (FID) metric, which measures the similarity between the distributions of generated and real motions. It is computed by measuring the L2-loss of the latent representations obtained through the feature extractor. Diversity and MModality. We utilize Diversity and MultiModality (MModality) metrics to gauge the range of motion variations across the entire dataset and the diversity of generated motions for each specific text input. To evaluate diversity, we randomly divide the data into {x 1 , . . . , x X d } and {x ′ 1 , . . . , x ′ X d }, two equal-sized subsets each containing motion feature vectors. To assess MultiModality, we randomly select a set of text descriptions from the entire collection, with a total of T d descriptions. Following this, we randomly sample two equal-sized subsets, X d , from all the motions generated by the t-th text descriptions. These subsets contain motion feature vectors: {x t,1 , . . . , x t,X d } and {x ′ t,1 , . . . , x ′ t,X d }, respectively. Diversity and MModality are then defined as the average L2-loss between corresponding samples. Baselines. We compare our proposed LADiff with the current state-of-the-art techniques for motion synthesis. MotionDiffuse <ref type="bibr" target="#b29">[30]</ref> and MDM <ref type="bibr" target="#b24">[25]</ref> are DDPM-based models that approach the task by directly processing raw poses. MLD <ref type="bibr" target="#b2">[3]</ref> is a two-stage process that uses VAE for encoding the input sequence and a latent diffusion model for conditional generation. T2M-GPT <ref type="bibr" target="#b28">[29]</ref> and Mo-tionGPT <ref type="bibr" target="#b9">[10]</ref> utilize GPT as the generation mechanism, leveraging VQ-VAE for motion encoding into a codebook. Fg-T2M <ref type="bibr" target="#b27">[28]</ref> combines linguistics-structure and context-aware progressive reasoning through graph attention network <ref type="bibr" target="#b26">[27]</ref>. M2DM <ref type="bibr" target="#b11">[12]</ref> encodes the sequence using a codebook and implements a generation process through a discrete diffusion model. AttT2M <ref type="bibr" target="#b31">[32]</ref> exploits a TCN that decodes the features learned through local and global attention between the sequence and text. ReMoDiffuse <ref type="bibr" target="#b30">[31]</ref> generates human motion from text using RAG <ref type="bibr" target="#b12">[13]</ref>, it retrieves relevant motion samples from a training set (assumed to be available at inference), and then uses a transformer to fuse them with the text input and generate motion sequences. We denote with "Real" the metrics obtained with the ground-truth motion. MDM <ref type="bibr" target="#b24">[25]</ref> 0.164 ±.004 0.291 ±.004 0.396 ±.004 0.497 ±.021 9.191 ±.022 10.85 ±.109 1.907 ±.214 MLD <ref type="bibr" target="#b2">[3]</ref> 0.390 ±.008 0.609 ±.008 0.734 ±.007 0.404 ±.027 3.204 ±.027 10.80 ±.117 2.192 ±.071 T2M-GPT <ref type="bibr" target="#b28">[29]</ref> 0.416 ±.006 0.627 ±.006 0.745 ±.006 0.514 ±.029 3.007 ±.023 10.92 ±.108 1.570 ±.039 MotionGPT <ref type="bibr" target="#b9">[10]</ref> 0.366 ±.005 0.558 ±.004 0.680 ±.005 0.510 ±.016 3.527 ±.021 10.35 ±.084 2.328 ±.117 Fg-T2M <ref type="bibr" target="#b27">[28]</ref> 0.418 ±.005 0.626 ±.006 0.745 ±.004 0.571 ±.047 3.114 ±.015 10.93 ±.083 1.019 ±.029 M2DM <ref type="bibr" target="#b11">[12]</ref> 0.416 ±.004 0.628 ±.004 0.743 ±.004 0.515 ±.029 3.015 ±.017 11.41 ±.097 3.325 ±.037 AttT2M <ref type="bibr" target="#b31">[32]</ref> 0.413 ±.006 0.632 ±.006 0.751 ±.006 0.870 ±.039 3.039 ±.021 10.96 ±.123 2.281 ±.047 Ours (r =48) 0.429 ±.007 0.647 ±.004 0.773 ±.004 0.470 ±.016 2.831 ±.020 11.30 ±.108 1.243 ±.057 ReMoDiffuse <ref type="bibr" target="#b30">[31]</ref> 0.427 ±.014 0.641 ±.004 0.765 ±.055 0.155 ±.006 2.814 ±.0.12 10.80 ±.105 1.239 ±.028 Ours (r =48) RAG 0.415 ±.006 0.632 ±.007 0.758 ±.005 0.386 ±.003 2.978 ±.020 11.20 ±.008 1.732 ±.066</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Quantitative Results</head><p>We validate our model on the HumanML3D and KIT-ML datasets. Following <ref type="bibr" target="#b5">[6]</ref>, we calculate the results based on 20 generations and report the average with a 95% confidence interval.</p><p>Generation. Table <ref type="table" target="#tab_0">1</ref> shows the results obtained on the HumanML3D dataset. Our model outperforms the current state-of-the-art techniques in all R-Precision scores and FID while maintaining a comparable Matching Score. In the bottom part of the table, we report the SOTA retrieval augmented <ref type="bibr" target="#b30">[31]</ref>, compare to LADiff, augmented with retrieval. Both techniques additionally assume access to the entire training set. LADiff enhances the FID by 47%, exhibiting superior Diversity and MModality.</p><p>The performance exceeds the top models even on the KIT-ML dataset (Table <ref type="table" target="#tab_1">2</ref>). LADiff outperform the current best techniques of 4.2% in Matching Score and 2.2% in R-Precision Top3. In the bottom part of the table, our method outperform <ref type="bibr" target="#b30">[31]</ref> in Diversity and MModality.</p><p>LADiff demonstrates consistent performance across different datasets in several metrics, including R precision, Matching score, and FID. These results and the R-precision, more specifically, demonstrate that LADiff can generate novel test motions from text that are most similar to the real ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Qualitative Results</head><p>First we show qualitatively the results obtained from LADiff and then we demonstrate how our model adapts to different lengths. Comparisons. From Fig. <ref type="figure" target="#fig_2">3</ref> we note that LADiff is the only framework to consider the desired length and to adapt to it, in terms of style and dynamics. GPT-based approaches have no control over the length of the output sequence. MLD can generate sequences of a desired length but lacks adaptation to the target length. See, for example, the second row, where it seems that MLD mainly fits the target length by subsampling without adapting style and dynamics. Our model is capable of rich and realistic behavior, respecting the textual conditioning and the target length (e.g., a perfect circle, realistic balance regain, correct shelf height). In the second row we see that our "man" runs to complete the circle when the number of frames is insufficient and walks when feasible. Variable target lengths. In Fig. <ref type="figure" target="#fig_3">4</ref>, we challenge our model to generate motions for the same textual input but with varying target lengths. At the top, the figure shows the attention maps of the decoder transformer, attending from 1 latent, in the case of querying a 48-frame motion, to 5 latents, for generating motions of 200 frames. Note how, starting from the case of 2 latents, each latent takes responsibility for different contiguous frames in the generated motion. Qualitatively, querying longer target lengths allows LADiff to arrange for richer and more careful generated motions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study</head><p>Table <ref type="table" target="#tab_2">3</ref> displays ablation studies for the main components of our model, including the number of frames represented by each latent subspace r, which determines the activation rate k, the use of DVAE (% Noised), and the adoption of the Length-Aware framework (LA). Activation rate. The rate influences the maximum number K of available latent subspaces. Values of 48 for r and 5 for K yield the best results and strike a good balance between precision and diversity. Amount of noise in the DVAE. Noise is a trade-off between metrics evaluating the movement's precision and the generation process's diversity. The perturbation in DVAE acts on the input sequence, but we explored its application directly on the latent space. The latter approach has a higher impact on the output since the "filtering" was entirely charged to the decoder. Length-awareness Finally, we showcase the positive impact of our length-aware framework on the results. By comparing it with a model that encodes motion using a fixed dimensionality K × D (Ours w/o LA), we observe an improvement of 1.6% in R-Precision Top 3 and 2.6% in Matching Score. In other words, constraining the model to use a variable latent vector dimensionality for varying target lengths improves most of its capability to generate motions closest to the real ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">In-Depth Analysis</head><p>This section investigates LADiff 's latent space, showcase the activation of length-dependent latent subspaces, and assess the efficiency of the proposed technique. Latent Representation. Fig. <ref type="figure" target="#fig_6">6a</ref> illustrates the 3D t-SNE reduced dimensionality manifold of the latent space. Each dimension corresponds to a subspace. We generate ten sequences of 30, 96, and 144 frames per action. To estimate the X coordinate, we take the elements of z associated with the first subspace from each motion and use t-SNE to extract one dimension. Then, we extract from z the elements associated with the first two subspaces and pad with zeros those without the second to compute with t-SNE the Y component. We use a similar approach to extract the Z component. Short actions generated using a single latent organize themself along a straight line (X-axis). "Medium" actions unlock the second latent/dimension with Y ̸ = 0, allowing a displacement of the points along the Y-axis. Finally, the longest motions are free to position in the three-dimensional space, with their orthogonal projection residing in the previous subspace but being free to roam along the third dimension. LADiff organizes actions in the latent space by separating the action types along the first subspace and then by clustering the lengths on the available subspaces.</p><p>Subspace specialization. Fig. <ref type="figure" target="#fig_4">5</ref> illustrates what happens when we activate only a subset of the subspaces to generate a motion that is long and that requires the entire latent space. From left to right, we activate the subspaces one by one. Each latent vector is accountable for almost precisely r contiguous frames. We observe a gradual transition between these blocks, possibly to connect different actions and movements and guarantee a smooth generation. When only the first or two latents are used (left-most two examples), the generated motion is focused on the action of sitting in the second row. The third latent subspace introduces the walking part; the motion is complete but has substantial jitter and unnatural movements. The fourth and fifth latent subspaces fill the gap between the initial and final movements, generating smooth transition and realistic behavior.</p><p>The two text conditionings A person walks slowly forward, picks something up, places it down, depicted in the third row, and The person waits and then starts jumping forward in the fourth row corroborate our analysis as we observe the first two subspaces attending to the final part of the action (e.g., "picks something up, places it down" or "jumping forward"). The third contributes to generating the initial frames. The fourth and fifth fill the gaps, harmonizing these two parts. The different responsibility of subspaces for different lengths ensures that longer sequences add new styles and dynamics.</p><p>Performance and Inference time across lengths. In Figure <ref type="figure" target="#fig_6">6b</ref>, we illustrate our model's influence on inference time and R-Precision Top 3 across different generated motion lengths (5-bins) of the HumanML3D dataset. We compare against the models T2M-GPT <ref type="bibr" target="#b28">[29]</ref> and MLD <ref type="bibr" target="#b2">[3]</ref>, top performers for which the code is available. Compared to T2M-GPT, LADiff outperforms precision scores (red curves) in every generated motion length (up to 4%) except for the shortest sequences where T2M-GPT prevails by a short margin. However, LADiff is considerably faster (blue curves) in every setting. Compared to MLD, LADiff has superior precision performance (up to 10%) with comparable inference times.</p><p>Length-aware motion dynamics. We question the correspondence between the length of the generated motions and the statistics of the motion dynamics, specifically considering velocity and acceleration of the body joints. In Table <ref type="table" target="#tab_3">4</ref>, we report these statistics for all the qualitatives pre-  sented in the main paper and the supplementary material. For MLD, the averages of acceleration and velocity remain unchanged as the generated length varies. By contrast, for LADiff the statistics increase by 44.4% and 17.3%, respectively, when the motion is shorter. We confirm therefore that LADiff produces different motion styles for different lengths. In Table <ref type="table" target="#tab_4">5</ref>, we repeat the analysis on the same analysis on a controlled set of atomic actions ("A man sits", "A man walks", and "A man throws"). Each action has peculiar velocities and acceleration, but the relative changes with motions are similar.</p><p>Limitations. LADiff assumes a linear correlation between motion lengths and the number of exploited subspaces. While functional and performant, we advocate for future research on this aspect.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have presented Length-Aware Latent Diffusion (LADiff ), a novel approach for generating motion sequences guided by textual descriptions with dynamics and style conditioned by the target motion length. To do so, LADiff leverages a customized latent space with subspaces representing sequences of different lengths. Addressing length awareness may be a first step in coding control mechanisms for the generated sequence attributes.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Fig.1:A pictorial illustration of the proposed LADiff generative model. The green latent space, learned via VAE, is subdivided into subspaces that activate progressively for longer target human motion sequences, i.e., the shortest sequence latent vectors lie on the 1D line, those for longer sequences lie on planes, cubes, or higher dimensions. Correspondingly, during the sequence generation via a latent DDPM, longer sequences learn attention patterns made up of more subspaces, i.e., more columns in the rectangles of latent-to-frame attention vectors. See Secs. 1 and 5 for more details. As a result shorter sequences depict faster actions, adopting the styles of motion that take the fewest frames. Longer sequences accommodate more frames with the longer version of the actions in terms of dynamics and style. We provide videos associated with the images in the paper in the additional materials.</figDesc><graphic coords="2,429.57,111.73,93.17,86.06" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Overview of our proposed Length-Aware Latent Diffusion (LADiff ). During the reconstruction phase (orange arrows), the Encoder, aided by the Decoder, learns to represent sequences of varying lengths into a latent space composed of subspaces, which activate progressively for longer sequences. In the Generation stage (blue arrows), the Denoiser learns to create latent vectors aligned to the textual input, which map to correct subspaces specified by input sequence length. The actual motion results from decoding the latent vectors of the Denoiser. For this purpose, the Decoder is made resilient to noise in the Reconstruction stage by learning to reconstruct sequences affected by noise</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>"Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Qualitative comparison of text-based human motion generations. multiple target lengths are provided to techniques that allow to set the length input. See Section 4.3 for discussion.</figDesc><graphic coords="10,317.91,271.98,57.05,62.97" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>"Fig. 4 :</head><label>4</label><figDesc>Fig. 4: (Rows) Generation of motions with the same textual input and varying queried target lengths, alongside (Top) the corresponding decoder transformer attention maps where y-axis represents the target motion length. Darker colors indicate higher attention scores for each subspace of the latent vector, represented in chunks along the x-axis. See Sec 4.4 for the discussion.</figDesc><graphic coords="11,260.79,362.44,67.42,64.12" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>Fig.5: The first row shows the decoder's attention map on the length-aware, denoised latent vectors. Then we depict the generated motion obtained using only the activated latent subspaces selected in red. See Sec 5 for the detailed description.</figDesc><graphic coords="13,414.01,313.14,58.23,55.32" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>( a )</head><label>a</label><figDesc>Length-aware latent space. (b) Time/performances trade-off.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: Length aware in-depth analysis. Zoom in for details.</figDesc><graphic coords="14,315.79,96.24,222.76,147.72" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>HumanML3D results for motion synthesis (above) and RAG (below).</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>KIT-ML results for motion synthesis (above) and RAG (below). Real 0.424 ±.005 0.649 ±.006 0.779 ±.006 0.031 ±.004 2.788 ±.012 11.08 ±.097 -MotionDiffuse<ref type="bibr" target="#b29">[30]</ref> 0.417 ±.004 0.621 ±.004 0.739 ±.004 1.954 ±.062 2.958 ±.005 11.10 ±.143 0.730 ±.013</figDesc><table><row><cell>Methods</cell><cell>Top 1</cell><cell>R Precision ↑ Top 2</cell><cell>Top 3</cell><cell>FID ↓ MM-Dist ↓ Diversity → MModality ↑</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 3 :</head><label>3</label><figDesc>Ablation study examining the number of frames per latent r, the incorporation of DVAE with a specific noise percentage (% Noised) both at the input and latent level and the implementation of the Length-Aware framework (LA).</figDesc><table><row><cell>Methods</cell><cell cols="5">LA r Noise (%) R Prec. Top 3 ↑ FID ↓ MM-Dist ↓ Diversity →</cell></row><row><cell>Real</cell><cell cols="2">--</cell><cell>-</cell><cell>0.797 ±.002</cell><cell>0.002 ±.000 2.974 ±.008 9.503 ±.065</cell></row><row><cell>SOTA</cell><cell cols="2">--</cell><cell>-</cell><cell>0.786 ±.002</cell><cell>0.112 ±.006 3.038 ±.007 9.528 ±.071</cell></row><row><cell></cell><cell cols="2">✓ 16</cell><cell></cell><cell>0.778 ±.001</cell><cell>0.250 ±.007 3.129 ±.008 9.620 ±.083</cell></row><row><cell></cell><cell cols="2">✓ 32</cell><cell></cell><cell>0.784 ±.001</cell><cell>0.110 ±.004 3.077 ±.010 9.622 ±.071</cell></row><row><cell>Ours</cell><cell cols="2">✓ 48</cell><cell>33</cell><cell>0.792 ±.002</cell><cell>0.182 ±.004 3.054 ±.008 9.795 ±.076</cell></row><row><cell></cell><cell cols="2">✓ 64</cell><cell></cell><cell>0.780 ±.002</cell><cell>0.238 ±.007 3.114 ±.009 9.760 ±.093</cell></row><row><cell></cell><cell cols="2">✓ all</cell><cell></cell><cell>0.741 ±.001</cell><cell>0.445 ±0.10 3.375 ±.009 9.556 ±.080</cell></row><row><cell></cell><cell>✓</cell><cell></cell><cell>0</cell><cell>0.788 ±.002</cell><cell>0.222 ±.005 3.062 ±.009 9.739 ±.095</cell></row><row><cell>Ours</cell><cell>✓</cell><cell>48</cell><cell>33</cell><cell>0.792 ±.002</cell><cell>0.182 ±.004 3.054 ±.008 9.795 ±.076</cell></row><row><cell></cell><cell>✓</cell><cell></cell><cell>50</cell><cell>0.787 ±.002</cell><cell>0.196 ±.006 3.081 ±.008 9.764 ±.087</cell></row><row><cell cols="2">Ours DVAE latent ✓ Ours DVAE input ✓</cell><cell>48</cell><cell>33</cell><cell>0.735 ±.002 0.792 ±.002</cell><cell>0.207 ±.004 3.389 ±.012 9.497 ±.070 0.182 ±.004 3.054 ±.008 9.795 ±.076</cell></row><row><cell>Ours w/o LA Ours</cell><cell>✓</cell><cell>48</cell><cell>33</cell><cell>0.776 ±.001 0.792 ±.002</cell><cell>0.179 ±.005 3.137 ±.011 9.848 ±.064 0.182 ±.004 3.054 ±.008 9.795 ±.076</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 4 :</head><label>4</label><figDesc>Comparison of dynamics on input text.</figDesc><table><row><cell cols="6">Avg. Vel. (m/s) Avg. Acc. (m/s 2 ) Max Acc. (m/s 2 )</cell></row><row><cell>84</cell><cell>170</cell><cell>84</cell><cell>170</cell><cell>84</cell><cell>170</cell></row><row><cell>MLD 0.39</cell><cell>0.39</cell><cell>0.06</cell><cell>0.05</cell><cell>0.31</cell><cell>0.33</cell></row><row><cell>LADiff 0.61</cell><cell>0.52</cell><cell>0.13</cell><cell>0.09</cell><cell>0.61</cell><cell>0.55</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>Comparison of dynamics on atomic actions with LADiff. .17 0.10 0.05 0.04 0.02 0.16 0.16 0.14 Walk 1.31 1.01 0.72 0.11 0.09 0.06 0.19 0.18 0.14 Throw 0.16 0.15 0.13 0.04 0.04 0.03 0.14 0.15 0.13 Mean 0.58 0.44 0.31 0.06 0.05 0.03 0.16 0.16 0.14</figDesc><table><row><cell cols="4">Avg. Vel. (m/s) Avg. Acc. (m/s 2 ) Max Acc. (m/s 2 )</cell></row><row><cell>Actions 48 84 170 48 84</cell><cell>170</cell><cell>48 84</cell><cell>170</cell></row><row><cell>Sit 0.27 0</cell><cell></cell><cell></cell><cell></cell></row></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><p>Acknowledgements. We acknowledge financial support from <rs type="funder">ItalAI S</rs>.r.l., the <rs type="projectName">PNRR MUR</rs> project <rs type="grantNumber">PE0000013-FAIR</rs> and from the <rs type="funder">Sapienza</rs> grant <rs type="grantNumber">RG123188B3EF6A80</rs> (CENTS).</p></div>
			</div>
			<listOrg type="funding">
				<org type="funded-project" xml:id="_MXDvdxv">
					<idno type="grant-number">PE0000013-FAIR</idno>
					<orgName type="project" subtype="full">PNRR MUR</orgName>
				</org>
				<org type="funding" xml:id="_9MfR7qK">
					<idno type="grant-number">RG123188B3EF6A80</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">ediff-i: Text-to-image diffusion models with an ensemble of expert denoisers</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Vahdat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kreis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Aittala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
		</author>
		<idno>ArXiv abs/2211.01324</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep representation learning for human motion prediction and classification</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bütepage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kragic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Kjellström</surname></persName>
		</author>
		<idno type="DOI">10.1109/CVPR.2017.173</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2017.173" />
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1591" to="1599" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Executing your commands via motion diffusion in latent space</title>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
			<biblScope unit="page" from="18000" to="18010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Diffusion models beat gans on image synthesis</title>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nichol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Vector quantized diffusion model for text-to-image synthesis</title>
		<author>
			<persName><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Bao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2022-06">June 2022</date>
			<biblScope unit="page" from="10696" to="10706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Generating diverse and natural 3d human motions from text</title>
		<author>
			<persName><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2022-06">June 2022</date>
			<biblScope unit="page" from="5152" to="5161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Action2motion: Conditioned generation of 3d human motions</title>
		<author>
			<persName><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="DOI">10.1145/3394171.3413635</idno>
		<ptr target="https://doi.org/10.1145/3394171.3413635" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th ACM International Conference on Multimedia</title>
		<meeting>the 28th ACM International Conference on Multimedia<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computing Machinery</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="2021" to="2029" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Denoising diffusion probabilistic models</title>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Neural Information Processing Systems (NIPS)</title>
		<meeting>the 34th International Conference on Neural Information Processing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Denoising criterion for variational auto-encoding framework</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Im</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Memisevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence</title>
		<meeting>the Thirty-First AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<author>
			<persName><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2306.14795</idno>
		<title level="m">Motiongpt: Human motion as a foreign language</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Auto-Encoding Variational Bayes</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2nd International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">2014</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Priority-centric human motion generation in discrete latent space</title>
		<author>
			<persName><forename type="first">H</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">B</forename><surname>Mi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2023-10">October 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Retrieval-augmented generation for knowledge-intensive nlp tasks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Piktus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Petroni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Karpukhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Küttler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><forename type="middle">T</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Rocktäschel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Riedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Kiela</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Neural Information Processing Systems</title>
		<meeting>the 34th International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">SMPL: A skinned multi-person linear model</title>
		<author>
			<persName><forename type="first">M</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graphics (Proc. SIGGRAPH Asia)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="248" />
			<date type="published" when="2015-10">Oct 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Decoupled weight decay regularization</title>
		<author>
			<persName><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Neural discrete representation learning</title>
		<author>
			<persName><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Neural Information Processing Systems (NIPS</title>
		<meeting>the 31st International Conference on Neural Information Processing Systems (NIPS</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Action-conditioned 3D human motion synthesis with transformer VAE</title>
		<author>
			<persName><forename type="first">M</forename><surname>Petrovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Varol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">TEMOS: Generating diverse human motions from textual descriptions</title>
		<author>
			<persName><forename type="first">M</forename><surname>Petrovich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Varol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The kit motion-language dataset</title>
		<author>
			<persName><forename type="first">M</forename><surname>Plappert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Mandery</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Asfour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Big Data</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">X</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Undersander</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Szot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">D</forename><surname>Cote</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><forename type="middle">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Partsey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Desai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">W</forename><surname>Clegg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Hlavac</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">Y</forename><surname>Min</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><surname>Vondruš</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Gervet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">V</forename><forename type="middle">P</forename><surname>Berges</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">M</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">O</forename><surname>Maksymets</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Kalakrishnan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">S</forename><surname>Chaplot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">U</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Rai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note>Habitat 3.0: A co-habitat for humans, avatars and robots</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Improving language understanding by generative pre-training</title>
		<author>
			<persName><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Narasimhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Neural Information Processing Systems (NIPS</title>
		<meeting>the 32nd International Conference on Neural Information Processing Systems (NIPS</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Hierarchical text-conditional image generation with clip latents</title>
		<author>
			<persName><forename type="first">A</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Dhariwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Nichol</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">High-resolution image synthesis with latent diffusion models</title>
		<author>
			<persName><forename type="first">R</forename><surname>Rombach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Blattmann</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Lorenz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Esser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Ommer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Image super-resolution via iterative refinement</title>
		<author>
			<persName><forename type="first">C</forename><surname>Saharia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">W</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="4713" to="4726" />
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Human motion diffusion model</title>
		<author>
			<persName><forename type="first">G</forename><surname>Tevet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Raab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Shafir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">H</forename><surname>Bermano</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=SJ1kSyO2jwu" />
	</analytic>
	<monogr>
		<title level="m">The Eleventh International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">A</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">N</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><surname>Polosukhin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page">17</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">P</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fg-t2m: Fine-grained text-driven human motion generation via diffusion model</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><forename type="middle">W B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2023-10">October 2023</date>
			<biblScope unit="page" from="22035" to="22044" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">T2m-gpt: Generating human motion from textual descriptions with discrete representations</title>
		<author>
			<persName><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Cun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<meeting>the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR</meeting>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2208.15001</idno>
		<title level="m">Motiondiffuse: Text-driven human motion generation with diffusion model</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Remodiffuse: Retrievalaugmented motion diffusion model</title>
		<author>
			<persName><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">F</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2023-10">October 2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Attt2m: Text-driven human motion generation with multiperspective attention mechanism</title>
		<author>
			<persName><forename type="first">C</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE/CVF International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2023-10">October 2023</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
