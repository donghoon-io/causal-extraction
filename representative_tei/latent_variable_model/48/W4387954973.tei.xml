<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">NONLINEAR DIMENSIONALITY REDUCTION THEN AND NOW: AIMS FOR DISSIPATIVE PDES IN THE ML ERA A PREPRINT</title>
				<funder ref="#_MT8GPns #_MykQW4z">
					<orgName type="full">unknown</orgName>
				</funder>
				<funder ref="#_WQU9KfD">
					<orgName type="full">US AFOSR</orgName>
				</funder>
				<funder ref="#_DKQuMZK">
					<orgName type="full">Deutsche Forschungsgemeinschaft (DFG)</orgName>
				</funder>
				<funder ref="#_ah56wsK">
					<orgName type="full">US Department of Energy</orgName>
				</funder>
				<funder ref="#_NMsNXDe">
					<orgName type="full">International</orgName>
				</funder>
				<funder ref="#_dSKWr6k">
					<orgName type="full">CRC</orgName>
				</funder>
				<funder ref="#_QdzQxT4">
					<orgName type="full">Luxembourg National Research Fund (FNR)</orgName>
				</funder>
				<funder ref="#_xPgr7TH">
					<orgName type="full">Qatar National Research Fund</orgName>
				</funder>
				<funder>
					<orgName type="full">Qatar Foundation</orgName>
				</funder>
				<funder ref="#_XCTyChu">
					<orgName type="full">King Abdullah University of Science and Technology (KAUST) Office of Sponsored Research</orgName>
					<orgName type="abbreviated">OSR</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Eleni</forename><forename type="middle">D</forename><surname>Koronaki</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facult√© des Sciences</orgName>
								<orgName type="institution" key="instit1">de la Technologie et de la Communication</orgName>
								<orgName type="institution" key="instit2">Universit√© de Luxembourg</orgName>
								<address>
									<addrLine>Maison du Nombre, Avenue de la Fonte 6</addrLine>
									<postCode>L-4364</postCode>
									<settlement>Esch-sur-Alzette</settlement>
									<country key="LU">Luxembourg</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Nikolaos</forename><surname>Evangelou</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Chemical and Biomolecular Engineering and Department of Applied Mathematics and Statistics</orgName>
								<orgName type="department" key="dep2">Whiting School of Engineering</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<addrLine>3400 North Charles Street</addrLine>
									<postCode>21218</postCode>
									<settlement>Baltimore</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Cristina</forename><forename type="middle">P</forename><surname>Martin-Linares</surname></persName>
							<affiliation key="aff2">
								<orgName type="department" key="dep1">Department of Mechanical Engineering</orgName>
								<orgName type="department" key="dep2">Whiting School of Engineering</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<addrLine>3400 North Charles Street</addrLine>
									<settlement>Baltimore</settlement>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Edriss</forename><forename type="middle">S</forename><surname>Titi</surname></persName>
							<affiliation key="aff3">
								<orgName type="department">Department of Mathematics</orgName>
								<orgName type="institution">Texas A &amp; M University</orgName>
								<address>
									<postCode>77843</postCode>
									<settlement>College Station</settlement>
									<region>TX</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Department of Applied Mathematics and Theoretical Physics</orgName>
								<orgName type="institution">University of Cambridge</orgName>
								<address>
									<postCode>CB3 0WA</postCode>
									<settlement>Cambridge</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Ioannis</forename><forename type="middle">G</forename><surname>Kevrekidis</surname></persName>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">Department of Chemical and Biomolecular Engineering and Department of Applied Mathematics and Statistics</orgName>
								<orgName type="department" key="dep2">Whiting School of Engineering</orgName>
								<orgName type="institution">Johns Hopkins University</orgName>
								<address>
									<addrLine>3400 North Charles Street</addrLine>
									<postCode>21218</postCode>
									<settlement>Baltimore</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">NONLINEAR DIMENSIONALITY REDUCTION THEN AND NOW: AIMS FOR DISSIPATIVE PDES IN THE ML ERA A PREPRINT</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-28T22:12+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>This study presents a collection of purely data-driven workflows for constructing reduced-order models (ROMs) for distributed dynamical systems. The ROMs we focus on, are data-assisted models inspired by, and templated upon, the theory of Approximate Inertial Manifolds (AIMs); the particular motivation is the so-called post-processing Galerkin method of Garcia-Archilla, Novo and Titi. Its applicability can be extended: the need for accurate truncated Galerkin projections and for deriving closed-formed corrections can be circumvented using machine learning tools. When the right latent variables are not a priori known, we illustrate how autoencoders as well as Diffusion Maps (a manifold learning scheme) can be used to discover good sets of latent variables and test their explainability. The proposed methodology can express the ROMs in terms of (a) theoretical (Fourier coefficients), (b) linear data-driven (POD modes) and/or (c) nonlinear data-driven (Diffusion Maps) coordinates. Both Black-Box and (theoretically-informed and data-corrected) Gray-Box models are described; the necessity for the latter arises when truncated Galerkin projections are so inaccurate as to not be amenable to post-processing. We use the Chafee-Infante reaction-diffusion and the Kuramoto-Sivashinsky dissipative partial differential equations to illustrate and successfully test the overall framework.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Separation of time-scales in dynamical systems is crucial toward the development of Reduced Order Models (ROMs). For a certain class of dissipative evolution equations, the long term dynamics are attracted exponentially fast to smooth invariant objects known as inertial manifolds (IMs), facilitating the construction of ROMs on those. The dynamics on the IM can be described by the Inertial Form (a finite ODE system), which accurately captures the long-term behavior of the original infinite-dimensional system <ref type="bibr" target="#b53">[Shvartsman and Kevrekidis, 1998</ref><ref type="bibr" target="#b29">, Jolly et al., 1990</ref><ref type="bibr">, Titi, 1990</ref><ref type="bibr" target="#b2">, Akram et al., 2020]</ref>. The purpose of this paper is to (somewhat systematically) outline (and demonstrate) links between "traditional" AIM technology and contemporary data-driven reduction tools, giving rise to "mathematics-assisted" algorithmic ROM workflows. Such connections had initially been experimentally attempted in the 1990s (e.g. <ref type="bibr" target="#b34">Krischer et al. [1993]</ref>, <ref type="bibr" target="#b58">Theodoropoulos et al. [2000]</ref>); they are currently experiencing a strong revival due to the explosion in machine-learning-assisted modelling <ref type="bibr" target="#b38">[Linot and Graham, 2020</ref><ref type="bibr" target="#b4">, Anirudh et al., 2020</ref><ref type="bibr" target="#b36">, Lee and Carlberg, 2020</ref><ref type="bibr" target="#b5">, Bar-Sinai et al., 2019</ref><ref type="bibr" target="#b6">, Benner et al., 2015]</ref>.</p><p>IMs have been proven to exist for only a few systems, and even then, they have not been constructed explicitly <ref type="bibr" target="#b29">[Jolly et al., 1990]</ref>. It is, nevertheless, still possible to find approximations of either the global attractor or the IM itself, i.e. Approximate Inertial Manifolds (AIMs), or the dynamics on it, i.e. the Approximate Inertial Form (AIF), and then track the dynamics in this reduced space. The key ansatz is that the attracting dynamics in the complement space to the AIM, are quickly slaved to, and embodied in, the AIF. Along these lines, the Galerkin projection, as well as nonlinear Galerkin projections on approximate inertial manifolds are also popular choices for reduced order modeling <ref type="bibr" target="#b43">[Marion and Temam, 1990</ref><ref type="bibr" target="#b30">, Jolly et al., 1991</ref><ref type="bibr" target="#b52">, Shen, 1990</ref><ref type="bibr" target="#b6">, Benner et al., 2015]</ref>. AIM-based ROMs have been proposed for reaction-diffusion systems <ref type="bibr">[Foias et al., 1988b</ref><ref type="bibr" target="#b1">, Adrover et al., 2002]</ref>, the Kuramoto-Sivashinsky equation <ref type="bibr">[Foias et al., 1988b</ref><ref type="bibr" target="#b57">, 1989</ref><ref type="bibr" target="#b29">, Jolly et al., 1990]</ref>, the two-dimensional Navier-Stokes equations <ref type="bibr">[Temam, 1989b</ref><ref type="bibr" target="#b27">,a, Jauberteau et al., 1990]</ref>, and for the three-dimensional Navier-Stokes equations <ref type="bibr" target="#b26">[Guermond and Prudhomme, 2008]</ref>.</p><p>It must also be noted, that, for a dissipative PDE, in the case where a clear time-scale separation exists, we converge to the inertial manifold exponentially fast. In the absence of a large separation we still converge, exponentially fast, to a thin layer around some manifold, also called an approximate inertial manifold. The dynamics in this case are exponentially fast attracted to this layer (that contains the attractor) but, once the trajectory enters the layer it may "take forever" to arrive to the attractor.</p><p>In the late 90s the post-processing Galerkin method was proposed <ref type="bibr" target="#b22">[Garc√≠a-Archilla et al., 1998, Garc√≠a-Archilla and</ref><ref type="bibr">Titi, 1999]</ref>, initially in the context of dissipative equations. Post-processing Galerkin takes into account the observation that the error between the result of integrating a truncated Galerkin on the one hand, and the projection of the true solution in the finite-dimensional Galerkin space, on the other, is significantly smaller than the error between the truncated Galerkin and the full solution (superconvergence <ref type="bibr" target="#b23">[Garc√≠a-Archilla et al., 1999</ref><ref type="bibr" target="#b61">, Wahlbin, 2006]</ref>). We will return to this below and illustrate it in Sec. 4 and Fig. <ref type="figure" target="#fig_5">6</ref>. Given this observation, one uses the dynamics expressed only in terms of the leading low modes (a truncated version of the equations) to integrate. Once the time integration is finished one can post-process the obtained solution by approximating the high modes as a function of the solution in the leading modes. Since, in the post-processing Galerkin framework, the correction is computed only at the end of time integration, this makes it much cheaper to implement computationally than true nonlinear Galerkin <ref type="bibr" target="#b22">[Garc√≠a-Archilla et al., 1998, Garc√≠a-Archilla and</ref><ref type="bibr">Titi, 1999]</ref>. Moreover, truncation analysis derivation of the spectral method for dissipative evolution equations, such as the Navies-Stokes equations, gives rise to the post-processing Galerkin as the leading order numerical scheme, and not the Galerkin scheme itself, as commonly believed <ref type="bibr" target="#b42">[Margolin et al., 2003]</ref>.</p><p>Model identification assisted by machine-learning has emerged in the 90s and is now experiencing a rebirth as a tool to discover minimal parametrizations of an IM, which can subsequently be used to evolve the dynamical system in a reduced space <ref type="bibr" target="#b41">[Lu et al., 2017</ref><ref type="bibr" target="#b10">, Chorin and Lu, 2015</ref><ref type="bibr" target="#b40">, Zeng and Graham, 2023</ref><ref type="bibr" target="#b63">, Zeng et al., 2022</ref><ref type="bibr" target="#b14">, De Jes√∫s and Graham, 2023</ref><ref type="bibr" target="#b40">, Linot et al., 2023]</ref>. Some efforts implemented linear methods like POD <ref type="bibr" target="#b34">[Krischer et al., 1993</ref><ref type="bibr" target="#b31">, Kang et al., 2015</ref><ref type="bibr" target="#b58">, Theodoropoulos et al., 2000]</ref>, to identify a suitable subspace that contains the majority of the variance of the system, and parametrizes the long term dynamics. More recently, operator inference with quadratic manifolds has been proposed for model reduction <ref type="bibr" target="#b25">[Geelen et al., 2023</ref><ref type="bibr" target="#b62">, Zastrow et al., 2023</ref><ref type="bibr" target="#b48">, Qian et al., 2022</ref><ref type="bibr" target="#b46">, McQuarrie et al., 2021]</ref>. Nonlinear dimensionality reduction methods, such as autoencoders <ref type="bibr" target="#b33">[Kramer, 1991]</ref> or Diffusion Maps (DMAPs) <ref type="bibr" target="#b13">[Coifman et al., 2008]</ref> have also been used to discover latent variables of data that originally live in a highdimensional space. Learning a dynamical system in the latent space of an autoencoder (even as a collection of local charts), or in Diffusion Maps space, also provides a systematic approach to ROM construction (e.g. <ref type="bibr" target="#b50">Rico-Martinez et al. [1992]</ref>, <ref type="bibr" target="#b56">Sonday et al. [2010]</ref>, <ref type="bibr" target="#b17">Evangelou et al. [2023]</ref>, <ref type="bibr" target="#b39">Linot and Graham [2022]</ref>, <ref type="bibr" target="#b36">Lee and Carlberg [2020]</ref>, <ref type="bibr" target="#b5">Bar-Sinai et al. [2019]</ref>). Needless to say, nonlinear system identification assisted by machine learning remains a very active current research endeavor, encompassing a plethora of directions from symbolic methods e.g. <ref type="bibr" target="#b7">[Brunton et al., 2016]</ref>, to physics-informed methods, e.g. <ref type="bibr" target="#b49">[Raissi et al., 2019]</ref>, to numerics-informed methods <ref type="bibr" target="#b5">[Bar-Sinai et al., 2019]</ref>.</p><p>In our view, the "1980s" IM and AIM efforts towards useful reduced order models of dissipative PDEs can be succinctly summarized as follows: Given the functional form of the PDE for which we know (or believe) an IM exists, and having an estimate of the dimensionality of said manifold:</p><p>(A) start by finding the (leading) eigenmodes, say k of them, of the (dissipative part of the) operator that "determines" (parametrizes) the IM. In that sense, the components of the solution in the remaining "higher order" eigenmodes can be expressed as functions of the components in the lower, determining, ones;</p><p>(B) guided by separation of time scales ideas, construct the AIM approximating this function, by writing the components of the higher eigenmodes as (approximate) functions of the components of the lower, determining ones. Several implementable such approximations have been proposed and analysed: e.g. the "steady" manifold, the "Euler-Galerkin", and the Foias-Manley-Temam (FMT) manifold among others. We already have a practical result: if somebody provides as observations the lower mode amplitudes, we can meaningfully and analytically improve the full spatiotemporal solution, complementing it with the recovered higher mode components. We will return to this theme when discussing post-processing Galerkin. Let it be noted here that even though the original motivation of AIM was to find an approximation to the IM whenever the latter exists, however, this idea was generalized later and implemented by finding a manifold which approximates the global attractor as a set; observing that global attractor always exists for genuine dissipative dynamical system;</p><p>(C) beyond just correcting such observations, these functions can be used to correct approximations of the dynamics through their low-order Galerkin truncation: From an accurate, high-order Galerkin truncation, we keep only the low, "determining" Galerkin ODEs; instead of omitting the higher order terms as negligible, we now substitute the AIM function in the low terms. We now have the "steady", or "Euler-Galerkin" or FMT inertial forms.</p><p>This original program is complemented by the "post-processing Galerkin" protocol: Here we actually keep the low order Galerkin truncation, ignoring the contribution of the higher order, slaved modes to it, expecting/believing that, in its low-dimensional space, these few ODEs are accurate enough to approximate the projection of the exact solution on the Galerkin space. The authors of <ref type="bibr" target="#b22">[Garc√≠a-Archilla et al., 1998</ref><ref type="bibr">, Garc√≠a-Archilla and Titi, 1999</ref><ref type="bibr">, Garc√≠a-Archilla et al., 1999]</ref> took into account the observation that the total error of the solutions predicted by the truncated low-order Galerkin is appreciably larger than the error after adding to them (in a sense, "reinjecting") the AIM-approximated higher order solution components. This reinjection is performed after the truncated low-order Galerkin equations have been integrated until each time instance of interest (we remind the reader that this is revisited in Sec. 4 and Figure <ref type="figure" target="#fig_5">6</ref>).</p><p>They named the approach "post-processing Galerkin" since it takes place after the truncated low-order Galerkin has been obtained and integrated: it is these concrete available solutions of the model that are being improved -not the model itself.</p><p>Explicit AIMs have been obtained in the context of spectral Galerkin approximation by writing approximations of the evolution equation of the high modes in terms of the low modes, a closure relation. In the context of spectral Galerkin approximation based on Fourier modes or eigenfunctions of the Stokes operator, one can naturally decouple the phase space into low Fourier (eigenfunctions) modes and their complement high Fourier (eigenfunctions) modes. Therefore, the above-described strategy of obtaining AIM is possible to be executed explicitly, and leads to an analytical closure.</p><p>For the examples we present in this work, the spectral Galerkin approximation could indeed provide a desirable closure.</p><p>However, we would like to note that in the context of the Finite Element Galerkin method, the above decomposition to coarse spatial scales and their complement is not a straightforward task. Therefore the above strategy can not be followed to obtain an explicit (paper and pencil) closure form, that expresses the fine spatial scales of the solution in terms of the coarse finite elements spatial scales. For this case, a more general framework for implementing the Post-processing Galerkin can be used <ref type="bibr" target="#b23">[Garc√≠a-Archilla and Titi, 1999]</ref>. In this more general case, an explicit form of an AIM in order to implement post-processing Galerkin is not required. We briefly present this more general scheme in Sec A.0.1.</p><p>The innovation of this work, as it progresses beyond symbolic model (AIM) or solution (post-processing Galerkin) improvement, is the introduction of data-driven methods that allow us (given accurate simulation data or observations), to efficiently address the following issues:</p><p>(a) Estimate the AIM dimensionality in a data-driven way (either through autoencoders or through manifold learning).</p><p>(b) Learn good reduced AIFs (the "correct", nonlinear Galerkin, right hand-side of the reduced, low order, components of the PDE) in a data-driven way.</p><p>(c) Learn the AIM functions (high order mode components as a function of low order model components) in a data driven way.</p><p>(d) Given the learned AIM in (c), correct the solutions of a low-order Galerkin truncation (a "data driven" postprocessing Galerkin).</p><p>Beyond the steps (b-d) above, that more or less correspond to the traditional (A-C) analytical steps, a couple of very useful data-driven "twists" are introduced.</p><p>(e) Circumvent the assumption of accuracy of the low-order (linear) Galerkin truncation; the low order AIF is learned from observations of the low-order components of accurate full PDE dynamics; and now the "postprocessing" that follows can be done (1) with the same "old" analytical AIMs, or, interestingly (2) with data-driven learned AIMs from the same accurate full PDE dynamics.</p><p>(f) Gray-Box (in some sense "physics-assisted") learning: instead of a fully black-box learning of the AIF using PDE observations, we now learn the correction of the not-so-quantitative low-order linear Galerkin trunca-tion. This correction can be learned as an additive (residual) term, or even as a functional correction -hoping for easier training, since what is learned is a perturbation of the identity <ref type="bibr" target="#b45">[Martin-Linares et al., 2023]</ref>.</p><p>(g) (This is not so much a step in our list, as a branching towards new capabilities). Up to now, everything but the eigenfunctions parametrizing the manifold was data-driven; the eigenfunctions themselves were still analytical. If we allow ourselves to find the parametrization of the manifold in a data-driven way, two individually significant new options arise:</p><p>(i) Use linear data-driven eigenfunctions: the leading Principal Components (PODs) of the full accurate PDE simulations. Now the low-order PODs parametrize the manifold, and the higher order POD components embody the AIM. POD-Galerkin takes the place of traditional Galerkin. (ii) Use a nonlinear, data-driven AIM parametrization and either:</p><p>‚Ä¢ use the latent variables of an autoencoder to parametrize the AIM, learn the corresponding accurate AIF, and post-process it to more accurate spatiotemporal PDE solution reconstruction; ‚Ä¢ use the leading POD components to parametrize the AIM, learn the accurate corresponding AIF, and post-process it for a more accurate spatiotemporal PDE solution reconstruction. At the risk of making this list ridiculously long, we also add -and illustrate below-the possibility ‚Ä¢ of using spectral (Diffusion Map) data mining to parametrize the AIM along with the associated Geometric Harmonics for the post-processing.</p><p>A schematic overview of the different options proposed in each case, are presented in Fig. <ref type="figure" target="#fig_0">1</ref>, with references to the subsequent sections where they are discussed in detail.  The remainder of the paper is organized as follows: After listing the illustrative examples used in this study (Sec 2), we proceed with describing the methodology (Sec. 3.) We start with briefly reviewing the "traditional" approximations of IMs and IFs (and AIMs and AIFs) (Sec. 3.1). We then discuss neural network-based alternatives to approximating IMs and IFs (Sec. 3.1.2), followed by nonlinear manifold learning methods for determining the dimensionality and parametrization of the latent space (Sec. 3.2.1 and 3.2.2). After presenting our results we conclude by pointing out that the technology can be easily "transferred" to POD parametrizations of the IM (Sec. 3.2.3).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data on</head><p>2 Illustrative examples: The Chafee-Infante and the Kuramoto-Sivashinsky equations Our first example is the reaction-diffusion Chafee-Infante partial differential equation (PDE), for which the dimensionality of the Inertial Manifold (IM), for the parameter range of interest, is known; it reads:</p><formula xml:id="formula_0">u t = u -u 3 + ŒΩu xx .</formula><p>(1)</p><p>The parameter ŒΩ was chosen as ŒΩ = 0.16 and Dirichlet boundary conditions, u(0, t) = u(œÄ, t) = 0, were used. The Chafee-Infante PDE, for ŒΩ = 0.16, has been shown to have a two-dimensional inertial manifold <ref type="bibr" target="#b55">[Sonday, 2011</ref><ref type="bibr" target="#b24">, Gear et al., 2011</ref><ref type="bibr" target="#b28">, Jolly, 1989</ref><ref type="bibr" target="#b16">, Evangelou et al., 2022]</ref>. To simulate the dynamics on/near this two-dimensional manifold, the Galerkin projection</p><formula xml:id="formula_1">u(x, t) ‚âà 3 k=1 Œ± k (t)sin(kx)<label>(2)</label></formula><p>was used <ref type="bibr" target="#b24">[Gear et al., 2011</ref><ref type="bibr" target="#b28">, Jolly, 1989</ref><ref type="bibr" target="#b16">, Evangelou et al., 2022]</ref>. The first two leading sine coefficients Œ± 1 (t), Œ± 2 (t) are sufficient to parameterize this two-dimensional manifold, and Galerkin equations based only on the first two modes provide a qualitatively correct approximation of the dynamics in these two modes. We will consider the solution of the Chafee-Infante equation with three modes as the ground truth (cf Fig. <ref type="figure" target="#fig_6">7a</ref>). For the post-processing process, the truncated equations with the first two sine coefficients Œ± = {Œ± 1 , Œ± 2 } are the ones used for integration up to time t = T . Then, their solution is post-processed to recover Œ± = Œ± 3 and reconstruct the full solution u(x, T ). For this first example, the truncated dynamics governed by the first two sine coefficients are (considered to be) qualitatively, but not quantitavely accurate; the post-processing step aims to correct the obtained solution from these truncated dynamics.</p><p>To demonstrate the potential of the proposed methodology in a case with more complex dynamics, we select the Kuramoto-Sivashinsky (KS) PDE,</p><formula xml:id="formula_2">u t = -ŒΩ(uu x + u xx ) -4u xxxx ; for x ‚àà [0, 2œÄ].<label>(3)</label></formula><p>The KS (Equation ( <ref type="formula" target="#formula_2">3</ref>)) is a prototypical equation with dynamics that include chaos, derived in the context of a diverse range of physical systems such as, but not limited to, thin film flow on inclined planes and instabilities in a laminar flame front <ref type="bibr" target="#b35">[Kuramoto and Tsuzuki, 1976</ref><ref type="bibr" target="#b54">, Sivashinsky, 1977</ref><ref type="bibr" target="#b3">, Alekseenko et al., 1985</ref><ref type="bibr">, Chang, 1986a</ref><ref type="bibr">,b, Jolly et al., 1990</ref><ref type="bibr">, Kevrekidis et al., 1990]</ref>. The parameter ŒΩ in our case is set to ŒΩ = 33 and periodic boundary conditions are used u(0, t) = u(2œÄ, t). In this example, Fourier series expansion with 8 terms is used to approximate the ground truth u(x, t):</p><formula xml:id="formula_3">u(x, t) ‚âà 8 k=1 Œ± k (t)sin(kx) + Œ≤ k (t)cos(kx); x ‚àà [0, 2œÄ],<label>(4)</label></formula><p>which results in 8 ODEs for the sine coefficients ({Œ± k } 8 k=1 ) and 8 for the cosine coefficients ({Œ≤ k } 8 k=1 ). Restriction to the space of odd functions leads to retaining only the sine terms, resulting in a system of 8 ODEs for the sine coefficients which is considered, in this work, as the exact solution of the KS. We use the truncation to the leading three sine coefficients Œ± = {Œ± 1 , Œ± 2 , Œ± 3 } to study the dynamics for ŒΩ = 33; however even though it has been shown that a 3D manifold exists, the truncated equations based on the leading coefficients do not provide an accurate approximation of the dynamics of these coefficients. In this case the traditional post-processing Galerkin methodology, does not apply (we do not have a good base solution to correct). We circumvent this issue by constructing Gray-Box models, as we show below in Sec. 4.3.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Methodology</head><p>3.1 Approximating the IM and the IF (known latent space)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Euler-Galerkin</head><p>As a preamble to traditional post-processing Galerkin, here we discuss nonlinear Galerkin schemes, in particular the "Euler-Galerkin" algorithm, that provides a closed-form approximation of inertial manifolds <ref type="bibr">[Foias et al., 1988a]</ref>. Consider the evolution equation</p><formula xml:id="formula_4">du dt + Au + F (u) = 0, u ‚àà H (5)</formula><p>where H is an appropriate Hilbert space, A is a self-adjoint positive-definite linear operator with compact inverse, and let F be a nonlinear operator such that equation ( <ref type="formula">5</ref>) is globally well-posed in time for all initial data in H. By denoting a projection onto the span of the first n eigenvectors of A by P and Q = I -P we can split Equation ( <ref type="formula">5</ref>) into dp dt + Ap + P F (p + q) = 0 (6)</p><formula xml:id="formula_5">dq dt + Aq + QF (p + q) = 0<label>(7)</label></formula><p>where p = P u, q = Qu and q+p = u. Assuming that the long-term dynamics of Equation ( <ref type="formula">5</ref>) live in a n-dimensional inertial manifold described as the graph of a function Œ¶ : P H ‚Üí QH we can write the projection of the inertial manifold onto P H as</p><formula xml:id="formula_6">dp dt + Ap + P F (p + Œ¶(p)) = 0. (<label>8</label></formula><formula xml:id="formula_7">)</formula><p>An approximation of Œ¶ is achieved through a Galerkin truncation of m modes in Equation ( <ref type="formula" target="#formula_5">7</ref>) , where m &gt; n. The projection to the space of the higher modes n + 1, . . . , m defines Q m . Since the higher modes are attracted exponentially fast to the IM and become functions of the lower modes, we perform an implicit Euler step to approximate the solution q with a step size œÑ . By assuming an initial condition q 0 = 0 we get</p><formula xml:id="formula_8">q + œÑ Aq + œÑ Q m F (p + q) = 0.<label>(9)</label></formula><p>Instead of completely solving equation ( <ref type="formula" target="#formula_8">9</ref>) we perform a single fixed-point iteration using an initial q = 0 and holding the lower modes 1, . . . , n (the components of p) constant. This gives the approximation:</p><formula xml:id="formula_9">Œ¶m (p) = -œÑ (I + œÑ A) -1 Q m F (p)<label>(10)</label></formula><p>an algebraic expression that estimates the higher modes {n + 1, . . . , m} as a function of the lower n modes and thus an approximation of the IM itself.</p><p>Substituting Œ¶m (p) for the m -n higher modes gives</p><formula xml:id="formula_10">dp dt + Ap + P F (p + Œ¶m (p)) = 0<label>(11)</label></formula><p>and more precisely an Euler-Galerkin approximation consisting of n differential equations dp dt</p><formula xml:id="formula_11">+ Ap + P F (p -œÑ (I + œÑ A) -1 Q m F (p)) = 0.<label>(12)</label></formula><p>In this work, the (nonliner) Euler-Galerkin algorithm was applied to the Chafee-Infante partial differential equation, as detailed in Sec. A.0.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Neural network derived AIM and AIF</head><p>The higher sine modes' coefficients Œ±, which are necessary for accurate reconstruction of the solution in physical space, can be obtained in a data-driven manner. Specifically, here we use deep neural networks, schematically shown in Fig. <ref type="figure">2</ref>, to learn the coefficients Œ±, given the values of leading (lower) sine modes' coefficients at a specific point in time, t = T , Œ±(T ). Œ±(t) = f N N ( Œ±(t)), where Œ± stands for leading sine coefficients (low modes) and Œ± stands for the higher sine modes coefficients. The leading coefficients Œ±(T ) have been obtained as a result of the time integration of the truncated dynamics.</p><p>Alternatively, when the result of time-integration of the two truncated lower sine coefficients equations, is inaccurate, we can correct it by learning a data-driven truncated ODE in the lower sine coefficients, with general form:</p><formula xml:id="formula_12">dŒ± dt = f (Œ±) (13) Œ± 2 (t) Œ± 1 (t) Œ± 3 (t) Figure 2:</formula><p>Illustrative example of a feed-forward neural network for prediction of higher coefficients. In this example the lower harmonics, Œ± 1 (t) and Œ± 2 (t) are used as inputs to the network that predicts Œ± 3 (t)</p><p>where Œ± ‚àà R m , here m = 2, are the variables in which we observe the evolution of the dynamics. Observe that since: m=2 here the Poincar√©-Bendixon theorem applies. Hence the dynamics of the low modes is either goes to a limit-cycle or to a steady state. This data-driven AIF was first explicitly described and implemented in <ref type="bibr" target="#b58">[Theodoropoulos et al., 2000]</ref> (see also in <ref type="bibr" target="#b34">[Krischer et al., 1993]</ref>).</p><p>The function f is approximated by a fully connected neural network, schematically represented in Fig. <ref type="figure" target="#fig_1">3</ref>. The goal is to predict the time derivatives of the lower sine coefficients from the their values. Once this is done, the right-hand-side of the ODEs in Eq. 13 can be used in conjunction with any method of integration in time, such as the Runge-Kutta, to accurately approximate Œ±(T ) and then proceed as above to post-process Œ±(T ).</p><p>ùëé ! Ãá(ùë°) ùëé " Ãá(ùë°) ùëé ! (ùë°) ùëé " (ùë°) For parameter values of the KS equation for which the long-term truncated dynamics may not be accurate, an appealing alternative to the Black-Box approach discussed above arises.</p><p>One can remedy the situation by first correcting the reduced dynamics, before deriving the missing terms for reconstruction. This can be achieved by constructing a "Gray-Box" data-driven dynamic model. This Gray-Box model describes the evolution of a reduced system, by adding to the truncated dynamics a learned correction term, which can be thought of as a closure. This correction is approximated by a neural network that takes as inputs the lower order sine coefficients and delivers the difference between their true time-derivatives and the truncated Galerkin time-derivatives:</p><formula xml:id="formula_13">d Œ±p dt - d Œ±t dt = g N N ( Œ±(t))</formula><p>where d Œ±p dt is the true vector field projected in the leading sine coefficients Œ± and d Œ±t dt is the vector field of the corresponding truncated Galerkin projection.</p><p>Here, g N N is approximated using a neural network implemented in tensorflow <ref type="bibr" target="#b0">[Abadi et al., 2015]</ref> with 6 hidden layers, 95 neurons each, and a tanh activation function. The loss function used is the mean squared error (MSE), and the Adam optimizer is employed.</p><p>Finally, it is worth noting that the proposed workflow works equally well, when considering evolution equation of the leading POD mode coefficients, as parametrizing the IM. An illustrative example, based on the Chafee-Infante POD-based equations can be found in A.0.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Learning the dimensionality of the latent space</head><p>In most cases, a minimal parametrization of the IM of a dynamical system is not known a priori. It is possible to discover it, using different data mining approaches, such as Diffusion Maps and autoencoders. Both methods are discussed in the following paragraphs and summarized in Fig. <ref type="figure" target="#fig_2">4</ref> ùúë 5 ùúë 1 ùúë 5</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Diffusion Maps Double DMaps</head><p>(a)</p><p>Œ± 1 Œ± 2 Œ± 3 Œ± 4 Œ± 5 Œ± 6 Œ± 7 Œ± 8</p><p>Bottleneck layer </p><formula xml:id="formula_14">L 1 L 2 L 3 encoder decoder ùëé " ! ùëé " " ùëé " # ùëé " $ ùëé " % ùëé " &amp; ùëé " ' ùëé " (<label>(b)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Diffusion Maps</head><p>Diffusion maps <ref type="bibr">[Coifman and Lafon, 2006b</ref><ref type="bibr" target="#b47">, Nadler et al., 2006</ref><ref type="bibr" target="#b13">, Coifman et al., 2008]</ref> is a manifold learning framework that can (based upon diffusion processes) facilitate discovering low-dimensional intrinsic geometric descriptions of data sets, even when the data is high-dimensional, nonlinear and/or corrupted by (relatively small) noise. It is used here to discover the dimensionality of the IM and provide a data-driven parametrization of it.</p><p>The parametrization of the manifold is obtained through a few eigenvectors, œï i , of a scaled affinity matrix, which contains the Euclidean distances between all the pairs of available data points. A detailed description of the Diffusion Maps algorithm is provided in the Sec. A.3 of the Appendix and for the Double Diffusion Maps in Sec. A.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Autoencoders</head><p>Autoencoders <ref type="bibr" target="#b33">[Kramer, 1991]</ref> are neural networks that are trained (a) to encode high-dimensional data into a lowdimensional representation (b) to reconstruct the original high-dimensional from this lower-dimensional representation (cf.Fig. <ref type="figure" target="#fig_2">4b</ref>). In this context, the input layer is the same as the output, and the low-dimensional encoding is parametrized by the weights of the bottleneck layer. The loss function</p><formula xml:id="formula_15">L Œ∏ = ||Œ± (k) -·æ±(k) || 2 (14)</formula><p>is commonly used to train an autoencoder where Œ± (k) represent a data point in the ambient space and ·æ±(k) the reconstructed data point k from the autoencoder.</p><p>In this work, we use autoencoders for an additional second use case, which relies on the observation that the discovered autoencoder latent coordinates are one-to-one with the leading sine coefficients Œ±, as discussed in detail in the following paragraph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Theoretical and data-driven latent variables: transformations and AIMs</head><p>The local one-to-one relation between the autoencoder's latent variables (L) and the leading sine coefficients ( Œ±) is tested by computing the Inverse Function Theorem across the training data. The Inverse Function Theorem guarantees local invertibility in a neighborhood of any point L i ‚àà L if the determinant of the Jacobian (det(J f (L)) is bounded away from zero. We provide a more detailed description of the Inverse Function Theorem in Sec. A.5 of the Appendix. The Jacobian computation in our case is performed by using automatic differentiation with Tensorflow.</p><p>In this case, a more global one-to-one correspondence (on the vicinity of our data) also exists. This is tested by constructing regression schemes between the leading sine coefficients and the latent coordinates of the autoencoder.</p><p>As shown in Fig. <ref type="figure" target="#fig_4">5</ref> upon training of the autoencoder, learning to infer the latent variables L from the leading sine coefficients, can be done either with a feedforward neural network or Double DMAPs. The fact that the latent variables are found to be one-to-one with the leading sine coefficients does not mean that we need to map back and forth between the latent variables and the leading sine coefficients during integration. It allows us to integrate in either set of variables (here we use the sine coefficients) and then uniquely map each point in one set to the other. </p><formula xml:id="formula_16">L i = f (! " 1 ! " 2 ! " 3 ), i=1, 2, 3</formula><p>Predicting full coefficients:</p><p>1. Decoder 2. Double DMAPs Alternatively, and this does not require training an additional regression scheme, the decoder part of the autoencoder can be used to compute an inverse-map. This inverse map utilizes the leading Fourier modes, Œ±, in which the dynamics have evolved, and the trained decoder, to find the latent autoencoder variables that minimize the algebraic optimization constraint</p><formula xml:id="formula_17">! " 1 ! " 2 ! " 3 ! " 4 ! " 5 ! " 6 ! " 7 ! " 8 ! " 1 ! " 2 ! " 3 ! " 4 ! " 5 ! " 6 ! " 7 ! " 8 Œ± 1 Œ± 2 Œ± 3 Œ± 4 Œ± 5 Œ± 6 Œ± 7 Œ± 8 L 1 L 2 L 3 L 1 L 2 L 3</formula><formula xml:id="formula_18">argmin L || Œ± -decoder(L)|| F . (<label>15</label></formula><formula xml:id="formula_19">)</formula><p>For the optimization in Equation ( <ref type="formula" target="#formula_18">15</ref>) only the variables ·æ±1 , ·æ±2 , ·æ±3 computed by the decoder are used; the latent autoencoder variables are denoted as L, the leading Fourier modes as Œ±. After solving the optimization problem in Equation ( <ref type="formula" target="#formula_18">15</ref>) the decoder can be used to recover all the Fourier modes given L.</p><p>This second use case of the autoencoder allows us to map from the leading Fourier modes to the latent space and back to the full Fourier models without the need of constructing an additional regression scheme. Once the latent variables are predicted, the decoder of the autoencoder or the inverse transformation from the DMAP to the ambient coordinates, is used to approximate the full set of reconstructed coefficients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results</head><p>Before presenting our results we remind the reader, through the illustration in Fig. <ref type="figure" target="#fig_5">6</ref> of the basic premise and the various errors associated with the post-processing Galerkin concept. The main premise is that the distance ‚àÜ 1 , between the projection of the true solution (point 5) and the truncated Galerkin solution (point 3) is much smaller than the distance ‚àÜ 3 between point 3 and the true solution (point 1), the total error of truncated Galerkin approximation <ref type="bibr" target="#b23">[Garc√≠a-Archilla et al., 1999]</ref>. This motivates the need for post-processing, which establishes that the distance ‚àÜ 4 between point 1 and the post-processed Galerkin (point 2) is also much smaller than the total error (and comparable to ‚àÜ 1 ) as shown in Fig. <ref type="figure" target="#fig_5">6</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Euler-Galerkin vs. neural-network AIMs: Chafee-Infante</head><p>A collection of time-instances of the Chafee-Infante equation, along trajectories for various initial conditions, that are sufficiently close to the global attractor are used for the purposes of the presented results. We make sure the density of the data is uniform by subsampling the collected data by using the datafold Python library <ref type="bibr" target="#b37">Lehmberg et al. [2020]</ref>.</p><p>We start by providing a comparison between the solution obtained with the three sine coefficients, here considered as the ground truth, (cf Fig. <ref type="figure" target="#fig_6">7a</ref>) and the truncated equations with the first two modes. The different post-processing schemes are applied to the solution of the truncated equations at the end of the desired integration. The comparison between the two is shown in Fig. <ref type="figure" target="#fig_6">7</ref> where the reconstructed solution is shown with a blue dashed line and the ground truth simulation with a red line. The relative error along each step of the time integration until time T = 5, is shown in Fig. <ref type="figure" target="#fig_6">7b</ref>. It is worth pointing out that, because the training data set is uniformly sampled on (or very close to) the global attractor (inertial manifold), all the trajectories remain on the global attractor, regardless of the integration time.</p><p>The solution of the 2D truncated dynamics is then corrected, using the value of Œ± 3 (T ) as predicted by a neural network (described in Sec. 3.1.1), using as inputs, the values of Œ± 1 (T ) and Œ± 2 (T ) at the final time-step, t = T . The results are shown in Fig. <ref type="figure" target="#fig_6">7c</ref> with a dashed blue line; included in the same figure, with a solid blue line, is the solution corrected with the theoretically (Euler-Galerkin AIM) derived value of Œ± 3 (T ). The relative error along the integration time till time T = 5 for the ML-derived Œ± 3 is shown in Fig. <ref type="figure" target="#fig_6">7d</ref>. Both, the ML-derived and the theoretical corrections help recover the accuracy and both lead to a mean absolute percentage error (MAPE) of less than 1%. The mean absolute percentage error (MAPE) is also computed at the same time instance (T = 5) but for 100 randomly selected initial conditions, for the 2D and the ML-corrected 2D model. This is shown in Fig. <ref type="figure" target="#fig_6">7e</ref>, where the favorable effect of the correction on the mean absolute percentage error is clearly visualized. In these and in subsequent results, the MAPE refers to point-wise average of the absolute percentage error in each sample.</p><p>As an alternative, it is also possible to correct the learned ODE in two dimensions, derived as described in 3.1.2. The accuracy achieved is similar to the accuracy of the true truncated 2D model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Kuramoto-Sivashinsky: Data-driven latent spaces and their AIMs</head><p>The KS equation is selected in order to explore the application of the proposed methodology in cases where the minimum dimension of the Approximate Inertial Form (AIF) is not known a priori, although it has been argued to be three-dimensional <ref type="bibr" target="#b29">[Jolly et al., 1990]</ref>. Nevertheless, the truncated dynamics are not always quantitavely close to the actual behavior, which will be addressed with "Gray-Box" modeling. The identification of the minimum dimension of the AIF is an important challenge in the implementation of post-processing Galerkin methods and will be addressed here with two different approaches: nonlinear manifold learning and in particular Diffusion Maps discussed in 3.2.1, and autoencoders discussed in 3.2.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Learning the dimensionality of the latent space</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Autoencoders</head><p>A collection of data is sampled for the KS parameter value ŒΩ = 33, in various time instances of time-integration sufficiently close or on the global attractor. The data are used as inputs to an autoencoder and are reduced by the encoder into into a low dimensional bottleneck layer which parametrizes an approximation of the inertial manifold. It is then possible to map to the approximation of the high dimensional variables with the decoder. The encoder/decoder components of the network can be used independently as it will be demonstrated in a subsequent section to improve the accuracy of the reduced order model.</p><p>The three latent variables of the bottleneck layer are one-to-one functions of the first three sine coefficients, Œ± 1 , Œ± 2 and Œ± 3 . This is shown in Fig. <ref type="figure">8</ref>, where the three bottleneck variables are plotted and colored according to the three sine coefficients. The smooth color variation suggests a one-to-one correlation between the latent and the ambient variables. It so happens that each one of the sine coefficients is one-to-one with each of the Diffusion Maps coordinates (the comparison is shown in the SI).</p><p>In Figure <ref type="figure">9</ref>, the histogram of the Jacobian determinant's values, det(J f (L)), along the training and test data shows that this quantity is always positive and thus the mapping f : L ‚Üí Œ± is locally invertible.</p><p>The one-to-one relationship between the leading sine coefficients and the autoencoder's latent variables L facilitates the second use case of the autoencoder we discussed earlier. This second use case utilizes the decoder to solve an inverse-problem and map the leading sine coefficients Œ± to the autoencoder's latent space. Since, we showed that f : L ‚Üí Œ± is a locally invertible map we can use the trained decoder and estimate L given Œ± by solving the optimization problem described in Equation <ref type="formula" target="#formula_18">15</ref>. As initial conditions to solve the optimization problem randomly sampled points from the training set were used. After optimization, the decoder can be used to reconstruct the remaining sine coefficients and from those the solution in u(x, t) space, from the obtained values in autoencoder's latent space.</p><p>In Figure <ref type="figure" target="#fig_0">10a</ref>, we contrast, for one reconstructed trajectory (i) the true solution u(x, T ) obtained from the full equations, (ii) the reconstructed solution based on the first three learned sine coefficients, and (iii) the reconstructed solution obtained by solving the inverse map and using the decoder to reconstruct the full solution. In Figure <ref type="figure" target="#fig_0">10b</ref> the histograms show a comparison of the MAPE in u(x, t) space between the true solution and the solutions based on (i) the three leading sine coefficients and (ii) the solution obtained after implementing the optimization step.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Diffusion Maps and their data-driven AIMs</head><p>Diffusion Maps is implemented to encode the high dimensional data to a low dimensional manifold parametrized by three Diffusion Maps coordinates shown in Fig. <ref type="figure">20</ref> of the Appendix. The Diffusion Maps coordinates œï 1 , œï 2 and œï 3 are one-to-one with the coefficients of the first three sine terms. This is shown in Fig. <ref type="figure">20</ref>, of the Appendix, by the smooth color transition in the diffusion maps plot when colored by Œ± 1 , Œ± 2 and Œ± 3 .</p><p>The sine coefficients, Œ± i , are reconstructed with the help of Double DMAPS and by the decoder of the autoencoder as discussed in Secs. 3.2.1 and 3.2.2 respectively. The MSE for the Double DMAPs approach is 0.00492, whereas for the autoencoder it is 0.0155. The precision of the autoencoder decreases for higher harmonics, which leads to the overall drop in accuracy of the reconstruction (this comparison is shown in the Sec. A.5). Double DMAPs predicts accurately all the coefficients (this comparison is shown in the Sec. A.5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Data-driven post-processing Galerkin</head><p>Having established that the first three sine coefficients are one-to-one with the data-driven latent variables, the next step is to learn a data-driven ODE of the time evolution of the first three reconstructed sine coefficients as described in Sec. 3.1.2.</p><p>The feedforward neural network is trained using as input the values of Œ±1 , Œ±2 and Œ±3 , that are reconstructed by the latent space learning methods, i.e. autoencoders and DMAPs (the results of latent space identification and reconstruction of the hight-dimensional variables is presented in the SI). The predicted time-derivatives, the right-hand-side of the learned ODE, for each one of the sine coefficients are pictured in Fig. <ref type="figure" target="#fig_18">22</ref> (of the SI) versus the actual values of that components time derivative, Œ±. The top row shows the predicted right-hand-side from the three sine coefficients resulting from the autoencoder, with M SE = 9.5. The respective predictions from the Double DMAPs reconstruction are shown on the bottom row, with M SE = 2.2.</p><p>The neural network-derived approximation is then used in conjunction with an ODE solver, such as the Runge-Kutta, in order to integrate in time. The outcome of integration is reconstructed in physical space and compared to the outcome of the ground truth integration (in 8D) and also the reconstructed solution using only the first three modes of the ground truth. This is shown in Fig. <ref type="figure" target="#fig_0">11</ref>, alongside the error between the learned 3D ODE and the actual 3 modes of the Galerkin approximation, which demonstrates that the learned ODE predicts accurately the low dimensional time evolution of the first three modes. It is worth looking into the time evolution of the first three modes, Œ± 1 , Œ± 2 and Œ± 3 that result from the truncated 3D dynamics and compare it to the evolution of the first three terms of the full 8D dynamics and those of the learned AIF ODE. This is shown in Fig. <ref type="figure" target="#fig_9">12</ref>, where it becomes evident that the first three terms of the learned 3D ODE are close to the trajectory of the first three terms of the 8D Galerkin. In contrast, the truncated 3D dynamics deviate significantly, past a certain point in time, from the ground truth dynamics. This observation suggests that using a post-processing scheme directly on the truncated equations won't be able to correct the dynamics. This motivates us to use an ML correction so-called "Gray-Box" model discussed further in the next section.</p><p>In essence, the post-processing Galerkin method relies on the premise that the solution of the truncated problem is reasonably close to the projection of the ground truth solution. Here it is demonstrated that even though the AIM is indeed three-dimensional, in the range of physical parameters examined, the truncated long term dynamics in three dimensional space are not accurate. This is demonstrated further in Fig. <ref type="figure" target="#fig_10">13</ref>, where the solution in physical space is reconstructed from the 8D Galerkin, the 3D Galerkin and the learned 3D AIF ODEs, in different instances along the same trajectory. At initial stages of the trajectory, the solutions of the three methods are reasonably close, as shown in Fig. <ref type="figure" target="#fig_10">13a</ref>. Later in time, the truncated 3D solution is growing quantitatively further apart from the ground truth, whereas the learned 3D AIF ODE follows closely the 8D dynamics (Fig. <ref type="figure" target="#fig_10">13b</ref>). This is made clear in Fig. <ref type="figure" target="#fig_10">13c</ref>, where the relative error between the learned and the truncated 3D solution is plotted along the trajectory. This can also be observed in phase space shown in Fig. <ref type="figure" target="#fig_10">13d</ref>, on the right. The red point corresponds to the initial condition. The values of sine coefficients initially evolve in a similar manner but eventually, the truncated 3D dynamics deviate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data-driven post-processing Galerkin</head><p>To recover the values of all the Œ± i s, necessary for accurate reconstruction, the first step involves predicting the latent variables, either the bottleneck variables from the autoencoder or alternatively the DMAPs latent coordinates. One way to achieve this, is with a feedforward neural network with three inputs (the the first three sine coefficients) and three outputs (the latent variables). In this implementation, the neural network consists of 5 hidden layers with 80 neurons each and a tanh activation function, implemented in tensorflow <ref type="bibr" target="#b0">[Abadi et al., 2015]</ref>. The mean squared error is used as the loss function along with the Adam optimizer.</p><p>It is then possible to employ either Double DMAPs, in the case of DMAPs, or the decoder of the autoencoder, and predict the corresponding Œ± i s with M SE = 0.09 for both cases. The reconstructed solution in physical space is compared to the ground truth in Fig. <ref type="figure" target="#fig_2">14</ref>. If we have a accurate low-dimensional observation we can correct, in principle theoretically with Euler-Galerkin, or in practice with machine learning approaches as described above. If this is not available, then we proceed to improve the AIF itself though the Gray-Box approach.</p><p>The method's performance is demonstrated in Fig. <ref type="figure" target="#fig_12">15</ref> for two cases. In the first case (cf Fig. <ref type="figure" target="#fig_12">15a</ref>), the 3D, 8D, and corrected Gray-Box dynamics are shown for the reconstructed physical space solution, at T 1 = 0.02, when the truncated 3D dynamics are close to the ground truth. At a later time-step, at T 2 = 0.05 (cf Fig. <ref type="figure" target="#fig_12">15b</ref>), the truncated dynamics have deviated far from the truth. The Gray-Box model corrects the deviation in both cases and accurately captures the ground truth with the addition of post-processing terms, as seen in Fig. <ref type="figure" target="#fig_12">15e</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Using POD coefficients to parametrize the IM/AIM</head><p>Here, the implementation of the proposed workflow is presented in the case where the manifold is parametrized by data-driven POD coefficients, rather than sine coefficients, for the Chafee-Infante equation. To start with, the POD modes that contain the greatest percentage of variance of an ensemble of solutions in physical space, are identified. Three POD modes represent 99.99% of the energy of the dataset (cf. Fig. <ref type="figure" target="#fig_5">16a</ref>), defined as the percentage of the cumulative sum of the leading three eigenvalues over the sum of all the eigenvalues.</p><p>The original dataset is then projected on the first three modes, leading to each solution vector, being represented by three coefficients. The mean absolute percentage error of the dataset, projected on a basis consisting of 3 POD vectors, is 0.06% (cf. Fig. <ref type="figure" target="#fig_5">16b</ref>). We use this collection of POD coefficients, to discover the latent variables, here with an autoencoder with a 2-neuron bottleneck layer. The mean absolute percentage error achieved for the autoencoderreconstructed POD coefficients is 1.2%. The latent variables are one-to-one with the two leading POD coefficients, as is evident in Fig. <ref type="figure" target="#fig_14">17</ref>, where they are plotted and coloured according to the values of the coefficients. The smooth colour transition is indicative of the one-to-one relationship.</p><p>The time-evolution law of the ODE for the two leading POD coefficients, is then learned from data. This is achieved using a feed forward neural network consisting of two hidden layers with 20 neurons each. The tanh activation function is implemented and the mean squared error is used as a loss function. The learned ODE is integrated with a Runge-Kutta solver over time T = 5. From the values of the two POD coefficients at the final time-step, the latent variables are then inferred using an appropriately trained neural network. Then, the decoder of the autoencoder is used to recover the entire set of POD coefficients. It is then possible to "lift" from POD space to the sine coefficients and reconstruct the solution: the solution reconstructed using 3 ML-derived terms compares very favorably to the ground   Figure <ref type="figure" target="#fig_0">19</ref>: (a) A data set sampled from the singularly perturbed system of ODEs is shown with a black solid line. The span of the first POD mode (P OD 1 ) is shown with a red vector and the span of the second POD mode (P OD 2 ) is shown with a blue vector. The projection of a data point (black solid circle) to P OD 1 and P OD 2 is depicted. (b) The components of the first POD vector (P OD 1 ) versus the components of the second POD vector (P OD 2 ). P OD 2 can be seen as a quadratic function of P OD 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion-Conclusions</head><p>In conclusion, this study has attempted to bridge theoretical approaches to reduced order modeling of dynamical systems (theoretically, closed form, approximations of AIMs and AIFs) with appropriately derived data-driven workflows. The data in question may consist of either (a) theoretical parametrizations of the IM (here sine coefficients) or (b) equally possibly, data-driven parametrizations (POD coefficients, autoencoder latent variables, manifold learning Diffusion Map coordinates). The use of machine learning techniques, specifically autoencoders and Diffusion Maps, allows for accurate and efficient modeling of high-dimensional systems while overcoming the limitations of traditional post-processing Galerkin methods.</p><p>Moreover, the proposed approach has demonstrated promising results in scenarios where the low-dimensional ROM significantly deviates from the correct long-term dynamics, which was previously challenging to address with postprocessing Galerkin techniques. The introduction of a "Gray-Box" model that adds a correction to the truncated Galerkin helps it regain its accuracy; it then allows for post-processing steps to recover even higher levels of accuracy, in ambient space.</p><p>At this point, it is worth discussing how noise affects the robustness of the different methods; regarding the DMapsbased approaches, the question of noisy data has been addressed in detail in the paper by <ref type="bibr">Coifman and Lafon Coifman and Lafon [2006b]</ref>: the DMaps parametrization will be robust to output noise as long as the scale parameter ‚àö œµ, in the denominator of equation ( <ref type="formula">25</ref>) in the Appendix, remains larger than the amplitude of the noise. Therefore, as long as the scale parameter is larger than the noise (and the noise is not large enough to distort the manifold) we expect the embedding to remain consistent. Along these lines, a more rigorous means of selection of this parameter was recently introduced in order to handle noisy data, based on a semigroup property for parameter tuning <ref type="bibr" target="#b51">Shan and Daubechies [2022]</ref>. Autoencoders, have been used for denoising data; therefore, we expect them capable of handling data with small amplitudes of noise and act as filters to them <ref type="bibr" target="#b60">Vincent et al. [2008]</ref>.</p><p>For the Post-processing Galerkin, we assume that noise affects the solution in the same way that error in the truncated Galerkin. An error of the order ||e m || in the computed Galerkin method yields an error of the O(m -Œ±2 ) + ||e m ||, where O(m -Œ±2 ) is the error between the exact solution of the system and the post-processing Galerkin based on the exactly computed solution of the Galerkin system, u m . Therefore one has to require ||e m || = O(m -Œ±2 ) in order to conclude robustness of the post-Processing Galerkin method with respect to errors commited in the computed Galerkin method. Exploiting this analysis we can also conclude that, by analogy, if we have noise of size smaller or equal to O(m -Œ±2 ) on the results of the truncated Galerkin we will also retain the robustness of the post-processing Galerkin (and our approach). A detailed analysis along these lines in presented in section A.0.2.</p><p>Overall, this work contributes to the growing body of literature on data-driven reduced order modeling techniques for dynamical systems and provides a valuable alternative to traditional post-processing Galerkin methods. The proposed workflows have the potential to significantly improve the accuracy and efficiency of reduced order models, which has important implications for a wide range of applications, including but not limited to, aerospace engineering, biomedical engineering, and climate modeling.</p><p>A promising future direction of our current work for the construction of reduced-order models is the combination of data-driven techniques with physics-based techniques. The work of R. <ref type="bibr" target="#b25">Geelen et al. Geelen et al. [2023]</ref>, in which the parameterization of the data is achieved by combining linear subspaces -spanned by the first few POD vectors -and quadratic components, is the most pertinent to this direction. One could express the dynamics in terms of the first few POD vectors and use the quadratic correction only as a post-processing step to obtain a more accurate reconstruction at the end of the integration. The ability to find a quadratic correction could provide improved explainability to the post-processing step, that we lose by learning a black-box post-processing step in our current work. A visualizable example is shown in Figure <ref type="figure" target="#fig_0">19</ref> where the 2-dimensional singularly perturbed system ( ·∫ã = 2 -x -y; ·∫è = 1/œµ(x -y)) was used to sample data. For this example one could write the dynamics in terms of P OD 1 and express the correction from P OD 2 = f (P OD 1 ) through a quadratic correction since P OD 2 can be seen as a quadratic function of P OD 1 (Figure <ref type="figure" target="#fig_0">19(b)</ref>).</p><p>The term -9Œ± 3 ŒΩ of the right-hand side of Equation ( <ref type="formula">18</ref>) corresponds to the diffusion term and all the other terms of the right-hand side to the reaction terms. We take an implicit Euler step of Equation ( <ref type="formula">18</ref>) of length œÑ by using as initial condition Œ± 3 (t = 0) = 0. This gives us the expression</p><formula xml:id="formula_20">Œ± 3 (œÑ ) = Œ± 3 (0) + œÑ »ß3<label>(19)</label></formula><p>By moving the diffusive term to the left-hand side and solving in terms of Œ± 3 we get the expression</p><formula xml:id="formula_21">Œ± 3 = œÑ (1 + 9œÑ ŒΩ) a 3 + a 3 1 4 - 3 2 a 3 3 - 3 2 a 2 2 a 3 - 3 2 a 2 1 a 3 - 3 4 a 1 a 2 2 . (<label>20</label></formula><formula xml:id="formula_22">)</formula><p>We then perform one fixed point iteration by considering a 3 = 0 and œÑ = 1. This leads to the Euler-Galerkin approximation</p><formula xml:id="formula_23">Œ± 3 = 1 4(1 + 9ŒΩ) Œ± 3 1 -Œ± 1 Œ± 2 2 .<label>(21)</label></formula><p>In our case, the Euler-Galerkin approximation in Equation ( <ref type="formula" target="#formula_23">21</ref>) was used as one of the post-processing schemes to correct the solution of √ª(x, T ) computed from the truncated dynamics.</p><p>A.1 Effect of error/noise in the Galerkin solution to the robustness of the ML-driven post-processing</p><p>For the post-processing Galerkin, we assume we assume that noise affects the solution in the same way that error in the truncated Galerkin solution does . For the sake of simplicity we outline the response in the context of spectral Galerkin method in the space H m = span &lt; w 1 , w 2 , ..., w m &gt;; where &lt; w j &gt; form a basis of the phase space of solutions , say H.</p><p>Denote by P m : H ‚Üí H m the orthogonal projection, and by u m ‚àà H m the solution of the m-truncated Galerkin method. The post-processing Galerkin method, capitalizes on the fact that</p><formula xml:id="formula_24">sup 0‚â§t‚â§T ||u(t) -u m (t)|| = O(m -Œ±1 ) ‚â´ sup 0‚â§t‚â§T ||P m u(t) -u m (t)|| = O(m -Œ±2 ), 0 &lt; Œ± 1 &lt; Œ± 2 .</formula><p>Let q = Œ¶ m (p), for p ‚àà H m , be the relevant approximate inertial manifold used in the post processing step. Observe that Œ¶ m is a m-dimensional Lipschiz manifold with a Lipschitz constant K m = O(m -Œ±3 ) for some Œ± 3 &gt; 0.</p><p>Let the actual numerically computed solution of the m-truncated Galerkin be v m = u m + e m , where u m is the exact solution of the m-truncated Galerkin system with error e m .</p><p>By using the Lipsichtz property of Œ¶ m (p) one has</p><formula xml:id="formula_25">||Œ¶ m (v m ) -Œ¶ m (u m )|| ‚â§ K m ||e m ||.</formula><p>Consequently the total error commited in the post-processing step is</p><formula xml:id="formula_26">||(v m + Œ¶ m (v m )) -(u m + Œ¶ m (u m ))|| ‚â§ ||e m ||(1 + K m ).</formula><p>However, using the fundamental approximation property of AIM, namely that</p><formula xml:id="formula_27">sup 0‚â§t‚â§T ||(I -P m )u(t) -Œ¶ m (P m u)|| = O(m -Œ±2 ),</formula><p>together with the above, implies the following total error in the post-processing Galerkin method:</p><formula xml:id="formula_28">||u -(v m + Œ¶ m (v m ))|| = ||P m u + (I -P m )u -(v m + Œ¶ m (v m ))||.<label>(22)</label></formula><p>If we add and subtract in Equation ( <ref type="formula" target="#formula_28">22</ref>) u m , Œ¶ m (P m u), Œ¶(u m ) and use the triangle inequality we get ) in order to conclude robustness of the Post Processing Galerkin method with respect to errors commited in the numerically computed Galerkin method. Exploiting this analysis we can also conclude that, by analogy that if we have noise of size smaller or equal to O(m -Œ±2 ) on the results of the truncated Galerkin we will also retain the robustness of the Post Processing Galerkin (and our approach)</p><formula xml:id="formula_29">||P m u + (I -P m )u -(v m + Œ¶ m (v m )) + u m -u m + Œ¶ m (P m u) -Œ¶ m (P m u) + Œ¶(u m ) -Œ¶(u m )</formula><p>A.2 Convergence in the presence and absense of time-scale separation.</p><p>Dissipative evolution equations are known to possess an absorbing ball in the phase space, whose radius depends on the physical parameters of the particular equation.</p><p>An absorbing ball is a ball that is invariant under the dynamics and attracts all the solutions with initial data outside the absorbing ball.</p><p>Notably, for most dissipative evolution equations that arise in physical applications, the absorbing ball attracts solutions starting outside of it exponentially fast with a rate that depends on the physical parameters of the particular equation.</p><p>Moreover, once a solution enters into the absorbing ball it converges toward the global attractor at rates whose speed (fast or slow) cannot be determined in advance.</p><p>In the presence of separation of time-scales:</p><p>It is worth mentioning that, in the presence of large enough gap (separation of time scales), say of O(m Œ± ), for some Œ± &gt; 0, for m large enough positive integer, between the backward dynamics of the m resolved modes and the forward dynamics of the unresolved, faster ones, one should be able to show the existence of a Lipschitz m-dimensional globally invariant manifold. In addition, such a manifold will attract every solution, with initial data inside the absorbing ball, exponentially fast with a rate of O(m Œ± ).</p><p>Such a manifold is called an Inertial Manifold (IM), and in particular it contains the global attractor.</p><p>Observe that the existence of IM relies on the separation of time-scales; however, global attractors exist regardless of the presence or absence of separation of time-scales.</p><p>In the absence of separation of time-scales:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Geometric Harmonics and Double Diffusion Maps</head><p>Geometric Harmonics Coifman and Lafon [2006a] is a regression scheme traditionally applied on a data set X to extend a function f . Extending means that we are able to evaluate the function f for points "outside" of X, for x new / ‚àà X.</p><p>In our previous work <ref type="bibr" target="#b16">Evangelou et al. [2022]</ref> we introduced a special case of Geometric Harmonics, termed Double Diffusion Maps (Double DMAPs), able to regress functions directly on the reduced Diffusion Maps coordinates Œ¶. In this case, similar to the first round of Diffusion Maps, an affinity matrix is computed</p><formula xml:id="formula_30">A * ij = exp -||œï i -œï j || 2 2 2Œµ * . (<label>29</label></formula><formula xml:id="formula_31">)</formula><p>An eigendecomposition of the symmetric and positive semidefinite matrix A * is then computed. From this eigendecomposition we obtain a set of orthonormal eigenvectors Œ® = {œà 0 , . . . , œà N -1 } ranked with their non-negative eigenvalues œÉ = {œÉ 0 , . . . , œÉ N -1 }. A set of those eigenvalues S Œ¥ = {i : œÉ i &gt; Œ¥œÉ 0 }, where Œ¥ &gt; 0, is considered as the basis in which we project and subsequently extend the function f . The projection of f in this truncated step is given as</p><formula xml:id="formula_32">f ‚Üí P Œ¥ f = i‚ààŒ£ Œ¥ ‚ü®f, œà i ‚ü©œà i (<label>30</label></formula><formula xml:id="formula_33">)</formula><p>where ‚ü®‚Ä¢, ‚Ä¢‚ü© denotes the inner product. For œï new / ‚àà Œ¶ we obtain (Ef )(œï new ) by firstly extending each eigenvector</p><formula xml:id="formula_34">œà i ‚àà Œ®, Œ® i (œï new ) = œÉ -1 i m j=1 A(œï new , œï j )œà i (œï j ),<label>(31)</label></formula><p>where œÉ i is the i th eigenvalue and œà i (œï j ) is the j th component of the eigenvector œà i . The extended eigenvectors can then used to estimate (Ef )(œï new ) as,</p><formula xml:id="formula_35">(Ef )(œï new ) = i‚ààS Œ¥ ‚ü®f, œà i ‚ü©Œ® i (œï new ).<label>(32)</label></formula><p>A.5 Inverse Function Theorem</p><p>Consider the vector function F (x) = y and assume that x ‚àà R n is a solution of F and that F : R n ‚Üí R n is differentiable . The Inverse Function Theorem <ref type="bibr" target="#b44">Marsden et al. [1993]</ref> states that, if the Jacobian matrix  is invertible, then in a neighborhood of x and y the function f -1 exists. This suggests a unique local solution close to any y. The Jacobian matrix is invertible if and only if its determinant is nonzero, therefore, showing that the det(J f (x)) has values of a single sign guarantees that the mapping is locally invertible and thus one-to-one.  </p><formula xml:id="formula_36">J f (x) = Ô£´ Ô£¨ Ô£≠ ‚àÇf1 ‚àÇx1</formula></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Flowchart of the proposed workflow</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: An example of a feed-forward neural network architecture for the approximation of the right-hand-side of an evolution ODE.</figDesc><graphic coords="7,270.54,359.70,65.31,98.27" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Learning a low dimensional embedding of data: (a) Manifold learning with Diffusion Maps and inverse transformation with Double Diffusion Maps [Evangelou et al., 2022]; (b) Representative autoencoder structure, including encoder/decoder and the bottleneck layer.</figDesc><graphic coords="8,360.12,162.33,170.59,148.67" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Schematic representation of computational workflow: First step includes learning the a minimum representation of the Approximate Inertial Manifold either with DMAPs or an autoencoder, as well as the inverse transformation, i.e. from the latent variables to the sine coefficients. Secondly, the latent variables are learned as a function of the leading three sines or POD coefficients, and finally, the full coefficients are predicted either with the decoder or Double DMAPS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure6: (a) A schematic illustrating the benefits of the post-processing Galerkin methodology. A trajectory of the exact solution is shown on the manifold in a 1 , a 2 , a 3 as a red solid trajectory, its final state denoted with an x marker and (1). The projection of the exact solution in a 1 , a 2 is shown with a red dashed line, its final state denoted as (3), and a blue square. The trajectory integrated by using the approximate inertial form is shown with a black dashed line, and its final state is shown with a black square and denoted as (4). The trajectory integrated by using the truncated Galerkin is shown with a blue dashed line, its final state denoted as (3) and a blue square, and its post-processing (mapping) on the manifold, denoted as (2), and indicated by a blue x marker. The dotted line ‚àÜ 1 shows the distance between (1) and (5), the dotted line ‚àÜ 2 shows the distance between (2) and (3), the dotted line ‚àÜ 3 shows the distance between (1) and (4). The main premise of post-processing Galerkin is that ‚àÜ 1 and ‚àÜ 4 are much smaller than ‚àÜ 2 . (b) The same components used in (a) are shown for the Chafee-Infante PDE. (c) The reconstructed solution in u(x, T ) for all possible options. (d) A blow-up of the reconstruction in u(x, T ).</figDesc><graphic coords="10,118.80,186.84,374.42,332.24" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Solution of Chafee Infante reconstructed in physical space; (a) The result of the 3D and the 2D Galerkin are shown in red and dashed black line respectively; (b) Relative error of reconstructed 2D Galerkin solution at each time-step; (c) Comparison of 3D Galerkin (red line) to the 2D Galerkin corrected with the neural network-derived term (dashed blue line); and the 2D Galerkin corrected with the theoretically derived Œ± 3 ; (d) Relative error of reconstructed solution of the 2D ODE, corrected with the ML-derived Œ± 3 , at each time-step; (e) Histogram of the mean absolute percentage error of the 2D and ML-corrected 2D model at time T = 5, for 100 randomly selected initial conditions.</figDesc><graphic coords="12,212.40,340.58,187.20,131.49" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :Figure 9 :</head><label>89</label><figDesc>Figure 8: Latent variables of the autoencoder bottleneck layer; The three latent variables colored by the value of the first three sine coefficients, Œ± 1 (left), Œ± 2 (center) and Œ± 3 (right). Smoothness in color gradation suggests a one-to-one relation.</figDesc><graphic coords="13,72.00,72.00,468.00,159.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>Figure 10: (a) A visual comparison for one data point between the true solution, the solution obtained after solving the inverse problem, and the truncated solution obtained by using the first three leading sine coefficients. (b) The mean absolute percentage error (MAPE) across all the test points between the true solution and (i) the solution based on the leading sine coefficients (red histogram) (ii) the solution obtained after solving the inverse problem with optimization (blue histogram).</figDesc><graphic coords="14,94.16,317.54,210.60,146.69" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Left: Comparison of time evolution of the first three sine coefficients of the Galerkin discretization; 8dimensional (red), learned 3-dimensional (black) the 3-dimensional (blue) Galerkin discretization</figDesc><graphic coords="15,72.00,128.70,468.00,120.90" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: Comparison of the solution, in physical space, in two time instances (a) T 1 , where the 3D and 8D dynamics are sufficiently close and (b) T 2 , when they are quite far apart; (c) Relative error between the truncated and the learned 3D AIF dynamics; (d) Comparison of time evolution of the first three sine coefficients of the Galerkin discretization : 8-dimensional (solid line), learned 3-dimensional (broken line) the 3-dimensional AIF (dotted line) Galerkin discretization</figDesc><graphic coords="16,318.94,232.59,163.80,150.62" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>Figure 14: (a) Comparison, at Œ±=33 and T = 0.5 of the reconstructed Kuramoto-Sivashinsky solution, between the 8-dimensional Galerkin (solid red line) and the 3-dimensional learned AIF ODE corrected with the decoder-derived higher harmonics terms (broken blue line); (b) Relative error over the time integration interval. (c) Comparison, at Œ±=33 and T = 0.5 of the reconstructed Kuramoto-Sivashinsky solution, between the 8-dimensional Galerkin (solid red line) and the 3-dimensional learned AIF ODE corrected with the DMAPs-derived higher harmonics terms (broken blue line); (d) Relative error along the time-span of integration. (e) Histograms of MAPE of the autoencoder-corrected and DMAPs-corrected solution at time T =0.5, for 100 randomly selected initial conditions.</figDesc><graphic coords="17,200.70,368.14,210.60,140.83" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 15 :</head><label>15</label><figDesc>Figure 15: Gray-Box correction of 3D Galerkin dynamics: Comparison of the solution, in physical space, in two time instances (a) T 1 = 0.02, where the 3D and the 8D Galerkin dynamics are sufficiently close and (b) T 2 = 0.05, when they are quite far apart; (c) Relative error between the truncated and the Gray-Box 3D dynamics; (d) Comparison of time evolution of the first three sine coefficients of the Galerkin discretization : 8-dimensional (solid red line), Gray-Box 3-dimensional (broken blue line) truncated 3-dimensional (blue solid line) Galerkin discretization (e) Comparison of the solution, in physical space, of the corrected Gray-Box with the 8D Galerkin at T 2 = 0.05</figDesc><graphic coords="18,200.70,425.94,210.60,139.07" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head></head><label></label><figDesc>Figure 16: (a) % energy of the data contained by progressively increasing POD basis size. 3 POD modes represent 99.99 % of the variance (b) MAPE of the dataset projected on progressively increasing POD basis, with respect to the original. When condidering 3 POD modes, the MAPE drops to 0.06%.</figDesc><graphic coords="19,72.00,285.05,468.00,146.25" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 17 :</head><label>17</label><figDesc>Figure 17: Latent variables discovered by the autoencoder, whose inputs are POD coefficients values. The smooth color transition implies that the latent variables are 1-to-1 with the leading POD coefficients.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 18 :</head><label>18</label><figDesc>Figure 18: Reconstructed solution of the Chafee-Infante equation, at time T = 5. The red line represents the ground truth, 8D Galerkin solution, which corresponds to 3 POD coefficients. The black broken line corresponds to the datadriven post-processed solution of the evolution of 2 POD coefficients. The uncorrected solution derived by 2 POD coefficients, is also depicted with a blue broken line.</figDesc><graphic coords="19,189.00,515.72,234.00,145.81" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>|| ‚â§ ||(P m u -u m )|| + ||u m -v m || + ||(I -P m )u -Œ¶ m (P m u)|| + ||Œ¶ m (P m u) -Œ¶ m (u m )|| + ||Œ¶ m (u m ) -Œ¶ m (v m )|| = O(m -Œ±2 ) + ||e m || + O(m -Œ±2 ) + K m ||P m u -u m || + K m ||e m ||. (23) Therefore, ||u -(v m + Œ¶ m (v m )|| ‚â§ O(m -Œ±2 ) + ||e m || + O(m -(Œ±2+Œ±3) ).(24) Conclusion: An error of the order ||e m || in the computed Galerkin method yields an error of the O(m -Œ±2 ) + ||e m ||, where O(m -Œ±2 ) is the error between the exact solution of the system and the post processing Galerkin based on the exactly computed solution of the m-truncated Galerkin system, u m . Therefore one has to require ||e m || = O(m -Œ±2</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head>Figure 20 :Figure 21 :</head><label>2021</label><figDesc>Figure 20: Diffusion Maps coordinates the parametrize the latent space: œï 1 , œï 2 and œï 3 ; Left: colored by sine coefficient Œ± 1 , center: colored by sine coefficient Œ± 2 , right: colored by sine coefficient Œ± 3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_18"><head>Figure 22 :</head><label>22</label><figDesc>Figure 22: Performance of the neural network predicting the right-hand-side of the learned ODE of the first three coefficients. Actual versus learned Œ±1 (left), Œ±2 (center) and Œ±3 (right), from the autoencoder (top) and Double DMAPs (bottom) derived values of sine coefficients.</figDesc><graphic coords="31,140.92,286.30,330.16,179.70" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="30,96.90,232.47,418.20,298.25" type="bitmap" /></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head n="6">Acknowledgements</head><p>The motivation for this work comes in part from initial efforts on reduced modeling of multiphase flows, as part of CML's Thesis. I.G.K. acknowledges partial support from the <rs type="funder">US AFOSR</rs> <rs type="grantNumber">FA9550-21-0317</rs> and the <rs type="funder">US Department of Energy</rs> <rs type="grantNumber">SA22-0052-S001</rs>. E.D.K. was funded by the <rs type="funder">Luxembourg National Research Fund (FNR)</rs>, grant reference <rs type="grantNumber">16758846</rs>. For the purpose of open access, EDK has applied for a Creative Commons Attribution 4.0 <rs type="funder">International</rs> (<rs type="grantNumber">CC BY 4.0</rs>) license to any Author Accepted Manuscript version arising from this submission. C.M.L. received the support of a <rs type="grantName">"la Caixa" Foundation Fellowship (ID 100010434</rs>), code <rs type="grantNumber">LCF/BQ/AA19/11720048</rs>. The research of E.S.T. was made possible by NPRP grant #<rs type="grantNumber">S-0207-200290</rs> from the <rs type="funder">Qatar National Research Fund</rs> (a member of <rs type="funder">Qatar Foundation</rs>), and is based upon work supported by <rs type="funder">King Abdullah University of Science and Technology (KAUST) Office of Sponsored Research (OSR)</rs> under Award No. <rs type="grantNumber">OSR-2020-CRG9-4336</rs>. The work of E.S.T. has also benefited from the inspiring environment of the <rs type="funder">CRC</rs> <rs type="grantNumber">1114</rs> "<rs type="projectName">Scaling Cascades in Complex Systems</rs>", Project Number <rs type="grantNumber">235221301</rs>, Project <rs type="grantNumber">A02</rs>, funded by <rs type="funder">Deutsche Forschungsgemeinschaft (DFG)</rs>. For the purpose of open access, E.S.T. has applied a Creative Commons Attribution (CC BY) licence to any Author Accepted Manuscript version arising from this submission.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_WQU9KfD">
					<idno type="grant-number">FA9550-21-0317</idno>
				</org>
				<org type="funding" xml:id="_ah56wsK">
					<idno type="grant-number">SA22-0052-S001</idno>
				</org>
				<org type="funding" xml:id="_QdzQxT4">
					<idno type="grant-number">16758846</idno>
				</org>
				<org type="funding" xml:id="_NMsNXDe">
					<idno type="grant-number">CC BY 4.0</idno>
					<orgName type="grant-name">&quot;la Caixa&quot; Foundation Fellowship (ID 100010434</orgName>
				</org>
				<org type="funding" xml:id="_MT8GPns">
					<idno type="grant-number">LCF/BQ/AA19/11720048</idno>
				</org>
				<org type="funding" xml:id="_xPgr7TH">
					<idno type="grant-number">S-0207-200290</idno>
				</org>
				<org type="funding" xml:id="_XCTyChu">
					<idno type="grant-number">OSR-2020-CRG9-4336</idno>
				</org>
				<org type="funded-project" xml:id="_dSKWr6k">
					<idno type="grant-number">1114</idno>
					<orgName type="project" subtype="full">Scaling Cascades in Complex Systems</orgName>
				</org>
				<org type="funding" xml:id="_MykQW4z">
					<idno type="grant-number">235221301</idno>
				</org>
				<org type="funding" xml:id="_DKQuMZK">
					<idno type="grant-number">A02</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Appendix</head><p>A.0.1 Post-processing Galerkin for the finite element Method Let X be the phase space of a nonlinear dissipative evolution equation of the form du dt + ŒΩAu = F (u).</p><p>Let X H be a finite dimensional (e.g. finite element) space of spatial scale H with P H : X ‚Üí X H an orthogonal projection. The Galerkin approximate solution u H ‚àà X H satisfies the equation</p><p>Therefore, for a given Galerkin solution u H and its time-derivative du H dt over the interval [0, T ], the post-processing Galekin solution is a function v ‚àà X (notice it is not in the complement of X H , i.e. not in X ‚äñ X H , but in X, so v involves both coarse as well as fine spatial scales), such that v satisfies</p><p>The right-hand side is a given function at time t = T , and v solves a linear elliptic equation. However, in practice we solve an approximation of v say ·πΩ ‚àà X h , where h ‚â™ H, and X h is a finer finite element space</p><p>A.0.2 Euler-Galerkin algorithm applied to Chafee-Infante PDE</p><p>The implementation of the Euler-Galerkin algorithm described in Sec. 3.1.1 is shown here for the Chafee-Infante reaction-diffusion equation. For this PDE, as discussed in Sec. 4.1, a two-dimensional inertial manifold exists (n = 2) parameterized by the first two sine Fourier modes Œ± 1 , Œ± 2 . By using the Galerkin projection u(x, t) ‚âà m=3 i=1 a i (t)sin(ix) a system of three coupled ordinary differential equations is derived. The derived system of equations reads</p><p>As we stated above, the dissipative evolution equation will still possess a global attractor. Even though in this case we are unable to show the existence of an invariant IM that contains the global attractor, we are able to construct a k-dimensional Lipschitz manifold, that is still called an AIM. At the moment k is an arbitrary positive integer.</p><p>In fact, one can construct the AIM M 0 = graph(Œ¶ 0 ), where Œ¶ 0 is a Lipschitz function from the k resolved modes to the complement.</p><p>Properties of the AIM:</p><p>‚Ä¢ For k large enough, depending on the physical parameters, one can show that, even though the global attractor is not contained in the AIM M 0 it is contained in a thin layer of thickness of the order O(k -Œ≤ ), for some Œ≤ &gt; 0, about M 0 . ‚Ä¢ For every initial data inside the absorbing ball the corresponding solution of the dissipative evolution equation converges exponentially fast to the above mentioned thin layer about the graph of the AIM M 0 , with a rate of the order O(k Œ≥ ), for some Œ≥ &gt; 0.</p><p>Summary: All solutions outside the absorbing ball converge exponentially fast to the absorbing ball at a rate that depends on the physical parameters. All solutions starting inside the absorbing ball converge to a thin layer about the AIM M 0 , whose thickness is of the order O(k -Œ≤ ), for some Œ≤ &gt; 0 (k is the dimension of M 0 ). Furthermore the convergence is exponentially fast with rate of the order O(k Œ≥ ), for some Œ≥ &gt; 0.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 Diffusion Maps</head><p>The Diffusion Maps algorithm reveals the intrinsic geometry of a data set X = {x i } N i=1 , where each data point x i ‚àà R m , by constructing a random walk on X. This random walk is constructed by means of an affinity matrix A with entries computed in terms of a kernel</p><p>where Œµ is a positive hyperparameter that specifies the rate of decay of the kernel (kernel bandwidth). The Gaussian kernel, Equation (25), is typically chosen for the construction of the affinity matrix A.</p><p>To obtain a random walk (parametrization) of X regardless of the sampling density the normalization</p><p>is applied, with Œ± = 1 to factor out the density effects.</p><p>A second normalization,</p><p>is applied to recover the row-stochastic matrix K. The eigendecomposition of K,</p><p>gives a set of eigenvectors œï and eigenvalues Œª. Proper selection of the eigenvectors that parameterize independent directions, (known as non-harmonics) is needed. This selection in practice can be achieved by using the local linear regression algorithm proposed in <ref type="bibr" target="#b15">Dsilva et al. [2018]</ref>. If the number of non-harmonic eigenvectors is smaller than the original dimension √± &lt; n then those eigenvectors Œ¶ = {œï 1 , . . . , œï √±} can provide a more parsimonious representation of the original data and thus to obtain dimensionality reduction.</p><p>It is therefore important to discover which eigenvectors parametrize independent directions, and do not span the same direction with different frequencies (harmonics).</p><p>To achieve this, the local linear regression algorithm, proposed in <ref type="bibr" target="#b15">Dsilva et al. [2018]</ref> is used, according to which, each DMAP coordinate is fitted as a function of the previous ones. To select the DMAP coordinates that are independent, the "goodness of fit" of this functions is used: A good fit is associated with a œï k that is a harmonic function of the previous eigenmodes, whereas a bad fit signifies that œï k is a new independent direction on the data manifold.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">TensorFlow: Large-scale machine learning on heterogeneous systems</title>
		<author>
			<persName><forename type="first">Mart√≠n</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Craig</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Manjunath</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josh</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dandelion</forename><surname>Man√©</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rajat</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sherry</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Derek</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kunal</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vijay</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernanda</forename><surname>Vi√©gas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pete</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoqiang</forename><surname>Zheng</surname></persName>
		</author>
		<ptr target="https://www.tensorflow.org/.Softwareavailablefromtensorflow.org" />
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Construction of approximate inertial manifold by decimation of collocation equations of distributed parameter systems</title>
		<author>
			<persName><surname>Adrover</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Continillo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Crescitelli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Giona</surname></persName>
		</author>
		<author>
			<persName><surname>Russo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computers &amp; chemical engineering</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="113" to="123" />
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A priori analysis of reduced description of dynamical systems using approximate inertial manifolds</title>
		<author>
			<persName><forename type="first">Maryam</forename><surname>Akram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Malik</forename><surname>Hassanaly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Venkat</forename><surname>Raman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational Physics</title>
		<imprint>
			<biblScope unit="volume">409</biblScope>
			<biblScope unit="page">109344</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Wave formation on vertical falling liquid films</title>
		<author>
			<persName><surname>Sv Alekseenko</surname></persName>
		</author>
		<author>
			<persName><surname>Ve Nakoryakov</surname></persName>
		</author>
		<author>
			<persName><surname>Pokusaev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Multiphase Flow</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="607" to="627" />
			<date type="published" when="1985">1985</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Improved surrogates in inertial confinement fusion with manifold and cycle consistencies</title>
		<author>
			<persName><forename type="first">Rushil</forename><surname>Anirudh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Jayaraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peer-Timo</forename><surname>Thiagarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brian</forename><forename type="middle">K</forename><surname>Bremer</surname></persName>
		</author>
		<author>
			<persName><surname>Spears</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page" from="9741" to="9746" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning data-driven discretizations for partial differential equations</title>
		<author>
			<persName><forename type="first">Yohai</forename><surname>Bar-Sinai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Hoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Hickey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">P</forename><surname>Brenner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Academy of Sciences</title>
		<meeting>the National Academy of Sciences</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="page" from="15344" to="15349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A survey of projection-based model reduction methods for parametric dynamical systems</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Benner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serkan</forename><surname>Gugercin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Willcox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM review</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="483" to="531" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Discovering governing equations from data by sparse identification of nonlinear dynamical systems</title>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">L</forename><surname>Steven L Brunton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathan</forename><surname>Proctor</surname></persName>
		</author>
		<author>
			<persName><surname>Kutz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">113</biblScope>
			<biblScope unit="issue">15</biblScope>
			<biblScope unit="page" from="3932" to="3937" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Nonlinear waves on liquid film surfaces-i. flooding in a vertical tube</title>
		<author>
			<persName><forename type="first">Hsueh-Chia</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chemical Engineering Science</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2463" to="2476" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Traveling waves on fluid interfaces: normal form analysis of the Kuramoto-Sivashinsky equation</title>
		<author>
			<persName><forename type="first">Hsueh-Chia</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Physics of Fluids</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="3142" to="3147" />
			<date type="published" when="1986">1986</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Discrete approach to stochastic parametrization and dimension reduction in nonlinear dynamics</title>
		<author>
			<persName><forename type="first">J</forename><surname>Alexandre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fei</forename><surname>Chorin</surname></persName>
		</author>
		<author>
			<persName><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="issue">32</biblScope>
			<biblScope unit="page" from="9804" to="9809" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Geometric harmonics: a novel tool for multiscale out-of-sample extension of empirical functions</title>
		<author>
			<persName><forename type="first">St√©phane</forename><surname>Ronald R Coifman</surname></persName>
		</author>
		<author>
			<persName><surname>Lafon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied and Computational Harmonic Analysis</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="31" to="52" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Diffusion maps</title>
		<author>
			<persName><forename type="first">St√©phane</forename><surname>Ronald R Coifman</surname></persName>
		</author>
		<author>
			<persName><surname>Lafon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied and computational harmonic analysis</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="30" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Diffusion maps, reduction coordinates, and low dimensional representation of stochastic systems</title>
		<author>
			<persName><surname>Ronald R Coifman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ioannis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">St√©phane</forename><surname>Kevrekidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mauro</forename><surname>Lafon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Boaz</forename><surname>Maggioni</surname></persName>
		</author>
		<author>
			<persName><surname>Nadler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multiscale Modeling &amp; Simulation</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="842" to="864" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Data-driven low-dimensional dynamic model of kolmogorov flow</title>
		<author>
			<persName><forename type="first">Carlos</forename><forename type="middle">E</forename></persName>
		</author>
		<author>
			<persName><forename type="first">P√©rez</forename><surname>De</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jes√∫s</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">D</forename><surname>Graham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review Fluids</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">44402</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Parsimonious representation of nonlinear dynamical systems through manifold learning: A chemotaxis case study</title>
		<author>
			<persName><forename type="first">J</forename><surname>Carmeline</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ronen</forename><surname>Dsilva</surname></persName>
		</author>
		<author>
			<persName><surname>Talmon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><forename type="middle">G</forename><surname>Ronald R Coifman</surname></persName>
		</author>
		<author>
			<persName><surname>Kevrekidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied and Computational Harmonic Analysis</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="759" to="773" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Double diffusion maps and their latent harmonics for scientific computations in latent space</title>
		<author>
			<persName><forename type="first">Nikolaos</forename><surname>Evangelou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Dietrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eliodoro</forename><surname>Chiavazzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Lehmberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marina</forename><surname>Meila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><forename type="middle">G</forename><surname>Kevrekidis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2204.12536</idno>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Double diffusion maps and their latent harmonics for scientific computations in latent space</title>
		<author>
			<persName><forename type="first">Nikolaos</forename><surname>Evangelou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Dietrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eliodoro</forename><surname>Chiavazzo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Lehmberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marina</forename><surname>Meila</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><forename type="middle">G</forename><surname>Kevrekidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational Physics</title>
		<imprint>
			<biblScope unit="volume">485</biblScope>
			<biblScope unit="page">112072</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">On the computation of inertial manifolds</title>
		<author>
			<persName><surname>Foias</surname></persName>
		</author>
		<author>
			<persName><surname>Jolly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">R</forename><surname>Kevrekidis</surname></persName>
		</author>
		<author>
			<persName><surname>Sell</surname></persName>
		</author>
		<author>
			<persName><surname>Titi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physics Letters A</title>
		<imprint>
			<biblScope unit="volume">131</biblScope>
			<biblScope unit="issue">7-8</biblScope>
			<biblScope unit="page" from="433" to="436" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Inertial manifolds for nonlinear evolutionary equations</title>
		<author>
			<persName><forename type="first">Ciprian</forename><surname>Foias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">R</forename><surname>Sell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><surname>Temam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of differential equations</title>
		<imprint>
			<biblScope unit="volume">73</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="309" to="353" />
			<date type="published" when="1988">1988</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Exponential tracking and approximation of inertial manifolds for dissipative nonlinear equations</title>
		<author>
			<persName><forename type="first">Ciprian</forename><surname>Foias</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">R</forename><surname>Sell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edriss</forename><forename type="middle">S</forename><surname>Titi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Dynamics and Differential Equations</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="199" to="244" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Postprocessing the Galerkin method: the finite-element case</title>
		<author>
			<persName><forename type="first">Bosco</forename><surname>Garc√≠a</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-Archilla</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Edriss</forename><forename type="middle">S</forename><surname>Titi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Numerical Analysis</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="470" to="499" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Postprocessing the Galerkin method: a novel approach to approximate inertial manifolds</title>
		<author>
			<persName><forename type="first">Bosco</forename><surname>Garc√≠a-Archilla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julia</forename><surname>Novo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edriss</forename><forename type="middle">S</forename><surname>Titi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Numerical Analysis</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="941" to="972" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">An approximate inertial manifolds approach to postprocessing the galerkin method for the navier-stokes equations</title>
		<author>
			<persName><forename type="first">Bosco</forename><surname>Garc√≠a-Archilla</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Julia</forename><surname>Novo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edriss</forename><surname>Titi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics of Computation</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="issue">227</biblScope>
			<biblScope unit="page" from="893" to="911" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Slow manifold integration on a diffusion map parameterization</title>
		<author>
			<persName><forename type="first">Charles</forename><surname>William Gear</surname></persName>
		</author>
		<author>
			<persName><surname>Kevrekidis</surname></persName>
		</author>
		<author>
			<persName><surname>Sonday</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AIP Conference Proceedings</title>
		<imprint>
			<publisher>American Institute of Physics</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">1389</biblScope>
			<biblScope unit="page" from="13" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Operator inference for non-intrusive model reduction with quadratic manifolds</title>
		<author>
			<persName><forename type="first">Rudy</forename><surname>Geelen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephen</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Willcox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Methods in Applied Mechanics and Engineering</title>
		<imprint>
			<biblScope unit="volume">403</biblScope>
			<biblScope unit="page">115717</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A fully discrete nonlinear Galerkin method for the 3D Navier-Stokes equations</title>
		<author>
			<persName><forename type="first">J-L</forename><surname>Guermond</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Serge</forename><surname>Prudhomme</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Numerical Methods for Partial Differential Equations: An International Journal</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="759" to="775" />
			<date type="published" when="2008">2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A nonlinear Galerkin method for the navier-stokes equations</title>
		<author>
			<persName><surname>Jauberteau</surname></persName>
		</author>
		<author>
			<persName><surname>Rosier</surname></persName>
		</author>
		<author>
			<persName><surname>Temam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Methods in Applied Mechanics and Engineering</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="245" to="260" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Explicit construction of an inertial manifold for a reaction diffusion equation</title>
		<author>
			<persName><forename type="first">S</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName><surname>Jolly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Differential Equations</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="220" to="261" />
			<date type="published" when="1989">1989</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Approximate inertial manifolds for the Kuramoto-Sivashinsky equation: analysis and computations</title>
		<author>
			<persName><surname>Michael S Jolly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edriss</forename><forename type="middle">S</forename><surname>Kevrekidis</surname></persName>
		</author>
		<author>
			<persName><surname>Titi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physica D: Nonlinear Phenomena</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="38" to="60" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Preserving dissipation in approximate inertial forms for the Kuramoto-Sivashinsky equation</title>
		<author>
			<persName><surname>Jolly</surname></persName>
		</author>
		<author>
			<persName><surname>Kevrekidis</surname></persName>
		</author>
		<author>
			<persName><surname>Titi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Dynamics and Differential Equations</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="179" to="197" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Nonlinear Galerkin method for low-dimensional modeling of fluid dynamic system using pod modes</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia-Zhong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sheng</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peng-Fei</forename><surname>Lei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications in Nonlinear Science and Numerical Simulation</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<biblScope unit="page" from="943" to="952" />
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Back in the saddle again: A computer assisted study of the Kuramoto-Sivashinsky equation</title>
		<author>
			<persName><forename type="first">G</forename><surname>Ioannis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Basil</forename><surname>Kevrekidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><forename type="middle">C</forename><surname>Nicolaenko</surname></persName>
		</author>
		<author>
			<persName><surname>Scovel</surname></persName>
		</author>
		<ptr target="http://www.jstor.org/stable/2101886" />
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Applied Mathematics</title>
		<idno type="ISSN">00361399</idno>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="760" to="790" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Nonlinear principal component analysis using autoassociative neural networks</title>
		<author>
			<persName><forename type="first">A</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName><surname>Kramer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AIChE journal</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="233" to="243" />
			<date type="published" when="1991">1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Model identification of a spatiotemporally varying catalytic reaction</title>
		<author>
			<persName><forename type="first">K</forename><surname>Krischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Rico-Mart√≠nez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I</forename><forename type="middle">G</forename><surname>Kevrekidis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H</forename><forename type="middle">H</forename><surname>Rotermund</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ertl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Hudson</surname></persName>
		</author>
		<idno type="DOI">10.1002/aic.690390110</idno>
		<ptr target="https://aiche.onlinelibrary.wiley.com/doi/abs/10.1002/aic.690390110" />
	</analytic>
	<monogr>
		<title level="j">AIChE Journal</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="89" to="98" />
			<date type="published" when="1993">1993</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Persistent propagation of concentration waves in dissipative media far from thermal equilibrium</title>
		<author>
			<persName><forename type="first">Yoshiki</forename><surname>Kuramoto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Toshio</forename><surname>Tsuzuki</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1976">1976</date>
			<publisher>Progress of theoretical physics</publisher>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="page" from="356" to="369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Model reduction of dynamical systems on nonlinear manifolds using deep convolutional autoencoders</title>
		<author>
			<persName><forename type="first">Kookjin</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><forename type="middle">T</forename><surname>Carlberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational Physics</title>
		<imprint>
			<biblScope unit="volume">404</biblScope>
			<biblScope unit="page">108973</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">datafold: data-driven models for point clouds and time series on manifolds</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Lehmberg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Felix</forename><surname>Dietrich</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerta</forename><surname>K√∂ster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hans-Joachim</forename><surname>Bungartz</surname></persName>
		</author>
		<idno type="DOI">10.21105/joss.02283</idno>
		<ptr target="https://doi.org/10.21105/joss.02283" />
	</analytic>
	<monogr>
		<title level="j">Journal of Open Source Software</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">51</biblScope>
			<biblScope unit="page">2283</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep learning to discover and predict dynamics on an inertial manifold</title>
		<author>
			<persName><forename type="first">Alec</forename><forename type="middle">J</forename><surname>Linot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">D</forename><surname>Graham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review E</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">62209</biblScope>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Data-driven reduced-order modeling of spatiotemporal chaos with neural ordinary differential equations</title>
		<author>
			<persName><forename type="first">Alec</forename><forename type="middle">J</forename><surname>Linot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">D</forename><surname>Graham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chaos: An Interdisciplinary Journal of Nonlinear Science</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">73110</biblScope>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Turbulence control in plane couette flow using low-dimensional neural ode-based models and deep reinforcement learning</title>
		<author>
			<persName><forename type="first">Alec</forename><forename type="middle">J</forename><surname>Linot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">D</forename><surname>Graham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Heat and Fluid Flow</title>
		<imprint>
			<biblScope unit="volume">101</biblScope>
			<biblScope unit="page">109139</biblScope>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Data-based stochastic model reduction for the Kuramoto-Sivashinsky equation</title>
		<author>
			<persName><forename type="first">Fei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><forename type="middle">K</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><forename type="middle">J</forename><surname>Chorin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physica D: Nonlinear Phenomena</title>
		<imprint>
			<biblScope unit="volume">340</biblScope>
			<biblScope unit="page" from="46" to="57" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">The postprocessing Galerkin and nonlinear Galerkin methods-a truncation analysis point of view</title>
		<author>
			<persName><forename type="first">Edriss</forename><forename type="middle">S</forename><surname>Len G Margolin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shannon</forename><surname>Titi</surname></persName>
		</author>
		<author>
			<persName><surname>Wynne</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Numerical Analysis</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="695" to="714" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Nonlinear Galerkin methods: the finite elements case</title>
		<author>
			<persName><forename type="first">Martine</forename><surname>Marion</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Temam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Numerische Mathematik</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="page" from="205" to="226" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Elementary classical analysis</title>
		<author>
			<persName><forename type="first">Jerrold</forename><forename type="middle">E</forename><surname>Marsden</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Hoffman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993">1993</date>
			<publisher>Macmillan</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<author>
			<persName><forename type="first">Yorgos</forename><forename type="middle">M</forename><surname>Cristina P Martin-Linares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Georgios</forename><surname>Psarellis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eleni</forename><forename type="middle">D</forename><surname>Karapetsas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><forename type="middle">G</forename><surname>Koronaki</surname></persName>
		</author>
		<author>
			<persName><surname>Kevrekidis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2301.12508</idno>
		<title level="m">Physics-agnostic and physics-infused machine learning for thin films flows: modeling, and predictions from small data</title>
		<imprint>
			<date type="published" when="2023">2023</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Data-driven reduced-order models via regularised operator inference for a single-injector combustion process</title>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Shane A Mcquarrie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><forename type="middle">E</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><surname>Willcox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Society of New Zealand</title>
		<imprint>
			<biblScope unit="volume">51</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="194" to="211" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Diffusion maps, spectral clustering and reaction coordinates of dynamical systems</title>
		<author>
			<persName><forename type="first">Boaz</forename><surname>Nadler</surname></persName>
		</author>
		<author>
			<persName><forename type="first">St√©phane</forename><surname>Lafon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><forename type="middle">G</forename><surname>Ronald R Coifman</surname></persName>
		</author>
		<author>
			<persName><surname>Kevrekidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied and Computational Harmonic Analysis</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="113" to="127" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Reduced operator inference for nonlinear partial differential equations</title>
		<author>
			<persName><forename type="first">Elizabeth</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ionut-Gabriel</forename><surname>Farcas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><surname>Willcox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Scientific Computing</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="1934">1934-A1959, 2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Physics-informed neural networks: A deep learning framework for solving forward and inverse problems involving nonlinear partial differential equations</title>
		<author>
			<persName><forename type="first">Maziar</forename><surname>Raissi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paris</forename><surname>Perdikaris</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Karniadakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Computational physics</title>
		<imprint>
			<biblScope unit="volume">378</biblScope>
			<biblScope unit="page" from="686" to="707" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Discrete-vs. continuous-time nonlinear signal processing of cu electrodissolution data</title>
		<author>
			<persName><forename type="first">Ramiro</forename><surname>Rico-Martinez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Krischer</surname></persName>
		</author>
		<author>
			<persName><surname>Kevrekidis</surname></persName>
		</author>
		<author>
			<persName><surname>Mc Kube</surname></persName>
		</author>
		<author>
			<persName><surname>Hudson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Chemical Engineering Communications</title>
		<imprint>
			<biblScope unit="volume">118</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="25" to="48" />
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Diffusion maps: Using the semigroup property for parameter tuning</title>
		<author>
			<persName><forename type="first">Shan</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ingrid</forename><surname>Daubechies</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Theoretical Physics, Wavelets, Analysis, Genomics: An Indisciplinary Tribute to Alex Grossmann</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="409" to="424" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Long time stability and convergence for fully discrete nonlinear Galerkin methods</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applicable Analysis</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="201" to="229" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Nonlinear model reduction for control of distributed systems: A computer-assisted study</title>
		<author>
			<persName><forename type="first">Y</forename><surname>Stanislav</surname></persName>
		</author>
		<author>
			<persName><surname>Shvartsman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">G</forename><surname>Ioannis</surname></persName>
		</author>
		<author>
			<persName><surname>Kevrekidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AIChE Journal</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1579" to="1595" />
			<date type="published" when="1998">1998</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Nonlinear analysis of hydrodynamic instability in laminar flames-i. derivation of basic equations</title>
		<author>
			<persName><forename type="first">I</forename><surname>Gregory</surname></persName>
		</author>
		<author>
			<persName><surname>Sivashinsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta astronautica</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1177" to="1206" />
			<date type="published" when="1977">1977</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<monogr>
		<title level="m" type="main">Systematic model reduction for complex systems through data mining and dimensionality reduction</title>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Sonday</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
		<respStmt>
			<orgName>Princeton University</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<monogr>
		<author>
			<persName><forename type="first">Amit</forename><surname>Benjamin E Sonday</surname></persName>
		</author>
		<author>
			<persName><forename type="first">C</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioannis</forename><forename type="middle">G</forename><surname>William Gear</surname></persName>
		</author>
		<author>
			<persName><surname>Kevrekidis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1011.5197</idno>
		<title level="m">Manifold learning techniques and model reduction applied to dissipative pdes</title>
		<imprint>
			<date type="published" when="2010">2010</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Do inertial manifolds apply to turbulence?</title>
		<author>
			<persName><surname>Temam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Roger Temam. Induced trajectories and approximate inertial manifolds. ESAIM: Mathematical Modelling and Numerical Analysis</title>
		<imprint>
			<date type="published" when="1989">1989. 1989</date>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="541" to="561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Order reduction for nonlinear dynamic models of distributed reacting systems</title>
		<author>
			<persName><surname>Theodoropoulos</surname></persName>
		</author>
		<author>
			<persName><surname>Kevrekidis</surname></persName>
		</author>
		<author>
			<persName><surname>Mountziaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Process Control</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="177" to="184" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">On approximate inertial manifolds to the navier-stokes equations</title>
		<author>
			<persName><forename type="first">S</forename><surname>Edriss</surname></persName>
		</author>
		<author>
			<persName><surname>Titi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Analysis and Applications</title>
		<imprint>
			<biblScope unit="volume">149</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="540" to="557" />
			<date type="published" when="1990">1990</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Extracting and composing robust features with denoising autoencoders</title>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre-Antoine</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
		<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1096" to="1103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<monogr>
		<title level="m" type="main">Superconvergence in Galerkin finite element methods</title>
		<author>
			<persName><forename type="first">Lars</forename><surname>Wahlbin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Autoencoders for discovering manifold dimension and coordinates in data from complex dynamical systems</title>
		<author>
			<persName><forename type="first">Anirban</forename><surname>Benjamin G Zastrow</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karen</forename><forename type="middle">E</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anthony</forename><forename type="middle">S</forename><surname>Willcox</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">C</forename><surname>Ashley</surname></persName>
		</author>
		<author>
			<persName><surname>Henson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AIAA SCITECH 2023 Forum, page 0330</title>
		<editor>
			<persName><forename type="first">Kevin</forename><surname>Zeng</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Michael</forename><forename type="middle">D</forename><surname>Graham</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2023">2023. 2023</date>
		</imprint>
	</monogr>
	<note>Data-driven model reduction via operator inference for coupled aeroelastic flutter</note>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Data-driven control of spatiotemporal chaos with reduced-order neural ode-based models and reinforcement learning</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alec</forename><forename type="middle">J</forename><surname>Linot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><forename type="middle">D</forename><surname>Graham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the Royal Society A</title>
		<imprint>
			<biblScope unit="volume">478</biblScope>
			<biblScope unit="page">20220297</biblScope>
			<date type="published" when="2022">2267. 2022</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
