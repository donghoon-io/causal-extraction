<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning to Infer User Hidden States for Online Sequential Advertising</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2020-09-03">3 Sep 2020</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Zhaoqing</forename><surname>Peng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Alibaba Group Tiejian Luo Univ. of Chinese Academy of Sciences</orgName>
								<orgName type="laboratory" key="lab1">Alibaba Group</orgName>
								<orgName type="laboratory" key="lab2">Alibaba Group</orgName>
								<orgName type="institution" key="instit1">University of Southern California Yaodong Yang</orgName>
								<orgName type="institution" key="instit2">Rui Luo University College London Jun Wang University College London Weinan Zhang Shanghai Jiao Tong University</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Alibaba Group Tiejian Luo Univ. of Chinese Academy of Sciences</orgName>
								<orgName type="laboratory" key="lab1">Alibaba Group</orgName>
								<orgName type="laboratory" key="lab2">Alibaba Group</orgName>
								<orgName type="institution" key="instit1">University of Southern California Yaodong Yang</orgName>
								<orgName type="institution" key="instit2">Rui Luo University College London Jun Wang University College London Weinan Zhang Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Junqi</forename><surname>Jin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Alibaba Group Tiejian Luo Univ. of Chinese Academy of Sciences</orgName>
								<orgName type="laboratory" key="lab1">Alibaba Group</orgName>
								<orgName type="laboratory" key="lab2">Alibaba Group</orgName>
								<orgName type="institution" key="instit1">University of Southern California Yaodong Yang</orgName>
								<orgName type="institution" key="instit2">Rui Luo University College London Jun Wang University College London Weinan Zhang Shanghai Jiao Tong University</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Alibaba Group Tiejian Luo Univ. of Chinese Academy of Sciences</orgName>
								<orgName type="laboratory" key="lab1">Alibaba Group</orgName>
								<orgName type="laboratory" key="lab2">Alibaba Group</orgName>
								<orgName type="institution" key="instit1">University of Southern California Yaodong Yang</orgName>
								<orgName type="institution" key="instit2">Rui Luo University College London Jun Wang University College London Weinan Zhang Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Lan</forename><surname>Luo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Alibaba Group Tiejian Luo Univ. of Chinese Academy of Sciences</orgName>
								<orgName type="laboratory" key="lab1">Alibaba Group</orgName>
								<orgName type="laboratory" key="lab2">Alibaba Group</orgName>
								<orgName type="institution" key="instit1">University of Southern California Yaodong Yang</orgName>
								<orgName type="institution" key="instit2">Rui Luo University College London Jun Wang University College London Weinan Zhang Shanghai Jiao Tong University</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Alibaba Group Tiejian Luo Univ. of Chinese Academy of Sciences</orgName>
								<orgName type="laboratory" key="lab1">Alibaba Group</orgName>
								<orgName type="laboratory" key="lab2">Alibaba Group</orgName>
								<orgName type="institution" key="instit1">University of Southern California Yaodong Yang</orgName>
								<orgName type="institution" key="instit2">Rui Luo University College London Jun Wang University College London Weinan Zhang Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Haiyang</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Alibaba Group Tiejian Luo Univ. of Chinese Academy of Sciences</orgName>
								<orgName type="laboratory" key="lab1">Alibaba Group</orgName>
								<orgName type="laboratory" key="lab2">Alibaba Group</orgName>
								<orgName type="institution" key="instit1">University of Southern California Yaodong Yang</orgName>
								<orgName type="institution" key="instit2">Rui Luo University College London Jun Wang University College London Weinan Zhang Shanghai Jiao Tong University</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Alibaba Group Tiejian Luo Univ. of Chinese Academy of Sciences</orgName>
								<orgName type="laboratory" key="lab1">Alibaba Group</orgName>
								<orgName type="laboratory" key="lab2">Alibaba Group</orgName>
								<orgName type="institution" key="instit1">University of Southern California Yaodong Yang</orgName>
								<orgName type="institution" key="instit2">Rui Luo University College London Jun Wang University College London Weinan Zhang Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Miao</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Alibaba Group Tiejian Luo Univ. of Chinese Academy of Sciences</orgName>
								<orgName type="laboratory" key="lab1">Alibaba Group</orgName>
								<orgName type="laboratory" key="lab2">Alibaba Group</orgName>
								<orgName type="institution" key="instit1">University of Southern California Yaodong Yang</orgName>
								<orgName type="institution" key="instit2">Rui Luo University College London Jun Wang University College London Weinan Zhang Shanghai Jiao Tong University</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Alibaba Group Tiejian Luo Univ. of Chinese Academy of Sciences</orgName>
								<orgName type="laboratory" key="lab1">Alibaba Group</orgName>
								<orgName type="laboratory" key="lab2">Alibaba Group</orgName>
								<orgName type="institution" key="instit1">University of Southern California Yaodong Yang</orgName>
								<orgName type="institution" key="instit2">Rui Luo University College London Jun Wang University College London Weinan Zhang Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Chuan</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Alibaba Group Tiejian Luo Univ. of Chinese Academy of Sciences</orgName>
								<orgName type="laboratory" key="lab1">Alibaba Group</orgName>
								<orgName type="laboratory" key="lab2">Alibaba Group</orgName>
								<orgName type="institution" key="instit1">University of Southern California Yaodong Yang</orgName>
								<orgName type="institution" key="instit2">Rui Luo University College London Jun Wang University College London Weinan Zhang Shanghai Jiao Tong University</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Alibaba Group Tiejian Luo Univ. of Chinese Academy of Sciences</orgName>
								<orgName type="laboratory" key="lab1">Alibaba Group</orgName>
								<orgName type="laboratory" key="lab2">Alibaba Group</orgName>
								<orgName type="institution" key="instit1">University of Southern California Yaodong Yang</orgName>
								<orgName type="institution" key="instit2">Rui Luo University College London Jun Wang University College London Weinan Zhang Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Han</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Alibaba Group Tiejian Luo Univ. of Chinese Academy of Sciences</orgName>
								<orgName type="laboratory" key="lab1">Alibaba Group</orgName>
								<orgName type="laboratory" key="lab2">Alibaba Group</orgName>
								<orgName type="institution" key="instit1">University of Southern California Yaodong Yang</orgName>
								<orgName type="institution" key="instit2">Rui Luo University College London Jun Wang University College London Weinan Zhang Shanghai Jiao Tong University</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Alibaba Group Tiejian Luo Univ. of Chinese Academy of Sciences</orgName>
								<orgName type="laboratory" key="lab1">Alibaba Group</orgName>
								<orgName type="laboratory" key="lab2">Alibaba Group</orgName>
								<orgName type="institution" key="instit1">University of Southern California Yaodong Yang</orgName>
								<orgName type="institution" key="instit2">Rui Luo University College London Jun Wang University College London Weinan Zhang Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jian</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Alibaba Group Tiejian Luo Univ. of Chinese Academy of Sciences</orgName>
								<orgName type="laboratory" key="lab1">Alibaba Group</orgName>
								<orgName type="laboratory" key="lab2">Alibaba Group</orgName>
								<orgName type="institution" key="instit1">University of Southern California Yaodong Yang</orgName>
								<orgName type="institution" key="instit2">Rui Luo University College London Jun Wang University College London Weinan Zhang Shanghai Jiao Tong University</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Alibaba Group Tiejian Luo Univ. of Chinese Academy of Sciences</orgName>
								<orgName type="laboratory" key="lab1">Alibaba Group</orgName>
								<orgName type="laboratory" key="lab2">Alibaba Group</orgName>
								<orgName type="institution" key="instit1">University of Southern California Yaodong Yang</orgName>
								<orgName type="institution" key="instit2">Rui Luo University College London Jun Wang University College London Weinan Zhang Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Kun</forename><surname>Gai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Alibaba Group Tiejian Luo Univ. of Chinese Academy of Sciences</orgName>
								<orgName type="laboratory" key="lab1">Alibaba Group</orgName>
								<orgName type="laboratory" key="lab2">Alibaba Group</orgName>
								<orgName type="institution" key="instit1">University of Southern California Yaodong Yang</orgName>
								<orgName type="institution" key="instit2">Rui Luo University College London Jun Wang University College London Weinan Zhang Shanghai Jiao Tong University</orgName>
							</affiliation>
							<affiliation key="aff0">
								<orgName type="department">Alibaba Group Tiejian Luo Univ. of Chinese Academy of Sciences</orgName>
								<orgName type="laboratory" key="lab1">Alibaba Group</orgName>
								<orgName type="laboratory" key="lab2">Alibaba Group</orgName>
								<orgName type="institution" key="instit1">University of Southern California Yaodong Yang</orgName>
								<orgName type="institution" key="instit2">Rui Luo University College London Jun Wang University College London Weinan Zhang Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Alibaba</forename><surname>Group</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Alibaba Group Tiejian Luo Univ. of Chinese Academy of Sciences</orgName>
								<orgName type="laboratory" key="lab1">Alibaba Group</orgName>
								<orgName type="laboratory" key="lab2">Alibaba Group</orgName>
								<orgName type="institution" key="instit1">University of Southern California Yaodong Yang</orgName>
								<orgName type="institution" key="instit2">Rui Luo University College London Jun Wang University College London Weinan Zhang Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yaodong</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Alibaba Group Tiejian Luo Univ. of Chinese Academy of Sciences</orgName>
								<orgName type="laboratory" key="lab1">Alibaba Group</orgName>
								<orgName type="laboratory" key="lab2">Alibaba Group</orgName>
								<orgName type="institution" key="instit1">University of Southern California Yaodong Yang</orgName>
								<orgName type="institution" key="instit2">Rui Luo University College London Jun Wang University College London Weinan Zhang Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Rui</forename><surname>Luo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Alibaba Group Tiejian Luo Univ. of Chinese Academy of Sciences</orgName>
								<orgName type="laboratory" key="lab1">Alibaba Group</orgName>
								<orgName type="laboratory" key="lab2">Alibaba Group</orgName>
								<orgName type="institution" key="instit1">University of Southern California Yaodong Yang</orgName>
								<orgName type="institution" key="instit2">Rui Luo University College London Jun Wang University College London Weinan Zhang Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jun</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Alibaba Group Tiejian Luo Univ. of Chinese Academy of Sciences</orgName>
								<orgName type="laboratory" key="lab1">Alibaba Group</orgName>
								<orgName type="laboratory" key="lab2">Alibaba Group</orgName>
								<orgName type="institution" key="instit1">University of Southern California Yaodong Yang</orgName>
								<orgName type="institution" key="instit2">Rui Luo University College London Jun Wang University College London Weinan Zhang Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Alibaba Group Tiejian Luo Univ. of Chinese Academy of Sciences</orgName>
								<orgName type="laboratory" key="lab1">Alibaba Group</orgName>
								<orgName type="laboratory" key="lab2">Alibaba Group</orgName>
								<orgName type="institution" key="instit1">University of Southern California Yaodong Yang</orgName>
								<orgName type="institution" key="instit2">Rui Luo University College London Jun Wang University College London Weinan Zhang Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Tiejian</forename><surname>Luo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Alibaba Group Tiejian Luo Univ. of Chinese Academy of Sciences</orgName>
								<orgName type="laboratory" key="lab1">Alibaba Group</orgName>
								<orgName type="laboratory" key="lab2">Alibaba Group</orgName>
								<orgName type="institution" key="instit1">University of Southern California Yaodong Yang</orgName>
								<orgName type="institution" key="instit2">Rui Luo University College London Jun Wang University College London Weinan Zhang Shanghai Jiao Tong University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Learning to Infer User Hidden States for Online Sequential Advertising</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2020-09-03">3 Sep 2020</date>
						</imprint>
					</monogr>
					<idno type="DOI">10.1145/3340531.3412721</idno>
					<idno type="arXiv">arXiv:2009.01453v1[cs.AI]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-28T22:11+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>Partially Observable Markov Decision Process; Online Advertising</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>To drive purchase in online advertising, it is of the advertiser's great interest to optimize the sequential advertising strategy whose performance and interpretability are both important. The lack of interpretability in existing deep reinforcement learning methods makes it not easy to understand, diagnose and further optimize the strategy. In this paper, we propose our Deep Intents Sequential Advertising (DISA) method to address these issues. The key part of interpretability is to understand a consumer's purchase intent which is, however, unobservable (called hidden states). In this paper, we model this intention as a latent variable and formulate the problem as a Partially Observable Markov Decision Process (POMDP) where the underlying intents are inferred based on the observable behaviors. Large-scale industrial offline and online experiments demonstrate our method's superior performance over several baselines. The inferred hidden states are analyzed, and the results prove the rationality of our inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CCS CONCEPTS</head><p>• Information systems → Display advertising; • Theory of computation → Sequential decision making.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Online advertising is an effective way for advertisers to reach their targeted audiences and drive conversions. Compared to a single ad exposure, sequential advertising <ref type="bibr" target="#b26">[27]</ref> has a higher chance of cultivating consumers' awareness, interest and driving purchases in several steps through multiple scenarios. Fig. <ref type="figure" target="#fig_0">1</ref> shows an example of sequential advertising on a Gaming chair in two scenarios. At time t 1 , the consumer browses and becomes aware of the chair in scenario No. 1. At time t 2 , he sees it again and shows interest by clicking it. After a while, the consumer visits scenario No. 2 and finally clicks and makes a purchase at time t 3 and t 4 . To maximize the return on investment (ROI), advertisers have a great desire to optimize sequential advertising strategies.</p><p>Advertising strategies' optimization and interpretability are both very crucial. The significance of optimization comes from its direct results of the ROI. Interpretability helps advertisers understand the strategy, provides ways to diagnose, conduct conversion attribution, and finally supports further optimization.</p><p>The advertising algorithm design for combining performance and interpretability is very challenging. The key to interpretability is modeling the consumer's mental states under a sequence of interactions with ads. However, these mental states/intents are difficult to define, and they are even unobservable. The only information related is the observed consumer's behaviors, e.g., click and purchase actions. Most interpretable algorithms tend to use shallow models such as logistic regression (non-neural network) for more convenient analysis; however, they cannot benefit from current advances of deep learning techniques <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b23">24]</ref>.</p><p>To overcome these difficulties, there are several related works. Interpretable methods like multi-touch attribution (MTA) <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b26">27]</ref> focus on assigning credits to the previously displayed ads before the conversion, but they usually do not provide future strategy optimization. Performance-oriented methods such as deep reinforcement learning (DRL) usually aggregate the consumer's historical behaviors as an input of a black-box neural network and obtain the advertising action directly from the output of the network <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b12">13]</ref>. This kind of straightforward aggregation of behaviors cannot represent and interpret the consumer's mental states well, which makes understanding, diagnosing, and optimizing the strategy difficult. Some algorithms give considerations to both interpretability and strategy optimization <ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b20">21]</ref>. Nonetheless, the majority of these methods are limited to theoretical analysis, and the experiments are conducted mostly in toy simulated environments, which are impractical for realistic industrial applications.</p><p>Considering the above challenges and shortcomings, we propose our Deep Intents Sequential Advertising (DISA) algorithm to address these issues in advertising applications. We formulate the multi-step advertising problem as a Markov Decision Process (MDP). In this MDP, the consumer intents (state) are not directly observable, so we use POMDP to model the state as a hidden variable inferred by observed behaviors. However, as a probabilistic framework, POMDP's parameters are not off-the-shelf. To tackle this issue, we derive an expectationâĂŞmaximization (EM) algorithm to estimate the parameters by learning from large-scale real-world data. The learned POMDP model can infer the probability distribution of user hidden states, defined as beliefs. Unlike noisy behavior data, beliefs are more abstract, and we can interpret how probable a user visits each hidden state and to which state it may transit. Finally, we optimize the sequential advertising strategy depending on the beliefs. Since the learning of the exact POMDP's optimum policy is intractable <ref type="bibr" target="#b20">[21]</ref>, we approximate the belief value function using a variant of Smooth Partially Observable Value Approximation (SPOVA) method <ref type="bibr" target="#b22">[23]</ref>. It is a more suitable deep architecture for POMDP than pure black-box Deep Q-Network (DQN).</p><p>Offline experiments show that our method is better than several baselines. Online results demonstrate our sequential advertising's superior performance over the existing system. In terms of interpretability, we analyze the inferred hidden states and provide examples of state transitions under different advertising strategies.</p><p>Our main contributions include: 1) To our best knowledge, our DISA is the first attempt focusing on the interpretability of realistic advertising strategies with POMDP. 2) To optimize the strategy performance, we propose a variant of SPOVA method, a more suitable deep neural network solution for POMDP than pure black-box deep networks commonly used in DRL. 3) We develop POMDP's application in large-scale industrial settings. The inferred hidden states are analyzed to show the efficacy of our method.</p><p>The rest of this paper is organized as follows. Section 2 introduces the recent work related to POMDPs, followed by an analysis of the sequential advertising problem in section 3. Section 4 formulates the problem. Section 5 presents our approach to the problem and gives detailed implementations. In Section 6, we discuss the experimental results and interpret the hidden states as well as the learned advertising strategies. Section 7 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Generally, a POMDP model can be considered as a belief-state MDP <ref type="bibr" target="#b20">[21]</ref>. The Hidden Markov Model (HMM) is usually used to represent the hidden states of POMDP <ref type="bibr" target="#b15">[16]</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref>. The Baum-Welch algorithm <ref type="bibr" target="#b14">[15]</ref> is extended to adjust the probabilities of the Markov model, while Bayesian-based methods <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref> can improve the model through interaction with the environment. For policy learning, structured representations are usually used to solve the value approximation <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b25">26]</ref>. The neural network is first introduced to yield good value approximations in SPOVA <ref type="bibr" target="#b22">[23]</ref>, and the recurrent neural network (RNN) is adopted in QMDP-net <ref type="bibr" target="#b13">[14]</ref> for the planning of POMDP. However, these methods are usually evaluated with simple tasks and impractical for realistic applications. Although MTA methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b26">27]</ref> know how each exposure contributes to the conversion, they do not model user latent states and cannot support online inference; thus, they cannot directly solve our problem.</p><p>In applications of MDP and POMDP, bandit-based models with Thompson sampling are widely used in simple recommendation problems <ref type="bibr" target="#b18">[19]</ref>. Yuan and Wang <ref type="bibr" target="#b30">[31]</ref> propose to utilize the correlation of ads to improve the efficiency of exploration. These applications haven't taken advantage of current deep learning merits for better performance. There are some DRL-based solutions <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b31">32]</ref> to ranking problems. Hu et al. <ref type="bibr" target="#b8">[9]</ref> propose a policy gradient algorithm to learn an optimal ranking policy by modeling the reward function. Ie et al. <ref type="bibr" target="#b9">[10]</ref> optimize the slate-based recommendations based on estimated long-term value. These works mainly use end-to-end deep learning methods and are weak in terms of interpretability. DeepIntent <ref type="bibr" target="#b31">[32]</ref> models the intents using the attention weights on top of RNN, its black-box learning cannot explicitly model intents' transitions; thus, the sample complexity could be higher without the prior knowledge that user behaviors are generated based on hidden state transitions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">MULTI-SCENARIO SEQUENTIAL ADVERTISING</head><p>In large mobile E-commerce platforms, e.g., Amazon, eBay, Taobao, there are millions of users visiting different scenarios every day. The repeated visits of these users allow the platform to help advertisers earn more revenue with appropriate multi-step advertising strategies. In this paper, we follow the framework of MDP, and we care about the interpretations of the displaying effect on a user purchase intention. This interpretability benefit us in 1) attribution: easily interpret the insights of user conversions, 2) optimization: guide the future advertising policy in other similar applications. However, the user intention is not directly observable, so we model it as a hidden state. To do this, we formalize this problem as a POMDP where the agent (the advertising engine) learns to maximize advertisers' revenue by inferring the consumers' hidden state. ) are selected and delivered back to the consumer. Here, K is determined by the type of scenarios.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">PROBLEM DEFINITION</head><p>(3) Feedback stage, for each displayed item I i , the advertising engine collects the feedback of the user purchase behavior y t and click behavior x t . The advertiser X i will pay money bid i to the advertising engine if the user clicks (x t = 1) and will obtain revenue price i when the user purchases (y t = 1).</p><p>Formally, given a sequence of requests Q = (U 1 ∼ U T ) from a consumer, our problem is defined as a sequential decision process to determine the appropriate ad items (L<ref type="foot" target="#foot_0">foot_0</ref> K ∼ L T K ) 1 to maximize the advertisers' profits. The ranking function here is designed to be a set of score actions f t = {a t 1 ∼ a t i } on each candidate item I i in D t , which are the output of the agent. To interpret each advertising action a t i , we need to know how a t i will affect or transit a user latent intent<ref type="foot" target="#foot_1">foot_1</ref> on the item, which can be explicitly modeled by a POMDP.</p><p>Specifically, a POMDP model is a 7-tuple (S, A, O, T , O, r , γ ) where S is a set of discrete hidden states describing the intents of a user, A is a set of score actions on an item, and O is a set of the agent's observations on user behavior to the item. The transition function T (s, s ′ , a) = P(s ′ |s, a) describes the probability of transition from state s to s ′ after executing action a, while observation function O(s ′ , a, o) = P(o|s ′ , a) specifies the probability that a next observation o will be received after the agent performs action a and lands in state s ′ . The reward r captures the expected feedback from the environment, and γ ∈ [0, 1] is the discounted factor.</p><p>At each time-step t, an advertising action a t is decided given an observation o t , which brings up two steps: 1) the agent infers a belief b t (defined as a probability distribution over all hidden states) with a state estimator, 2) the action a t is chosen based on b t with a policy learner. Fig. <ref type="figure">2</ref> gives the two steps as following.</p><p>State Estimator (SE). According to the parameters of T and O, the state estimator produces the current belief b t with the observation o t , the previous b t -1 and a t -1 using the Bayes rule:</p><formula xml:id="formula_0">b t (s ′ ) = ρO(s ′ , a t -1 , o t ) s ∈S T (s, s ′ , a t -1 )b t -1 (s),<label>(1)</label></formula><p>where ρ is the normalized factor, and b(s) represents the probability that a consumer hidden state is under state s. Policy Learner. After estimating b t , the agent has to learn the mappings from beliefs to actions, denoted by a policy a t = π (b t ). One could think of a POMDP as an MDP defined over belief states, then the well-known Bellman equation for POMDP still holds <ref type="bibr" target="#b20">[21]</ref>. In particular,</p><formula xml:id="formula_1">V * (b t ) = max a t [r t + γ o ∈O P(o t |a t , b t )V * (b t +1 )]<label>(2)</label></formula><p>where V * (b t ) is the belief value function with an optimal policy π * . Unlike the budget constraint setting in <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b29">30]</ref>, our agent's goal is to maximize the advertiser's profits within a certain time window T w . The window is usually set according to how soon most of the conversions are reached after ad exposures. The profits are defined as the advertiser's revenue subtracting the budget cost. The reward is therefore given by r t,i = price i y t -bid i x t . The objective of learning is to find an optimal policy to maximize the expected return of each item I i .</p><formula xml:id="formula_2">π * i = arg max π i E       T t =1 i ∈L k (D l , f t ) γ t r t,i |π i      <label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">METHODOLOGY</head><p>In this section, we introduce our proposed DISA with three parts. We first present an EM-based method to estimate the parameters of the state estimator. We then adopt an approximated method for the policy learner to optimize the value function over beliefs. Finally, we give the specific implementation of DISA with the real advertising engine.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">EM-based Parameters Estimation</head><p>To perform belief updates with the Eq. ( <ref type="formula" target="#formula_0">1</ref>), we firstly need to know the transition function T and observation function O. However, these two fundamental functions are not available priori in our case, and we have to estimate them in advance. Essentially, a POMDP can be regarded as an extended HMMs conditioned on a sequence of actions. As such, we can learn the parameters of POMDP by building a conditional HMMs and solving it with EM-based algorithms.</p><p>Based on the analysis, we now describe the parameter estimation as a learning problem of a conditional HMM model parameterized by θ = (b 0 ,T , O) where b 0 is the initial distribution of hidden states. Given a trajectory J T = (a 0 , o 1 , a 1 , o 2 , a 2 , . . . , o T ) on an ad, we try to find the parameters to best fit the trajectory with user latent variables S.</p><formula xml:id="formula_3">Specifically, let O = {o 1 ∼ o T } and A = {a 0 ∼ a T -1 }</formula><p>denote the sequence of observations and corresponding actions in J T (each observation is given equal weight), we study the maximization of the log-likelihood of O conditioned on A:</p><formula xml:id="formula_4">l(θ ) = log P(O|A; θ ) ≥ s ∈S q(s) log P(O, s |A; θ ) - s ∈S q(s) log q(s) = L(q, θ ; O, A)<label>(4)</label></formula><p>where q(s) is a density function that satisfies s q(s) = 1. In the lower bound L(q, θ ; O, A), the ≥ follows Jensen's inequality, and the equality is only reached at q(s) = P(s |O, A; θ ). Following the EM algorithm, at each time-step t, our E-step is to estimate:</p><formula xml:id="formula_5">q t = arg max q L(q, θ t -1 ; O, A) = P(s |O, A; θ t -1 )<label>(5)</label></formula><p>The M-step is to adjust θ by maximizing the Q-function with q t :</p><formula xml:id="formula_6">θ t = arg max θ E q t (s) log P(s, O|A; θ t -1 )<label>(6)</label></formula><p>We derive a variant of Baum-Welch algorithm to implement the above iterative procedures, and the details can be found in Supplementary A.1. When we obtain the estimated T and O, the current belief b t can be updated by Eq. ( <ref type="formula" target="#formula_0">1</ref>). Our next step is to learn the action policy a t = π (b t ) with the given belief b t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Belief Value Function Approximation</head><p>A critical question for policy learning is how to represent the value function for beliefs. Sondik <ref type="bibr" target="#b23">[24]</ref> showed the value function V (b) can be represented as the max over a finite set of vectors. However, exact methods for solving this are impractical <ref type="bibr" target="#b20">[21]</ref>, and function approximation is a more attractive alternative than exact methods. In this paper, we prefer to implement the approximation with deep neural networks to improve the learning of our strategies. As such, we approximate V (b) using a set of parameterized Q-functions:</p><formula xml:id="formula_7">V (b) = max a Q a (b; η a )<label>(7)</label></formula><p>where Q a (b; η a ) is the expected return for taking action a in belief b, and each Q-function is approximated by a soft max function SPOVA <ref type="bibr" target="#b23">[24]</ref>:</p><formula xml:id="formula_8">Q a (b; η a ) = z n i=1 (b • η a i ) z<label>(8)</label></formula><p>here each η a i is the output vector of deep neural networks w.r.t an action a, and the value of n determines how many vectors are used to split the belief space into linear representations. z is an indicator interpreted as a measure of how "rigid" the approximation is <ref type="bibr" target="#b22">[23]</ref>.</p><p>Given the Q-function, our policy π is then to select the action with the largest Q-value:</p><formula xml:id="formula_9">a t = arg max a Q a (b t ; η a ).</formula><p>Assuming b ′ is the updated belief after performing best action a in b, the optimization of the value function is performed by minimizing the square of Bellman residual E(b) 2 <ref type="bibr" target="#b19">[20]</ref> where <ref type="formula" target="#formula_8">8</ref>) is differentiable, a typical gradient descent method can be used to update each vector η a i . The updates for the j-th component of the i-th η vector, η a j i turns out to be:</p><formula xml:id="formula_10">E(b) = γV (b ′ ) + r -Q a (b; η a ). Since Eq. (</formula><formula xml:id="formula_11">△ η a j i = αE(b)b j (b • η a i ) z V (b) z<label>(9)</label></formula><p>where α refers to a step size or learning rate. Note that we should keep each η a i positive to allow the second derivative of Eq. ( <ref type="formula" target="#formula_8">8</ref>) always positive in each dimension, so the function is always convex. This can be done by replacing</p><formula xml:id="formula_12">(b • η a ) with (b • η a + υ)</formula><p>where υ is a constant offset <ref type="bibr" target="#b22">[23]</ref>. However, we found that a large constant υ will bring updating bias when γ &gt; 0, which leads to an unstable learning process. To address this, we compensate the bias in Bellman residual:</p><formula xml:id="formula_13">E(b) = γV (b ′ ) + r -Q a (b; η a ) + (1 -γ )υ.</formula><p>An alternative is to use reward shaping to keep rewards r always positive, which can prevent the learning direction of η a i from going towards negative values.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Implementations</head><p>Here, we illustrate our detailed solution to the real advertising optimization, including some key concepts of applying DISA, as well as the implementation of the state estimator and policy learner. Item modeling level. From the online data, we found the samples of repeated exposures for a specific item are sparse, which brings difficulties in training. In this paper, we relax the POMDPs modeling level from items to categories, and different consumers share the parameters of DISA during learning and execution. This setting can largely increase the quantity and diversity of learning samples and improve the model's generalization. Note that we use the most fine-grained categories maintained by the advertising system. According to our data, although the category features will lose some individual information, our categories are detailed enough that the individual differences within a category are small. We will study better aggregation methods in the future.</p><p>Action. The ranking function f for an advertising platform is usually designed using eCPM sorting mechanism <ref type="bibr" target="#b12">[13]</ref>, which aims to maximize the revenue of the platform, given by rank_score = pCT R × bid. We follow this setting, and we perform actions on the rank_scores at the categorical level. In particular, we use a ratio δ j to adjust the rank score of an item I i that belongs to the j-th category, that is rank_score ′ i = rank_score i × δ j . This δ is the output of the agent's action that could affect the ranking of items so as to decide the final displayed item. For a discrete action setting, we define three actions: a boosting action with δ j &gt; 1, a restraining action with δ j &lt; 1, and a keeping action with δ j = 1; the value of δ for each action should be tuned under this setting.</p><p>Reward. As mentioned in Section 4, our reward is defined as the advertisers' revenue subtracting their budget cost. This reward setting may suffer from a local-optimal policy: the agent learns to increase the rewards by reducing advertisers' budget cost. To tackle this problem, we propose a bid punishment mechanism to force the agent focusing on improving the revenue rather than reducing budget cost. We increase the bid price for the boosting action: bid ′ = bid × β where β ⩾ 1 is a punishment variable which controls the extra cost of performing the boosting action. Therefore, the reward function is reshaped to be:</p><formula xml:id="formula_14">r t,i = λprice i y t -β j bid i x t , if a j is boosting action λprice i y t -bid i x t , otherwise</formula><p>where λ is set to balance the data magnitude between revenue and cost so that the agent can equally optimize revenue and cost (purchase y t is more sparse than click x t ). More details will be discussed in our experiment.</p><p>State estimator. Following the Eq. ( <ref type="formula" target="#formula_0">1</ref>), we illustrate a belief updating process in Fig. <ref type="figure" target="#fig_2">3</ref> </p><formula xml:id="formula_15">[i] = b t [i]/ j b t [j],</formula><p>and it produces the next belief b t+1 .</p><p>Policy learner. The policy learner is implemented with a deep neural network such as multi-layer perception (MLP) as Fig. <ref type="figure" target="#fig_2">3</ref>(b) depicts. The input of this policy network is the belief vector, and the output is split into |A| groups. The output of each group is conducted with the max smooth function of Eq. ( <ref type="formula" target="#formula_8">8</ref>) to obtain the Q-value function for each action. In this case, the η vectors for each action are embedded into the parameters of hidden layers, which are trained end-to-end through the whole policy network by a gradient descent method in Eq. ( <ref type="formula" target="#formula_11">9</ref>).</p><p>Simulator. For offline experiments, we offer a simulator to imitate consumers' feedback by applying supervised learning techniques on real consumer behavior. Similar simulator settings can be found in <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b27">28]</ref>. Since a userâĂŹs preferences can be timedependent and also depend on the history of past ad impressions, we choose a recurrent model to make multi-task predictions on the real click x t and purchase y t . In particular, at each time-step t, we adopt an RNN model to output a vector pt = ( xt , ŷt ) where xt and ŷt are the predicted probability of the click and purchase action on I t . The recurrent model is implemented by one stack layer LSTM <ref type="bibr" target="#b28">[29]</ref> with the hidden size of 256, and we unroll the LSTM cell in a maximum sequence length of 25. We optimize the simulator network using the sum of cross-entropy loss between the ground-truth p t and pt across all time-steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">EXPERIMENT</head><p>We showcase the effectiveness of our approach in a series of simulated experiments and live experiments in a real-world Taobao ad system. We consider two scenarios in the homepage of Taobao App: 1) Good Items targets the consumers with a high expense, so the ad items are usually in high quality; 2) Guess What You Like aims to perform personalized advertising strategies, and thus the items are chosen based on users' preferences, interests, and recent behaviors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Empirical Evaluation: Simulations</head><p>The dataset 3 includes 58,648 request sessions from 4,988 sampled users in the two scenarios within three days. Each request contains a candidate ad set D (50 ≤ |D| ≤ 400). The whole dataset involves 52,749 ad items and 4,543,880 records in total. 3 Dataset is available: <ref type="url" target="https://github.com/465935564/sequential_advertising_data">https://github.com/465935564/sequential_advertising_data</ref>     Each category has 5 ads on average. As each scenario has only one ad position, we have K=1. All the request sessions of a consumer are sorted in session time to form a consumer trajectory. We use 90% of the trajectories as a training set while the rest 10% leaves for test evaluation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.1">Simulator Training.</head><p>To conduct offline experiments, we train an environment simulator to imitate user click and purchase actions on an ad item. When the agent decides on an item for a user, our simulator will generate the click and conversion rate for this ad-user pair, from which we sample the final click/purchase actions. The consistency of simulated data and the real-world data is important, and thus we evaluate the simulator in 3 ways:</p><p>We first show the learning loss of the training and test set in Fig. <ref type="figure" target="#fig_6">4</ref>(a), which illustrates the loss converges well. The learning accuracy (AUC score) of two predictions are given in Fig. <ref type="figure" target="#fig_6">4(b)</ref>. We achieved 0.732 AUC for click and 0.771 AUC for purchase at 50-th epochs, which proves the prediction ability of our learned simulator. Apart from the accuracy curves, we compare the simulated prediction with the ground-truth data, depicted as Fig. <ref type="figure" target="#fig_6">4(c)</ref>. The figure shows that the simulator can correctly predict the trends of real data. Beyond that, we also find an interesting phenomenon: the conversion rate (the blue line in Fig. <ref type="figure" target="#fig_6">4(c)</ref>) will increase if we impress a user by repeated displays, which indicates the potential benefits of sequentially repeated advertising exposures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2">Policy Learning and Evaluation.</head><p>In this part, the agent optimizes its advertising strategies based on the user latent states by the feedback provided by the simulator. To infer user states, we train and evaluate the EM model by the log probability curves of the observed sequences in the test set shown in Fig. <ref type="figure" target="#fig_6">4(d)</ref>, where the parameters converge well. The number of |S| controls how fine we split users' latent states S = {s 1 , s 2 , ...s j }. In our case, we use |S| = 3 because we find the converged log probability does not increase much when |S| &gt; 3. The following algorithms are  compared with our method with the same observations, actions, and rewards settings <ref type="foot" target="#foot_2">4</ref> . Manual bid. It's the bid strategy using humans' experience <ref type="bibr" target="#b12">[13]</ref>.</p><p>Bandit. Contextual bandit <ref type="bibr" target="#b1">[2]</ref> is an online algorithm that maximizes the total payoff of the chosen actions given the context.</p><p>DQN. DQN <ref type="bibr" target="#b19">[20]</ref> is a model-free RL algorithm. It directly takes in the observations and outputs the ranking policy by selecting the largest Q-value action.</p><p>ADRQN. It is a recurrent variant of DQN where the current observation and the last time-step action are fed to an LSTM network <ref type="bibr" target="#b33">[34]</ref>. This is a model-free POMDP where the latent state is implicitly captured and modeled by the LSTM.</p><p>DISA. This is our proposed model-based POMDP algorithm. DISA explicitly estimates the beliefs (distribution of hidden states) and learns to optimize its policy by the belief value approximation.</p><p>EM-DQN. It is a variant of DQN where its input is the beliefs of DISA rather than observations. This method attempts to learn the mappings from beliefs to actions with the model-free RL.</p><p>For fair comparisons, several experiments are conducted to show the performance of different methods with the same γ parameter in Table <ref type="table" target="#tab_9">3</ref>. Each method is evaluated by the ROI indicator (revenue/cost) and the average rewards (advertisers' profits). A higher ROI shows the stronger ability of earning more income with the </p><formula xml:id="formula_16">i ) = O (o[j]|s i )</formula><p>Table <ref type="table" target="#tab_4">2</ref>: Statistics of the learned parameters in SE. same budget cost. A higher reward is also important as it indicates a method can help advertisers obtain more profits. From Table <ref type="table" target="#tab_9">3</ref>, almost all the RL-based methods achieve higher ROI than Bandit method with γ ∈ {0.5, 0.7, 0.9} since their decisionmaking is based on the long-term rewards. Under the same setting of γ , DISA outperforms all the others in ROI while achieving almost the same cost as other baselines. These results indicate the superiority of DISA as it not only helps advertisers earn more income per budget cost but also improves profits. Compared with DQN, for all γ , a higher ROI of EM-DQN shows the benefits of inferring beliefs over the behavior-action mappings (black-box) in modelfree fashion. Furthermore, DISA also demonstrates its advantage of the belief value approximation in SPOVA over the general neural network (pure belief-action mappings) in EM-DQN by ROI.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.3">Interpretations of Learned Hidden</head><p>States. Essentially, the EM learns a mapping from high-dimensional historical observations/actions to a compressed belief state, and this mapping is reflected in the learned parameters T (s ′ |s i , a), O(o|s i , a) and b 0 (s i ). By analyzing these parameters, we can know how each state connects with different observations, so we can further interpret the property of each state s i . To do this, one direct way is to compare the distribution 5 of an observation O(o|s) w.r.t each state, e.g., Fig. <ref type="figure" target="#fig_7">5</ref>(Left) illustrates that a large value of o 1 is more likely to be observed under s 2 rather than s 3 and s 1 , so we can distinguish s 2 by the large value of the expectation 6 E[O(o 1 |s 2 )]. According to such different expectations of each observation, we can easily explain the characteristics of each state.</p><p>In our ad system, the observations reflecting a user's intent mainly include the number of exposure, click and purchase of the ad to the user, as well as how the user behaves in different scenarios. More concretely, our observations are that: pv gi and clk gi represent how many previous exposure and clicks of an ad have been made in Good Items (similar for pv gw and clk gw in Guess What You Like), and scen describes how frequently a user switches to other scenarios. Here, we neglect purchase observations as the data is too sparse. )] = 0.71). Compared with state s 2 , state s 1 is more active because the users are more likely to switch 5 For better explanation, we slightly abuse the notation in this section. We marginalize out O (o |s, a) and T (s ′ |s, a) for all a to obtain O (o |s) and T (s ′ |s). 6 We define E[O (o |s, a)] = i O (o i |s, a)o i where o i is the observed value.    ); this explains that users in s 1 start to actively search for their interested items across different scenarios, and thus we label s 1 as a search state. Note that our analysis is compatible with the definition of customer funnel revealed in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b21">22]</ref>, and the differences are that: 1) our results are datadriven and learned from a validated EM model, and 2) we treat the final conversion state as an observable state instead of a latent state that requires inference. Furthermore, we can also verify our interpretations above by b 0 and T (s ′ |s), depicted in Fig. <ref type="figure" target="#fig_7">5</ref>(Right). b 0 tells us that almost 73% of users start from the awareness state, while 24% of users begin with the search state. T (s ′ |s) describes how each state transits: i) awareness s 3 15% ----→ search s 1 3% ---→ interest s 2 , and ii) awareness s 3 13% ----→ interest s 2 . These transition routes indicate that a user's status always transits from awareness to interest/search rather than going in reverse, which is consistent with our common sense.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.4">Interpretations of Learned Strategies.</head><p>Based on the interpretable state, we can compare the difference of the belief's evolutionary tracks by performing two different advertising strategies (DISA, Manual bid) on the same user trajectories.</p><p>We collect all the inferred belief vectors and project them into a 2dim space with PCA techniques as Fig. <ref type="figure" target="#fig_11">6(b)</ref>. For better visualization, we use K-means to cluster those nodes into 3 clusters with different colors so that each cluster is dominated by one type of hidden state, e.g., more than 90% of the belief nodes in cluster1 belong to state s 3 , depicted as Fig. <ref type="figure" target="#fig_11">6(a)</ref>. So we can label each cluster with the property of each state: cluster 1, cluster 2 and cluster 3 are regarded as an awareness stage (s 3 ), an search stage (s 1 ) and an interest stage (s 2 ) respectively. Furthermore, we compare the average reward collected at each stage in Fig. <ref type="figure" target="#fig_11">6(c)</ref>, which shows the search/interest stage earns much higher rewards than the awareness stage; this in turn proves the rationality of our analysis on each state.</p><p>Let's examine a typical trajectory where consumers browse dress items in Good Items first with 6 requests and then in Guess What You Like with 2 more requests. Fig. <ref type="figure" target="#fig_11">6</ref>(b) gives two evolutionary trajectories of their states under the strategy of DISA and the manual bid baseline. We can see that both two trajectories start from the awareness stage and also end in the interest stage, but they get separated after the 3-rd advertising action. This separation leads to the main difference of two trajectories: DISA successfully guides the hidden state transiting to the search stage while the human bid baseline does not. We draw the performed actions and corresponding rewards in Fig. <ref type="figure" target="#fig_11">6(c)</ref>, which shows that the boosting actions in DISA dominate after the 3-rd action. One reasonable explanation is that: the boosting action can guarantee the display of ad items and further impact the consumer's perception on the items, especially in Good Items. Therefore, after the consumer switches to Guess What You Like, the repeated boosting on the same item helps transit consumer's state to the search stage, which leads to a relatively higher reward as shown in Fig. <ref type="figure" target="#fig_11">6(c</ref>).</p><p>6.1.5 Reward Settings. The value of β determines the degree of punishment for performing the boosting action (β = 1 means no punishment). With small β, the agent is easier to use boosting action to win the bidding, leading to the increase of impressions/cost and further reaching low rewards. Large β means fewer impression opportunities to obtain revenue and will also achieve low rewards. In Fig. <ref type="figure" target="#fig_12">7</ref> (Left), we find β = 1.2 can well control the frequency of boosting actions so that rewards are maximized. In our data, click behavior happens 5-10 times more than purchase (shown in 4(c)), and therefore, λ = 5 is enough to adjust the data magnitude between revenue and cost; besides, we also find λ = 5 performs best in ROI by the parameter grid search. The window T w is set to 3 hours since we find 90% conversions are reached within 3 hours.</p><p>6.1.6 Performance within Different Items. We compare the rewards under the items with different visiting frequencies in Fig. <ref type="figure" target="#fig_12">7</ref> (Right). It is clear that the more a user interacts with an item, the more reward is gained. However, when the visiting frequency is less than 2, the reward becomes much lower, which can be reasoned that it is hard to transfer users to the interest/search state with only two steps. It also shows our model works better with a longer sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Live Experiments</head><p>We conduct online A/B experiments running in the live ad platform. Experiments are run from Oct.26 to Nov. 2 in 2019, which involves randomly sampled 9,165,752 users, 664 advertisers, and 72,381 ad items from 12,401 categories. Our sequential advertising model (experimental group) is trained continuously using all user behaviors across 9 scenarios with a lag under 24 hours. The control group is a deployed production model (Cross Entropy Method, CEM <ref type="bibr" target="#b5">[6]</ref>) that optimizes for immediate rewards. We allocate the same budget cost to the control and experimental group for each advertiser (we asked the advertisers for permission to adjust their budgets). We focus our discussion on the amount of revenue and ROI of the advertisers. In Fig. <ref type="figure">8</ref>, we achieved +9.02% of revenues with the same budget cost (-0.81%), resulting in +9.75% of ROI for the experimental group. As our live results are promising, our algorithm has been officially deployed online and allows advertisers to customize their advertising strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">CONCLUSIONS</head><p>In this paper, we proposed our DISA to model the sequential advertising problem, which optimized the strategies by taking account of interpretability. We developed POMDP framework in large-scale industrial settings to infer hidden states based on the consumer's historical behaviors. To best fit our interpretable model, a variant of SPOVA based on deep neural networks has been proposed to learn value function and optimize advertising policies. Many details of our implementation were provided. The simulation and A/B online results have validated the superiority of the proposed algorithm against several DRL baselines. In several cases' analysis, we try to interpret the learned hidden states, which are meaningful and consistent with our business common sense.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A SUPPLEMENTARY A.1 Derivation of parameter learning for DISA</head><p>Let us consider a discrete extended HMM in session 5.1 with length L. Let the space of observations, hidden states, and actions be M, N , and A respectively. Given a sequence of observations O = {o 1 ∼ o L } and corresponding actions A = {a 0 ∼ a L-1 }, a POMDP model is parameterized by a extended HMMs with θ = (b 0 ,T , O). Specifically, b 0 (i) = P(s 1 = i) is the initial state distribution, T j i,k = P(s t +1 = j, s t = i |a t = k) is the transition function, and O i,k (j) = P(o t = j, s t = i |a t -1 = k) is the observation function. The Q-function is defined as the expectation term that we need to maximize:</p><formula xml:id="formula_17">Q(θ, θ t ) = q(s) s ∈S log P(O, s |A; θ t ) = E q(s) log P(O, s |A; θ t )</formula><p>A.1.1 Extension of Baum-Welch procedures. We extend Baum-Welch procedure for estimating θ * from O and A. Our method can be described as repeating the following steps until convergence:</p><formula xml:id="formula_18">(1) E-step: compute Q(θ, θ t ) = s log[P(O, s |A; θ )]P(s |O, A; θ t ) (2) M-step: set θ t +1 = arg max θ Q(θ, θ t -1 )</formula><p>Firstly, noting that P(s, O|A) = P(s |O, A)P(O|A), we can write the Q function as Q(θ, θ t ) = s log[P(O, s |A; θ )]P(s, O|A; θ t ) since P(O|A) does not affect the maximization of Q in M-step. Now the P(O, s |A; θ ) is easy to write: Note that parameters are subjective to the constraints:</p><formula xml:id="formula_19">P(O, S|A; θ ) = P(o 1 ∼ o L , s 1 ∼ s L |a 0 ∼ a L-1 ; θ ) = b 0 (s 1 ) L t =2 T s t s t -1 ,a t -1 L t =1 O s t ,</formula><formula xml:id="formula_20">s ′ T s ′ s,a = 1; o O o s,a = 1; s b 0 (s) = 1;</formula><p>Applying Lagrange multiplier method, let L(θ, θ t ) be the Lagrangian</p><formula xml:id="formula_21">L(θ, θ t ) = Q(θ, θ t ) -λ b 0 N i=1 b 0 (i) -1 - N ,A i,k =1 λ T i,k N j=1 T j i,k -1 - N ,A i,k =1 λ O i,k M j=1 O i,k (j) -1</formula><p>First let us focus on the b 0 (i). Let ∂ L(θ, θ t )/∂b 0 (i) = 0 and ∂ L(θ, θ t ) /∂λ b 0 = 0, we obtain:</p><formula xml:id="formula_22">b 0 (i) = P(s 1 = i |O, a 0 ; θ t )</formula><p>Following a similar process for the b 0 , we have:</p><formula xml:id="formula_23">T j i,k = L t =2 P(s t -1 = i, s t = j |O, a t -1 = k; θ t ) L t =2 P(s t -1 = i |O, a t -1 = k; θ t ) The final thing is O i,k (j)</formula><p>, which is slightly trickier, let I (x) denotes an indicator function which is 1 if x is true, 0 otherwise. Similar with T j i,k , we finally get:</p><formula xml:id="formula_24">O i,k (j) = L t =1 P(s t = i |O, a t -1 = k; θ t )I (x t = j) L t =1 P(s t = i |O, a t -1 = k; θ t )</formula><p>For brevity, we use simple denotations γ (i, j, k) = P(s t -1 = i, s t = j |O, a t -1 = k; θ t ) and γ (i, k) = N j=1 γ (i, j, k) = P(s t -1 = i |O, a t -1 = k; θ t ). Note that γ (i, j, k) and γ (i, k) are both quantities and can be computed efficiently by a variant of forward-backwards algorithm for extended HMMs.</p><p>A.1.2 Inference of extended HMMs. In order to compute the γ (i, j, k), we need to solve the forward-backward pass, and the γ algorithm in extended HMMs.</p><p>Forward pass: We use notations α(s t , a t -1 ) (t &lt; L) to represent the probability of being in hidden state s t given observations o 1 ∼ o t and conditioned on a 0 ∼ a</p><formula xml:id="formula_25">t -1 , α(s t , a t -1 ) = P(o 1 ∼ o t , s t |a 0 ∼ a t -1 ) = s t -1 α(s t -1 , a t -2 )T s t s t -1 ,a t -1 O s t ,a t -1 (o t )</formula><p>where α(s 1 , a 0 ) = b 0 (s </p><formula xml:id="formula_26">(s L-1 , a L-1 ) = s L T s L s L-1 ,a L-1 O s L ,a L-1 (o L )</formula><p>γ algorithm: after we recursively compute α(s t , a t -1 ) and β(s t , a t ) for each s t , we can easily obtain a γ ′ (s t , s t +1 , a t ) which is used to compute</p><formula xml:id="formula_27">γ (s t , s t +1 , a t ), γ ′ (s t , s t +1 , a t ) = P(s t , s t +1 , o 1 ∼ o L |a 0 ∼ a L-1 ) = α(s t , a t -1 )T s t +1 s t ,a t O s t +1 ,a t (o t +1 )β(s t +1 , a t +1</formula><p>) Finally, we have:</p><formula xml:id="formula_28">γ (i, j, k) = P(s t -1 = i, s t = j |O, a t -1 = k; θ t ) = γ ′ (s t -1 = i, s t = j, a t -1 = k) N i=1 N j=1 γ ′ (s t -1 = i, s t = j, a t -1 = k)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Experiment Details</head><p>A.2.1 Observation and Action Settings. To know the accumulated effect of a user repeated action in different scenarios, we also have a few features on top of the basic features. In specific, for each user trajectory, we compute the accumulated pv, pCVR and click to represent how many previous impressions and clicks have been  Since we are modeling on the categorical level, we use an aggregation method to summarize the features of the items that belong to the same category as Fig. <ref type="figure">9</ref>. Then, the observation for each category is described by a vector of statistical features, e.g., the mean, max, min, and standard deviation of each item-level observations. To speed up the calculations, the agent feeds in all the categorical features of a request as a learning/execution batch, and outputs the corresponding actions.</p><formula xml:id="formula_29">J = {J 1 ∼ J m } from D ; 5 Construct O = {o 1 ∼ o T }, A = {a 0 ∼ o T -1 }</formula><p>The recurrent model is implemented by one stack layer LSTM with the hidden size of 256, and we unroll the LSTM cell in a maximum sequence length of 25. At each time-step, the simulator outputs a 2-dim vector representing the probability of click and purchase, which are optimized by real user feedbacks. Based on the training results of the simulator, we choose several important features to the infer of a user hidden state, which contain price, bid, pCTR, pv gw , clk gw , pCVR gw , pv gi , clk gi , pCVR gi and scen. To work with a discrete conditional HMM, we use a quantile-based discretization for each observed feature.</p><p>In particular, the pv gi is discretized into a range of [0, 6], and the clk gi is discretized into a range of <ref type="bibr">[0,</ref><ref type="bibr" target="#b4">5]</ref>   scen will equal to 1 if the current scenario is under Guess What You Like otherwise equal to 0. Note that the discretization will not affect the features' monotonicity, e.g., pv gw = 5 means a stronger impression being made than that of pv gw = 4 in Guess What You Like; scen = 0.8 means a higher probability of switching into Guess What You Like than scen = 0.5. The action is also discretized into three distinct values where the boosting, keeping and restraining action are defined by δ =10, δ =1 and δ =0.1 respectively. For an ad item, the boosting action with δ =10 can almost guarantee to win the bidding, while the restraining action with δ =0.1 can almost prevent its winning of the bidding. Since the distribution of user hidden states is stationary and will not migrate over time in our experiment, the learned parameters are fixed while optimizing the agent's policies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.2 Policy</head><p>Learning with Trajectory Replays. The off-policy RL is identical to our problem because the agent passively responds to user requests, and the next request might come from a different user. Thus, for every user and category pair, we rely on a trajectory reply pool to store the corresponding experience tuple, used for constructing transition samples. The updating of the state estimator is performed along with the policy learning to cover the patterns of newly arrived user trajectories. For each Q-network, the target network freezing technique is also adopted to stabilize the learning process. The training of DISA is formalized as Algorithm 1.</p><p>A.2.3 Hyper-parameter Tunning. The discount factor γ determines the importance of future rewards. In Table <ref type="table" target="#tab_9">3</ref>, we find almost all the methods will perform better as γ increases from 0 to 0.5. This result shows the existence of the future delayed rewards and proves the multi-step decision-making property of our problem. The value of n in Eq. ( <ref type="formula" target="#formula_11">9</ref>) decides how many regions the belief space will be split. When n is 1, the belief value function for each action is represented by a linear hyperplane, and with the increase of n, the value function will be represented by more hyperplanes, leading to a finer and more accurate belief region. In our experiments, by tuning on γ and n, we find the hyper-parameters of γ = 0.9, n = 5 can achieve the best performance. Fig. <ref type="figure" target="#fig_15">10</ref> <ref type="foot" target="#foot_3">foot_3</ref> illustrates the learning process of different methods with the best parameter setting, from which we can see that our method DISA achieves higher ROI and also converges faster than others.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An advertised item on consumer trajectories across multiple scenarios.</figDesc><graphic coords="2,155.32,84.43,54.12,106.71" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>𝑠𝑖, 𝑎𝑡 = 𝑗 𝑂(𝑜𝑡[𝑗]|𝑠𝑖, 𝑎𝑡) 𝐎 𝐭 𝑆 𝐎 𝐭 𝑆 = [𝑂 𝑜𝑡 𝑠1, 𝑎𝑡 , … , 𝑂 𝑜𝑡 𝑠𝑖, 𝑎𝑡 , … ]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Implementation of DISA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Learning Curves of EM model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Training result of the simulator and EM model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Distributions of the learned parameter O(o 1 |s i ) (Left), and the diagram for state transitions (Right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>Evolutionary trajectories of beliefs in 2-d projection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>Advertising actions and rewards.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Belief clustering and the evolutionary trajectories with different strategies. to Guess What You Like while maintaining a relative high level of browsing behaviors (E[O(o 5 |s 1 )] = 0.78, E[O(o 3 |s 1 )] = 1.90, E[O(o 1 |s 1 )] &gt; E[O(o 1|s 3 )]); this explains that users in s 1 start to actively search for their interested items across different scenarios, and thus we label s 1 as a search state. Note that our analysis is compatible with the definition of customer funnel revealed in<ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b21">22]</ref>, and the differences are that: 1) our results are datadriven and learned from a validated EM model, and 2) we treat the final conversion state as an observable state instead of a latent state that requires inference.Furthermore, we can also verify our interpretations above by b 0 and T (s ′ |s), depicted in Fig.5(Right). b 0 tells us that almost 73% of users start from the awareness state, while 24% of users begin with the search state. T (s ′ |s) describes how each state transits: i) awareness s 3</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Rewards under different β (Left), and rewards under different visiting frequencies (Right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>day of 1 Figure 8 :</head><label>18</label><figDesc>Figure 8: Increase in ROI over the control group.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Algorithm 1: DISA 1 2 4 Sample M trajectories</head><label>14</label><figDesc>Init a Q-network Q a (b; η a ) for each action a and a trajectory replay memory D; Init the estimator state with parameters θ 0 = (T , O, b 0 ); 3 for e = 1 to E do</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: The learning curves of cost and ROI.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head></head><label></label><figDesc>(a). Considering a case where actions and observations are discrete, the transition function T : |S| × |A| × |S| can be parameterized by a 3-dim vector cube. Given a |S|-dim belief vector b t and a performed action a t , our first step is a dot production:b t = b t • T t | S |×| S | where T t | S |× | S | is a transition matrix sliced from T along action a t . Suppose there are G-dim observations and each dimension is independent with each other, so we have O(o t |s i , a t ) = G j O(o t [j]|s i , a t ), where O(o t [j]|s i , a t ) is the probability of observing j-th dimension in o t given state s i and action a t . Then our second step is an element-wise multiplication of b t with a vector O t | S | = [O(o t |s 1 , a t ), O(o t |s 2 , a t ), ...], that is b t = b t ⊗O t | S | . Our final step is followed by a normalized operation ρ: b t</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 1 :</head><label>1</label><figDesc>Performance under different parameter settings.</figDesc><table><row><cell></cell><cell cols="4">Parameter</cell><cell cols="2">Method</cell><cell cols="2">Revenue</cell><cell>Cost</cell><cell>ROI</cell><cell>Reward</cell></row><row><cell></cell><cell></cell><cell>-</cell><cell></cell><cell></cell><cell cols="2">Manual bid</cell><cell>100%</cell><cell></cell><cell>100%</cell><cell>100%</cell><cell>100%</cell></row><row><cell></cell><cell></cell><cell>-</cell><cell></cell><cell></cell><cell>Bandit</cell><cell></cell><cell cols="2">107.6%</cell><cell>99.6%</cell><cell>108.1%</cell><cell>112.8%</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>DQN</cell><cell></cell><cell>99.3%</cell><cell></cell><cell>99.1%</cell><cell>100.2%</cell><cell>101.4%</cell></row><row><cell></cell><cell></cell><cell>γ =0.1</cell><cell></cell><cell></cell><cell cols="2">EM-DQN ADRQN</cell><cell>92.2% 95.5%</cell><cell></cell><cell>91.1% 97.1%</cell><cell>101.2% 98.3%</cell><cell>103.3% 107.2%</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>DISA</cell><cell></cell><cell cols="2">100.3%</cell><cell>98.2%</cell><cell>102.2 %</cell><cell>107.9%</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>DQN</cell><cell></cell><cell cols="2">104.5%</cell><cell>100.9%</cell><cell>103.5%</cell><cell>112.9%</cell></row><row><cell></cell><cell></cell><cell>γ =0.3</cell><cell></cell><cell></cell><cell cols="2">EM-DQN ADRQN</cell><cell cols="2">105.1% 110.7%</cell><cell>101.2% 101.3%</cell><cell>103.8% 109.2%</cell><cell>107.8% 114.5%</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>DISA</cell><cell></cell><cell cols="2">111.7%</cell><cell>100.7%</cell><cell>110.9%</cell><cell>115.9%</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>DQN</cell><cell></cell><cell cols="2">110.3%</cell><cell>101.2%</cell><cell>109.0%</cell><cell>114.6%</cell></row><row><cell></cell><cell></cell><cell>γ =0.5</cell><cell></cell><cell></cell><cell cols="2">EM-DQN ADRQN</cell><cell cols="2">111.2% 112.9%</cell><cell>101.7% 102.7%</cell><cell>109.2% 110.0%</cell><cell>114.5% 116.7%</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>DISA</cell><cell></cell><cell cols="2">113.8%</cell><cell>101.1%</cell><cell>112.5%</cell><cell>117.5%</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>DQN</cell><cell></cell><cell cols="2">109.9%</cell><cell>101.5%</cell><cell>108.3%</cell><cell>113.3%</cell></row><row><cell></cell><cell></cell><cell>γ =0.7</cell><cell></cell><cell></cell><cell cols="2">EM-DQN ADRQN</cell><cell cols="2">109.9% 116.8%</cell><cell>101.0% 103.4%</cell><cell>108.8% 112.8%</cell><cell>112.8% 120.0%</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>DISA</cell><cell></cell><cell cols="2">119.0%</cell><cell>101.9%</cell><cell>116.7%</cell><cell>122.1%</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>DQN</cell><cell></cell><cell cols="2">112.3%</cell><cell>101.9%</cell><cell>110.1%</cell><cell>115.7%</cell></row><row><cell></cell><cell></cell><cell>γ =0.9</cell><cell></cell><cell></cell><cell cols="2">EM-DQN ADRQN</cell><cell cols="2">113.4% 117.0%</cell><cell>102.1% 103.0%</cell><cell>111.0% 113.6%</cell><cell>116.2% 121.0%</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>DISA</cell><cell></cell><cell cols="2">120.9%</cell><cell>100.7%</cell><cell>120.0%</cell><cell>125.2%</cell></row><row><cell></cell><cell>0.30 0.35 0.40</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>s1 s2 s3</cell><cell></cell><cell>0.73</cell><cell>Start</cell><cell>0.24</cell></row><row><cell>Probability</cell><cell>0.15 0.20 0.25</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">0.72</cell><cell>0.13</cell><cell>0.03</cell><cell>0.01</cell><cell>0.97</cell></row><row><cell></cell><cell>0.10</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>𝑠 "</cell><cell>0.99</cell><cell>𝑠 #</cell><cell>0.03</cell><cell>𝑠 $</cell></row><row><cell></cell><cell>0.05 0.00</cell><cell>0</cell><cell>2</cell><cell cols="2">4 pv in Good Items (pvgi) 6</cell><cell>8</cell><cell>10</cell><cell cols="2">Awareness</cell><cell>Interest 0.15</cell><cell>Search</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 2</head><label>2</label><figDesc>lists the learned parameters in Section 6.1.2 w.r.t these observations, so now we can interpret each state as following: State s 3 is an awareness state since the users under s 3 are observed to have little advertising exposure and clicks, particularly in Guess What You Like. (E[O(o 3 |s 3 )] ≈ 0, E[O(o 4 |s 3 )] ≈ 0). State s 2 is an interest state because we observe a large number of user browsing and click behaviors in this state, especially in Good Items (E[O(o 1 |s 2 )] = 8.41, E[O(o 2 |s 2</figDesc><table /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>a t -1 (o t )</figDesc><table><row><cell cols="2">Taking the log gives us:</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">log P(O, S|A; θ ) = log b 0 (s 1 )+</cell><cell>L</cell><cell>logT s t s t -1 ,a t -1 +</cell><cell>L</cell><cell>log O s t ,a t -1 (o t )</cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell>t =2</cell><cell>t =1</cell></row><row><cell cols="6">Plugging this into Q(θ, θ t ), we get</cell></row><row><cell>Q(θ, θ t ) =</cell><cell cols="5">log b 0 (s 1 )P(s, O|A; θ t )</cell></row><row><cell>s</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>+</cell><cell></cell><cell>L</cell><cell cols="3">logT s t s t -1 ,a t -1 P(s, O|A; θ t )</cell></row><row><cell></cell><cell>s</cell><cell>t =2</cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell>L</cell><cell></cell><cell></cell></row><row><cell>+</cell><cell></cell><cell></cell><cell cols="3">log O s t ,a t -1 (o t )P(s, O|A; θ t )</cell></row><row><cell></cell><cell>s</cell><cell>t =1</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head></head><label></label><figDesc>to represent the probability of observing o t +1 ∼ o L conditioned on s t and a t ∼ a L-1 ,</figDesc><table /><note><p><p><p><p><ref type="bibr" target="#b0">1</ref> </p>)O s 1 ,a 0 (o 1 ) Backward pass: Similarly, we use notations</p>β(s t ) (t &lt; L) β(s t , a t ) = P(o t +1 ∼ o T |s t , a t ∼ a L-1 ) = s t +1 β(s t +1 , a t +1 )T s t +1 s t ,a t O s t +1 ,a t (o t +1 )</p>where β</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head></head><label></label><figDesc>from J ; Update the state estimator θ e = 0.99 × θ e-1 + 0.01 × θ I ; Get the current observation o t and previous b t -1 , a t -1 ;</figDesc><table><row><cell>6</cell><cell>for i=1 to I do</cell></row><row><cell>7</cell><cell>θ i = arg max θ E p(s | O, A;θ i -1 ) log p(s, O|A; θ i-1 ) until log p(s, O|A; θ i-1 ) does not increase ;</cell></row><row><cell>8</cell><cell>end</cell></row><row><cell>9</cell><cell></cell></row><row><cell>10</cell><cell>Create a new user trajectory J for a category;</cell></row><row><cell>11</cell><cell>for t=1 to T do</cell></row><row><cell>12</cell><cell></cell></row></table><note><p>13 Perform belief updating b t = SE(b t -1 , a t -1 , o t ; θ e ); 14 With probability ϵ select a random action a t otherwise select a t = arg max a Q(b t ; η a ); 15 Execute a t and receive an reward r t , and store a transition ⟨b t , o t , a t , r t ⟩ into the trajectory J ; 16 Sample a minibatch of N transitions ⟨b j , a j , r j , b j+1 ⟩ from all trajectories in D; 17 Update η a by minimizing loss with Eq. (9) 18 end 19 Update the trajectory replay memory D = {D ∪ J } 20 end made to a user for a category in different scenarios. Let gw denotes the the subscript of all accumulated features in Guess What You Like while gi denotes that in Good Items, thus we have 6 more observation features: pv gw , pCVR gw , clk gw , pv gi , pCVR gi , clk gi . In total, we use a 31-dim vector to describe a data record, which includes item-related features, session-related features, and accumulated features.</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head></head><label></label><figDesc>. For the pv gw , we use the range [0, 11], and clk gw is mapped into a range of [0, 3]. The Batch training and execution in categorical level.</figDesc><table><row><cell>Item 1</cell><cell></cell><cell></cell><cell></cell><cell>𝑠𝑐𝑜𝑟𝑒1 × 𝛿1</cell><cell></cell></row><row><cell>Item 2 Item 3</cell><cell>Cate. 1 features</cell><cell>Batch Input</cell><cell>Boosting (𝛿1 &gt; 1 )</cell><cell>𝑠𝑐𝑜𝑟𝑒2 × 𝛿1 𝑠𝑐𝑜𝑟𝑒3 × 𝛿1</cell><cell></cell></row><row><cell>Item 4 Item 5</cell><cell>Cate. 2 features</cell><cell>Agent</cell><cell>Keep (𝛿2 = 1 )</cell><cell>𝑠𝑐𝑜𝑟𝑒4 × 𝛿2 𝑠𝑐𝑜𝑟𝑒5 × 𝛿2</cell><cell></cell></row><row><cell>Item 6 Item 7</cell><cell>Cate. 3 features</cell><cell>Output</cell><cell>Restrain (𝛿3 &lt; 1 )</cell><cell>𝑠𝑐𝑜𝑟𝑒6 × 𝛿3 𝑠𝑐𝑜𝑟𝑒7 × 𝛿3</cell><cell></cell></row><row><cell cols="2">Aggregate features in category-level</cell><cell></cell><cell cols="2">Revert rank score in item-level</cell><cell></cell></row><row><cell>Figure 9: Method</cell><cell>Parameter</cell><cell>Revenue</cell><cell>Cost</cell><cell>ROI</cell><cell>Reward</cell></row><row><cell>Manual bid</cell><cell>-</cell><cell>100%</cell><cell>100%</cell><cell>100%</cell><cell>100%</cell></row><row><cell></cell><cell>γ =0.1, n=5</cell><cell>100.3%</cell><cell>98.2%</cell><cell>102.2%</cell><cell>107.9%</cell></row><row><cell></cell><cell>γ =0.3, n=5</cell><cell>111.7%</cell><cell>100.7%</cell><cell>110.9%</cell><cell>115.9%</cell></row><row><cell></cell><cell>γ =0.5, n=5</cell><cell>113.8%</cell><cell>101.1%</cell><cell>112.5%</cell><cell>117.5%</cell></row><row><cell></cell><cell>γ =0.7, n=5</cell><cell>119.0%</cell><cell>101.9%</cell><cell>116.7%</cell><cell>122.1%</cell></row><row><cell>DISA</cell><cell>γ =0.9, n=5 γ =0.9, n=1</cell><cell>120.9% 110.6%</cell><cell>100.7% 100.2%</cell><cell>120.0% 110.3%</cell><cell>125.2% 113.3%</cell></row><row><cell></cell><cell>γ =0.9, n=2</cell><cell>117.1%</cell><cell>102.0%</cell><cell>114.8%</cell><cell>122.3%</cell></row><row><cell></cell><cell>γ =0.9, n=3</cell><cell>117.6%</cell><cell>101.6%</cell><cell>115.8%</cell><cell>122.7%</cell></row><row><cell></cell><cell>γ =0.9, n=4</cell><cell>119.5%</cell><cell>103.3%</cell><cell>115.6%</cell><cell>124.6%</cell></row><row><cell></cell><cell>γ =0.9, n=5</cell><cell>120.9%</cell><cell>100.7%</cell><cell>120.0%</cell><cell>125.2%</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 3 :</head><label>3</label><figDesc>Hyper-parameter tunning in DISA</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Usually, the final items L t K can be affected by recalling different ads D t in the matching stage or adjusting the ranking function f t in the sorting stage. In this paper, we only consider how to use f t to control the final displayed items.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_1"><p>A user may have multiple intents on different items, and we can feed the model with different items to get different intents.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_2"><p>Due to our settings in discrete-actions and memory replays, we do not consider continuous-action or asynchronous-specific RL techniques, such as DDPG, A3C, etc.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="7" xml:id="foot_3"><p>Each curve is smoothed on average, and the shaded area shows the standard deviation.</p></note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Media exposure through the funnel: A model of multi-stage attribution</title>
		<author>
			<persName><forename type="first">Vibhanshu</forename><surname>Abhishek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Fader</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kartik</forename><surname>Hosanagar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012">2012. 2012</date>
			<biblScope unit="page">2158421</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A neural networks committee for the contextual bandit problem</title>
		<author>
			<persName><forename type="first">Robin</forename><surname>Allesiardo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raphaël</forename><surname>Féraud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Djallel</forename><surname>Bouneffouf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="374" to="381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Computing optimal policies for partially observable decision processes using compact representations</title>
		<author>
			<persName><forename type="first">Craig</forename><surname>Boutilier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the National Conference on Artificial Intelligence</title>
		<meeting>the National Conference on Artificial Intelligence</meeting>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="1996">1996</date>
			<biblScope unit="page" from="1168" to="1175" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Real-time bidding by reinforcement learning in display advertising</title>
		<author>
			<persName><forename type="first">Han</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kan</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kleanthis</forename><surname>Malialis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yong</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Defeng</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth ACM International Conference on Web Search and Data Mining</title>
		<meeting>the Tenth ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="661" to="670" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Stabilizing reinforcement learning in dynamic environment with application to online recommendation</title>
		<author>
			<persName><forename type="first">Yang</forename><surname>Shi-Yong Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Da</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hai-Kuan</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hai-Hong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 24th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1187" to="1196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A tutorial on the cross-entropy method</title>
		<author>
			<persName><forename type="first">Pieter-Tjerk De</forename><surname>Boer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><forename type="middle">P</forename><surname>Kroese</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shie</forename><surname>Mannor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reuven</forename><forename type="middle">Y</forename><surname>Rubinstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annals of operations research</title>
		<imprint>
			<biblScope unit="volume">134</biblScope>
			<biblScope unit="page" from="19" to="67" />
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning to Collaborate: Multi-Scenario Ranking via Multi-Agent Reinforcement Learning</title>
		<author>
			<persName><forename type="first">Jun</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minlie</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shichen</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenwu</forename><surname>Ou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhirong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoyan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 World Wide Web Conference on World Wide Web. International World Wide Web Conferences Steering Committee</title>
		<meeting>the 2018 World Wide Web Conference on World Wide Web. International World Wide Web Conferences Steering Committee</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1939" to="1948" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Towards a digital attribution model: Measuring the impact of display advertising on online consumer behavior</title>
		<author>
			<persName><forename type="first">Anindya</forename><surname>Ghose</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vilma</forename><surname>Todri</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015. 2015</date>
			<biblScope unit="page">2672090</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Yujing</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Da</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anxiang</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinghui</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.00710</idno>
		<title level="m">Reinforcement Learning to Rank in E-Commerce Search Engine: Formalization, Analysis, and Application</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Reinforcement learning for slate-based recommender systems: A tractable decomposition and practical methodology</title>
		<author>
			<persName><forename type="first">Eugene</forename><surname>Ie</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vihan</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanmit</forename><surname>Navrekar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ritesh</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng-Tze</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Morgane</forename><surname>Lustman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vince</forename><surname>Gatto</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paul</forename><surname>Covington</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1905.12767</idno>
		<imprint>
			<date type="published" when="2019">2019. 2019</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bidding on the buying funnel for sponsored search and keyword advertising</title>
		<author>
			<persName><forename type="first">J</forename><surname>Bernard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Simone</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName><surname>Schuster</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Electronic Commerce Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2011">2011. 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Additional Multi-Touch Attribution for Online Advertising</title>
		<author>
			<persName><forename type="first">Wendi</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoling</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1360" to="1366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Junqi</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengru</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Gai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Weinan</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.09756</idno>
		<title level="m">Real-Time Bidding with Multi-Agent Reinforcement Learning in Display Advertising</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Qmdp-net: Deep learning for planning under partial observability</title>
		<author>
			<persName><forename type="first">Peter</forename><surname>Karkus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wee</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lee</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4694" to="4704" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Unsupervised learning of probabilistic models for robot navigation</title>
		<author>
			<persName><forename type="first">Sven</forename><surname>Koenig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Reid</forename><forename type="middle">G</forename><surname>Simmons</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings., 1996 IEEE International Conference on</title>
		<meeting>1996 IEEE International Conference on</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1996">1996. 1996</date>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="2301" to="2308" />
		</imprint>
	</monogr>
	<note>Robotics and Automation</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Constructing states for reinforcement learning</title>
		<author>
			<persName><surname>Mahmud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Machine Learning (ICML-10</title>
		<meeting>the 27th International Conference on Machine Learning (ICML-10</meeting>
		<imprint>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="727" to="734" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Reinforcement learning with selective perception and hidden state</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Kachites</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mccallum</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Dana</forename><surname>Ballard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996">1996</date>
		</imprint>
		<respStmt>
			<orgName>University of Rochester. Dept. of Computer Science</orgName>
		</respStmt>
	</monogr>
	<note>Ph.D. Dissertation.</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Overcoming incomplete perception with utile distinction memory</title>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Tenth International Conference on Machine Learning</title>
		<meeting>the Tenth International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="1993">1993</date>
			<biblScope unit="page" from="190" to="196" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Optimal recommendation to users that react: Online learning for a class of POMDPs</title>
		<author>
			<persName><forename type="first">Rahul</forename><surname>Meshram</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aditya</forename><surname>Gopalan</surname></persName>
		</author>
		<author>
			<persName><surname>Manjunath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Decision and Control (CDC)</title>
		<imprint>
			<date type="published" when="2016">2016. 2016</date>
			<biblScope unit="page" from="7210" to="7215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Human-level control through deep reinforcement learning</title>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="issue">7540</biblScope>
			<biblScope unit="page">529</biblScope>
			<date type="published" when="2015">2015. 2015. 2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">A survey of POMDP solution techniques</title>
		<author>
			<persName><forename type="first">Kevin P</forename><surname>Murphy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000. 2000</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">Steven</forename><surname>Noble</surname></persName>
		</author>
		<ptr target="http://www.forrester.com/rb/Research/time_to_bury_marketing_funnel/q/id/574952" />
		<title level="m">It&apos;s time to bury the marketing funnel</title>
		<imprint>
			<date type="published" when="2010">2010. 2010</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Approximating optimal policies for partially observable stochastic domains</title>
		<author>
			<persName><forename type="first">Ronald</forename><surname>Parr</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stuart</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In IJCAI</title>
		<imprint>
			<biblScope unit="volume">95</biblScope>
			<biblScope unit="page" from="1088" to="1094" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Reinforcement learning using approximate belief states</title>
		<author>
			<persName><forename type="first">Ronald</forename><surname>Andres C Rodriguez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daphne</forename><surname>Parr</surname></persName>
		</author>
		<author>
			<persName><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2000">2000</date>
			<biblScope unit="page" from="1036" to="1042" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Bayes-adaptive pomdps</title>
		<author>
			<persName><forename type="first">Stephane</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brahim</forename><surname>Chaib-Draa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joelle</forename><surname>Pineau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="page" from="1225" to="1232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Finding approximate POMDP solutions through belief compression</title>
		<author>
			<persName><forename type="first">Nicholas</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sebastian</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of artificial intelligence research</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="1" to="40" />
			<date type="published" when="2005">2005. 2005</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Data-driven multi-touch attribution models</title>
		<author>
			<persName><forename type="first">Xuhui</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lexin</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 17th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="258" to="264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Virtual-Taobao: Virtualizing Real-world Online Retail Environment for Reinforcement Learning</title>
		<author>
			<persName><forename type="first">Jing-Cheng</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Da</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shi-Yong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">An-Xiang</forename><surname>Zeng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1805.10000</idno>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">LSTM neural networks for language modeling</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Sundermeyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ralf</forename><surname>Schlüter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirteenth annual conference of the international speech communication association</title>
		<imprint>
			<date type="published" when="2012">2012</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Budget constrained bidding by model-free reinforcement learning in display advertising</title>
		<author>
			<persName><forename type="first">Di</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiujun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xun</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qing</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaoxun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Gai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th ACM International Conference on Information and Knowledge Management</title>
		<meeting>the 27th ACM International Conference on Information and Knowledge Management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="1443" to="1451" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Sequential selection of correlated ads by POMDPs</title>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st ACM international conference on Information and knowledge management</title>
		<meeting>the 21st ACM international conference on Information and knowledge management</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="515" to="524" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deepintent: Learning attentions for online advertising with recurrent neural networks</title>
		<author>
			<persName><forename type="first">Shuangfei</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keng-Hao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruofei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhongfei</forename><forename type="middle">Mark</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining</title>
		<meeting>the 22nd ACM SIGKDD international conference on knowledge discovery and data mining</meeting>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1295" to="1304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<author>
			<persName><forename type="first">Han</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengye</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guozheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Han</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kun</forename><surname>Gai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.02294</idno>
		<title level="m">Learning Tree-based Deep Model for Recommender Systems</title>
		<imprint>
			<date type="published" when="2018">2018. 2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pascal</forename><surname>Poupart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guanghui</forename><surname>Miao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.06309</idno>
		<title level="m">On improving deep reinforcement learning for pomdps</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
