<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Scalable Font Reconstruction with Dual Latent Manifolds</title>
				<funder ref="#_zPGNbq7 #_TcnuWuR">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
				<funder ref="#_KdXVqBt">
					<orgName type="full">NEH</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Nikita</forename><surname>Srivatsan</surname></persName>
							<email>nsrivats@cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science and Engineering</orgName>
								<orgName type="institution" key="instit1">Language Technologies Institute Carnegie Mellon University</orgName>
								<orgName type="institution" key="instit2">Khoury College of Computer Science Northeastern University</orgName>
								<orgName type="institution" key="instit3">University of California</orgName>
								<address>
									<settlement>San Diego</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Si</forename><surname>Wu</surname></persName>
							<email>siwu@ccs.neu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science and Engineering</orgName>
								<orgName type="institution" key="instit1">Language Technologies Institute Carnegie Mellon University</orgName>
								<orgName type="institution" key="instit2">Khoury College of Computer Science Northeastern University</orgName>
								<orgName type="institution" key="instit3">University of California</orgName>
								<address>
									<settlement>San Diego</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
							<email>barron@google.com</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science and Engineering</orgName>
								<orgName type="institution" key="instit1">Language Technologies Institute Carnegie Mellon University</orgName>
								<orgName type="institution" key="instit2">Khoury College of Computer Science Northeastern University</orgName>
								<orgName type="institution" key="instit3">University of California</orgName>
								<address>
									<settlement>San Diego</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Science and Engineering</orgName>
								<orgName type="institution" key="instit1">Language Technologies Institute Carnegie Mellon University</orgName>
								<orgName type="institution" key="instit2">Khoury College of Computer Science Northeastern University</orgName>
								<orgName type="institution" key="instit3">University of California</orgName>
								<address>
									<settlement>San Diego</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Scalable Font Reconstruction with Dual Latent Manifolds</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-28T23:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We propose a deep generative model that performs typography analysis and font reconstruction by learning disentangled manifolds of both font style and character shape. Our approach enables us to massively scale up the number of character types we can effectively model compared to previous methods. Specifically, we infer separate latent variables representing character and font via a pair of inference networks which take as input sets of glyphs that either all share a character type, or belong to the same font. This design allows our model to generalize to characters that were not observed during training time, an important task in light of the relative sparsity of most fonts. We also put forward a new loss, adapted from prior work that measures likelihood using an adaptive distribution in a projected space, resulting in more natural images without requiring a discriminator. We evaluate on the task of font reconstruction over various datasets representing character types of many languages, and compare favorably to modern style transfer systems according to both automatic and manually-evaluated metrics.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The majority of written natural language comes to us in the form of glyphs, visual representations of characters generally rendered in a font with a contextually appropriate style. In order to be legible a glyph must be recognizable as its corresponding character from its underlying shape, but it must also be stylistically consistent with the other glyphs in that font. While this labor intensive process is typically performed manually by human artists, the number of character types that a font may be expected to support is extremely large, with Unicode 13.0.0 including as many as 143,859 character types (Unicode). As a result, graphic designers often create glyphs only for a subset of these characters, which tends to be determined by their own cultural context. This can create an accessibility gap for users seeking to create or read digital content in languages with less widespread orthographies, due to the relative lack of available options. Figure <ref type="figure" target="#fig_0">1</ref> shows that within the Google Fonts library <ref type="bibr">(Google)</ref> there is a long tail of fonts with a large proportion of missing glyphs.</p><p>Font reconstruction is a task that attempts to solve this problem. The goal is for a model, given a small set of example glyphs from an incomplete font, to generate glyphs for the remaining characters in a consistent style. Prior work has approached this in various ways, albeit with some limitations. While some approaches use a variational framework <ref type="bibr">(Srivat-san et al., 2019;</ref><ref type="bibr" target="#b20">Lopes et al., 2019)</ref>, generally their models only treat the font style as a latent variable, and not the character shape. Such methods can therefore only handle a small, fixed, and an a priori known set of character types. Other work such as that by by <ref type="bibr" target="#b36">Zhang et al. (2018)</ref>; <ref type="bibr" target="#b11">Gao et al. (2019)</ref> use discriminative models which dynamically compute both embeddings, allowing them to generalize to unseen characters. However these networks typically require a pre-specified number of observations as input, and their lack of a probabilistic prior can lead to learning a brittle manifold on datasets with a large number of infrequently observed characters.</p><p>By contrast, our method learns two smooth manifolds over character shape and font style in order to better share parameters across structurally similar characters, letting it scale to a larger set and more effectively generalize to characters never seen during training. Our model treats font reconstruction as a matrix factorization problem, where we view our corpus as a matrix with rows corresponding to character type, and columns corresponding to fonts. Each row and column is assigned a latent variable that determines its structure or style respectively. A decoder network consisting of transposed convolutional layers parameterizes the model's distribution on each cell in that matrix, i.e. an image of a glyph, conditioned on the corresponding row and column embeddings. This approach can be thought of as a generalization of <ref type="bibr" target="#b27">Srivatsan et al. (2019)</ref>, who used a similar factorization framework, but with only one manifold over font style.</p><p>In addition to model structure, the loss function is also important in font reconstruction as pixel independent losses like L 2 tend to produce blurry output, reflecting an averaged expectation instead of something realistic. Some have used generative adversarial networks (GANs) to mitigate this <ref type="bibr" target="#b0">(Azadi et al., 2018)</ref>, but these can suffer from missing modes and collapse issues. We instead introduce a novel adaptive loss to font reconstruction that operates on a wavelet image representation, while still permitting a well formed likelihood. Specifically, in this paper we make the following contributions: (1) Propose the "Dual Manifold" model which treats both style and structure representations as latent variables (2) Propose a new adaptive loss function for synthesizing glyphs, and demonstrate its improvements over more common losses (3) Put forward two datasets that emphasize few-shot reconstruction, along with a preprocessing technique to remove near-duplicate fonts resulting in more challenging train/test splits.</p><p>We evaluate on the task of few-shot font reconstruction, reporting the structural similarity (SSIM) -a popular metric for image synthesis better correlated with human judgement than L 2 <ref type="bibr" target="#b26">(Snell et al., 2017)</ref> -between reconstructions and a gold reference. These experiments are further split into known characters, which the model observed in at least one font at train time, and unknown characters, which can be thought of as a few-shot task. In addition we also perform human evaluation using Amazon Mechanical Turk. Our approach outperforms various baselines including nearest neighbor, the single manifold approach we build on <ref type="bibr" target="#b27">(Srivatsan et al., 2019)</ref>, and the previously mentioned discriminative model <ref type="bibr" target="#b36">(Zhang et al., 2018)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>A variety of style transfer work has focused specifically on font style, and therefore, font reconstruction. Some approaches have sought to model the style aspect as a transformation on an underlying topological or stroke-based representation which must be learned for each character <ref type="bibr" target="#b3">(Campbell and Kautz, 2014;</ref><ref type="bibr" target="#b23">Phan et al., 2015;</ref><ref type="bibr" target="#b28">Suveeranont and Igarashi, 2010)</ref>. However this requires characters to have consistent topologies across fonts. Other work has learned a font skeleton manifold using Gaussian Process Latent Variable Models <ref type="bibr" target="#b19">(Lian et al., 2018)</ref>. One of the more philosophically similar approaches to ours is the bilinear factorization model of <ref type="bibr" target="#b10">Freeman and Tenenbaum (Freeman and Tenenbaum, 1997;</ref><ref type="bibr" target="#b29">Tenenbaum and Freeman, 2000)</ref> which also learns vector representations for each font and character type, albeit in a non-probabilistic and linear manner. Some more recent research has treated font reconstruction as a discriminative task, using modern neural architectures and techniques from the style transfer literature <ref type="bibr" target="#b36">(Zhang et al., 2018</ref><ref type="bibr" target="#b37">(Zhang et al., , 2020;;</ref><ref type="bibr" target="#b0">Azadi et al</ref>  2018; <ref type="bibr" target="#b11">Gao et al., 2019)</ref>. Furthermore the concept of learning manifolds for Chinese characters based on shared structure has also been studied <ref type="bibr" target="#b4">(Cao et al., 2018)</ref>, albeit with different downstream goals. Lopes et al. ( <ref type="formula">2019</ref>) used VAEs which do not observe font alignment across glyphs, but condition on the character type (this work also primarily focuses on generating vector instead of pixel representations). Finally, more general-purpose style transfer methods for images are well explored <ref type="bibr" target="#b12">(Gatys et al., 2015;</ref><ref type="bibr" target="#b34">Yang et al., 2019;</ref><ref type="bibr" target="#b15">Johnson et al., 2016;</ref><ref type="bibr" target="#b33">Wang and Gupta, 2016;</ref><ref type="bibr" target="#b16">Kazemi et al., 2019;</ref><ref type="bibr" target="#b5">Chen et al., 2017;</ref><ref type="bibr">Ulyanov et al., 2016a)</ref>, although these largely lack inductive biases specially suited to typography.</p><p>3 Dual Manifold Model <ref type="bibr" target="#b27">Srivatsan et al. (2019)</ref> is the most similar prior work, as it also builds from a matrix factorization framework, and learns a latent manifold over font embeddings. Our model generalizes theirs by learning a second manifold over character shape, letting us massively scale up the number of characters that can be modeled. In Section 4 we also describe our novel loss.</p><p>Figure <ref type="figure" target="#fig_1">2</ref> depicts our model's generative process. For a corpus consisting of J fonts, each defined over up to I character types, we characterize each particular glyph image as a combination of properties relating to the style of that particular font and to the shape of that character. Our model effectively factorizes the data by assigning a vector representation to every row and column which correspond to char-acter and font respectively. Therefore, our approach works by leveraging the fact that all glyphs of the same character type (i.e. an entire row in our data) share the same underlying structural shape, and all glyphs within the same font (i.e. an entire column) share the same stylistic properties. By forming separate representations over each of these two axes of variation, we can reconstruct missing glyphs in our data by separately inferring the relevant row and column variables, and then pushing new combinations of those inferred variables through our generative process. This can be thought of as a form of matrix completion, where unobserved entries correspond to characters not supported by particular fonts.</p><p>Given a corpus X consisting of I characters across J fonts, we assign to each observed glyph X ij a pair of latent variables which model the properties of that glyph's character type and font style. Specifically we define these as Y i ∈ R k and Z j ∈ R k , which we draw from a standard Gaussian prior N (0, I k ), with Y modeling the shape of the character (e.g. a q or &lt;), and Z modeling the properties of the font (e.g. Times New Roman or Roboto Light Italic). Given a particular Y i and Z j , we combine them via a neural decoder to obtain a distribution p(X ij |Y i , Z j ; θ) which scores the corresponding glyph image X ij . This yields the following likelihood function:</p><formula xml:id="formula_0">p(X, Y, Z; θ) = ∏ I p(Y i ) ∏ J p(Z j ) ∏ I,J p(X ij |Y i , Z j ; θ)</formula><p>Both Y and Z are unobserved, and we must therefore infer both to train our model and pro-duce reconstructions at test time. Note that by contrast, <ref type="bibr" target="#b27">Srivatsan et al. (2019)</ref> represents characters as fixed parameters, and must only perform inference over font representations.</p><p>We use a pair of encoder networks to perform amortized inference, as depicted in Figure <ref type="figure">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Decoder Architecture</head><p>The basic structure of our decoder is largely identical to the popular U-Net architecture <ref type="bibr" target="#b25">(Ronneberger et al., 2015)</ref> which has seen much success on image generation tasks with its coarse-to-fine layout of transposed convolutional layers. However, we make a few key modifications (depicted in Figure <ref type="figure" target="#fig_1">2</ref>) in order to imbue our decoder with stronger inductive bias for this particular task. Following Srivatsan et al. ( <ref type="formula">2019</ref>), instead of directly parameterizing the transposed convolutional layers that appear within each block of the network, we allow the weights of each layer to be the output of an MLP that takes as its input the font variable Z j . This is effectively a form of HyperNetwork <ref type="bibr" target="#b14">(Ha et al., 2016)</ref>, a framework in which one network is used to produce the weights of another. In this way, the parameters of the transposed convolutional layers are dynamically chosen based on the font variable. By contrast, Y i is the input fed in at the top of the decoder, to which these filters are applied. The purpose of this asymmetry is to encourage Z j to learn properties relating to finer stylistic information, while Y i learns more spatial information about the characters. In another manner of speaking, Y i should learn "what" to write, and Z j should learn "how" to write it.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Adaptive Wavelet Loss</head><p>Our decoder architecture outputs a grid of values, but an important decision is what distribution (and therefore loss) these should finally parameterize to score actual pixels. Traditional approaches using variational autoencoders have modeled each pixel as an independent Normal distribution, which results in the model minimizing the L 2 loss between its output and gold. This however leads to oversmoothed images, as it treats adjacent pixels as independent despite their strong correlations (Bell and Sejnowski, 1997), and fails to account for the heavy-tailed distribution of oriented edges in natural images <ref type="bibr" target="#b9">(Field, 1987)</ref>. As a result L 2 penalizes the model for generating images that are realistic but slightly transposed or otherwise not perfectly aligned with gold, which encourages models to produce fuzzy edges in order to be closer on average. GANs are often employed to force sharper output <ref type="bibr" target="#b0">(Azadi et al., 2018;</ref><ref type="bibr" target="#b11">Gao et al., 2019)</ref>, but following recent work we instead use a projected loss for a similar effect. At a high level, our approach will first project images to a feature space, and let the model's output parameterize a distribution on this projection. If that projection is invertible and volume-preserving, this is equivalent to directly parameterizing a distribution on pixels, but allows for more expressivity <ref type="bibr" target="#b24">(Rezende and Mohamed, 2015;</ref><ref type="bibr" target="#b7">Dinh et al., 2014</ref><ref type="bibr" target="#b8">Dinh et al., , 2016))</ref>. Ideally, such a loss requires a distribution expressive enough to capture the variable frequency characteristics of natural images, and a representation of the image that explicitly reasons about spatially-localized edges.</p><p>A good example of this technique is that of <ref type="bibr" target="#b27">Srivatsan et al. (2019)</ref>, which modeled images by placing a Cauchy distribution on a 2D Discrete Cosine Transform (DCT) representation of glyphs. Though this is an improvement over the default choice of placing a Normal distribution on individual pixels as it both decorrelates pixels and is tolerant of outliers, this approach is limited in its expressiveness and its ability to model spatially localized edges: Cauchy distributions are excessively heavy-tailed and so have difficulty modeling inliers, and since DCT is a global representation it does not allow the model to reason about where image gradient content is located. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A</head><p>We extend this approach in two ways (as depicted above): (1) by using a wavelet image representation instead of DCT, and (2) by using a distribution with an adaptive shape instead of a Cauchy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Representation</head><p>We opt for a wavelet representation, as unlike DCT it jointly encodes the frequency and spatial location of an image feature. As might be expected, an image representation in which location is directly encoded is helpful in our task; a stroke has a fundamentally different meaning at the top of a character than at the bottom. <ref type="bibr" target="#b1">Barron (2019)</ref> quantitatively demonstrated the advantages of specifically the Cohen-Daubechies-Feauveau (CDF) 9/7 wavelet decomposition <ref type="bibr" target="#b6">(Cohen et al., 1992)</ref> for training likelihood-based models of natural images. Based on their findings that CDF 9/7 in front of an adaptive loss achieves better performance than DCT in front of a Cauchy (the setup of Srivatsan et al. ( <ref type="formula">2019</ref>)), we expect similar performance benefits in the context of our own model, and our ablations in Table <ref type="table" target="#tab_1">1</ref> (Right) support this belief empirically.</p><p>Distribution An adaptive distribution lets the model select between using leptokurtotic (Cauchy-like) distributions that are well suited to the high-frequency image edges found at the finer levels of the wavelet decomposition, or more platykurtic (Normal-like) distributions that are better suited to low-frequency DClike average image intensities found at the coarsest levels of the wavelet decomposition. Specifically, we use the probability distribution of <ref type="bibr" target="#b1">Barron (2019)</ref>:</p><formula xml:id="formula_1">f (x|µ, σ, α) = exp ( - |α-2| α ( ( (x-µ) 2 σ 2 |α-2| +1 ) α/2 -1</formula><p>))</p><formula xml:id="formula_2">σZ(α)</formula><p>where Z(α) is the distribution's partition function, and α determines the distribution's shape. As α → 0 the distribution approaches a Cauchy distribution, as α → 2 the distribution approaches a Normal distribution. Taken together, these yield a conditional likelihood function parameterized by the decoder of our variational model, which we now describe. Given an image X i,j , we first project it using the CDF 9/7 wavelet decomposition -which we denote as ψ(X i,j ). Because this decomposition is a biorthogonal volumepreserving transformation, it can be applied before the likelihood computation. It further serves as a whitening transformation, avoiding the need to learn a covariance matrix for X i,j .</p><p>Our decoder outputs a grid of parameters Xi,j , the projection of which serves as the mean µ of our adaptive distribution for scoring ψ(X i,j ). For the other distribution parameters σ and α, rather than using fixed settings we construct a set of latent variables for both: we allow each wavelet coefficient to have its own vector of latent shape parameter ℓ α and scale parameter ℓ σ , where the non-latent shape and scale are parameterized as scaled and shifted sigmoids and softplus of those latent values:</p><formula xml:id="formula_3">α k = 2 1+exp(ℓ α k ) , σ k = log(1+exp(ℓ α k )) log(2)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>+ ϵ</head><p>We initialize ℓ α = ℓ σ = ⃗ 0, thereby initializing α = σ = ⃗ 1. These latent variables (ℓ α , ℓ σ ) are optimized during training using gradient descent along with all other model parameters θ, which allows the model to adapt the shape and scale of each wavelet coefficient's distribution during training. Overall, this yields the following likelihood function:</p><formula xml:id="formula_4">p(X i,j |Y i , Z j ; θ) = ∏ k f (ψ(X i,j ) k | ψ( Xi,j ) k , σ k , α k )</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Learning and Inference</head><p>We now describe our approach to training this model. This process mirrors that of previous variational work, although since we are learning a dual manifold, our model will require two separate inference networks. The projected loss we add (Section 4) will not fundamentally affect the learning process, but does change how the reconstruction term is computed.</p><p>As our model is generative, we wish to maximize the log likelihood of the training data with respect to the model parameters, which requires summing out the unobserved variables Y and Z. However, this integral is intractable and does not permit a closed form solution. We therefore resort to optimizing a variational approximation, a strategy which has seen success in similar settings <ref type="bibr" target="#b18">(Kingma and Welling, 2014;</ref><ref type="bibr" target="#b27">Srivatsan et al., 2019)</ref>. Rather than directly optimize the likelihood (which we cannot compute the gradient of), we maximize a lower bound on it known as the Evidence Lower Bound (ELBO). We compute the ELBO via a function q(Y, Z|X) = q(Y |X) * q(Z|Y, X) which approximates the posterior p(Z, Y |X) of the distribution defined by our decoder network.</p><formula xml:id="formula_5">ELBO = E q [log p(X|Z, Y )] -KL(q(Z, Y |X)||p(Z)p(Y ))</formula><p>Figure <ref type="figure">3</ref>: Overview of the inference procedure. First the character encoder infers a representation of structure over each row, and then the font encoder infers a representation of style conditioned on a (perhaps partially observed) column and the character embeddings.</p><p>We define q(Y |X) and q(Z|Y, X) via a pair of encoder networks which operate over one row or column of the matrix respectively. An encoder passes each glyph in that row or column through a series of convolutional layers, and then max pools the output features across all glyphs, ensuring it can handle a variable number of observations (See Figure <ref type="figure">3</ref>). Note that the method of pooling (e.g. min, max, avg), as well as the order in which to infer Y and Z are important choice points that allow for different inductive biases. The pooled feature representation is then passed through an MLP which outputs parameters µ and Σ to define a Gaussian posterior over Y i or Z j . Given these, we compute approximate gradients on the ELBO via the reparameterization trick described by <ref type="bibr" target="#b18">Kingma and Welling (2014)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>We evaluate on the task of font reconstruction, in which given a small random subset of glyphs from a held out font, models must reconstruct the remaining ones. We separately report performance on known (i.e. observed at least once during training) and unknown character types. During training, we mask out a randomly chosen 20% of character types to serve as unknowns. At test time, models observe examples of previously masked characters to infer their representations for reconstruction. This can be thought of as a few-shot task, where models must generate glyphs for character types they did not observe at train time based on limited test-time examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Datasets</head><p>Capitals64, the dataset used by <ref type="bibr" target="#b0">Azadi et al. (2018)</ref> and <ref type="bibr" target="#b27">Srivatsan et al. (2019)</ref>, only con-tains the 26 English capital letters, with no missing characters, meaning it does not require learning a manifold over character shape. We instead evaluate on the following datasets to best demonstrate our method's ability to scale to settings with a large number of character types and a high degree of sparsity.</p><p>Google Fonts Google Fonts is a dataset of 991 font families, which is publicly available<ref type="foot" target="#foot_0">foot_0</ref> . Most fonts in the dataset support standard Latin characters, but many also support special symbols, and characters found in Greek, Cyrillic, Tamil, and several other orthographies. A visualization of this is shown in Figure <ref type="figure" target="#fig_0">1</ref>. We restrict our work to the 2000 most frequently supported character types for simplicity. After removing near duplicates (described below) we are left with 2017 fonts in total, split into train, dev, and test in a 3 : 1 : 1 ratio. The data was split by font family rather than individual fonts, to ensure that there are no fonts in train with a "sibling" in test.</p><p>Chinese Simplified We scraped a list of the most common 2000 Chinese simplified characters from the internet as well as a dataset that labels each character's radical. Together, we compile a new dataset that consists of the most common 2000 Chinese characters along with their radicals for further analysis on the character embeddings. For each Chinese character, we scraped over 1524 fonts, split similarly to Google Fonts. The total font number shrinks down to 623 after removing near duplicates, which we now discuss.</p><p>Removing near duplicates One major issue with font corpora is that most fonts belong to a small handful of modes, within which there is little stylistic diversity. To ensure that our metrics best measure generalization to novel fonts unlike those seen in train, we preprocess out fonts that are extremely similar to others in the data. We first perform agglomerative clustering, and then retain only the centroid of each cluster. The number of clusters is determined by cutting the dendrogram at a height which eliminates most fonts that are to a human largely indistinguishable from their nearest neighbor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Baselines</head><p>We compare our model -which we refer to as Dual Manifold -to two baselines and various ablations. Our primary baseline is EMD <ref type="bibr" target="#b36">(Zhang et al., 2018)</ref>, a discriminative encoder-decoder model that does not share embeddings across "rows" and "columns", but rather computes style and content representations for each glyph given a set of provided examples, and then passes them to a generator which constructs the final image. This model is useful for comparison as it has a similar computation graph and also learns separate embeddings for font and character shape, but computes its loss directly in pixel space, and lacks a probabilistic prior.</p><p>We also compare to a naive nearest neighbor (NN) model, which reconstructs fonts at test time by finding the font in train with the closest L 2 distance over the observed characters, and outputs that neighbor's corresponding glyphs for the missing characters. If the neighbor does not support all missing characters, we pull the remaining from the 2nd nearest neighbor, and so on. It should be noted that NN cannot reconstruct any character that is not present in train.</p><p>Similarly to EMD, the first of our ablations, denoted -KL, does not explicitly model the character and font embeddings as random variables. This effectively removes the KL divergence from the loss function, resulting in a non-probabilistic autoencoder. The next, denoted -Dual, is an ablation which treats the character representations as parameters of the model, rather than latent variables which must be inferred. This is essentially the model of <ref type="bibr" target="#b27">Srivatsan et al. (2019)</ref> with our architecture*. We also ablate our adaptive wavelet loss against the DCT + Cauchy loss used by <ref type="bibr" target="#b27">Srivatsan et al. (2019)</ref>, denoted with -Adapt. Finally, we compare performing Max or Min pooling over elements of a row/column within the encoder network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Training Details</head><p>We perform stochastic gradient descent using Adam <ref type="bibr" target="#b17">(Kingma and Ba, 2015)</ref>, with a step size of 10 -4 . Batches contain 10 fonts, each with only 20 random characters observable to encourage robustness to the number of inputs. However at test time, the model can infer the character representations Y based on the the entire training set. We find best results when both character and style representations are k = 256 dimensional. See Appendix A for a full description of architecture.</p><p>Our model trains on one NVIDIA 2080ti GPU in roughly a week, and is implemented in Py-Torch <ref type="bibr" target="#b22">(Paszke et al., 2017)</ref> version 1.3.1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Metrics</head><p>We measure average SSIM per glyph <ref type="bibr" target="#b0">(Azadi et al., 2018;</ref><ref type="bibr" target="#b11">Gao et al., 2019)</ref>, having scaled pixel intensities to [0, 1]. While the details of SSIM are beyond the scope of this paper, it can be thought of as a feature-based metric that does not factor over individual pixels, but rather looks at the matches between higher level features regarding the structure of the image. SSIM is widely used in image processing tasks since it measures structural similarity instead of raw pixel distance, and has been shown to better correlate with human judgement than L 2 <ref type="bibr" target="#b26">(Snell et al., 2017)</ref>. Evaluating models using L 2 can reward unrealistic reconstructions that split the difference between many hypotheses as opposed to picking just one (part of the reason we avoid training our model on such a loss). Over the course of individual training runs, we found it was almost counter correlated with human judgement, with the lowest distance early in training while output was blurry, becoming larger as the model converged. We do however include these numbers in Appendix B, as they nonetheless support our findings.</p><p>We also perform human evaluation using Amazon Mechanical Turk. For each font in our test set, 5 turkers were shown 8 example glyphs, and a sample of reconstructions for the remaining characters by Dual Manifold and EMD. Turkers were also shown examples of   each character in a neutral style. They were asked to select which if either reconstruction was better, and briefly justify their reasoning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">Quantitative Evaluation</head><p>We list SSIM results in Table <ref type="table" target="#tab_1">1</ref> for various numbers of observed characters. Note that NN is not capable of reconstructing character types not observed at training time. On Google Fonts, our model performs best overall; however on Chinese Simplified, we see NN winning on known characters, as well as a marked drop in SSIM overall. This could be due to the increased challenge in generating Chinese characters given the relatively higher number of strokes, leading SSIM to prefer the realism of NN, or because fonts in this dataset generally contain most characters, unlike Google Fonts which is much sparser. Observing more characters taperingly increases similarity, which matches our intuition that this allows for a better understanding of stylistic properties. Performance drops when evaluating on characters not observed in training. This makes sense as models may have less support in their manifolds for structural forms they were not trained on, but the drop is small enough to suggest our model is able to infer meaningful representations for novel character types at test time.</p><p>We see also that EMD has significant issues at 16 and 32 observed characters (it's worth noting that EMD must be separately trained for each number of observations). Qualitatively, we find certain fonts for which EMD emits the same output for every character in that font. We suspect this indicates overfitting leading to broken style representations for some novel fonts when given more observations than its default of only 10. Within our ablations, we find that using a dual latent manifold, as opposed to treating character embeddings as model parameters, is responsible for the majority of our gain in SSIM over prior work. The next largest difference comes from using either Min pooling within the autoencoder or Max pooling. We also see more of a drop in performance from removing the KL divergence, than we do from replacing our adaptive wavelet loss with the DCT + Cauchy loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">Human Evaluation</head><p>In our AMT experiments, we found that for 48.2% of known character reconstructions, turkers preferred our model's output, with 42.0% preferring EMD, and 9.8% finding both equal. For unknown character reconstructions, 50.5% preferred ours, vs 38.7% for EMD, and 10.9% finding no difference. A majority of turkers agreed 86.3% of the time in the case of known characters, and 83.2% for unknown.</p><formula xml:id="formula_6">Others 亻 口 扌 木 氵 Others 心 火 王 纟 马 Others 丨 丿 刂 攵 木</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.1">Qualitative Inspection</head><p>In Figure <ref type="figure" target="#fig_4">4</ref> we show output from our model interpolating between a bold font and a light one, as well as a capital E and a Σ simultaneously. This demonstrates the smoothness of our manifolds and also suggests how they might offer support for font and character types not seen during training. Figure <ref type="figure" target="#fig_5">5</ref> shows examples of reconstructions by models on two fonts for a variety of both known and unknown characters. Our approach is more coherent and faithful than EMD, and NN is realistic but often stylistically incorrect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.2">Chinese Radicals</head><p>Figure <ref type="figure" target="#fig_6">6</ref> shows t-SNE projections <ref type="bibr" target="#b21">(Maaten and Hinton, 2008;</ref><ref type="bibr" target="#b32">Van Der Maaten, 2014)</ref> of learned character embeddings colored by their radical, a sub-component of Chinese characters. Radicals like 丨, 丿, and 火 -which either share forms with others or can occur in different structures -don't cluster together, while unique radicals like 心, 刂, and 攵 do.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">Conclusion</head><p>In this work introduced a generative model for typography capable of reconstructing characters in a novel font, of a novel shape, or both, and demonstrated its improvements over previous approaches on two datasets containing large numbers of characters. We analyzed the results qualitatively, and inspected learned manifolds for smoothness.</p><p>In future work, this methodology has potential value not just to fonts, but to any domain which can also be factored over independent axes of variation, such as handwriting by different authors. One could also incorporate this model into more complex downstream tasks such as OCR. That being said, these domains also feature complex interactions between physically adjacent glyphs (our model treats different characters within a font as conditionally independent), so some further innovation would likely still be required.</p><p>There are also extensions to the model itself that might be worth exploring in future work, for instance operating on a stroke-based representation in order to perform reconstruction in the original TTF space instead of raw pixel space as we do here. This would also likely assist with smoothness of edges and reduce the incidence of "corroded"output glyphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Broader Impact</head><p>As our work can be used to augment or even replace the labor of human artists, it is worth discussing its potential broader impacts. The most obvious positive is that this technique can add value to font designers, by minimizing the overhead required to design a font that supports widespread internationalization. Our model's ability to interpolate stylistic properties can also make it easy to automatically generate completely novel fonts that are roughly similar to existing ones.</p><p>This also benefits speakers of languages that rely on less common glyphs, as it broadens their font selection. It can make it easier for them to both produce and consume digital content, allowing for better accessibility for demo-graphics that currently have fewer options for orthographies they are most familiar with.</p><p>One potential negative impact is on the business of some font artists who cater to niche audiences that have less common glyph needs. Our model could potentially be used to replace such workers, and if so could also lead to less coherent renderings for uncommon orthographies if those who are not fluent in such scripts simply employ our system without a thorough understanding of the types of errors it may make.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Architecture Details</head><p>The architectures of our encoder and decoder are largely identical to that of U-Net <ref type="bibr" target="#b25">(Ronneberger et al., 2015)</ref> with key differences described here. We find significantly improved results by inserting Instance Normalization layers <ref type="bibr">(Ulyanov et al., 2016b)</ref> after convolution layers in our decoder. We also replace the max pool layers within the encoder with blur pool layers <ref type="bibr" target="#b35">(Zhang, 2019)</ref>. As stated previously, we max pool the output of the encoders across character types or fonts, and then pass the flattened pooled representation through a fully connected layer to obtain the approximate posterior parameters µ and Σ. A similar fully connected layer projects the character representation Y i to the appropriate size before being passed to the decoder. As noted earlier, the parameters of the last two transposed convolutional layers in the decoder are dynamically output by MLPs which take as input the font representation Z j . These consist of a 256 dimensional fully connected layer, a ReLU, and then a second fully connected layer to produce the relevant parameter.</p><p>We now provide further details on the specific layer sizes used in our model and inference network. The following abbreviations are used to represent various components:</p><p>• F i : fully connected layer with i hidden units Our encoder is:</p><formula xml:id="formula_7">C 64 -R -C 64 -R -C 64 -R -B -C 128 - R -C 128 -R -B -C 256 -R -C 256 -R -B - C 512 -R -C 512 -R -B -M -F 512</formula><p>Our decoder is:</p><formula xml:id="formula_8">F 1024×8×8 -T 1024 -C 512 -I -R -C 512 - I -R -T 512 -C 256 -I -R -C 256 -I -R - D 256 -C 128 -I -R -C 128 -I -R -D 128 - C 64 -I -R -C 64 -I -R -C 1 -S</formula><p>MLP to compute transpose convolutional parameter of size j is:</p><formula xml:id="formula_9">F 256 -R -F j B L 2 Results</formula><p>In Table <ref type="table" target="#tab_3">2</ref> we show results on Google Fonts and Chinese simplified for our model and baselines in terms of L 2 . Rankings are generally the same, and see that our approach performs best by this metric as well as SSIM. We do however note that in places the L 2 numbers and SSIM numbers are not well correlated, and attribute this to L 2 's propensity for rewarding blurry output that minimizes expected distance over sharp output that may have slightly misaligned edges. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Supported glyphs in Google Fonts organized by character type and font. A blue pixel indicates that column's font includes that row's character. Our proposed model allows font reconstruction over this large, sparse character set.</figDesc><graphic coords="1,329.19,272.66,109.26,122.78" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Overview of our generative factorization model, and important architecture details. Glyphs in the same row share a latent variable Y i representing character shape, and those in the same column share Z j representing font style. These variables are inferred by a network that takes in an entire row or column. Our decoder combines these representations to output a distribution on the glyph image.</figDesc><graphic coords="3,62.37,69.68,235.60,151.56" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>) SSIM per glyph by number of observed characters for Google Fonts and Chinese Simplified. (Right) Ablations of our model, showing SSIM results on known characters in Google Fonts.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Generated glyphs for interpolating between both character type (horizontal axis) and font style (vertical axis) simultaneously.</figDesc><graphic coords="8,70.87,217.41,109.13,109.45" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Reconstructions of two fonts from our model, EMD, and NN -shown in black in that order -for both known and unknown character types. Green characters show the expected shape in a neutral font, and blue characters are a sample of those observed by the models for either font.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: t-SNE plot of Chinese character embeddings from our model for the top 5 radicals (left), and randomly chosen groups of 5 (middle, right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>•</head><label></label><figDesc>R : ReLU activation • S : sigmoid activation • M : batch max pool • B : 2 × 2 spatial blur pool (Zhang, 2019) • C i : convolutional layer with i filters of 3 × 3, 1 pixel zero-padding, stride of 1 • I : instance normalization • T i : transpose convolution with i filters of 2 × 2, stride of 2 • D i : transpose convolution with i filters of 2×2, stride of 2, where kernel and bias are the output of an MLP (described below) • H : reshape to -1 × 256 × 8 × 8</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0"><head></head><label></label><figDesc></figDesc><graphic coords="6,70.87,70.86,333.95,152.60" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head></head><label></label><figDesc>.,</figDesc><table><row><cell>Char structure</cell><cell></cell></row><row><cell>defines initial</cell><cell></cell></row><row><cell>activation</cell><cell>Font style defines</cell></row><row><cell></cell><cell>convolution</cell></row><row><cell></cell><cell>filters</cell></row><row><cell></cell><cell></cell><cell>Max Pooling</cell></row><row><cell cols="2">Transposed Convolutional Decoder</cell><cell>Convolutional Encoder</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>.799 0.828 0.833 0.834Table 1 :</head><label>1</label><figDesc>(Left</figDesc><table><row><cell>Observations</cell><cell>1</cell><cell>8</cell><cell>16</cell><cell>32</cell><cell>1</cell><cell>8</cell><cell>16</cell><cell>32</cell></row><row><cell cols="5">Google Fonts: Known Char NN 0.755 0.816 0.830 0.839 EMD 0.706 0.702 0.539 0.597 Dual Manifold 0.799 0.828 0.833 0.834</cell><cell cols="4">Google Fonts: Unknown Char ----0.698 0.695 0.534 0.595 0.801 0.826 0.829 0.830</cell><cell>Observations +Dual, +KL, +Adapt, Min 0.7728 0.8041 0.8083 0.8088 1 8 16 32 Srivatsan et al. (2019)* 0.713 0.702 0.701 0.698 -Dual, +KL, +Adapt, Max 0.704 0.703 0.701 0.703</cell></row><row><cell></cell><cell cols="8">Chinese Simplified: Known Char Chinese Simplified: Unknown Char</cell><cell>+Dual, -KL, +Adapt, Max 0.785</cell><cell>0.817</cell><cell>0.821</cell><cell>0.823</cell></row><row><cell cols="5">NN 0.428 0.488 0.495 0.499 EMD 0.278 0.271 0.291 0.288 Dual Manifold 0.392 0.405 0.407 0.407</cell><cell cols="3">-0.270 0.266 0.283 --0.375 0.387 0.390</cell><cell>-0.390 0.280</cell><cell>+Dual, +KL, -Adapt, Max 0.795 +Dual, +KL, +Adapt, Max 0</cell><cell>0.823</cell><cell>0.828</cell><cell>0.829</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>.56 202.58 193.34 189.94 276.47 212.76 205.34 202.91</head><label></label><figDesc></figDesc><table><row><cell></cell><cell></cell><cell cols="2">Google Fonts: Known Char</cell><cell></cell><cell cols="4">Google Fonts: Unknown Char</cell></row><row><cell>Observations</cell><cell>1</cell><cell>8</cell><cell>16</cell><cell>32</cell><cell>1</cell><cell>8</cell><cell>16</cell><cell>32</cell></row><row><cell cols="5">NN 405.15 EMD 371.06 Dual Manifold 275Chinese Simplified: Known Char 258.11 227.91 207.04 367.18 658.85 512.26</cell><cell cols="4">-378.08 Chinese Simplified: Unknown Char ---375.19 667.66 511.69</cell></row><row><cell cols="9">NN 1086.58 908.58 EMD 1013.80 1019.97 1288.32 1287.85 1303.96 1020.89 1303.92 1303.48 883.18 872.52 ----Dual Manifold 916</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>.41 879.03 873.48 868.41 917.91 883.60 878.77 875.03Table 2 :</head><label>2</label><figDesc>L 2 per glyph by number of observed characters for Google Fonts and Chinese Simplified.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>https://github.com/google/fonts</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This project is funded in part by the <rs type="funder">NSF</rs> under grants <rs type="grantNumber">1618044</rs> and <rs type="grantNumber">1936155</rs>, and by the <rs type="funder">NEH</rs> under grant <rs type="grantNumber">HAA256044-17</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_zPGNbq7">
					<idno type="grant-number">1618044</idno>
				</org>
				<org type="funding" xml:id="_TcnuWuR">
					<idno type="grant-number">1936155</idno>
				</org>
				<org type="funding" xml:id="_KdXVqBt">
					<idno type="grant-number">HAA256044-17</idno>
				</org>
			</listOrg>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Multi-content GAN for few-shot font style transfer</title>
		<author>
			<persName><forename type="first">Samaneh</forename><surname>Azadi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthew</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vladimir</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhaowen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018">2018</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A general and adaptive robust loss function</title>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Edges are the&apos;independent components&apos; of natural scenes</title>
		<author>
			<persName><forename type="first">J</forename><surname>Anthony</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Terrence</forename><forename type="middle">J</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName><surname>Sejnowski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>NeurIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Learning a manifold of fonts</title>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">F</forename><surname>Neill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Campbell</surname></persName>
		</author>
		<author>
			<persName><surname>Kautz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>ACM TOG</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">cw2vec: Learning chinese word embeddings with stroke n-gram information</title>
		<author>
			<persName><forename type="first">Shaosheng</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaolong</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Stylebank: An explicit representation for neural image style transfer</title>
		<author>
			<persName><forename type="first">Dongdong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nenghai</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1897" to="1906" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Biorthogonal bases of compactly supported wavelets</title>
		<author>
			<persName><forename type="first">Albert</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ingrid</forename><surname>Daubechies</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J-C</forename><surname>Feauveau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications on pure and applied mathematics</title>
		<imprint>
			<biblScope unit="page">45</biblScope>
			<date type="published" when="1992">1992</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.8516</idno>
		<title level="m">Nice: Non-linear independent components estimation</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.08803</idno>
		<title level="m">Density estimation using real nvp</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Relations between the statistics of natural images and the response properties of cortical cells</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">J</forename><surname>Field</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JOSA-A</title>
		<imprint>
			<date type="published" when="1987">1987</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning bilinear models for two-factor problems in vision</title>
		<author>
			<persName><forename type="first">T</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">B</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting>IEEE Computer Society Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1997">1997</date>
			<biblScope unit="page" from="554" to="560" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Artistic glyph image synthesis via one-stage few-shot learning</title>
		<author>
			<persName><forename type="first">Yue</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuan</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhouhui</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yingmin</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianguo</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<author>
			<persName><forename type="first">Leon</forename><forename type="middle">A</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Bethge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1508.06576</idno>
		<title level="m">A neural algorithm of artistic style</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Google fonts</title>
		<author>
			<persName><surname>Google</surname></persName>
		</author>
		<ptr target="http://fonts.google.com" />
		<imprint>
			<biblScope unit="page" from="2020" to="2026" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">David</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.09106</idno>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">Hypernetworks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexandre</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Style and content disentanglement in generative adversarial networks</title>
		<author>
			<persName><forename type="first">Seyed</forename><surname>Hadi Kazemi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nasser</forename><surname>Mehdi Iranmanesh</surname></persName>
		</author>
		<author>
			<persName><surname>Nasrabadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2019 IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="848" to="856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<title level="m">Adam: A method for stochastic optimization</title>
		<imprint>
			<publisher>ICLR</publisher>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Autoencoding variational bayes</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Easyfont: a style learning-based system to easily build your large-scale handwriting fonts</title>
		<author>
			<persName><forename type="first">Zhouhui</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xudong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianguo</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="18" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A learned representation for scalable vector graphics</title>
		<author>
			<persName><forename type="first">Gontijo</forename><surname>Raphael</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Lopes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Douglas</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathon</forename><surname>Eck</surname></persName>
		</author>
		<author>
			<persName><surname>Shlens</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7930" to="7939" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11">2008. Nov</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Automatic differentiation in PyTorch</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Autodiff Workshop</title>
		<imprint>
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Flexyfont: Learning transferring rules for flexible typeface synthesis</title>
		<author>
			<persName><forename type="first">Hongbo</forename><surname>Huy Quoc Phan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoni</forename><forename type="middle">B</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Graphics Forum</title>
		<imprint>
			<publisher>Wiley Online Library</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="245" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Variational inference with normalizing flows</title>
		<author>
			<persName><forename type="first">Danilo</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="1530" to="1538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">U-Net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015">2015</date>
			<publisher>MICCAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning to generate images with perceptual similarity metrics</title>
		<author>
			<persName><forename type="first">Jake</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karl</forename><surname>Ridgeway</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Renjie</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Brett</forename><forename type="middle">D</forename><surname>Roads</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Richard</forename><forename type="middle">S</forename><surname>Michael C Mozer</surname></persName>
		</author>
		<author>
			<persName><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="4277" to="4281" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">A deep factorization of style and structure in fonts</title>
		<author>
			<persName><forename type="first">Nikita</forename><surname>Srivatsan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Berg-Kirkpatrick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
			<publisher>EMNLP</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Example-based automatic font generation</title>
		<author>
			<persName><forename type="first">Rapee</forename><surname>Suveeranont</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Takeo</forename><surname>Igarashi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Smart Graphics</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010">2010</date>
			<biblScope unit="page" from="127" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Separating style and content with bilinear models</title>
		<author>
			<persName><forename type="first">B</forename><surname>Joshua</surname></persName>
		</author>
		<author>
			<persName><forename type="first">William</forename><forename type="middle">T</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1247" to="1283" />
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Texture networks: Feed-forward synthesis of textures and stylized images</title>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Vadim</forename><surname>Lebedev</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><forename type="middle">S</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<author>
			<persName><forename type="first">Dmitry</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.08022</idno>
		<title level="m">The missing ingredient for fast stylization</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Accelerating tsne using tree-based algorithms</title>
		<author>
			<persName><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3221" to="3245" />
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Generative image modeling using style and structure adversarial networks</title>
		<author>
			<persName><forename type="first">Xiaolong</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="318" to="335" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Tet-gan: Text effects transfer via stylization and destylization</title>
		<author>
			<persName><forename type="first">Shuai</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiaying</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenjing</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zongming</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1238" to="1245" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Making convolutional networks shift-invariant again</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7324" to="7334" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Separating style and content for generalized style transfer</title>
		<author>
			<persName><forename type="first">Yexun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbin</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="8447" to="8455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">A unified framework for generalizable style transfer: Style and content separation</title>
		<author>
			<persName><forename type="first">Yexun</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ya</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wenbin</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="4085" to="4098" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
