<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GRAPH-GUIDED NETWORK FOR IRREGULARLY SAMPLED MULTIVARIATE TIME SERIES</title>
				<funder ref="#_ZGPUR4H #_T4YeqpY">
					<orgName type="full">National Science Foundation</orgName>
					<orgName type="abbreviated">NSF</orgName>
				</funder>
				<funder>
					<orgName type="full">Appendix. Python implementation of RAINDROP</orgName>
				</funder>
				<funder ref="#_77EXd7y">
					<orgName type="full">Under Secretary of Defense for Research and Engineering under Air Force</orgName>
				</funder>
				<funder>
					<orgName type="full">Harvard Data Science Initiative</orgName>
				</funder>
				<funder>
					<orgName type="full">Amazon Research Award, Bayer Early Excellence in Science Award</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2022-03-16">16 Mar 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
							<email>xiang_zhang@hms.harvard.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">MIT Lincoln Laboratory</orgName>
								<orgName type="institution" key="instit1">Harvard University</orgName>
								<orgName type="institution" key="instit2">University of Ljubljana</orgName>
								<orgName type="institution" key="instit3">Harvard University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Marko</forename><surname>Zeman</surname></persName>
							<email>marko.zeman@fri.uni-lj.si</email>
							<affiliation key="aff0">
								<orgName type="laboratory">MIT Lincoln Laboratory</orgName>
								<orgName type="institution" key="instit1">Harvard University</orgName>
								<orgName type="institution" key="instit2">University of Ljubljana</orgName>
								<orgName type="institution" key="instit3">Harvard University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Theodoros</forename><surname>Tsiligkaridis</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">MIT Lincoln Laboratory</orgName>
								<orgName type="institution" key="instit1">Harvard University</orgName>
								<orgName type="institution" key="instit2">University of Ljubljana</orgName>
								<orgName type="institution" key="instit3">Harvard University</orgName>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
							<email>marinka@hms.harvard.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">MIT Lincoln Laboratory</orgName>
								<orgName type="institution" key="instit1">Harvard University</orgName>
								<orgName type="institution" key="instit2">University of Ljubljana</orgName>
								<orgName type="institution" key="instit3">Harvard University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">GRAPH-GUIDED NETWORK FOR IRREGULARLY SAMPLED MULTIVARIATE TIME SERIES</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-03-16">16 Mar 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2110.05357v2[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-28T23:05+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>In many domains, including healthcare, biology, and climate science, time series are irregularly sampled with varying time intervals between successive readouts and different subsets of variables (sensors) observed at different time points. Here, we introduce RAINDROP, a graph neural network that embeds irregularly sampled and multivariate time series while also learning the dynamics of sensors purely from observational data. RAINDROP represents every sample as a separate sensor graph and models time-varying dependencies between sensors with a novel message passing operator. It estimates the latent sensor graph structure and leverages the structure together with nearby observations to predict misaligned readouts. This model can be interpreted as a graph neural network that sends messages over graphs that are optimized for capturing time-varying dependencies among sensors. We use RAINDROP to classify time series and interpret temporal dynamics on three healthcare and human activity datasets. RAINDROP outperforms state-of-the-art methods by up to 11.4% (absolute F1-score points), including techniques that deal with irregular sampling using fixed discretization and set functions. RAINDROP shows superiority in diverse setups, including challenging leave-sensor-out settings.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Multivariate time series are prevalent in a variety of domains, including healthcare, space science, cyber security, biology, and finance <ref type="bibr" target="#b33">(Ravuri et al., 2021;</ref><ref type="bibr" target="#b44">Sousa et al., 2020;</ref><ref type="bibr" target="#b38">Sezer et al., 2020;</ref><ref type="bibr" target="#b8">Fawaz et al., 2019)</ref>. Practical issues often exist in collecting sensor measurements that lead to various types of irregularities caused by missing observations, such as saving costs, sensor failures, external forces in physical systems, medical interventions, to name a few <ref type="bibr" target="#b5">(Choi et al., 2020)</ref>. While temporal machine learning models typically assume fully observed and fixed-size inputs, irregularly sampled time series raise considerable challenges <ref type="bibr" target="#b42">(Shukla &amp; Marlin, 2021;</ref><ref type="bibr" target="#b16">Hu et al., 2021)</ref>. For example, observations of different sensors might not be aligned, time intervals among adjacent observations are different across sensors, and different samples have different numbers of observations for different subsets of sensors recorded at different time points <ref type="bibr" target="#b14">(Horn et al., 2020;</ref><ref type="bibr" target="#b50">Wang et al., 2011)</ref>.</p><p>Prior methods for dealing with irregularly sampled time series involve filling in missing values using interpolation, kernel methods, and probabilistic approaches <ref type="bibr" target="#b37">(Schafer &amp; Graham, 2002)</ref>. However, the absence of observations can be informative on its own <ref type="bibr" target="#b28">(Little &amp; Rubin, 2014)</ref> and thus imputing missing observations is not necessarily beneficial <ref type="bibr" target="#b0">(Agniel et al., 2018)</ref>. While modern techniques involve recurrent neural network architectures (e.g., RNN, LSTM, GRU) <ref type="bibr" target="#b4">(Cho et al., 2014)</ref> and transformers <ref type="bibr" target="#b47">(Vaswani et al., 2017)</ref>, they are restricted to regular sampling or assume aligned measurements across modalities. For misaligned measurements, existing methods tend to rely on a two-stage approach that first imputes missing values to produce a regularly-sampled dataset and then optimizes a model of choice for downstream performance. This decoupled approach does not fully exploit informative missingness patterns or deal with irregular sampling, thus producing suboptimal performance <ref type="bibr" target="#b51">(Wells et al., 2013;</ref><ref type="bibr" target="#b24">Li &amp; Marlin, 2016)</ref>. Thus, recent methods circumvent the imputation stage and directly model irregularly sampled time series <ref type="bibr" target="#b1">(Che et al., 2018;</ref><ref type="bibr" target="#b14">Horn et al., 2020)</ref>.</p><p>Previous studies <ref type="bibr" target="#b53">(Wu et al., 2021;</ref><ref type="bibr">Li et al., 2020a;</ref><ref type="bibr" target="#b60">Zhang et al., 2019)</ref> have noted that inter-sensor correlations bring rich information in modeling time series. However, only few studies consider relational structure of irregularly sampled time series, and those which do have limited ability in capturing inter-sensor connections <ref type="bibr" target="#b53">(Wu et al., 2021;</ref><ref type="bibr" target="#b41">Shukla &amp; Marlin, 2018)</ref>. In contrast, we integrate recent advances in graph neural networks to take advantage of relational structure among sensors. We learn latent graphs from multivariate time series and model time-varying inter-sensor dependencies through neural message passing, establishing graph neural networks as a way to model sample-varying and time-varying structure in complex time series. Present work. To address the characteristics of irregularly sampled time series, we propose to model temporal dynamics of sensor dependencies and how those relationships evolve over time. Our intuitive assumption is that the observed sensors can indicate how the unobserved sensors currently behave, which can further improve the representation learning of irregular multivariate time series. We develop RAINDROP<ref type="foot" target="#foot_0">foot_0</ref> , a graph neural network that leverages relational structure to embed and classify irregularly sampled multivariate time series. RAINDROP takes samples as input, each sample containing multiple sensors and each sensor consisting of irregularly recorded observations (e.g., in clinical data, an individual patient's state of health is recorded at irregular time intervals with different subsets of sensors observed at different times). RAINDROP model is inspired by how raindrops hit a surface at varying times and create ripple effects that propagate through the surface. Mathematically, in RAINDROP, observations (i.e., raindrops) hit a sensor graph (i.e., surface) asynchronously and at irregular time intervals. Every observation is processed by passing messages to neighboring sensors (i.e., creating ripples), taking into account the learned sensor dependencies (Figure <ref type="figure" target="#fig_0">1</ref>). As such, RAINDROP can handle misaligned observations, varying time gaps, arbitrary numbers of observations, and produce multi-scale embeddings via a novel hierarchical attention.</p><p>We represent dependencies with a separate sensor graph for every sample wherein nodes indicate sensors and edges denote relationships between them. Sensor graphs are latent in the sense that graph connectivity is learned by RAINDROP purely from observational time series. In addition to capturing sensor dependencies within each sample, RAINDROP i) takes advantage of similarities between different samples by sharing parameters when calculating attention weights, and ii) considers importance of sequential sensor observations via temporal attention.</p><p>RAINDROP adaptively estimates observations based on both neighboring readouts in the temporal domain and similar sensors as determined by the connectivity of optimized sensor graphs. We compare RAINDROP to five state-of-the-art methods on two healthcare datasets and an activity recognition dataset across three experimental settings, including a setup where a subset of sensors in the test set is malfunctioning (i.e., have no readouts at all). Experiments show that RAINDROP outperforms baselines on all datasets with an average AUROC improvement of 3.5% in absolute points on various classification tasks. Further, RAINDROP improves prior work by a 9.3% margin (absolute points in accuracy) when varying subsets of sensors malfunction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Our work here builds on time-series representation learning and notions of graph neural networks and attempts to resolve them by developing a single, unified approach for analysis of complex time series.</p><p>Learning with irregularly sampled multivariate time series. Irregular time series are characterized by varying time intervals between adjacent observations <ref type="bibr" target="#b58">(Zerveas et al., 2021;</ref><ref type="bibr" target="#b46">Tipirneni &amp; Reddy, 2021;</ref><ref type="bibr" target="#b7">Chen et al., 2020)</ref>. In a multivariate case, irregularity means that observations can be misaligned across different sensors, which can further complicate the analysis. Further, because of a multitude of sampling frequencies and varying time intervals, the number of observations can also vary considerably across samples <ref type="bibr" target="#b7">(Fang &amp; Wang, 2020;</ref><ref type="bibr" target="#b19">Kidger et al., 2020)</ref>. Predominant downstream tasks for time series are classification (i.e., predicting a label for a given sample, e.g., <ref type="bibr" target="#b45">Tan et al. (2020)</ref>; <ref type="bibr" target="#b29">Ma et al. (2020)</ref>) and forecasting (i.e., anticipating future observations based on historical observations, e.g., <ref type="bibr">Wu et al. (2020a)</ref>). The above mentioned characteristics create considerable challenges for models that expect well-aligned and fixed-size inputs <ref type="bibr" target="#b43">(Shukla &amp; Marlin, 2020)</ref>. An intuitive way to deal with irregular time series is to impute missing values and process them as regular time series <ref type="bibr" target="#b30">(Mikalsen et al., 2021;</ref><ref type="bibr" target="#b25">Li &amp; Marlin, 2020;</ref><ref type="bibr" target="#b39">Shan &amp; Oliva, 2021)</ref>. However, imputation methods can distort the underlying distribution and lead to unwanted distribution shifts. To this end, recent methods directly learn from irregularly sampled time series <ref type="bibr" target="#b2">(Chen et al., 2018)</ref>. For example, <ref type="bibr" target="#b1">Che et al. (2018)</ref> develop a decay mechanism based on gated recurrent units (GRU-D) and binary masking to capture long-range temporal dependencies. SeFT <ref type="bibr" target="#b14">(Horn et al., 2020)</ref> takes a set-based approach and transforms irregularly sampled time series datasets into sets of observations modeled by set functions insensitive to misalignment. mTAND (Shukla &amp; Marlin, 2021) leverages a multi-time attention mechanism to learn temporal similarity from non-uniformly collected measurements and produce continuous-time embeddings. <ref type="bibr">IP-Net (Shukla &amp; Marlin, 2018)</ref> and DGM 2 <ref type="bibr" target="#b53">(Wu et al., 2021)</ref> adopt imputation to interpolate irregular time series against a set of reference points using a kernel-based approach. The learned inter-sensor relations are static ignoring sample-specific and time-specific characteristics. In contrast with the above methods, RAINDROP leverages dynamic graphs to address the characteristics of irregular time series and produce high-quality representations.</p><p>Learning with graphs and neural message passing. There has been a surge of interest in applying neural networks to graphs, leading to the development of graph embeddings <ref type="bibr" target="#b62">(Zhou et al., 2020;</ref><ref type="bibr" target="#b23">Li et al., 2021)</ref>, graph neural networks <ref type="bibr">(Wu et al., 2020b)</ref>, and message passing neural networks <ref type="bibr" target="#b12">(Gilmer et al., 2017)</ref>. To address the challenges of irregular time series, RAINDROP specifies a message passing strategy to exchange neural message along edges of sensor graphs and deal with misaligned sensor readouts <ref type="bibr" target="#b36">(Riba et al., 2018;</ref><ref type="bibr" target="#b32">Nikolentzos et al., 2020;</ref><ref type="bibr" target="#b10">Galkin et al., 2020;</ref><ref type="bibr" target="#b9">Fey et al., 2020;</ref><ref type="bibr" target="#b27">Lin et al., 2018;</ref><ref type="bibr" target="#b61">Zhang et al., 2020)</ref>. In particular, RAINDROP considers message passing on latent sensor graphs, each graph describing a different sample (e.g., patient, Figure <ref type="figure" target="#fig_0">1</ref>), and it specifies a message-passing network with learnable adjacency matrices. The key difference with the predominant use of message passing is that RAINDROP uses it to estimate edges (dependencies) between sensors rather than applying it on a fixed, apriori-given graph. To the best of our knowledge, prior work did not utilize sensor dependencies for irregularly sampled time series. While prior work used message passing for regular time series <ref type="bibr" target="#b49">(Wang et al., 2020;</ref><ref type="bibr">Wu et al., 2020c;</ref><ref type="bibr" target="#b18">Kalinicheva et al., 2020;</ref><ref type="bibr" target="#b59">Zha et al., 2022)</ref>, its utility for irregularly sampled time series has not yet been studied.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">RAINDROP</head><p>Let D = {(S i , y i ) | i = 1, . . . , N } denote an irregular time series dataset with N labeled samples (Figure <ref type="figure" target="#fig_1">2</ref>). Every sample S i is an irregular multivariate time series with a corresponding label y i ∈ {1, . . . , C}, indicating which of the C classes S i is associated with. Each sample contains M non-uniformly measured sensors that are denoted as u, v, etc. RAINDROP can also work on samples with only a subset of active sensors (see Sec. 4.1). Each sensor is given by a sequence of observations ordered by time. For sensor u in sample S i , we denote a single observation as a tuple (t, x t i,u ), meaning that sensor u was recorded with value x t i,u ∈ R at timestamp t ∈ R + . We omit sample index i and sensor index u in timestamp t. Sensor observations are irregularly recorded, meaning that time intervals between successive observations can vary across sensors. For sensor u in sample S i , we use T i,u to denote the set of timestamps that u, or at least one of u's L-hop neighbors (L is the number of layers in RAINDROP's message passing) is recorded. We use || and T to denote concatenation and transpose, respectively. We omit layer index l ∈ {1, . . . , L} for simplicity when clear from the text.</p><p>Problem (Representation learning for irregularly sampled multivariate time series). A dataset D of irregularly sampled multivariate time series is given, where each sample S i has multiple sensors and each sensor has a variable number of observations. RAINDROP learns a function f : S i → z i that maps S i to a fixed-length representation z i suitable for downstream task of interest, such as classification. Using learned z i , RAINDROP can predict label ŷi ∈ {1, . . . , C} for S i .</p><p>RAINDROP learns informative embeddings for irregularly samples time series. The learned embeddings capture temporal patterns of irregular observations and explicitly consider varying dependencies between sensors. While we focus on time-series classification in this work, the proposed method can be easily extended to broader applications such as regression, clustering and generation tasks. RAINDROP aims to learn a fixed-dimensional embedding z i for a given sample S i and predict the associated label ŷi . To this end, it generates sample embeddings using a hierarchical architecture composed of three levels to model observations (sensor readouts), sensors, and whole samples (Figure <ref type="figure" target="#fig_1">2</ref>). Without loss of generality, we describe RAINDROP's procedure as if observations arrive one at a time (one sensor is observed at time t and other sensors do not have observations). If there are multiple observations at the same time, RAINDROP can effortlessly process them in parallel.</p><p>RAINDROP first constructs a graph for every sample where nodes represent sensors and edges indicate relations between sensors (Sec. 3.2). We use G i to denote the sensor graph for sample S i and e i,uv to represent the weight of a directed edge from sensor u to sensor v in G i . Sensor graphs are automatically optimized considering sample-wise and time-wise specificity.</p><p>The key idea of RAINDROP is to borrow information from u's neighbors based on estimated relationships between u and other sensors. This is achieved via message passing carried out on S i 's dependency graph and initiated at node u in the graph. When an observation (t, x t i,u ) is recorded for sample S i at time t, RAINDROP first embeds the observation at active sensor u (i.e., sensor whose value was recorded) and then propagates messages (i.e., the observation embeddings) from u to neighboring sensors along edges in sensor dependency graph G i . As a result, recording the value of u can affect u's embedding as well as embeddings of other sensors that related to u (Sec. 3.3). Finally, RAINDROP generates sensor embeddings by aggregating all observation embeddings for each sensor (across all timestamps) using temporal attention weights (Sec. 3.4). At last, RAINDROP embeds sample S i based on sensor embeddings (Sec. 3.5) and feeds the sample embedding into a downstream predictor.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">CONSTRUCTING SENSOR DEPENDENCY GRAPHS</head><p>We build a directed weighted graph G i = {V, E i } for every sample S i and refer to it as the sensor dependency graph for S i . Nodes V represent sensors and edges E i describe dependencies between sensors in sample S i that RAINDROP infers. As we show in experiments, RAINDROP can be directly used with samples that only contain a subset of sensors in V. We denote edge from u to v as a triplet (u, e i,uv , v), where e i,uv ∈ [0, 1] represents the strength of relationship between sensors u and v in sample S i . Edge (u, e i,uv , v) describes the relationship between u and v: when u receives an observation, it will send a neural message to v following edge e i,uv . If e i,uv = 0, there is no exchange of neural information between u and v, indicating that the two sensors are unrelated. We assume that the importance of u to v is different than the importance of v to u, and so we treat sensor dependency graphs as directed, i.e., e i,uv = e i,vu . All graphs are initialized as fully-connected graphs (i.e., e i,uv = 1 for any u, v and S i ) and edge weights e i,uv are updated following Eq. 3 during model training. If available, it is easy to integrate additional domain knowledge into graph initialization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">GENERATING EMBEDDINGS OF INDIVIDUAL OBSERVATIONS</head><p>Let u indicate active sensor at time t ∈ T i,u , i.e., sensor whose value x t i,u is observed at t, and let u be connected to v through edge (u, e i,uv , v). We next describe how to produce observation embeddings h t i,u ∈ R d h and h t i,v ∈ R d h for sensors u and v, respectively (Figure <ref type="figure">3a</ref>). We omit layer index l and note that the proposed strategy applies to any number of layers. i and weight vector rv. Edge weight ei,uv is shared by all timestamps. (b) An illustration of generating sensor embedding. Apply the message passing in (a) to all timestamps and produce corresponding observation embeddings. We aggregate arbitrary number of observation embeddings into a fixed-length sensor embedding zi,v while paying distinctive attentions to different observations. We independently apply the processing procedure to all sensors. (c) RAINDROP updates edge weight e (l) i,uv based on the edge weight e (l-1) i,uv from previous layer and the learned inter-sensor attention weights in all time steps. We explicitly show layer index l as multiple layers are involved.</p><p>Embedding an observation of an active sensor. Let u denote an active sensor whose value has just been observed as x t i,u . For sufficient expressive power <ref type="bibr" target="#b48">(Veličković et al., 2018)</ref>, we map observation x t i,u to a high-dimensional space using a nonlinear transformation:</p><formula xml:id="formula_0">h t i,u = σ(x t i,u R u ).</formula><p>We use sensorspecific transformations because values recorded at different sensors can follow different distributions, which is achieved by trainable weight vectors R u depending on what sensor is activated <ref type="bibr">(Li et al., 2020b)</ref>. Alternatives, such as a multilayer perceptron, can be considered to transform x t i,u into h t i,u . As h t i,u represents information brought on by observing x t i,u , we regard h t i,u as the embedding of u's observation at t. Sensor-specific weight vectors R u are shared across samples.</p><p>Passing messages along sensor dependency graphs. For sensors that are not active at timestamp t but are neighbors of the active sensor u in the sensor dependency graph G i , RAINDROP uses relationships between u and those sensors to estimate observation embeddings for them. We proceed by describing how RAINDROP generates observation embedding h t i,v for sensor v assuming v is a neighbor of u in G i . Given h t i,u and edge (u, e i,uv , v), we first calculate inter-sensor attention weight</p><formula xml:id="formula_1">α t i,uv ∈ [0, 1],</formula><p>representing how important u is to v via the following equation:</p><formula xml:id="formula_2">α t i,uv = σ(h t i,u D[r v ||p t i ] T ),<label>(1)</label></formula><p>where r v ∈ R dr is a trainable weight vector that is specific to the sensor receiving the message (i.e., h t i,u ). Vector r v allows the model to learn distinct attention weights for different edges going out from the same sensor u. Further, p t i ∈ R dt is the time representation obtained by converting a 1-dimensional timestamp t into a multi-dimensional vector p t i by passing t through a series of trigonometric functions <ref type="bibr" target="#b14">(Horn et al., 2020)</ref>. See Appendix A.1 for details. RAINDROP uses p t i to calculate attention weights that are sensitive to time. Finally, D is a trainable weight matrix mapping h t i,u from d h dimensions to (d r +d t ) dimensions. Taken this together, we can estimate the embedding h t i,v for u's neighbor v as follows:</p><formula xml:id="formula_3">h t i,v = σ(h t i,u w u w T v α t i,uv e i,uv ),<label>(2)</label></formula><p>where w u , w v ∈ R d h are trainable weight vectors shared across all samples. The w u is specific to active sensor u and w v is specific to neighboring sensor v. In the above equation, e i,uv denotes edge weight shared across all timestamps. The above message passing describes the processing of a single observation at a single timestamp. In case multiple sensors are active at time t and connected with v, we normalize α t i,uv (with softmax function) across active sensors and aggregate messages at v. Overall, RAINDROP produces observation embedding h t i,v for sensor v through its relational connection with u, even though there is no direct measurement of v at time t. These message passing operations are performed to adaptively and dynamically estimate missing observations in the embedding space based on recorded information and learned graph structure.</p><p>Updating sensor dependency graphs. We describe the update of edge weights and prune of graph structures in the situation that stacks multiple RAINDROP layers (Figure <ref type="figure">3</ref>). Here we explicitly show layer index l because multiple layers are involved in the computation. As no prior knowledge is assumed, we initialize the graph as all sensors connected with each other. However, the fully connected edges may bridge sensors that should be independent, which will introduce spurious correlations and prevent the model from paying attention to the truly important connections. Addressing this issue, RAINDROP automatically updates edge weights and prunes out less important edges. Based on the aggregated temporal influence driven by the inter-sensor attention weights α (l),t i,uv , we update edge weights e (l) i,uv in each layer l ∈ {1, . . . , L} by:</p><formula xml:id="formula_4">e (l) i,uv = e (l-1) i,uv |T i,u | t∈Ti,u α (l),t i,uv ,<label>(3)</label></formula><p>where T i,u denotes the set of all timestamps where there is message passes from u to v. In particular, we set e</p><p>i,uv = 1 in the initialization of graph structures. We use L = 2 in all our experiments. In every layer, we order the estimated values e (l) i,uv for all edges in sample S i and prune bottom K% edges with smallest edge weights <ref type="bibr" target="#b56">(Yang et al., 2021)</ref>. Pruned edges are not re-added in later layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">GENERATING SENSOR EMBEDDINGS</head><p>Next we describe how to aggregate observation embeddings into sensor embeddings z i,v , taking sensor v as an example (Figure <ref type="figure">3b</ref>). Previous step (Sec. 3.3) generates observation embeddings for every timestamp when either v or v's neighbor is observed. The observation embeddings at different timestamps have unequal importance to the the sensor embedding <ref type="bibr" target="#b58">(Zerveas et al., 2021)</ref>. We use the temporal attention weight (scalar) β t i,v to represent the importance of observation embedding at t. We use T i,v = {t 1 , t 2 , . . . , t T } to denote all the timestamps when a readout is observed in v (we can directly generate h t i,v ) or in v's neighbor (we can generate h t i,v through message passing). The β t i,v is the corresponding element of vector β i,v which include the temporal attention weights at all timestamps t ∈ T i,v .</p><p>We use temporal self-attention to calculate β i,v , which is different from the standard self-attention <ref type="bibr" target="#b17">(Hu et al., 2020;</ref><ref type="bibr" target="#b57">Yun et al., 2019)</ref>. The standard dot-product self-attention generates an attention matrix with dimension of T × T (where T = |T i,v | can vary across samples) that has an attention weight for each pair of observation embeddings. In our case, we only need a single attention vector where each element denotes the temporal attention weight of an observation embedding when generating the sensor embedding. Thus, we modify the typical self-attention model to fit our case: using a trainable s ∈ R T ×1 to map the self-attention matrix (R T ×T ) to T -dimensional vector β i,v (R T ×1 ) through matrix product (Appendix A.2).</p><p>The following steps describe how to generate sensor embeddings. We first concatenate observation embedding h t i,v with time representation p t i to include information of timestamp. Then, we stack the concatenated embeddings [h t i,v ||p t i ] for all t ∈ T i,v into a matrix H i,v . The H i,v contains all information of observations and timestamps for sensor v. We calculate β t i,v through:</p><formula xml:id="formula_6">β i,v = softmax Q i,v K T i,v √ d k s ,<label>(4)</label></formula><p>where Q i,v and K i,v are two intermediate matrices that are derived from the stacked observation embeddings. In practice,</p><formula xml:id="formula_7">Q i,v = H i,v W Q and K i,v = H i,v W K are linearly mapped from H i,v</formula><p>parameterized by W Q and W K , respectively <ref type="bibr" target="#b47">(Vaswani et al., 2017)</ref>. The √ d k is a scaling factor where d k is the dimension after linear mapping. Based on the learned temporal attention weights β t i,v , we calculate sensor embedding z i,v through:</p><formula xml:id="formula_8">z i,v = t∈Ti,v (β t i,v [h t i,v ||p t i ]W ),<label>(5)</label></formula><p>where weight matrix W is a linear projector shared by all sensors and samples. It is worth to mention that all attention weights (such as α t i,uv and β i,v ) can be multi-head. In this work, we describe the model in the context of single head for brevity.</p><p>Using attentional aggregation, RAINDROP can learn a fixed-length sensor embedding for arbitrary number of observations. Meanwhile, RAINDROP is capable of focusing on the most informative observation embeddings. We process all observation embeddings as a whole instead of sequentially, which allows parallel computation for faster training and also mitigates the performance drop caused by modeling long dependencies sequentially. In the case of sensors with very large number of observations, we can reduce the length of time series by subsampling or splitting a long series into multiple short series.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">GENERATING SAMPLE EMBEDDINGS</head><p>Finally, for sample S i , we aggregate sensor embeddings z i,v (Eq. 5) across all sensors to obtain an embedding z i ∈ R dz through a readout function g as follows:</p><formula xml:id="formula_9">z i = g(z i,v | v = 1, 2, . . . , M ) (such as concatenation).</formula><p>When a sample contains a large number of sensors, RAINDROP can seamlessly use a set-based readout function such as averaging aggregation (Appendix A.3). Given an input sample S i , RAINDROP's strategy outlined in Sec. 3.2-3.5 produces a sample embedding z i that can be further optimized for downstream tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6">IMPLEMENTATION AND PRACTICAL CONSIDERATIONS</head><p>Loss function. RAINDROP's loss function is formulated as:</p><formula xml:id="formula_10">L = L CE + λL r , where L r = 1 M 2 u,v∈V i,j∈V ||e i,uv -e j,uv || 2 /(N -1) 2</formula><p>, where L CE is cross entropy and L r is a regularizer to encourage the model to learn similar sensor dependency graphs for similar samples. The L r measures averaged Euclidean distance between edge weights across all samples pairs, in all sensor pairs (including self-connections). The λ is a user-defined coefficient. Practically, as N can be large, we calculate L r only for samples in a batch. Downstream tasks. If a sample has auxiliary attributes (e.g., a patient's demographics) that do not change over time, we can project the attribute vector to a d a -dimensional vector a i with a fullyconnected layer and concatenate it with the sample embedding, getting [z i ||a i ]. At last, we feed [z i ||a i ] (or only z i if a i is not available) into a neural classifier ϕ : R dz+da → {1, . . . , C}. In our experiments, ϕ is a 2-layer fully-connected network with C neurons at the output layer returning prediction ŷi = ϕ([z i ||a i ]) for sample S i .</p><p>Sensor dependencies. While modeling sensor dependencies, we involve observation embedding (h t i,u , Eq. 1) of each sample in the calculation of attention weights. Similarly, to model time-wise specificity in graph structures, we consider time information (p t i , Eq. 1) when measuring α t i,uv . RAINDROP can capture similar graph structures across samples from three aspects (Appendix A.4):</p><p>(1) the initial graphs are the same in all samples; (2) the parameters in message passing (R u ; w u , w v , Eq. 2), inter-sensor attention weights calculation (D, Eq. 1), and temporal attention weights calculation (s, Eq. 4; W , Eq. 5) are shared by all samples; (3) we encourage the model to learn similar graph structures by adding a penalty to disparity of structures (L r ).</p><p>Scalability. RAINDROP is efficient because embeddings can be learned in parallel. In particular, processing of observation embeddings is independent across timestamps. Similarly, sensor embeddings can be processed independently across different sensors (Figure <ref type="figure">3</ref>). While the complexity of temporal self-attention calculation grows quadratically with the number of observations, it can be practically implemented using highly-optimized matrix multiplication.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS</head><p>Datasets. Below we briefly overview healthcare and human activity datasets. ( <ref type="formula" target="#formula_2">1</ref> Baselines. We compare RAINDROP with five state-of-the-art baselines: Transformer <ref type="bibr" target="#b47">(Vaswani et al., 2017)</ref>, Trans-mean, GRU-D <ref type="bibr" target="#b1">(Che et al., 2018)</ref>, SeFT <ref type="bibr" target="#b14">(Horn et al., 2020)</ref>, and mTAND (Shukla &amp; <ref type="bibr" target="#b42">Marlin, 2021)</ref>. The Trans-mean is an imputation method combining transformer architecture with commonly used average interpolation (i.e., missing values are replaced by average observations in each sensor). The mTAND (Shukla &amp; Marlin, 2021) method has been shown to outperform numerous recurrent models including RNN-Impute <ref type="bibr" target="#b1">(Che et al., 2018)</ref>, RNN-Simple, and Phased-LSTM <ref type="bibr" target="#b31">(Neil et al., 2016)</ref>, along with ordinary differential equations (ODE)-based models such as LATENT-ODE and ODE-RNN <ref type="bibr" target="#b2">(Chen et al., 2018)</ref>. For this reason, we compare with mTAND and do not report comparison with those techniques in this paper. Even though, to better show the superiority of RAINDROP, we provide extensive comparison with popular approaches, such as DGM 2 -O <ref type="bibr" target="#b53">(Wu et al., 2021)</ref> and MTGNN <ref type="bibr">(Wu et al., 2020c)</ref>, that are designed for forecasting tasks. Further details are in  <ref type="table" target="#tab_3">1</ref>, RAINDROP obtains the best performance across three benchmark datasets, suggesting its strong performance for time series classification. In particular, in binary classification (P19 and P12), RAINDROP outperforms the strongest baselines by 5.3% in AUROC and 4.8% in AUPRC on average. In a more challenging 8-way classification on the PAM dataset, RAINDROP outperforms existing approaches by 5.7% in accuracy and 5.5% in F1 score. Further exploratory analyses and benchmarking results are shown in Appendix A.9-A.10. To this end, we test whether RAINDROP can achieve good performance when a subset of sensors are completely missing. This setting is practically relevant in situations when, for example, sensors fail or are unavailable. We select a fraction of sensors and hide all their observations in both validation and test sets (training samples are not changed). In particular, we leave out the most informative sensors as defined by information gain analysis <ref type="bibr">(Appendix A.8)</ref>  <ref type="table" target="#tab_6">2</ref> (right block). We find that RAINDROP achieves better performance than baselines in 16 out of 20 settings and that Trans-mean and GRU-D are the strongest competitors. Further, we evaluated RAINDROP in another setting where the model is trained on one group of samples (e.g., females) and tested on another group not seen during training (e.g., males). Experimental setup and results are detailed in Appendix A.13. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">ABLATION STUDY AND VISUALIZATION OF OPTIMIZED SENSOR GRAPHS</head><p>Ablation study. Considering the PAM dataset and a typical setup (Setting 1), we conduct an ablation study to evaluate how much various RAINDROP's components contribute towards its final performance. We examine the following components: inter-sensor dependencies (further decomposed into weights including e i,uv , r v , p t i , and α t i,uv ), temporal attention, and sensor-level concatenation. We show in Appendix A.14 (Table <ref type="table" target="#tab_11">7</ref>) that all model components are necessary and that regularization L r contributes positively to RAINDROP's performance.</p><p>Visualizing sensor dependency graphs. We investigate whether samples with the same labels get more similar sensor dependency graphs than samples with different labels. To this end, we visualize inter-sensor dependencies (P19; Setting 1) and explore them. Figure <ref type="figure" target="#fig_3">4</ref> shows distinguishable patterns between graphs of negative and positive samples, indicating that RAINDROP can extract relationships that are specific to downstream sample labels. Further differential analysis provides insights that can inform early detection of sepsis from P19 clinical data. Details are in Appendix A.15.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>We introduce RAINDROP, a graph-guided network for irregularly sampled time series. RAINDROP learns a distinct sensor dependency graph for every sample capturing time-varying dependencies between sensors. The ability to leverage graph structure gives RAINDROP unique capability to naturally handle misaligned observations, non-uniform time intervals between successive observations, and sensors with varying numbers of recorded observations. Our findings have implications for using message passing as a way to leverage relational information in multivariate time series.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A APPENDIX</head><p>A.1 ENCODING TIMESTAMPS For a given time value t, we pass it to trigonometric functions with the frequency of 10,000 <ref type="bibr" target="#b47">(Vaswani et al., 2017)</ref> and generate time representation p t ∈ R ξ (omit sample index i for brevity) through <ref type="bibr" target="#b14">(Horn et al., 2020)</ref>:</p><formula xml:id="formula_11">p t 2k = sin( t 10000 2k/ξ ), p t 2k+1 = cos( t 10000 2k/ξ ), (<label>6</label></formula><formula xml:id="formula_12">)</formula><p>where ξ is the expected dimension. In this work, we set ξ = 16 in all experimental settings for all models. Please note, we encode the time value which is a continuous timestamp, instead of time position which is a discrete integer indicating the order of observation in time series.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 ADDITIONAL INFORMATION ON THE CALCULATION OF TEMPORAL ATTENTION WEIGHT</head><p>The Eq. 4 describes how we learn the temporal attention weights vector β i,v for sensor v, following the self-attention formalism. Different from the standard self-attention mechanism that generates an self-attention matrix, we generate a temporal attention weight vector. The reason is that we only need an attention weight vector (instead of a matrix) to aggregate the observation embeddings into a single sensor embedding through weighted sum.</p><p>In the standard self-attention matrix, each element denotes the dependency of an observation embedding on another observation embedding. Similarly, each row describes the dependencies of an observation embedding on all other observation embeddings (all the observations belong to the same sensor). Our intuition is to aggregate a row in the self-attention matrix into a scalar that denotes the importance of the observation embedding to the whole sensor embedding.</p><p>In practice, we apply the weighted aggregation, parameterized by s, to every row in the self-attention matrix and concatenate the generated scalars into an attention vector. Next, we give a concrete example to specifically describe the meaning of s. Each row, j, of the self-attention matrix captures relationships of observation embedding h tj i,v to all observation embeddings {h t k i,v : k = 1, ..., T }. Then, using the learnable weight vector s, these correlations between observations are aggregated across time to obtain temporal importance weight β tj i,v . The β tj i,v represents the importance of the corresponding observation to the whole sensor embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 ADDITIONAL INFORMATION ON SAMPLE EMBEDDING</head><p>As we generate sample embedding by concatenating all sensor embeddings, the sample embedding could be relatively long when there is a large number of sensors. To alleviate this issue, on one hand, we can reduce the dimension of sample embeddings by adding a neural layer (such as a simple fully-connected layer) after the concatenation. On the other hand, when the number of sensors is super large, our model is flexible and can effortlessly switch the concatenation to other readout functions (such as averaging aggregation): this will naturally solve the problem of long vectors. We empirically show that concatenation works better than averaging in our case. We see a boost in the AUROC score by 0.6% using concatenation instead of averaging for generating sample embeddings(P19; Setting 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 ADDITIONAL INFORMATION ON SAMPLE SIMILARITIES</head><p>In this work, we assume all samples share some common characteristics to some extent. When modeling the similarities across samples, we do not consider the situation where the samples are similar within latent groups and different across groups.</p><p>Our study focuses on the question of irregularity rather than the question of distribution shifts in time series. To this end, in our experiments, we first rigorously benchmark Raindrop using a standard evaluating setup (Setting 1, which is classification of irregular time series). This is the only setup that most existing methods consider (e.g., <ref type="bibr" target="#b42">Shukla &amp; Marlin (2021)</ref>; <ref type="bibr" target="#b1">Che et al. (2018)</ref>) and we want to make sure our comparisons are fair. In order to provide a more rigorous assessment of Raindrop's performance, we also consider more challenging setups in our experiments (i.e., Settings 2-4) when the dataset is evaluated in a non-standard manner and the split is informed by a select data attribute.  <ref type="bibr" target="#b14">(Horn et al., 2020)</ref>. Each patient contains multivariate time series with 36 sensors (excluding weight), which are collected in the first 48-hour stay in ICU. Each sample has a static vector with 9 elements including age, gender, etc. Each patient is associated with a binary label indicating length of stay in ICU, where negative label means hospitalization is not longer than 3 days and positive label marks hospitalization is longer than 3 days. P12 is imbalanced with ∼93% positive samples.</p><p>PAM: PAMAP2 Physical Activity Monitoring. PAM dataset <ref type="bibr" target="#b34">(Reiss &amp; Stricker, 2012)</ref> measures daily living activities of 9 subjects with 3 inertial measurement units. We modify it to suit our scenario of irregular time series classification. We excluded the ninth subject due to short length of sensor readouts. We segment the continuous signals into samples with the time window of 600 and the overlapping rate of 50%. PAM originally has 18 activities of daily life. We exclude the ones associated with less than 500 samples, remaining 8 activities. After modification, PAM dataset contains 5,333 segments (samples) of sensory signals. Each sample is measured by 17 sensors and contains 600 continuous observations with the sampling frequency 100 Hz. To make time series irregular, we randomly remove 60% of observations. To keep fair comparison, the removed observations are randomly selected but kept the same for all experimental settings and approaches. PAM is labelled by 8 classes where each class represents an activity of daily living. PAM does not include static attributes and the samples are approximately balanced across all 8 categories.</p><p>To feed given data into neural networks, we set the input as zero if no value was measured. In highly imbalanced datasets (P19 and P12) we perform batch minority class upsampling, which means that every processed batch has the same number of positive and negative class samples. The dataset statistics including sparse ratio are provided in Table <ref type="table" target="#tab_7">3</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.6 FURTHER DETAILS ON MODEL HYPERPARAMETERS</head><p>Baseline hyperparameters. The implementation of baselines follows the corresponding papers including SeFT <ref type="bibr" target="#b14">(Horn et al., 2020)</ref>, GRU-D <ref type="bibr" target="#b1">(Che et al., 2018), and</ref><ref type="bibr">mTAND (Shukla &amp;</ref><ref type="bibr" target="#b42">Marlin, 2021)</ref>. We follow the settings of Transformer baseline in <ref type="bibr" target="#b14">(Horn et al., 2020)</ref> while implementing Transformer in our work. For average imputation in Trans-mean, we replace the missing values by the global mean value of observations in the sensor <ref type="bibr" target="#b43">(Shukla &amp; Marlin, 2020)</ref>. We use batch size of 128 and learning rate of 0.0001. Note that we upsample the minority class in each batch to make the batch balance (64 positive samples and 64 negative samples in each batch).</p><p>The chosen hyperparameters are the same across datasets (P19, P12, PAM), models (both baselines and RAINDROP), and experimental settings. Remarkably, we found that all the baselines make dummy predictions (classify all testing samples as the majority label) on PAM in Setting 2-3 while RAINDROP makes reasonable predictions. For the comparison to make sense (i.e., the baselines can make meaningful predictions), we use learning rate of 0.001 for baselines on PAM. GRU-D has 49 layers while other models have 2 layers. We run all models for 20 epochs, store the parameters that obtain the highest AUROC in the validation set, and use it to make predictions for testing samples. We use the Adam algorithm for gradient-based optimization <ref type="bibr" target="#b21">(Kingma &amp; Ba, 2014)</ref>.</p><p>RAINDROP hyperparameters. Next, we report the setting of unique hyperparameters in our RAIN-DROP. In the generation of observation embedding, we set R u as a 4-dimensional vector, thus the produced observation embedding has 4 dimensions. The dimensions of time representation p t and r v are both 16. The trainable weight matrix D has shape of 4 × 32. The dimensions of w u and w v are the same as the number of sensors: 34 in P19, 36 in P12, and 17 in PAM. We set the number of RAINDROP layers L as 2 while the first layer prunes edges and the second layer does not. We set the proportion of edge pruning as 50% (K=50), which means we remove half of the existing edges that have the lowest weights. The d k is set to 20, while the shape of W is 20 × 20. All the activation functions, without specific clarification, are sigmoid functions. The d a is set equal to the number of sensors. The first layer of ϕ has 128 neurons while the second layer has C neurons (i.e., 2 for P19 and P12; 8 for PAM). We set λ = 0.02 to adjust L r regularization scale. All the preprocessed datasets and implementation codes are made available online. Further details are available through RAINDROP's code and dataset repository.</p><p>Readout function. Here we discuss the selection of readout function g in section 3.5. Our preliminary experiments show that concatenation outperforms other popular aggregation functions such as averaging <ref type="bibr" target="#b6">(Errica et al., 2021)</ref> and squeeze-excitation readout function <ref type="bibr" target="#b20">(Kim et al., 2021;</ref><ref type="bibr" target="#b15">Hu et al., 2018)</ref>. While any of those aggregation functions can be considered, we used concatenation throughout all experiments in this manuscript.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.7 PERFORMANCE METRICS</head><p>Since P19 and P12 datasets are imbalanced, we use the Area Under a ROC Curve (AUROC) and Area Under Precision-Recall Curve (AUPRC) to measure performance. As the PAM dataset is nearly balanced, we also report accuracy, precision, recall and F1 score. We report mean and standard deviation values over 5 independent runs. Model parameters that achieve the best AUROC value on the validation set are used for test set.</p><p>A.8 FURTHER DETAILS ON SETUP DETAILS FOR SETTING 2</p><p>In Setting 2, the selected missing sensors are fixed across different models and chosen in the following way. First, we calculate the importance score for each sensor and rank them in a descending order. The importance score is based on information gain, which we calculate with feeding the observations into a Random Forest classifier with 20 decision trees. In particular, we treat each sample as only having one sensor, then feed the single sensor into random forest classifier and record the AUROC. The higher AUROC indicates the sensor provides higher information gain. When we have sensors ranked by their AUROC values, we choose the first n sensors (the ones with highest AUROC values) and replace all observations in these sensors by zeros in all samples in validation and test set. The number of missing sensors is defined indirectly from the user with the sensors' missing ratio which ranges from 0.1 to 0.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.9 ADDITIONAL INFORMATION ON MISSING PATTERN</head><p>This work propose RAINDROP which is a novel solution for irregularity in multivariate time series through inter-sensor dependencies. RAINDROP is not in conflict with other solutions (such as missing pattern and temporal decay) for irregularity. However, as the missing pattern is widely discussed in modelling incomplete time series <ref type="bibr" target="#b1">(Che et al., 2018)</ref>, we explore how to combine the advantages of relational structures and missing pattern. We adopt mask matrix as a proxy of missing pattern as in <ref type="bibr" target="#b1">Che et al. (2018)</ref>. Taking the architecture of RAINDROP, we concatenate the observation x t i,u with a binary mask indicator b t i,u as input. The indicator b t i,u is set as 1 when there is an observation of sensor i at time t and set as 0 otherwise. All the experimental settings and hyperparameters are the same as in RAINDROP (P19; Setting 1). The experimental results show that taking advantage of missing pattern can slightly boost the AUROC by 1.2% and AUPRC by 0.9% in P19. This empirically shed the light for future research on integrating multiple characteristics in representation of irregularly time series.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.10 COMPARISON BETWEEN TEMPORAL ATTENTION AND LSTM</head><p>We conduct extensive experiments to compare the effectiveness of temporal attention and LSTM. To this end, we replace the temporal attention in sensor embedding generation (Eq 4-5) in RAINDROP by LSTM layer which processes all observation embeddings sequentially. We use zero padding to convert the irregular observations into fixed-length time series so the data can be fed into LSTM architecture. We regard the last output of LSTM as generated sensor embedding. The number of LSTM cells equal to the dimension of observation embedding. All the model structures are identical except in the part of temporal attention and LSTM. We keep all experimental settings (P19; Setting 1) and hyperparameter selections the same. The experimental results show that the temporal self-attention outperform LSTM by 1.8% (AUROC) and additionally saved 49% of the training time. One potential reason is that the self-attention mechanism avoids recursion and allows parallel computation and also reduces performance degradation caused by long-term dependencies <ref type="bibr" target="#b11">(Ganesh et al., 2021;</ref><ref type="bibr" target="#b47">Vaswani et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.11 ADDITIONAL INFORMATION ON METHOD BENCHMARKING</head><p>Taking experimental Setting 1 (i.e., classic time series classification) as an example, we conduct extensive experiments to compare Raindrop with ODE-RNN <ref type="bibr" target="#b7">(Chen et al., 2020)</ref>, DGM 2 -O <ref type="bibr" target="#b53">(Wu et al., 2021)</ref>, EvoNet <ref type="bibr" target="#b16">(Hu et al., 2021)</ref>, and MTGNN <ref type="bibr">(Wu et al., 2020c)</ref>. As <ref type="bibr">IP-Net (Shukla &amp; Marlin, 2018)</ref> and mTAND <ref type="bibr" target="#b42">(Shukla &amp; Marlin, 2021)</ref> are from the same authors, we only compare with mTAND which is the latest model. For the baselines, we follow the settings as provided in their public codes. For methods, which cannot deal with irregular data (e.g., EvoNet and MTGNN), we first impute the missing data using mean imputation and then feed data into the model. For forecasting models (e.g., MTGNN) which are strictly not comparable with the proposed classification model, we formulate the task as a single-step forecasting, concatenate the learned representations from all sensors and feed into a fully-connected layer (work as classifier) to make prediction, and use cross-entropy to quantify the loss.</p><p>A.12 RESULTS FOR P19 (SETTINGS 2-3)</p><p>Here we report the experimental results for P19 in Setting 2 (Table <ref type="table" target="#tab_8">4</ref>) and Setting 3 (Table <ref type="table" target="#tab_9">5</ref>).  Although RAINDROP is not designed to address domain adaptation explicitly, the results show that RAINDROP performs better than baselines when transferring from one group of samples to another. One reason for our good performance is that the learned inter-sensor weights and dependency graphs are sample-specific and their learning is based on the sample's observations. Thus, the proposed RAINDROP has the power, to some extent, to adaptively learn the inter-sensor dependencies based on the test sample's measurements. RAINDROP is not generalizing to new groups, but generalizing to new samples, which leads to a good performance even though our model is not designed for domain adaptation. We validate the reason empirically. We remove the inter-sensor dependencies (set all sensors isolated in the dependency graph; set all α t i,uv and e t i,uv as 0) in RAINDROP and evaluate the model in group-wise time series classification. The experimental results show that the performance drops a lot when excluding dependency graphs and message passing in RAINDROP (Table <ref type="table" target="#tab_10">6</ref>). Without inter-sensor dependencies our model is on par with other baselines and does not outperform them by a large margin. The nodes numbered from 0 to 33 denote 34 sensors used in P19 (sensor names are listed in Appendix A.15). To make the visualized structures easier to understand, we use darker green to denote higher weight value and yellow to denote lower weight value. We can observe distinguishable patterns across two learned sensor dependency graphs, indicating RAINDROP is able to adaptively learn graph structures that are sensitive to the classification task. For example, we find that the nodes 1 (pulse oximetry), 5 (diastolic BP), and 12 (partial pressure of carbon dioxide from arterial blood) have lower weights in negative samples. We provide ablation study, taking PAM at Setting 1 as an example, in Table <ref type="table" target="#tab_11">7</ref>. In the setup of 'W/o sensor level concatenation', we take the average of all sensor embeddings (in stead of concatenating them together) to obtain sample embedding. Experimental results show that the full RAINDROP model achieves the best performance, indicating every component or designed structure is useful to the model. For example, we find that excluding inter-sensor attention weights α t i,uv will cause a decrease of 3.9% in accuracy while excluding edge weights e i,uv (i.e., dependency graphs) will drop the accuracy by 7.1%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.15 VISUALIZATION OF INTER-SENSOR DEPENDENCY GRAPHS LEARNED BY RAINDROP</head><p>We visualize the learned inter-sensor dependencies (i.e., e i,uv before the averaging operation in Eq. 3) on P19 in early sepsis prediction. The visualizations are implemented with Cytoscape <ref type="bibr" target="#b40">(Shannon et al., 2003)</ref>. The data shown are for testing set of P19 including 3,881 samples (3708 negative and 173 positive). As RAINDROP learns the specific graph for each sample, we take average of all positive samples and visualize it in Figure <ref type="figure" target="#fig_3">4b</ref>; and visualize the average of all negative samples in Figure <ref type="figure" target="#fig_3">4b</ref>. As we take average, the edges with weights smaller than 0.1 (means they rarely appear in graphs) are ignored. The averaged edge weights range from 0.1 to 1. We initialize all sample graphs as complete graph that has 1,156 = 34 × 34 edges, then prune out 50% of them in training phase, remaining 578 edges. The 34 nodes in figures denote 34 sensors measured in P19, as listed Figure <ref type="figure">5</ref>: Differential structure of dependency graphs between positive and negative samples. The edges are directed. We select the top 50 edges with largest difference (in absolute value) between two patterns. The edges are colored by the divergences. The darker color denotes the connection is more crucial to classification task. Node 0 is not included in this figure as it is not connected with any sensor. We can infer that the heart rate is stable whether the patient will get sepsis or not. Moreover, we can see the edge from node 3 (systolic BP) to node 13 (Oxygen saturation from arterial blood) and the connection from node 6 (Respiration rate) to node 25 (Potassium) are informative for distinguishing sample classes. <ref type="url" target="https://physionet.org/content/challenge-2019/1.0.0/">https://physionet.org/content/challenge-2019/1.0.0/</ref>. We list the sensor names here: 0: HR; 1: O2Sat; 2: Temp; 3: SBP; 4: MAP; 5: DBP; 6: Resp; 7: EtCO2; 8: BaseExcess; 9: HCO3; 10: FiO2; 11: pH; 12: PaCO2; 13: SaO2; 14: AST; 15: <ref type="bibr">BUN;</ref><ref type="bibr">16: Alkalinephos;</ref><ref type="bibr">17: Calcium;</ref><ref type="bibr">18: Chloride;</ref><ref type="bibr">19: Creatinine;</ref><ref type="bibr">20: Bilirubin_direct;</ref><ref type="bibr">21: Glucose;</ref><ref type="bibr">22: Lactate;</ref><ref type="bibr">23: Magnesium;</ref><ref type="bibr">24: Phosphate;</ref><ref type="bibr">25: Potassium;</ref><ref type="bibr">26: Bilirubin_total;</ref><ref type="bibr">27: TroponinI;</ref><ref type="bibr">28: Hct;</ref><ref type="bibr">29: Hgb;</ref><ref type="bibr">30: PTT;</ref><ref type="bibr">31: WBC;</ref><ref type="bibr">32: Fibrinogen;</ref><ref type="bibr">33: Platelets.</ref> We also visualize the differential inter-sensor connections between the learned dependency graphs from patients who are likely to have sepsis and the graphs from patients who are unlikely to suffer from sepsis. Based on the aggregated graph structures of positive and negative samples, we calculate the divergence between two groups of patients and report the results in Figure <ref type="figure">5</ref>. In detail, we sort edges by the absolute difference of edge weights across negative and positive samples. On top of the visualization of the 50 most distinctive edges, we can have a series of concrete insights. For example, the dependency between node 6 (Respiration rate) to node 25 (Potassium) is important to the early prediction of sepsis. Note these data-driven observations could be biased and still need confirmation and future analysis from healthcare professionals. The edges in both Figure <ref type="figure" target="#fig_3">4</ref> and Figure <ref type="figure">5</ref> are directed. The edge arrows might be difficult to recognize due to the small figure size. We will provide high-resolution figures to our public repository. Furthermore, we statistically measure the similarities across samples within the same class and dissimilarities across samples from different classes. Specifically, for every sample, we calculate: 1) the average Euclidean distance between its dependency graph and the dependency graphs of all samples from the same class; 2) the average distance with all samples from the different classes. The P19 dataset has 38,803 samples including 1,623 positive samples and 37,180 negative samples. For a fair comparison, we randomly select 1,623 samples from the negative cohort, then mixed them with an equal number of positive samples to measure the averaged Euclidean distances intra-and inter-classes. We select the cohort for 5 independent times with replacement. We find that the distance ((8.6 ± 1.7) × 10 -5 ) among dependency graphs of positive samples is smaller than the distance ((12.9 ± 3.1) × 10 -5 ) across samples. The results show that the learned dependency graphs are similar within the same class and dissimilar across classes, which demonstrates RAINDROP can learn label-sensitive dependency graphs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure1: The RAINDROP approach. For sample Si, sensor u is recorded at time t1 as value x t 1 i,u , triggering a propagation and transformation of neural messages along edges of Si's sensor dependency graph.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Hierarchical structure of irregular multivariate time series dataset. RAINDROP embeds individual observations considering inter-sensor dependencies (Sec. 3.3), aggregates them into a sensor embedding using temporal attention (Sec. 3.4), and finally integrates sensor embeddings into a sample embedding (Sec. 3.5).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>) P19 (Reyna et al., 2020) includes 38,803 patients that are monitored by 34 sensors. Each patient is associated with a binary label representing the occurrence of sepsis. (2) P12 (Goldberger et al., 2000) records temporal measurements of 36 sensors of 11,988 patients in the first 48-hour stay in ICU. The samples are labeled based on hospitalization length. (3) PAM (Reiss &amp; Stricker, 2012) contains 5,333 segments from 8 activities of daily living that are measured by 17 sensors. Details are in Appendix A.5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure4: Learned structure for negative and positive samples (P19; Setting 1). The nodes numbered from 0 to 33 denote 34 sensors used in P19 (sensor names are listed in Appendix A.15). To make the visualized structures easier to understand, we use darker green to denote higher weight value and yellow to denote lower weight value. We can observe distinguishable patterns across two learned sensor dependency graphs, indicating RAINDROP is able to adaptively learn graph structures that are sensitive to the classification task. For example, we find that the nodes 1 (pulse oximetry), 5 (diastolic BP), and 12 (partial pressure of carbon dioxide from arterial blood) have lower weights in negative samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Sensor-level processing in sample</head><label></label><figDesc></figDesc><table><row><cell>Observation</cell><cell></cell><cell></cell></row><row><cell>u</cell><cell>v</cell><cell></cell></row><row><cell>u</cell><cell>v</cell><cell>Attention</cell></row><row><cell></cell><cell></cell><cell>weights</cell></row><row><cell>u</cell><cell>v</cell><cell>Sensor</cell></row><row><cell></cell><cell></cell><cell>embedding</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>-level processing in sample</head><label></label><figDesc></figDesc><table><row><cell>u</cell><cell>v</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell>Learned embeddings</cell></row><row><cell>Observation (input)</cell><cell>Sensors</cell><cell>Dot product</cell><cell>Message passing</cell><cell>Weight vector</cell><cell>Time representation</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Edge weight attention weight Inter-sensor a b Stacked observation embeddings Sample records the value of sensor at time c Update edge weight in Sample at layer β</head><label></label><figDesc></figDesc><table><row><cell>Figure 3: (a) RAINDROP generates observation embedding h t i,u based on observed value x t i,u at t, passes</cell></row><row><cell>message to neighbor sensors such as v, and generates h t i,v through inter-sensor dependencies. The α t i,uv denotes</cell></row><row><cell>a time-specific attention weight, calculated based on time representation p t</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3"><head>Table 1 and</head><label>1</label><figDesc>Appendix A.11. Details on hyperparameter selection and baselines are in Appendix A.6, and evaluation metrics are presented in Appendix A.7. 4.1 RESULTS ACROSS DIVERSE EVALUATION SETTINGS Setting 1: Classic time series classification. Setup. Setup. Setup. Setup. Setup. Setup. Setup. Setup. Setup. We randomly split the dataset into training (80%), validation (10%), and test (10%) set. The indices of these splits are fixed across all methods. Results. Results. Results. Results. Results. Results. Results. Results. Results. As shown in Table</figDesc><table><row><cell>Setup. Setup. Setup. Setup. Setup. Setup. Setup. Setup.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 1 :</head><label>1</label><figDesc>Method benchmarking on irregularly sampled time series classification (Setting 1).</figDesc><table><row><cell></cell><cell></cell><cell>P19</cell><cell>P12</cell><cell></cell><cell></cell><cell></cell><cell>PAM</cell><cell></cell></row><row><cell>Methods</cell><cell>AUROC</cell><cell>AUPRC</cell><cell>AUROC</cell><cell>AUPRC</cell><cell>Accuracy</cell><cell>Precision</cell><cell>Recall</cell><cell>F1 score</cell></row><row><cell>Transformer</cell><cell>83.2 ± 1.3</cell><cell>47.6 ± 3.8</cell><cell>65.1 ± 5.6</cell><cell>95.7 ± 1.6</cell><cell>83.5 ± 1.5</cell><cell>84.8 ± 1.5</cell><cell>86.0 ± 1.2</cell><cell>85.0 ± 1.3</cell></row><row><cell>Trans-mean</cell><cell>84.1 ± 1.7</cell><cell>47.4 ± 1.4</cell><cell>66.8 ± 4.2</cell><cell>95.9 ± 1.1</cell><cell>83.7 ± 2.3</cell><cell>84.9 ± 2.6</cell><cell>86.4 ± 2.1</cell><cell>85.1 ± 2.4</cell></row><row><cell>GRU-D</cell><cell>83.9 ±1.7</cell><cell>46.9 ± 2.1</cell><cell>67.2 ± 3.6</cell><cell>95.9 ± 2.1</cell><cell>83.3 ± 1.6</cell><cell>84.6 ± 1.2</cell><cell>85.2 ± 1.6</cell><cell>84.8 ± 1.2</cell></row><row><cell>SeFT</cell><cell>78.7 ± 2.4</cell><cell>31.1 ± 2.8</cell><cell>66.8 ± 0.8</cell><cell>96.2 ± 0.2</cell><cell>67.1 ± 2.2</cell><cell>70.0 ± 2.4</cell><cell>68.2 ± 1.5</cell><cell>68.5 ± 1.8</cell></row><row><cell>mTAND</cell><cell>80.4 ± 1.3</cell><cell>32.4 ± 1.8</cell><cell>65.3 ± 1.7</cell><cell>96.5 ± 1.2</cell><cell>74.6 ± 4.3</cell><cell>74.3 ± 4.0</cell><cell>79.5 ± 2.8</cell><cell>76.8 ± 3.4</cell></row><row><cell>IP-Net</cell><cell>84.6 ± 1.3</cell><cell>38.1 ± 3.7</cell><cell>72.5 ± 2.4</cell><cell>96.7 ± 0.3</cell><cell>74.3 ± 3.8</cell><cell>75.6 ± 2.1</cell><cell>77.9 ± 2.2</cell><cell>76.6 ± 2.8</cell></row><row><cell>DGM 2 -O</cell><cell>86.7 ± 3.4</cell><cell>44.7 ± 11.7</cell><cell>71.2 ± 2.5</cell><cell>96.9 ± 0.4</cell><cell>82.4 ± 2.3</cell><cell>85.2 ± 1.2</cell><cell>83.9 ± 2.3</cell><cell>84.3 ± 1.8</cell></row><row><cell>MTGNN</cell><cell>81.9 ± 6.2</cell><cell>39.9 ± 8.9</cell><cell>67.5 ± 3.1</cell><cell>96.4 ± 0.7</cell><cell>83.4 ± 1.9</cell><cell>85.2 ± 1.7</cell><cell>86.1 ± 1.9</cell><cell>85.9 ± 2.4</cell></row><row><cell>RAINDROP</cell><cell>87.0 ± 2.3</cell><cell>51.8 ± 5.5</cell><cell>72.1 ± 1.3</cell><cell>97.0 ± 0.4</cell><cell>88.5 ± 1.5</cell><cell>89.9 ± 1.5</cell><cell>89.9 ± 0.6</cell><cell>89.8 ± 1.0</cell></row><row><cell cols="9">Setting 2: Leave-fixed-sensors-out. Setup. Setup. Setup. Setup. Setup. Setup. Setup. Setup. Setup. Setup. Setup. Setup. Setup. Setup. Setup. Setup. Setup. RAINDROP can compensate for missing sensor obser-</cell></row><row><cell cols="5">vations by exploiting dependencies between sensors.</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5"><head></head><label></label><figDesc>. The left-out sensors are fixed across samples and models. Results. Results. Results. Results. Results. Results. Results. Results. Results. We report results taking PAM as an example. In Table2(left block), we observe that RAINDROP achieves top performance in 18 out of 20 settings when the number of left-out sensors goes from 10% to 50%. With the increased amount of missing data, RAINDROP yield greater performance improvements. RAINDROP outperforms baselines by up to 24.9% in accuracy, 50.3% in precision, 29.3% in recall, and 42.8% in F1 score.Setting 3: Leave-random-sensors-out. Setup. Setup. Setup. Setup. Setup. Setup. Setup. Setup. Setup. Setting 3 is similar to Setting 2 except that left-out sensors are randomly selected in each sample instead of being fixed. In each test sample, we select a subset of sensors and regard them as missing by replacing all of their observations with zeros. Results. Results. Results. Results. Results. Results. Results. Results. Results. We provide results for the PAM dataset in Table</figDesc><table><row><cell>Results. Results. Results. Results. Results. Results. Results. Results. Setup. Setup. Setup. Setup. Setup. Setup. Setup. Setup. Results. Results. Results. Results. Results. Results. Results. Results.</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6"><head>Table 2 :</head><label>2</label><figDesc>Classification performance on samples with a fixed set of left-out sensors (Setting 2) or random missing sensors (Setting 3) on the PAM dataset. Results for P19 dataset (Settings 2-3) are shown in Appendix A.12.</figDesc><table><row><cell>Missing sensor ratio</cell><cell>Methods</cell><cell>Accuracy</cell><cell cols="4">PAM (Setting 2: leave-fixed-sensors-out) Precision Recall F1 score</cell><cell cols="4">PAM (Setting 3: leave-random-sensors-out) Accuracy Precision Recall F1 score</cell></row><row><cell></cell><cell>Transformer</cell><cell cols="2">60.3 ± 2.4</cell><cell>57.8 ± 9.3</cell><cell>59.8 ± 5.4</cell><cell>57.2 ± 8.0</cell><cell>60.9 ± 12.8</cell><cell>58.4 ± 18.4</cell><cell>59.1 ± 16.2</cell><cell>56.9 ± 18.9</cell></row><row><cell></cell><cell>Trans-mean</cell><cell cols="2">60.4 ± 11.2</cell><cell>61.8 ± 14.9</cell><cell>60.2 ± 13.8</cell><cell>58.0 ± 15.2</cell><cell>62.4 ± 3.5</cell><cell>59.6 ± 7.2</cell><cell>63.7 ± 8.1</cell><cell>62.7 ± 6.4</cell></row><row><cell>10%</cell><cell>GRU-D</cell><cell cols="2">65.4 ± 1.7</cell><cell>72.6 ± 2.6</cell><cell>64.3 ± 5.3</cell><cell>63.6 ± 0.4</cell><cell>68.4 ± 3.7</cell><cell>74.2 ± 3.0</cell><cell>70.8 ± 4.2</cell><cell>72.0 ± 3.7</cell></row><row><cell></cell><cell>SeFT</cell><cell cols="2">58.9 ± 2.3</cell><cell>62.5 ± 1.8</cell><cell>59.6 ± 2.6</cell><cell>59.6 ± 2.6</cell><cell>40.0 ± 1.9</cell><cell>40.8 ± 3.2</cell><cell>41.0 ± 0.7</cell><cell>39.9 ± 1.5</cell></row><row><cell></cell><cell>mTAND</cell><cell cols="2">58.8 ± 2.7</cell><cell>59.5 ± 5.3</cell><cell>64.4 ± 2.9</cell><cell>61.8 ± 4.1</cell><cell>53.4 ± 2.0</cell><cell>54.8 ± 2.7</cell><cell>57.0 ± 1.9</cell><cell>55.9 ± 2.2</cell></row><row><cell></cell><cell>RAINDROP</cell><cell cols="2">77.2 ± 2.1</cell><cell>82.3 ± 1.1</cell><cell>78.4 ± 1.9</cell><cell>75.2 ± 3.1</cell><cell>76.7 ± 1.8</cell><cell>79.9 ± 1.7</cell><cell>77.9 ± 2.3</cell><cell>78.6 ± 1.8</cell></row><row><cell></cell><cell>Transformer</cell><cell cols="2">63.1 ± 7.6</cell><cell>71.1 ± 7.1</cell><cell>62.2 ± 8.2</cell><cell>63.2 ± 8.7</cell><cell>62.3 ± 11.5</cell><cell>65.9 ± 12.7</cell><cell>61.4 ± 13.9</cell><cell>61.8 ± 15.6</cell></row><row><cell></cell><cell>Trans-mean</cell><cell cols="2">61.2 ± 3.0</cell><cell>74.2 ± 1.8</cell><cell>63.5 ± 4.4</cell><cell>64.1 ± 4.1</cell><cell>56.8 ± 4.1</cell><cell>59.4 ± 3.4</cell><cell>53.2 ± 3.9</cell><cell>55.3 ± 3.5</cell></row><row><cell>20%</cell><cell>GRU-D</cell><cell cols="2">64.6 ± 1.8</cell><cell>73.3 ± 3.6</cell><cell>63.5 ± 4.6</cell><cell>64.8 ± 3.6</cell><cell>64.8 ± 0.4</cell><cell>69.8 ± 0.8</cell><cell>65.8 ± 0.5</cell><cell>67.2 ± 0.0</cell></row><row><cell></cell><cell>SeFT</cell><cell cols="2">35.7 ± 0.5</cell><cell>42.1 ± 4.8</cell><cell>38.1 ± 1.3</cell><cell>35.0 ± 2.2</cell><cell>34.2 ± 2.8</cell><cell>34.9 ± 5.2</cell><cell>34.6 ± 2.1</cell><cell>33.3 ± 2.7</cell></row><row><cell></cell><cell>mTAND</cell><cell cols="2">33.2 ± 5.0</cell><cell>36.9 ± 3.7</cell><cell>37.7 ± 3.7</cell><cell>37.3 ± 3.4</cell><cell>45.6 ± 1.6</cell><cell>49.2 ± 2.1</cell><cell>49.0 ± 1.6</cell><cell>49.0 ± 1.0</cell></row><row><cell></cell><cell>RAINDROP</cell><cell cols="2">66.5 ± 4.0</cell><cell>72.0 ± 3.9</cell><cell>67.9 ± 5.8</cell><cell>65.1 ± 7.0</cell><cell>71.3 ± 2.5</cell><cell>75.8 ± 2.2</cell><cell>72.5 ± 2.0</cell><cell>73.4 ± 2.1</cell></row><row><cell></cell><cell>Transformer</cell><cell cols="2">31.6 ± 10.0</cell><cell>26.4 ± 9.7</cell><cell>24.0 ± 10.0</cell><cell>19.0 ± 12.8</cell><cell>52.0 ± 11.9</cell><cell>55.2 ± 15.3</cell><cell>50.1 ± 13.3</cell><cell>48.4 ± 18.2</cell></row><row><cell></cell><cell>Trans-mean</cell><cell cols="2">42.5 ± 8.6</cell><cell>45.3 ± 9.6</cell><cell>37.0 ± 7.9</cell><cell>33.9 ± 8.2</cell><cell>65.1 ± 1.9</cell><cell>63.8 ± 1.2</cell><cell>67.9 ± 1.8</cell><cell>64.9 ± 1.7</cell></row><row><cell>30%</cell><cell>GRU-D</cell><cell cols="2">45.1 ± 2.9</cell><cell>51.7 ± 6.2</cell><cell>42.1 ± 6.6</cell><cell>47.2 ± 3.9</cell><cell>58.0 ± 2.0</cell><cell>63.2 ± 1.7</cell><cell>58.2 ± 3.1</cell><cell>59.3 ± 3.5</cell></row><row><cell></cell><cell>SeFT</cell><cell cols="2">32.7 ± 2.3</cell><cell>27.9 ± 2.4</cell><cell>34.5 ± 3.0</cell><cell>28.0 ± 1.4</cell><cell>31.7 ± 1.5</cell><cell>31.0 ± 2.7</cell><cell>32.0 ± 1.2</cell><cell>28.0 ± 1.6</cell></row><row><cell></cell><cell>mTAND</cell><cell cols="2">27.5 ± 4.5</cell><cell>31.2 ± 7.3</cell><cell>30.6 ± 4.0</cell><cell>30.8 ± 5.6</cell><cell>34.7 ± 5.5</cell><cell>43.4 ± 4.0</cell><cell>36.3 ± 4.7</cell><cell>39.5 ± 4.4</cell></row><row><cell></cell><cell>RAINDROP</cell><cell cols="2">52.4 ± 2.8</cell><cell>60.9 ± 3.8</cell><cell>51.3 ± 7.1</cell><cell>48.4 ± 1.8</cell><cell>60.3 ± 3.5</cell><cell>68.1 ± 3.1</cell><cell>60.3 ± 3.6</cell><cell>61.9 ± 3.9</cell></row><row><cell></cell><cell>Transformer</cell><cell cols="2">23.0 ± 3.5</cell><cell>7.4 ± 6.0</cell><cell>14.5 ± 2.6</cell><cell>6.9 ± 2.6</cell><cell>43.8 ± 14.0</cell><cell>44.6 ± 23.0</cell><cell>40.5 ± 15.9</cell><cell>40.2 ± 20.1</cell></row><row><cell></cell><cell>Trans-mean</cell><cell cols="2">25.7 ± 2.5</cell><cell>9.1 ± 2.3</cell><cell>18.5 ± 1.4</cell><cell>9.9 ± 1.1</cell><cell>48.7 ± 2.7</cell><cell>55.8 ± 2.6</cell><cell>54.2 ± 3.0</cell><cell>55.1 ± 2.9</cell></row><row><cell>40%</cell><cell>GRU-D</cell><cell cols="2">46.4 ± 2.5</cell><cell>64.5 ± 6.8</cell><cell>42.6 ± 7.4</cell><cell>44.3 ± 7.9</cell><cell>47.7 ± 1.4</cell><cell>63.4 ± 1.6</cell><cell>44.5 ± 0.5</cell><cell>47.5 ± 0.0</cell></row><row><cell></cell><cell>SeFT</cell><cell cols="2">26.3 ± 0.9</cell><cell>29.9 ± 4.5</cell><cell>27.3 ± 1.6</cell><cell>22.3 ± 1.9</cell><cell>26.8 ± 2.6</cell><cell>24.1 ± 3.4</cell><cell>28.0 ± 1.2</cell><cell>23.3 ± 3.0</cell></row><row><cell></cell><cell>mTAND</cell><cell cols="2">19.4 ± 4.5</cell><cell>15.1 ± 4.4</cell><cell>20.2 ± 3.8</cell><cell>17.0 ± 3.4</cell><cell>23.7 ± 1.0</cell><cell>33.9 ± 6.5</cell><cell>26.4 ± 1.6</cell><cell>29.3 ± 1.9</cell></row><row><cell></cell><cell>RAINDROP</cell><cell cols="2">52.5 ± 3.7</cell><cell>53.4 ± 5.6</cell><cell>48.6 ± 1.9</cell><cell>44.7 ± 3.4</cell><cell>57.0 ± 3.1</cell><cell>65.4 ± 2.7</cell><cell>56.7 ± 3.1</cell><cell>58.9 ± 2.5</cell></row><row><cell></cell><cell>Transformer</cell><cell cols="2">21.4 ± 1.8</cell><cell>2.7 ± 0.2</cell><cell>12.5 ± 0.4</cell><cell>4.4 ± 0.3</cell><cell>43.2 ± 2.5</cell><cell>52.0 ± 2.5</cell><cell>36.9 ± 3.1</cell><cell>41.9 ± 3.2</cell></row><row><cell></cell><cell>Trans-mean</cell><cell cols="2">21.3 ± 1.6</cell><cell>2.8 ± 0.4</cell><cell>12.5 ± 0.7</cell><cell>4.6 ± 0.2</cell><cell>46.4 ± 1.4</cell><cell>59.1 ± 3.2</cell><cell>43.1 ± 2.2</cell><cell>46.5 ± 3.1</cell></row><row><cell>50%</cell><cell>GRU-D</cell><cell cols="2">37.3 ± 2.7</cell><cell>29.6 ± 5.9</cell><cell>32.8 ± 4.6</cell><cell>26.6 ± 5.9</cell><cell>49.7 ± 1.2</cell><cell>52.4 ± 0.3</cell><cell>42.5 ± 1.7</cell><cell>47.5 ± 1.2</cell></row><row><cell></cell><cell>SeFT</cell><cell cols="2">24.7 ± 1.7</cell><cell>15.9 ± 2.7</cell><cell>25.3 ± 2.6</cell><cell>18.2 ± 2.4</cell><cell>26.4 ± 1.4</cell><cell>23.0 ± 2.9</cell><cell>27.5 ± 0.4</cell><cell>23.5 ± 1.8</cell></row><row><cell></cell><cell>mTAND</cell><cell cols="2">16.9 ± 3.1</cell><cell>12.6 ± 5.5</cell><cell>17.0 ± 1.6</cell><cell>13.9 ± 4.0</cell><cell>20.9 ± 3.1</cell><cell>35.1 ± 6.1</cell><cell>23.0 ± 3.2</cell><cell>27.7 ± 3.9</cell></row><row><cell></cell><cell>RAINDROP</cell><cell cols="2">46.6 ± 2.6</cell><cell>44.5 ± 2.6</cell><cell>42.4 ± 3.9</cell><cell>38.0 ± 4.0</cell><cell>47.2 ± 4.4</cell><cell>59.4 ± 3.9</cell><cell>44.8 ± 5.3</cell><cell>47.6 ± 5.2</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7"><head>Table 3 :</head><label>3</label><figDesc>Dataset statistics. The '#-timestamps' refers to the number of all sampling timestamps measured in this dataset. The '#-classes' means the number of categories in dataset labels. The 'Static info' indicates if sample's static attributes (e.g., height and weight) are available. The 'missing ratio' denotes the ratio between the number of missing observations and the number of all possible observations if the dataset is fully-observed.Our results on Setting 1 are consistent with those on Settings 2-4. Results on harder Settings 2-4 show that Raindrop can perform comparably better than baselines. Results across these diverse settings increase our confidence that Raindrop is quite flexible and widely applicable.A.5 FURTHER DETAILS ON DATASETS P19: PhysioNet Sepsis Early Prediction Challenge 2019. P19 dataset (Reyna et al., 2020) contains 38,803 patients and each patient is monitored by 34 irregularly sampled sensors including 8 vital signs and 26 laboratory values. The original dataset has 40,336 patients, we remove the samples with too short or too long time series, remaining 38,803 patients (the longest time series of the patient has more than one and less than 60 observations). Each patient is associated with a static vector indicating attributes: age, gender, time between hospital admission and ICU admission, ICU type, and ICU length of stay (days). Each patient has a binary label representing occurrence of sepsis within the next 6 hours. The dataset is highly imbalanced with only ∼4% positive samples.</figDesc><table><row><cell cols="6">Datasets #-samples #-sensors #-timestamps #-classes Static info Missing ratio (%)</cell></row><row><cell>P19</cell><cell>38,803</cell><cell>34</cell><cell>60</cell><cell>2 True</cell><cell>94.9</cell></row><row><cell>P12</cell><cell>11,988</cell><cell>36</cell><cell>215</cell><cell>2 True</cell><cell>88.4</cell></row><row><cell>PAM</cell><cell>5,333</cell><cell>17</cell><cell>600</cell><cell>8 False</cell><cell>60.0</cell></row></table><note><p><p><p>P12: PhysioNet Mortality Prediction Challenge 2012. P12 dataset</p><ref type="bibr" target="#b13">(Goldberger et al., 2000)</ref> </p>includes 11,988 patients (samples), after removing 12 inappropriate samples following</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8"><head>Table 4 :</head><label>4</label><figDesc>Classification on samples with fixed missing sensors (P19; Setting 2)</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Missing ratio</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Models</cell><cell></cell><cell>0%</cell><cell></cell><cell>10%</cell><cell></cell><cell>20%</cell><cell></cell><cell>30%</cell><cell></cell><cell>40%</cell><cell></cell><cell>50%</cell></row><row><cell></cell><cell>AUROC</cell><cell>AUPRC</cell><cell>AUROC</cell><cell>AUPRC</cell><cell>AUROC</cell><cell>AUPRC</cell><cell>AUROC</cell><cell>AUPRC</cell><cell>AUROC</cell><cell>AUPRC</cell><cell>AUROC</cell><cell>AUPRC</cell></row><row><cell cols="13">Transformer 83.2 ± 1.3 47.6 ± 3.8 77.4 ± 3.5 38.2 ± 4.2 75.7 ± 3.4 35.2 ± 5.4 75.1 ± 3.5 35.5 ± 4.4 75.3 ± 3.5 36.2 ± 4.2 74.9 ± 3.1 35.5 ± 5.0</cell></row><row><cell cols="13">Trans-mean 84.1 ± 1.7 47.4 ± 1.4 79.2 ± 2.7 40.6 ± 5.7 79.8 ± 2.5 38.3 ± 2.8 76.9 ± 2.4 37.5 ± 5.9 76.4 ± 2.0 36.3 ± 5.8 74.1 ± 2.3 41.3 ± 4.7</cell></row><row><cell>GRU-D</cell><cell cols="12">83.9 ± 1.7 46.9 ± 2.1 79.6 ± 2.2 37.4 ± 2.5 77.5 ± 3.1 36.5 ± 4.6 76.6 ± 2.9 35.1 ± 2.4 74.6 ± 2.7 35.9± 2.7 74.1 ± 2.9 33.2 ± 3.8</cell></row><row><cell>SeFT</cell><cell cols="10">78.7 ± 2.4 31.1 ± 2.8 77.3 ± 2.4 25.5 ± 2.3 63.5 ± 2.0 14.0 ± 1.1 62.3 ± 2.1 12.9 ± 1.2 57.8 ± 1.7 9.8 ± 1.1</cell><cell cols="2">56.0 ± 3.1 7.8 ± 1.3</cell></row><row><cell>mTAND</cell><cell cols="12">80.4 ± 1.3 32.4 ± 1.8 79.7 ± 2.2 29.0 ± 4.3 77.8 ± 1.9 25.3 ± 2.4 77.7 ± 1.9 27.8 ± 2.6 79.4 ± 2.0 32.1 ± 2.1 77.3 ± 2.1 27.0 ± 2.5</cell></row><row><cell>RAINDROP</cell><cell cols="12">87.0 ± 2.3 51.8 ± 5.5 84.3 ± 2.5 46.1 ± 3.5 81.9 ± 2.1 45.2 ± 6.4 81.4 ± 2.1 43.7 ± 7.2 81.8 ± 2.2 44.9 ± 6.6 79.7 ± 1.9 43.8 ± 5.6</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9"><head>Table 5 :</head><label>5</label><figDesc>Classification on samples with random missing sensors (P19; Setting 3)</figDesc><table><row><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell cols="2">Missing ratio</cell><cell></cell><cell></cell><cell></cell><cell></cell></row><row><cell>Models</cell><cell></cell><cell>0%</cell><cell></cell><cell>10%</cell><cell></cell><cell>20%</cell><cell></cell><cell>30%</cell><cell></cell><cell>40%</cell><cell></cell><cell>50%</cell></row><row><cell></cell><cell>AUROC</cell><cell>AUPRC</cell><cell>AUROC</cell><cell>AUPRC</cell><cell>AUROC</cell><cell>AUPRC</cell><cell>AUROC</cell><cell>AUPRC</cell><cell>AUROC</cell><cell>AUPRC</cell><cell>AUROC</cell><cell>AUPRC</cell></row><row><cell cols="13">Transformer 83.2 ± 1.3 47.6 ± 3.8 82.2 ± 2.7 46.8 ± 3.5 81.6 ± 3.5 42.5 ± 8.5 81.3 ± 3.1 42.1 ± 4.5 80.2 ± 2.9 41.9 ± 6.8 79.2 ± 1.9 43.7 ± 3.7</cell></row><row><cell cols="13">Trans-mean 84.1 ± 1.7 47.4 ± 1.4 82.5 ± 3.7 44.7 ± 6.8 81.7 ± 2.0 45.9 ± 3.6 81.2 ± 2.2 43.2 ± 6.3 80.2 ± 1.7 41.5 ± 4.8 79.8 ± 3.1 39.3 ± 5.1</cell></row><row><cell>GRU-D</cell><cell cols="12">83.9 ± 1.7 46.9 ± 2.1 81.2 ± 3.4 46.4 ± 2.7 78.6 ± 4.1 43.3 ± 2.4 76.3 ± 2.5 28.5 ± 2.1 74.2 ± 2.7 29.6 ± 3.1 74.6 ± 3.5 26.5 ± 4.2</cell></row><row><cell>SeFT</cell><cell cols="12">78.7 ± 2.4 31.1 ± 2.8 76.8 ± 2.2 28.3 ± 2.5 77.0 ± 2.2 24.1 ± 2.4 75.2 ± 2.2 22.5 ± 3.0 73.6 ± 2.7 18.3 ± 3.2 72.6 ± 2.5 15.7 ± 1.9</cell></row><row><cell>mTAND</cell><cell>80.</cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table><note><p>4 ± 1.3 32.4 ± 1.8 75.2 ± 2.5 24.5 ± 2.4 74.4 ± 3.5 24.6 ± 3.5 74.2 ± 3.2 22.6 ± 2.3 74.1 ± 2.6 23.1 ± 3.6 73.9 ± 3.7 24.6 ± 3.7 RAINDROP 87.0 ± 2.3 51.8 ± 5.5 85.5 ± 2.1 50.2 ± 5.5 83.5 ± 3.2 47.4 ± 7.0 83.1 ± 1.5 48.2 ± 4.7 82.6 ± 1.7 48.0 ± 5.5 80.9 ± 2.4 45.2 ± 6.9</p></note></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10"><head>Table 6 :</head><label>6</label><figDesc>Comparison of results when excluding dependency graph in RAINDROP (P19; Setting 4). The results are the same as in Table8except the row of 'RAINDROP w/o graph', where we do not consider inter-sensor dependencies and set all sensors as independent in the dependency graph.</figDesc><table><row><cell>Model</cell><cell></cell><cell></cell><cell></cell><cell cols="3">Generalizing to a new patient group</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Train: Young → Test: Old</cell><cell cols="2">Train: Old → Test: Young</cell><cell cols="2">Train: Male → Test: Female</cell><cell cols="2">Train: Female → Test: Male</cell></row><row><cell></cell><cell>AUROC</cell><cell>AUPRC</cell><cell>AUROC</cell><cell>AUPRC</cell><cell>AUROC</cell><cell>AUPRC</cell><cell>AUROC</cell><cell>AUPRC</cell></row><row><cell>Transformer</cell><cell>76.2 ± 0.7</cell><cell>30.5 ± 4.8</cell><cell>76.5 ± 1.1</cell><cell>33.7 ± 5.7</cell><cell>77.8 ± 1.1</cell><cell>26.0 ± 6.2</cell><cell>75.2 ± 1.0</cell><cell>30.3 ± 5.5</cell></row><row><cell>Trans-mean</cell><cell>80.6 ± 1.4</cell><cell>39.8 ± 4.2</cell><cell>78.4 ± 1.1</cell><cell>35.8 ± 2.9</cell><cell>80.2 ± 1.7</cell><cell>32.1 ± 1.9</cell><cell>76.4 ± 0.8</cell><cell>32.5 ± 3.3</cell></row><row><cell>GRU-D</cell><cell>76.5 ± 1.7</cell><cell>29.5 ± 2.3</cell><cell>79.6 ± 1.7</cell><cell>35.2 ± 4.6</cell><cell>78.5 ± 1.6</cell><cell>31.9 ± 4.8</cell><cell>76.3 ± 2.5</cell><cell>31.1 ± 2.6</cell></row><row><cell>SeFT</cell><cell>77.5 ± 0.7</cell><cell>26.6 ± 1.2</cell><cell>78.9 ± 1.0</cell><cell>32.7 ± 2.7</cell><cell>78.6 ± 0.6</cell><cell>31.1 ± 1.2</cell><cell>76.9 ± 0.5</cell><cell>26.4 ± 1.1</cell></row><row><cell>mTAND</cell><cell>79.0 ± 0.8</cell><cell>28.8 ± 2.3</cell><cell>79.4 ± 0.6</cell><cell>29.8 ± 1.2</cell><cell>78.0 ± 0.9</cell><cell>26.5 ± 1.7</cell><cell>78.9 ± 1.2</cell><cell>29.2 ± 2.0</cell></row><row><cell>RAINDROP w/o graph</cell><cell>80.5 ± 1.1</cell><cell>31.6 ± 2.1</cell><cell>78.5 ± 0.9</cell><cell>36.7 ± 2.7</cell><cell>81.3 ± 1.5</cell><cell>36.8 ± 1.7</cell><cell>77.5 ± 1.9</cell><cell>33.4 ± 2.6</cell></row><row><cell>RAINDROP</cell><cell>83.2 ± 1.6</cell><cell>43.6 ± 4.7</cell><cell>82.0 ± 4.4</cell><cell>44.3 ± 3.6</cell><cell>85.0 ± 1.4</cell><cell>45.2 ± 2.9</cell><cell>81.2 ± 3.8</cell><cell>40.7 ± 2.9</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_11"><head>Table 7 :</head><label>7</label><figDesc>Results of ablation study on the PAM dataset (Setting 1).To understand whether RAINDROP can adaptively adjust its structure and generalize well to other groups of samples which were not observed while training the model. In this setting we split the data into two groups, based on a specific static attribute. The first split attribute is age, where we classify people into young (&lt; 65 years) and old (≥ 65 years) groups. We also split patients into male and female by gender attribute. Given the split attribute, we use one group as a train set and randomly split the other group into equally sized validation and test set. Taking P19 as an example, we present the classification results when the training and testing samples are from different groups. As shown in Table8, RAINDROP achieves the best results over all of the four given cross-group scenarios. For instance, RAINDROP claims large margins (with 4.8% in AUROC and 13.1% in AUPRC absolute improvement) over the second best model while training on males and testing on female patients.</figDesc><table><row><cell>RAINDROP Model</cell><cell></cell><cell>Accuracy</cell><cell>Precision</cell><cell>Recall</cell><cell>F1 score</cell></row><row><cell>W/o weights vector R u</cell><cell></cell><cell cols="4">81.1 ± 2.6 81.9 ± 2.4 80.1 ± 1.6 81.6 ± 2.1</cell></row><row><cell></cell><cell cols="5">W/o e i,uv 82.6 ± 1.2 82.9± 1.6 84.3± 1.4 83.8 ± 1.7</cell></row><row><cell>W/o inter-sensor dependency</cell><cell>W/o r v W/o p t i W/o α t i,uv</cell><cell cols="4">86.5 ± 2.4 83.3 ± 1.9 82.6± 1.5 82.9 ± 1.4 79.8 ± 2.7 80.1 ± 3.6 80.6 ± 1.7 80.2 ± 2.9 85.2 ± 2.5 86.4 ± 2.7 84.5 ± 2.9 85.6 ± 2.9</cell></row><row><cell>W/o temporal attention</cell><cell></cell><cell cols="4">81.5 ± 1.9 84.6± 1.7 83.9 ± 2.5 84.2 ± 2.2</cell></row><row><cell>W/o sensor level concatenation</cell><cell></cell><cell cols="4">84.4 ± 2.1 86.7± 1.1 85.2± 1.9 85.8 ± 2.6</cell></row><row><cell>W/o regularization term L r</cell><cell></cell><cell cols="4">87.3 ± 2.9 88.6± 3.4 87.1± 2.8 87.6 ± 3.1</cell></row><row><cell>Full RAINDROP</cell><cell></cell><cell>88.5±1.5</cell><cell>89.9±1.5</cell><cell>89.9±0.6</cell><cell>89.8±1.0</cell></row><row><cell cols="4">A.13 EVALUATION ON GROUP-WISE TIME SERIES CLASSIFICATION</cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_12"><head>Table 8 :</head><label>8</label><figDesc>Classification results when train and test samples originate from different groups (P19).</figDesc><table><row><cell>Model</cell><cell></cell><cell></cell><cell></cell><cell cols="3">Generalizing to a new patient group</cell><cell></cell><cell></cell></row><row><cell></cell><cell cols="2">Train: Young → Test: Old</cell><cell cols="2">Train: Old → Test: Young</cell><cell cols="2">Train: Male → Test: Female</cell><cell cols="2">Train: Female → Test: Male</cell></row><row><cell></cell><cell>AUROC</cell><cell>AUPRC</cell><cell>AUROC</cell><cell>AUPRC</cell><cell>AUROC</cell><cell>AUPRC</cell><cell>AUROC</cell><cell>AUPRC</cell></row><row><cell>Transformer</cell><cell>76.2 ± 0.7</cell><cell>30.5 ± 4.8</cell><cell>76.5 ± 1.1</cell><cell>33.7 ± 5.7</cell><cell>77.8 ± 1.1</cell><cell>26.0 ± 6.2</cell><cell>75.2 ± 1.0</cell><cell>30.3 ± 5.5</cell></row><row><cell>Trans-mean</cell><cell>80.6 ± 1.4</cell><cell>39.8 ± 4.2</cell><cell>78.4 ± 1.1</cell><cell>35.8 ± 2.9</cell><cell>80.2 ± 1.7</cell><cell>32.1 ± 1.9</cell><cell>76.4 ± 0.8</cell><cell>32.5 ± 3.3</cell></row><row><cell>GRU-D</cell><cell>76.5 ± 1.7</cell><cell>29.5 ± 2.3</cell><cell>79.6 ± 1.7</cell><cell>35.2 ± 4.6</cell><cell>78.5 ± 1.6</cell><cell>31.9 ± 4.8</cell><cell>76.3 ± 2.5</cell><cell>31.1 ± 2.6</cell></row><row><cell>SeFT</cell><cell>77.5 ± 0.7</cell><cell>26.6 ± 1.2</cell><cell>78.9 ± 1.0</cell><cell>32.7 ± 2.7</cell><cell>78.6 ± 0.6</cell><cell>31.1 ± 1.2</cell><cell>76.9 ± 0.5</cell><cell>26.4 ± 1.1</cell></row><row><cell>mTAND</cell><cell>79.0 ± 0.8</cell><cell>28.8 ± 2.3</cell><cell>79.4 ± 0.6</cell><cell>29.8 ± 1.2</cell><cell>78.0 ± 0.9</cell><cell>26.5 ± 1.7</cell><cell>78.9 ± 1.2</cell><cell>29.2 ± 2.0</cell></row><row><cell>RAINDROP</cell><cell>83.2 ± 1.6</cell><cell>43.6 ± 4.7</cell><cell>82.0 ± 4.4</cell><cell>44.3 ± 3.6</cell><cell>85.0 ± 1.4</cell><cell>45.2 ± 2.9</cell><cell>81.2 ± 3.8</cell><cell>40.7 ± 2.9</cell></row><row><cell cols="5">A.14 FURTHER DETAILS ON ABLATION STUDY</cell><cell></cell><cell></cell><cell></cell><cell></cell></row></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1" xml:id="foot_0"><p>Code and datasets are available at https://github.com/mims-harvard/Raindrop.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>ACKNOWLEDGMENTS</head><p>This material is based upon work supported by the <rs type="funder">Under Secretary of Defense for Research and Engineering under Air Force</rs> Contract No. <rs type="grantNumber">FA8702-15-D-0001</rs>. M.Z. is supported, in part, by <rs type="funder">NSF</rs> under nos. <rs type="grantNumber">IIS-2030459</rs> and <rs type="grantNumber">IIS-2033384</rs>, <rs type="funder">Harvard Data Science Initiative</rs>, <rs type="funder">Amazon Research Award, Bayer Early Excellence in Science Award</rs>, <rs type="person">AstraZeneca Research</rs>, and <rs type="person">Roche Alliance</rs> with Distinguished Scientists Award. Any opinions, findings, conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the funders. The authors declare that there are no conflict of interests.</p></div>
<div><head>REPRODUCIBILITY STATEMENT</head><p>We ensure the reproducibility of our work by clearly presenting the model and providing publicly accessible code and data. For all datasets used in this work, we share downloadable links to the raw sources and processed and ready-to-run datasets with the research community through this link: <ref type="url" target="https://github.com/mims-harvard/Raindrop">https://github.com/mims-harvard/Raindrop</ref>. We specify all training details (e.g., preprocessing, data splits, hyperparameters, sensor selection) in the main text and <rs type="funder">Appendix. Python implementation of RAINDROP</rs> and all baseline methods is available at the aforementioned link. Detailed description of data, scripts, and configurations along with examples of usage are also provided.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_77EXd7y">
					<idno type="grant-number">FA8702-15-D-0001</idno>
				</org>
				<org type="funding" xml:id="_ZGPUR4H">
					<idno type="grant-number">IIS-2030459</idno>
				</org>
				<org type="funding" xml:id="_T4YeqpY">
					<idno type="grant-number">IIS-2033384</idno>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ETHICS STATEMENT</head><p>The ability of RAINDROP to learn robust information about sensors' representations and dependencies creates new opportunities for applications, where time series are predominant, e.g., in healthcare, biology, and finance. In all these fields, especially in healthcare applications, our method should be used with caution. Although our model can gain valuable insights from time series, users must consider the limitations of machine-guided predictions. As with all data-driven solutions, our model may make biased predictions. In the case of biomedical data, biases can exist within the data itself, which can be, for example, caused by considering demographic attributes, such as age, weight, and gender, that might correlate with protected/regulated attributes. When target classes are highly imbalanced, our model can mitigate the issues by upsampling minority classes in every processed batch.</p><p>All datasets in this paper are publicly available and are not associated with any privacy or security concern. Further, all data are anonymized to guard against breaching patients' protected health information. We followed PhysioNet privacy policy and guidelines (<ref type="url" target="https://archive.physionet.org/privacy.shtml">https://archive.physionet.org/ privacy.shtml</ref>) when experimenting with P12 and P19 datasets.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Biases in electronic health record data due to processes within the healthcare system: retrospective observational study</title>
		<author>
			<persName><forename type="first">Denis</forename><surname>Agniel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Griffin</forename><forename type="middle">M</forename><surname>Isaac S Kohane</surname></persName>
		</author>
		<author>
			<persName><surname>Weber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">British Medical Journal</title>
		<imprint>
			<biblScope unit="volume">361</biblScope>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Recurrent neural networks for multivariate time series with missing values</title>
		<author>
			<persName><forename type="first">Zhengping</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Purushotham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yan</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scientific Reports</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="12" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Neural ordinary differential equations</title>
		<author>
			<persName><forename type="first">Tian</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Yulia</forename><surname>Rubanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jesse</forename><surname>Bettencourt</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">K</forename><surname>Duvenaud</surname></persName>
		</author>
		<editor>NeurIPS</editor>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multi-task time series forecasting with shared attention</title>
		<author>
			<persName><forename type="first">Zekai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">E</forename><surname>Jiaze</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Sheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiuzheng</forename><surname>Cheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM Workshop</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="917" to="925" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Learning phrase representations using rnn encoder-decoder for statistical machine translation</title>
		<author>
			<persName><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bart</forename><surname>Van Merriënboer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fethi</forename><surname>Bougares</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Holger</forename><surname>Schwenk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning the graphical structure of electronic health records with graph convolutional transformer</title>
		<author>
			<persName><forename type="first">Edward</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Dusenberry</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gerardo</forename><surname>Flores</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Emily</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="606" to="613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Graph mixture density networks</title>
		<author>
			<persName><forename type="first">Federico</forename><surname>Errica</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Davide</forename><surname>Bacciu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessio</forename><surname>Micheli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="3025" to="3035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Time series data imputation: A survey on deep learning approaches</title>
		<author>
			<persName><forename type="first">Chenguang</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chen</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2011.11347</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep learning for time series classification: a review</title>
		<author>
			<persName><forename type="first">Hassan</forename><surname>Ismail Fawaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Germain</forename><surname>Forestier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lhassane</forename><surname>Idoumghar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre-Alain</forename><surname>Muller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Data Mining and Knowledge Discovery</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="917" to="963" />
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Hierarchical inter-message passing for learning on molecular graphs</title>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Fey</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan-Gin</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frank</forename><surname>Weichert</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2006.12179</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Message passing for hyper-relational knowledge graphs</title>
		<author>
			<persName><forename type="first">Mikhail</forename><surname>Galkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Priyansh</forename><surname>Trivedi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gaurav</forename><surname>Maheshwari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ricardo</forename><surname>Usbeck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jens</forename><surname>Lehmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2009.10847</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Compressing large-scale transformer-based models: A case study on bert</title>
		<author>
			<persName><forename type="first">Prakhar</forename><surname>Ganesh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Ali Khan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hassan</forename><surname>Sajjad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Preslav</forename><surname>Nakov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Deming</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marianne</forename><surname>Winslett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="1061" to="1080" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Patrick</forename><forename type="middle">F</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName><surname>Dahl</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="1263" to="1272" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<author>
			<persName><forename type="first">L</forename><surname>Ary</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luis</forename><forename type="middle">A Nunes</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">L</forename><surname>Amaral</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeffrey</forename><forename type="middle">M</forename><surname>Glass</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Plamen</forename><surname>Hausdorff</surname></persName>
		</author>
		<author>
			<persName><surname>Ch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><forename type="middle">G</forename><surname>Ivanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><forename type="middle">E</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><forename type="middle">B</forename><surname>Mietus</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chung-Kang</forename><surname>Moody</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Harry</forename><forename type="middle">Eugene</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><surname>Stanley</surname></persName>
		</author>
		<title level="m">PhysioBank, PhysioToolkit, and PhysioNet: components of a new research resource for complex physiologic signals</title>
		<imprint>
			<date type="published" when="2000">2000</date>
		</imprint>
	</monogr>
	<note>Circulation, 101 23:E215-20</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Set functions for time series</title>
		<author>
			<persName><forename type="first">Max</forename><surname>Horn</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Moor</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christian</forename><surname>Bock</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bastian</forename><surname>Rieck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karsten</forename><surname>Borgwardt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4303" to="4313" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gang</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7132" to="7141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Time-series event prediction with evolutionary state graph</title>
		<author>
			<persName><forename type="first">Wenjie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ziqiang</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiang</forename><surname>Ren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WSDM</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="580" to="588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Heterogeneous graph transformer</title>
		<author>
			<persName><forename type="first">Ziniu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuxiao</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kuansan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yizhou</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Web Conference</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="2704" to="2710" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Unsupervised change detection analysis in satellite image time series using deep learning combined with graph-based approaches</title>
		<author>
			<persName><forename type="first">Ekaterina</forename><surname>Kalinicheva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dino</forename><surname>Ienco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jérémie</forename><surname>Sublime</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Trocan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="1450" to="1466" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<author>
			<persName><forename type="first">Patrick</forename><surname>Kidger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Morrill</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Terry</forename><surname>Lyons</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.08926</idno>
		<title level="m">Neural controlled differential equations for irregular time series</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Learning dynamic graph representation of brain connectome with spatio-temporal attention</title>
		<author>
			<persName><forename type="first">Byung-Hoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jong</forename><surname>Chul Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jae-Jin</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2105.13495</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Ba</surname></persName>
		</author>
		<author>
			<persName><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Exploring inter-sensor correlation for missing data estimation</title>
		<author>
			<persName><forename type="first">Liying</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tongquan</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
			<publisher>IEEE</publisher>
			<biblScope unit="page" from="2108" to="2114" />
		</imprint>
		<respStmt>
			<orgName>IECON</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<author>
			<persName><forename type="first">Michelle M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kexin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marinka</forename><surname>Zitnik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2104.04883</idno>
		<title level="m">Representation learning for networks in biology and medicine: Advancements, challenges, and opportunities</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A scalable end-to-end gaussian process adapter for irregularly sampled time series classification</title>
		<author>
			<persName><forename type="first">S</forename><forename type="middle">C</forename></persName>
		</author>
		<author>
			<persName><forename type="first">-X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">M</forename><surname>Marlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1804" to="1812" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning from irregularly-sampled time series: A missing data perspective</title>
		<author>
			<persName><forename type="first">Steven</forename><surname>Cheng-Xian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Li</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Marlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="5937" to="5946" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Type-aware anchor link prediction across heterogeneous networks based on graph attention network</title>
		<author>
			<persName><forename type="first">Xiaoxue</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanmin</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanan</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yangxi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianlong</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanbing</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="147" to="155" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Variational message passing with structured inference networks</title>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Wu Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><forename type="middle">Emtiyaz</forename><surname>Hubacher</surname></persName>
		</author>
		<author>
			<persName><surname>Khan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<author>
			<persName><forename type="first">R</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
		<title level="m">Statistical Analysis with Missing Data</title>
		<imprint>
			<publisher>John Wiley &amp; Sons</publisher>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
	<note>3 edition</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Adversarial joint-learning recurrent neural network for incomplete time series classification</title>
		<author>
			<persName><forename type="first">Qianli</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Garrison</forename><surname>Cottrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Time series cluster kernels to exploit informative missingness and incomplete label information</title>
		<author>
			<persName><forename type="first">Karl</forename><surname>Øyvind Mikalsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cristina</forename><surname>Soguero-Ruiz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Filippo</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Bianchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Revhaug</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Jenssen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page">107896</biblScope>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Phased lstm: accelerating recurrent network training for long or event-based sequences</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Neil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shih-Chii</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="3889" to="3897" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Message passing attention networks for document understanding</title>
		<author>
			<persName><forename type="first">Giannis</forename><surname>Nikolentzos</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Tixier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michalis</forename><surname>Vazirgiannis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="8544" to="8551" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Skillful precipitation nowcasting using deep generative models of radar, arxiv</title>
		<author>
			<persName><surname>Ravuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Lenc</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Willson</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Kangin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Lam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Mirowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Athanassiadou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Kashem</surname></persName>
		</author>
		<author>
			<persName><surname>Madge</surname></persName>
		</author>
		<author>
			<persName><surname>Prudden</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">597</biblScope>
			<biblScope unit="page" from="672" to="677" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Introducing a new benchmarked dataset for activity monitoring</title>
		<author>
			<persName><forename type="first">Attila</forename><surname>Reiss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Didier</forename><surname>Stricker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISWC</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="108" to="109" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Early prediction of sepsis from clinical data: The physionet/computing in cardiology challenge</title>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">S</forename><surname>Matthew A Reyna</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Russell</forename><surname>Josef</surname></persName>
		</author>
		<author>
			<persName><surname>Jeter</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Supreeth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Shashikumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shamim</forename><surname>Brandon Westover</surname></persName>
		</author>
		<author>
			<persName><surname>Nemati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">D</forename><surname>Gari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Clifford</surname></persName>
		</author>
		<author>
			<persName><surname>Sharma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Critical Care Medicine</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="210" to="217" />
			<date type="published" when="2019">2019. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning graph distances with message passing neural networks</title>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Pau Riba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Josep</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alicia</forename><surname>Lladós</surname></persName>
		</author>
		<author>
			<persName><surname>Fornés</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="2239" to="2244" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Missing data: Our view of the state of the art</title>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">L</forename><surname>Schafer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">W</forename><surname>Graham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological Methods</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2002">2002</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Financial time series forecasting with deep learning: A systematic literature review</title>
		<author>
			<persName><forename type="first">Omer Berat</forename><surname>Sezer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mehmet</forename><surname>Ugur Gudelek</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ahmet</forename><forename type="middle">Murat</forename><surname>Ozbayoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Applied Soft Computing</title>
		<imprint>
			<biblScope unit="volume">90</biblScope>
			<biblScope unit="page">106181</biblScope>
			<date type="published" when="2005">2005-2019. 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Nrtsi: Non-recurrent time series imputation for irregularly-sampled data</title>
		<author>
			<persName><forename type="first">Siyuan</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junier B</forename><surname>Oliva</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2102.03340</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Cytoscape: a software environment for integrated models of biomolecular interaction networks</title>
		<author>
			<persName><forename type="first">Paul</forename><surname>Shannon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Markiel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Owen</forename><surname>Ozier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitin</forename><forename type="middle">S</forename><surname>Baliga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Ramage</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nada</forename><surname>Amin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benno</forename><surname>Schwikowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trey</forename><surname>Ideker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genome Research</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2498" to="2504" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Interpolation-prediction networks for irregularly sampled time series</title>
		<author>
			<persName><forename type="first">Satya</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shukla</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Marlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Multi-time attention networks for irregularly sampled time series</title>
		<author>
			<persName><forename type="first">Satya</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shukla</forename></persName>
		</author>
		<author>
			<persName><forename type="first">Benjamin</forename><surname>Marlin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">A survey on principles, models and methods for learning from irregularly sampled time series</title>
		<author>
			<persName><forename type="first">Satya</forename><surname>Narayan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shukla</forename><surname>Benjamin M Marlin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Improving irregularly sampled time series learning with dense descriptors of time</title>
		<author>
			<persName><forename type="first">Lucas</forename><forename type="middle">A</forename><surname>Rafael T Sousa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anderson</forename><forename type="middle">S</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName><surname>Soares</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2003.09291</idno>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">DATA-GRU: Dual-attention time-aware gated recurrent unit for irregular multivariate time series</title>
		<author>
			<persName><forename type="first">Qingxiong</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mang</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Baoyao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siqi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andy</forename><forename type="middle">Jinhua</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Terry</forename><surname>Cheuk-Fung Yip</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grace</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">-Hung</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pongchi</forename><surname>Yuen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="930" to="937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Self-supervised transformer for multivariate clinical time-series with missing values</title>
		<author>
			<persName><forename type="first">Sindhu</forename><surname>Tipirneni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><surname>Chandan</surname></persName>
		</author>
		<author>
			<persName><surname>Reddy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2107.14293</idno>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Attention is all you need</title>
		<author>
			<persName><forename type="first">Ashish</forename><surname>Vaswani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niki</forename><surname>Parmar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jakob</forename><surname>Uszkoreit</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Llion</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aidan</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Łukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Illia</forename><surname>Polosukhin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="5998" to="6008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Graph attention networks</title>
		<author>
			<persName><forename type="first">Petar</forename><surname>Veličković</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guillem</forename><surname>Cucurull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arantxa</forename><surname>Casanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adriana</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Liò</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Traffic flow prediction via spatial temporal graph neural network</title>
		<author>
			<persName><forename type="first">Xiaoyang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yiqi</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiliang</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Caiyan</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jian</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Web Conference 2020</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="1082" to="1092" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Dama-net: A novel predictive model for irregularly asynchronously and sparsely sampled multivariate time series</title>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ai</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ji</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhao</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ke</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chenhao</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML&apos;W</title>
		<imprint>
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Strategies for handling missing data in electronic health record derived data</title>
		<author>
			<persName><forename type="first">B</forename><forename type="middle">J</forename><surname>Wells</surname></persName>
		</author>
		<author>
			<persName><forename type="first">K</forename><forename type="middle">M</forename><surname>Chagin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">A</forename><forename type="middle">S</forename><surname>Nowacki</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><forename type="middle">W</forename><surname>Kattan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">EGEMS</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Adversarial sparse transformer for time series forecasting</title>
		<author>
			<persName><forename type="first">Sifan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xi</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qianggang</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peilin</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ying</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Dynamic gaussian mixture based deep generative model for robust forecasting on sparse multivariate time series</title>
		<author>
			<persName><forename type="first">Yinjun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingchao</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongjin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengzhang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yanchi</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuchao</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Susan</forename><surname>Davidson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">A comprehensive survey on graph neural networks</title>
		<author>
			<persName><forename type="first">Zonghan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fengwen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks and Learning Systems</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="4" to="24" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Connecting the dots: Multivariate time series forecasting with graph neural networks</title>
		<author>
			<persName><forename type="first">Zonghan</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shirui</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guodong</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jing</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaojun</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chengqi</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="753" to="763" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Mtag: Modal-temporal attention graph for unaligned human multimodal language sequences</title>
		<author>
			<persName><forename type="first">Jianing</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yongxin</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruitao</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuying</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Azaan</forename><surname>Rehman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Amir</forename><surname>Zadeh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soujanya</forename><surname>Poria</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Louis-Philippe</forename><surname>Morency</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter</title>
		<meeting>the 2021 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Human Language Technologies</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="1009" to="1021" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Graph transformer networks</title>
		<author>
			<persName><forename type="first">Seongjun</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Minbyul</forename><surname>Jeong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raehyun</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jaewoo</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hyunwoo J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NeurIPS</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="11983" to="11993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">A transformer-based framework for multivariate time series representation learning</title>
		<author>
			<persName><forename type="first">George</forename><surname>Zerveas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Srideepika</forename><surname>Jayaraman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhaval</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anuradha</forename><surname>Bhamidipaty</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carsten</forename><surname>Eickhoff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">KDD</title>
		<imprint>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="2114" to="2124" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Towards similarity-aware time-series classification</title>
		<author>
			<persName><forename type="first">Daochen</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kwei-Herng</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kaixiong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xia</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SDM</title>
		<imprint>
			<date type="published" when="2022">2022</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">A deep neural network for unsupervised anomaly detection and diagnosis in multivariate time series data</title>
		<author>
			<persName><forename type="first">Chuxu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongjin</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yuncong</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyang</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cristian</forename><surname>Lumezanu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingchao</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bo</forename><surname>Zong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nitesh V</forename><surname>Chawla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="1409" to="1416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Dynamic graph message passing networks</title>
		<author>
			<persName><forename type="first">Li</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anurag</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Philip Hs</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="3726" to="3735" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Graph neural networks: A review of methods and applications</title>
		<author>
			<persName><forename type="first">Jie</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ganqu</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shengding</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhengyan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Cheng</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lifeng</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Changcheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI Open</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="57" to="81" />
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
