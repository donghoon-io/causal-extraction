<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Event Temporal Relation Extraction with Bayesian Translational Model</title>
				<funder ref="#_aBcrzyc #_GGKRSNc #_dnhNju6">
					<orgName type="full">UK Engineering and Physical Sciences Research Council</orgName>
				</funder>
				<funder ref="#_9gvuR3d">
					<orgName type="full">UK Research and Innovation</orgName>
				</funder>
				<funder ref="#_qzYsAaa">
					<orgName type="full">Research Development Fund</orgName>
					<orgName type="abbreviated">RDF</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Xingwei</forename><surname>Tan</surname></persName>
							<email>xingwei.tan@warwick.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Warwick</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Gabriele</forename><surname>Pergola</surname></persName>
							<email>gabriele.pergola.1@warwick.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Warwick</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName><forename type="first">Yulan</forename><surname>He</surname></persName>
							<email>yulan.he@kcl.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">University of Warwick</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Department of Informatics</orgName>
								<address>
									<addrLine>King&apos;s</addrLine>
									<settlement>College London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">The Alan Turing Institute</orgName>
								<address>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Event Temporal Relation Extraction with Bayesian Translational Model</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-28T23:06+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Existing models to extract temporal relations between events lack a principled method to incorporate external knowledge. In this study, we introduce Bayesian-Trans, a Bayesian learningbased method that models the temporal relation representations as latent variables and infers their values via Bayesian inference and translational functions. Compared to conventional neural approaches, instead of performing point estimation to find the best set parameters, the proposed model infers the parameters' posterior distribution directly, enhancing the model's capability to encode and express uncertainty about the predictions. Experimental results on the three widely used datasets show that Bayesian-Trans outperforms existing approaches for event temporal relation extraction.</p><p>We additionally present detailed analyses on uncertainty quantification, comparison of priors, and ablation studies, illustrating the benefits of the proposed approach. 1</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Understanding events and how they evolve in time has been shown beneficial for natural language understanding (NLU) and for a growing number of related tasks <ref type="bibr" target="#b9">(Cheng et al., 2013;</ref><ref type="bibr" target="#b42">Wang et al., 2018;</ref><ref type="bibr" target="#b30">Ning et al., 2020;</ref><ref type="bibr" target="#b12">Geva et al., 2021;</ref><ref type="bibr" target="#b38">Sun et al., 2022)</ref>. Howeover, events often form complex structures with each other through various temporal relations, which is challenging to track even for humans <ref type="bibr">(Wang et al., 2020a)</ref>.</p><p>One of the main difficulties is the wide variety of linguistic expressions of temporal relations across different contexts. Although many of them share some linguistic similarities, most of the topics in which they occur are characterized by some shared but unspoken knowledge that determines how temporal information is expressed. For example, when it comes to health, prevention is widely 1 Experimental source code is available at <ref type="url" target="https://github.com/Xingwei-Warwick/Bayesian-Trans">https:// github.com/Xingwei-Warwick/Bayesian-Trans</ref>  practised, with many treatments (e.g., vaccinations) being effective only if administered before the onset of a disorder. On the contrary, in the automotive industry, it is common that most people repair their car after a problem occurs. However, despite its simplicity, such commonsense knowledge is rarely stated explicitly in text and varies greatly across different domains. For example, in Figure <ref type="figure" target="#fig_0">1</ref>, a detection model lacking the commonsense knowledge that vaccination can protect people from infection, tends to get confused by the complex linguistic structures in the excerpt and returns the wrong prediction entailing that 'died' happens after 'vaccinated'. Instead, with the consideration of prior temporal knowledge involving the vaccination event from an external knowledge source ATOMIC <ref type="bibr" target="#b17">(Hwang et al., 2021)</ref>, a model gives the correct prediction that 'died' occurs before 'vaccinated'.</p><p>Methods proposed in recent studies for event relation extraction are mostly end-to-end neural architectures making rather limited use of such commonsense knowledge <ref type="bibr">(Han et al., 2019a,b)</ref>. Only a few works have explored the incorporation of external knowledge to mitigate the scarcity of event annotations <ref type="bibr" target="#b29">(Ning et al., 2019;</ref><ref type="bibr">Wang et al., 2020b)</ref>.</p><p>Nevertheless, these approaches typically update the event representations with knowledge features derived from external sources, lacking a principled way of updating models' beliefs in seeing more data in the domains of interests.</p><p>In this work, we posit that the Bayesian learning framework combined with translational models can provide a principled methodology to incorporate knowledge and mitigate the lack of annotated data for event temporal relations. Translational models, such as TransE <ref type="bibr" target="#b2">(Bordes et al., 2013)</ref>, are energybased models based on the intuition that the relations between entities can be naturally represented by geometric translations in the embedding space. More concretely, a relation between a head entity and a tail entity holds if there exists a translational operation bringing the head close to the tail vector.</p><p>Specifically, we introduce a novel Bayesian Translational model (Bayesian-Trans) for event temporal relation extraction. Compared to conventional neural translational models, which only yield a point estimation of the network parameters, the Bayesian architecture can be seen as an ensemble of an infinite number of neural predictors, drawing samples from the posterior distribution of the translational parameters, refining its belief over the initial prior. As a result, event temporal relations are determined by the stochastic translational parameters drawn from posterior distributions. Additionally, such posteriors are conditioned upon the prior learned on external knowledge graphs, providing the commonsense knowledge required to interpret more accurately the temporal information across different contexts. As shown in the results obtained from the experimental evaluation on three commonly used datasets for event temporal relation extraction, the combination of translational models and Bayesian learning is particularly beneficial when tailored to the detection of event relations. Moreover, a favorable by-product of our Bayesian-Trans model is the inherent capability to express degrees of uncertainty, avoiding the overconfident predictions on out-of-distribution context. Our contributions are summarized in the following:</p><p>• We formulate a novel Bayesian translational model for the extraction of event temporal relations, in which event temporal relations are modeled through the stochastic translational parameters, considered as latent variables in Bayesian inference.</p><p>• We devise and explore 3 different priors under</p><p>Bayesian framework to study how to effectively incorporate knowledge about events.</p><p>• We conduct thorough experimental evaluations on three benchmarking event temporal datasets and show that Bayesian-Trans achieves state-of-the-art performance on all of them. We also provide comprehensive analyses of multiple aspects of the proposed model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>This work is related to at least three lines of research: event temporal relation detection, prior knowledge incorporation, and graph embedding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Event Temporal Relation</head><p>Similar to entity-level relation extraction <ref type="bibr" target="#b47">(Zeng et al., 2014;</ref><ref type="bibr" target="#b34">Peng et al., 2017)</ref>, the latest event temporal relation extraction models are based on neural networks, but in order to learn from limited labeled data and capture complex event hierarchies, a wide range of optimization or regularization approaches have been explored. <ref type="bibr" target="#b29">Ning et al. (2019)</ref> proposed an LSTM-based network and ensured global consistency of all the event relations in the documents by integer linear programming. <ref type="bibr">Wang et al. (2020b)</ref> employed RoBERTa <ref type="bibr" target="#b23">(Liu et al., 2019)</ref> and converted a set of predefined logic rules into differentiable objective functions to regularize the consistency of the relations inferred and explore multi-task joint training. <ref type="bibr" target="#b39">Tan et al. (2021)</ref> proposed using hyperbolic-based methods to encode temporal information in a hyperbolic space, which has been shown to capture and model asymmetric temporal relations better than their Euclidean counterparts. <ref type="bibr" target="#b16">Hwang et al. (2022)</ref> adopted instead a probabilistic box embeddings to extract asymmetric relations. <ref type="bibr" target="#b46">Wen and Ji (2021)</ref> proposed to add an auxiliary task for relative time prediction of events described over an event timeline. <ref type="bibr" target="#b5">Cao et al. (2021)</ref> developed a semi-supervised approach via an uncertainty-aware self-training framework, composing a training set of samples with actual and pseudo labels depending on the estimated uncertainty scores. None of the aforementioned approaches explored Bayesian learning for incorporating prior event temporal knowledge.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Incorporation of Prior Knowledge</head><p>Knowledge plays a key role in understanding event relations because people often skip inessential details and express event relations implicitly which is difficult to understand without relevant knowledge. For example, TEMPROB <ref type="bibr">(Ning et al., 2018b)</ref> contains temporal relation probabilistic knowledge which is encoded by Siamese network and incorporated into neural models as additional features <ref type="bibr" target="#b29">(Ning et al., 2019;</ref><ref type="bibr">Wang et al., 2020b;</ref><ref type="bibr" target="#b39">Tan et al., 2021)</ref>. Unlike previous works, we combine the Bayesian Neural Network with distancebased models, treating the translational parameters as latent variables to be inferred. To this end, we adopt the variational inference (Kingma and Welling, 2014a; <ref type="bibr" target="#b1">Blei et al., 2016;</ref><ref type="bibr" target="#b13">Gui et al., 2019;</ref><ref type="bibr">Pergola et al., 2021a;</ref><ref type="bibr" target="#b48">Zhu et al., 2022)</ref>, and derive the prior distribution of the temporal relation information from commonsense knowledge bases <ref type="bibr">(Pergola et al., 2021b;</ref><ref type="bibr" target="#b24">Lu et al., 2022)</ref>. <ref type="bibr" target="#b10">Christopoulou et al. (2021)</ref> explored a similar intuition of using knowledge base priors as distant supervision signals, but the approach and the task are different.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Graph Embedding Learning</head><p>Multi-relational data are commonly interpreted in terms of directed graphs with nodes and edges representing entities and their relations, respectively. Several works have recently focused on modelling these multi-relational data with relational embeddings by detecting and encoding local and global connectivity patterns between entities. TransE <ref type="bibr" target="#b2">(Bordes et al., 2013)</ref> has been a seminal work adopting geometric translations of entities to represent relations in the embedding space. If a relation between a head and a tail entity holds, it is encoded via the translational parameters learned at training time. However, TransE cannot model symmetry relation well by simple addition which led to several subsequent studies exploring diverse types of transformation resulting in a family of translational models <ref type="bibr" target="#b45">(Wang et al., 2014;</ref><ref type="bibr" target="#b18">Ji et al., 2015;</ref><ref type="bibr" target="#b22">Lin et al., 2015)</ref>. Among them, <ref type="bibr" target="#b0">Balazevic et al. (2019)</ref> proposed to utilize the Poincaré model, mapping the entity embeddings onto a Poincaré ball, and using the Poincaré metric to compute the score function and predict their relations. <ref type="bibr">Chami et al. (2020b)</ref> further expanded the idea of embedding learning over manifolds by additionally considering reflections and rotations and redefining the translation over a learned manifold.</p><p>Although translational models are shown efficient in modeling graph relation, they provide relatively limited interaction between nodes than neural network-based methods, such as Graph Neu-ral Networks <ref type="bibr" target="#b11">(Estrach et al., 2014;</ref><ref type="bibr">Chami et al., 2020a)</ref>. Under this framework, nodes in a graph are neural units, which can iteratively propagate information through edges, and whose representations are learnt during the training process. In particular, Relational Graph Convolutional Networks (RGCN) <ref type="bibr" target="#b37">(Schlichtkrull et al., 2018)</ref> encode relational data through link prediction and entity classification tasks, while enforcing sparsity via a parameter-sharing technique. Although modeling knowledge graphs has been one of the main focuses of the above-mentioned graph learning approaches, they lack any systematic mechanism to inject prior knowledge and update it during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Bayesian-Trans Model</head><p>In identifying temporal relations between events, we aim at predicting the relation type of two events given in text, commonly denoted as head event x h and tail event x t :</p><formula xml:id="formula_0">ŷ = arg max y∈R p(y|x h , x t )<label>(1)</label></formula><p>where R denotes a set of possible relation types, while x h and x t the head and tail event triggers, respectively. Assuming that a set of latent variables Λ denotes the collection of all relation-specific transformation parameters Λ r . For example, in the knowledge embedding learning model such as MuRE <ref type="bibr" target="#b0">(Balazevic et al., 2019)</ref>, the head entity is first transformed through a relation-specific matrix W r , followed by a relation-specific translation vector t r , then Λ r = {W r , t r }. By Bayesian learning, the probability of inferring a relation type r can be written as:</p><formula xml:id="formula_1">p(y = r|x h , x t ) = Λ p(y r |x h , x t , Λ)p(Λ|G)d Λ (2)</formula><p>Here, p(Λ|G) denotes the prior distribution of Λ derived from an existing knowledge graph encoded as G. Directly inferring Eq. ( <ref type="formula">2</ref>) is intractable. But we can resort to amortised variational inference to learn model parameters. In what follows, we present our proposed Bayesian learning framework built on translational models for event temporal relation extraction, called Bayesian-Trans, with its architecture shown in Figure <ref type="figure" target="#fig_1">2</ref>.</p><p>In particular, the context S in which the two events occur is the input to our Bayesian-Trans. First, we encode S via a pre-trained language model generating the contextual embeddings e h and e t for the triggers of the head and tail events, respectively. The contextualised event trigger representations, e h and e t , are fed as input into a Bayesian translational module. This module, by means of variational inference, determines the parameters of the translational model, encoding the posterior distribution of the temporal relations conditioned upon the input events. Finally, we use a score function on the translated head and tail triggers to predict their temporal relation. We provide a more detailed description in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Contextual Encoder</head><p>The proposed model uses COMET-BART <ref type="bibr" target="#b17">(Hwang et al., 2021)</ref> as the context encoder. COMET-BART is a BART pre-trained language model <ref type="bibr" target="#b21">(Lewis et al., 2020)</ref> fine-tunned on ATOMIC <ref type="bibr" target="#b3">(Bosselut et al., 2019;</ref><ref type="bibr" target="#b17">Hwang et al., 2021)</ref>, which is an event-centric knowledge graph encoding inferential knowledge about entities and events, including event temporal relations. The COMET-BART is able to generate consequence events given the antecedent event and a relation with good accuracy thus is regarded encodes knowledge well. Following the approach adopted in previous works <ref type="bibr" target="#b29">(Ning et al., 2019;</ref><ref type="bibr">Wang et al., 2020b;</ref><ref type="bibr" target="#b39">Tan et al., 2021)</ref>, we use the representation of the first token of an event trigger as the contextual embedding of that event<ref type="foot" target="#foot_0">foot_0</ref> , e h , e t = COMET-BART(x h , x t ), where e h , e t ∈ R d . The event representations are then concatenated together and fed through MLPs to generate the parameters of the variational distribution, from which the latent event-pair representation z is sampled. z is then mapped to the parameter space of the translational model as Λ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Incorporating Knowledge via Bayesian Learning</head><p>The proposed model utilizes relation embeddings for classifying event relation in a similar manner as the translational models in knowledge graph embedding, such as TransE <ref type="bibr" target="#b2">(Bordes et al., 2013)</ref>. If the embedding of the tail event is close enough to the embedding of head event after applying a series of relation-specific transformation, the relation stands, and vice versa. A wide range of translational models typically proposed for learning knowledge graph embeddings can be adopted in the proposed Bayesian-Trans. Additionally, to incorporate prior knowledge, we extend translational models to operate within the Bayesian inference framework. We proceed with introducing a standard translational model in the context of temporal relations, and describe how we extend it to work in the Bayesian framework.</p><p>Translational Model Generally speaking, a translational model uses relation representations Λ r to perform "translation" for relation r on the head and tail events. Then, the transformed head and tail event embeddings are compared using a distance-based score function, whose score is indicative of the temporal relation between the events. The score function ϕ(•) takes the general form:</p><formula xml:id="formula_2">ϕ(e h , r, e t ) = -d(T h Λr (e h ), T t Λr (e t ))<label>(3)</label></formula><p>where r is a relation type, T Λr (•) is a function depending on the parameters Λ r of relation r to transform the event embeddings e h and e t , and d(•) is any distance metrics (e.g., Euclidean distance).</p><p>We explored several models with different translation functions and distance metrics in the context of temporal relations, including TransE <ref type="bibr" target="#b2">(Bordes et al., 2013)</ref>, AttH <ref type="bibr">(Chami et al., 2020b)</ref>, MuRE <ref type="bibr" target="#b0">(Balazevic et al., 2019)</ref> and MuRP <ref type="bibr" target="#b0">(Balazevic et al., 2019)</ref>, and based on our preliminary results 3 , we eventually adopted MuRE as it strikes a good balance of training efficiency and accuracy of temporal relation classification. We define the scoring function in the proposed model as follows:</p><formula xml:id="formula_3">ϕ(e h , r, e t ) = -∥W r e h + t r -e t ∥ 2 2 (4)</formula><p>where W r ∈ R d×d is a diagonal relation matrix and t r ∈ R d a translation vector of relation r,</p><formula xml:id="formula_4">Λ r = {W r , t r }, r ∈ R.</formula><p>Although the number of parameters to train is rather low, the number of annotated samples is usually small compared to the wide range of linguistic expressions capturing temporal relations. We thus extend the MuRE model into a Bayesian framework to enhance its scalability by treating the translational parameters Λ as latent variables. The proposed framework enhances generalization by defining a variational inference process that optimizes the regularization and leverages the additional information injected via the prior distributions.</p><p>Bayesian Inference As shown in the inference equation 2, the prior is derived from an external knowledge graph, such as ATOMIC, as a means to inject prior information about events and temporal relations. In particular, Λ is assumed to follow a Gaussian distribution with unit variance and with mean determined by the relation representations trained on the knowledge graph. The probability function is formulated as a softmax function over a pre-defined scoring function:</p><formula xml:id="formula_5">p(y r |e h , e t , Λ) = exp ϕ(e h , r, e t ) r ′ ∈R exp ϕ(e h , r ′ , e t )<label>(5)</label></formula><p>with e h and e t denoting the embedding for the head and the tail events, respectively. Yet, Eq. ( <ref type="formula">2</ref>) is intractable and cannot be inferred directly. Thus, we resort to amortized variational inference by introducing a variational posterior q θ (Λ|x h , x t ), which follows the isotropic Gaussian distribution and can be modeled as: 3 Experimental results using different translational models are shown in Table <ref type="table" target="#tab_0">A1</ref>. µ = f µ (e h ; e t ) Σ = diag f Σ (e h ; e t ) q θ (Λ|e h , e t ) = N (Λ|µ, Σ), <ref type="bibr">(6)</ref> where f µ and f Σ are both fully connected layers that map the event pair representation into the parameters of the variational distribution.</p><p>Following the amortized variational inference, we maximize the evidence lower bound (ELBO) L e , defined in Eq. ( <ref type="formula">7</ref>), and approximated by a Monte Carlo estimation with sample size N , as described in Eq. ( <ref type="formula" target="#formula_6">8</ref>):</p><formula xml:id="formula_6">L e = E q θ (Λ|x h ,xt),{x h ,xt}∈D log p θ (y|x h , x t , Λ) - Reg q θ (Λ|x h , x t , G)||p(Λ|G) (7) ≈ 1 N N n=1 {x h ,xt}∈D log p θ (y|x h , x t , Λ (n) )- Reg q θ (Λ (n) |x h , x t , G)||p(Λ (n) |G)<label>(8)</label></formula><p>where Reg(•) is a regularization term which will be discussed in 3.3. To train end-to-end a fully differentiable model, we adopt the reparameterization trick (Kingma and Welling, 2014b).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Prior Distribution and Regularization</head><p>We proceed to discuss how the Bayesian framework enabled the incorporation of prior acquired from an external knowledge source. Then, we provide the details of how we compute the regularization term to induce a more stable training.</p><p>Prior Distribution One of the main advantages of the Bayesian inference framework is the possibility to inject commonsense knowledge into the model through the prior distribution of the latent variables, i.e., p(Λ|G) in Eq. ( <ref type="formula">2</ref>), where Λ are the translational parameters and G denotes an external knowledge graph, in our case, the ATOMIC knowledge graph <ref type="bibr" target="#b17">(Hwang et al., 2021)</ref>. ATOMIC is a commonsense knowledge graph containing inferential knowledge tuples about entities and events encoding social and physical aspects of human everyday experiences. For our task of event temporal relation extraction, we are only interested in the events linked via temporal relations, such as 'ISBEFORE' (23,208 triples) or 'ISAFTER' <ref type="bibr">(22,453 triples)</ref>. By conducting link prediction on these links, we use relation embeddings learnt using an RGCN <ref type="bibr" target="#b37">(Schlichtkrull et al., 2018)</ref> as the mean of the prior distribution for the translational latent variables. For the relations in the experiment dataset that do not have applicable counterparts in ATOMIC (e.g., VAGUE), we set their priors to standard Gaussian. The variance of the priors is defined as the identity matrix. Specifically, we use COMET-BART to encode the event nodes from ATOMIC, then use their context embeddings as the node features in the RGCN. In our preliminary experiment, we also found that RGCN cannot train well on the commonsense graph with only the event-event relation links. The graph is too sparse which makes the information difficult to propagate through the nodes. Thus, we added semantic similarity links based on the cosine similarity of the event context embeddings. During the training of the RGCN, the node embeddings are kept frozen. After the training of the link prediction task, we extract the relation embeddings of the RGCN.</p><p>Regularization Term To mitigate the posterior collapse problem <ref type="bibr" target="#b25">(Lucas et al., 2019)</ref> and have a stable inference process, we adopt the Maximum Mean Discrepancy (MMD) <ref type="foot" target="#foot_1">4</ref> which is an estimation of Wasserstein distance <ref type="bibr" target="#b40">(Tolstikhin et al., 2018)</ref> as the regularization term (Eq. 8).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Setup</head><p>Datasets We evaluated the proposed Bayesian-Trans model on three event temporal relation datasets: MATRES <ref type="bibr">(Ning et al., 2018c)</ref>, Temporal and Causal Reasoning (TCR) <ref type="bibr">(Ning et al., 2018a)</ref>, and TimeBank-Dense (TBD) <ref type="bibr" target="#b6">(Cassidy et al., 2014)</ref>. TimeBank-Dense is a densely annotated dataset focusing on the most salient events and providing 6 event temporal relations. MATRES follows a new annotation scheme which focuses on main time axes, with the temporal relations between events determined by their endpoints, resulting in a consistent inter-annotator agreement (IAA) on the event annotations <ref type="bibr">(Ning et al., 2018c)</ref>. TCR follows the same annotation scheme, yet with a much smaller number of event relation pairs than in MATRES. Table <ref type="table" target="#tab_0">1</ref> shows the statistics of the datasets.</p><p>Baselines We compare the proposed Bayesian-Trans<ref type="foot" target="#foot_2">foot_2</ref> with the following baselines:</p><p>CogCompTime <ref type="bibr">(Ning et al., 2018d</ref>) is a multi-step system which detect temporal relation using semantic features and structured inference.  <ref type="bibr" target="#b39">(Tan et al., 2021)</ref> learns event embeddings based on a Poincaré ball and determines the temporal relation base on the relative position of events. HGRU + knowledge <ref type="bibr" target="#b39">(Tan et al., 2021</ref>) is a neural architecture processing temporal relations via hyperbolic recurrent units which also incorporates knowledge features like LSTM + knowledge.</p><p>Relative Event Time <ref type="bibr" target="#b46">(Wen and Ji, 2021</ref>) is a neural network classifier combining an auxiliary task for relative time extraction over an event timeline. UAST <ref type="bibr" target="#b5">(Cao et al., 2021)</ref> is an uncertainty-aware self-training model. We show the result of the model which is trained on all the labeled data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Results</head><p>Temporal Relation Classification We first compare Bayesian-Trans with the most recent approaches for temporal event classification in Table 2, including methods with or without commonsense knowledge injection. The results are obtained by training models on the MATRES training set and evaluated on both the MATRES test set and TCR. Table <ref type="table">3</ref> shows results from the TBD dataset which are generated using the provided train, development, and test sets. We report F 1 score on MA-TRES and TCR following the definition in <ref type="bibr" target="#b29">(Ning et al., 2019)</ref>, and micro-F 1 on TimeBank-Dense.</p><p>Compared with existing methods, the proposed  <ref type="bibr">(2020b)</ref> and <ref type="bibr" target="#b46">(Wen and Ji, 2021)</ref> on TCR are generated from our run of the source code provided by the authors since they are not available in their original papers. The others are taken from the cited papers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>Micro-F 1 BiLSTM <ref type="bibr">(Han et al., 2019b)</ref> 61.9 Deep Structured <ref type="bibr">(Han et al., 2019a)</ref> 63.2 Relative Event Time <ref type="bibr" target="#b46">(Wen and Ji, 2021)</ref> 63.2 UAST <ref type="bibr" target="#b5">(Cao et al., 2021)</ref> 64.3</p><p>Bayesian-Trans 65.0</p><p>Table <ref type="table">3</ref>: Experimental results on TBD. All compared methods do not incorporate commonsense knowledge explicitly. The result of <ref type="bibr" target="#b46">Wen and Ji (2021)</ref> is generated from our run of the source code provided by the authors since they are not available in their original paper. The others are taken from the cited papers.</p><p>Bayesian-Trans has generally better performance on all three datasets, with more noticeably improvements on MATRES. Bayesian-Trans has significant performance gains over previous methods with knowledge incorporation, which shows that it can utilize knowledge more extensively. Details of the per-class performance can be found in Table <ref type="table" target="#tab_1">A2</ref> and A3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Study</head><p>We conducted an ablation study to highlight the impact of the different modules composing Bayesian-Trans. The results are shown in Table <ref type="table" target="#tab_2">4</ref>. In particular, we have the following variants: (1) RoBERTa+MLP, using RoBERTa to encode the context and then feeding representations of head and tail events to a multi-layer percep- (2), and (4) cf. ( <ref type="formula" target="#formula_5">5</ref>)). Regardless of the contextual encoder used, the results of ( <ref type="formula" target="#formula_2">3</ref>) and ( <ref type="formula">6</ref>) show the benefit of employing Bayesian learning which naturally incorporates prior knowledge of event temporal relations learned from an external knowledge source for event temporal relation detection.</p><p>With our proposed Bayesian translational model, we observe an improvement of 0.9-1.8% in micro-F 1 on MATRES and 0.2 -2.5% in micro-F 1 on TimeBank-Dense compare to their non-Bayesian counterparts.</p><p>Effects of the Priors We further investigate the impact of different priors on the model performance. Inspired by the work on VAEs by <ref type="bibr" target="#b4">Burda et al. (2016)</ref> and <ref type="bibr" target="#b41">Truong et al. (2021)</ref>, we employed   an 'activity' score, τ = Cov e h ,et (E q(Λ|e h ,et) <ref type="bibr">[Λ]</ref>) to evaluate the quality and diversity of the latent encodings. The intuition behind the "activity" score is that if a latent dimension encodes relevant information and is not redundant, its value is expected to vary significantly over different inputs. By computing the score across all the test instances, every dimension of Λ is given an 'activity' value.</p><p>Latent units with a higher value are considered more active and thus more informative. Figure <ref type="figure" target="#fig_2">3</ref> shows activity scores with respect to different prior distributions, including the standard Gaussian prior and priors learned on ATOMIC using MuRE or RGCN, in which the latent variables are the least active when using standard Gaussian as the prior distribution. The higher activation is obtained using the priors learnt on the external knowledge base. In particular, the prior based on RGCN and MuRE over ATOMIC displays the most active units, with RGCN showing the most active units on average. Table <ref type="table" target="#tab_4">5</ref> shows the performance of the proposed model based on different priors. Twosided Welch's t-test (p &lt; 0.05) also supports that the RGCN-learned prior improves over standard Gaussian prior.</p><p>Uncertainty Quantification We present an analysis of uncertainty quantification of the Bayesian-Trans predictions. We adopted the uncertainty quantification methods as in Malinin and Gales (2018), computing the entropy (total uncertainty) and mutual information (model uncertainty) to visualize the predictive probabilities on a 2-simplex. Each forward pass on the same test instance is represented as a point on the simplex. For the sake of clarity of the visualization, we removed the EQUAL class, which is hardly ever predicted by the models.</p><p>In one of the test cases (Figure <ref type="figure" target="#fig_3">4</ref>(a)), the true label is "die" BEFORE "vaccinate". This example exhibits a rather complex linguistic structure, as such, the model exhibits some uncertainty. Most of the predictions located at the corner are associated with BEFORE, but there also are several predictions scattered around it. We then simplified the sentence structure by removing "but four", and fed the modified sentence to the same model. This time, the model predicted the right temporal relation with much lower uncertainty (Figure <ref type="figure" target="#fig_3">4(b)</ref>).</p><p>In another case study (Figure <ref type="figure" target="#fig_3">4</ref>(c)), the true label is "depart" AFTER "reveal". This test case is rather straightforward, because of the explicit temporal word "before". The model predicted AFTER with high confidence, as shown by the predictive probabilities cluster at the top of the simplex. To show the impact of the temporal description, we swapped it from "before" to "after" and fed it to the same model. The model recognized the reversed meaning and correctly predicted BEFORE with low uncertainty <ref type="bibr">(Figure 4(d))</ref>. The above cases demon-strate that the proposed model reacts to different inputs with reasonable uncertainty, on both the total and model uncertainty scores.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We propose a principled approach to incorporate knowledge for event temporal relation extraction named Bayesian-Trans, which models the relation representations in the MuRE translational model, as latent variables. The latent variables are inferred through variational inference, during which commonsense knowledge is incorporated in the form of the prior distribution. The experiments on MA-TRES, TCR, and TBD show that Bayesian-Trans achieves state-of-the-art performance. Comprehensive analyses of the experimental results also demonstrate the characteristics and benefits of the proposed model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Limitations</head><p>Our approach takes an event pair as input for the prediction of their temporal relation. We observe that if two events reside in different sentences, the error rate increases by 19%. A promising future direction is to construct a global event graph where temporal relations of any two events are refined with the consideration of global consistency constraints, for example, no temporal relation loop is allowed in a set of events. Our current work only deals with even temporal relations, it could be extended to consider other event semantic relations such as causal, hierarchical or entailment relations. The event temporal knowledge in this paper is derived from ATOMIC which can possibly be extended to more sources. Bayesian learning could also be extended to life-long learning. But we need to explore approaches to address the problem of catastrophic forgetting. We didn't exhaustively investigate all the translational models due to the large volume of work in that area. There might be a translational model which can achieve better performance, but the core idea of the proposed framework stays the same.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ethical Considerations</head><p>The goal of the proposed method is to understand the temporal relation between events based on the descriptions in the given text. What the method can achieve in the most optimistic scenario is no more than giving the same text to a human reader and letting him or her explain the event relations.</p><p>Therefore, the ethical concerns only come from the data collection. In this paper, we only use publicly available datasets which have already been widely used in the research field. As for potential application, as long as the user collects the training data legally, the proposed method does not have the potential to have a direct harmful impact.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Comparison between with or without external knowledge incorporation on event relation extraction.</figDesc><graphic coords="1,306.14,212.59,218.28,153.51" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The network structure of Bayesian-Trans. Context sentences are first fed into a COMET encoder to generate event representations. With MLP layers, the event representations are mapped to generate a variational distribution of relation representations which is guided by KG priors. The relation representations are then used in the translational model to generate prediction scores.</figDesc><graphic coords="4,82.20,70.84,430.89,130.40" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The box chart of the activity scores across all the dimensions of the latent encoding Λ with respect to the priors used in the model.</figDesc><graphic coords="8,70.86,70.85,218.28,132.96" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Examples of temporal relations in text and uncertainty quantification (entropy and mutual information) for the Bayesian-Trans model. Examples (a),(b) show how simplifying the linguistic structure without altering the temporal relation increases the model confidence. While examples (c),(d) illustrate the model's detection of temporal linguistic hints and its confidence.</figDesc><graphic coords="8,306.14,70.84,218.29,201.63" type="bitmap" /></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0"><head>Table 1 :</head><label>1</label><figDesc>The statistics of MATRES, TCR, and TBD.</figDesc><table><row><cell>Class</cell><cell>MATRES</cell><cell>TCR</cell><cell>TBD</cell></row><row><cell>BEFORE AFTER EQUAL/SIMULTANEOUS VAGUE/NONE INCLUDE ISINCLUDED</cell><cell cols="2">6, 852 1, 780 4, 752 862 448 4 1, 425 N/A N/A N/A N/A N/A</cell><cell>2, 590 2, 104 215 5, 910 836 1, 060</cell></row><row><cell>Total</cell><cell cols="3">12, 740 2, 646 12, 715</cell></row><row><cell cols="4">BiLSTM is a basic relation prediction model built</cell></row><row><cell>by Han et al. (2019b).</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="4">LSTM + knowledge (Ning et al., 2019) incorpo-</cell></row><row><cell cols="4">rates knowledge features learnt from an external</cell></row><row><cell cols="4">source and optimize global consistency by ILP.</cell></row><row><cell cols="4">Deep Structured (Han et al., 2019a) adds a struc-</cell></row><row><cell cols="4">tured support vector machine on top of a BiLSTM.</cell></row><row><cell cols="4">Joint Constrained Learning (Wang et al., 2020b)</cell></row><row><cell cols="4">constrains the training of a RoBERTa-based event</cell></row><row><cell cols="4">pair classifier using predefined logic rules, while</cell></row><row><cell cols="4">knowledge incorporation and global optimization</cell></row><row><cell>are also included.</cell><cell></cell><cell></cell><cell></cell></row><row><cell cols="2">Poincaré Event Embedding</cell><cell></cell><cell></cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1"><head>Table 2 :</head><label>2</label><figDesc>Experimental results on MATRES and TCR. The first three lines contain methods without commonsense knowledge incorporation. The rest are methods which inject commonsense knowledge. The results ofWang et al.   </figDesc><table><row><cell>MATRES</cell><cell>TCR</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2"><head>Table 4 :</head><label>4</label><figDesc>Ablation test results on MATRES and TBD.</figDesc><table><row><cell>tron (MLP) for temporal relation classification; (2)</cell></row><row><cell>RoBERTa+ Vanilla MuRE, using MuRE to extract</cell></row><row><cell>temporal relations without modeling its parameters</cell></row><row><cell>as latent variables; (3) RoBERTa+Bayesian-Trans,</cell></row><row><cell>our proposed model by replacing COMERT-BART</cell></row><row><cell>with RoBERTa as the text encoder; (4) COMET-</cell></row></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4"><head>Table 5 :</head><label>5</label><figDesc>F 1 values based on different priors used in the proposed model.</figDesc><table /></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2" xml:id="foot_0"><p>We conducted some exploratory experiments adopting the last token or the average representation, but results showed that the first token was still the best option in this context.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4" xml:id="foot_1"><p>MMD calculation can be found in Appendix A.</p></note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5" xml:id="foot_2"><p>Hyperparameter setting can be found in Appendix B.</p></note>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgements</head><p>This work was supported in part by the <rs type="funder">UK Engineering and Physical Sciences Research Council</rs> (grant no. <rs type="grantNumber">EP/T017112/1</rs>, <rs type="grantNumber">EP/V048597/1</rs>, <rs type="grantNumber">EP/X019063/1</rs>). YH is supported by a <rs type="grantName">Turing AI Fellowship</rs> funded by the <rs type="funder">UK Research and Innovation</rs> (grant no. <rs type="grantNumber">EP/V020579/1</rs>). This work was conducted on the <rs type="institution" subtype="infrastructure">UKRI/EPSRC HPC platform, Avon</rs>, hosted in the <rs type="institution">University of Warwick's Scientific Computing Group</rs>. XT was partially supported by the <rs type="funder">Research Development Fund (RDF)</rs> <rs type="grantNumber">2022/23</rs> (<rs type="affiliation">University of Warwick</rs>): '<rs type="projectName">An Event-Centric Dialogue System for Second Language Learners</rs>'.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_aBcrzyc">
					<idno type="grant-number">EP/T017112/1</idno>
				</org>
				<org type="funding" xml:id="_GGKRSNc">
					<idno type="grant-number">EP/V048597/1</idno>
				</org>
				<org type="funding" xml:id="_dnhNju6">
					<idno type="grant-number">EP/X019063/1</idno>
					<orgName type="grant-name">Turing AI Fellowship</orgName>
				</org>
				<org type="funding" xml:id="_9gvuR3d">
					<idno type="grant-number">EP/V020579/1</idno>
				</org>
				<org type="funded-project" xml:id="_qzYsAaa">
					<idno type="grant-number">2022/23</idno>
					<orgName type="project" subtype="full">An Event-Centric Dialogue System for Second Language Learners</orgName>
				</org>
			</listOrg>

			<listOrg type="infrastructure">
				<org type="infrastructure">					<orgName type="extracted">UKRI/EPSRC HPC platform, Avon</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Maximum Mean Discrepancy (MMD)</head><p>The Maximum Mean Discrepancy (MMD) can be unbiasedly estimated using the following equation <ref type="bibr" target="#b27">(Nan et al., 2019)</ref>:</p><p>where z 1 , ..., z n are sampled from variational distribution q ϕ and z1 , ..., zm are sampled from prior distribution p,</p><p>which is often chosen for high-dimensional Gaussians.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model Precision</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Hyperparameter Settings and Resource Consumption</head><p>We conducted a grid-search to determine the optimal hyperparameters and dimensionality of the relation embeddings. The searching range for the dimension of the latent vector the transformation parameters is <ref type="bibr">[50,</ref><ref type="bibr">200]</ref>, with a step size of 50. As a result, on the MATRES, the dimension of the latent vector z is 200, the dimension of relation transformation vectors t r or matrices W r is 50, the dropout rate is 0.1. On the TBD, the dimension of the latent vector z is 100, and the dimension of relation transformation vectors t r or matrices W r is 100, dropout rate is 0. Based on the above settings, the number of parameters of the Bayesian-Trans is 443 thousand (excluding the COMET-BART). The COMET-BART encoder has 204 million parameters. The learning rate α c for the context encoder is set to 1e -5, while for other components of the architecture α = 1e -3. To calibrate the influence of the regularization term, we set a scaling weight smoothly increasing from 1e -2 to 2.0 during training. We ran the training for 60 epochs which is enough for the model to converge, and evaluated on the validation set after each training epoch.</p><p>All the experiments were conducted on an Nvidia GeForce RTX 3090 GPU. On the TBD dataset, the average training time is 93 seconds per epoch, while the inference time is 4 seconds. On the MATRES dataset, the average training time is 74 seconds per epoch, and the inference time is 7 seconds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Comparison of Translational Models</head><p>Table <ref type="table">A1</ref> shows the performance on MATRES using different translational models in the Bayesian framework. TransE <ref type="bibr" target="#b2">(Bordes et al., 2013)</ref> is one of the most commonly used translational models, which only performs the addition transformation on the head event. AttH <ref type="bibr">(Chami et al., 2020b)</ref> expands the idea of hyperbolic translational models by making the curvature learnable. It also introduces more types of transformation, reflection and rotation. MuRE <ref type="bibr" target="#b0">(Balazevic et al., 2019)</ref> strikes a balance by conducting diagonal matrix transformation and addition transformation. MuRP <ref type="bibr" target="#b0">(Balazevic et al., 2019)</ref> is the Poincarè version of MuRE, which projects the head and tail onto a Poincarè ball before performing scaling and addition. The score function of MuRP computes the Poincarè distance instead of the Euclidean distance.</p><p>We can observe that TransE performs well, beating the previous state-of-the-art (F 1 = 81.7) but gives slightly worse results compared to MuRE. Both translational models in the hyperbolic space, MuRP and AttH, are inferior to the Euclideanbased translational models. As MuRE gives more balanced precision and recall values, it is therefore adopted in our Bayesian learning framework. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relation Precision</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Class-Specific Results</head><p>In Table <ref type="table">A2</ref> and A3, We show the results obtained using Bayesian-Trans under each temporal relation </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Multi-relational poincaré graph embeddings</title>
		<author>
			<persName><forename type="first">Ivana</forename><surname>Balazevic</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Carl</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>In NeurIPS</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Variational inference: A review for statisticians</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alp</forename><surname>Kucukelbir</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jon</forename><forename type="middle">D</forename><surname>Mcauliffe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">112</biblScope>
			<biblScope unit="page" from="859" to="877" />
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Translating embeddings for modeling multirelational data</title>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alberto</forename><surname>García-Durán</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Oksana</forename><surname>Yakhnenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">COMET: Commonsense transformers for automatic knowledge graph construction</title>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hannah</forename><surname>Rashkin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maarten</forename><surname>Sap</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chaitanya</forename><surname>Malaviya</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Asli</forename><surname>Celikyilmaz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1470</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="4762" to="4779" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Importance weighted autoencoders</title>
		<author>
			<persName><forename type="first">Yuri</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><forename type="middle">B</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016">2016</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Uncertainty-aware selftraining for semi-supervised event temporal relation extraction</title>
		<author>
			<persName><forename type="first">Pengfei</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xinyu</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yubo</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wei</forename><surname>Bi</surname></persName>
		</author>
		<idno>agement. 1133</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 30th ACM International Conference on Information &amp; Knowledge Man</title>
		<meeting>the 30th ACM International Conference on Information &amp; Knowledge Man</meeting>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An annotation framework for dense event ordering</title>
		<author>
			<persName><forename type="first">Taylor</forename><surname>Cassidy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bill</forename><surname>Mcdowell</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nathanael</forename><surname>Chambers</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Steven</forename><surname>Bethard</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/P14-2082</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 52nd Annual Meeting of the Association for Computational Linguistics<address><addrLine>Baltimore, Maryland</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="501" to="506" />
		</imprint>
	</monogr>
	<note>Short Papers)</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<author>
			<persName><forename type="first">Ines</forename><surname>Chami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sami</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bryan</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2005.03675</idno>
		<title level="m">Machine learning on graphs: A model and comprehensive taxonomy</title>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Lowdimensional hyperbolic knowledge graph embeddings</title>
		<author>
			<persName><forename type="first">Ines</forename><surname>Chami</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adva</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Da-Cheng</forename><surname>Juan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Frederic</forename><surname>Sala</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sujith</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Ré</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.617</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="6901" to="6914" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Temporal relation discovery between events and temporal expressions identified in clinical narrative</title>
		<author>
			<persName><forename type="first">Yao</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Anick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pengyu</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nianwen</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of biomedical informatics</title>
		<imprint>
			<biblScope unit="page">46</biblScope>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Distantly supervised relation extraction with sentence reconstruction and knowledge base priors</title>
		<author>
			<persName><forename type="first">Fenia</forename><surname>Christopoulou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Makoto</forename><surname>Miwa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sophia</forename><surname>Ananiadou</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.naacl-main.2</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="11" to="26" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Spectral networks and deep locally connected networks on graphs</title>
		<author>
			<persName><forename type="first">Joan</forename><surname>Bruna Estrach</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2nd international conference on learning representations</title>
		<imprint>
			<date type="published" when="2014">2014. 2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Did aristotle use a laptop? a question answering benchmark with implicit reasoning strategies</title>
		<author>
			<persName><forename type="first">Mor</forename><surname>Geva</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Khashabi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elad</forename><surname>Segal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tushar</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00370</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="346" to="361" />
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Neural topic model with reinforcement learning</title>
		<author>
			<persName><forename type="first">Lin</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jia</forename><surname>Leng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Pergola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ruifeng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulan</forename><surname>He</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1350</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="3478" to="3483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">2019a. Deep structured neural network for event temporal relation extraction</title>
		<author>
			<persName><forename type="first">Rujun</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">I-Hung</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mu</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aram</forename><surname>Galstyan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ralph</forename><surname>Weischedel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/K19-1062</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL)</title>
		<meeting>the 23rd Conference on Computational Natural Language Learning (CoNLL)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="666" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Joint event and temporal relation extraction with shared representations and structured prediction</title>
		<author>
			<persName><forename type="first">Rujun</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1041</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="434" to="444" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Event-event relation extraction using probabilistic box embedding</title>
		<author>
			<persName><forename type="first">Eunjeong</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jay-Yoon</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tianyi</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dhruvesh</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dongxu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 60th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="235" to="244" />
		</imprint>
	</monogr>
	<note>Short Papers). Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Comet-atomic 2020: On symbolic and neural commonsense knowledge graphs</title>
		<author>
			<persName><forename type="first">Jena</forename><forename type="middle">D</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chandra</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Ronan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jeff</forename><surname>Bras</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Keisuke</forename><surname>Da</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antoine</forename><surname>Sakaguchi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yejin</forename><surname>Bosselut</surname></persName>
		</author>
		<author>
			<persName><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2021">2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding via dynamic mapping matrix</title>
		<author>
			<persName><forename type="first">Guoliang</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Shizhu</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liheng</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="DOI">10.3115/v1/P15-1067</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing</title>
		<meeting>the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing<address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="687" to="696" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
		<idno>CoRR, abs/1312.6114</idno>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Autoencoding variational Bayes</title>
		<author>
			<persName><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2nd International Conference on Learning Representations ICLR</title>
		<meeting>the 2nd International Conference on Learning Representations ICLR</meeting>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension</title>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marjan</forename><surname>Ghazvininejad</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Abdelrahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.acl-main.703</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 58th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="7871" to="7880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning entity and relation embeddings for knowledge graph completion</title>
		<author>
			<persName><forename type="first">Yankai</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhiyuan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maosong</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xuan</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2015">2015</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName><forename type="first">Yinhan</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Myle</forename><surname>Ott</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Naman</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jingfei</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mandar</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mike</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luke</forename><surname>Zettlemoyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Veselin</forename><surname>Stoyanov</surname></persName>
		</author>
		<idno type="DOI">10.48550/ARXIV.1907.11692</idno>
		<imprint>
			<date type="published" when="2019">2019</date>
		</imprint>
	</monogr>
	<note>Roberta: A robustly optimized bert pretraining approach</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Event-centric question answering via contrastive learning and invertible event transformation</title>
		<author>
			<persName><forename type="first">Junru</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xingwei</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Pergola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulan</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Findings of the Association for Computational Linguistics: EMNLP 2022</title>
		<meeting><address><addrLine>Abu Dhabi</addrLine></address></meeting>
		<imprint>
			<publisher>United Arab Emirates. Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="2377" to="2389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Don&apos;t blame the elbo! a linear vae perspective on posterior collapse</title>
		<author>
			<persName><forename type="first">James</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">George</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Roger</forename><forename type="middle">B</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Predictive uncertainty estimation via prior networks</title>
		<author>
			<persName><forename type="first">Andrey</forename><surname>Malinin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mark</forename><surname>Gales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Neural Information Processing Systems</title>
		<meeting>the 32nd International Conference on Neural Information Processing Systems</meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="7047" to="7058" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Topic modeling with Wasserstein autoencoders</title>
		<author>
			<persName><forename type="first">Feng</forename><surname>Nan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ran</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bing</forename><surname>Xiang</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1640</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6345" to="6381" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Joint reasoning for temporal and causal relations</title>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhili</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1212</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2278" to="2288" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">An improved neural baseline for temporal relation extraction</title>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sanjay</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D19-1642</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</title>
		<meeting>the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)<address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="6203" to="6209" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">TORQUE: A reading comprehension dataset of temporal ordering questions</title>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rujun</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matt</forename><surname>Gardner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Improving temporal relation extraction with a globally acquired statistical resource</title>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoruo</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/N18-1077</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<meeting>the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Long Papers; New Orleans, Louisiana</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="841" to="851" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">2018c. A multiaxis annotation scheme for event temporal relations</title>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1122</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1318" to="1328" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">CogCompTime: A tool for understanding time in natural language</title>
		<author>
			<persName><forename type="first">Qiang</forename><surname>Ning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ben</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhili</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Haoruo</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/D18-2013</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</title>
		<meeting>the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations<address><addrLine>Brussels, Belgium</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="72" to="77" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Cross-sentence n-ary relation extraction with graph LSTMs. Transactions of the Association for</title>
		<author>
			<persName><forename type="first">Nanyun</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Quirk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristina</forename><surname>Toutanova</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Wen-Tau</forename><surname>Yih</surname></persName>
		</author>
		<idno type="DOI">10.1162/tacl_a_00049</idno>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="101" to="115" />
			<date type="published" when="2017">2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">2021a. A disentangled adversarial neural topic model for separating opinions from plots in user reviews</title>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Pergola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulan</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference of the North American Chapter</title>
		<meeting>the 2021 Conference of the North American Chapter</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="2870" to="2883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">2021b. Boosting low-resource biomedical QA via entity-aware masking strategies</title>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Pergola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elena</forename><surname>Kochkina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lin</forename><surname>Gui</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maria</forename><surname>Liakata</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulan</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 16th Conference of the European Chapter</title>
		<meeting>the 16th Conference of the European Chapter</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<biblScope unit="page" from="1977" to="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Modeling relational data with graph convolutional networks</title>
		<author>
			<persName><forename type="first">Michael</forename><surname>Schlichtkrull</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Rianne</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ivan</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Max</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">15th Extended Semantic Web Conference (ESWC)</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="593" to="607" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">PHEE: A dataset for pharmacovigilance event extraction from text</title>
		<author>
			<persName><forename type="first">Zhaoyue</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jiazheng</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Pergola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Byron</forename><surname>Wallace</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bino</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nigel</forename><surname>Greene</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Joseph</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulan</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2022 Conference on Empirical Methods in Natural Language Processing<address><addrLine>Abu Dhabi</addrLine></address></meeting>
		<imprint>
			<publisher>United Arab Emirates. Association for Computational Linguistics</publisher>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="5571" to="5587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Extracting event temporal relations via hyperbolic geometry</title>
		<author>
			<persName><forename type="first">Xingwei</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Pergola</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yulan</forename><surname>He</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.636</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<publisher>Dominican Republic. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="8065" to="8077" />
		</imprint>
		<respStmt>
			<orgName>Online and Punta Cana</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Wasserstein autoencoders</title>
		<author>
			<persName><forename type="first">Ilya</forename><surname>Tolstikhin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Olivier</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sylvain</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Bernhard</forename><surname>Schoelkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Bilateral variational autoencoder for collaborative filtering</title>
		<author>
			<persName><forename type="first">Quoc-Tuan</forename><surname>Truong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aghiles</forename><surname>Salah</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hady</forename><forename type="middle">W</forename><surname>Lauw</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 14th ACM International Conference on Web Search and Data Mining</title>
		<meeting>the 14th ACM International Conference on Web Search and Data Mining</meeting>
		<imprint>
			<date type="published" when="1135">2021. 1135</date>
			<biblScope unit="page" from="292" to="300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Event phase oriented news summarization</title>
		<author>
			<persName><forename type="first">Chengyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaofeng</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Aoying</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">World Wide Web</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1069" to="1092" />
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Joint constrained learning for eventevent relation extraction</title>
		<author>
			<persName><forename type="first">Haoyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="696" to="706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Joint constrained learning for eventevent relation extraction</title>
		<author>
			<persName><forename type="first">Haoyu</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Muhao</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hongming</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Roth</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2020.emnlp-main.51</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<meeting>the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</meeting>
		<imprint>
			<publisher>Online. Association for Computational Linguistics</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="696" to="706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Knowledge graph embedding by translating on hyperplanes</title>
		<author>
			<persName><forename type="first">Zhen</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jianlin</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2014">2014</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Utilizing relative event time to enhance event-event temporal relation extraction</title>
		<author>
			<persName><forename type="first">Haoyang</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Heng</forename><surname>Ji</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2021.emnlp-main.815</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Process</title>
		<meeting>the 2021 Conference on Empirical Methods in Natural Language Process</meeting>
		<imprint>
			<publisher>Dominican Republic. Association for Computational Linguistics</publisher>
			<date type="published" when="2021">2021</date>
			<biblScope unit="page" from="10431" to="10437" />
		</imprint>
		<respStmt>
			<orgName>Online and Punta Cana</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Relation classification via convolutional deep neural network</title>
		<author>
			<persName><forename type="first">Daojian</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Siwei</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Guangyou</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jun</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers</title>
		<meeting>COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers<address><addrLine>Dublin, Ireland</addrLine></address></meeting>
		<imprint>
			<publisher>Dublin City University and Association for Computational Linguistics</publisher>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="2335" to="2344" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Disentangled learning of stance and aspect topics for vaccine attitude detection in social media</title>
		<author>
			<persName><forename type="first">Lixing</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zheng</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gabriele</forename><surname>Pergola</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/2022.naacl-main.112</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</title>
		<title level="s">Robert Procter, and Yulan He</title>
		<meeting>the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies<address><addrLine>Seattle, United States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2022">2022</date>
			<biblScope unit="page" from="1566" to="1580" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
