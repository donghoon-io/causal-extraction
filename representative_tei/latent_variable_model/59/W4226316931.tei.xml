<?xml version="1.0" encoding="UTF-8"?>
<TEI xml:space="preserve" xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 https://raw.githubusercontent.com/kermitt2/grobid/master/grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Recursive Bayesian Networks: Generalising and Unifying Probabilistic Context-Free Grammars and Dynamic Bayesian Networks</title>
				<funder ref="#_SbJBqPQ">
					<orgName type="full">European Research Council</orgName>
					<orgName type="abbreviated">ERC</orgName>
				</funder>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability  status="unknown">
					<licence/>
				</availability>
				<date type="published" when="2022-01-15">15 Jan 2022</date>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName><forename type="first">Robert</forename><surname>Lieck</surname></persName>
						</author>
						<author role="corresp">
							<persName><forename type="first">Martin</forename><surname>Rohrmeier</surname></persName>
							<email>martin.rohrmeier@epfl.ch</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department">Digital and Cognitive Musicology Lab École Polytechnique Fédérale de Lausanne</orgName>
								<address>
									<postCode>1015</postCode>
									<settlement>Lausanne</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="department">Digital and Cognitive Musicology Lab École Polytechnique Fédérale de Lausanne</orgName>
								<address>
									<postCode>1015</postCode>
									<settlement>Lausanne</settlement>
									<country key="CH">Switzerland</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Recursive Bayesian Networks: Generalising and Unifying Probabilistic Context-Free Grammars and Dynamic Bayesian Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date type="published" when="2022-01-15">15 Jan 2022</date>
						</imprint>
					</monogr>
					<idno type="arXiv">arXiv:2111.01853v4[cs.LG]</idno>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<encodingDesc>
			<appInfo>
				<application version="0.8.1" ident="GROBID" when="2025-10-28T22:29+0000">
					<desc>GROBID - A machine learning software for extracting information from scholarly documents</desc>
					<ref target="https://github.com/kermitt2/grobid"/>
				</application>
			</appInfo>
		</encodingDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Linear</term>
					<term>Chain Hierarchical</term>
					<term>Tree Variables Only Discrete Regular Grammars</term>
					<term>Hidden Markov Models Probabilistic Context-Free Grammars Discrete</term>
					<term>Continuous Dynamic Bayesian Networks Recursive Bayesian Networks</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Probabilistic context-free grammars (PCFGs) and dynamic Bayesian networks (DBNs) are widely used sequence models with complementary strengths and limitations. While PCFGs allow for nested hierarchical dependencies (tree structures), their latent variables (non-terminal symbols) have to be discrete. In contrast, DBNs allow for continuous latent variables, but the dependencies are strictly sequential (chain structure). Therefore, neither can be applied if the latent variables are assumed to be continuous and also to have a nested hierarchical dependency structure. In this paper, we present Recursive Bayesian Networks (RBNs), which generalise and unify PCFGs and DBNs, combining their strengths and containing both as special cases. RBNs define a joint distribution over tree-structured Bayesian networks with discrete or continuous latent variables. The main challenge lies in performing joint inference over the exponential number of possible structures and the continuous variables. We provide two solutions: 1) For arbitrary RBNs, we generalise inside and outside probabilities from PCFGs to the mixed discrete-continuous case, which allows for maximum posterior estimates of the continuous latent variables via gradient descent, while marginalising over network structures. 2) For Gaussian RBNs, we additionally derive an analytic approximation of the marginal data likelihood (evidence) and marginal posterior distribution, allowing for robust parameter optimisation and Bayesian inference. The capacity and diverse applications of RBNs are illustrated on two examples: In a quantitative evaluation on synthetic data, we demonstrate and discuss the advantage of RBNs for segmentation and tree induction from noisy sequences, compared to change point detection and hierarchical clustering. In an application to musical data, we approach the unsolved problem of hierarchical music analysis from the raw note level and compare our results to expert annotations.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Long-term dependencies with a nested hierarchical structure are one of the major challenges in modelling sequential data. This type of dependencies is common in many domains, such as natural Figure <ref type="figure" target="#fig_8">1</ref>: RBNs generalise PCFGs by allowing for continuous latent variables and DBNs by incorporating nested hierarchical dependencies. language <ref type="bibr" target="#b0">[1]</ref>, music <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>, or decision making <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>. Two of the most widely used probabilistic models for sequential data are probabilistic context-free grammars (PCFGs) and dynamic Bayesian networks (DBNs), both having complementary strengths.</p><p>PCFGs are well-established and widely used for modelling hierarchical long-term dependencies in symbolic data <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b9">10]</ref>. They generalise local (Markov) transition models by allowing for infinitely many levels of nested hierarchical dependencies and a flexible number of latent variables. However, parsing methods such as the Cocke-Younger-Kasami (CYK) algorithm <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b5">6]</ref> rely on the discrete nature of the rules and variables.</p><p>In contrast, DBNs are sequential models with a fixed set of random variables that reoccur at each time step <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>. The variables at each time step may be discrete or continuous, latent or observed, and may have an arbitrary non-cyclic dependency structure among each other, with additional links from the previous and to the next time slice. They comprise important model classes as special cases, such as hidden Markov models (HMMs) if there is only a single discrete latent variable or linear dynamical systems if all dependencies are linear Gaussians <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b15">16]</ref>. However, DBNs only allow for a fixed chain of Markov dependencies between time slices and cannot represent nested hierarchical structures.</p><p>In this paper, we present Recursive Bayesian Networks (RBNs), a novel class of probabilistic models that combines the strengths of PCFGs and DBNs by allowing for nested hierarchical dependencies in combination with arbitrary discrete or continuous random variables (Figure <ref type="figure" target="#fig_8">1</ref>). Our main contributions are as follows:</p><p>1. With RBNs, we provide a unified theoretical framework for a large class of important sequence models, including PCFGs and DBNs. 2. We generalise inside and outside probabilities from PCFGs to continuous latent variables, allowing for maximum posterior (MAP) inference in arbitrary RBNs. 3. For Gaussian RBNs, we derive an analytic approximation for the marginal likelihood and marginal posterior distribution, allowing for robust parameter optimisation and Bayesian inference. 4. We provide a quantitative evaluation on synthetic data and an application to the challenging task of hierarchical music analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Related Work</head><p>PCFGs have a long tradition for modelling nested hierarchical dependencies in symbolic data with a variety of parsing algorithms for inferring the structure and variables' values <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b13">14]</ref>. Beyond their application to sequential data, PCFGs have been generalised to graph structures <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22]</ref>, which readily transfers to applications of RBNs. Latent vector grammars (LVeGs) <ref type="bibr" target="#b22">[23]</ref> are an extension of latent variable grammars (LVGs) <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref> with continuous latent states. As for RBNs, approximate parsing is possible in the Gaussian case. However, both LVGs and LVeGs are special cases of RBNs and do not draw the connection to graphical models. More recently, the availability of automatic differentiation libraries, such as PyTorch <ref type="bibr" target="#b25">[26]</ref>, has lead to a number of applications where gradients are propagated through the entire parsing process <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30]</ref>.</p><p>x z x</p><p>x y z=N z=T We use gates <ref type="bibr" target="#b57">[58]</ref> to describe structural distributions and extended factor graph notation [black squares <ref type="bibr">; 59]</ref> for conditional joint distributions. Considering all possible ways how n observations can be generated by recursively applying the RBN cell produces an RBN chart, as shown in Figure <ref type="figure">3</ref>.</p><p>x 0:1</p><p>x 0:2</p><p>x 0:3</p><p>x 0:4</p><p>x 0:5</p><p>x 0:6</p><p>x 0:7</p><p>x 1:2</p><p>x 1:3</p><p>x 1:4</p><p>x 1:5</p><p>x 1:6</p><p>x 1:7</p><p>x 2:3</p><p>x 2:4</p><p>x 2:5</p><p>x 2:6</p><p>x 2:7</p><p>x 3:4</p><p>x 3:5</p><p>x 3:6</p><p>x 3:7</p><p>x 4:5</p><p>x 4:6</p><p>x 4:7</p><p>x 5:6</p><p>x 5:7</p><p>x 6:7 y 1 y 2 y 3 y 4 y 5 y 6 y 7</p><p>x 0:3</p><p>x 0:7</p><p>x 1:3 x 3:5</p><p>x 3:7</p><p>x 5:7</p><p>x 0:1 y 1</p><p>x 1:2 y 2</p><p>x 2:3 y 3</p><p>x 3:4 y 4</p><p>x 4:5 y 5</p><p>x 5:6 y 6</p><p>x 6:7</p><formula xml:id="formula_0">y 7</formula><p>Figure <ref type="figure">3</ref>: Chart of an RBN in CNF for sequential data of length n = 7. The network obtained by fixing one specific dependency structure is highlighted in blue; latent non-terminal variables that are not part of this particular structure are shown in grey.</p><p>Orange and red boxes, respectively, indicate the subsets X 0:3 and Y 0:3 of latent non-terminal and observed terminal variables generated from x 0:3 .</p><p>The process of parsing a PCFG or RBN can be formally rewritten as a sum-product network [SPN; <ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b32">33]</ref>. Factor graph grammars <ref type="bibr">[FGGs;</ref><ref type="bibr" target="#b33">34]</ref> generalise PCFGs, case-factor diagrams <ref type="bibr" target="#b34">[35]</ref> and SPNs by using a hyperedge replacement graph grammar <ref type="bibr" target="#b18">[19]</ref> to describe a distribution over graph structures that is more general than that of RBNs (not only trees). However, none of the approaches addresses the problem of inference with continuous variables that we are facing in RBNs (exponentially many terms with exponentially many nested integrals).</p><p>A wide range of probabilistic and neural models operate with a fixed graphical structure and are loosely related to RBNs. Hidden tree Markov models <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b36">37]</ref> generalise HMMs from chain to fixed tree structures. They model data at each node as observations of a latent Markov process on the underlying tree, which is part of the input data. Additionally estimating the underlying tree structure has been addressed in <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b39">40]</ref>. Recursive neural tensor networks <ref type="bibr" target="#b40">[41]</ref> use a PCFG for parsing a given sequence of symbols to obtain a tree structure, which is then fixed and used as the backbone for a neural network. More generally, there is a number of methods for inferring a fixed structure for graphical models <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44]</ref>, SPNs <ref type="bibr" target="#b44">[45,</ref><ref type="bibr" target="#b45">46,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b48">49]</ref>, or graph neural networks <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b51">52]</ref>. All these methods have in common that a fixed structure is either given or estimated but not treated in a probabilistic Bayesian manner.</p><p>Some approaches attempt a Bayesian treatment of the unknown structure of a graphical model or SPN via dynamic programming <ref type="bibr" target="#b52">[53,</ref><ref type="bibr" target="#b53">54]</ref> or Markov chain Monte-Carlo sampling <ref type="bibr" target="#b54">[55,</ref><ref type="bibr" target="#b55">56,</ref><ref type="bibr" target="#b56">57]</ref>. However, the structure is assumed to be independent of the latent variables (they only become dependent conditional on the data) and the latent variables cannot be used to control the structure, as it is the case in RBNs. The challenge of continuous variables also remains unsolved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Recursive Bayesian Networks</head><p>RBNs are template-based graphical models that define a joint distribution over network structures and variables' values. The number of template variables is fixed, but the number of instantiated variables, their connectivity and values are governed by the joint distribution. As a rough analogy, RBNs can be thought of as DBNs that can not only be connected linearly to form a chain but also hierarchically to form a tree structure. Alternatively, they can be thought of as a PCFG in which each symbol is a (possibly continuous) random variable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Definition</head><p>RBNs have three types of template variables: 1) latent non-terminal variables (discrete or continuous), 2) observed terminal variables (discrete or continuous), and 3) latent structural variables (always discrete). In the simplest case, illustrated in Figure <ref type="figure" target="#fig_0">2</ref>, an RBN has one template variable of each type. Formally, an RBN is defined as follows: Definition 1 (Recursive Bayesian Network). An RBN is a tuple (X , Y, Z, T , S, p P ) with X : a set of latent non-terminal template variables (1) Y : a set of observed terminal template variables</p><p>(2) Z : a set of latent structural template variables, paired up with the non-terminal variables</p><p>(3) T : a set of transition distributions p(v 1 , . . . , v η | x) from a single non-terminal variable x ∈ X to a set of non-terminal and/or terminal variables v 1 , . . . , v η ∈ X ∪ Y (4) S : a set of structural distributions p(z | x), one for each non-terminal/structural pair (5) p P : a prior/start distribution for exactly one non-terminal variable.</p><p>(</p><formula xml:id="formula_1">)<label>6</label></formula><p>The cardinality of a structural variable z ∈ Z corresponds to the number of possible transitions from the associated non-terminal variable x ∈ X ; η in (4) is called the arity of the transition.</p><p>Generating with an RBN is straightforward. We start by sampling the value of the first non-terminal variable x from the prior distribution p P (x) and then repeat the following steps until no unprocessed non-terminal variables are left:</p><p>1. sample the value of the associated structural variable from p(z | x) 2. choose a transition distribution p(v 1 , . . . , v η | x) based on the structural variable's value 3. sample the variables v 1 , . . . , v η from the transition distribution 4. for all newly generated non-terminal variables, go to step 1.</p><p>The major challenge and focus of this paper is to perform joint inference over the latent structure and non-terminal variables' values conditional on a given set of observations.</p><p>Chomsky Normal Form: In the simplest non-trivial case, an RBN has one latent non-terminal, one observed terminal, and one latent structural template variable, with one non-terminal transition of arity η = 2 and one terminal transition of arity η = 1, as illustrated in Figures <ref type="figure" target="#fig_0">2</ref> and<ref type="figure">3</ref>. It is defined by four distributions</p><formula xml:id="formula_2">p P (x) : prior/start distribution (7) p N (x , x | x) : non-terminal transition (8) p T (y | x) : terminal transition (9) p S (z | x) : termination probability .<label>(10)</label></formula><p>In analogy to PCFGs, we call this the Chomsky normal form (CNF). Any RBN may be rewritten in CNF (see Appendix A.1 for details).</p><p>RBN Chart: During inference, we will make use of an RBN chart, similar to the parse chart for PCFGs <ref type="bibr" target="#b5">[6]</ref>. Each non-terminal variable is associated to a layer in the chart. For discrete variables, they store the actual distributions, while for continuous variables they either hold the point estimate (for MAP inference) or the parameters of the approximate distributions (for inference in Gaussian RBN). Different instances of the same template variable are identified by a subscript indicating the span of data generated from them, which also corresponds to their position in the chart (see Figure <ref type="figure">3</ref>). Sets of variables that are generated from a specific latent non-terminal variable x i:k are denoted by a bold capital letter with a corresponding subscript (X i:k , Y i:k , Z i:k ); omitting the subscript refers to all variables (X, Y, Z); for X and Z this also includes the root variables x 0:n and z 0:n , respectively. The subscripts are to be interpreted as time intervals, that is, Y i:i is empty, Y 0:1 = y 1 is the first observation, Y n-2:n = (y n-1 , y n ) are the last two observations etc.</p><p>Comparison to PCFGs: Any PCFG can be rewritten as an RBN in two different ways, which we call abstraction and expansion (see Appendix A.2 for details). Abstraction of a PCFG produces a discrete RBN with one latent non-terminal and one observed terminal variable. The resulting RBN is exactly equivalent to the original PCFG but describes the same relations in a more abstract and compact way. In contrast, expansion of a PCFG considers the symbols of the grammar as random variables in their own right, thereby endowing them with additional (possibly continuous) degrees of freedom. The resulting RBN is therefore more powerful than the original PCFG. A PCFG is abstracted to a discrete RBN by defining the start/prior, transition, and structural distributions (7-10) as</p><formula xml:id="formula_3">p P (x=A) = W S→A A W S→A (11) p N (x =B, x =C | x=A) = W A→BC B ,C W A→B C (12) p T (y=b | x=A) = W A→b b W A→b (13) p S (z | x=A) =      B,C W A→BC X W A→X if z=N b W A→b X W A→X if z=T , (<label>14</label></formula><formula xml:id="formula_4">)</formula><p>where S is the grammar's start symbol, A, B, C are non-terminal symbols, b is a terminal symbol, X is any right-hand side of a rule, z=N and z=T indicate a non-terminal and terminal transition, respectively, W • → • is the weight of the corresponding rule, and rules that do not exist in the original PCFG are taken to have zero weight. In expansion, the PCFG is only used to define a "skeleton" for the RBN, while the specific random variables and the concrete transition distributions need to be additionally specified. This means that the resulting RBN model is more powerful than the original PCFG, as the symbols may, for instance, be expanded to continuous random variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Inference</head><p>The two main goals of inference in RBNs are to 1) train model parameters by maximising the marginal data likelihood and to 2) compute posterior distributions or maximum posterior (MAP) estimates of the network structure and non-terminal variables. In PCFGs, both is achieved by computing inside and outside probabilities <ref type="bibr" target="#b13">[14]</ref>, which will be the starting point for our generalisation to continuous variables.</p><p>Inside and Outside Probabilities: We define inside and outside probabilities, β and α, for RBNs in analogy to how they are defined for PCFGs, the only difference being that the variables may be continuous. We thus have</p><formula xml:id="formula_5">β(x i:k ) := p(Y i:k | x i:k ) (15) and α(x i:k ) := p(Y 0:i , x i:k , Y k:n ) ,<label>(16)</label></formula><p>where n is the length of the sequence and Y is fixed (and therefore omitted as argument on the left-hand side). That is, β(x i:k ) is the marginal likelihood of generating the sub-sequence Y i:k conditional on the respective non-terminal variable x i:k , while α(x i:k ) is the marginal likelihood of generating the two sub-sequences Y 0:i and Y k:n as well as the non-terminal variable x i:k . In both cases, β and α are functions of the corresponding non-terminal variable with the structure and the remaining variables being marginalised out. Based on the inside and outside probabilities, the marginal data likelihood and the marginal posterior distributions over non-terminal variables are</p><formula xml:id="formula_6">p(Y) = β(x 0:n ) p P (x 0:n ) dx 0:n (17) and p(x i:k | Y) = α(x i:k ) β(x i:k ) p(Y) ,<label>(18)</label></formula><p>respectively. p(x i:k | Y) is an unnormalised probability distribution that specifies the probability of x i:k to exist via the normalisation constant p(x i:k | Y) dx i:k , while the normalised version corresponds to the marginal posterior distribution of x i:k for the case that it does exist.</p><p>Inside probabilities are recursively computed bottom-up. For an RBN in CNF we start with the base case <ref type="bibr" target="#b18">(19)</ref> for single observations and then iterate <ref type="bibr" target="#b19">(20)</ref> to the top of the RBN chart</p><formula xml:id="formula_7">β(x i:i+1 ) = p S (z i:i+1 =T | x i:i+1 ) p T (y i+1 | x i:i+1 )<label>(19)</label></formula><formula xml:id="formula_8">β(x i:k ) = p S (z i:k =N | x i:k ) k-1 j=i+1 p N (x i:j , x j:k | x i:k ) β(x i:j ) β(x j:k ) dx i:j dx j:k . (<label>20</label></formula><formula xml:id="formula_9">)</formula><p>Outside probabilities are recursively computed top-down, while making use of the inside probabilities α(x 0:n ) = p P (x 0:n )</p><formula xml:id="formula_10">α(x j:k ) = j-1 i=0 p S (z i:k =N | x i:k ) p N (x i:j , x j:k | x i:k ) α(x i:k ) β(x i:j ) dx i:j dx i:k + n l=k+1<label>(21)</label></formula><p>p S (z j:l =N | x j:l ) p N (x j:k , x k:l | x j:l ) α(x j:l ) β(x k:l ) dx j:l dx k:l .</p><p>As for PCFGs, the two terms in <ref type="bibr" target="#b21">(22)</ref> correspond to the possibility of x j:k being generated as the right or the left child, respectively. The main conceptual difference to PCFGs is that we treat the discrete structural part (marginalised out by the sums) separately from the potentially continuous variables (marginalised out by the integrals). For RBNs that are not in CNF, the equations have to be adapted accordingly (see Appendix A.3 for the general case).</p><p>Marginalisation: Computing the marginal data likelihood <ref type="bibr" target="#b16">(17)</ref> and the marginal posterior distributions over non-terminal variables <ref type="bibr" target="#b17">(18)</ref> requires to solve an exponential (w.r.t. the length n of the sequence) number of nested integrals in <ref type="bibr" target="#b18">(19)</ref><ref type="bibr" target="#b19">(20)</ref><ref type="bibr" target="#b20">(21)</ref><ref type="bibr" target="#b21">(22)</ref>, which is generally intractable. However, for the special case of Gaussian RBNs, we provide an adaptive closed-form approximation in Section 2.3. Moreover, marginalising only over the network structure for a fixed assignment of the non-terminal variables X is straight forward and allows for maximum posterior (MAP) inference in general RBNs.</p><p>Maximum Posterior Inference: For a fixed assignment of all non-terminal variables X, we can compute the joint marginal likelihood p(X, Y) over observed terminal and latent non-terminal variables by only marginalising over the structure. This follows the same principle as above but uses the modified joint inside and outside probabilities</p><formula xml:id="formula_12">β i:k := p(X i:k , Y i:k | x i:k ) (23) and α j:k := p(X 0:j , Y 0:j , x j:k , X k:n , Y k:n ) ,<label>(24)</label></formula><p>where all variables are fixed (and therefore omitted as arguments on the left-hand side). Analogously, the joint marginal likelihood and the marginal posterior probability of x i:k to exist then are p(X, Y) = β 0:n p P (x 0:n ) (25) and</p><formula xml:id="formula_13">p i:k = α i:k β i:k p(X, Y) ,<label>(26)</label></formula><p>where p i:k is the probability of x i:k to exist for this specific assignment of X. The corresponding equations for the recursion differ from <ref type="bibr" target="#b18">(19)</ref><ref type="bibr" target="#b19">(20)</ref><ref type="bibr" target="#b20">(21)</ref><ref type="bibr" target="#b21">(22)</ref> only in that they do not integrate out the latent nonterminal variables (see Appendix A.3.3). As before, all computations can be efficiently performed via dynamic programming. Gradients w.r.t. the variables and/or parameters are readily obtained from libraries such as PyTorch <ref type="bibr" target="#b25">[26]</ref>. Optimising the values of the latent non-terminal variables X via gradient descent yields maximum posterior (MAP) estimates, while the structure is marginalised out. MAP estimates for the structure (i.e. the best tree) conditional on an assignment for X can be computed (as for PCFGs) by replacing summation with maximisation <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b59">60]</ref>.</p><p>There are two caveats: First, due to marginalising over multiple (exponentially many) network structures, p(X, Y) may be highly non-convex and optimising X via gradient descent is not guaranteed to find the global optimum. This is even the case for purely Gaussian RBNs, for which p(X, Y) is a mixture of Gaussians (one for each structure). Second, we can optimise X while marginalising out the structure and we can optimise the structure for a fixed assignment of X. However, successively optimising X and the structure is not equivalent to jointly optimising both and the maximum of p(X, Y) may be unrelated to the maximum of the best structure (also see Figure <ref type="figure">4</ref>). This means that generally, exact joint MAP inference over the latent variables and the structure is hard. For Gaussian RBNs, we provide an approximate solution below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Gaussian RBNs</head><p>In a Gaussian RBN (GRBN), the prior, non-terminal, and terminal distributions are linear Gaussians and the termination probability (structural distribution) is constant</p><formula xml:id="formula_14">p P (x) := N (x; µ P , Σ P ) [prior] (27) p N (x , x | x) := N (x ; x, Σ NL ) N (x ; x, Σ NR ) [non-terminal] (28) p T (y | x) := N (y; x, Σ T ) [terminal] (29) p S (z=T | x) := p term . [termination/structural]<label>(30)</label></formula><p>For clarity, we will show all derivations for GRBNs in this basic form. For our evaluations and the application to music, we use a slightly extended version that includes linear transformations, mixtures of Gaussians, and multi-terminal transitions (Section 2.3.1). The derivations do not fundamentally change for the extended case (see Appendix A.4). In Appendix B, we show all calculations on a simple example.</p><p>Adaptive Approximation: If the structure of a GRBN was fixed, all variables would be jointly Gaussian distributed as in a conventional Gaussian Bayesian network <ref type="bibr" target="#b15">[16]</ref>. However, due to the mixture max approximation components</p><p>Figure <ref type="figure">4</ref>: Three Gaussians components, the resulting mixture (green), maximum (orange), and moment-matching single Gaussian approximation (blue). Note that the maximum of the mixture (), the best component (), and the approximation () may be unrelated. unknown structure, we effectively have a mixture of exponentially many Gaussians, one for each possible structure. While in principle all integrals can be solved analytically, the exponential growth makes exact inference intractable. Therefore, our goal is to derive a parsing strategy that retains tractability by adaptively applying local approximations to the Gaussian mixtures occurring in each recursion step. We will here focus on the simplest case of approximating the mixtures with a single Gaussian (illustrated in Figure <ref type="figure">4</ref>, details in Appendix A.4.2), which can be efficiently computed in closed form <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b60">61]</ref>. The inside and outside probabilities are thus represented by a simple Gaussian</p><formula xml:id="formula_15">β(x i:k ) ≈ c (β) i:k N (x i:k ; µ (β) i:k , Σ<label>(β)</label></formula><formula xml:id="formula_16">i:k ) (31) α(x j:k ) ≈ c (α) j:k N (x j:k ; µ (α) j:k , Σ<label>(α)</label></formula><formula xml:id="formula_17">j:k )<label>(32)</label></formula><p>and this form is reestablished in each iteration by approximating the occurring mixtures. Consequently, the marginal posterior distributions over latent variables <ref type="bibr" target="#b17">(18)</ref> are also simple Gaussians and the marginal data likelihood ( <ref type="formula">17</ref>) can be computed in closed form. This approximation scheme can be extended and refined by using existing methods for approximating each Gaussian mixture by one with fewer components <ref type="bibr" target="#b61">[62,</ref><ref type="bibr" target="#b62">63]</ref>.</p><p>Marginalisation: In ( <ref type="formula" target="#formula_8">20</ref>) and ( <ref type="formula" target="#formula_11">22</ref>), we have to integrate over products of Gaussian distributions to marginalise out the latent variables. To solve these integrals, we make use of the fact that the product of two Gaussians over a variable x can be rewritten as [see e.g. 64]</p><formula xml:id="formula_18">N (x; µ 1 , Σ 1 ) N (x; µ 2 , Σ 2 ) = N (µ 1 ; µ 2 , Σ 1 + Σ 2 ) N (x; μ, Σ)<label>(33)</label></formula><p>with</p><formula xml:id="formula_19">Σ := (Σ -1 1 + Σ -1 2 ) -1 and μ := Σ Σ -1 1 µ 1 + Σ -1 2 µ 2 .<label>(34)</label></formula><p>Hence, when integrating over x, only the first term on the rhs. of (33) remains. A detailed step-by-step derivation of all results can be found in Appendix A.4.1. With the latent variables being marginalised out, ( <ref type="formula" target="#formula_8">20</ref>) and ( <ref type="formula" target="#formula_11">22</ref>) become simple mixtures of Gaussians that can be easily approximated to retain the simple analytic form of the inside and outside probabilities.</p><p>Tree Induction: As described above, exact joint MAP inference over the continuous latent variables and the structure is generally intractable. Moreover, the maximum of the approximate posterior does not necessarily coincide with the maximum of the exact posterior or that of a particular structure (see Figure <ref type="figure">4</ref>). Thus, first optimising X (based on the approximation) and then estimating the structure (conditional on the picked value of X) may lead to arbitrarily bad results for tree induction. Therefore, we leverage the adaptive character of our approximation scheme to compute local structure estimates in each step, before loosing relevant information due to further approximations. Specifically, during the bottom-up pass for computing inside probabilities, all structures are scored by the maximum of their marginal likelihood, based on its current approximation <ref type="bibr" target="#b30">(31)</ref>. The best overall structure is then selected (as usual) in a top-down pass (see Appendix A.4.3 and our example in Appendix B).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Gaussian RBNs for Music</head><p>For the application to music, we slightly extend the basic GRBN discussed so far by introducing transpositions and multi-terminal transitions (changes in the equations highlighted in blue). The corresponding graphical model of the RBN cell is shown in Figure <ref type="figure" target="#fig_1">5</ref>. Furthermore, we describe how GRBNs can be applied to categorical data.</p><p>Transpositions: A transposition rotates the dimensions of the latent variable by a number of steps τ before generating the child. This is achieved by multiplying with an orthonormal transposition matrix T τ that corresponds to the identity matrix with cyclicly rearranged columns. For the prior distribution, we assume a uniform weighting of all possible transpositions</p><formula xml:id="formula_20">p P (x) := D-1 τ =0 1 D N (x; T τ µ p , Σ P ) , [prior] (<label>35</label></formula><formula xml:id="formula_21">)</formula><p>where D is the dimensionality of the data (D = 12 for music in 12-tone equal temperament). For the non-terminal transitions, the probability for a specific transposition is determined by the weight parameter</p><formula xml:id="formula_22">W p N (x , x | x) := D-1 τ =0 p(τ | W ) N (x ; T τ x, Σ NL )N (x ; x, Σ NR ) .<label>(36)</label></formula><p>Note that transpositions are only applied to the left child, because Western classical music is thought to be fundamentally goal directed <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b64">65]</ref>. This means that the character of a section is largely determined by how it ends (the right child), which should also be reflected in the value of the parent node. In contrast, the role of the left child is to harmonically prepare the ending (or prepare a preparation to the ending etc). We therefore allow for arbitrary transpositions in the left child and we will see below that our model indeed captures the most important type of preparation in Western classical music: the cadential dominant-tonic progression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Multi-Terminal Transitions:</head><p>A multi-terminal transition generates multiple observed variables from a single latent variable. The variables are generated i.i.d. and their number is governed by a Poisson distribution with rate parameter λ</p><formula xml:id="formula_23">p T (y i:k | x i:k ) := Pois(k -i -1 | λ) k j=i+1 N (y j ; x i:k , Σ T ) . [multi-terminal]<label>(37)</label></formula><p>Multi-terminal transitions do not conform to the CNF assumed so far and we need to add the term</p><formula xml:id="formula_24">β(x i:k ) = • • • + p S (z i:k =T | x i:k ) p T (y i:k | x i:k ) (38) = • • • + p term p T (y i:k | x i:k )</formula><p>[for GRBNs, see ( <ref type="formula" target="#formula_14">30</ref>)] (39) to <ref type="bibr" target="#b19">(20)</ref> in order to account for the possibility to terminate from a higher-level variable. For k = i + 1, this term becomes the base case <ref type="bibr" target="#b18">(19)</ref> of an RBN in CNF.</p><p>Multi-terminal transitions account for the situation where changes in the hierarchical structure occur at a lower rate than the time series is sampled. In between the structural changes, the data is assumed to be generated from the same model, which could also be more elaborate than i.i.d. samples, as long as the relevant model parameters are captured by the RBN's latent variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Categorical Data:</head><p>The observed variables of a GRBN are unconstrained real-valued, which poses a problem if the data are categorical. This situation is comparable to using Gaussian processes (GPs) <ref type="bibr" target="#b65">[66]</ref> for classification and can be approached with similar methods. In our application to musical data, we observe one or more notes being played at any particular time and normalise these counts to obtain observations that correspond to the parameter of a categorical distribution. The natural likelihood function for this type of observations is a Dirichlet distribution. Therefore, we adapt the approach suggested in <ref type="bibr" target="#b66">[67]</ref> for GPs, who assume a Dirichlet likelihood, which is then approximated by a Gaussian likelihood in log-space. Since an observation from a Dirichlet distribution corresponds to a normalised sample from independent Gamma distributions, each Gamma distribution can be separately approximated by a log-normal distribution, which results in a diagonal covariance matrix for the Gaussian likelihood in log-space. Matching the first and second moment yields <ref type="bibr" target="#b66">[67]</ref> </p><formula xml:id="formula_25">y (l) j = log y (l) j -Σ (j) ll /2</formula><p>and Σ (j)</p><formula xml:id="formula_26">ll = log(1/y (l) j + 1) ,<label>(40)</label></formula><p>where 0 &lt; y (l) j &lt; 1 is the l th element (normalised count) of the j th observation, y</p><p>j is the corresponding mean of the approximate Gaussian likelihood in log-space, and Σ (j) ll is the l th element on the diagonal of the covariance matrix for the j th observation. We thus have to replace y j and Σ (j) for y j and Σ T in <ref type="bibr" target="#b36">(37)</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>We performed a quantitative evaluation on synthetic data and applied our model to hierarchical music analysis of Bach preludes. We show that RBNs are superior to change point detection (CPD) and hierarchical clustering (HC) for tree induction and our method is able to infer fundamental harmonic principles of Western classical music. Experiments were run on a 3.6 GHz Quad-Core Intel Core i7 processor with 32GB RAM. The model parameters were trained via gradient descent on the (approximate) marginal neg-log likelihood.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Quantitative Evaluation on Tree Induction</head><p>We performed a quantitative evaluation on synthetic data for the task of segmenting a noisy time series and inferring the underlying tree. For comparison, we used the best-performing change point detection (CPD) method from the ruptures library <ref type="bibr" target="#b71">[72]</ref> for segmenting the time series, combined with bottom-up hierarchical clustering (HC) for inferring the tree structure ("HC/CPD"). For details of the methodology, see Appendix C.1.</p><p>The evaluation results in Figure <ref type="figure" target="#fig_2">6</ref>(a) show that the RBN tree estimates (Section 2.3) consistently outperform the one from HC/CPD, in terms of both precision and recall (and thus also in F1 measure). The marginal node probabilities show an interesting performance pattern. They excel in terms of precision, which means that a node with high marginal probability is very likely to actually exist in the tree (low false-positive rate). However, they severely underestimate the overall node probabilities, which leads to recall falling far below the baseline. This means that a node with low marginal probability may in fact occur in the tree (high false-negative rate).</p><p>We think that the poor recall measure of the marginal probabilities is primarily due to (and the downside of) a fully Bayesian treatment that quantifies uncertainty. Even if the marginal probabilities have a maximum at the correct node location, probability mass will still spread around it and be allocated to a number of less probable locations. While this is the desired behaviour of a Bayesian method, it inevitably results in a lower recall value. The high precision value confirms that uncertainty is adequately quantified and not underestimated. That being said, the marginal probabilities provide an exceptionally rich basis for qualitative analyses. For instance, all ground-truth nodes are located at local maxima of the marginal probabilities and we can read off a number of other potential node locations, which essentially trace out the grid defined by the piece-wise constant segments (see Figure <ref type="figure">8</ref> in Appendix C.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Hierarchical Music Analysis</head><p>Harmonies in Western classical music exhibit a nested hierarchical structure that can be modeled by PCFGs operating on abstract chord symbols <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b72">73,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b2">3]</ref>. While these grammars can be applied to expert annotations of a musical score, hierarchical music analysis from the raw note level is an unsolved problem. We trained a GRBN (Section 2.3.1) on the 24 major preludes of Johann Sebastian Bach's "Wohltemperiertes Klavier I &amp; II" (see Appendix C.2 for technical details and complete results).</p><p>Our first major finding is that the prior mean, shown in Figure <ref type="figure" target="#fig_2">6</ref>(b), corresponds to a major pitch profile (as could be expected from the training data) and is in excellent agreement with recent Bayesian estimates from the literature <ref type="bibr" target="#b67">[68]</ref>. The fact that the major profile appears in the prior (i.e. as the continuous equivalent of a grammar's start symbol) shows that our model picks up fundamentally important structures from the musical data. Our second finding is that only two transpositions have non-zero weights: the identity with a weight of 78% and the fifth scale degree (7 semitones) with a weight of 22%. This corresponds to the left child being generated as the dominant of the parent and realises the most important harmonic preparation in Western classical music: the cadential dominant-tonic relation. A closer inspection of the expert analysis (Figure <ref type="figure" target="#fig_10">10</ref> in Appendix C.2) reveals that when considering the possible surface patterns (raw notes) of the labeled chords, most non-identity transitions can indeed be explained as (noisy) fifth transpositions. The strong weight of fifth transpositions in our model is a highly non-trivial empirical confirmation of the established music theoretical insight that Baroque music is fundamentally driven by dominant-tonic relations. While the estimated tree in Figure <ref type="figure" target="#fig_2">6</ref>(c) fails to reproduce the large-scale structure of the expert analysis (e.g. the separation into two main parts), it accurately captures the measure-wise harmonic changes on the bottom level.</p><p>On the one hand, we see considerable room for improvement by integrating more advanced concepts, such as different modes (major/minor), diatonic in addition to chromatic transposition, or balancing of trees. On the other hand, our model was able to capture fundamental properties of Western classical music based on only 24 pieces. We therefore think that Gaussian RBNs are a highly promising approach for hierarchical music analysis from the raw note level, which should be further investigated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>We introduced Recursive Bayesian Networks (RBNs), a novel class of probabilistic models that unifies the strengths of probabilistic context-free grammars (PCFGs) and dynamic Bayesian networks (DBNs), generalising both model classes. We defined RBNs as a joint distribution over tree-structured Bayesian networks and their (discrete or continuous) variables and described how to perform inference over both the model structure and the variables by leveraging parsing methods for PCFGs. The provided formalisation connects with the methods for formal grammar as well as with the versatile notation for graphical models. On two data sets, we demonstrated the potential of RBNs for modelling nested hierarchical dependencies in real-valued time series and musical data. The class of RBNs represents a substantial contribution to the machine learning toolkit by unifying two of the most important approaches for modelling sequential data and bears a large potential for further development and applications.</p><p>. . . that convert each non-terminal to its equivalent terminal variable. For the newly added non-terminals, there is only a single transition and hence a degenerate structural variable that can only take a single value.</p><p>2) Eliminate more than two latent non-terminal variables: This is done by introducing new nonterminals that capture combinations of multiple old non-terminals. Below, we show how the number of non-terminals can be reduced by one. Applying this procedure repeatedly allows for reducing the number of non-terminals from an arbitrary number down to two, as required for CNF. A transition p(x (1) , x (2) , . . . ,</p><formula xml:id="formula_28">x (n) | x)<label>(44)</label></formula><p>that generates n non-terminals x (1) , . . . , x (n) is rewritten as p(x (1) , x (2) , . . . ,</p><formula xml:id="formula_29">x (n-1) | x ) p(x , x (n) | x) ,<label>(45)</label></formula><p>where we introduced the new non-terminal variable x = (x (1) , x (2) , . . . , x (n-1) ) that stores all the information from the first n -1 original non-terminals. The actual "work" is done by p(x , x (n) | x), which is the equivalent of the original n-fold transition. p(x (1) , x (2) , . . . , x (n-1) | x ) is a deterministic transition that just "unpacks" the information stored in x . Repeating this procedure to come to only pairwise transitions corresponds to a chain of these deterministic "unpacking" operations. As above, the newly added non-terminals have only a single possible transition.</p><p>3) Eliminate unary cycles: Unary cycles p cycle (x | x), where x and x are the same non-terminal template variable (x ≡ x ), are first transformed into unary transitions to a new non-terminal variable and then eliminated as described below. We define a new non-terminal variable x = (x , n), where n &gt; 0 represents the number of steps taken in the cycle before exiting it and x is the value at the moment of exiting it. The transition distribution to x is</p><formula xml:id="formula_30">p(x | x) = p(x , n | x) = p(z =cycle | x) p cycle (x | x) if n = 1 p(z=cycle | x ) p cycle (x | x ) p(x , n -1 | x) dx if n &gt; 1 ,<label>(46)</label></formula><p>where in the recursive case, the variables of all intermediate steps are successively marginalised out. In practical applications, if p(z=cycle | x ) &lt; 1, the probability of remaining in the cycle decays exponentially and the recursion can be truncated after a number of steps. If on the other hand p(z=cycle | x ) ≈ 1 so that truncating is not possible, one can work with the stationary distribution of the resulting Markov chain (i.e. the Markov chain with transition distribution p cycle (x | x)).</p><p>The structural probability to take a transition from x to x is p(z=cycle | x), i.e. the probability of entering the cycle in the first place. The RBN cell of x is identical to that of x, except for the transition into the cycle, which is eliminated (the structural distribution thus has to be renormalised for the remaining transitions). The transitions use only the x -component of x, ignoring the n-component. In this way, we have expresses the state after an arbitrary number of steps in the unary cycle as a distinct value of the new non-terminal variable x.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4) Eliminate unary transitions between non-terminal variables:</head><p>Unary transitions p unary (x | x), where x and x are different non-terminal template variables, are transformed by treating x as an intermediate variable and marginalising it out. All transitions p (1) , . . . , p (n) from x to some other variables (terminal and/or non-terminal)</p><formula xml:id="formula_31">p (1) (. . . | x ) . . . (<label>47</label></formula><formula xml:id="formula_32">) p (n) (. . . | x )</formula><p>are replaced by a set of new transitions p 1 , . . . , p n from x directly to the respective variables, with the intermediate variable x marginalised out</p><formula xml:id="formula_33">p 1 (. . . | x) = p (1) (. . . | x ) p unary (x | x) dx . . . (<label>48</label></formula><formula xml:id="formula_34">) p n (. . . | x) = p (n) (. . . | x ) p unary (x | x) dx .</formula><p>The intermediate variable x and its RBN cell is eliminated if it was only reachable via x. The new transitions p 1 , . . . , p n are merged into the cell of x, while the original transition p unary to x is removed. This requires redefining the structural distribution p(z | x) such that the probability mass p(z=unary | x) that was formerly assigned to p unary is now split among the new transitions p 1 to p n according to the structural distribution p(z | x ) of x . Specifically, for a new transition p i , we define</p><formula xml:id="formula_35">p(z=i | x) := p(z=unary | x) p(z =i | x ) .<label>(49)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2 Relation to PCFGs</head><p>As described in Section 2.1, a PCFG can be rewritten as an RBN by abstraction or expansion, where abstraction produces an equivalent RBN that describes the same relations in a more abstract and compact way, while expansion produces a more general RBN using the original PCFG as a skeleton.</p><p>We describe the two procedures in detail below and use the following definition of a PCFG: </p><formula xml:id="formula_36">p P (x=A) = W S→A A W S→A (11) p N (x =B, x =C | x=A) = W A→BC B ,C W A→B C<label>(12)</label></formula><formula xml:id="formula_37">p T (y=b | x=A) = W A→b b W A→b (13) p S (z | x=A) =      B,C W A→BC X W A→X if z=N b W A→b X W A→X if z=T ,<label>(14)</label></formula><p>where A, B, C ∈ N are non-terminal symbols of the PCFG, b ∈ T is a terminal symbol, X ∈ N 2 ∪T is any right-hand side of a rule, z=N and z=T indicate a non-terminal and terminal transition, respectively, W A→X is the weight of the corresponding PCFG rule, and rules that do not exist in the original PCFG are taken to have zero weight.</p><p>To show equivalence, we need to prove that the transition probabilities from a given non-terminal symbol A are the same in the original PCFG and the new RBN.</p><p>Proof. In the RBN, the probability for a non-terminal transition A → B C is</p><formula xml:id="formula_38">P (A → B C) = p S (z=N | x=A) p N (x =B, x =C | x=A) (52) = B ,C W A→B C X W A→X W A→BC B ,C W A→B C (53) = W A→BC X W A→X<label>(54)</label></formula><p>and that for a terminal transition A → b is</p><formula xml:id="formula_39">P (A → b) = p S (z=T | x=A) p T (y=b | x=A) (55) = b W A→b X W A→X W A→b b W A→b (56) = W A→b X W A→X ,<label>(57)</label></formula><p>which matches the corresponding probabilities in the PCFG, gained by normalising the respective weights.</p><p>Conversely, any discrete RBN can be rewritten as a PCFG. Theorem 2. A discrete RBN with n latent non-terminal template variables x 1 , . . . , x n , m observed terminal template variables y 1 , . . . , y m , and a prior p P (x 1 ) over x 1 can be rewritten as a PCFG with</p><formula xml:id="formula_40">N := x 1 ⊕ • • • ⊕ x n (58) T := y 1 ⊕ • • • ⊕ y m (<label>59</label></formula><formula xml:id="formula_41">)</formula><formula xml:id="formula_42">W A→X :=    p P (X) if A = S ∧ X ∈ x 1 p(z=i | A) p i (X | A) if a matching transition exists in the RBN 0 else ,<label>(60)</label></formula><p>where • ⊕ • concatenates the value ranges of the respective variables, X ∈ x i denote that the value X is in the value range of the RBN variable x i , and the second case in (60) requires there be a transition</p><formula xml:id="formula_43">p i (x 1 , . . . , x k | x i ) such that A ∈ x i and X ∈ x 1 ⊕ • • • ⊕ x k .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.2.2 Expansion of a PCFG</head><p>Expansion of a PCFG to an RBN uses the PCFG as a "skeleton" to define the number of template variables and the structural transitions. The domains and transitions for the variables need to be added, which results in an RBN that is more powerful than the original PCFG. Specifically, we have</p><formula xml:id="formula_44">X := {x A | A ∈ N } and Y := {y b | b ∈ T }<label>(61)</label></formula><p>for the sets of latent non-terminal and observed terminal template variables and</p><formula xml:id="formula_45">p(z A =X | x A ) = W A→X X W A→X with A ∈ N and X, X ∈ (N ∪ T ) *<label>(62)</label></formula><p>for the structural transitions. Additional, we have to define the domain for each of the non-terminal and terminal variables in X and Y, and for each rule A → X 1 X 2 . . . from the original PCFG, we have to define a concrete transition distribution p(v X1 , v X2 , . . . | x A ) for the RBN (where v X1 , v X2 , . . . ∈ X ∪ Y are non-terminal or terminal variables in the RBN, respectively, depending on whether X 1 , X 2 , . . . ∈ N ∪ T are non-terminal or terminal symbols in the PCFG).</p><p>Expansion of a PCFG into an RBN seems appealing if a simple PCFG can be used to describe the type of variables (as opposed to their values) as well as the structure of the generative process. The actual transitions on the variables' values may then take place on a sub-symbolic/continuous level, which cannot be described by a PCFG.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3 General Inside and Outside Probabilities</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3.1 Inside Probabilities</head><p>The inside probability</p><formula xml:id="formula_46">β(x i:k ) = p(Y i:k | x i:k )<label>(63)</label></formula><p>is the probability of generating the observed terminal variables Y i:k from the latent non-terminal variable x i:k . This means that we need to marginalise over all possible paths of generation. Transitions may directly generate observed variables, but they may also generate lower-level non-terminals, in which case we have to recurse using the respective inside probabilities from those variables.</p><p>Let T x ⊆ T be the set of possible transitions from the latent non-terminal template variable x ∈ X (of which x i:k is one specific instantiation), with p(z i:k =τ | x i:k ) being the probability for the transition τ ∈ T x to be selected. This constitutes the first sum in (64) below, which marginalises over the transitions.</p><p>The transition τ generates η new non-terminal and/or terminal variables, where η is the arity of τ . These may be located at different positions in the parse chart, depending on which part of the observed variables Y i:k is generated from them. That is, the variables' locations in the parse chart are not known during generation and are determined in hindsight once all observed variables are generated; thus, they are known for parsing. We denote the respective splitting points by j 1 , . . . , j η-1 (they have to fulfill the condition i &lt; j 1 &lt; . . . &lt; j η-1 &lt; k) and the respective variables by v i:j1 , . . . , v jη-1:k ∈ X ∪ Y. The second multi-sum in <ref type="bibr" target="#b63">(64)</ref> aggregates the probabilities of the different splitting possibilities, that is, of all valid assignments of j 1 , . . . , j η-1 (η -1 degrees of freedom). For instance, a transition of arity η = 2 has one free splitting point j 1 to sum over.</p><p>Some of the generated variables may be observed/terminal variables, for which nothing more needs to be done as they directly constitute the respective part of Y i:k . For the subset of non-terminal variables, which we denote by {v j:j ∈ X }, we need to insert their respective inside probabilities and marginalise them out. This constitutes the product and multi-integral in <ref type="bibr" target="#b63">(64)</ref>.</p><p>The general form of the inside probabilities then is</p><formula xml:id="formula_47">β(x i:k ) = τ ∈Tx p S (z i:k =τ | x i:k ) • • • i&lt;j1&lt;...&lt;jη-1&lt;k • • • {v j:j ∈X } p τ (v i:j1 , . . . , v jη-1:k | x i:k ) {v j:j ∈X } β(v j:j ) . (<label>64</label></formula><formula xml:id="formula_48">)</formula><p>The concrete RBNs considered in the paper have only two transitions, one non-terminal transition of arity two and one terminal transition of arity one (for CNF) or more (for the extended GRBNs used in the quantitative evaluation and for modelling music). For non-terminal transition of arity two, the multi-sum in <ref type="bibr" target="#b63">(64)</ref> reduces to a single sum and the multi-integral to a double integral, which gives us <ref type="bibr" target="#b19">(20)</ref>. For the terminal transition, (64) simplifies to <ref type="bibr" target="#b18">(19)</ref> or the extended version <ref type="bibr" target="#b37">(38)</ref>, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3.2 Outside Probabilities</head><p>The outside probability</p><formula xml:id="formula_49">α(x j:j ) = p(Y 0:j , x j:j , Y j :n )<label>(65)</label></formula><p>is the joint probability of generating the latent non-terminal variable x j:j as well as the prefix and suffix of observed terminal variables, Y 0:j and Y j :n , respectively. For this, we now have to consider all possible ways how x j:j as well as the prefix and suffix could have been generated from a parent non-terminal x (x and x may correspond to the same template variable or to two different ones).</p><p>Let T -1</p><p>x ⊆ T denote the set of transitions that include x as one of the generated variables. Importantly, if x appears multiple times in the generated variables of a particular transition, these different options of generating x are represented as multiple distinct entries in T -1</p><p>x , one for each occurrence. The first sum in <ref type="bibr" target="#b65">(66)</ref> runs over these different possibilities of generating x.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>For a transition τ ∈ T -1</head><p>x of arity η, let j 0 , . . . , j η be the splitting points, including the start and end point j 0 and j η of the parent variable xj0:jη , which have to fulfill the condition 0 ≤ j 0 &lt; . . . &lt; j η ≤ n (where n is the length of the sequence). One pair of adjacent splitting points (j m , j m+1 ) corresponds to the occurrence of x j:j , where m is the position (starting at zero) at which x appears in the generated variables of the particular transition τ . We therefore have the additional constraints j m = j and j m+1 = j , resulting in η -1 remaining free indices to sum over (as for the inside probabilities above). This corresponds to the second multi-sum in <ref type="bibr" target="#b65">(66)</ref>.</p><p>The set of non-terminal variables generated from the parent xj0:jη , excluding x j:j , is denoted by {v l:l ∈ X } \ x j:j . Together with the directly generated terminal variables, these generate part of the prefix and suffix, Y j0:j and Y j :jη . The remaining prefix and suffix, Y 0:j0 and Y jη:n , are generated from the parent variable xj0:jη . For the parent, we recurse via its outside probability α(x j0:jη ), while for the newly generated non-terminal variables (except x j:j ), we have to use the respective inside probability β(v l:l ) in <ref type="bibr" target="#b65">(66)</ref>. Additionally, we have to marginalise out the parent (first integral) and the newly generated non-terminal variables (second multi-integral).</p><p>The general outside probabilities then are</p><formula xml:id="formula_50">α(x j:j ) = τ ∈T -1 x • • • 0≤j0&lt;...&lt;jη≤n jm=j∧j m+1 =j xj 0 :jη • • • {v l:l ∈X }\x j:j p sa (z j0:jη =τ | xj0:jη )<label>(66)</label></formula><p>p τ (v j0:j1 , . . . , x j:j , . . . , v jη-1:jη | xj0:jη ) α(x j0:jη ) {v l:l ∈X }\x j:j β(v l:l ) .</p><p>For a non-terminal transition of arity two, as we have it in the paper, the multi-sum in <ref type="bibr" target="#b65">(66)</ref> reduces to a single sum and {v l:l ∈ X } \ x j:j contains only a single non-terminal, the second child. Importantly, T -1</p><p>x has two elements, one for x j:j being generated as the right child and one for it being generated as the left child, which gives us <ref type="bibr" target="#b21">(22)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.3.3 Joint Inside and Outside Probabilities</head><p>The joint inside and outside probabilities ( <ref type="formula">23</ref>) and ( <ref type="formula" target="#formula_12">24</ref>) for an RBN in CNF are computed analogously to <ref type="bibr" target="#b18">(19)</ref><ref type="bibr" target="#b19">(20)</ref><ref type="bibr" target="#b20">(21)</ref><ref type="bibr" target="#b21">(22)</ref> for the normal inside and outside probabilities, that is,</p><formula xml:id="formula_51">β i:i+1 = p S (z i:i+1 =T | x i:i+1 ) p T (y i+1 | x i:i+1 ) (<label>67</label></formula><formula xml:id="formula_52">)</formula><formula xml:id="formula_53">β i:k = p S (z i:k =N | x i:k ) k-1 j=i+1 p N (x i:j , x j:k | x i:k ) β i:j β j:k (<label>68</label></formula><formula xml:id="formula_54">)</formula><formula xml:id="formula_55">α 0:n = p P (x 0:n ) (<label>69</label></formula><formula xml:id="formula_56">)</formula><formula xml:id="formula_57">α j:k = j-1 i=0 p S (z i:k =N | x i:k ) p N (x i:j , x j:k | x i:k ) α i:k β i:j + n l=k+1 p S (z j:l =N | x j:l ) p N (x j:k , x k:l | x j:l ) α j:l β k:l . (<label>70</label></formula><formula xml:id="formula_58">)</formula><p>This differs from <ref type="bibr" target="#b18">(19)</ref><ref type="bibr" target="#b19">(20)</ref><ref type="bibr" target="#b20">(21)</ref><ref type="bibr" target="#b21">(22)</ref> only by dropping the integrals and dependencies on the non-terminal variables (as their values are now fixed). Joint inside and outside probabilities for the general case are obtained from ( <ref type="formula" target="#formula_47">64</ref>) and ( <ref type="formula" target="#formula_50">66</ref>) analogously, i.e. again by dropping the integrals and dependencies on the non-terminal variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4 Gaussian RBNs</head><p>In the following, we present derivations for the extended case of GRBNs, described in Section 2.3.1, which includes linear transformations T for the left child. For this, we will make use of the fact that a normal distribution over a transformed variable T x can be rewritten as</p><formula xml:id="formula_59">N (T x; µ, Σ) = 1 ||T || N (x; T -1 µ, T -1 Σ T -1 ) (71) = N (x; T µ, T Σ T ) ,<label>(72)</label></formula><p>where ||T || is the absolute value of the determinant of T and in <ref type="bibr" target="#b71">(72)</ref> we made use of the fact that in our case, the transformation matrices are orthonormal, so that T -1 = T and ||T || = 1.</p><p>Note that for an implementation, some of the results should be rewritten in order to minimise the number of matrix inverses that need to be taken. In particular, the identity</p><formula xml:id="formula_60">(Σ -1 1 + Σ -1 2 ) -1 = Σ 1 (Σ 1 + Σ 2 ) -1 Σ 2 (<label>73</label></formula><formula xml:id="formula_61">)</formula><p>is useful for the implementation, but we omit it in our derivation for clarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4.1 Marginalisation</head><p>For the inside probability β(x i:k ), the integral in ( <ref type="formula" target="#formula_8">20</ref>) is</p><formula xml:id="formula_62">p N (x i:j , x j:k | x i:k ) β(x i:j ) β(x j:k ) dx i:j dx j:k = τ w τ c (β) i:j c (β) j:k N (x i:j ; T τ x i:k , Σ NL ) N (x j:k ; x i:k , Σ NR ) N (x i:j ; µ (β) i:j , Σ (β) i:j ) N (x j:k ; µ (β) j:k , Σ (β) j:k ) dx i:j dx j:k (74) = τ w τ c (β) i:j c (β) j:k N (T τ x i:k ; µ (β) i:j , Σ NL + Σ (β) i:j ) N (x i:k ; µ (β) j:k , Σ NR + Σ (β) j:k ) (75) = τ w τ c (β) i:j c (β) j:k N (x i:k ; T τ µ (β) i:j , T τ [Σ NL + Σ (β) i:j ]T τ ) N (x i:k ; µ (β) j:k , Σ NR + Σ (β) j:k ) (76) = τ w τ c (β) i:j:k N (x i:k ; µ (β) i:j:k , Σ (β) i:j:k ) (77) with c (β) i:j:k := c (β) i:j c (β) j:k N (T τ µ (β) i:j ; µ (β) j:k , T τ [Σ NL + Σ (β) i:j ]T τ + Σ NR + Σ (β) j:k ) (78) µ (β) i:j:k := Σ (β) i:j:k T τ Σ NL + Σ (β) i:j -1 µ (β) i:j + Σ NR + Σ (β) j:k -1 µ (β) j:k (79) Σ (β) i:j:k := T τ [Σ NL + Σ (β) i:j ]T τ -1 + Σ NR + Σ (β) j:k -1 -1 ,<label>(80)</label></formula><p>where in <ref type="bibr" target="#b73">(74)</ref> we inserted ( <ref type="formula" target="#formula_104">31</ref>) and <ref type="bibr" target="#b35">(36)</ref>; in <ref type="bibr" target="#b74">(75)</ref> we used <ref type="bibr" target="#b32">(33)</ref> twice to rewrite the pairwise products of Gaussians over x i:j and x j:k and marginalise them out; in <ref type="bibr" target="#b75">(76)</ref> we used (72) to rewrite the transformation; and in (77) we used <ref type="bibr" target="#b32">(33)</ref> a third time to rewrite the resulting product as a single Gaussian over x i:k .</p><p>For the outside probability α(x j:k ), the integrals in ( <ref type="formula" target="#formula_11">22</ref>) for x j:k being generated as the right child are</p><formula xml:id="formula_63">p N (x i:j , x j:k | x i:k ) α(x i:k ) β(x i:j ) dx i:j dx i:k (81) = τ w τ c (α) i:k c (β) i:j N (x i:j ; T τ x i:k , Σ NL ) N (x j:k ; x i:k , Σ NR ) N (x i:k ; µ (α) i:k , Σ<label>(α)</label></formula><formula xml:id="formula_64">i:k ) N (x i:j ; µ (β) i:j , Σ<label>(β)</label></formula><formula xml:id="formula_65">i:j ) dx i:j dx i:k (82) = τ w τ c<label>(α) i:k c (β) i:j</label></formula><formula xml:id="formula_66">N (x j:k ; x i:k , Σ NR ) N (x i:k ; µ (α) i:k , Σ<label>(α) i</label></formula><formula xml:id="formula_67">:k ) N (T τ x i:k ; µ (β) i:j , Σ (1) ) dx i:k (83) = τ w τ c<label>(α) i:k c (β) i:j</label></formula><formula xml:id="formula_68">N (x j:k ; x i:k , Σ NR ) N (x i:k ; µ (α) i:k , Σ<label>(α) i</label></formula><formula xml:id="formula_69">:k ) N (x i:k ; T τ µ (β) i:j , T τ Σ (1) T τ ) dx i:k (84) = τ w τ c (α) i:k c (β) i:j N (x j:k ; x i:k , Σ NR ) N (x i:k ; µ (2) , Σ (2) ) N (µ (α) i:k ; T τ µ (β) i:j , Σ (3) ) dx i:k (85) = τ w τ c (α) i:k c (β) i:j N (µ (α) i:k ; T τ µ<label>(β)</label></formula><p>i:j , Σ (3) ) N (x j:k ; µ (2) , Σ (4) ) (86)</p><formula xml:id="formula_70">= τ w τ c (α) i:j:k N (x j:k ; µ (α) i:j:k , Σ<label>(α)</label></formula><formula xml:id="formula_71">i:j:k )<label>(87)</label></formula><p>with</p><formula xml:id="formula_72">Σ (1) = Σ NL + Σ (β) i:j Σ (2) = Σ (α) i:k -1 + T τ Σ (1) T τ -1 -1<label>(88)</label></formula><formula xml:id="formula_73">Σ (3) = Σ (α) i:k + T τ Σ (1) T τ µ (2) = Σ (2) Σ (α) i:k -1 µ (α) i:k + T τ Σ (1) -1 µ (β) i:j<label>(89)</label></formula><formula xml:id="formula_74">Σ (4) = Σ NR + Σ (2)<label>(90) and c (α)</label></formula><formula xml:id="formula_75">i:j:k = c (α) i:k c (β) i:j N (µ (α) i:k ; T τ µ (β) i:j , Σ<label>(α)</label></formula><formula xml:id="formula_76">i:k + T τ [Σ NL + Σ (β) i:j ]T τ )<label>(91) µ (α)</label></formula><formula xml:id="formula_77">i:j:k = Σ (α) i:k -1 + T τ [Σ NL + Σ (β) i:j ]T τ -1 -1 Σ (α) i:k -1 µ (α) i:k + T τ Σ NL + Σ (β) i:j -1 µ (β) i:j<label>(92)</label></formula><formula xml:id="formula_78">Σ (α) i:j:k = Σ NR + Σ (α) i:k -1 + T τ [Σ NL + Σ (β) i:j ]T τ -1 -1 ,<label>(93)</label></formula><p>where in (81) we took the constant termination probability (30) out of the integral and dropped it; in (82) we inserted ( <ref type="formula" target="#formula_104">31</ref>), ( <ref type="formula" target="#formula_17">32</ref>) and <ref type="bibr" target="#b35">(36)</ref>; in (83) we applied <ref type="bibr" target="#b32">(33)</ref> to marginalise out x i:j ; in (84) we used <ref type="bibr" target="#b71">(72)</ref> to rewrite the transformation; in (85) and (86) we used (33) twice to marginalise out x i:k ; and in (87) we rewrote the final result using (91-93). Due to the asymmetric terms in the outside probabilities, the result is somewhat more complex than for the inside probabilities.</p><p>Analogously, the integrals in ( <ref type="formula" target="#formula_11">22</ref>) for x j:k being generated as the left child are</p><formula xml:id="formula_79">p N (x j:k , x k:l | x j:l ) α(x j:l ) β(x k:l ) dx j:l dx k:l (94) = τ w τ c (α) j:l c (β) k:l N (x j:k ; T τ x j:l , Σ NL ) N (x k:l ; x j:l , Σ NR ) N (x j:l ; µ (α) j:l , Σ<label>(α)</label></formula><formula xml:id="formula_80">j:l ) N (x k:l ; µ (β) k:l , Σ<label>(β)</label></formula><p>k:l ) dx j:l dx k:l (95)</p><formula xml:id="formula_81">= τ w τ c (α) j:l c (β) k:l N (x j:k ; T τ x j:l , Σ NL ) N (x j:l ; µ (α) j:l , Σ<label>(α)</label></formula><formula xml:id="formula_82">j:l ) N (x j:l ; µ (β) k:l , Σ (1 ) ) dx j:l (96) = τ w τ c (α) j:l c (β) k:l N (T τ x j:k ; x j:l , T τ Σ NL T τ ) N (x j:l ; µ (2 ) , Σ (2 ) )N (µ (α) j:l ; µ (β) k:l , Σ (3 ) ) dx j:l (97) = τ w τ c (α) j:l c (β) k:l N (µ (α) j:l ; µ (β) k:l , Σ (3 ) ) N (T τ x j:k ; µ (2 ) , Σ (4 ) )<label>(98)</label></formula><formula xml:id="formula_83">= τ w τ c (α) j:l c (β) k:l N (µ (α) j:l ; µ (β) k:l , Σ (3 ) ) N (x j:k ; T τ µ (2 ) , T τ Σ (4 ) T τ )<label>(99)</label></formula><formula xml:id="formula_84">= τ w τ c (α) j:k:l N (x j:k ; µ (α) j:k:l , Σ<label>(α)</label></formula><formula xml:id="formula_85">j:k:l )<label>(100)</label></formula><p>with</p><formula xml:id="formula_86">Σ (1 ) = Σ NR + Σ (β) k:l Σ (2 ) = Σ (α) j:l -1 + Σ (1 ) -1 -1<label>(101)</label></formula><formula xml:id="formula_87">Σ (3 ) = Σ (α) j:l + Σ (1 ) µ (2 ) = Σ (2 ) Σ<label>(α) j:l -1 µ (α)</label></formula><formula xml:id="formula_88">j:l + Σ<label>(1 ) -1 µ (β) k:l (102)</label></formula><formula xml:id="formula_89">Σ (4 ) = T τ Σ NL T τ + Σ (2 )<label>(103) and c (α)</label></formula><formula xml:id="formula_90">j:k:l = c (α) j:l c (β) k:l N (µ (α) j:l ; µ (β) k:l , Σ<label>(α)</label></formula><formula xml:id="formula_91">j:l + Σ NR + Σ (β) k:l )<label>(104) µ (α)</label></formula><formula xml:id="formula_92">j:k:l = T τ Σ (α) j:l -1 + Σ NR + Σ (β) k:l -1 -1 Σ (α) j:l -1 µ (α) j:l + Σ NR + Σ (β) k:l -1 µ (β) k:l<label>(105)</label></formula><formula xml:id="formula_93">Σ (α) j:k:l = Σ NL + T τ Σ (α) j:l -1 + Σ NR + Σ (β) k:l -1 -1 T τ .<label>(106)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4.2 Approximation</head><p>A Gaussian mixture distribution p(x) with normalised mixture weights c i , means µ i , and covariance matrices Σ i can be approximated with a single Gaussian as</p><formula xml:id="formula_94">p(x) = N (x; µ, Σ) with µ = i c i µ i and Σ = i c i Σ i + (µ i -µ)(µ i -µ) .<label>(107)</label></formula><p>The approximation p(x) matches the first and second moments of p(x) and minimises the Kullback-Leibler divergence (KLD) D KL [p(x) p(x)] <ref type="bibr" target="#b60">[61,</ref><ref type="bibr" target="#b17">18]</ref>. This direction of the KLD is the one used e.g. in expectation propagation, not the one used in e.g. variational methods <ref type="bibr" target="#b17">[18]</ref>. That means, p(x) will adequately represent the support and uncertainty of p(x) (e.g. it will be non-zero wherever p(x) is non-zero). On the other hand, a value of x may have a high probability in p(x) even though in p(x) it has not (also see Figure <ref type="figure">4</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A.4.3 Tree Induction</head><p>Exact joint optimisation of the structure and the continuous latent variables is intractable. We therefore choose the best tree for a GRBN based the maximum of the (approximated) inside probability. Inserting <ref type="bibr" target="#b29">(30)</ref> and ( <ref type="formula">77</ref>) into (20), we have</p><formula xml:id="formula_95">β(x i:k ) = (1 -p term ) k-1 j=i+1 τ w τ c (β) i:j:k N (x i:k ; µ (β) i:j:k , Σ<label>(β)</label></formula><formula xml:id="formula_96">i:j:k ) ,<label>(108)</label></formula><p>x 0:1</p><p>x 0:2</p><p>x 0:3</p><p>x 0:4</p><p>x 1:2</p><p>x 1:3</p><p>x 1:4</p><p>x 2:3</p><p>x 2:4</p><p>x 3:4</p><formula xml:id="formula_97">y 1 =0 y 2 =1 y 3 =2 y 4 =0</formula><p>x 0:4</p><p>x 0:3 which is maximised by taking the mode of the Gaussian and maximising over j and (if using transpositions) τ max</p><formula xml:id="formula_98">x 3:4 x 0:1 x 1:3 x 1:2 x 2:3 y 1 y 2 y 3 y 4</formula><formula xml:id="formula_99">(x i:k ,j,τ ) w τ c (β) i:j:k N (x i:k ; µ (β) i:j:k , Σ<label>(β)</label></formula><p>i:j:k ) = max</p><formula xml:id="formula_100">(j,τ ) w τ c (β) i:j:k 2πΣ (β) i:j:k -1 2<label>(109)</label></formula><p>If we have multi-terminal transitions (or more generally other possible transitions), we also have to maximise over the different possible transitions. For each non-terminal variable, we compute and store the best choice during bottom-up computations of the inside probabilities. Afterwards, we can construct the best tree by starting at the root node and recursively picking the best structure top-down.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Example</head><p>In this section, we present the complete calculations for the inside probabilities, the tree estimate, and the marginal likelihood for a basic GRBN (no transpositions or multi-terminal transitions) on a simple example sequence of length n = 4 (also see Figure <ref type="figure" target="#fig_3">7</ref>). We assume parameters</p><formula xml:id="formula_101">µ P = 0 Σ P = Σ NL = Σ NR = Σ T = 1 p term = 1/2<label>(110)</label></formula><p>in <ref type="bibr" target="#b26">(27)</ref><ref type="bibr" target="#b27">(28)</ref><ref type="bibr" target="#b28">(29)</ref><ref type="bibr" target="#b29">(30)</ref> and a scalar sequence Y = (y 1 , y 2 , y 3 , y 4 ) = (0, 1, 2, 0) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.1 Preliminaries</head><p>Inside probabilities are approximated with a single Gaussian</p><formula xml:id="formula_103">β(x i:k ) ≈ c (β) i:k N (x i:k ; µ (β) i:k , Σ<label>(β)</label></formula><formula xml:id="formula_104">i:k ) ,<label>(31)</label></formula><p>specified by c</p><formula xml:id="formula_105">(β) i:k , µ<label>(β)</label></formula><p>i:k , and Σ</p><p>i:k , which are the relevant quantities to be computed. At the bottom level, we use <ref type="bibr" target="#b18">(19)</ref> for the base case and insert ( <ref type="formula">29</ref>) and ( <ref type="formula" target="#formula_14">30</ref>) to obtain</p><formula xml:id="formula_107">β(x i:i+1 ) = p term N (y i+1 ; x i:i+1 , Σ T ) ,<label>(112)</label></formula><p>where we can directly read off c</p><formula xml:id="formula_108">(β) i:k , µ<label>(β)</label></formula><p>i:k , and Σ</p><p>i:k . For the higher levels, we have to use <ref type="bibr" target="#b19">(20)</ref> for the recursive case, where inserting (77) to solve the integrals in closed form gives</p><formula xml:id="formula_110">β(x i:k ) = (1 -p term ) k-1 j=i+1 c (β) i:j:k N (x i:k ; µ (β) i:j:k , Σ (β) i:j:k )<label>(113)</label></formula><p>with parameters given by (78-80) as</p><formula xml:id="formula_111">c (β) i:j:k = c (β) i:j c (β) j:k N (µ (β) i:j ; µ (β) j:k , 1 + Σ (β) i:j + 1 + Σ (β) j:k ) (114) µ (β) i:j:k = Σ (β) i:j:k 1 + Σ (β) i:j -1 µ (β) i:j + 1 + Σ (β) j:k -1 µ (β) j:k (115) Σ (β) i:j:k = 1 + Σ (β) i:j -1 + 1 + Σ (β) j:k -1 -1 ,<label>(116)</label></formula><p>where we already inserted Σ NL = Σ NR = 1.</p><p>If the sum in (113) has only a single term, we immediately get</p><formula xml:id="formula_112">c (β) i:k = (1 -p term ) c (β) i:j:k µ (β) i:k = µ (β) i:j:k Σ (β) i:k = Σ (β) i:j:k . (<label>117</label></formula><formula xml:id="formula_113">)</formula><p>If there is more than one term in the sum in (113), this means that there are multiple splitting options that are marginalised out and we therefore need to do two things.</p><p>First, we have to identify the best splitting option to be able to compute the tree estimate. This is done by using (109) and comparing the values of</p><formula xml:id="formula_114">c (β) i:j:k Σ (β) i:j:k ,<label>(118)</label></formula><p>where</p><formula xml:id="formula_115">Σ (β) i:j:k = Σ<label>(β)</label></formula><p>i:j:k in the scalar case and we left out shared constant factors.</p><p>Second, we have to approximate the resulting mixture with a single Gaussian using (107), where the mixture weights have to be normalised. For the univariate/scalar case considered here, we then get</p><formula xml:id="formula_116">µ (β) i:k = k-1 j=i+1 c (β) i:j:k µ (β) i:j:k k-1 j=i+1 c (β) i:j:k (119) Σ (β) i:k = k-1 j=i+1 c (β) i:j:k Σ (β) i:j:k + (µ (β) i:j:k -µ (β) i:k ) 2 k-1 j=i+1 c (β) i:j:k (120) c (β) i:k = (1 -p term ) k-1 j=i+1 c (β) i:j:k .<label>(121)</label></formula><p>Finally, the marginal likelihood ( <ref type="formula">17</ref>) is obtained as </p><formula xml:id="formula_117">p(Y) = β(x 0:n ) p P (x 0:n ) dx 0:n<label>(122)</label></formula><p>where we have used <ref type="bibr" target="#b32">(33)</ref> to rewrite the product of Gaussians and inserted µ P = 0 and Σ P = 1.</p><p>The normal distribution is defined as</p><formula xml:id="formula_119">N (x; µ, Σ) = 1 2π|Σ| exp - 1 2 (x -µ) Σ -1 (x -µ) (126) = 1 √ 2πΣ exp - 1 2 (x -µ) 2 Σ ,<label>(127)</label></formula><p>where the second line is for the scalar case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B.2 Calculations</head><p>We start with the inside probabilities at the bottom level for the latent variables x 0:1 , x 1:2 , x 2:3 , x 3:4 and from (112) we read off (without any approximations) </p><formula xml:id="formula_120">c (β) i:i+1 = p term = 1/2 µ (β) i:i+1 = y i+1 Σ (β) i:i+1 = Σ T = 1 (128) ≈ 1.66 • 10 -3 µ (β) 0:1:3 = 3/4 Σ (β) 0:1:3 = 1<label>(135) c (β</label></formula><p>that for x 0:3 both splits are equally well and for x 0:3 the split x 1:4 → (x 1:3 , x 3:4 ) at j = 3 is better. This is intuitively clear, since generating (y 2 , y 3 ) = (1, 2) from the same non-terminal variable x 1:3 = 1.5 is more likely than generating (y 3 , y 4 ) = (2, 0) from x 2:4 = 1, given that in both cases the values are generated from a Gaussian with variance 1.</p><p>We approximate the mixtures with a single Gaussian with parameters given by (119-121) as  </p><formula xml:id="formula_122">j:k , 1 + Σ (β) i:j + 1 + Σ (β) j:k ) (142) µ (β) i:j:k = Σ (β) i:j:k 1 + Σ (β) i:j -1 µ (β) i:j + 1 + Σ (β) j:k -1 µ (β) j:k (143) Σ (β) i:j:k = 1 + Σ (β) i:j -1 + 1 + Σ (β) j:k -1 -1<label>(144)</label></formula><p>and we see that the split x 0:4 → (x 0:3 , x 3:4 ) for j = 3 is the best one. Intuitively, this makes sense because it splits between y 3 and y 4 , which is the biggest step. Not we have only minor differences between the split options, because for simplicity we have chosen our variance parameters with a value of 1, which is relatively large compared to the spread of the values. Choosing smaller variances would result in more prominent splitting preferences.</p><p>We can now construct the full tree by also picking the best split for x 0:3 , which is a tie between splitting at j = 1 and j = 2, so we can choose either one (in practice one might consider random tie breaking to avoid biases due to variable order). The resulting tree is shown in Figure <ref type="figure" target="#fig_3">7</ref>.</p><p>The parameters for the inside probability of x 0:4 , given by approximating the three Gaussian mixture components, are We performed a quantitative evaluation on synthetic data for the task of segmenting a noisy time series and inferring the underlying tree. We used the Gaussian RBN for music (Section 2.  </p><formula xml:id="formula_124">C Dm7/C G7/B C Am D7/C G/B Cmaj7/B Am7 D7 G C#dim7/G Dm/F Bdim7/F C/E Fmaj7/E Dm7 G7 C C7 Fmaj7 F#dim7 Abdim7 G7 Gsus64 G7sus4 G7 F#dim7/G Gsus64 G7sus4 G7 C7/C F/C G7/C C D7 Dm/F G7 D7/C D7 Bdim7/F G7 G7 G7 Gsus64 G7 G7/B G/B G C/E C Fmaj7 G7 G7 G7 F/C C G C G7 G7 G7/C C C G7 C C C C</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2 Hierarchical Music Analysis</head><p>The scores were pre-processed by computing pitch-class distributions (PCDs), as used for the identification of musical keys <ref type="bibr" target="#b73">[74,</ref><ref type="bibr" target="#b74">75,</ref><ref type="bibr" target="#b75">76]</ref>, using the pitchscapes library <ref type="bibr" target="#b70">[71]</ref>. We used a resolution of 70 equally spaced time slices per piece, resulting in sequences of 12-dimensional categorical distributions. The tree shown in Figure <ref type="figure" target="#fig_2">6</ref>(c) for Johann Sebastian Bach's Prelude No. 1 in C major, BWV 846, corresponds to a harmonic expert analysis performed by the authors. Figure <ref type="figure" target="#fig_10">10</ref> shows the annotated tree with additional chord labels, which are provided in a simplified notation commonly used in Jazz lead sheets to be more accessible to a broad audience. In Table <ref type="table">1</ref>, we list the results for all 24 preludes. For a better interpretation of the model and the presented results, there are two relevant points to consider.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2.1 Chromatic versus Diatonic Transposition</head><p>It is interesting to look in more detail at what musical aspects the model can or cannot represent. In a nutshell, it can represent chromatic transposition but cannot represent diatonic transposition, which has a number of consequences, as described in the following.</p><p>The transpositions of the left child perform a cyclic rotation of the corresponding probabilities in the pitch-class distribution represented by the latent variable, which corresponds to a chromatic transposition. This determines not only which pitch classes have a significant probability to occur (the in-scale tones) but also the specific weights. For instance, the tonic and fifth scale degree typically have the highest weights. A transposition by 5 or 7 semitones from a current major key (say C major) thus corresponds to a modulation to the sub-dominant (F major) or dominant (G major) key, respectively. This includes adaptation of the fourth and seventh scale degree of the target key, respectively (B→B for F major; F→F for G major), as well as the correct assignment of strong weights to the tonic and fifth scale degree.</p><p>However, diatonic transposition cannot be represented in this way. For instance, to represent a modulation from C major to A minor, the model has two options that are both far from optimal. 1) It can choose not to apply a chromatic transposition, which ensures that all in-scale tones are correctly represented (i.e. they have significant weight). This, however, means that the relative weights are not appropriate for A minor. In particular, the strong weights on the tonic and fifth scale degree are not present and, instead, the third and seventh scale degree (C and G, the former tonic and fifth scale degree) have disproportionally strong weight. Correcting these weights has to occur through the Gaussian transitions, which can only be explained with a relatively high transition variance.</p><p>2) The second option would be to perform a chromatic transposition by 9 semitones, which ensures that the strongest weights remain on the tonic and fifth scale degree of the new key. However, three out-of-scale tones (C , F , G ) now have a high weight, while the respective in-scale tones do not.</p><p>Again, this has to be corrected for by the Gaussian transition noise at a potentially even higher cost than in the first case. This is a highly plausible explanation for why we only see non-zero weights for the identity and (chromatic) transposition by a fifth in our experiments. Any diatonic modulations are best explained by reweighting using via Gaussian transition noise without a transposition, rather than by a chromatic transposition, which would require an even stronger reweighting (except for modulation to the sub-dominant and dominant key, which can be appropriately explained by a chromatic transposition).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C.2.2 Chord Labels</head><p>It is important to note that the chord labels in the expert annotation convey significantly more information than just what pitch classes can be expected to occur in the respective section. For example, the very same pitch-class distribution of G-C-E could amongst others be labeled as a C major chord in second inversion, a G major chord with 64-suspension (Gsus64), or an A minor seventh chord with omitted root, which might be easily confused by a musically untrained annotator. Which of these labels is correct depends in many cases on the context, such as how a chord resolves to the next one. While these differences are important from a musical perspective (they express a different experience of the same musical events), our model was trained to only predict pitch-class distributions. Therefore, in its current state, it cannot reproduce these distinctions, but we expect future versions to significantly improve in this respect. End of Table <ref type="table">1</ref> </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: RBN in Chomsky normal form. The non-terminal transition p N (x , x | x) and terminal transition p T (y | x) are grouped into an RBN cell with a structural distribution p S (z | x).We use gates<ref type="bibr" target="#b57">[58]</ref> to describe structural distributions and extended factor graph notation [black squares; 59] for conditional joint distributions. Considering all possible ways how n observations can be generated by recursively applying the RBN cell produces an RBN chart, as shown in Figure3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Graphical model of the Gaussian RBN for modelling music. The additional transposition variable τ is marginalised out during inference; the number of jointly generated observations n is uniquely determined by the location in the parse chart.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: (a): Precision and recall w.r.t. the ground-truth trees of 500 sequences for different noise levels for the baseline (blue) and the maximum and marginal RBN estimates (orange, green); error bars indicate 95% confidence intervals from bootstrapping (see Appendix C.1 for technical details). (b): Prior mean learned by our GRBN for music in comparison to recent values from the literature [68]. (c): Comparison of our model (orange/grey) to an expert annotation (blue) for Johann Sebastian Bach's Prelude No. 1 in C major, BWV 846. The greyscale indicates the marginal probability of a node to exist at that particular location; the small numbers indicate the transposition in semitones for a left child; time is indicated in beats (quarter notes); the piece was divided into two-beat (half note) intervals. The plot follows the idea of scape plots [69, 70, 71].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Parse chart for a sequence of length n = 4 with best tree estimate (see text for details).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>) N (x 0:n ; 0, 1) dx 0:n + 1) N (x 0:n ; μ, Σ) dx 0:n</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>best split for each variable based on (118), we see (all variances are equal) from c</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>have the inside probability for the root variable x 0:4 with three terms in the sum in (</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 : 4 ≈ 1 .</head><label>841</label><figDesc>Figure 8: Example of the synthetic data used in the quantitative evaluation with noise levels of 0.01 (left) and 0.15 (right). (bottom): Generated three-dimensional time series; vertical dashed lines indicate the segments identified by the change point detection (CPD) method. (top): The ground-truth tree (green), the tree estimate from hierarchical clustering (HC) based on the CPD segmentation (blue), and the tree estimate of the RBN (orange). The grey scale indicates the marginal node probabilities based on the RBN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>1</head><label>1</label><figDesc>125) this results in a marginal likelihood of p(Y) ≈ 4.63 • 10 -5 . Details for Quantitative Evaluation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Harmonic analysis of Johann Sebastian Bach's Prelude No. 1 in C major, BWV 846</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Table 1 :((</head><label>1</label><figDesc>Results for all major preludes in Johann Sebastian Bach's "Wohltemperiertes Klavier I &amp; II". (left): Expected value of the latent variables, i.e. the mean of (18), colour-coded using a key-finding algorithm from the pitchscapes library<ref type="bibr" target="#b70">[71]</ref>. (right): Marginal node probability, i.e. the normalisation of (18) as well as the RBN tree estimate. Johann Sebastian Bach Prelude No. 1 in C major BWV 846 Wohltemperiertes Klavier I Prelude No. 13 in F major BWV 858 Wohltemperiertes Klavier I Johann Sebastian Bach Prelude No. 17 in A major BWV 862 Wohltemperiertes Klavier I continued on next page) Johann Sebastian Bach Prelude No. 21 in B major BWV 866 Wohltemperiertes Klavier I continued on next page) Johann Sebastian Bach Prelude No. 13 in F major BWV 882 Wohltemperiertes Klavier II Johann Sebastian Bach Prelude No. 17 in A major BWV 886 Wohltemperiertes Klavier II Prelude No. 21 in B major BWV 890 Wohltemperiertes Klavier II</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div><head>Acknowledgments and Disclosure of Funding</head><p>This project has received funding from the <rs type="funder">European Research Council (ERC)</rs> under the European Union's <rs type="programName">Horizon 2020 research and innovation programme</rs> under grant agreement No <rs type="grantNumber">760081 -PMSB</rs>. This project was conducted at the Latour Chair in Digital and <rs type="person">Cognitive Musicology</rs>, generously funded by <rs type="person">Mr. Claude Latour</rs>.</p></div>
			</div>
			<listOrg type="funding">
				<org type="funding" xml:id="_SbJBqPQ">
					<idno type="grant-number">760081 -PMSB</idno>
					<orgName type="program" subtype="full">Horizon 2020 research and innovation programme</orgName>
				</org>
			</listOrg>
			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>Recursive Bayesian Networks: Generalising and Unifying Probabilistic Context-Free Grammars and Dynamic Bayesian Networks  (1) , x (2) , . . . , y (1) , y (2) , . . . | x) ,</p><p>with non-terminal variables x (1) , x (2) , . . . ∈ X and terminal variables y (1) , y (2) , . . . ∈ Y, we introduce new non-terminal variables x y (1) , x y (2) , . . . and replace the transition by p(x (1) , x (2) , . . . , x y (1) , x y (2) , . .</p><p>where the new non-terminals x y (i) replace the original terminals y (i) . We then add new deterministic transitions p(y (1) | x y (1) )</p><p>with</p><p>Next, we compute the inside probabilities on the first level for the variables x 0:2 , x 1:3 , x 2:4 . The only possible splitting option is for j = i + 1 and from (117) we get (again without approximation)</p><p>i:i+2 = 1 (130) and hence</p><p>Turning to the values for x 0:3 and x 1:4 , we now have two terms in the sum in (113), which means that we need to evaluate the best split and approximate the mixture. The corresponding parameters of the mixture are given by (114-116) as</p><p>i:j:k = (µ  some simplifications: 1) data were continuous and not categorical, 2) they had only three dimensions instead of twelve, 3) the prior distribution did not have any transpositions, 4) the left child could only be transposed by zero or one step. The data were sampled from this model, using zero prior mean, Σ p = 1, Σ NL = Σ NR = 1 • 0.1 2 , λ = 5, equal weights W 0 = W 1 = 0.5 for transposition by zero or one, and Σ T = 1 • noise 2 with different noise levels {0.01, 0.05, 0.1, 0.15, 0.2, 0.25}. We used a terminal probability of p term = 0.6 for sampling and rejected any sequences with a length outside the range of 50-55. An example of the data is shown in Figure <ref type="figure">8</ref>.</p><p>For comparison, we used the best-performing change point detection (CPD) method from the ruptures library <ref type="bibr" target="#b71">[72]</ref> for segmenting the time series, combined with bottom-up hierarchical clustering (HC) for inferring the tree structure ("HC/CPD"). For each noise level, we selected the CPD method and parameters with best F1 score based on the ground-truth segments of 100 training sequences (see Figure <ref type="figure">9</ref>). In HC, pairs of adjacent segments with the smallest Euclidean/L2 distance between their mean values were successively combined to construct the tree. For the RBN, all parameters were trained from scratch by minimising the marginal likelihood of the observations of only 10 training sequences (i.e. no ground-truth information and less training data than for the baseline was used), separately for each noise level.</p><p>The models were evaluated on 500 test sequences by computing their precision and recall w.r.t. the ground-truth trees. Each possible node was treated as a separate binary classification task and the results reflect the number of correctly predicted nodes. Note that due to the strongly unbalanced class distribution (many more possible node locations than actual nodes in the tree) precision and recall or the combined F1 score are the appropriate performance metrics (as opposed to e.g. accuracy). For the RBN they were computed in two different ways: 1) based on the best-tree estimate ("RBN max") and 2) based on the marginal node probabilities (18) ("RBN marginal"). For the single-tree estimates (baseline model and best-tree estimate from RBNs) we compared the ground-truth and estimated tree node-by-node to count correctly predicted nodes (TP), nodes that are in the prediction but not the ground-truth (FP), and nodes that are in the ground-truth but not the prediction (FN). For the marginal node probabilities, we computed the corresponding rates by counting all nodes in the ground-truth tree (TP+FN), summing the marginal probabilities over all</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Speech &amp; Language Processing</title>
		<author>
			<persName><forename type="first">Dan</forename><surname>Jurafsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000">2000</date>
			<publisher>Pearson Education India</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">A Generative Theory of Tonal Music</title>
		<author>
			<persName><forename type="first">Fred</forename><surname>Lerdahl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ray</forename><surname>Jackendoff</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1983">1983</date>
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<author>
			<persName><forename type="first">Martin</forename><surname>Rohrmeier</surname></persName>
		</author>
		<idno type="DOI">10.11116/MTA.7.1.1</idno>
		<title level="m">The Syntax of Jazz Harmony: Diatonic Tonality, Phrase Structure, and Form. Music Theory and Analysis (MTA)</title>
		<imprint>
			<date type="published" when="2020-04">April 2020</date>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1" to="63" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Recent Advances in Hierarchical Reinforcement Learning</title>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">G</forename><surname>Barto</surname></persName>
		</author>
		<author>
			<persName><surname>Sridhar Mahadevan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Discrete Event Dynamic Systems</title>
		<idno type="ISSN">0924-6703</idno>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="341" to="379" />
			<date type="published" when="2003">2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Automated Planning and Acting</title>
		<author>
			<persName><forename type="first">Malik</forename><surname>Ghallab</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dana</forename><surname>Nau</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Paolo</forename><surname>Traverso</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016">2016</date>
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Parsing techniques. Monographs in Computer Science</title>
		<author>
			<persName><forename type="first">Dick</forename><surname>Grune</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><forename type="middle">H</forename><surname>Ceriel</surname></persName>
		</author>
		<author>
			<persName><surname>Jacobs</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007">2007</date>
			<publisher>Springer</publisher>
			<biblScope unit="page">13</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A probabilistic plan recognition algorithm based on plan tree grammars</title>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">W</forename><surname>Geib</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><forename type="middle">P</forename><surname>Goldman</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.artint.2009.01.003</idno>
	</analytic>
	<monogr>
		<title level="j">Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">173</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1101" to="1132" />
			<date type="published" when="2009-07">July 2009</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Towards a generative syntax of tonal harmony</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Rohrmeier</surname></persName>
		</author>
		<idno type="DOI">10.1080/17459737.2011.573676</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematics and Music</title>
		<idno type="ISSN">1745-9737</idno>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1745" to="9745" />
			<date type="published" when="2011-03">March 2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A structural theory of rhythm notation based on tree representations and term rewriting</title>
		<author>
			<persName><forename type="first">Florent</forename><surname>Jacquemard</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pierre</forename><surname>Donat-Bouillud</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Bresson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Mathematics and Computation in Music</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="3" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A Generalized Parsing Framework for Generative Models of Harmonic Syntax</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Harasim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Rohrmeier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Timothy</forename><forename type="middle">J</forename><surname>O'donnell</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.1492367</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 19th International Society for Music Information Retrieval Conference</title>
		<meeting>the 19th International Society for Music Information Retrieval Conference<address><addrLine>Paris</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="152" to="159" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">An efficient recognition and syntax-analysis algorithm for context-free languages</title>
		<author>
			<persName><forename type="first">Tadao</forename><surname>Kasami</surname></persName>
		</author>
		<idno>no. R-257</idno>
		<imprint>
			<date type="published" when="1966">1966</date>
		</imprint>
	</monogr>
	<note type="report_type">Coordinated Science Laboratory Report</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Recognition and parsing of context-free languages in time n3</title>
		<author>
			<persName><forename type="first">H</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><surname>Younger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information and control</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="189" to="208" />
			<date type="published" when="1967">1967</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Semiring parsing</title>
		<author>
			<persName><forename type="first">Joshua</forename><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="573" to="605" />
			<date type="published" when="1999">1999</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Foundations of Statistical Natural Language Processing</title>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hinrich</forename><surname>Schutze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1999">1999</date>
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Dynamic Bayesian Networks: Representation, Inference and Learning</title>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Murphy</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2002">2002</date>
		</imprint>
		<respStmt>
			<orgName>University of California, Berkeley</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Probabilistic Graphical Models: Principles and Techniques</title>
		<author>
			<persName><forename type="first">Daphne</forename><surname>Koller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nir</forename><surname>Friedman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009">2009</date>
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">An introduction to the Kalman filter</title>
		<author>
			<persName><forename type="first">Greg</forename><surname>Welch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gary</forename><surname>Bishop</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995">1995</date>
		</imprint>
		<respStmt>
			<orgName>University of North Carolina</orgName>
		</respStmt>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
		<title level="m">Pattern Recognition and Machine Learning (Information Science and Statistics</title>
		<editor>
			<persName><forename type="first">1</forename><surname>Springer</surname></persName>
		</editor>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
	<note>st ed. 2006. corr. 2nd printing edition</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Hyperedge replacement graph grammars. In Handbook Of Graph Grammars And Computing By Graph Transformation</title>
		<author>
			<persName><forename type="first">Frank</forename><surname>Drewes</surname></persName>
		</author>
		<author>
			<persName><forename type="first">H.-J</forename><surname>Kreowski</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Annegret</forename><surname>Habel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="95" to="162" />
			<date type="published" when="1997">1997</date>
			<publisher>World Scientific</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Node replacement graph grammars. In Handbook Of Graph Grammars And Computing By Graph Transformation</title>
		<author>
			<persName><forename type="first">Joost</forename><surname>Engelfriet</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Grzegorz</forename><surname>Rozenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="94" />
			<date type="published" when="1997">1997</date>
			<publisher>World Scientific</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Parsing visual languages with picture layout grammars</title>
		<author>
			<persName><forename type="first">Eric</forename><forename type="middle">J</forename><surname>Golin</surname></persName>
		</author>
		<idno type="DOI">10.1016/S1045-926X(05)80005-9</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Visual Languages &amp; Computing</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="371" to="393" />
			<date type="published" when="1991-12">December 1991</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Handbook of Graph Grammars and Computing by Graph Transformation</title>
		<author>
			<persName><forename type="first">Grzegorz</forename><surname>Rozenberg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997">1997</date>
			<publisher>World scientific</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Gaussian Mixture Latent Vector Grammars</title>
		<author>
			<persName><forename type="first">Yanpeng</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liwen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kewei</forename><surname>Tu</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P18-1109</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 56th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Melbourne, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1181" to="1189" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Parsing with compositional vector grammars</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 51st Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="455" to="465" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Latent-Variable PCFGs: Background and Applications</title>
		<author>
			<persName><forename type="first">B</forename><surname>Shay</surname></persName>
		</author>
		<author>
			<persName><surname>Cohen</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W17-3405</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 15th Meeting on the Mathematics of Language</title>
		<meeting>the 15th Meeting on the Mathematics of Language<address><addrLine>London, UK</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2017">2017</date>
			<biblScope unit="page" from="47" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">PyTorch: An imperative style, high-performance deep learning library</title>
		<author>
			<persName><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Francisco</forename><surname>Massa</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">James</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Trevor</forename><surname>Killeen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Natalia</forename><surname>Gimelshein</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andreas</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Raison</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sasank</forename><surname>Chilamkurthy</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Benoit</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lu</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Junjie</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Dalché-Buc</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="8024" to="8035" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Inside-Outside and Forward-Backward Algorithms Are Just Backprop (tutorial paper)</title>
		<author>
			<persName><forename type="first">Jason</forename><surname>Eisner</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/W16-5901</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Workshop on Structured Prediction for NLP</title>
		<meeting>the Workshop on Structured Prediction for NLP<address><addrLine>Austin, TX</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016">2016</date>
			<biblScope unit="page" from="1" to="17" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Compound Probabilistic Context-Free Grammars for Grammar Induction</title>
		<author>
			<persName><forename type="first">Yoon</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alexander</forename><surname>Rush</surname></persName>
		</author>
		<idno type="DOI">10.18653/v1/P19-1228</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 57th Annual Meeting of the Association for Computational Linguistics<address><addrLine>Florence, Italy</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="2369" to="2385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fast and accurate neural CRF constituency parsing</title>
		<author>
			<persName><forename type="first">Yu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Houquan</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zhenghua</forename><surname>Li</surname></persName>
		</author>
		<idno type="DOI">10.24963/ijcai.2020/560</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IJCAI</title>
		<meeting>IJCAI</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="4046" to="4053" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<author>
			<persName><forename type="first">Alexander</forename><forename type="middle">M</forename><surname>Rush</surname></persName>
		</author>
		<idno type="arXiv">arXiv:2002.00876</idno>
		<title level="m">Torch-Struct: Deep Structured Prediction Library</title>
		<imprint>
			<date type="published" when="2020-02">February 2020</date>
		</imprint>
	</monogr>
	<note>cs, stat</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Sum-product networks: A new deep architecture</title>
		<author>
			<persName><forename type="first">Hoifung</forename><surname>Poon</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pedro</forename><surname>Domingos</surname></persName>
		</author>
		<idno type="DOI">10.1109/ICCVW.2011.6130310</idno>
	</analytic>
	<monogr>
		<title level="m">2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops)</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-11">November 2011</date>
			<biblScope unit="page" from="689" to="690" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Mixed sum-product networks: A deep architecture for hybrid domains</title>
		<author>
			<persName><forename type="first">Alejandro</forename><surname>Molina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Vergari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicola</forename><forename type="middle">Di</forename><surname>Mauro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sriraam</forename><surname>Natarajan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Floriana</forename><surname>Esposito</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristian</forename><surname>Kersting</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Thirty-Second AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2018">2018</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Conditional sum-product networks: Imposing structure on deep probabilistic architectures</title>
		<author>
			<persName><forename type="first">Xiaoting</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alejandro</forename><surname>Molina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Vergari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Karl</forename><surname>Stelzner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Peharz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Thomas</forename><surname>Liebig</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristian</forename><surname>Kersting</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Probabilistic Graphical Models</title>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="401" to="412" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Factor Graph Grammars</title>
		<author>
			<persName><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Darcey</forename><surname>Riley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="volume">33</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Case-factor diagrams for structured probabilistic modeling</title>
		<author>
			<persName><forename type="first">David</forename><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fernando</forename><surname>Pereira</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.jcss.2007.04.015</idno>
	</analytic>
	<monogr>
		<title level="j">Journal of Computer and System Sciences</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="84" to="96" />
			<date type="published" when="2008-02">February 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Hidden tree Markov models for document image classification</title>
		<author>
			<persName><forename type="first">M</forename><surname>Diligenti</surname></persName>
		</author>
		<author>
			<persName><forename type="first">P</forename><surname>Frasconi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<idno type="DOI">10.1109/TPAMI.2003.1190578</idno>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<idno type="ISSN">1939-3539</idno>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="519" to="523" />
			<date type="published" when="2003-04">April 2003</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Compositional generative mapping for treestructured data-Part I: Bottom-up probabilistic modeling of trees</title>
		<author>
			<persName><forename type="first">Davide</forename><surname>Bacciu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessio</forename><surname>Micheli</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alessandro</forename><surname>Sperduti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<date type="published" when="2012">2012</date>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="page" from="1987" to="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Spectral methods for learning multivariate latent tree structure</title>
		<author>
			<persName><forename type="first">Animashree</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kamalika</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName><forename type="first">J</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Sham</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Le</forename><surname>Kakade</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tong</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">J</forename><surname>Shawe-Taylor</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Zemel</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">P</forename><surname>Bartlett</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Pereira</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="volume">24</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning latent tree graphical models</title>
		<author>
			<persName><forename type="first">Jin</forename><surname>Myung</surname></persName>
		</author>
		<author>
			<persName><surname>Choi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Animashree</forename><surname>Vincent Yf Tan</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alan</forename><forename type="middle">S</forename><surname>Anandkumar</surname></persName>
		</author>
		<author>
			<persName><surname>Willsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="1771" to="1812" />
			<date type="published" when="2011">2011</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Guaranteed scalable learning of latent tree models</title>
		<author>
			<persName><forename type="first">Furong</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Niranjan</forename><surname>Uma Naresh</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Ioakeim</forename><surname>Perros</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jimeng</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Anima</forename><surname>Anandkumar</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">Uncertainty in Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="883" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2013 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning Bayesian networks: The combination of knowledge and statistical data</title>
		<author>
			<persName><forename type="first">David</forename><surname>Heckerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">M</forename><surname>Chickering</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="197" to="243" />
			<date type="published" when="1995">1995</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Learning Bayesian networks: A unification for discrete and Gaussian domains</title>
		<author>
			<persName><forename type="first">David</forename><surname>Heckerman</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dan</forename><surname>Geiger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1302.4957</idno>
		<imprint>
			<date type="published" when="2013">2013</date>
		</imprint>
	</monogr>
	<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Structure Learning in Graphical Modeling</title>
		<author>
			<persName><forename type="first">Mathias</forename><surname>Drton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marloes</forename><forename type="middle">H</forename><surname>Maathuis</surname></persName>
		</author>
		<idno type="DOI">10.1146/annurev-statistics-060116-053803</idno>
	</analytic>
	<monogr>
		<title level="j">Annual Review of Statistics and Its Application</title>
		<idno type="ISSN">2326-8298</idno>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="2326" to="2831" />
			<date type="published" when="2017-03">March 2017</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Learning the structure of sum-product networks</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>Gens</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Domingos</forename><surname>Pedro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="873" to="880" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Online incremental structure learning of sum-product networks</title>
		<author>
			<persName><forename type="first">Sang-Woo</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Min-Oh</forename><surname>Heo</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Byoung-Tak</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013">2013</date>
			<biblScope unit="page" from="220" to="227" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning sum-product networks with direct and indirect variable interactions</title>
		<author>
			<persName><forename type="first">Amirmohammad</forename><surname>Rooshenas</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Lowd</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2014">2014</date>
			<biblScope unit="page" from="710" to="718" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Simplifying, regularizing and strengthening sumproduct network structure learning</title>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Vergari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicola</forename><forename type="middle">Di</forename><surname>Mauro</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Floriana</forename><surname>Esposito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Joint European Conference on Machine Learning and Knowledge Discovery in Databases</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015">2015</date>
			<biblScope unit="page" from="343" to="358" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Automatic Bayesian density analysis</title>
		<author>
			<persName><forename type="first">Antonio</forename><surname>Vergari</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Alejandro</forename><surname>Molina</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Peharz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kristian</forename><surname>Kersting</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Isabel</forename><surname>Valera</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the AAAI Conference on Artificial Intelligence</title>
		<meeting>the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="5207" to="5215" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning discrete structures for graph neural networks</title>
		<author>
			<persName><forename type="first">Luca</forename><surname>Franceschi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mathias</forename><surname>Niepert</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Massimiliano</forename><surname>Pontil</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiao</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="1972" to="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Graph structure learning for robust graph neural networks</title>
		<author>
			<persName><forename type="first">Wei</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Yao</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Xiaorui</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</title>
		<meeting>the 26th ACM SIGKDD International Conference on Knowledge Discovery &amp; Data Mining</meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="66" to="74" />
		</imprint>
	</monogr>
	<note>Xianfeng Tang, Suhang Wang, and Jiliang Tang</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Dag-gnn: Dag structure learning with graph neural networks</title>
		<author>
			<persName><forename type="first">Yue</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tian</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Mo</forename><surname>Yu</surname></persName>
		</author>
		<idno>PMLR</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2019">2019</date>
			<biblScope unit="page" from="7154" to="7163" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Model averaging for prediction with discrete Bayesian networks</title>
		<author>
			<persName><forename type="first">Denver</forename><surname>Dash</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Gregory</forename><forename type="middle">F</forename><surname>Cooper</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="1177" to="1203" />
			<date type="published" when="2004-09">Sep. 2004</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Tractable Bayesian learning of tree belief networks</title>
		<author>
			<persName><forename type="first">Marina</forename><surname>Meilȃ</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Tommi</forename><surname>Jaakkola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and Computing</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="77" to="92" />
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Improving the structure MCMC sampler for Bayesian networks by introducing a new edge reversal move</title>
		<author>
			<persName><forename type="first">Marco</forename><surname>Grzegorczyk</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Dirk</forename><surname>Husmeier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2008">2008</date>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="page">265</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Bayesian structure learning using dynamic programming and MCMC</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Eaton</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Third Conference on Uncertainty in Artificial Intelligence (UAI 2007)</title>
		<meeting>the Twenty-Third Conference on Uncertainty in Artificial Intelligence (UAI 2007)</meeting>
		<imprint>
			<date type="published" when="2007">2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Bayesian Learning of Sum-Product Networks</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Trapp</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Robert</forename><surname>Peharz</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Hong</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Franz</forename><surname>Pernkopf</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>
			<persName><forename type="first">H</forename><surname>Wallach</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">A</forename><surname>Beygelzimer</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">F</forename><surname>Alché-Buc</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">E</forename><surname>Fox</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">R</forename><surname>Garnett</surname></persName>
		</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2019">2019</date>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="6344" to="6355" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<monogr>
		<author>
			<persName><forename type="first">Tom</forename><surname>Minka</surname></persName>
		</author>
		<author>
			<persName><forename type="first">John</forename><forename type="middle">Winn</forename><surname>Gates</surname></persName>
		</author>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2009">2009</date>
			<biblScope unit="page" from="1073" to="1080" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Extending factor graphs so as to unify directed and undirected graphical models</title>
		<author>
			<persName><forename type="first">Brendan</forename><forename type="middle">J</forename><surname>Frey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UAI &apos;03, Proceedings of the 19th Conference in Uncertainty in Artificial Intelligence</title>
		<editor>
			<persName><forename type="first">Christopher</forename><surname>Meek</surname></persName>
		</editor>
		<editor>
			<persName><forename type="first">Uffe</forename><surname>Kjaerulff</surname></persName>
		</editor>
		<meeting><address><addrLine>Acapulco, Mexico</addrLine></address></meeting>
		<imprint>
			<publisher>Morgan Kaufmann</publisher>
			<date type="published" when="2003">August 7-10 2003. 2003</date>
			<biblScope unit="page" from="257" to="264" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Better k-best parsing</title>
		<author>
			<persName><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
		<idno type="DOI">10.3115/1654494.1654500</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth International Workshop on Parsing Technology -Parsing &apos;05</title>
		<meeting>the Ninth International Workshop on Parsing Technology -Parsing &apos;05<address><addrLine>Vancouver, British Columbia, Canada</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2005">2005</date>
			<biblScope unit="page" from="53" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Analysis of single Gaussian approximation of Gaussian mixtures in Bayesian filtering applied to mixed multiple-model estimation</title>
		<author>
			<persName><forename type="first">U</forename><surname>Orguner</surname></persName>
		</author>
		<author>
			<persName><forename type="first">M</forename><surname>Demırekler</surname></persName>
		</author>
		<idno type="DOI">10.1080/00207170701261952</idno>
	</analytic>
	<monogr>
		<title level="j">International Journal of Control</title>
		<idno type="ISSN">0020-7179</idno>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="952" to="967" />
			<date type="published" when="2007-06">June 2007</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Progressive Gaussian mixture reduction</title>
		<author>
			<persName><forename type="first">Marco</forename><forename type="middle">F</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Uwe</forename><forename type="middle">D</forename><surname>Hanebeck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2008 11th International Conference on Information Fusion</title>
		<imprint>
			<date type="published" when="2008-06">June 2008</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">A look at Gaussian mixture reduction algorithms</title>
		<author>
			<persName><forename type="first">David</forename><forename type="middle">F</forename><surname>Crouse</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Peter</forename><surname>Willett</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Krishna</forename><surname>Pattipati</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lennart</forename><surname>Svensson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">14th International Conference on Information Fusion</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011">2011</date>
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<monogr>
		<title level="m" type="main">Vasile Sima, and We The. The matrix cookbook</title>
		<author>
			<persName><forename type="first">Kaare</forename><surname>Brandt Petersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Michael</forename><surname>Syskind Pedersen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Jan</forename><surname>Larsen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Korbinian</forename><surname>Strimmer</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lars</forename><surname>Christiansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Kai</forename><surname>Hansen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Liguo</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Loic</forename><surname>Thibaut</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Miguel</forename><surname>Barão</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Stephan</forename><surname>Hattinger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
	<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Processing of hierarchical syntactic structure in music</title>
		<author>
			<persName><forename type="first">Stefan</forename><surname>Koelsch</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Rohrmeier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">R</forename><surname>Torrecuso</surname></persName>
		</author>
		<author>
			<persName><forename type="first">S</forename><surname>Jentschke</surname></persName>
		</author>
		<idno type="DOI">10.1073/pnas.1300272110</idno>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the National Academy of Sciences</title>
		<idno type="ISSN">0027-8424</idno>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="issue">38</biblScope>
			<biblScope unit="page" from="1091" to="6490" />
			<date type="published" when="2013-09">September 2013</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<monogr>
		<author>
			<persName><forename type="first">Carl</forename><surname>Edward Rasmussen</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Christopher</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<title level="m">Gaussian Processes for Machine Learning</title>
		<meeting><address><addrLine>Cambridge, Massachusetts / London, England</addrLine></address></meeting>
		<imprint>
			<publisher>The MIT Press</publisher>
			<date type="published" when="2006">2006</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Dirichlet-based Gaussian processes for large-scale calibrated classification</title>
		<author>
			<persName><forename type="first">Dimitrios</forename><surname>Milios</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Raffaello</forename><surname>Camoriano</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Pietro</forename><surname>Michiardi</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Lorenzo</forename><surname>Rosasco</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Maurizio</forename><surname>Filippone</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="6005" to="6015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Exploring the foundations of tonality: Statistical cognitive modeling of modes in the history of Western classical music</title>
		<author>
			<persName><forename type="first">Daniel</forename><surname>Harasim</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Fabian</forename><forename type="middle">C</forename><surname>Moss</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Matthias</forename><surname>Ramirez</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Rohrmeier</surname></persName>
		</author>
		<idno type="DOI">10.1057/s41599-020-00678-6</idno>
	</analytic>
	<monogr>
		<title level="j">Humanities and Social Sciences Communications</title>
		<idno type="ISSN">2662-9992</idno>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="11" />
			<date type="published" when="2021-01">January 2021</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b68">
	<analytic>
		<title level="a" type="main">Harmonic Visualizations of Tonal Music</title>
		<author>
			<persName><forename type="first">Craig</forename><surname>Stuart</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Sapp</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. International Computer Music Conference (ICMC)</title>
		<meeting>International Computer Music Conference (ICMC)<address><addrLine>Havana, Cuba</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001">2001</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b69">
	<analytic>
		<title level="a" type="main">A Scape Plot Representation for Visualizing Repetitive Structures of Music Recordings</title>
		<author>
			<persName><forename type="first">Meinard</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nanzhu</forename><surname>Jiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISMIR</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2012">2012</date>
			<biblScope unit="page" from="97" to="102" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b70">
	<analytic>
		<title level="a" type="main">Modelling Hierarchical Key Structure With Pitch Scapes</title>
		<author>
			<persName><forename type="first">Robert</forename><surname>Lieck</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Martin</forename><surname>Rohrmeier</surname></persName>
		</author>
		<idno type="DOI">10.5281/zenodo.4245558</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Society for Music Information Retrieval Conference</title>
		<meeting>the 21st International Society for Music Information Retrieval Conference<address><addrLine>Montréal, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2020">2020</date>
			<biblScope unit="page" from="811" to="818" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b71">
	<analytic>
		<title level="a" type="main">Selective review of offline change point detection methods</title>
		<author>
			<persName><forename type="first">Charles</forename><surname>Truong</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Laurent</forename><surname>Oudre</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Nicolas</forename><surname>Vayatis</surname></persName>
		</author>
		<idno type="DOI">10.1016/j.sigpro.2019.107299</idno>
	</analytic>
	<monogr>
		<title level="j">Signal Processing</title>
		<imprint>
			<biblScope unit="volume">167</biblScope>
			<biblScope unit="page">107299</biblScope>
			<date type="published" when="2020-02">February 2020</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b72">
	<analytic>
		<title level="a" type="main">Musical Syntax I: Theoretical Perspectives</title>
		<author>
			<persName><forename type="first">Martin</forename><surname>Rohrmeier</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Marcus</forename><surname>Pearce</surname></persName>
		</author>
		<idno type="DOI">10.1007/978-3-662-55004-5_25</idno>
	</analytic>
	<monogr>
		<title level="m">Springer Handbook of Systematic Musicology</title>
		<editor>
			<persName><forename type="first">Rolf</forename><surname>Bader</surname></persName>
		</editor>
		<meeting><address><addrLine>Berlin Heidelberg, Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018">2018</date>
			<biblScope unit="page" from="473" to="486" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b73">
	<analytic>
		<title level="a" type="main">Tracing the dynamic changes in perceived tonal organization in a spatial representation of musical keys</title>
		<author>
			<persName><forename type="first">Carol</forename><forename type="middle">L</forename><surname>Krumhansl</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Edward</forename><forename type="middle">J</forename><surname>Kessler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Psychological review</title>
		<imprint>
			<biblScope unit="volume">89</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">334</biblScope>
			<date type="published" when="1982">1982</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b74">
	<analytic>
		<title level="a" type="main">Pitch-Class Distribution and the Identification of Key</title>
		<author>
			<persName><forename type="first">David</forename><surname>Temperley</surname></persName>
		</author>
		<author>
			<persName><forename type="first">Elizabeth</forename><forename type="middle">West</forename><surname>Marvin</surname></persName>
		</author>
		<idno type="DOI">10.1525/mp.2008.25.3.193</idno>
	</analytic>
	<monogr>
		<title level="j">Music Perception</title>
		<idno type="ISSN">0730-7829</idno>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="193" to="212" />
			<date type="published" when="2008-02">February 2008</date>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b75">
	<analytic>
		<title level="a" type="main">A Statistical Approach to Tracing the Historical Development of Major and Minor Pitch Distributions</title>
		<author>
			<persName><forename type="first">Joshua</forename><forename type="middle">D</forename><surname>Albrecht</surname></persName>
		</author>
		<author>
			<persName><forename type="first">David</forename><surname>Huron</surname></persName>
		</author>
		<idno type="DOI">10.1525/mp.2014.31.3.223</idno>
	</analytic>
	<monogr>
		<title level="j">Music Perception</title>
		<idno type="ISSN">0730-7829</idno>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="223" to="243" />
			<date type="published" when="2014-02">February 2014</date>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
